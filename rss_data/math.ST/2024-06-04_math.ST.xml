<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Jun 2024 04:00:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Matrix-valued Factor Model with Time-varying Main Effects</title>
      <link>https://arxiv.org/abs/2406.00128</link>
      <description>arXiv:2406.00128v1 Announce Type: new 
Abstract: We introduce the matrix-valued time-varying Main Effects Factor Model (MEFM). MEFM is a generalization to the traditional matrix-valued factor model (FM). We give rigorous definitions of MEFM and its identifications, and propose estimators for the time-varying grand mean, row and column main effects, and the row and column factor loading matrices for the common component. Rates of convergence for different estimators are spelt out, with asymptotic normality shown. The core rank estimator for the common component is also proposed, with consistency of the estimators presented. We propose a test for testing if FM is sufficient against the alternative that MEFM is necessary, and demonstrate the power of such a test in various simulation settings. We also demonstrate numerically the accuracy of our estimators in extended simulation experiments. A set of NYC Taxi traffic data is analysed and our test suggests that MEFM is indeed necessary for analysing the data against a traditional FM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00128v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Clifford Lam, Zetai Cen</dc:creator>
    </item>
    <item>
      <title>Algebraic Geometrical Analysis of Metropolis Algorithm When Parameters Are Non-identifiable</title>
      <link>https://arxiv.org/abs/2406.00369</link>
      <description>arXiv:2406.00369v1 Announce Type: new 
Abstract: The Metropolis algorithm is one of the Markov chain Monte Carlo (MCMC) methods that realize sampling from the target probability distribution. In this paper, we are concerned with the sampling from the distribution in non-identifiable cases that involve models with Fisher information matrices that may fail to be invertible. The theoretical adjustment of the step size, which is the variance of the candidate distribution, is difficult for non-identifiable cases. In this study, to establish such a principle, the average acceptance rate, which is used as a guideline to optimize the step size in the MCMC method, was analytically derived in non-identifiable cases. The optimization principle for the step size was developed from the viewpoint of the average acceptance rate. In addition, we performed numerical experiments on some specific target distributions to verify the effectiveness of our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00369v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kenji Nagata, Yoh-ichi Mototake</dc:creator>
    </item>
    <item>
      <title>Products, Abstractions and Inclusions of Causal Spaces</title>
      <link>https://arxiv.org/abs/2406.00388</link>
      <description>arXiv:2406.00388v1 Announce Type: new 
Abstract: Causal spaces have recently been introduced by Park et al., in their paper "A Measure-Theoretic Axiomatisation of Causality" which appeared in the NeurIPS 2023 conference, as a measure-theoretic framework to encode the notion of causality. While it has some advantages over established frameworks, such as structural causal models, the theory is so far only developed for single causal spaces. In many mathematical theories, not least the theory of probability spaces of which causal spaces are a direct extension, combinations of objects and maps between objects form a central part. In this paper, taking inspiration from such objects in probability theory, we propose the definitions of products of causal spaces, as well as (stochastic) transformations between causal spaces. In the context of causality, these quantities can be given direct semantic interpretations as causally independent components, abstractions and extensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00388v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Buchholz, Junhyung Park, Bernhard Sch\"olkopf</dc:creator>
    </item>
    <item>
      <title>Non-asymptotic Properties of Generalized Mondrian Forests in Statistical Learning</title>
      <link>https://arxiv.org/abs/2406.00660</link>
      <description>arXiv:2406.00660v1 Announce Type: new 
Abstract: Since the publication of Breiman (2001), Random Forests (RF) have been widely used in both regression and classification. Later on, other forests are also proposed and studied in literature and Mondrian Forests are notable examples built on the Mondrian process; see Lakshminarayanan et al. (2014). In this paper, we propose an ensemble estimator in general statistical learning based on Mondrian Forests, which can be regarded as an extension of RF. This general framework includes many common learning problems, such as least squared regression, least $\ell_1$ regression, quantile regression and classification. Under mild conditions of loss functions, we give the upper bound of the regret function of our estimator and show that such estimator is statistically consistent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00660v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoran Zhan, Jingli Wang, Yingcun Xia</dc:creator>
    </item>
    <item>
      <title>Profiled Transfer Learning for High Dimensional Linear Model</title>
      <link>https://arxiv.org/abs/2406.00701</link>
      <description>arXiv:2406.00701v1 Announce Type: new 
Abstract: We develop here a novel transfer learning methodology called Profiled Transfer Learning (PTL). The method is based on the \textit{approximate-linear} assumption between the source and target parameters. Compared with the commonly assumed \textit{vanishing-difference} assumption and \textit{low-rank} assumption in the literature, the \textit{approximate-linear} assumption is more flexible and less stringent. Specifically, the PTL estimator is constructed by two major steps. Firstly, we regress the response on the transferred feature, leading to the profiled responses. Subsequently, we learn the regression relationship between profiled responses and the covariates on the target data. The final estimator is then assembled based on the \textit{approximate-linear} relationship. To theoretically support the PTL estimator, we derive the non-asymptotic upper bound and minimax lower bound. We find that the PTL estimator is minimax optimal under appropriate regularity conditions. Extensive simulation studies are presented to demonstrate the finite sample performance of the new method. A real data example about sentence prediction is also presented with very encouraging results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00701v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziqian Lin, Junlong Zhao, Fang Wang, Hansheng Wang</dc:creator>
    </item>
    <item>
      <title>Weak convergence of adaptive Markov chain Monte Carlo</title>
      <link>https://arxiv.org/abs/2406.00820</link>
      <description>arXiv:2406.00820v1 Announce Type: new 
Abstract: This article develops general conditions for weak convergence of adaptive Markov chain Monte Carlo processes and is shown to imply a weak law of large numbers for bounded Lipschitz continuous functions. This allows an estimation theory for adaptive Markov chain Monte Carlo where previously developed theory in total variation may fail or be difficult to establish. Extensions of weak convergence to general Wasserstein distances are established along with a weak law of large numbers for possibly unbounded Lipschitz functions. Applications are applied to auto-regressive processes in various settings, unadjusted Langevin processes, and adaptive Metropolis-Hastings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00820v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Austin Brown, Jeffrey S. Rosenthal</dc:creator>
    </item>
    <item>
      <title>Dynamic Structural Causal Models</title>
      <link>https://arxiv.org/abs/2406.01161</link>
      <description>arXiv:2406.01161v1 Announce Type: new 
Abstract: We study a specific type of SCM, called a Dynamic Structural Causal Model (DSCM), whose endogenous variables represent functions of time, which is possibly cyclic and allows for latent confounding. As a motivating use-case, we show that certain systems of Stochastic Differential Equations (SDEs) can be appropriately represented with DSCMs. An immediate consequence of this construction is a graphical Markov property for systems of SDEs. We define a time-splitting operation, allowing us to analyse the concept of local independence (a notion of continuous-time Granger (non-)causality). We also define a subsampling operation, which returns a discrete-time DSCM, and which can be used for mathematical analysis of subsampled time-series. We give suggestions how DSCMs can be used for identification of the causal effect of time-dependent interventions, and how existing constraint-based causal discovery algorithms can be applied to time-series data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01161v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philip Boeken, Joris M. Mooij</dc:creator>
    </item>
    <item>
      <title>Integral Probability Metrics on submanifolds: interpolation inequalities and optimal inference</title>
      <link>https://arxiv.org/abs/2406.01268</link>
      <description>arXiv:2406.01268v1 Announce Type: new 
Abstract: We study interpolation inequalities between H\"older Integral Probability Metrics (IPMs) in the case where the measures have densities on closed submanifolds. Precisely, it is shown that if two probability measures $\mu$ and $\mu^\star$ have $\beta$-smooth densities with respect to the volume measure of some submanifolds $\mathcal{M}$ and $\mathcal{M}^\star$ respectively, then the H\"older IPMs $d_{\mathcal{H}^\gamma_1}$ of smoothness $\gamma\geq 1$ and $d_{\mathcal{H}^\eta_1}$ of smoothness $\eta&gt;\gamma$, satisfy $d_{ \mathcal{H}_1^{\gamma}}(\mu,\mu^\star)\lesssim d_{ \mathcal{H}_1^{\eta}}(\mu,\mu^\star)^\frac{\beta+\gamma}{\beta+\eta}$, up to logarithmic factors. We provide an application of this result to high-dimensional inference. These functional inequalities turn out to be a key tool for density estimation on unknown submanifold. In particular, it allows to build the first estimator attaining optimal rates of estimation for all the distances $d_{\mathcal{H}_1^\gamma}$, $\gamma \in [1,\infty)$ simultaneously.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01268v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arthur St\'ephanovitch</dc:creator>
    </item>
    <item>
      <title>On the Convergence Rates of Set Membership Estimation of Linear Systems with Disturbances Bounded by General Convex Sets</title>
      <link>https://arxiv.org/abs/2406.00574</link>
      <description>arXiv:2406.00574v1 Announce Type: cross 
Abstract: This paper studies the uncertainty set estimation of system parameters of linear dynamical systems with bounded disturbances, which is motivated by robust (adaptive) constrained control. Departing from the confidence bounds of least square estimation from the machine-learning literature, this paper focuses on a method commonly used in (robust constrained) control literature: set membership estimation (SME). SME tends to enjoy better empirical performance than LSE's confidence bounds when the system disturbances are bounded. However, the theoretical guarantees of SME are not fully addressed even for i.i.d. bounded disturbances. In the literature, SME's convergence has been proved for general convex supports of the disturbances, but SME's convergence rate assumes a special type of disturbance support: $l_\infty$ ball. The main contribution of this paper is relaxing the assumption on the disturbance support and establishing the convergence rates of SME for general convex supports, which closes the gap on the applicability of the convergence and convergence rates results. Numerical experiments on SME and LSE's confidence bounds are also provided for different disturbance supports.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00574v1</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haonan Xu, Yingying Li</dc:creator>
    </item>
    <item>
      <title>Generalization Bound and New Algorithm for Clean-Label Backdoor Attack</title>
      <link>https://arxiv.org/abs/2406.00588</link>
      <description>arXiv:2406.00588v1 Announce Type: cross 
Abstract: The generalization bound is a crucial theoretical tool for assessing the generalizability of learning methods and there exist vast literatures on generalizability of normal learning, adversarial learning, and data poisoning. Unlike other data poison attacks, the backdoor attack has the special property that the poisoned triggers are contained in both the training set and the test set and the purpose of the attack is two-fold. To our knowledge, the generalization bound for the backdoor attack has not been established. In this paper, we fill this gap by deriving algorithm-independent generalization bounds in the clean-label backdoor attack scenario. Precisely, based on the goals of backdoor attack, we give upper bounds for the clean sample population errors and the poison population errors in terms of the empirical error on the poisoned training dataset. Furthermore, based on the theoretical result, a new clean-label backdoor attack is proposed that computes the poisoning trigger by combining adversarial noise and indiscriminate poison. We show its effectiveness in a variety of settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00588v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lijia Yu, Shuang Liu, Yibo Miao, Xiao-Shan Gao, Lijun Zhang</dc:creator>
    </item>
    <item>
      <title>On the modelling and prediction of high-dimensional functional time series</title>
      <link>https://arxiv.org/abs/2406.00700</link>
      <description>arXiv:2406.00700v1 Announce Type: cross 
Abstract: We propose a two-step procedure to model and predict high-dimensional functional time series, where the number of function-valued time series $p$ is large in relation to the length of time series $n$. Our first step performs an eigenanalysis of a positive definite matrix, which leads to a one-to-one linear transformation for the original high-dimensional functional time series, and the transformed curve series can be segmented into several groups such that any two subseries from any two different groups are uncorrelated both contemporaneously and serially. Consequently in our second step those groups are handled separately without the information loss on the overall linear dynamic structure. The second step is devoted to establishing a finite-dimensional dynamical structure for all the transformed functional time series within each group. Furthermore the finite-dimensional structure is represented by that of a vector time series. Modelling and forecasting for the original high-dimensional functional time series are realized via those for the vector time series in all the groups. We investigate the theoretical properties of our proposed methods, and illustrate the finite-sample performance through both extensive simulation and two real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00700v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Qin Fang, Xinghao Qiao, Qiwei Yao</dc:creator>
    </item>
    <item>
      <title>A Tutorial on Doubly Robust Learning for Causal Inference</title>
      <link>https://arxiv.org/abs/2406.00853</link>
      <description>arXiv:2406.00853v1 Announce Type: cross 
Abstract: Doubly robust learning offers a robust framework for causal inference from observational data by integrating propensity score and outcome modeling. Despite its theoretical appeal, practical adoption remains limited due to perceived complexity and inaccessible software. This tutorial aims to demystify doubly robust methods and demonstrate their application using the EconML package. We provide an introduction to causal inference, discuss the principles of outcome modeling and propensity scores, and illustrate the doubly robust approach through simulated case studies. By simplifying the methodology and offering practical coding examples, we intend to make doubly robust learning accessible to researchers and practitioners in data science and statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00853v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hlynur Dav\'i{\dh} Hlynsson</dc:creator>
    </item>
    <item>
      <title>Faster Diffusion-based Sampling with Randomized Midpoints: Sequential and Parallel</title>
      <link>https://arxiv.org/abs/2406.00924</link>
      <description>arXiv:2406.00924v1 Announce Type: cross 
Abstract: In recent years, there has been a surge of interest in proving discretization bounds for diffusion models. These works show that for essentially any data distribution, one can approximately sample in polynomial time given a sufficiently accurate estimate of its score functions at different noise levels. In this work, we propose a new discretization scheme for diffusion models inspired by Shen and Lee's randomized midpoint method for log-concave sampling~\cite{ShenL19}. We prove that this approach achieves the best known dimension dependence for sampling from arbitrary smooth distributions in total variation distance ($\widetilde O(d^{5/12})$ compared to $\widetilde O(\sqrt{d})$ from prior work). We also show that our algorithm can be parallelized to run in only $\widetilde O(\log^2 d)$ parallel rounds, constituting the first provable guarantees for parallel sampling with diffusion models.
  As a byproduct of our methods, for the well-studied problem of log-concave sampling in total variation distance, we give an algorithm and simple analysis achieving dimension dependence $\widetilde O(d^{5/12})$ compared to $\widetilde O(\sqrt{d})$ from prior work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00924v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shivam Gupta, Linda Cai, Sitan Chen</dc:creator>
    </item>
    <item>
      <title>Kronecker-product random matrices and a matrix least squares problem</title>
      <link>https://arxiv.org/abs/2406.00961</link>
      <description>arXiv:2406.00961v1 Announce Type: cross 
Abstract: We study the eigenvalue distribution and resolvent of a Kronecker-product random matrix model $A \otimes I_{n \times n}+I_{n \times n} \otimes B+\Theta \otimes \Xi \in \mathbb{C}^{n^2 \times n^2}$, where $A,B$ are independent Wigner matrices and $\Theta,\Xi$ are deterministic and diagonal. For fixed spectral arguments, we establish a quantitative approximation for the Stieltjes transform by that of an approximating free operator, and a diagonal deterministic equivalent approximation for the resolvent. We further obtain sharp estimates in operator norm for the $n \times n$ resolvent blocks, and show that off-diagonal resolvent entries fall on two differing scales of $n^{-1/2}$ and $n^{-1}$ depending on their locations in the Kronecker structure.
  Our study is motivated by consideration of a matrix-valued least-squares optimization problem $\min_{X \in \mathbb{R}^{n \times n}} \frac{1}{2}\|XA+BX\|_F^2+\frac{1}{2}\sum_{ij} \xi_i\theta_j x_{ij}^2$ subject to a linear constraint. For random instances of this problem defined by Wigner inputs $A,B$, our analyses imply an asymptotic characterization of the minimizer $X$ and its associated minimum objective value as $n \to \infty$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00961v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhou Fan, Renyuan Ma</dc:creator>
    </item>
    <item>
      <title>Joint Learning of Linear Dynamical Systems under Smoothness Constraints</title>
      <link>https://arxiv.org/abs/2406.01094</link>
      <description>arXiv:2406.01094v1 Announce Type: cross 
Abstract: We consider the problem of joint learning of multiple linear dynamical systems. This has received significant attention recently under different types of assumptions on the model parameters. The setting we consider involves a collection of $m$ linear systems each of which resides on a node of a given undirected graph $G = ([m], \mathcal{E})$. We assume that the system matrices are marginally stable, and satisfy a smoothness constraint w.r.t $G$ -- akin to the quadratic variation of a signal on a graph. Given access to the states of the nodes over $T$ time points, we then propose two estimators for joint estimation of the system matrices, along with non-asymptotic error bounds on the mean-squared error (MSE). In particular, we show conditions under which the MSE converges to zero as $m$ increases, typically polynomially fast w.r.t $m$. The results hold under mild (i.e., $T \sim \log m$), or sometimes, even no assumption on $T$ (i.e. $T \geq 2$).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01094v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hemant Tyagi</dc:creator>
    </item>
    <item>
      <title>Adaptive discretization algorithms for locally optimal experimental design</title>
      <link>https://arxiv.org/abs/2406.01541</link>
      <description>arXiv:2406.01541v1 Announce Type: cross 
Abstract: We develop adaptive discretization algorithms for locally optimal experimental design of nonlinear prediction models. With these algorithms, we refine and improve a pertinent state-of-the-art algorithm in various respects. We establish novel termination, convergence, and convergence rate results for the proposed algorithms. In particular, we prove a sublinear convergence rate result under very general assumptions on the design criterion and, most notably, a linear convergence result under the additional assumption that the design criterion is strongly convex and the design space is finite. Additionally, we prove the finite termination at approximately optimal designs, including upper bounds on the number of iterations until termination. And finally, we illustrate the practical use of the proposed algorithms by means of two application examples from chemical engineering: one with a stationary model and one with a dynamic model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01541v1</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jochen Schmid, Philipp Seufert, Michael Bortz</dc:creator>
    </item>
    <item>
      <title>Examining properness in the external validation of survival models with squared and logarithmic losses</title>
      <link>https://arxiv.org/abs/2212.05260</link>
      <description>arXiv:2212.05260v2 Announce Type: replace 
Abstract: Scoring rules promote rational and honest decision-making, which is becoming increasingly important for automated procedures in `auto-ML'. In this paper we survey common squared and logarithmic scoring rules for survival analysis and determine which losses are proper and improper. We prove that commonly utilised squared and logarithmic scoring rules that are claimed to be proper are in fact improper, such as the Integrated Survival Brier Score (ISBS). We further prove that under a strict set of assumptions a class of scoring rules is strictly proper for, what we term, `approximate' survival losses. Despite the difference in properness, experiments in simulated and real-world datasets show there is no major difference between improper and proper versions of the widely-used ISBS, ensuring that we can reasonably trust previous experiments utilizing the original score for evaluation purposes. We still advocate for the use of proper scoring rules, as even minor differences between losses can have important implications in automated processes such as model tuning. We hope our findings encourage further research into the properties of survival measures so that robust and honest evaluation of survival models can be achieved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.05260v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raphael Sonabend, John Zobolas, Philipp Kopper, Lukas Burk, Andreas Bender</dc:creator>
    </item>
    <item>
      <title>Weighted (residual) varentropy and its applications</title>
      <link>https://arxiv.org/abs/2305.00852</link>
      <description>arXiv:2305.00852v2 Announce Type: replace 
Abstract: In information theory, it is of recent interest to study variability of the uncertainty measures. In this regard, the concept of varentropy has been introduced and studied by several authors in recent past. In this communication, we study the weighted varentropy and weighted residual varentropy. Several theoretical results of these variability measures such as the effect under monotonic transformations and bounds are investigated. Importance of the weighted residual varentropy over the residual varentropy is presented. Further, we study weighted varentropy for coherent systems and weighted residual varentropy for proportional hazard rate models. A kernel-based non-parametric estimator for the weighted residual varentropy is also proposed. The estimation method is illustrated using simulated and two real data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.00852v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shital Saha, Suchandan Kayal</dc:creator>
    </item>
    <item>
      <title>Kernel Stein Discrepancy on Lie Groups: Theory and Applications</title>
      <link>https://arxiv.org/abs/2305.12551</link>
      <description>arXiv:2305.12551v3 Announce Type: replace 
Abstract: Distributional approximation is a fundamental problem in machine learning with numerous applications across all fields of science and engineering and beyond. The key challenge in most approximation methods is the need to tackle the intractable normalization constant pertaining to the parametrized distributions used to model the data. In this paper, we present a novel Stein operator on Lie groups leading to a kernel Stein discrepancy (KSD) which is a normalization-free loss function. We present several theoretical results characterizing the properties of this new KSD on Lie groups and its minimizers namely, the minimum KSD estimator (MKSDE). Proof of several properties of MKSDE are presented, including strong consistency, CLT and a closed form of the MKSDE for the von Mises-Fisher distribution on SO(N). Finally, we present experimental evidence depicting advantages of minimizing KSD over maximum likelihood estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.12551v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoda Qu, Xiran Fan, Baba C. Vemuri</dc:creator>
    </item>
    <item>
      <title>General weighted information and relative information generating functions with properties</title>
      <link>https://arxiv.org/abs/2305.18746</link>
      <description>arXiv:2305.18746v2 Announce Type: replace 
Abstract: In this work, we propose two information generating functions: general weighted information and relative information generating functions, and study their properties. { It is shown that the general weighted information generating function (GWIGF) is shift-dependent and can be expressed in terms of the weighted Shannon entropy. The GWIGF of a transformed random variable has been obtained in terms of the GWIGF of a known distribution. Several bounds of the GWIGF have been proposed. We have obtained sufficient conditions under which the GWIGFs of two distributions are comparable. Further, we have established a connection between the weighted varentropy and varentropy with proposed GWIGF. An upper bound for GWIGF of the sum of two independent random variables is derived. The effect of general weighted relative information generating function (GWRIGF) for two transformed random variables under strictly monotone functions has been studied. } Further, these information generating functions are studied for escort, generalized escort and mixture distributions. {Specially, we propose weighted $\beta$-cross informational energy and establish a close connection with GWIGF for escort distribution.} The residual versions of the newly proposed generating functions are considered and several similar properties have been explored. A non-parametric estimator of the residual general weighted information generating function is proposed. A simulated data set and two real data sets are considered for the purpose of illustration. { Finally, we have compared the non-parametric approach with a parametric approach in terms of the absolute bias and mean squared error values.}</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.18746v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shital Saha, Suchandan Kayal</dc:creator>
    </item>
    <item>
      <title>A supervised deep learning method for nonparametric density estimation</title>
      <link>https://arxiv.org/abs/2306.10471</link>
      <description>arXiv:2306.10471v2 Announce Type: replace 
Abstract: Nonparametric density estimation is an unsupervised learning problem. In this work we propose a two-step procedure that casts the density estimation problem in the first step into a supervised regression problem. The advantage is that we can afterwards apply supervised learning methods. Compared to the standard nonparametric regression setting, the proposed procedure creates, however, dependence among the training samples. To derive statistical risk bounds, one can therefore not rely on the well-developed theory for i.i.d. data. To overcome this, we prove an oracle inequality for this specific form of data dependence. As an application, it is shown that under a compositional structure assumption on the underlying density, the proposed two-step method achieves convergence rates that are faster than the standard nonparametric rates. A simulation study illustrates the finite sample performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.10471v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thijs Bos, Johannes Schmidt-Hieber</dc:creator>
    </item>
    <item>
      <title>Testing for the extent of instability in nearly unstable processes</title>
      <link>https://arxiv.org/abs/2310.13444</link>
      <description>arXiv:2310.13444v2 Announce Type: replace 
Abstract: This paper deals with unit root issues in time series analysis. It has been known for a long time that unit root tests may be flawed when a series although stationary has a root close to unity. That motivated recent papers dedicated to autoregressive processes where the bridge between stability and instability is expressed by means of time-varying coefficients. The process we consider has a companion matrix $A_{n}$ with spectral radius $\rho(A_{n}) &lt; 1$ satisfying $\rho(A_{n}) \rightarrow 1$, a situation described as `nearly-unstable'. The question we investigate is: given an observed path supposed to come from a nearly-unstable process, is it possible to test for the `extent of instability', i.e. to test how close we are to the unit root? In this regard, we develop a strategy to evaluate $\alpha$ and to test for $\mathcal{H}_0 : ``\alpha = \alpha_0"$ against $\mathcal{H}_1 : ``\alpha &gt; \alpha_0"$ when $\rho(A_{n})$ lies in an inner $O(n^{-\alpha})$-neighborhood of the unity, for some $0 &lt; \alpha &lt; 1$. Empirical evidence is given about the advantages of the flexibility induced by such a procedure compared to the common unit root tests. We also build a symmetric procedure for the usually left out situation where the dominant root lies around $-1$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13444v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marie Badreau, Fr\'ed\'eric Pro\"ia</dc:creator>
    </item>
    <item>
      <title>Copula-based extropy measures, properties and dependence in bivariate distributions</title>
      <link>https://arxiv.org/abs/2311.08061</link>
      <description>arXiv:2311.08061v2 Announce Type: replace 
Abstract: In this work, we propose extropy measures based on density copula, distributional copula, and survival copula, and explore their properties. We study the effect of monotone transformations for the proposed measures and obtain bounds. We establish connections between cumulative copula extropy and three dependence measures: Spearman's rho, Kendall's tau, and Blest's measure of rank correlation. Finally, we propose estimators for the cumulative copula extropy and survival copula extropy with an illustration using real life datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.08061v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shital Saha, Suchandan Kayal</dc:creator>
    </item>
    <item>
      <title>Consistency of semi-supervised learning, stochastic tug-of-war games, and the p-Laplacian</title>
      <link>https://arxiv.org/abs/2401.07463</link>
      <description>arXiv:2401.07463v2 Announce Type: replace 
Abstract: In this paper we give a broad overview of the intersection of partial differential equations (PDEs) and graph-based semi-supervised learning. The overview is focused on a large body of recent work on PDE continuum limits of graph-based learning, which have been used to prove well-posedness of semi-supervised learning algorithms in the large data limit. We highlight some interesting research directions revolving around consistency of graph-based semi-supervised learning, and present some new results on the consistency of $p$-Laplacian semi-supervised learning using the stochastic tug-of-war game interpretation of the $p$-Laplacian. We also present the results of some numerical experiments that illustrate our results and suggest directions for future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07463v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeff Calder, Nadejda Drenska</dc:creator>
    </item>
    <item>
      <title>Rational Maximum Likelihood Estimators of Kronecker Covariance Matrices</title>
      <link>https://arxiv.org/abs/2401.08280</link>
      <description>arXiv:2401.08280v2 Announce Type: replace 
Abstract: As is the case for many curved exponential families, the computation of maximum likelihood estimates in a multivariate normal model with a Kronecker covariance structure is typically carried out with an iterative algorithm, specifically, a block-coordinate ascent algorithm. In this article we highlight a setting, specified by a coprime relationship between the sample size and dimension of the Kronecker factors, where the likelihood equations have algebraic degree one and an explicit, easy-to-evaluate rational formula for the maximum likelihood estimator can be found. A partial converse of this result is provided that shows that outside of the aforementioned special setting and for large sample sizes, examples of data sets can be constructed for which the degree of the likelihood equations is larger than one.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.08280v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mathias Drton, Alexandros Grosdos, Andrew McCormack</dc:creator>
    </item>
    <item>
      <title>General Inferential Limits Under Differential and Pufferfish Privacy</title>
      <link>https://arxiv.org/abs/2401.15491</link>
      <description>arXiv:2401.15491v3 Announce Type: replace 
Abstract: Differential privacy (DP) is a class of mathematical standards for assessing the privacy provided by a data-release mechanism. This work concerns two important flavors of DP that are related yet conceptually distinct: pure $\epsilon$-differential privacy ($\epsilon$-DP) and Pufferfish privacy. We restate $\epsilon$-DP and Pufferfish privacy as Lipschitz continuity conditions and provide their formulations in terms of an object from the imprecise probability literature: the interval of measures. We use these formulations to derive limits on key quantities in frequentist hypothesis testing and in Bayesian inference using data that are sanitised according to either of these two privacy standards. Under very mild conditions, the results in this work are valid for arbitrary parameters, priors and data generating models. These bounds are weaker than those attainable when analysing specific data generating models or data-release mechanisms. However, they provide generally applicable limits on the ability to learn from differentially private data - even when the analyst's knowledge of the model or mechanism is limited. They also shed light on the semantic interpretations of the two DP flavors under examination, a subject of contention in the current literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15491v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>James Bailie, Ruobin Gong</dc:creator>
    </item>
    <item>
      <title>Estimating the normal-inverse-Wishart distribution</title>
      <link>https://arxiv.org/abs/2405.16088</link>
      <description>arXiv:2405.16088v2 Announce Type: replace 
Abstract: The normal-inverse-Wishart (NIW) distribution is commonly used as a prior distribution for the mean and covariance parameters of a multivariate normal distribution. The family of NIW distributions is also a minimal exponential family. In this short note we describe a convergent procedure for converting from mean parameters to natural parameters in the NIW family, or -- equivalently -- for performing maximum likelihood estimation of the natural parameters given observed sufficient statistics. This is needed, for example, when using a NIW base family in expectation propagation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16088v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan So</dc:creator>
    </item>
    <item>
      <title>Revisiting Step-Size Assumptions in Stochastic Approximation</title>
      <link>https://arxiv.org/abs/2405.17834</link>
      <description>arXiv:2405.17834v2 Announce Type: replace 
Abstract: Many machine learning and optimization algorithms are built upon the framework of stochastic approximation (SA), for which the selection of step-size (or learning rate) is essential for success. For the sake of clarity, this paper focuses on the special case $\alpha_n = \alpha_0 n^{-\rho}$ at iteration $n$, with $\rho \in [0,1]$ and $\alpha_0&gt;0$ design parameters. It is most common in practice to take $\rho=0$ (constant step-size), while in more theoretically oriented papers a vanishing step-size is preferred. In particular, with $\rho \in (1/2, 1)$ it is known that on applying the averaging technique of Polyak and Ruppert, the mean-squared error (MSE) converges at the optimal rate of $O(1/n)$ and the covariance in the central limit theorem (CLT) is minimal in a precise sense.
  The paper revisits step-size selection in a general Markovian setting. Under readily verifiable assumptions, the following conclusions are obtained provided $0&lt;\rho&lt;1$:
  $\bullet$ Parameter estimates converge with probability one, and also in $L_p$ for any $p\ge 1$.
  $\bullet$ The MSE may converge very slowly for small $\rho$, of order $O(\alpha_n^2)$ even with averaging.
  $\bullet$ For linear stochastic approximation the source of slow convergence is identified: for any $\rho\in (0,1)$, averaging results in estimates for which the error $\textit{covariance}$ vanishes at the optimal rate, and moreover the
  CLT covariance is optimal in the sense of Polyak and Ruppert. However, necessary and sufficient conditions are obtained under which the $\textit{bias}$ converges to zero at rate $O(\alpha_n)$.
  This is the first paper to obtain such strong conclusions while allowing for $\rho \le 1/2$. A major conclusion is that the choice of $\rho =0$ or even $\rho&lt;1/2$ is justified only in select settings -- In general, bias may preclude fast convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17834v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caio Kalil Lauand, Sean Meyn</dc:creator>
    </item>
    <item>
      <title>Kernel Ridge Riesz Representers: Generalization Error and Mis-specification</title>
      <link>https://arxiv.org/abs/2102.11076</link>
      <description>arXiv:2102.11076v3 Announce Type: replace-cross 
Abstract: Kernel balancing weights provide confidence intervals for average treatment effects, based on the idea of balancing covariates for the treated group and untreated group in feature space, often with ridge regularization. Previous works on the classical kernel ridge balancing weights have certain limitations: (i) not articulating generalization error for the balancing weights, (ii) typically requiring correct specification of features, and (iii) providing inference for only average effects.
  I interpret kernel balancing weights as kernel ridge Riesz representers (KRRR) and address these limitations via a new characterization of the counterfactual effective dimension. KRRR is an exact generalization of kernel ridge regression and kernel ridge balancing weights. I prove strong properties similar to kernel ridge regression: population $L_2$ rates controlling generalization error, and a standalone closed form solution that can interpolate. The framework relaxes the stringent assumption that the underlying regression model is correctly specified by the features. It extends inference beyond average effects to heterogeneous effects, i.e. causal functions. I use KRRR to infer heterogeneous treatment effects, by age, of 401(k) eligibility on assets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2102.11076v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rahul Singh</dc:creator>
    </item>
    <item>
      <title>Least Squares Regression Can Exhibit Under-Parameterized Double Descent</title>
      <link>https://arxiv.org/abs/2305.14689</link>
      <description>arXiv:2305.14689v2 Announce Type: replace-cross 
Abstract: The relationship between the number of training data points, the number of parameters, and the generalization capabilities has been widely studied. Previous work has shown that double descent can occur in the over-parameterized regime, and believe that the standard bias-variance trade-off holds in the under-parameterized regime. These works provide multiple reasons for the existence of the peak. We postulate that the location of the peak depends on the technical properties of both the spectrum as well as the eigenvectors of the sample covariance. We present two simple examples that provably exhibit double descent in the under-parameterized regime and do not seem to occur for reasons provided in prior work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.14689v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyue Li, Rishi Sonthalia</dc:creator>
    </item>
    <item>
      <title>Multivariate Mat\'ern Models -- A Spectral Approach</title>
      <link>https://arxiv.org/abs/2309.02584</link>
      <description>arXiv:2309.02584v2 Announce Type: replace-cross 
Abstract: The classical Mat\'ern model has been a staple in spatial statistics. Novel data-rich applications in environmental and physical sciences, however, call for new, flexible vector-valued spatial and space-time models. Therefore, the extension of the classical Mat\'ern model has been a problem of active theoretical and methodological interest. In this paper, we offer a new perspective to extending the Mat\'ern covariance model to the vector-valued setting. We adopt a spectral, stochastic integral approach, which allows us to address challenging issues on the validity of the covariance structure and at the same time to obtain new, flexible, and interpretable models. In particular, our multivariate extensions of the Mat\'ern model allow for asymmetric covariance structures. Moreover, the spectral approach provides an essentially complete flexibility in modeling the local structure of the process. We establish closed-form representations of the cross-covariances when available, compare them with existing models, simulate Gaussian instances of these new processes, and demonstrate estimation of the model's parameters through maximum likelihood. An application of the new class of multivariate Mat\'ern models to environmental data indicate their success in capturing inherent covariance-asymmetry phenomena.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.02584v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Drew Yarger, Stilian Stoev, Tailen Hsing</dc:creator>
    </item>
    <item>
      <title>Weak approximation of Schr\"odinger-F\"ollmer diffusion</title>
      <link>https://arxiv.org/abs/2403.03446</link>
      <description>arXiv:2403.03446v2 Announce Type: replace-cross 
Abstract: We consider the weak convergence of the Euler-Maruyama approximation for Schr\"odinger-F\"ollmer diffusions, which are solutions of Schr\"odinger bridge problems and can be used for sampling from given distributions. We show that the distribution of the terminal random variable of the time-discretized process weakly converges to the target one under mild regularity conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03446v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Koya Endo, Yumiharu Nakano</dc:creator>
    </item>
    <item>
      <title>Optimal single threshold stopping rules and sharp prophet inequalities</title>
      <link>https://arxiv.org/abs/2404.12949</link>
      <description>arXiv:2404.12949v2 Announce Type: replace-cross 
Abstract: This paper considers a finite horizon optimal stopping problem for a sequence of independent and identically distributed random variables. The objective is to design stopping rules that attempt to select the random variable with the highest value in the sequence. The performance of any stopping rule may be benchmarked relative to the selection of a ``prophet" that has perfect foreknowledge of the largest value. Such comparisons are typically stated in the form of "prophet inequalities." In this paper we characterize sharp prophet inequalities for single threshold stopping rules as solutions to infinite two person zero sum games on the unit square with special payoff kernels. The proposed game theoretic characterization allows one to derive sharp non-asymptotic prophet inequalities for different classes of distributions. This, in turn, gives rise to a simple and computationally tractable algorithmic paradigm for deriving optimal single threshold stopping rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12949v2</guid>
      <category>math.PR</category>
      <category>cs.GT</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Goldenshluger, Yaakov Malinovsky, Assaf Zeevi</dc:creator>
    </item>
    <item>
      <title>Comparison of Point Process Learning and its special case Takacs-Fiksel estimation</title>
      <link>https://arxiv.org/abs/2405.19523</link>
      <description>arXiv:2405.19523v3 Announce Type: replace-cross 
Abstract: Recently, Cronie et al. (2024) introduced the notion of cross-validation for point processes and a new statistical methodology called Point Process Learning (PPL). In PPL one splits a point process/pattern into a training and a validation set, and then predicts the latter from the former through a parametrised Papangelou conditional intensity. The model parameters are estimated by minimizing a point process prediction error; this notion was introduced as the second building block of PPL. It was shown that PPL outperforms the state-of-the-art in both kernel intensity estimation and estimation of the parameters of the Gibbs hard-core process. In the latter case, the state-of-the-art was represented by pseudolikelihood estimation. In this paper we study PPL in relation to Takacs-Fiksel estimation, of which pseudolikelihood is a special case. We show that Takacs-Fiksel estimation is a special case of PPL in the sense that PPL with a specific loss function asymptotically reduces to Takacs-Fiksel estimation if we let the cross-validation regime tend to leave-one-out cross-validation. Moreover, PPL involves a certain type of hyperparameter given by a weight function which ensures that the prediction errors have expectation zero if and only if we have the correct parametrisation. We show that the weight function takes an explicit but intractable form for general Gibbs models. Consequently, we propose different approaches to estimate the weight function in practice. In order to assess how the general PPL setup performs in relation to its special case Takacs-Fiksel estimation, we conduct a simulation study where we find that for common Gibbs models we can find loss functions and hyperparameters so that PPL typically outperforms Takacs-Fiksel estimation significantly in terms of mean square error. Here, the hyperparameters are the cross-validation parameters and the weight function estimate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19523v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julia Jansson, Ottmar Cronie</dc:creator>
    </item>
  </channel>
</rss>
