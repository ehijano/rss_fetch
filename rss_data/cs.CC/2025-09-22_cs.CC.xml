<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Sep 2025 04:01:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>AC^0[p]-Frege Cannot Efficiently Prove that Constant-Depth Algebraic Circuit Lower Bounds are Hard</title>
      <link>https://arxiv.org/abs/2509.16824</link>
      <description>arXiv:2509.16824v1 Announce Type: new 
Abstract: We study whether lower bounds against constant-depth algebraic circuits computing the Permanent over finite fields (Limaye-Srinivasan-Tavenas, J. ACM 2025; Forbes, CCC 2024) are hard to prove in certain proof systems. We focus on a DNF formula that expresses that such lower bounds are hard for constant-depth algebraic proofs. Using an adaptation of the diagonalization framework of Santhanam and Tzameret (SIAM J. Comput. 2025), we show unconditionally that this family of DNF formulas does not admit polynomial-size propositional AC0[p]-Frege proofs infinitely often. This rules out the possibility that the DNF family is easy, and establishes that its status is either that of a hard tautology for AC0[p]-Frege or else unprovable (not a tautology). While it remains open whether the DNFs in question are tautologies, we provide evidence in this direction. In particular, under the plausible assumption that certain weak properties of multilinear algebra, specifically those involving tensor rank, do not admit short constant-depth algebraic proofs, the DNFs are tautologies. We also observe that several weaker variants of the DNF formula are provably tautologies, and we show that the question of whether the DNFs are tautologies connects to conjectures of Razborov (ICALP 1996) and Krajicek (J. Symb. Log. 2004). Our result has two additional features. (i) Existential depth amplification: the DNF formula is parameterised by a constant depth d bounding the depth of the algebraic proofs. We show that there exists some fixed depth d such that if there are no small depth-d algebraic proofs of certain circuit lower bounds for the Permanent, then there are no such small algebraic proofs in any constant depth. (ii) Necessity: we show that our result is a necessary step towards establishing lower bounds against constant-depth algebraic proofs, and more generally against any sufficiently strong proof system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16824v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaqi Lu, Rahul Santhanam, Iddo Tzameret</dc:creator>
    </item>
    <item>
      <title>Sketching approximations and LP approximations for finite CSPs are related</title>
      <link>https://arxiv.org/abs/2509.17926</link>
      <description>arXiv:2509.17926v1 Announce Type: new 
Abstract: We identify a connection between the approximability of CSPs in two models: (i) sublinear space streaming algorithms, and (ii) the basic LP relaxation. We show that whenever the basic LP admits an integrality gap, there is an $\Omega(\sqrt{n})$-space sketching lower bound. We also show that all existing linear space streaming lower bounds for Max-CSPs can be lifted to integrality gap instances for basic LPs. For bounded-degree graphs, by combining the distributed algorithm of Yoshida (STOC 2011) for approximately solving the basic LP with techniques described in Saxena, Singer, Sudan, and Velusamy (SODA 2025) for simulating a distributed algorithm by a sublinear space streaming algorithm on bounded-degree instances of Max-DICUT, it appears that there are sublinear space streaming algorithms implementing the basic LP, for every CSP.
  Based on our results, we conjecture the following dichotomy theorem: Whenever the basic LP admits an integrality gap, there is a linear space single-pass streaming lower bound, and when the LP is roundable, there is a sublinear space streaming algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17926v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noah G. Singer, Madhur Tulsiani, Santhoshini Velusamy</dc:creator>
    </item>
    <item>
      <title>Supersimulators</title>
      <link>https://arxiv.org/abs/2509.17994</link>
      <description>arXiv:2509.17994v1 Announce Type: new 
Abstract: We prove that every randomized Boolean function admits a supersimulator: a randomized polynomial-size circuit whose output on random inputs cannot be efficiently distinguished from reality with constant advantage, even by polynomially larger distinguishers. Our result builds on the landmark complexity-theoretic regularity lemma of Trevisan, Tulsiani and Vadhan (2009), which, in contrast, provides a simulator that fools smaller distinguishers. We circumvent lower bounds for the simulator size by letting the distinguisher size bound vary with the target function, while remaining below an absolute upper bound independent of the target function. This dependence on the target function arises naturally from our use of an iteration technique originating in the graph regularity literature.
  The simulators provided by the regularity lemma and recent refinements thereof, known as multiaccurate and multicalibrated predictors, respectively, as per Hebert-Johnson et al. (2018), have previously been shown to have myriad applications in complexity theory, cryptography, learning theory, and beyond. We first show that a recent multicalibration-based characterization of the computational indistinguishability of product distributions actually requires only (calibrated) multiaccuracy. We then show that supersimulators yield an even tighter result in this application domain, closing a complexity gap present in prior versions of the characterization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17994v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cynthia Dwork, Pranay Tankala</dc:creator>
    </item>
    <item>
      <title>Vanishing of Schubert coefficients in probabilistic polynomial time</title>
      <link>https://arxiv.org/abs/2509.16467</link>
      <description>arXiv:2509.16467v1 Announce Type: cross 
Abstract: The Schubert vanishing problem asks whether Schubert structure constants are zero. We give a complete solution of the problem from an algorithmic point of view, by showing that Schubert vanishing can be decided in probabilistic polynomial time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16467v1</guid>
      <category>math.CO</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>math.AG</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Igor Pak, Colleen Robichaux</dc:creator>
    </item>
    <item>
      <title>The Complexity of Finding Local Optima in Contrastive Learning</title>
      <link>https://arxiv.org/abs/2509.16898</link>
      <description>arXiv:2509.16898v1 Announce Type: cross 
Abstract: Contrastive learning is a powerful technique for discovering meaningful data representations by optimizing objectives based on $\textit{contrastive information}$, often given as a set of weighted triplets $\{(x_i, y_i^+, z_{i}^-)\}_{i = 1}^m$ indicating that an "anchor" $x_i$ is more similar to a "positive" example $y_i$ than to a "negative" example $z_i$. The goal is to find representations (e.g., embeddings in $\mathbb{R}^d$ or a tree metric) where anchors are placed closer to positive than to negative examples. While finding $\textit{global}$ optima of contrastive objectives is $\mathsf{NP}$-hard, the complexity of finding $\textit{local}$ optima -- representations that do not improve by local search algorithms such as gradient-based methods -- remains open. Our work settles the complexity of finding local optima in various contrastive learning problems by proving $\mathsf{PLS}$-hardness in discrete settings (e.g., maximize satisfied triplets) and $\mathsf{CLS}$-hardness in continuous settings (e.g., minimize Triplet Loss), where $\mathsf{PLS}$ (Polynomial Local Search) and $\mathsf{CLS}$ (Continuous Local Search) are well-studied complexity classes capturing local search dynamics in discrete and continuous optimization, respectively. Our results imply that no polynomial time algorithm (local search or otherwise) can find a local optimum for various contrastive learning problems, unless $\mathsf{PLS}\subseteq\mathsf{P}$ (or $\mathsf{CLS}\subseteq \mathsf{P}$ for continuous problems). Even in the unlikely scenario that $\mathsf{PLS}\subseteq\mathsf{P}$ (or $\mathsf{CLS}\subseteq \mathsf{P}$), our reductions imply that there exist instances where local search algorithms need exponential time to reach a local optimum, even for $d=1$ (embeddings on a line).</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16898v1</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingming Yan, Yiyuan Luo, Vaggos Chatziafratis, Ioannis Panageas, Parnian Shahkar, Stelios Stavroulakis</dc:creator>
    </item>
    <item>
      <title>Reinforced Generation of Combinatorial Structures: Applications to Complexity Theory</title>
      <link>https://arxiv.org/abs/2509.18057</link>
      <description>arXiv:2509.18057v1 Announce Type: cross 
Abstract: We explore whether techniques from AI can help discover new combinatorial structures that improve provable limits on efficient algorithms. Specifically, we use AlphaEvolve (an LLM coding agent) to study two settings:
  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a recent result of Kunisky and Yu to obtain near-optimal upper and (conditional) lower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on random 3- and 4-regular graphs. Our improved lower bounds are obtained by constructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using AlphaEvolve. Additionally, via analytical arguments we strengthen the upper bounds to settle the computational hardness of these questions up to an error in the third decimal place.
  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new inapproximability results, proving that it is NP-hard to approximate MAX-4-CUT and MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using AlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves upon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current best gadget-based inapproximability result of $0.9853$, but falls short of improving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget reduction from "standard" H{\aa}stad-style PCPs.
  A key technical challenge we faced: verifying a candidate construction produced by AlphaEvolve is costly (often requiring exponential time). In both settings above, our results were enabled by using AlphaEvolve itself to evolve the verification procedure to be faster (sometimes by $10,000\times$). We conclude with a discussion of norms by which to assess the assistance from AI in developing proofs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18057v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>math.CO</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ansh Nagda, Prabhakar Raghavan, Abhradeep Thakurta</dc:creator>
    </item>
    <item>
      <title>Intersection and Union Hierarchies of Deterministic Context-Free Languages and Pumping Lemmas</title>
      <link>https://arxiv.org/abs/2112.09383</link>
      <description>arXiv:2112.09383v2 Announce Type: replace-cross 
Abstract: We study the computational complexity of finite intersections and finite unions of deterministic context-free (dcf) languages. Earlier, Wotschke [J. Comput. System Sci. 16 (1978) 456--461] demonstrated that intersections of $(d+1)$ dcf languages are in general more powerful than intersections of $d$ dcf languages for any positive integer $d$ based on the separation result of the intersection hierarchy of Liu and Weiner [Math. Systems Theory 7 (1973) 185--192]. The argument of Liu and Weiner, however, works only on bounded languages of particular forms, and therefore Wotschke's result is not directly extendable to other non-bounded languages. To deal with a wide range of languages for the non-membership to the intersection hierarchy, we circumvent the specialization of their proof technics and devise a new and practical technical tool: two pumping lemmas for finite unions of dcf languages. Since the family of dcf languages is closed under complementation and also under intersection with regular languages, these pumping lemmas help us establish the non-membership relation of languages formed by finite intersections of target languages. We also concern ourselves with a relationship to deterministic limited automata of Hibbard [Inf. Control 11 (1967) 196--238] in this regard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.09383v2</guid>
      <category>cs.FL</category>
      <category>cs.CC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomoyuki Yamakami</dc:creator>
    </item>
    <item>
      <title>Computing the $D$-base and $D$-relation in finite closure systems</title>
      <link>https://arxiv.org/abs/2404.07037</link>
      <description>arXiv:2404.07037v3 Announce Type: replace-cross 
Abstract: Implicational bases (IBs) are a common representation of finite closure systems and lattices, along with meet-irreducible elements. They appear in a wide variety of fields ranging from logic and databases to Knowledge Space Theory. Different IBs can represent the same closure system. Therefore, several IBs have been studied, such as the canonical and canonical direct bases. In this paper, we investigate the $D$-base, a refinement of the canonical direct base. It is connected with the $D$-relation, an essential tool in the study of free lattices. The $D$-base demonstrates desirable algorithmic properties, and together with the $D$-relation, it conveys essential properties of the underlying closure system. Hence, computing the $D$-base and the $D$-relation of a closure system from another representation is crucial to enjoy its benefits. However, complexity results for this task are lacking. In this paper, we give algorithms and hardness results for the computation of the $D$-base and $D$-relation. Specifically, we establish the $NP$-completeness of finding the $D$-relation from an arbitrary IB; we give an output-quasi-polynomial time algorithm to compute the $D$-base from meet-irreducible elements; and we obtain a polynomial-delay algorithm computing the $D$-base from an arbitrary IB. These results complete the picture regarding the complexity of identifying the $D$-base and $D$-relation of a closure system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07037v3</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>math.CO</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kira Adaricheva, Lhouari Nourine, Simon Vilmin</dc:creator>
    </item>
    <item>
      <title>Neural Networks and (Virtual) Extended Formulations</title>
      <link>https://arxiv.org/abs/2411.03006</link>
      <description>arXiv:2411.03006v3 Announce Type: replace-cross 
Abstract: Neural networks with piecewise linear activation functions, such as rectified linear units (ReLU) or maxout, are among the most fundamental models in modern machine learning. We make a step towards proving lower bounds on the size of such neural networks by linking their representative capabilities to the notion of the extension complexity $\mathrm{xc}(P)$ of a polytope $P$. This is a well-studied quantity in combinatorial optimization and polyhedral geometry describing the number of inequalities needed to model $P$ as a linear program. We show that $\mathrm{xc}(P)$ is a lower bound on the size of any monotone or input-convex neural network that solves the linear optimization problem over $P$. This implies exponential lower bounds on such neural networks for a variety of problems, including the polynomially solvable maximum weight matching problem.
  In an attempt to prove similar bounds also for general neural networks, we introduce the notion of virtual extension complexity $\mathrm{vxc}(P)$, which generalizes $\mathrm{xc}(P)$ and describes the number of inequalities needed to represent the linear optimization problem over $P$ as a difference of two linear programs. We prove that $\mathrm{vxc}(P)$ is a lower bound on the size of any neural network that optimizes over $P$. While it remains an open question to derive useful lower bounds on $\mathrm{vxc}(P)$, we argue that this quantity deserves to be studied independently from neural networks by proving that one can efficiently optimize over a polytope $P$ using a small virtual extended formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03006v3</guid>
      <category>math.CO</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Hertrich, Georg Loho</dc:creator>
    </item>
    <item>
      <title>Benford's Law from Turing Ensembles and Integer Partitions</title>
      <link>https://arxiv.org/abs/2502.16314</link>
      <description>arXiv:2502.16314v5 Announce Type: replace-cross 
Abstract: We develop two complementary generative mechanisms that explain when and why Benford's first-digit law arises. First, a probabilistic Turing machine (PTM) ensemble induces a geometric law for code length. Maximizing its entropy under a constraint on halting length yields Benford statistics. This model shows a phase transition with respect to the halt probability. Second, a constrained partition model (Einstein-solid combinatorics) recovers the same logarithmic profile as the maximum-entropy solution under a coarse-grained entropy-rate constraint, clarifying the role of non-ergodicity (ensemble vs. trajectory averages). We also perform numerical experiments that corroborate our conclusions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16314v5</guid>
      <category>cs.IT</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>math-ph</category>
      <category>math.IT</category>
      <category>math.MP</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Kolpakov, Aidan Rocke</dc:creator>
    </item>
    <item>
      <title>The Complexity of Pure Strategy Relevant Equilibria in Concurrent Games</title>
      <link>https://arxiv.org/abs/2505.07501</link>
      <description>arXiv:2505.07501v4 Announce Type: replace-cross 
Abstract: We study rational synthesis problems for concurrent games with omega-regular objectives. Our model of rationality considers only pure strategy Nash equilibria that satisfy either a social welfare or Pareto optimality condition with respect to an omega-regular objective for each agent. This extends earlier work on equilibria in concurrent games, without consideration about their quality. Our results show that the existence of Nash equilibria satisfying social welfare conditions can be computed as efficiently as the constrained Nash equilibrium existence problem. On the other hand, the existence of Nash equilibria satisfying the Pareto optimality condition possibly involves a higher upper bound, except in the case of  Buchi and Muller games, for which all three problems are in the classes P and PSPACE-complete, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07501v4</guid>
      <category>cs.GT</category>
      <category>cs.CC</category>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <category>cs.MA</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.428.6</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 428, 2025, pp. 62-75</arxiv:journal_reference>
      <dc:creator>Purandar Bhaduri (IIT Guwahati)</dc:creator>
    </item>
    <item>
      <title>Tighter Bounds for Personalized PageRank</title>
      <link>https://arxiv.org/abs/2507.14462</link>
      <description>arXiv:2507.14462v2 Announce Type: replace-cross 
Abstract: We study Personalized PageRank (PPR), where for nodes $s,t$ in a graph $G$, $\pi(s,t)$ is the probability that an $\alpha$-decay random walk from $s$ ends at $t$. Two key queries are: Single-Source PPR (SSPPR), computing $\pi(s,\cdot)$ for fixed $s$, and Single-Target PPR (STPPR), computing $\pi(\cdot,t)$ for fixed $t$. SSPPR is studied under absolute error (SSPPR-A), requiring $|\hat{\pi}(s,t)-\pi(s,t)|\le \epsilon$, and relative error (SSPPR-R), requiring $|\hat{\pi}(s,t)-\pi(s,t)|\le c\pi(s,t)$ for $t$ with $\pi(s,t)\ge \delta$; STPPR adopts the same relative criterion. These queries support web search, recommendation, sparsification, and graph neural networks.
  The best known upper bounds are $O(\min(\tfrac{\log(1/\epsilon)}{\epsilon^{2}},\tfrac{\sqrt{m\log n}}{\epsilon},m\log\tfrac{1}{\epsilon}))$ for SSPPR-A and $O(\min(\tfrac{\log(1/\delta)}{\delta},\sqrt{\tfrac{m\log n}{\delta}},m\log\tfrac{\log n}{\delta m}))$ for SSPPR-R, while lower bounds remain $\Omega(\min(n,1/\epsilon))$, $\Omega(\min(m,1/\delta))$, and $\Omega(\min(n,1/\delta))$, leaving large gaps. We close these gaps by (i) presenting a Monte Carlo algorithm that tightens the SSPPR-A upper bound to $O(1/\epsilon^{2})$, and (ii) proving, via an arc-centric construction, lower bounds $\Omega(\min(m,\tfrac{\log(1/\delta)}{\delta}))$ for SSPPR-R, $\Omega(\min(m,\tfrac{1}{\epsilon^{2}}))$ (and intermediate $\Omega(\min(m,\tfrac{\log(1/\epsilon)}{\epsilon}))$) for SSPPR-A, and $\Omega(\min(m,\tfrac{n}{\delta}\log n))$ for STPPR. For practical settings ($\delta=\Theta(1/n)$, $\epsilon=\Theta(n^{-1/2})$, $m\in\Omega(n\log n)$) these bounds meet the best known upper bounds, establishing the optimality of Monte Carlo and FORA for SSPPR-R, our algorithm for SSPPR-A, and RBS for STPPR, and yielding a near-complete complexity landscape for PPR queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14462v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinpeng Jiang, Haoyu Liu, Siqiang Luo, Xiaokui Xiao</dc:creator>
    </item>
    <item>
      <title>Computational aspects of the trace norm contraction coefficient</title>
      <link>https://arxiv.org/abs/2507.16737</link>
      <description>arXiv:2507.16737v2 Announce Type: replace-cross 
Abstract: We show that approximating the trace norm contraction coefficient of a quantum channel within a constant factor is NP-hard. Equivalently, this shows that determining the optimal success probability for encoding a bit in a quantum system undergoing noise is NP-hard. This contrasts with the classical analogue of this problem that can clearly be solved efficiently. We also establish the NP-hardness of deciding if the contraction coefficient is equal to 1, i.e., the channel can perfectly preserve a bit. As a consequence, deciding if a non-commutative graph has an independence number of at least 2 is NP-hard. In addition, we establish a converging hierarchy of semidefinite programming upper bounds on the contraction coefficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16737v2</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Idris Delsol, Omar Fawzi, Jan Kochanowski, Akshay Ramachandran</dc:creator>
    </item>
    <item>
      <title>Information-Theoretic Bounds and Task-Centric Learning Complexity for Real-World Dynamic Nonlinear Systems</title>
      <link>https://arxiv.org/abs/2509.06599</link>
      <description>arXiv:2509.06599v2 Announce Type: replace-cross 
Abstract: Dynamic nonlinear systems exhibit distortions arising from coupled static and dynamic effects. Their intertwined nature poses major challenges for data-driven modeling. This paper presents a theoretical framework grounded in structured decomposition, variance analysis, and task-centric complexity bounds.
  The framework employs a directional lower bound on interactions between measurable system components, extending orthogonality in inner product spaces to structurally asymmetric settings. This bound supports variance inequalities for decomposed systems. Key behavioral indicators are introduced along with a memory finiteness index. A rigorous power-based condition establishes a measurable link between finite memory in realizable systems and the First Law of Thermodynamics. This offers a more foundational perspective than classical bounds based on the Second Law.
  Building on this foundation, we formulate a `Behavioral Uncertainty Principle,' demonstrating that static and dynamic distortions cannot be minimized simultaneously. We identify that real-world systems seem to resist complete deterministic decomposition due to entangled static and dynamic effects. We also present two general-purpose theorems linking function variance to mean-squared Lipschitz continuity and learning complexity. This yields a model-agnostic, task-aware complexity metric, showing that lower-variance components are inherently easier to learn.
  These insights explain the empirical benefits of structured residual learning, including improved generalization, reduced parameter count, and lower training cost, as previously observed in power amplifier linearization experiments. The framework is broadly applicable and offers a scalable, theoretically grounded approach to modeling complex dynamic nonlinear systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06599v2</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sri Satish Krishna Chaitanya Bulusu, Mikko Sillanp\"a\"a</dc:creator>
    </item>
  </channel>
</rss>
