<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Jun 2024 01:35:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 26 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Unraveling the Dynamics of SPY Trading Volumes: A Comprehensive Analysis of Daily and Intraday Liquidity Trends</title>
      <link>https://arxiv.org/abs/2406.17198</link>
      <description>arXiv:2406.17198v1 Announce Type: new 
Abstract: In this project, we investigate the accuracy of forecasting intraday and daily trading volume of the exchange-traded fund SPY. The ability to forecast volume over varying time intervals with high accuracy is a critical element to many trading strategies. After performing exploratory data analysis on intraday and daily SPY data we identify three methods for our analysis: ARIMA and ARIMAX models, with or without seasonality, as well as a Frequency Domain Process Representation. To evaluate predictive power of our models, we use mean squared error, mean absolute percentage error, and volume weighted average price (VWAP) tracking error. All models for both intraday and daily data output strong VWAP predictions in comparison to the VWAP estimates produced by naive baseline methodologies. In both cases volume is most accurately forecasted using ARIMA models with exogenous variables in the form of technical indicators, with intraday incorporating a seasonal component and daily not.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17198v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ananya Krishnan, Martin Pollack, Alma Cooper</dc:creator>
    </item>
    <item>
      <title>NFL Ghosts: A framework for evaluating defender positioning with conditional density estimation</title>
      <link>https://arxiv.org/abs/2406.17220</link>
      <description>arXiv:2406.17220v1 Announce Type: new 
Abstract: Player attribution in American football remains an open problem due to the complex nature of twenty-two players interacting on the field, but the granularity of player tracking data provides ample opportunity for novel approaches. In this work, we introduce the first public framework to evaluate spatial and trajectory tracking data of players relative to a baseline distribution of "ghost" defenders. We demonstrate our framework in the context of modeling the nearest defender positioning at the moment of catch. In particular, we provide estimates of how much better or worse their observed positioning and trajectory compared to the expected play value of ghost defenders. Our framework leverages high-dimensional tracking data features through flexible random forests for conditional density estimation in two ways: (1) to model the distribution of receiver yards gained enabling the estimation of within-play expected value, and (2) to model the 2D spatial distribution of baseline ghost defenders. We present novel metrics for measuring player and team performance based on tracking data, and discuss challenges that remain in extending our framework to other aspects of American football.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17220v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ronald Yurko, Quang Nguyen, Konstantinos Pelechrinis</dc:creator>
    </item>
    <item>
      <title>A hybrid predictive and prescriptive modelling framework for long-term mental healthcare workforce planning</title>
      <link>https://arxiv.org/abs/2406.17463</link>
      <description>arXiv:2406.17463v1 Announce Type: new 
Abstract: Over the past decade, there has been a severe staffing shortage in mental healthcare, exacerbated by increased demand for mental health services due to COVID-19. This demand is projected to increase over the next decade or so, necessitating proactive workforce planning to ensure sufficient staffing for ongoing service delivery. Despite the subject's critical significance, the present literature lacks thorough research dedicated to developing a model that addresses the long-term workforce needs required for efficient mental healthcare planning. Furthermore, our interactions with mental health practitioners within the United Kingdom's National Health Service (NHS) revealed the practical need for such a model. To address this gap, we aim to develop a hybrid predictive and prescriptive modelling framework, which combines long-term probabilistic forecasting with an analytical stock-flow model, designed specifically for mental health workforce planning. Given the vital role of nurses, who account for one-third of the total mental health workforce, we focus on modelling the headcount of nurses, but the proposed model can be generalised to other types of workforce planning in the healthcare sector. Using statistical and machine learning approaches and real-world data from NHS, we first identify factors contributing to variations in workforce requirements, then develop a long-term forecasting model to estimate future workforce needs, and finally integrate it into an analytical stock-flow method to provide policy analysis. Our findings highlight the unsustainable nature of present staffing plans, showing a growing nursing shortage. Furthermore, the policy analysis demonstrates the ineffectiveness of blanket remedies, highlighting the need for regional-level policy developments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17463v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harsha Chamara Hewage, Bahman Rostami-Tabar</dc:creator>
    </item>
    <item>
      <title>Using iterated local alignment to aggregate GPS trajectories into a traffic flow map</title>
      <link>https://arxiv.org/abs/2406.17500</link>
      <description>arXiv:2406.17500v1 Announce Type: new 
Abstract: Desire line maps are widely deployed for traffic flow analysis by virtue of their ease of interpretation and computation. They can be considered to be simplified traffic flow maps, whereas the computational challenges in aggregating small scale traffic flows prevent the wider dissemination of high resolution flow maps. GPS trajectories are a promising data source to solve this challenging problem. The solution begins with the alignment (or map matching) of the GPS trajectories to the road network. However even the state-of-the-art map matching APIs produce sub-optimal results with small misalignments. While these misalignments are negligible for large scale flow aggregation in desire line maps, they pose substantial obstacles for small scale flow aggregation in high resolution maps. To remove these remaining misalignments, we introduce innovative local alignment algorithms, where we infer road segments to serve as local reference segments, and proceed to align nearby road segments to them. With each local alignment iteration, the misalignments of the GPS trajectories with each other and with the road network are reduced, and so converge closer to a minimal flow map. By analysing a set of empirical GPS trajectories collected in Hannover, Germany, we confirm that our minimal flow map has high levels of spatial resolution, accuracy and coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17500v1</guid>
      <category>stat.AP</category>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tarn Duong</dc:creator>
    </item>
    <item>
      <title>Rapid Shear Capacity Prediction of TRM-Strengthened Unreinforced Masonry Walls through Interpretable Machine Learning using a Web App</title>
      <link>https://arxiv.org/abs/2406.16889</link>
      <description>arXiv:2406.16889v1 Announce Type: cross 
Abstract: The presented study aims to provide an efficient and reliable tool for rapid estimation of the shear capacity of a TRM-strengthened masonry wall. For this purpose, a data-driven methodology based on a machine learning system is proposed using a dataset constituted of experimental results selected from the bibliography. The outlier points were detected using Cook's distance methodology and removed from the raw dataset, which consisted of 113 examples and 11 input variables. In the processed dataset, 17 Machine Learning methods were trained, optimized through hyperparameter tuning, and compared on the test set. The most effective models are combined into a voting model to leverage the predictive capacity of more than a single regressor. The final blended model shows remarkable predicting capacity with the determination factor ($R^2$) equal to 0.95 and the mean absolute percentage error equal to 8.03\%. In sequence, machine learning interpretation methods are applied to find how the predictors influence the target output. $A_m$, $f_t$, and $n\cdot t_f$ were identified as the most significant predictors with a mainly positive influence on the shear capacity. Finally, the built ML system is employed in a user-friendly web app for easy access and usage by professionals and researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16889v1</guid>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Petros Lazaridis, Athanasia Thomoglou</dc:creator>
    </item>
    <item>
      <title>Benchmarking mortality risk prediction from electrocardiograms</title>
      <link>https://arxiv.org/abs/2406.17002</link>
      <description>arXiv:2406.17002v2 Announce Type: cross 
Abstract: Several recent high-impact studies leverage large hospital-owned electrocardiographic (ECG) databases to model and predict patient mortality. MIMIC-IV, released September 2023, is the first comparable public dataset and includes 800,000 ECGs from a U.S. hospital system. Previously, the largest public ECG dataset was Code-15, containing 345,000 ECGs collected during routine care in Brazil. These datasets now provide an excellent resource for a broader audience to explore ECG survival modeling. Here, we benchmark survival model performance on Code-15 and MIMIC-IV with two neural network architectures, compare four deep survival modeling approaches to Cox regressions trained on classifier outputs, and evaluate performance at one to ten years. Our results yield AUROC and concordance scores comparable to past work (circa 0.8) and reasonable AUPRC scores (MIMIC-IV: 0.4-0.5, Code-15: 0.05-0.13) considering the fraction of ECG samples linked to a mortality (MIMIC-IV: 27\%, Code-15: 4\%). When evaluating models on the opposite dataset, AUROC and concordance values drop by 0.1-0.15, which may be due to cohort differences. All code and results are made public.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17002v2</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Platon Lukyanenko, Joshua Mayourian, Mingxuan Liu, John K. Triedman, Sunil J. Ghelani, William G. La Cava</dc:creator>
    </item>
    <item>
      <title>Bayesian temporal biclustering with applications to multi-subject neuroscience studies</title>
      <link>https://arxiv.org/abs/2406.17131</link>
      <description>arXiv:2406.17131v1 Announce Type: cross 
Abstract: We consider the problem of analyzing multivariate time series collected on multiple subjects, with the goal of identifying groups of subjects exhibiting similar trends in their recorded measurements over time as well as time-varying groups of associated measurements. To this end, we propose a Bayesian model for temporal biclustering featuring nested partitions, where a time-invariant partition of subjects induces a time-varying partition of measurements. Our approach allows for data-driven determination of the number of subject and measurement clusters as well as estimation of the number and location of changepoints in measurement partitions. To efficiently perform model fitting and posterior estimation with Markov Chain Monte Carlo, we derive a blocked update of measurements' cluster-assignment sequences. We illustrate the performance of our model in two applications to functional magnetic resonance imaging data and to an electroencephalogram dataset. The results indicate that the proposed model can combine information from potentially many subjects to discover a set of interpretable, dynamic patterns. Experiments on simulated data compare the estimation performance of the proposed model against ground-truth values and other statistical methods, showing that it performs well at identifying ground-truth subject and measurement clusters even when no subject or time dependence is present.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17131v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Federica Zoe Ricci, Erik B. Sudderth, Jaylen Lee, Megan A. K. Peters, Marina Vannucci, Michele Guindani</dc:creator>
    </item>
    <item>
      <title>An information-geometric approach for network decomposition using the q-state Potts model</title>
      <link>https://arxiv.org/abs/2406.17144</link>
      <description>arXiv:2406.17144v1 Announce Type: cross 
Abstract: Complex networks are critical in many scientific, technological, and societal contexts due to their ability to represent and analyze intricate systems with interdependent components. Often, after labeling the nodes of a network with a community detection algorithm, its modular organization emerges, allowing a better understanding of the underlying structure by uncovering hidden relationships. In this paper, we introduce a novel information-geometric framework for the filtering and decomposition of networks whose nodes have been labeled. Our approach considers the labeled network as the outcome of a Markov random field modeled by a q-state Potts model. According to information geometry, the first and second order Fisher information matrices are related to the metric and curvature tensor of the parametric space of a statistical model. By computing an approximation to the local shape operator, the proposed methodology is able to identify low and high information nodes, allowing the decomposition of the labeled network in two complementary subgraphs. Hence, we call this method as the LO-HI decomposition. Experimental results with several kinds of networks show that the high information subgraph is often related to edges and boundaries, while the low information subgraph is a smoother version of the network, in the sense that the modular structure is improved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17144v1</guid>
      <category>cs.IT</category>
      <category>cs.SI</category>
      <category>math-ph</category>
      <category>math.IT</category>
      <category>math.MP</category>
      <category>nlin.AO</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandre L. M. Levada</dc:creator>
    </item>
    <item>
      <title>Causal Responder Detection</title>
      <link>https://arxiv.org/abs/2406.17571</link>
      <description>arXiv:2406.17571v1 Announce Type: cross 
Abstract: We introduce the causal responders detection (CARD), a novel method for responder analysis that identifies treated subjects who significantly respond to a treatment. Leveraging recent advances in conformal prediction, CARD employs machine learning techniques to accurately identify responders while controlling the false discovery rate in finite sample sizes. Additionally, we incorporate a propensity score adjustment to mitigate bias arising from non-random treatment allocation, enhancing the robustness of our method in observational settings. Simulation studies demonstrate that CARD effectively detects responders with high power in diverse scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17571v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tzviel Frostig, Oshri Machluf, Amitay Kamber, Elad Berkman, Raviv Pryluk</dc:creator>
    </item>
    <item>
      <title>Using spatial extreme-value theory with machine learning to model and understand spatially compounding weather extremes</title>
      <link>https://arxiv.org/abs/2401.12195</link>
      <description>arXiv:2401.12195v2 Announce Type: replace 
Abstract: When extreme weather events affect large areas, their regional to sub-continental spatial scale is important for their impacts. We propose a novel machine learning (ML) framework that integrates spatial extreme-value theory to model weather extremes and to quantify probabilities associated with the occurrence, intensity, and spatial extent of these events. Our approach employs new loss functions adapted to extreme values, enabling our model to prioritize the tail instead of the bulk of the data distribution. Applied to a case study of Western European summertime heat extremes, we use daily 500-hPa geopotential height fields and local soil moisture as predictors to capture the complex interplay between local and remote physical processes. Our generative model reveals the importance of individual circulation features in determining different facets of heat extremes, thereby enriching our process understanding from a data-driven perspective. The occurrence, intensity, and spatial extent of heat extremes are sensitive to the relative position of upper-level ridges and troughs that are part of a large-scale wave pattern. Our approach is able to extrapolate beyond the range of the data to make risk-related probabilistic statements, applies more generally to other weather extremes, and offers an attractive alternative to traditional physical and ML-based techniques that focus less on the extremal aspects of weather data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12195v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Koh, Daniel Steinfeld, Olivia Martius</dc:creator>
    </item>
    <item>
      <title>Model-assisted analysis of covariance estimators for stepped wedge cluster randomized experiments</title>
      <link>https://arxiv.org/abs/2306.11267</link>
      <description>arXiv:2306.11267v3 Announce Type: replace-cross 
Abstract: Stepped wedge cluster randomized experiments (SW-CREs) represent a class of unidirectional crossover designs. Although SW-CREs have become popular, definitions of estimands and robust methods to target estimands under the potential outcomes framework remain insufficient. To address this gap, we describe a class of estimands that explicitly acknowledge the multilevel data structure in SW-CREs and highlight three typical members of the estimand class that are interpretable. We then introduce four analysis of covariance (ANCOVA) working models to achieve estimand-aligned analyses with covariate adjustment. Each ANCOVA estimator is model-assisted, as its point estimator is consistent even when the working model is misspecified. Under the stepped wedge randomization scheme, we establish the finite population Central Limit Theorem for each estimator. We study the finite-sample operating characteristics of the ANCOVA estimators in simulations and illustrate their application by analyzing the Washington State Expedited Partner Therapy study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.11267v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyuan Chen, Fan Li</dc:creator>
    </item>
    <item>
      <title>The Influence of Nuisance Parameter Uncertainty on Statistical Inference in Practical Data Science Models</title>
      <link>https://arxiv.org/abs/2406.15078</link>
      <description>arXiv:2406.15078v2 Announce Type: replace-cross 
Abstract: For multiple reasons -- such as avoiding overtraining from one data set or because of having received numerical estimates for some parameters in a model from an alternative source -- it is sometimes useful to divide a model's parameters into one group of primary parameters and one group of nuisance parameters. However, uncertainty in the values of nuisance parameters is an inevitable factor that impacts the model's reliability. This paper examines the issue of uncertainty calculation for primary parameters of interest in the presence of nuisance parameters. We illustrate a general procedure on two distinct model forms: 1) the GARCH time series model with univariate nuisance parameter and 2) multiple hidden layer feed-forward neural network models with multivariate nuisance parameters. Leveraging an existing theoretical framework for nuisance parameter uncertainty, we show how to modify the confidence regions for the primary parameters while considering the inherent uncertainty introduced by nuisance parameters. Furthermore, our study validates the practical effectiveness of adjusted confidence regions that properly account for uncertainty in nuisance parameters. Such an adjustment helps data scientists produce results that more honestly reflect the overall uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15078v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunrong Wan</dc:creator>
    </item>
    <item>
      <title>Distance-based Chatterjee correlation: a new generalized robust measure of directed association for multivariate real and complex-valued data</title>
      <link>https://arxiv.org/abs/2406.16458</link>
      <description>arXiv:2406.16458v2 Announce Type: replace-cross 
Abstract: Building upon the Chatterjee correlation (2021: J. Am. Stat. Assoc. 116, p2009) for two real-valued variables, this study introduces a generalized measure of directed association between two vector variables, real or complex-valued, and of possibly different dimensions. The new measure is denoted as the "distance-based Chatterjee correlation", owing to the use here of the "distance transformed data" defined in Szekely et al (2007: Ann. Statist. 35, p2769) for the distance correlation. A main property of the new measure, inherited from the original Chatterjee correlation, is its predictive and asymmetric nature: it measures how well one variable can be predicted by the other, asymmetrically. This allows for inferring the causal direction of the association, by using the method of Blobaum et al (2019: PeerJ Comput. Sci. 1, e169). Since the original Chatterjee correlation is based on ranks, it is not available for complex variables, nor for general multivariate data. The novelty of our work is the extension to multivariate real and complex-valued pairs of vectors, offering a robust measure of directed association in a completely non-parametric setting. Informally, the intuitive assumption used here is that distance correlation is mathematically equivalent to Pearson's correlation when applied to "distance transformed" data. The next logical step is to compute Chatterjee's correlation on the same "distance transformed" data, thereby extending the analysis to multivariate vectors of real and complex valued data. As a bonus, the new measure here is robust to outliers, which is not true for the distance correlation of Szekely et al. Additionally, this approach allows for inference regarding the causal direction of the association between the variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16458v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Roberto D. Pascual-Marqui, Kieko Kochi, Toshihiko Kinoshita</dc:creator>
    </item>
  </channel>
</rss>
