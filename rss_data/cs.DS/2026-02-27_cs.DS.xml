<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Feb 2026 05:00:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Testable Learning of General Halfspaces under Massart Noise</title>
      <link>https://arxiv.org/abs/2602.22300</link>
      <description>arXiv:2602.22300v1 Announce Type: new 
Abstract: We study the algorithmic task of testably learning general Massart halfspaces under the Gaussian distribution. In the testable learning setting, the aim is the design of a tester-learner pair satisfying the following properties: (1) if the tester accepts, the learner outputs a hypothesis and a certificate that it achieves near-optimal error, and (2) it is highly unlikely that the tester rejects if the data satisfies the underlying assumptions. Our main result is the first testable learning algorithm for general halfspaces with Massart noise and Gaussian marginals. The complexity of our algorithm is $d^{\mathrm{polylog}(\min\{1/\gamma, 1/\epsilon \})}$, where $\epsilon$ is the excess error and $\gamma$ is the bias of the target halfspace, which qualitatively matches the known quasi-polynomial Statistical Query lower bound for the non-testable setting. The analysis of our algorithm hinges on a novel sandwiching polynomial approximation to the sign function with multiplicative error that may be of broader interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22300v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilias Diakonikolas, Giannis Iakovidis, Daniel M. Kane, Sihan Liu</dc:creator>
    </item>
    <item>
      <title>static_maps: consteval std::map and std::unordered_map Implementations in C++23</title>
      <link>https://arxiv.org/abs/2602.22506</link>
      <description>arXiv:2602.22506v1 Announce Type: new 
Abstract: Using consteval from C++23, we implement efficient, new versions of std::map and std::unordered_map for use when the keys are known at compile time. We demonstrate superior performance of our unordered_map on three demonstration use-cases: Lookup of elemental mass from atomic symbol, lookup of amino acid from codon, and modification of stock prices from S&amp;P 500 ticker symbols all produced runtimes &lt;40%, &lt;35%, &lt;73% of the respective runtimes of the std implementations. Our library runimes were &lt;80%, &lt;45%, &lt;97% of the lookup time of Frozen, an alternative perfect hashing implementation in C++ for problems also using constexpr keys. To our knowledge, this makes our library the overall fastest drop-in (i.e., with a similar API) alternative to std::unordered_map. On one arbitrarily chosen demo, we demonstrate runtimes &lt;35% of PTHash and &lt;89% gperf, state-of-the-art but not drop-in hashing libraries via external tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22506v1</guid>
      <category>cs.DS</category>
      <category>cs.SE</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Isaac D. Myhal, Oliver Serang</dc:creator>
    </item>
    <item>
      <title>An $\mathcal{O}(\log N)$ Time Algorithm for the Generalized Egg Dropping Problem</title>
      <link>https://arxiv.org/abs/2602.22870</link>
      <description>arXiv:2602.22870v1 Announce Type: new 
Abstract: The generalized egg dropping problem is a canonical benchmark in sequential decision-making. Standard dynamic programming evaluates the minimum number of tests in the worst case in $\mathcal{O}(K \cdot N^2)$ time. The previous state-of-the-art approach formulates the testable thresholds as a partial sum of binomial coefficients and applies a combinatorial search to reduce the time complexity to $\mathcal{O}(K \log N)$. In this paper, we demonstrate that the discrete binary search over the decision tree can be bypassed entirely. By utilizing a relaxation of the binomial bounds, we compute an approximate root that tightly bounds the optimal value. We mathematically prove that this approximation restricts the remaining search space to exactly $\mathcal{O}(K)$ discrete steps. Because constraints inherently enforce $K &lt; \log_2(N+1)$, our algorithm achieves an unconditional worst-case time complexity of $\mathcal{O}(\min(K, \log N))$. Furthermore, we formulate an explicit $\mathcal{O}(1)$ space deterministic policy to dynamically retrace the optimal sequential choices, eliminating classical state-transition matrices completely.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22870v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kleitos Papadopoulos</dc:creator>
    </item>
    <item>
      <title>Efficient Parallel Algorithms for Hypergraph Matching</title>
      <link>https://arxiv.org/abs/2602.22976</link>
      <description>arXiv:2602.22976v1 Announce Type: new 
Abstract: We present efficient parallel algorithms for computing maximal matchings in hypergraphs. Our algorithm finds locally maximal edges in the hypergraph and adds them in parallel to the matching. In the CRCW PRAM models our algorithms achieve $O(\log{m})$ time with $O((\kappa + n) \log {m})$ work w.h.p. where $m$ is the number of hyperedges, and $\kappa$ is the sum of all vertex degrees. The CREW PRAM model algorithm has a running time of $O((\log{\Delta}+\log{d})\log{m})$ and requires $O((\kappa + n) \log {m})$ work w.h.p. It can be implemented work-optimal with $O(\kappa +n)$ work in $O((\log{m}+\log{n})\log{m})$ time. We prove a $1/d$-approximation guarantee for our algorithms.
  We evaluate our algorithms experimentally by implementing and running the proposed algorithms on the GPU using CUDA and Kokkos. Our experimental evaluation demonstrates the practical efficiency of our approach on real-world hypergraph instances, yielding a speed up of up to 76 times compared to a single-core CPU algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22976v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henrik Reinst\"adtler, Christian Schulz, Nodari Sitchinava, Fabian Walliser</dc:creator>
    </item>
    <item>
      <title>Equivalent Dichotomies for Triangle Detection in Subgraph, Induced, and Colored H-Free Graphs</title>
      <link>https://arxiv.org/abs/2602.23196</link>
      <description>arXiv:2602.23196v1 Announce Type: new 
Abstract: A recent paper by the authors (ITCS'26) initiates the study of the Triangle Detection problem in graphs avoiding a fixed pattern $H$ as a subgraph and proposes a \emph{dichotomy hypothesis} characterizing which patterns $H$ make the Triangle Detection problem easier in $H$-free graphs than in general graphs.
  In this work, we demonstrate that this hypothesis is, in fact, equivalent to analogous hypotheses in two broader settings that a priori seem significantly more challenging: \emph{induced} $H$-free graphs and \emph{colored} $H$-free graphs.
  Our main contribution is a reduction from the induced $H$-free case to the non-induced $\H^+$-free case, where $\H^+$ preserves the structural properties of $H$ that are relevant for the dichotomy, namely $3$-colorability and triangle count. A similar reduction is given for the colored case.
  A key technical ingredient is a self-reduction to Unique Triangle Detection that preserves the induced $H$-freeness property, via a new color-coding-like reduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.23196v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>math.CO</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amir Abboud, Ron Safier, Nathan Wallheimer</dc:creator>
    </item>
    <item>
      <title>SYK thermal expectations are classically easy at any temperature</title>
      <link>https://arxiv.org/abs/2602.22619</link>
      <description>arXiv:2602.22619v1 Announce Type: cross 
Abstract: Estimating thermal expectations of local observables is a natural target for quantum advantage. We give a simple classical algorithm that approximates thermal expectations, and we show it has quasi-polynomial cost $n^{O(\log n/\epsilon)}$ for all temperatures above a phase transition in the free energy. For many natural models, this coincides with the entire fast-mixing, quantumly easy phase. Our results apply to the Sachdev-Ye-Kitaev (SYK) model at any constant temperature -- including when the thermal state is highly entangled and satisfies polynomial quantum circuit lower bounds, a sign problem, and nontrivial instance-to-instance fluctuations. Our analysis of the SYK model relies on the replica trick to control the complex zeros of the partition function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22619v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Zlokapa, Bobak T. Kiani</dc:creator>
    </item>
    <item>
      <title>Flip Distance of Triangulations of Convex Polygons / Rotation Distance of Binary Trees is NP-complete</title>
      <link>https://arxiv.org/abs/2602.22874</link>
      <description>arXiv:2602.22874v1 Announce Type: cross 
Abstract: Flips in triangulations of convex polygons arise in many different settings. They are isomorphic to rotations in binary trees, define edges in the 1-skeleton of the Associahedron and cover relations in the Tamari Lattice.
  The complexity of determining the minimum number of flips that transform one triangulation of a convex point set into another remained a tantalizing open question for many decades. We settle this question by proving that computing shortest flip sequences between triangulations of convex polygons, and therefore also computing the rotation distance of binary trees, is NP-hard.
  For our proof we develop techniques for flip sequences of triangulations whose counterparts were introduced for the study of flip sequences of non-crossing spanning trees by Bjerkevik, Kleist, Ueckerdt, and Vogtenhuber~[SODA25] and Bjerkevik, Dorfer, Kleist, Ueckerdt, and Vogtenhuber~[SoCG26].</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22874v1</guid>
      <category>cs.CG</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joseph Dorfer</dc:creator>
    </item>
    <item>
      <title>A Simple Distributed Deterministic Planar Separator</title>
      <link>https://arxiv.org/abs/2602.22916</link>
      <description>arXiv:2602.22916v1 Announce Type: cross 
Abstract: A balanced separator of a graph $G$ is a set of vertices whose removal disconnects the graph into connected components that are a constant factor smaller than $G$. Lipton and Tarjan [FOCS'77] famously proved that every planar graph admits a balanced separator of size $O(\sqrt{n})$, as well as a balanced separator of size $O(D)$ that is a simple path (where $D$ is $G$'s diameter). In the centralized setting, both separators can be found in linear time. In the distributed setting, $D$ is a universal lower bound for the round complexity of solving many optimization problems, so, separators of size $O(D)$ are preferable.
  It was not until [DISC'17] that a distributed algorithm was devised by Ghaffari and Parter to compute such an $O(D)$-size separator in $\tilde O(D)$ rounds, by adapting the Lipton-Tarjan algorithm to the distributed model. Since then, this algorithm was used in several distributed algorithms for planar graphs, e.g., [GP, DISC'17], [LP, STOC'19], [AEDPW, PODC'25]. However, the algorithm is randomized, deeming the algorithms that use it to be randomized as well. Obtaining a deterministic algorithm remained an interesting open question until [PODC'25], when a (complex) deterministic separator algorithm was given by Jauregui, Montealegre and Rapaport.
  We present a much simpler deterministic separator algorithm with the same (near-optimal) $\tilde O(D)$-round complexity. While previous works devised either complicated or randomized ways of transferring weights from vertices to faces of $G$, we show that a straightforward way also works: Each vertex simply transfers its weight to one arbitrary face it lies on. That's it!
  We note that a deterministic separator algorithm directly derandomizes the state-of-the-art distributed algorithms for classical problems on planar graphs such as single-source shortest-paths, maximum-flow, directed global min-cut, and reachability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22916v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yaseen Abd-Elhaleem, Michal Dory, Oren Weimann</dc:creator>
    </item>
    <item>
      <title>Isolation critical graphs under multiple edge subdivision</title>
      <link>https://arxiv.org/abs/2602.22980</link>
      <description>arXiv:2602.22980v1 Announce Type: cross 
Abstract: This paper introduces the notion of $(\iota,q)$-critical graphs. The isolation number of a graph $G$, denoted by $\iota(G)$ and also known as the vertex-edge domination number, is the minimum number of vertices in a set $D$ such that the subgraph induced by the vertices not in the closed neighbourhood of $D$ has no edges.
  A graph $G$ is $(\iota,q)$-critical, $q \ge 1$, if the subdivision of any $q$ edges in $G$ gives a graph with isolation number greater than $\iota(G)$ and there exists a set of $q-1$ edges such that subdividing them gives a graph with isolation number equal to $\iota(G)$.
  We prove that for each integer $q \ge 1$ there exists a $(\iota,q)$-critical graph, while for a given graph $G$, the admissible values of $q$ satisfy $1 \le q \le |E(G)| - 1$. In addition, we provide a general characterisation of $(\iota,1)$-critical graphs as well as a constructive characterisation of $(\iota,1)$-critical trees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22980v1</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karl Bartolo, Peter Borg, Magda Dettlaff, Magdalena Lema\'nska, Pawe{\l} \.Zyli\'nski</dc:creator>
    </item>
    <item>
      <title>Dequantization Barriers for Guided Stoquastic Hamiltonians</title>
      <link>https://arxiv.org/abs/2602.23183</link>
      <description>arXiv:2602.23183v1 Announce Type: cross 
Abstract: We construct a probability distribution, induced by the Perron--Frobenius eigenvector of an exponentially large graph, which cannot be efficiently sampled by any classical algorithm, even when provided with the best-possible warm-start distribution. In the quantum setting, this problem can be viewed as preparing the ground state of a stoquastic Hamiltonian given a guiding state as input, and is known to be efficiently solvable on a quantum computer. Our result suggests that no efficient classical algorithm can solve a broad class of stoquastic ground-state problems.
  Our graph is constructed from a class of high-degree, high-girth spectral expanders to which self-similar trees are attached. This builds on and extends prior work of Gily\'en, Hastings, and Vazirani [Quantum 2021, STOC 2021], which ruled out dequantization for a specific stoquastic adiabatic path algorithm. We strengthen their result by ruling out any classical algorithm for guided ground-state preparation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.23183v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yassine Hamoudi, Yvan Le Borgne, Shrinidhi Teganahally Sridhara</dc:creator>
    </item>
    <item>
      <title>Mean Estimation from Coarse Data: Characterizations and Efficient Algorithms</title>
      <link>https://arxiv.org/abs/2602.23341</link>
      <description>arXiv:2602.23341v1 Announce Type: cross 
Abstract: Coarse data arise when learners observe only partial information about samples; namely, a set containing the sample rather than its exact value. This occurs naturally through measurement rounding, sensor limitations, and lag in economic systems. We study Gaussian mean estimation from coarse data, where each true sample $x$ is drawn from a $d$-dimensional Gaussian distribution with identity covariance, but is revealed only through the set of a partition containing $x$. When the coarse samples, roughly speaking, have ``low'' information, the mean cannot be uniquely recovered from observed samples (i.e., the problem is not identifiable). Recent work by Fotakis, Kalavasis, Kontonis, and Tzamos [FKKT21] established that sample-efficient mean estimation is possible when the unknown mean is identifiable and the partition consists of only convex sets. Moreover, they showed that without convexity, mean estimation becomes NP-hard. However, two fundamental questions remained open: (1) When is the mean identifiable under convex partitions? (2) Is computationally efficient estimation possible under identifiability and convex partitions? This work resolves both questions. [...]</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.23341v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alkis Kalavasis, Anay Mehrotra, Manolis Zampetakis, Felix Zhou, Ziyu Zhu</dc:creator>
    </item>
    <item>
      <title>Connectivity-Preserving Important Separators: A Framework for Cut-Uncut Problems</title>
      <link>https://arxiv.org/abs/2511.15849</link>
      <description>arXiv:2511.15849v2 Announce Type: replace 
Abstract: Graph separation problems are a cornerstone of parameterized complexity, often tackled using the "Important Separators" technique introduced by Marx. While this technique is powerful for standard separation problems, it is inapplicable to problems with connectivity constraints, where the goal is to separate terminal sets while maintaining the internal connectivity of specific components (e.g., Node Multiway Cut-Uncut and 2-sets cut-uncut). In such settings, the standard branching strategies for enumerating important separators fail, and the solution space becomes complex and seemingly unstructured.
  In this paper, we introduce the framework of Connectivity-Preserving (CP) Important Separators. We prove that the number of CP-important separators of size $k$ is $2^{O(k\log k)}$. Leveraging this bound, we present an efficient algorithm to enumerate all such separators. Our approach relies on a fundamental property regarding the union of minimum separators, which allows us to characterize valid separators by systematically "repairing" the connectivity violations of unconstrained separators.
  As a primary application, we present a new fixed-parameter tractable (FPT) algorithm for the Node Multiway Cut-Uncut (N-MWCU) problem. For the fundamental case of 2-Sets Cut-Uncut, and more generally whenever the number of equivalence classes is constant, we improve the running time from the previous best of $2^{O(k^2 \log k)}$ to $2^{O(k \log k)}$. Crucially, our approach avoids the heavy machinery of randomized contractions (and their expensive derandomization) employed by previous work, replacing it with a direct enumeration algorithm that reduces both the exponential dependence on the parameter $k$ and the polynomial overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15849v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Batya Kenig</dc:creator>
    </item>
    <item>
      <title>Robust Algorithms for Finding Cliques in Random Intersection Graphs via Sum-of-Squares</title>
      <link>https://arxiv.org/abs/2511.20376</link>
      <description>arXiv:2511.20376v4 Announce Type: replace 
Abstract: We study efficient algorithms for recovering cliques in dense random intersection graphs (RIGs). In this model, $d = n^{\Omega(1)}$ cliques of size approximately $k$ are randomly planted by choosing the vertices to participate in each clique independently with probability $\delta$. While there has been extensive work on recovering one, or multiple disjointly planted cliques in random graphs, the natural extension of this question to recovering overlapping cliques has been, surprisingly, largely unexplored. Moreover, because every vertex can be part of polynomially many cliques, this task is significantly more challenging than in case of disjointly planted cliques (as recently studied by Kothari, Vempala, Wein and Xu [COLT'23]).
  In this work we obtain the first efficient algorithms for recovering the community structure of RIGs both from the perspective of exact and approximate recovery. Our algorithms are further robust to noise, monotone adversaries, and a certain, optimal number of edge corruptions. They work whenever $k \gg \sqrt{n \log(n)}$. Our techniques follow the proofs-to-algorithms framework utilizing the sum-of-squares hierarchy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20376v4</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Andreas G\"obel, Janosch Ruff, Leon Schiller</dc:creator>
    </item>
    <item>
      <title>DRESS: A Continuous Framework for Structural Graph Refinement</title>
      <link>https://arxiv.org/abs/2602.20833</link>
      <description>arXiv:2602.20833v2 Announce Type: replace 
Abstract: The Weisfeiler-Lehman (WL) hierarchy is a cornerstone framework for graph isomorphism testing and structural analysis. However, scaling beyond 1-WL to 3-WL and higher requires tensor-based operations that scale as $\mathcal{O}(n^3)$ or $\mathcal{O}(n^4)$, making them computationally prohibitive for large graphs. In this paper, we start from the Original-DRESS equation (Castrillo, Le\'{o}n, and G\'{o}mez, 2018) -- a parameter-free, continuous dynamical system on edges -- and show that it distinguishes the prism graph from $K_{3,3}$, a pair that 1-WL provably cannot separate. We then generalize it to Motif-DRESS, which replaces triangle neighborhoods with arbitrary structural motifs and converges to a unique fixed point under three sufficient conditions, and further to Generalized-DRESS, an abstract template parameterized by the choice of neighborhood operator, aggregation function and norm. Finally, we introduce $\Delta$-DRESS, which runs DRESS on each node-deleted subgraph $G \setminus \{v\}$, connecting the framework to the Kelly--Ulam reconstruction conjecture. Both Motif-DRESS and $\Delta$-DRESS empirically distinguish Strongly Regular Graphs (SRGs) -- such as the Rook and Shrikhande graphs -- that confound 3-WL. Our results establish the DRESS family as a highly scalable framework that empirically surpasses both 1-WL and 3-WL on well-known benchmark graphs, without the prohibitive $\mathcal{O}(n^4)$ computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20833v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eduar Castrillo Velilla</dc:creator>
    </item>
    <item>
      <title>DRESS and the WL Hierarchy: Climbing One Deletion at a Time</title>
      <link>https://arxiv.org/abs/2602.21557</link>
      <description>arXiv:2602.21557v2 Announce Type: replace 
Abstract: The Cai--F\"urer--Immerman (CFI) construction provides the canonical family of hard instances for the Weisfeiler--Leman (WL) hierarchy: distinguishing the two non-isomorphic CFI graphs over a base graph $G$ requires $k$-WL where $k$ meets or exceeds the treewidth of $G$. In this paper, we introduce $\Delta^\ell$-DRESS, which applies $\ell$ levels of iterated node deletion to the DRESS continuous structural refinement framework. $\Delta^\ell$-DRESS runs Original-DRESS on all $\binom{n}{\ell}$ subgraphs obtained by removing $\ell$ nodes, and compares the resulting histograms. We show empirically on the canonical CFI benchmark family that Original-DRESS ($\Delta^0$) already distinguishes $\text{CFI}(K_3)$ (requiring 2-WL), and that each additional deletion level extends the range by one WL level: $\Delta^1$ reaches 3-WL, $\Delta^2$ reaches 4-WL, and $\Delta^3$ reaches 5-WL, distinguishing CFI pairs over $K_n$ for $n = 3, \ldots, 6$. Crucially, $\Delta^3$ fails on $\text{CFI}(K_7)$ (requiring 6-WL), confirming a sharp boundary at $(\ell+2)$-WL. The computational cost is $\mathcal{O}\bigl(\binom{n}{\ell} \cdot I \cdot m \cdot d_{\max}\bigr)$ -- polynomial in $n$ for fixed $\ell$. These results establish $\Delta^\ell$-DRESS as a practical framework for systematically climbing the WL hierarchy on the canonical CFI benchmark family.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21557v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eduar Castrillo Velilla</dc:creator>
    </item>
    <item>
      <title>Instance-optimal estimation of L2-norm</title>
      <link>https://arxiv.org/abs/2602.21937</link>
      <description>arXiv:2602.21937v2 Announce Type: replace 
Abstract: The $L_2$-norm, or collision norm, is a core entity in the analysis of distributions and probabilistic algorithms. Batu and Canonne (FOCS 2017) presented an extensive analysis of algorithmic aspects of the $L_2$-norm and its connection to uniformity testing. However, when it comes to estimating the $L_2$-norm itself, their algorithm is not always optimal compared to the instance-specific second-moment bounds, $O(1/(\varepsilon\|\mu\|_2) + (\|\mu\|_3^3 - \|\mu\|_2^4) / (\varepsilon^2 \|\mu\|_2^4))$, as stated by Batu (WoLA 2025, open problem session).
  In this paper, we present an unbiased $L_2$-estimation algorithm whose sample complexity matches the instance-specific second-moment analysis. Additionally, we show that $\Omega(1/(\varepsilon \|\mu\|_2))$ is indeed a per-instance lower bound for estimating the norm of a distribution $\mu$ by sampling (even for non-unbiased estimators).</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21937v2</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomer Adar</dc:creator>
    </item>
    <item>
      <title>Hardness of Maximum Likelihood Learning of DPPs</title>
      <link>https://arxiv.org/abs/2205.12377</link>
      <description>arXiv:2205.12377v4 Announce Type: replace-cross 
Abstract: Determinantal Point Processes (DPPs) are a widely used probabilistic model for negatively correlated sets. DPPs have been successfully employed in Machine Learning applications to select a diverse, yet representative subset of data. In these applications, a set of parameters that maximize the likelihood of the data is typically desirable. The algorithms used for this task to date either optimize over a limited family of DPPs, or use local improvement heuristics that do not provide theoretical guarantees of optimality.
  In his seminal work on DPPs in Machine Learning, Kulesza (2011) conjectured that the problem is NP-complete. The lack of a formal proof prompted Brunel et al. (COLT 2017) to suggest that, in opposition to Kulesza's conjecture, there might exist a polynomial-time algorithm for computing a maximum-likelihood DPP. They also presented some preliminary evidence supporting a conjecture that they suggested might lead to such an algorithm.
  In this work we prove Kulesza's conjecture. In fact, we prove the following stronger hardness of approximation result: even computing a $\left(1-O(\frac{1}{\log^9{N}})\right)$-approximation to the maximum log-likelihood of a DPP on a ground set of $N$ elements is NP-complete.
  From a technical perspective, we reduce the problem of approximating the maximum log-likelihood of a DPP to solving a gap instance of a \textsc{$3$-Coloring} problem on a hypergraph. This hypergraph is based on the bounded-degree construction of Bogdanov et al. (FOCS 2002), which we enhance using the strong expanders of Alon and Capalbo (FOCS 2007). We demonstrate that if a rank-$3$ DPP achieves near-optimal log-likelihood, its marginal kernel must encode an almost perfect ``vector-coloring" of the hypergraph. Finally, we show that these continuous vectors can be decoded into a proper $3$-coloring after removing a small fraction of ``noisy" edges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.12377v4</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Grigorescu, Brendan Juba, Karl Wimmer, Ning Xie</dc:creator>
    </item>
    <item>
      <title>On the Complexity of Neural Computation in Superposition</title>
      <link>https://arxiv.org/abs/2409.15318</link>
      <description>arXiv:2409.15318v3 Announce Type: replace-cross 
Abstract: Superposition, the ability of neural networks to represent more features than neurons, is increasingly seen as key to the efficiency of large models. This paper investigates the theoretical foundations of computing in superposition, establishing complexity bounds for explicit, provably correct algorithms. We present the first lower bounds for a neural network computing in superposition, showing that for a broad class of problems, including permutations and pairwise logical operations, computing $m'$ features in superposition requires at least $\Omega(\sqrt{m' \log m'})$ neurons and $\Omega(m' \log m')$ parameters. This implies an explicit limit on how much one can sparsify or distill a model while preserving its expressibility, and complements empirical scaling laws by implying the first subexponential bound on capacity: a network with $n$ neurons can compute at most $O(n^2 / \log n)$ features. Conversely, we provide a nearly tight constructive upper bound: logical operations like pairwise AND can be computed using $O(\sqrt{m'} \log m')$ neurons and $O(m' \log^2 m')$ parameters. There is thus an exponential gap between the complexity of computing in superposition (the subject of this work) versus merely representing features, which can require as little as $O(\log m')$ neurons based on the Johnson-Lindenstrauss Lemma. Our work analytically establishes that the number of parameters is a good estimator of the number of features a neural network computes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15318v3</guid>
      <category>cs.CC</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>cs.NE</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Micah Adler, Nir Shavit</dc:creator>
    </item>
    <item>
      <title>Implicit Decision Diagrams</title>
      <link>https://arxiv.org/abs/2602.20793</link>
      <description>arXiv:2602.20793v3 Announce Type: replace-cross 
Abstract: Decision Diagrams (DDs) have emerged as a powerful tool for discrete optimization, with rapidly growing adoption. DDs are directed acyclic layered graphs; restricted DDs are a generalized greedy heuristic for finding feasible solutions, and relaxed DDs compute combinatorial relaxed bounds. There is substantial theory that leverages DD-based bounding, yet the complexity of constructing the DDs themselves has received little attention. Standard restricted DD construction requires $O(w \log(w))$ per layer; standard relaxed DD construction requires $O(w^2)$, where $w$ is the width of the DD. Increasing $w$ improves bound quality at the cost of more time and memory.
  We introduce implicit Decision Diagrams, storing arcs implicitly rather than explicitly, and reducing per-layer complexity to $O(w)$ for restricted and relaxed DDs. We prove this is optimal: any framework treating state-update and merge operations as black boxes cannot do better.
  Optimal complexity shifts the challenge from algorithmic overhead to low-level engineering. We show how implicit DDs can drive a MIP solver, and release ImplicitDDs, an open-source Julia solver exploiting the implementation refinements our theory enables. Experiments demonstrate the solver outperforms Gurobi on Subset Sum.
  Code (https://github.com/IsaacRudich/ImplicitDDs.jl)</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20793v3</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isaac Rudich, Louis-Martin Rousseau</dc:creator>
    </item>
  </channel>
</rss>
