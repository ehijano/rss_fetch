<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Feb 2026 05:00:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A context-specific causal model for estimating the effect of extended length of overnight stay on traveller's total expenditure</title>
      <link>https://arxiv.org/abs/2602.18039</link>
      <description>arXiv:2602.18039v1 Announce Type: new 
Abstract: Tourism significantly affects the economies of many countries. Understanding the causal relationship between the length of overnight stay and traveller's expenditure is crucial for stakeholders to characterize spending profiles and to design marketing strategies. Causal mechanisms differ between personal and work-related travel because the decision-making processes have different drivers and constraints. We apply context-specific independence relations to model causal mechanisms in contexts specified by trip purpose and identify the causal effect of the length of stay on expenditure. Using the international visitor survey data on foreign travellers to Finland, we fit a hierarchical Bayesian model to estimate the posterior distribution of the counterfactual expenditure due to extending the length of stay by one night. We also perform a Bayesian sensitivity analysis of the estimated causal effect with respect to omitted variable bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18039v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lauri Valkonen, Juha Karvanen</dc:creator>
    </item>
    <item>
      <title>Forecasting the Evolving Composition of Inbound Tourism Demand: A Bayesian Compositional Time Series Approach Using Platform Booking Data</title>
      <link>https://arxiv.org/abs/2602.18358</link>
      <description>arXiv:2602.18358v1 Announce Type: new 
Abstract: Understanding how the composition of guest origin markets evolves over time is critical for destination marketing organizations, hospitality businesses, and tourism planners. We develop and apply Bayesian Dirichlet autoregressive moving average (BDARMA) models to forecast the compositional dynamics of guest origin market shares using proprietary Airbnb booking data spanning 2017--2024 across four major destination regions. Our analysis reveals substantial pandemic-induced structural breaks in origin composition, with heterogeneous recovery patterns across markets. The BDARMA framework achieves the lowest average forecast error across all destination regions, outperforming standard benchmarks including na\"ive forecasts, exponential smoothing, and SARIMA on log-ratio transformed data. For EMEA destinations, BDARMA achieves 23% lower forecast error than naive methods, with statistically significant improvements. By modeling compositions directly on the simplex with a Dirichlet likelihood and incorporating seasonal variation in both mean and precision parameters, our approach produces coherent forecasts that respect the unit-sum constraint while capturing complex temporal dependencies. The methodology provides destination stakeholders with probabilistic forecasts of source market shares, enabling more informed strategic planning for marketing resource allocation, infrastructure investment, and crisis response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18358v1</guid>
      <category>stat.AP</category>
      <category>q-fin.ST</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison Katz</dc:creator>
    </item>
    <item>
      <title>Hidden multistate models to study multimorbidity trajectories</title>
      <link>https://arxiv.org/abs/2602.18369</link>
      <description>arXiv:2602.18369v1 Announce Type: new 
Abstract: Multimorbidity in older adults is common, heterogeneous, and highly dynamic, and it is strongly associated with disability and increased healthcare utilization. However, existing approaches to studying multimorbidity trajectories are largely descriptive or rely on discrete-time models, which struggle to handle irregular observation intervals and right-censoring. We developed a continuous-time hidden multistate modeling framework to capture transitions among latent multimorbidity patterns while accounting for interval censoring and misclassification. A simulation study compared alternative model specifications under varying sample sizes and follow-up schemes, and the best-performing specification was applied to longitudinal data from the Swedish National study on Aging and Care-Kungsholmen (SNAC-K), including 2,716 multimorbid participants followed for up to 18 years. Simulation results showed that hidden multistate models substantially reduced bias in transition hazard estimates compared to non-hidden models, with fully time-inhomogeneous models outperforming piecewise approximations. Application to SNAC-K confirmed the feasibility and practical utility of this framework, enabling identification of risk factors for accelerated progression toward complex multimorbidity and revealing a gradient of mortality risk across patterns. Continuous-time hidden multistate models provide a robust alternative to traditional approaches, supporting individualized predictions and informing targeted interventions and secondary prevention strategies for multimorbidity in aging populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18369v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Valentina Manzoni, Francesca Ieva, Amaia Calder\'on-Larra\~naga, Davide Liborio Vetrano, Caterina Gregorio</dc:creator>
    </item>
    <item>
      <title>Anisotropic local law for non-separable sample covariance matrices</title>
      <link>https://arxiv.org/abs/2602.17960</link>
      <description>arXiv:2602.17960v1 Announce Type: cross 
Abstract: We establish local laws for sample covariance matrices $K = N^{-1}\sum_{i=1}^N \g_i\g_i^*$ where the random vectors $\g_1, \ldots, \g_N \in \R^n$ are independent with common covariance $\Sigma$. Previous work has largely focused on the separable model $\g = \Sigma^{1/2}\w$ with $\w$ having independent entries, but this structure is rarely present in statistical applications involving dependent or nonlinearly transformed data. Under a concentration assumption for quadratic forms $\g^*A\g$, we prove an optimal averaged local law showing that the Stieltjes transform of $K$ converges to its deterministic limit uniformly down to the optimal scale $\eta \geq N^{-1+\eps}$. Under an additional structural assumption on the cumulant tensors of $\g$ -- which interpolates between the highly structured case of independent entries and generic dependence -- we establish the full anisotropic local law, providing entrywise control of the resolvent $(K-zI)^{-1}$ in arbitrary directions. We discuss several classes of non-separable examples satisfying our assumptions, including conditionally mean-zero distributions, the random features model $\g = \sigma(X\w)$ arising in machine learning, and Gaussian measures with nonlinear tilting. The proofs introduce a tensor network framework for analyzing fluctuation averaging in the presence of higher-order cumulant structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17960v1</guid>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhou Fan, Renyuan Ma, Elliot Paquette, Zhichao Wang</dc:creator>
    </item>
    <item>
      <title>Hybrid Non-informative and Informative Prior Model-assisted Designs for Mid-trial Dose Insertion</title>
      <link>https://arxiv.org/abs/2602.17995</link>
      <description>arXiv:2602.17995v1 Announce Type: cross 
Abstract: In oncology phase I trials, model-assisted designs have been increasingly adopted because they enable adaptive yet operationally simple dose adjustment based on accumulating safety data, leading to a paradigm shift in dose-escalation methodology. In practice, a single mid-trial dose insertion may be considered to examine safer doses and/or to collect more informative efficacy data. In this study, we investigate methods to improve dose assignment and the selection of the maximum tolerated dose (MTD) or the optimal biological dose (OBD) when a new dose level is added during an ongoing trial under a model-assisted framework, by assigning informative prior information to the inserted dose. We propose a hybrid design that uses a non-informative model-assisted design at trial initiation and, upon dose insertion, applies an informative-prior extension only to the newly added dose. In addition, to address potential skeleton misspecification, we propose two adaptive extensions: (i) an online-weighting approach that updates the skeleton over time, and (ii) a Bayesian-mixture approach that robustly combines multiple candidate skeletons. We evaluate the proposed methods through simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17995v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kana Yamada, Hisato Sunami, Kentaro Takeda, Keisuke Hanada, Masahiro Kojima</dc:creator>
    </item>
    <item>
      <title>Probabilistic Methods for Initial Orbit Determination and Orbit Determination in Cislunar Space</title>
      <link>https://arxiv.org/abs/2602.18058</link>
      <description>arXiv:2602.18058v1 Announce Type: cross 
Abstract: In orbital mechanics, Gauss's method for orbit determination (OD) is a popular, minimal assumption solution for obtaining the initial state estimate of a passing resident space object (RSO). Since much of the cislunar domain relies on three-body dynamics, a key assumption of Gauss's method is rendered incompatible, creating a need for a new, minimal assumption method for initial orbit determination (IOD). In this work, we present a framework for short and long term probabilistic target tracking in cislunar space which produces an initial state estimate with as few assumptions as possible. Specifically, we propose an IOD method involving the kinematic fitting of several series of noisy, consecutive ground-based observations. Once a probabilistic initial state estimate in the form of a particle cloud is formed, we apply the powerful Particle Gaussian Mixture (PGM) Filter to reduce the uncertainty of our state estimate over time. This combined IOD/OD framework is demonstrated for several classes of trajectories in cislunar space and compared to better-known filtering frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18058v1</guid>
      <category>astro-ph.EP</category>
      <category>astro-ph.IM</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <category>physics.data-an</category>
      <category>physics.space-ph</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ishan Paranjape, Tarun Hejmadi, Suman Chakravorty</dc:creator>
    </item>
    <item>
      <title>PRISM-FCP: Byzantine-Resilient Federated Conformal Prediction via Partial Sharing</title>
      <link>https://arxiv.org/abs/2602.18396</link>
      <description>arXiv:2602.18396v1 Announce Type: cross 
Abstract: We propose PRISM-FCP (Partial shaRing and robust calIbration with Statistical Margins for Federated Conformal Prediction), a Byzantine-resilient federated conformal prediction framework that utilizes partial model sharing to improve robustness against Byzantine attacks during both model training and conformal calibration. Existing approaches address adversarial behavior only in the calibration stage, leaving the learned model susceptible to poisoned updates. In contrast, PRISM-FCP mitigates attacks end-to-end. During training, clients partially share updates by transmitting only $M$ of $D$ parameters per round. This attenuates the expected energy of an adversary's perturbation in the aggregated update by a factor of $M/D$, yielding lower mean-square error (MSE) and tighter prediction intervals. During calibration, clients convert nonconformity scores into characterization vectors, compute distance-based maliciousness scores, and downweight or filter suspected Byzantine contributions before estimating the conformal quantile. Extensive experiments on both synthetic data and the UCI Superconductivity dataset demonstrate that PRISM-FCP maintains nominal coverage guarantees under Byzantine attacks while avoiding the interval inflation observed in standard FCP with reduced communication, providing a robust and communication-efficient approach to federated uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18396v1</guid>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ehsan Lari, Reza Arablouei, Stefan Werner</dc:creator>
    </item>
    <item>
      <title>Density correction for multivariate spatial fields of global climate model output using deep learning</title>
      <link>https://arxiv.org/abs/2411.18799</link>
      <description>arXiv:2411.18799v3 Announce Type: replace 
Abstract: Global Climate Models (GCMs) are numerical models that simulate complex physical processes within the Earth's climate system and are essential for understanding and predicting climate change. However, GCMs suffer from systemic biases due to simplifications made to the underlying physical processes. GCM output therefore needs to be bias corrected before it can be used for future climate projections. Most common bias correction methods, however, cannot preserve spatial, temporal, or inter-variable dependencies. We propose a new semi-parametric estimation of conditional densities (SPECD) approach for density correction of the joint distribution of daily precipitation and maximum temperature data obtained from gridded GCM spatial fields. The Vecchia approximation is employed to preserve dependencies in the observed field during the density correction process, which is carried out using semi-parametric quantile regression. The ability to calibrate joint distributions of GCM projections has potential advantages not only in estimating extremes, but also in better estimating compound hazards, like heat waves and drought, under potential climate change. Illustration on historical data from 1951-2014 over two 5 x 5 spatial grids in the US indicate that SPECD can preserve key marginal and joint distribution properties of precipitation and maximum temperature, and predictions obtained using SPECD are better calibrated compared to predictions using asynchronous quantile mapping and canonical correlation analysis, two commonly used bias correction approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18799v3</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Reetam Majumder, Shiqi Fang, A. Sankarasubramanian, Emily C. Hector, Brian J. Reich</dc:creator>
    </item>
    <item>
      <title>Dynamic Count Models with Flexible Innovation Processes for Irregular Maritime Migration</title>
      <link>https://arxiv.org/abs/2508.18716</link>
      <description>arXiv:2508.18716v2 Announce Type: replace 
Abstract: Motivated by the challenge of analyzing the dynamics of weekly sea border crossings in the Mediterranean (2015-2025) and the English Channel (2018-2025), we develop a Bayesian dynamic framework for modeling heteroskedastic count time series. Building on theoretical considerations and empirical stylized facts, our approach utilizes a Poisson random walk model that allows for heavy-tailed innovations or stochastic volatility dynamics, while incorporating an explicit mechanism to separate structural from sampling zeros. Posterior inference is carried out via a straightforward Markov chain Monte Carlo algorithm. Applying this methodology to the Mediterranean and English Channel data, we compare alternative model specifications through a comprehensive out-of-sample density forecasting exercise. Evaluating each model using log predictive scores and empirical coverage up to the 99th percentile, we find strong evidence for stochastic volatility in the migration innovations, with these models producing well-calibrated forecasts even at extreme quantiles. Our framework can be used to develop risk indicators with direct policy implications for improving governance and preparedness for migration surges. More broadly, the methodology extends to other zero-inflated non-stationary count time series applications, including epidemiological surveillance and public safety incident monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18716v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gregor Zens, Jakub Bijak</dc:creator>
    </item>
    <item>
      <title>Learning from geometry-aware near misses to real-time COR: A corridor-wide grouped random parameters GEV framework</title>
      <link>https://arxiv.org/abs/2509.02871</link>
      <description>arXiv:2509.02871v4 Announce Type: replace 
Abstract: Real-time corridor-wide crash-occurrence risk (COR) prediction is challenging because existing near-miss extreme value theory (EVT) models often oversimplify collision geometry, neglect vehicle-infrastructure (V-I) interactions, and inadequately account for spatial heterogeneity in traffic and roadway conditions. This study develops a geometry-aware two-dimensional time-to-collision (2D-TTC) near-miss extraction framework and integrates it with a hierarchical Bayesian grouped random parameter unified generalized extreme value model (HBSGRP-UGEV) to estimate short-term COR in urban corridors. The proposed framework builds on prior grouped EVT formulations while explicitly accommodating both vehicle-vehicle (V-V) and vehicle-infrastructure (V-I) near-miss processes within a unified corridor-wide modeling structure. High-resolution trajectories from the Argoverse-2 dataset were analyzed across 28 sites along Miami's Biscayne Boulevard to extract extreme near-miss events. The model incorporates vehicle dynamics and roadway features as covariates, with partial pooling across segments and intersections to capture corridor-wide heterogeneity. Results indicate that the HBSGRP-UGEV framework outperforms the fixed-parameter HBSFP-UGEV model, reducing the deviance information criterion (DIC) by up to 7.5 percent for V-V interactions and 3.1 percent for V-I interactions. Predictive validation using receiver operating characteristic area under the curve (ROC-AUC) demonstrates strong classification performance, with values of 0.89 for V-V segments, 0.82 for V-V intersections, 0.79 for V-I segments, and 0.75 for V-I intersections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02871v4</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Anis, Yang Zhou, Dominique Lord</dc:creator>
    </item>
    <item>
      <title>A semi-parametric approach for estimating consumer valuation distributions using second price auctions</title>
      <link>https://arxiv.org/abs/2312.07882</link>
      <description>arXiv:2312.07882v3 Announce Type: replace-cross 
Abstract: We focus on online second price auctions, where bids are made sequentially, and the winning bidder pays the maximum of the second-highest bid and a seller specified starting price. For many such auctions, the seller does not see all the bids or the total number of bidders accessing the auction, and only observes the current selling prices throughout the course of the auction. We develop a novel semi-parametric approach to estimate the underlying consumer valuation distribution based on this data. Previous semi-parametric or non-parametric approaches in the literature only use the final selling price and assume knowledge of the total number of bidders. The resulting estimate, in particular, can be used by the seller to compute the optimal profit-maximizing price for the product. Our approach is free of tuning parameters, and we demonstrate its computational and statistical efficiency in a variety of simulation settings, and also on an Xbox 7-day auction dataset on eBay.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07882v3</guid>
      <category>stat.ME</category>
      <category>cs.GT</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourav Mukherjee, Ziqian Yang, Rohit K Patra, Kshitij Khare</dc:creator>
    </item>
    <item>
      <title>Adaptive tuning of Hamiltonian Monte Carlo methods</title>
      <link>https://arxiv.org/abs/2506.04082</link>
      <description>arXiv:2506.04082v3 Announce Type: replace-cross 
Abstract: With the recently increased interest in probabilistic models, the efficiency of an underlying sampler becomes a crucial consideration. Hamiltonian Monte Carlo (HMC) is one popular option for models of this kind. Performance of the method, however, strongly relies on a choice of parameters associated with an integration for Hamiltonian equations. Up to date, such a choice remains mainly heuristic or introduces time complexity. We propose a novel computationally inexpensive and flexible approach (we call it Adaptive Tuning or ATune) that, by combining a theoretical analysis of the multivariate Gaussian model with simulation data generated during a burn-in stage of a HMC simulation, detects a system specific splitting integrator with a set of reliable sampler's hyperparameters, including their credible randomization intervals, to be readily used in a production simulation. The method automatically eliminates those values of simulation parameters which could cause undesired extreme scenarios, such as resonance artifacts, low accuracy or poor sampling. The new approach is implemented in the in-house software package HaiCS, with no computational overheads introduced in a production simulation, and can be easily incorporated in any package for Bayesian inference with HMC. The tests on popular statistical models reveal the superiority of adaptively tuned standard and generalized HMC (GHMC) methods in terms of stability, performance and accuracy over conventional HMC tuned heuristically and coupled with the well-established integrators. We also claim that GHMC is preferable for achieving high sampling performance. The efficiency of the new methodology is assessed in comparison with state-of-the-art samplers, e.g. NUTS, in real-world applications, such as endocrine therapy resistance in cancer, modeling of cell-cell adhesion dynamics and influenza A epidemic outbreak.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04082v3</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Akhmatskaya, Lorenzo Nagar, Jose Antonio Carrillo, Leonardo Gavira Balmacz, Hristo Inouzhe, Mart\'in Parga Pazos, Mar\'ia Xos\'e Rodr\'iguez \'Alvarez</dc:creator>
    </item>
    <item>
      <title>Classification errors distort findings in automated speech processing: examples and solutions from child-development research</title>
      <link>https://arxiv.org/abs/2508.15637</link>
      <description>arXiv:2508.15637v2 Announce Type: replace-cross 
Abstract: With the advent of wearable recorders, scientists are increasingly turning to automated methods of analysis of audio and video data in order to measure children's experience, behavior, and outcomes, with a sizable literature employing long-form audio-recordings to study language acquisition. While numerous articles report on the accuracy and reliability of the most popular automated classifiers, less has been written on the downstream effects of classification errors on measurements and statistical inferences (e.g., the estimate of correlations and effect sizes in regressions). This paper's main contributions are drawing attention to downstream effects of confusion errors, and providing an approach to measure and potentially recover from these errors. Specifically, we use a Bayesian approach to study the effects of algorithmic errors on key scientific questions, including the effect of siblings on children's language experience and the association between children's production and their input. By fitting a joint model of speech behavior and algorithm behavior on real and simulated data, we show that classification errors can significantly distort estimates for both the most commonly used \gls{lena}, and a slightly more accurate open-source alternative (the Voice Type Classifier from the ACLEW system). We further show that a Bayesian calibration approach for recovering unbiased estimates of effect sizes can be effective and insightful, but does not provide a fool-proof solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15637v2</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas Gautheron, Evan Kidd, Anton Malko, Marvin Lavechin, Alejandrina Cristia</dc:creator>
    </item>
    <item>
      <title>The Global Representativeness Index: A Total Variation Distance Framework for Measuring Demographic Fidelity in Survey Research</title>
      <link>https://arxiv.org/abs/2602.14835</link>
      <description>arXiv:2602.14835v2 Announce Type: replace-cross 
Abstract: Global survey research increasingly informs high-stakes decisions in AI governance and cross-cultural policy, yet no standardized metric quantifies how well a sample's demographic composition matches its target population. Response rates and demographic quotas -- the prevailing proxies for sample quality -- measure effort and coverage but not distributional fidelity. This paper introduces the Global Representativeness Index (GRI), a framework grounded in Total Variation Distance that scores any survey sample against population benchmarks across multiple demographic dimensions on a [0, 1] scale. Validation on seven waves of the Global Dialogues survey (N = 7,500 across 60+ countries) finds fine-grained demographic GRI scores of only 0.33--0.36 -- roughly 43% of the theoretical maximum at that sample size. Cross-validation on the World Values Survey (seven waves, N = 403,000), Afrobarometer Round 9 (N = 53,000), and Latinobarometro (N = 19,000) reveals that even large probability surveys score below 0.22 on fine-grained global demographics when country coverage is limited. The GRI connects to classical survey statistics through the design effect; both metrics are recommended as a minimum summary of sample quality, since GRI quantifies demographic distance symmetrically while effective N captures the asymmetric inferential cost of underrepresentation. The framework is released as an open-source Python library with UN and Pew Research Center population benchmarks, applicable to survey research, machine learning dataset auditing, and AI evaluation benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14835v2</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Evan Hadfield, Andrew Konya</dc:creator>
    </item>
  </channel>
</rss>
