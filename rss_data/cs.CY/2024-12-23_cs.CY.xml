<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Dec 2024 05:00:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Investigating the importance of social vulnerability in opioid-related mortality across the United States</title>
      <link>https://arxiv.org/abs/2412.15218</link>
      <description>arXiv:2412.15218v1 Announce Type: new 
Abstract: The opioid crisis remains a critical public health challenge in the United States. Despite national efforts which reduced opioid prescribing rates by nearly 45\% between 2011 and 2021, opioid overdose deaths more than tripled during this same period. Such alarming trends raise important questions about what underlying social factors may be driving opioid misuse. Using county-level data across the United States, this study begins with a preliminary data analysis of how the rates of thirteen social vulnerability index variables manifest in counties with both anomalously high and low mortality rates, identifying patterns that warrant further investigation. Building on these findings, we further investigate the importance of the thirteen SVI variables within a machine learning framework by employing two predictive models: XGBoost and a modified autoencoder. Both models take the thirteen SVI variables as input and predict county-level opioid-related mortality rates. This allows us to leverage two distinct feature importance metrics: information gain for XGBoost and a Shapley gradient explainer for the autoencoder. These metrics offer two unique insights into the most important SVI factors in relation to opioid-related mortality. By identifying the variables which consistently rank as most important, this study highlights key social vulnerability factors that may play critical roles in the opioid crisis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15218v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Deas, Adam Spannaus, Dakotah D. Maguire, Jodie Trafton, Anuj J. Kapadia, Vasileios Maroulas</dc:creator>
    </item>
    <item>
      <title>Analyzing Computing Undergraduate Majors from Job Market Perspective</title>
      <link>https://arxiv.org/abs/2412.15219</link>
      <description>arXiv:2412.15219v1 Announce Type: new 
Abstract: The demand for computing education increases due to the rapid development of technology and its involvement in most daily activities. Academic institutes offer a variety of computing majors, such as Computer Engineering, Computer Science, Information Systems, Information Technology, Software Engineering, Cybersecurity, and Data Science. Since a major objective of earning a bachelor's degree is to improve career opportunities, it is crucial to understand how the job market perceives these computing majors. This study analyzed the relationships between various computing majors and the job market in Saudi Arabia, using LinkedIn public profile data, discovering insights into the strong relationship between the focus of certain computing majors and the employment of relevant job positions. Moreover, job category trends were analyzed over the past ten years, observing that demands for System Admin and Technical Support positions declined while demands for Business Analysis and Artificial Intelligence and Data Science inclined. This study also compared earned professional certifications between different computing major graduates that correspond to job position findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15219v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yazeed Alabdulkarim, Khalid Alruwayti, Hamad Alsaleh, Sultan Alfallaj, Ahmed Bablail, Abdulaziz Almaslukh</dc:creator>
    </item>
    <item>
      <title>GPS-2-GTFS: A Python package to process and transform raw GPS data of public transit to GTFS format</title>
      <link>https://arxiv.org/abs/2412.15221</link>
      <description>arXiv:2412.15221v1 Announce Type: new 
Abstract: The gps2gtfs package addresses a critical need for converting raw Global Positioning System (GPS) trajectory data from public transit vehicles into the widely used GTFS (General Transit Feed Specification) format. This transformation enables various software applications to efficiently utilize real-time transit data for purposes such as tracking, scheduling, and arrival time prediction. Developed in Python, gps2gtfs employs techniques like geo-buffer mapping, parallel processing, and data filtering to manage challenges associated with raw GPS data, including high volume, discontinuities, and localization errors. This open-source package, available on GitHub and PyPI, enhances the development of intelligent transportation solutions and fosters improved public transit systems globally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15221v1</guid>
      <category>cs.CY</category>
      <category>cs.MS</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shiveswarran Ratneswaran, Uthayasanker Thayasivam, Sivakumar Thillaiambalam</dc:creator>
    </item>
    <item>
      <title>Public Engagement in Action: Developing an Introductory Programming Module for Apprentices</title>
      <link>https://arxiv.org/abs/2412.15223</link>
      <description>arXiv:2412.15223v1 Announce Type: new 
Abstract: Programming is a crucial skill in today's world and being taught worldwide at different levels. However, in the literature there is little research investigating a formal approach to embedding public engagement into programming module design. This paper explores the integration of public engagement into an introductory programming module, at the University of Warwick, UK, as part of the Digital and Technology Solutions (DTS) degree apprenticeship. The module design follows a 'V' model, which integrates community engagement with traditional programming education, providing a holistic learning experience. The aim is to enhance learning by combining programming education with community engagement. Apprentices participate in outreach activities, teaching programming and Arduino hardware to local secondary school students. This hands-on approach aligns with Kolb's experiential learning model, improving communication skills and solidifying programming concepts through teaching. The module also includes training in safeguarding, presentation skills, and storytelling to prepare apprentices for public engagement. Pedagogical techniques in the module include live coding, group exercises, and Arduino kit usage, as well as peer education, allowing apprentices to learn from and teach each other. Degree apprentices, who balance part-time studies with full-time employment, bring diverse knowledge and motivations. The benefit of public engagement is that it helps bridge their skills gap, fostering teamwork and creating a positive learning environment. Embedding public engagement in programming education also enhances both technical and soft skills, providing apprentices with a deeper understanding of community issues and real-world applications. Our design supports their academic and professional growth, ensuring the module's ongoing success and impact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15223v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.21125/iceri.2024.0716</arxiv:DOI>
      <dc:creator>Jianhua Yang, Mir Seyedebrahimi, Margaret Low, Holly Heshmati</dc:creator>
    </item>
    <item>
      <title>Learning-by-teaching with ChatGPT: The effect of teachable ChatGPT agent on programming education</title>
      <link>https://arxiv.org/abs/2412.15226</link>
      <description>arXiv:2412.15226v1 Announce Type: new 
Abstract: This study investigates the potential of using ChatGPT as a teachable agent to support students' learning by teaching process, specifically in programming education. While learning by teaching is an effective pedagogical strategy for promoting active learning, traditional teachable agents have limitations, particularly in facilitating natural language dialogue. Our research explored whether ChatGPT, with its ability to engage learners in natural conversations, can support this process. The findings reveal that interacting with ChatGPT improves students' knowledge gains and programming abilities, particularly in writing readable and logically sound code. However, it had limited impact on developing learners' error-correction skills, likely because ChatGPT tends to generate correct code, reducing opportunities for students to practice debugging. Additionally, students' self-regulated learning (SRL) abilities improved, suggesting that teaching ChatGPT fosters learners' higher self-efficacy and better implementation of SRL strategies. This study discussed the role of natural dialogue in fostering socialized learning by teaching, and explored ChatGPT's specific contributions in supporting students' SRL through the learning by teaching process. Overall, the study highlights ChatGPT's potential as a teachable agent, offering insights for future research on ChatGPT-supported education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15226v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Angxuan Chen, Yuang Wei, Huixiao Le, Yan Zhang</dc:creator>
    </item>
    <item>
      <title>Do Voters Get the Information They Want? Understanding Authentic Voter FAQs in the US and How to Improve for Informed Electoral Participation</title>
      <link>https://arxiv.org/abs/2412.15273</link>
      <description>arXiv:2412.15273v1 Announce Type: new 
Abstract: Accurate information is crucial for democracy as it empowers voters to make informed decisions about their representatives and keeping them accountable. In the US, state election commissions (SECs), often required by law, are the primary providers of Frequently Asked Questions (FAQs) to voters, and secondary sources like non-profits such as League of Women Voters (LWV) try to complement their information shortfall. However, surprisingly, to the best of our knowledge, there is neither a single source with comprehensive FAQs nor a study analyzing the data at national level to identify current practices and ways to improve the status quo. This paper addresses it by providing the {\bf first dataset on Voter FAQs covering all the US states}. Second, we introduce metrics for FAQ information quality (FIQ) with respect to questions, answers, and answers to corresponding questions. Third, we use FIQs to analyze US FAQs to identify leading, mainstream and lagging content practices and corresponding states. Finally, we identify what states across the spectrum can do to improve FAQ quality and thus, the overall information ecosystem. Across all 50 U.S. states, 12% were identified as leaders and 8% as laggards for FIQS\textsubscript{voter}, while 14% were leaders and 12% laggards for FIQS\textsubscript{developer}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15273v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vipula Rawte, Deja N Scott, Gaurav Kumar, Aishneet Juneja, Bharat Sowrya Yaddanapalli, Biplav Srivastava</dc:creator>
    </item>
    <item>
      <title>Making Transparency Advocates: An Educational Approach Towards Better Algorithmic Transparency in Practice</title>
      <link>https://arxiv.org/abs/2412.15363</link>
      <description>arXiv:2412.15363v1 Announce Type: new 
Abstract: Concerns about the risks and harms posed by artificial intelligence (AI) have resulted in significant study into algorithmic transparency, giving rise to a sub-field known as Explainable AI (XAI). Unfortunately, despite a decade of development in XAI, an existential challenge remains: progress in research has not been fully translated into the actual implementation of algorithmic transparency by organizations. In this work, we test an approach for addressing the challenge by creating transparency advocates, or motivated individuals within organizations who drive a ground-up cultural shift towards improved algorithmic transparency.
  Over several years, we created an open-source educational workshop on algorithmic transparency and advocacy. We delivered the workshop to professionals across two separate domains to improve their algorithmic transparency literacy and willingness to advocate for change. In the weeks following the workshop, participants applied what they learned, such as speaking up for algorithmic transparency at an organization-wide AI strategy meeting. We also make two broader observations: first, advocacy is not a monolith and can be broken down into different levels. Second, individuals' willingness for advocacy is affected by their professional field. For example, news and media professionals may be more likely to advocate for algorithmic transparency than those working at technology start-ups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15363v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Andrew Bell, Julia Stoyanovich</dc:creator>
    </item>
    <item>
      <title>Predicting Long-Term Student Outcomes from Short-Term EdTech Log Data</title>
      <link>https://arxiv.org/abs/2412.15473</link>
      <description>arXiv:2412.15473v1 Announce Type: new 
Abstract: Educational stakeholders are often particularly interested in sparse, delayed student outcomes, like end-of-year statewide exams. The rare occurrence of such assessments makes it harder to identify students likely to fail such assessments, as well as making it slow for researchers and educators to be able to assess the effectiveness of particular educational tools. Prior work has primarily focused on using logs from students full usage (e.g. year-long) of an educational product to predict outcomes, or considered predictive accuracy using a few minutes to predict outcomes after a short (e.g. 1 hour) session. In contrast, we investigate machine learning predictors using students' logs during their first few hours of usage can provide useful predictive insight into those students' end-of-school year external assessment. We do this on three diverse datasets: from students in Uganda using a literacy game product, and from students in the US using two mathematics intelligent tutoring systems. We consider various measures of the accuracy of the resulting predictors, including its ability to identify students at different parts along the assessment performance distribution. Our findings suggest that short-term log usage data, from 2-5 hours, can be used to provide valuable signal about students' long-term external performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15473v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706468.3706552</arxiv:DOI>
      <dc:creator>Ge Gao, Amelia Leon, Andrea Jetten, Jasmine Turner, Husni Almoubayyed, Stephen Fancsali, Emma Brunskill</dc:creator>
    </item>
    <item>
      <title>AI Apology: A Critical Review of Apology in AI Systems</title>
      <link>https://arxiv.org/abs/2412.15787</link>
      <description>arXiv:2412.15787v1 Announce Type: new 
Abstract: Apologies are a powerful tool used in human-human interactions to provide affective support, regulate social processes, and exchange information following a trust violation. The emerging field of AI apology investigates the use of apologies by artificially intelligent systems, with recent research suggesting how this tool may provide similar value in human-machine interactions. Until recently, contributions to this area were sparse, and these works have yet to be synthesised into a cohesive body of knowledge. This article provides the first synthesis and critical analysis of the state of AI apology research, focusing on studies published between 2020 and 2023. We derive a framework of attributes to describe five core elements of apology: outcome, interaction, offence, recipient, and offender. With this framework as the basis for our critique, we show how apologies can be used to recover from misalignment in human-AI interactions, and examine trends and inconsistencies within the field. Among the observations, we outline the importance of curating a human-aligned and cross-disciplinary perspective in this research, with consideration for improved system capabilities and long-term outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15787v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hadassah Harland, Richard Dazeley, Hashini Senaratne, Peter Vamplew, Francisco Cruz, Bahareh Nakisa</dc:creator>
    </item>
    <item>
      <title>Applying Predictive Analytics to Occupational Health and Safety in India</title>
      <link>https://arxiv.org/abs/2412.16038</link>
      <description>arXiv:2412.16038v1 Announce Type: new 
Abstract: Predictive analytics is revolutionizing occupational health and safety (OHS). It offers evidence-based insights. These insights enable proactive risk management and informed, data-driven decision-making in organizational settings. This paper explores the key components of predictive analytics in OHS, beginning with data collection, management, and preparation, and moving through to advanced predictive modelling techniques. We emphasize the importance of data integrity through processes such as missing value imputation, anomaly detection, and feature engineering to ensure accurate model predictions. Risk prioritization identifies and ranks hazards across various factors, including employee behaviours, organizational policies, environmental conditions, and operational practices. We posit that insights derived from predictive models must be effectively interpreted and implemented. These insights guide organizations to focus on high-impact areas for accident prevention and resource optimization. The integration of predictive analytics in OHS brings notable benefits, including enhanced decision-making, greater operational efficiency, cost savings, and improved compliance with safety standards. We examine applications of predictive analytics in OHS in Indian settings. India has the largest workforce in the world, and the predominance of it is in the informal sector - a sector largely unprotected by the already inadequate OHS laws. Ethical considerations, data privacy concerns, and the risk of overdependence on predictive models are discussed. We conclude with a discussion on the potential for predictive analytics to create a data-oriented, adaptive approach to OHS in India. We posit that, using predictive analytics, India can develop high safety standards while traversing the complexities of its workforce setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16038v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ritwik Raj Saxena</dc:creator>
    </item>
    <item>
      <title>On the Impact of 3D Visualization of Repository Metrics in Software Engineering Education</title>
      <link>https://arxiv.org/abs/2412.16061</link>
      <description>arXiv:2412.16061v1 Announce Type: new 
Abstract: Context: Software development is a complex socio-technical process requiring a deep understanding of various aspects. In order to support practitioners in understanding such a complex activity, repository process metrics, like number of pull requests and issues, emerged as crucial for evaluating CI/CD workflows and guiding informed decision-making. The research community proposed different ways to visualize these metrics to increase their impact on developers' process comprehension: VR is a promising one. Nevertheless, despite such promising results, the role of VR, especially in educational settings, has received limited research attention. Objective: This study aims to address this gap by exploring how VR-based repository metrics visualization can support the teaching of process comprehension. Method: The registered report proposes the execution of a controlled experiment where VR and non-VR approaches will be compared, with the final aim to assess whether repository metrics in VR's impact on learning experience and software process comprehension. By immersing students in an intuitive environment, this research hypothesizes that VR can foster essential analytical skills, thus preparing software engineering students more effectively for industry requirements and equipping them to navigate complex software development tasks with enhanced comprehension and critical thinking abilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16061v1</guid>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dario Di Dario, Stefano Lambiase, Fabio Palomba, Carmine Gravino</dc:creator>
    </item>
    <item>
      <title>Blockchain in Environmental Sustainability Measures: a Survey</title>
      <link>https://arxiv.org/abs/2412.15261</link>
      <description>arXiv:2412.15261v1 Announce Type: cross 
Abstract: Real and effective regulation of contributions to greenhouse gas emissions and pollutants requires unbiased and truthful monitoring. Blockchain has emerged not only as an approach that provides verifiable economical interactions but also as a mechanism to keep the measurement, monitoring, incentivation of environmental conservationist practices and enforcement of policy. Here, we present a survey of areas in what blockchain has been considered as a response to concerns on keeping an accurate recording of environmental practices to monitor levels of pollution and management of environmental practices. We classify the applications of blockchain into different segments of concerns, such as greenhouse gas emissions, solid waste, water, plastics, food waste, and circular economy, and show the objectives for the addressed concerns. We also classify the different blockchains and the explored and designed properties as identified for the proposed solutions. At the end, we provide a discussion about the niches and challenges that remain for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15261v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.3390/blockchains2030016</arxiv:DOI>
      <arxiv:journal_reference>Blockchains 2024</arxiv:journal_reference>
      <dc:creator>Maria-Victoria Vladucu, Hailun Wu, Jorge Medina, Khondaker M. Salehin, Ziqian Dong, Roberto Rojas-Cessa</dc:creator>
    </item>
    <item>
      <title>Baichuan4-Finance Technical Report</title>
      <link>https://arxiv.org/abs/2412.15270</link>
      <description>arXiv:2412.15270v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning, yet their potential in finance remains underexplored due to the complexity and specialization of financial knowledge. In this work, we report the development of the Baichuan4-Finance series, including a comprehensive suite of foundational Baichuan4-Finance-Base and an aligned language model Baichuan4-Finance, which are built upon Baichuan4-Turbo base model and tailored for finance domain. Firstly, we have dedicated significant effort to building a detailed pipeline for improving data quality. Moreover, in the continual pre-training phase, we propose a novel domain self-constraint training strategy, which enables Baichuan4-Finance-Base to acquire financial knowledge without losing general capabilities. After Supervised Fine-tuning and Reinforcement Learning from Human Feedback and AI Feedback, the chat model Baichuan4-Finance is able to tackle various financial certification questions and real-world scenario applications. We evaluate Baichuan4-Finance on many widely used general datasets and two holistic financial benchmarks. The evaluation results show that Baichuan4-Finance-Base surpasses almost all competitive baselines on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. At the same time, Baichuan4-Finance demonstrates even more impressive performance on financial application scenarios, showcasing its potential to foster community innovation in the financial LLM field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15270v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hanyu Zhang, Boyu Qiu, Yuhao Feng, Shuqi Li, Qian Ma, Xiyuan Zhang, Qiang Ju, Dong Yan, Jian Xie</dc:creator>
    </item>
    <item>
      <title>Quantifying detection rates for dangerous capabilities: a theoretical model of dangerous capability evaluations</title>
      <link>https://arxiv.org/abs/2412.15433</link>
      <description>arXiv:2412.15433v1 Announce Type: cross 
Abstract: We present a quantitative model for tracking dangerous AI capabilities over time. Our goal is to help the policy and research community visualise how dangerous capability testing can give us an early warning about approaching AI risks. We first use the model to provide a novel introduction to dangerous capability testing and how this testing can directly inform policy. Decision makers in AI labs and government often set policy that is sensitive to the estimated danger of AI systems, and may wish to set policies that condition on the crossing of a set threshold for danger. The model helps us to reason about these policy choices. We then run simulations to illustrate how we might fail to test for dangerous capabilities. To summarise, failures in dangerous capability testing may manifest in two ways: higher bias in our estimates of AI danger, or larger lags in threshold monitoring. We highlight two drivers of these failure modes: uncertainty around dynamics in AI capabilities and competition between frontier AI labs. Effective AI policy demands that we address these failure modes and their drivers. Even if the optimal targeting of resources is challenging, we show how delays in testing can harm AI policy. We offer preliminary recommendations for building an effective testing ecosystem for dangerous capabilities and advise on a research agenda.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15433v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paolo Bova, Alessandro Di Stefano, The Anh Han</dc:creator>
    </item>
    <item>
      <title>How to Manage My Data? With Machine--Interpretable GDPR Rights!</title>
      <link>https://arxiv.org/abs/2412.15451</link>
      <description>arXiv:2412.15451v1 Announce Type: cross 
Abstract: The EU GDPR is a landmark regulation that introduced several rights for individuals to obtain information and control how their personal data is being processed, as well as receive a copy of it. However, there are gaps in the effective use of rights due to each organisation developing custom methods for rights declaration and management. Simultaneously, there is a technological gap as there is no single consistent standards-based mechanism that can automate the handling of rights for both organisations and individuals. In this article, we present a specification for exercising and managing rights in a machine-interpretable format based on semantic web standards. Our approach uses the comprehensive Data Privacy Vocabulary to create a streamlined workflow for individuals to understand what rights exist, how and where to exercise them, and for organisations to effectively manage them. This work pushes the state of the art in GDPR rights management and is crucial for data reuse and rights management under technologically intensive developments, such as Data Spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15451v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Beatriz Esteves, Harshvardhan J. Pandit, Georg P. Krog, Paul Ryan</dc:creator>
    </item>
    <item>
      <title>Climate Policy Elites' Twitter Interactions across Nine Countries</title>
      <link>https://arxiv.org/abs/2412.15545</link>
      <description>arXiv:2412.15545v1 Announce Type: cross 
Abstract: We identified the Twitter accounts of 941 climate change policy actors across nine countries, and collected their activities from 2017--2022, totalling 48 million activities from 17,700 accounts at different organizational levels. There is considerable temporal and cross-national variation in how prominent climate-related activities were, but all national policy systems generally responded to climate-related events, such as climate protests, in a similar manner. Examining patterns of interaction within and across countries, we find that these national policy systems rarely directly interact with one another, but are connected through consistently engaging with the same content produced by accounts of international organizations, climate activists, and researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15545v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ted Hsuan Yun Chen, Arttu Malkam\"aki, Ali Faqeeh, Esa Palosaari, Anniina Kotkaniemi, Laura Funke, C\'ait Gleeson, James Goodman, Antti Gronow, Marlene Kammerer, Myanna Lahsen, Alexandre Marques, Petr Ocelik, Shivangi Seth, Mark Stoddart, Martin Svozil, Pradip Swarnakar, Matthew Trull, Paul Wagner, Yixi Yang, Mikko Kivel\"a, Tuomas Yl\"a-Anttila</dc:creator>
    </item>
    <item>
      <title>Grade of the law of distribution of the vector of the digital twin of the enterprise</title>
      <link>https://arxiv.org/abs/2412.15562</link>
      <description>arXiv:2412.15562v1 Announce Type: cross 
Abstract: Digital transformation acts as a main factor in the development and competitiveness of Japanese companies. In this context, the digital copy management algorithm based on the P2M (Project to Method) method plays a significant role in increasing efficiency and reducing costs through the implementation of digital technologies. This research aims to develop and implement an algorithm for managing a digital copy of a project using the P2M method using the Kolmogorov test to evaluate the effectiveness of various processes and determine the best ways to improve productivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15562v1</guid>
      <category>math.OC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1051/e3sconf/202454908022</arxiv:DOI>
      <arxiv:journal_reference>E3S Web of Conferences 549, 08022 (2024)</arxiv:journal_reference>
      <dc:creator>Sergei Masaev, Ivan Oreshnikov, Nikita Ivanitskiy, Valentina Vingert</dc:creator>
    </item>
    <item>
      <title>Safe Spaces or Toxic Places? Content Moderation and Social Dynamics of Online Eating Disorder Communities</title>
      <link>https://arxiv.org/abs/2412.15721</link>
      <description>arXiv:2412.15721v1 Announce Type: cross 
Abstract: Social media platforms have become critical spaces for discussing mental health concerns, including eating disorders. While these platforms can provide valuable support networks, they may also amplify harmful content that glorifies disordered cognition and self-destructive behaviors. While social media platforms have implemented various content moderation strategies, from stringent to laissez-faire approaches, we lack a comprehensive understanding of how these different moderation practices interact with user engagement in online communities around these sensitive mental health topics. This study addresses this knowledge gap through a comparative analysis of eating disorder discussions across Twitter/X, Reddit, and TikTok. Our findings reveal that while users across all platforms engage similarly in expressing concerns and seeking support, platforms with weaker moderation (like Twitter/X) enable the formation of toxic echo chambers that amplify pro-anorexia rhetoric. These results demonstrate how moderation strategies significantly influence the development and impact of online communities, particularly in contexts involving mental health and self-harm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15721v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kristina Lerman, Minh Duc Chu, Charles Bickham, Luca Luceri, Emilio Ferrara</dc:creator>
    </item>
    <item>
      <title>Understanding the Structure and Resilience of the Brazilian Federal Road Network Through Network Science</title>
      <link>https://arxiv.org/abs/2412.15865</link>
      <description>arXiv:2412.15865v1 Announce Type: cross 
Abstract: Understanding how transportation networks work is important for improving connectivity, efficiency, and safety. In Brazil, where road transport is a significant portion of freight and passenger movement, network science can provide valuable insights into the structural properties of the infrastructure, thus helping decision makers responsible for proposing improvements to the system. This paper models the federal road network as weighted networks, with the intent to unveil its topological characteristics and identify key locations (cities) that play important roles for the country through 75,000 kilometres of roads. We start with a simple network to examine basic connectivity and topology, where weights are the distance of the road segment. We then incorporate other weights representing number of incidents, population, and number of cities in-between each segment. We then focus on community detection as a way to identify clusters of cities that form cohesive groups within a network. Our findings aim to bring clarity to the overall structure of federal roads in Brazil, thus providing actionable insights for improving infrastructure planning and prioritising resources to enhance network resilience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15865v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julio Taveira, Fernando Buarque de Lima Neto, Ronaldo Menezes</dc:creator>
    </item>
    <item>
      <title>Born With a Silver Spoon? Investigating Socioeconomic Bias in Large Language Models</title>
      <link>https://arxiv.org/abs/2403.14633</link>
      <description>arXiv:2403.14633v4 Announce Type: replace 
Abstract: Socioeconomic bias in society exacerbates disparities, influencing access to opportunities and resources based on individuals' economic and social backgrounds. This pervasive issue perpetuates systemic inequalities, hindering the pursuit of inclusive progress as a society. In this paper, we investigate the presence of socioeconomic bias, if any, in large language models. To this end, we introduce a novel dataset SilverSpoon, consisting of 3000 samples that illustrate hypothetical scenarios that involve underprivileged people performing ethically ambiguous actions due to their circumstances, and ask whether the action is ethically justified. Further, this dataset has a dual-labeling scheme and has been annotated by people belonging to both ends of the socioeconomic spectrum. Using SilverSpoon, we evaluate the degree of socioeconomic bias expressed in large language models and the variation of this degree as a function of model size. We also perform qualitative analysis to analyze the nature of this bias. Our analysis reveals that while humans disagree on which situations require empathy toward the underprivileged, most large language models are unable to empathize with the socioeconomically underprivileged regardless of the situation. To foster further research in this domain, we make SilverSpoon and our evaluation harness publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14633v4</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Smriti Singh, Shuvam Keshari, Vinija Jain, Aman Chadha</dc:creator>
    </item>
    <item>
      <title>Data Publishing in Mechanics and Dynamics: Challenges, Guidelines, and Examples from Engineering Design</title>
      <link>https://arxiv.org/abs/2410.18358</link>
      <description>arXiv:2410.18358v2 Announce Type: replace 
Abstract: Data-based methods have gained increasing importance in engineering, especially but not only driven by successes with deep artificial neural networks. Success stories are prevalent, e.g., in areas such as data-driven modeling, control and automation, as well as surrogate modeling for accelerated simulation. Beyond engineering, generative and large-language models are increasingly helping with tasks that, previously, were solely associated with creative human processes. Thus, it seems timely to seek artificial-intelligence-support for engineering design tasks to automate, help with, or accelerate purpose-built designs of engineering systems, e.g., in mechanics and dynamics, where design so far requires a lot of specialized knowledge. However, research-wise, compared to established, predominantly first-principles-based methods, the datasets used for training, validation, and test become an almost inherent part of the overall methodology. Thus, data publishing becomes just as important in (data-driven) engineering science as appropriate descriptions of conventional methodology in publications in the past. This article analyzes the value and challenges of data publishing in mechanics and dynamics, in particular regarding engineering design tasks, showing that the latter raise also challenges and considerations not typical in fields where data-driven methods have been booming originally. Possible ways to deal with these challenges are discussed and a set of examples from across different design problems shows how data publishing can be put into practice. The analysis, discussions, and examples are based on the research experience made in a priority program of the German research foundation focusing on research on artificially intelligent design assistants in mechanics and dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18358v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.ET</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Henrik Ebel, Jan van Delden, Timo L\"uddecke, Aditya Borse, Rutwik Gulakala, Marcus Stoffel, Manish Yadav, Merten Stender, Leon Schindler, Kristin Miriam de Payrebrune, Maximilian Raff, C. David Remy, Benedict R\"oder, Rohit Raj, Tobias Rentschler, Alexander Tismer, Stefan Riedelbauch, Peter Eberhard</dc:creator>
    </item>
    <item>
      <title>Trust Dynamics and Market Behavior in Cryptocurrency: A Comparative Study of Centralized and Decentralized Exchanges</title>
      <link>https://arxiv.org/abs/2404.17227</link>
      <description>arXiv:2404.17227v2 Announce Type: replace-cross 
Abstract: In the rapidly evolving cryptocurrency landscape, trust is a critical yet underexplored factor shaping market behaviors and driving user preferences between centralized exchanges (CEXs) and decentralized exchanges (DEXs). Despite its importance, trust remains challenging to measure, limiting the study of its effects on market dynamics. The collapse of FTX, a major CEX, provides a unique natural experiment to examine the measurable impacts of trust and its sudden erosion on the cryptocurrency ecosystem. This pivotal event raised questions about the resilience of centralized trust systems and accelerated shifts toward decentralized alternatives. This research investigates the impacts of the FTX collapse on user trust, focusing on token valuation, trading flows, and sentiment dynamics. Employing causal inference methods, including Regression Discontinuity Design (RDD) and Difference-in-Differences (DID), we reveal significant declines in WETH prices and NetFlow from CEXs to DEXs, signaling a measurable transfer of trust. Additionally, natural language processing methods, including topic modeling and sentiment analysis, uncover the complexities of user responses, highlighting shifts from functional discussions to emotional fragmentation in Binance's community, while Uniswap's sentiment exhibits a gradual upward trend. Despite data limitations and external influences, the findings underscore the intricate interplay between trust, sentiment, and market behavior in the cryptocurrency ecosystem. By bridging blockchain analytics, behavioral finance, and decentralized finance (DeFi), this study contributes to interdisciplinary research, offering a deeper understanding of distributed trust mechanisms and providing critical insights for future investigations into the socio-technical dimensions of trust in digital economies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17227v2</guid>
      <category>econ.GN</category>
      <category>cs.CE</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>q-fin.EC</category>
      <category>q-fin.RM</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xintong Wu, Wanlin Deng, Yutong Quan, Luyao Zhang</dc:creator>
    </item>
    <item>
      <title>Using Case Studies to Teach Responsible AI to Industry Practitioners</title>
      <link>https://arxiv.org/abs/2407.14686</link>
      <description>arXiv:2407.14686v3 Announce Type: replace-cross 
Abstract: Responsible AI (RAI) encompasses the science and practice of ensuring that AI design, development, and use are socially sustainable -- maximizing the benefits of technology while mitigating its risks. Industry practitioners play a crucial role in achieving the objectives of RAI, yet there is a persistent a shortage of consolidated educational resources and effective methods for teaching RAI to practitioners.
  In this paper, we present a stakeholder-first educational approach using interactive case studies to foster organizational and practitioner-level engagement and enhance learning about RAI. We detail our partnership with Meta, a global technology company, to co-develop and deliver RAI workshops to a diverse company audience. Assessment results show that participants found the workshops engaging and reported an improved understanding of RAI principles, along with increased motivation to apply them in their work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14686v3</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Julia Stoyanovich, Rodrigo Kreis de Paula, Armanda Lewis, Chloe Zheng</dc:creator>
    </item>
    <item>
      <title>AI-assisted summary of suicide risk Formulation</title>
      <link>https://arxiv.org/abs/2412.10388</link>
      <description>arXiv:2412.10388v2 Announce Type: replace-cross 
Abstract: Background: Formulation, associated with suicide risk assessment, is an individualised process that seeks to understand the idiosyncratic nature and development of an individual's problems. Auditing clinical documentation on an electronic health record (EHR) is challenging as it requires resource-intensive manual efforts to identify keywords in relevant sections of specific forms. Furthermore, clinicians and healthcare professionals often do not use keywords; their clinical language can vary greatly and may contain various jargon and acronyms. Also, the relevant information may be recorded elsewhere. This study describes how we developed advanced Natural Language Processing (NLP) algorithms, a branch of Artificial Intelligence (AI), to analyse EHR data automatically. Method: Advanced Optical Character Recognition techniques were used to process unstructured data sets, such as portable document format (pdf) files. Free text data was cleaned and pre-processed using Normalisation of Free Text techniques. We developed algorithms and tools to unify the free text. Finally, the formulation was checked for the presence of each concept based on similarity using NLP-powered semantic matching techniques. Results: We extracted information indicative of formulation and assessed it to cover the relevant concepts. This was achieved using a Weighted Score to obtain a Confidence Level. Conclusion: The rigour to which formulation is completed is crucial to effectively using EHRs, ensuring correct and timely identification, engagement and interventions that may potentially avoid many suicide attempts and suicides.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10388v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rajib Rana, Niall Higgins, Kazi N. Haque, John Reilly, Kylie Burke, Kathryn Turner, Anthony R. Pisani, Terry Stedman</dc:creator>
    </item>
  </channel>
</rss>
