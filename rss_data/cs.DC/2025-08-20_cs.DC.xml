<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Aug 2025 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Time-optimal Asynchronous Minimal Vertex Covering by Myopic Robots</title>
      <link>https://arxiv.org/abs/2508.14247</link>
      <description>arXiv:2508.14247v1 Announce Type: new 
Abstract: In a connected graph with an autonomous robot swarm with limited visibility, it is natural to ask whether the robots can be deployed to certain vertices satisfying a given property using only local knowledge. This paper affirmatively answers the question with a set of \emph{myopic} (finite visibility range) luminous robots with the aim of \emph{filling a minimal vertex cover} (MVC) of a given graph $G = (V, E)$. The graph has special vertices, called \emph{doors}, through which robots enter sequentially. Starting from the doors, the goal of the robots is to settle on a set of vertices that forms a minimal vertex cover of $G$ under the asynchronous ($\mathcal{ASYNC}$) scheduler. We are also interested in achieving the \emph{minimum vertex cover} (MinVC, which is NP-hard \cite{Karp1972} for general graphs) for a specific graph class using the myopic robots. We establish lower bounds on the visibility range for the robots and on the time complexity (which is $\Omega(|E|)$). We present two algorithms for trees: one for single door, which is both time and memory-optimal, and the other for multiple doors, which is memory-optimal and achieves time-optimality when the number of doors is a constant. Interestingly, our technique achieves MinVC on trees with a single door. We then move to the general graph, where we present two algorithms, one for the single door and the other for the multiple doors with an extra memory of $O(\log \Delta)$ for the robots, where $\Delta$ is the maximum degree of $G$. All our algorithms run in $O(|E|)$ epochs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14247v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saswata Jana, Subhajit Pramanick, Adri Bhattacharya, Partha Sarathi Mandal</dc:creator>
    </item>
    <item>
      <title>Pure Data Spaces</title>
      <link>https://arxiv.org/abs/2508.14271</link>
      <description>arXiv:2508.14271v1 Announce Type: new 
Abstract: In a previous work, "pure data" is proposed as an axiomatic foundation for mathematics and computing, based on "finite sequence" as the foundational concept rather than based on logic or type. Within this framework, objects with mathematical meaning are "data" and collections of mathematical objects must then be associative data, called a "space." A space is then the basic collection in this framework analogous to sets in Set Theory or objects in Category Theory. A theory of spaces is developed,where spaces are studied via their semiring of endomorphisms. To illustrate these concepts, and as a way of exploring the implications of the framework, pure data spaces are "grown organically" from the substrate of pure data with minimal combinatoric definitions. Familiar objects from classical mathematics emerge this way, including natural numbers, integers, rational numbers, boolean spaces, matrix algebras, Gaussian Integers, Quaternions, and non-associative algebras like the Integer Octonions. Insights from these examples are discussed with a view towards new directions in theory and new exploration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14271v1</guid>
      <category>cs.DC</category>
      <category>math.LO</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saul Youssef</dc:creator>
    </item>
    <item>
      <title>SSSP-Del: Fully Dynamic Distributed Algorithm for Single-Source Shortest Path</title>
      <link>https://arxiv.org/abs/2508.14319</link>
      <description>arXiv:2508.14319v1 Announce Type: new 
Abstract: Modern graphs are both large and dynamic, presenting significant challenges for fundamental queries, such as the Single-Source Shortest Path (SSSP) problem. Naively recomputing the SSSP tree after each topology change is prohibitively expensive, causing on-demand computation to suffer from high latency. Existing dynamic SSSP algorithms often cannot simultaneously handle both edge additions and deletions, operate in distributed memory, and provide low-latency query results. To address these challenges, this paper presents SSSP-Del, a new vertex-centric, asynchronous, and fully distributed algorithm for dynamic SSSP. Operating in a shared-nothing architecture, our algorithm processes streams of both edge insertions and deletions. We conduct a comprehensive evaluation on large real-world and synthetic graphs with millions of vertices, and provide a thorough analysis by evaluating result latency, solution stability, and throughput.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14319v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Parshan Javanrood, Matei Ripeanu</dc:creator>
    </item>
    <item>
      <title>A Hierarchical Sharded Blockchain Balancing Performance and Availability</title>
      <link>https://arxiv.org/abs/2508.14457</link>
      <description>arXiv:2508.14457v1 Announce Type: new 
Abstract: Blockchain networks offer decentralization, transparency, and immutability for managing critical data but encounter scalability problems as the number of network members and transaction issuers grows. Sharding is considered a promising solution to enhance blockchain scalability. However, most existing blockchain sharding techniques prioritize performance at the cost of availability (e.g., a failure in a few servers holding a shard leads to data unavailability). In this paper, we propose PyloChain, a hierarchical sharded blockchain that balances availability and performance. PyloChain consists of multiple lower-level local chains and one higher-level main chain. Each local chain speculatively executes local transactions to achieve high parallelism across multiple local chains. The main chain leverages a directed-acyclic-graph (DAG)-based mempool to guarantee local block availability and to enable efficient Byzantine Fault Tolerance (BFT) consensus to execute global (or cross-shard) transactions within a collocated sharding. PyloChain speculatively executes local transactions across multiple local chains to achieve high parallelism. In order to reduce the number of aborted local transactions, PyloChain applies a simple scheduling technique to handle global transactions in the main chain. PyloChain provides a fine-grained auditing mechanism to mitigate faulty higher-level members by externalizing main chain operations to lower-level local members. We implemented and evaluated PyloChain, demonstrating its performance scalability with 1.49x higher throughput and 2.63x faster latency compared to the state-of-the-art balanced hierarchical sharded blockchain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14457v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongrae Jo, Chanik Park</dc:creator>
    </item>
    <item>
      <title>Auditable Shared Objects: From Registers to Synchronization Primitives</title>
      <link>https://arxiv.org/abs/2508.14506</link>
      <description>arXiv:2508.14506v1 Announce Type: new 
Abstract: Auditability allows to track operations performed on a shared object, recording who accessed which information. This gives data owners more control on their data. Initially studied in the context of single-writer registers, this work extends the notion of auditability to other shared objects, and studies their properties.
  We start by moving from single-writer to multi-writer registers, and provide an implementation of an auditable $n$-writer $m$-reader read / write register, with $O(n+m)$ step complexity. This implementation uses $(m+n)$-sliding registers, which have consensus number $m+n$. We show that this consensus number is necessary. The implementation extends naturally to support an auditable load-linked / store-conditional (LL/SC) shared object. LL/SC is a primitive that supports efficient implementation of many shared objects. Finally, we relate auditable registers to other access control objects, by implementing an anti-flickering deny list from auditable registers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14506v1</guid>
      <category>cs.DC</category>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>DISC 2025</arxiv:journal_reference>
      <dc:creator>Hagit Attiya, Antonio Fern\'andez Anta, Alessia Milani, Alexandre Rapetti, Corentin Travers</dc:creator>
    </item>
    <item>
      <title>Boosting Payment Channel Network Liquidity with Topology Optimization and Transaction Selection</title>
      <link>https://arxiv.org/abs/2508.14524</link>
      <description>arXiv:2508.14524v1 Announce Type: new 
Abstract: Payment channel networks (PCNs) are a promising technology that alleviates blockchain scalability by shifting the transaction load from the blockchain to the PCN. Nevertheless, the network topology has to be carefully designed to maximise the transaction throughput in PCNs. Additionally, users in PCNs also have to make optimal decisions on which transactions to forward and which to reject to prolong the lifetime of their channels. In this work, we consider an input sequence of transactions over $p$ parties. Each transaction consists of a transaction size, source, and target, and can be either accepted or rejected (entailing a cost). The goal is to design a PCN topology among the $p$ cooperating parties, along with the channel capacities, and then output a decision for each transaction in the sequence to minimise the cost of creating and augmenting channels, as well as the cost of rejecting transactions. Our main contribution is an $\mathcal{O}(p)$ approximation algorithm for the problem with $p$ parties. We further show that with some assumptions on the distribution of transactions, we can reduce the approximation ratio to $\mathcal{O}(\sqrt{p})$. We complement our theoretical analysis with an empirical study of our assumptions and approach in the context of the Lightning Network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14524v1</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Krishnendu Chatterjee, Jan Maty\'a\v{s} K\v{r}i\v{s}\v{t}an, Stefan Schmid, Jakub Svoboda, Michelle Yeo</dc:creator>
    </item>
    <item>
      <title>A Systematic Evaluation of the Potential of Carbon-Aware Execution for Scientific Workflows</title>
      <link>https://arxiv.org/abs/2508.14625</link>
      <description>arXiv:2508.14625v1 Announce Type: new 
Abstract: Scientific workflows are widely used to automate scientific data analysis and often involve computationally intensive processing of large datasets on compute clusters. As such, their execution tends to be long-running and resource-intensive, resulting in substantial energy consumption and, depending on the energy mix, carbon emissions. Meanwhile, a wealth of carbon-aware computing methods have been proposed, yet little work has focused specifically on scientific workflows, even though they present a substantial opportunity for carbon-aware computing because they are often significantly delay tolerant, efficiently interruptible, highly scalable and widely heterogeneous. In this study, we first exemplify the problem of carbon emissions associated with running scientific workflows, and then show the potential for carbon-aware workflow execution. For this, we estimate the carbon footprint of seven real-world Nextflow workflows executed on different cluster infrastructures using both average and marginal carbon intensity data. Furthermore, we systematically evaluate the impact of carbon-aware temporal shifting, and the pausing and resuming of the workflow. Moreover, we apply resource scaling to workflows and workflow tasks. Finally, we report the potential reduction in overall carbon emissions, with temporal shifting capable of decreasing emissions by over 80%, and resource scaling capable of decreasing emissions by 67%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14625v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kathleen West, Youssef Moawad, Fabian Lehmann, Vasilis Bountris, Ulf Leser, Yehia Elkhatib, Lauritz Thamsen</dc:creator>
    </item>
    <item>
      <title>DAG it off: Latency Prefers No Common Coins</title>
      <link>https://arxiv.org/abs/2508.14716</link>
      <description>arXiv:2508.14716v1 Announce Type: new 
Abstract: We introduce Black Marlin, the first Directed Acyclic Graph (DAG)-based Byzantine atomic broadcast protocol in a partially synchronous setting that successfully forgoes the reliable broadcast and common coin primitives. Black Marlin achieves the optimal latency of 3 rounds of communication (4.25 with Byzantine faults) while maintaining optimal communication and amortized communication complexities. We present a formal security analysis of the protocol, accompanied by empirical evidence that Black Marlin outperforms state-of-the-art DAG-based protocols in both throughput and latency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14716v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Amores-Sesar Ignacio, Gr{\o}ndal Viktor, Holmg{\aa}rd Adam, Ottendal Mads</dc:creator>
    </item>
    <item>
      <title>MOHAF: A Multi-Objective Hierarchical Auction Framework for Scalable and Fair Resource Allocation in IoT Ecosystems</title>
      <link>https://arxiv.org/abs/2508.14830</link>
      <description>arXiv:2508.14830v1 Announce Type: new 
Abstract: The rapid growth of Internet of Things (IoT) ecosystems has intensified the challenge of efficiently allocating heterogeneous resources in highly dynamic, distributed environments. Conventional centralized mechanisms and single-objective auction models, focusing solely on metrics such as cost minimization or revenue maximization, struggle to deliver balanced system performance. This paper proposes the Multi-Objective Hierarchical Auction Framework (MOHAF), a distributed resource allocation mechanism that jointly optimizes cost, Quality of Service (QoS), energy efficiency, and fairness. MOHAF integrates hierarchical clustering to reduce computational complexity with a greedy, submodular optimization strategy that guarantees a (1-1/e) approximation ratio. A dynamic pricing mechanism adapts in real time to resource utilization, enhancing market stability and allocation quality. Extensive experiments on the Google Cluster Data trace, comprising 3,553 requests and 888 resources, demonstrate MOHAF's superior allocation efficiency (0.263) compared to Greedy (0.185), First-Price (0.138), and Random (0.101) auctions, while achieving perfect fairness (Jain's index = 1.000). Ablation studies reveal the critical influence of cost and QoS components in sustaining balanced multi-objective outcomes. With near-linear scalability, theoretical guarantees, and robust empirical performance, MOHAF offers a practical and adaptable solution for large-scale IoT deployments, effectively reconciling efficiency, equity, and sustainability in distributed resource coordination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14830v1</guid>
      <category>cs.DC</category>
      <category>cs.GT</category>
      <category>cs.NE</category>
      <category>cs.NI</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kushagra Agrawal, Polat Goktas, Anjan Bandopadhyay, Debolina Ghosh, Junali Jasmine Jena, Mahendra Kumar Gourisaria</dc:creator>
    </item>
    <item>
      <title>Leveraging Hardware-Aware Computation in Mixed-Precision Matrix Multiply: A Tile-Centric Approach</title>
      <link>https://arxiv.org/abs/2508.14848</link>
      <description>arXiv:2508.14848v1 Announce Type: new 
Abstract: General Matrix Multiplication (GEMM) is a critical operation underpinning a wide range of applications in high-performance computing (HPC) and artificial intelligence (AI). The emergence of hardware optimized for low-precision arithmetic necessitates a reevaluation of numerical algorithms to leverage mixed-precision computations, achieving improved performance and energy efficiency. This research introduces an adaptive mixed-precision GEMM framework that supports different precision formats at fine-grained tile/block levels. We utilize the PaRSEC runtime system to balance workloads across various architectures. The performance scales well on ARM CPU-based Fugaku supercomputer, Nvidia GPU-based A100 DGX, and AMD GPU-based Frontier supercomputer. This research aims to enhance computational efficiency and accuracy by bridging algorithmic advancements and hardware innovations, driving transformative progress in various applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14848v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiao Zhang, Rabab Alomairy, Dali Wang, Zhuowei Gu, Qinglei Cao</dc:creator>
    </item>
    <item>
      <title>The Cost Advantage of Virtual Machine Migrations: Empirical Insights into Amazon's EC2 Marketspace</title>
      <link>https://arxiv.org/abs/2508.14883</link>
      <description>arXiv:2508.14883v1 Announce Type: new 
Abstract: In recent years, cloud providers have introduced novel approaches for trading virtual machines. For example, Virtustream introduced so-called muVMs to charge cloud computing resources while other providers such as Google, Microsoft, or Amazon re-invented their marketspaces. Today, the market leader Amazon runs six marketspaces for trading virtual machines. Consumers can purchase bundles of virtual machines, which are called cloud-portfolios, from multiple marketspaces and providers. An industry-relevant field of research is to identify best practices and guidelines on how such optimal portfolios are created. In the paper at hand, a cost analysis of cloud portfolios is presented. Therefore, pricing data from Amazon was used as well as a real virtual machine utilization dataset from the Bitbrains datacenter. The results show that a cost optimum can only be reached if heterogeneous portfolios are created where virtual machines are purchased from different marketspaces. Additionally, the cost-benefit of migrating virtual machines to different marketplaces during runtime is presented. Such migrations are especially cost-effective for virtual machines of cloud-portfolios which run between 6 hours and 1 year. The paper further shows that most of the resources of virtual machines are never utilized by consumers, which represents a significant future potential for cost optimization. For the validation of the results, a second dataset of the Bitbrains datacenter was used, which contains utility data of virtual machines from a different domain of application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14883v1</guid>
      <category>cs.DC</category>
      <category>cs.GT</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benedikt Pittl, Werner Mach, Erich Schikuta</dc:creator>
    </item>
    <item>
      <title>Federated Action Recognition for Smart Worker Assistance Using FastPose</title>
      <link>https://arxiv.org/abs/2508.14113</link>
      <description>arXiv:2508.14113v1 Announce Type: cross 
Abstract: In smart manufacturing environments, accurate and real-time recognition of worker actions is essential for productivity, safety, and human-machine collaboration. While skeleton-based human activity recognition (HAR) offers robustness to lighting, viewpoint, and background variations, most existing approaches rely on centralized datasets, which are impractical in privacy-sensitive industrial scenarios. This paper presents a federated learning (FL) framework for pose-based HAR using a custom skeletal dataset of eight industrially relevant upper-body gestures, captured from five participants and processed using a modified FastPose model. Two temporal backbones, an LSTM and a Transformer encoder, are trained and evaluated under four paradigms: centralized, local (per-client), FL with weighted federated averaging (FedAvg), and federated ensemble learning (FedEnsemble). On the global test set, the FL Transformer improves over centralized training by +12.4 percentage points, with FedEnsemble delivering a +16.3 percentage points gain. On an unseen external client, FL and FedEnsemble exceed centralized accuracy by +52.6 and +58.3 percentage points, respectively. These results demonstrate that FL not only preserves privacy but also substantially enhances cross-user generalization, establishing it as a practical solution for scalable, privacy-aware HAR in heterogeneous industrial settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14113v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.HC</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vinit Hegiste, Vidit Goyal, Tatjana Legler, Martin Ruskowski</dc:creator>
    </item>
    <item>
      <title>A High Performance GPU CountSketch Implementation and Its Application to Multisketching and Least Squares Problems</title>
      <link>https://arxiv.org/abs/2508.14209</link>
      <description>arXiv:2508.14209v1 Announce Type: cross 
Abstract: Random sketching is a dimensionality reduction technique that approximately preserves norms and singular values up to some $O(1)$ distortion factor with high probability. The most popular sketches in literature are the Gaussian sketch and the subsampled randomized Hadamard transform, while the CountSketch has lower complexity. Combining two sketches, known as multisketching, offers an inexpensive means of quickly reducing the dimension of a matrix by combining a CountSketch and Gaussian sketch.
  However, there has been little investigation into high performance CountSketch implementations. In this work, we develop an efficient GPU implementation of the CountSketch, and demonstrate the performance of multisketching using this technique. We also demonstrate the potential for using this implementation within a multisketched least squares solver that is up to $77\%$ faster than the normal equations with significantly better numerical stability, at the cost of an $O(1)$ multiplicative factor introduced into the relative residual norm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14209v1</guid>
      <category>math.NA</category>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <category>cs.PF</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew J. Higgins, Erik G. Boman, Ichitaro Yamazaki</dc:creator>
    </item>
    <item>
      <title>Power Stabilization for AI Training Datacenters</title>
      <link>https://arxiv.org/abs/2508.14318</link>
      <description>arXiv:2508.14318v1 Announce Type: cross 
Abstract: Large Artificial Intelligence (AI) training workloads spanning several tens of thousands of GPUs present unique power management challenges. These arise due to the high variability in power consumption during the training. Given the synchronous nature of these jobs, during every iteration there is a computation-heavy phase, where each GPU works on the local data, and a communication-heavy phase where all the GPUs synchronize on the data. Because compute-heavy phases require much more power than communication phases, large power swings occur. The amplitude of these power swings is ever increasing with the increase in the size of training jobs. An even bigger challenge arises from the frequency spectrum of these power swings which, if harmonized with critical frequencies of utilities, can cause physical damage to the power grid infrastructure. Therefore, to continue scaling AI training workloads safely, we need to stabilize the power of such workloads. This paper introduces the challenge with production data and explores innovative solutions across the stack: software, GPU hardware, and datacenter infrastructure. We present the pros and cons of each of these approaches and finally present a multi-pronged approach to solving the challenge. The proposed solutions are rigorously tested using a combination of real hardware and Microsoft's in-house cloud power simulator, providing critical insights into the efficacy of these interventions under real-world conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14318v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Esha Choukse, Brijesh Warrier, Scot Heath, Luz Belmont, April Zhao, Hassan Ali Khan, Brian Harry, Matthew Kappel, Russell J. Hewett, Kushal Datta, Yu Pei, Caroline Lichtenberger, John Siegler, David Lukofsky, Zaid Kahn, Gurpreet Sahota, Andy Sullivan, Charles Frederick, Hien Thai, Rebecca Naughton, Daniel Jurnove, Justin Harp, Reid Carper, Nithish Mahalingam, Srini Varkala, Alok Gautam Kumbhare, Satyajit Desai, Venkatesh Ramamurthy, Praneeth Gottumukkala, Girish Bhatia, Kelsey Wildstone, Laurentiu Olariu, Mohammed Ayna, Mike Kendrick, Ricardo Bianchini, Aaron Hurst, Reza Zamani, Xin Li, Gene Oden, Rory Carmichael, Tom Li, Apoorv Gupta, Nilesh Dattani, Lawrence Marwong, Rob Nertney, Jeff Liott, Miro Enev, Divya Ramakrishnan, Ian Buck, Jonah Alben</dc:creator>
    </item>
    <item>
      <title>Lagrangian Simulation Volume-Based Contour Tree Simplification</title>
      <link>https://arxiv.org/abs/2508.14339</link>
      <description>arXiv:2508.14339v1 Announce Type: cross 
Abstract: Many scientific and engineering problems are modelled by simulating scalar fields defined either on space-filling meshes (Eulerian) or as particles (Lagrangian). For analysis and visualization, topological primitives such as contour trees can be used, but these often need simplification to filter out small-scale features. For parcel-based convective cloud simulations, simplification of the contour tree requires a volumetric measure rather than persistence. Unlike for cubic meshes, volume cannot be approximated by counting regular vertices. Typically, this is addressed by resampling irregular data onto a uniform grid. Unfortunately, the spatial proximity of parcels requires a high sampling frequency, resulting in a massive increase in data size for processing. We therefore extend volume-based contour tree simplification to parcel-in-cell simulations with a graph adaptor in Viskores (VTK-m), using Delaunay tetrahedralization of the parcel centroids as input. Instead of relying on a volume approximation by counting regular vertices -- as was done for cubic meshes -- we adapt the 2D area splines reported by Bajaj et al. 10.1145/259081.259279, and Zhou et al. 10.1109/TVCG.2018.2796555. We implement this in Viskores (formerly called VTK-m) as prefix-sum style hypersweeps for parallel efficiency and show how it can be generalized to compute any integrable property. Finally, our results reveal that contour trees computed directly on the parcels are orders of magnitude faster than computing them on a resampled grid, while also arguably offering better quality segmentation, avoiding interpolation artifacts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14339v1</guid>
      <category>cs.CG</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Domantas Dilys, Hamish Carr, Steven Boeing</dc:creator>
    </item>
    <item>
      <title>FedEve: On Bridging the Client Drift and Period Drift for Cross-device Federated Learning</title>
      <link>https://arxiv.org/abs/2508.14539</link>
      <description>arXiv:2508.14539v1 Announce Type: cross 
Abstract: Federated learning (FL) is a machine learning paradigm that allows multiple clients to collaboratively train a shared model without exposing their private data. Data heterogeneity is a fundamental challenge in FL, which can result in poor convergence and performance degradation. Client drift has been recognized as one of the factors contributing to this issue resulting from the multiple local updates in FedAvg. However, in cross-device FL, a different form of drift arises due to the partial client participation, but it has not been studied well. This drift, we referred as period drift, occurs as participating clients at each communication round may exhibit distinct data distribution that deviates from that of all clients. It could be more harmful than client drift since the optimization objective shifts with every round.
  In this paper, we investigate the interaction between period drift and client drift, finding that period drift can have a particularly detrimental effect on cross-device FL as the degree of data heterogeneity increases. To tackle these issues, we propose a predict-observe framework and present an instantiated method, FedEve, where these two types of drift can compensate each other to mitigate their overall impact. We provide theoretical evidence that our approach can reduce the variance of model updates. Extensive experiments demonstrate that our method outperforms alternatives on non-iid data in cross-device settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14539v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tao Shen, Zexi Li, Didi Zhu, Ziyu Zhao, Chao Wu, Fei Wu</dc:creator>
    </item>
    <item>
      <title>Cooperative SGD with Dynamic Mixing Matrices</title>
      <link>https://arxiv.org/abs/2508.14565</link>
      <description>arXiv:2508.14565v1 Announce Type: cross 
Abstract: One of the most common methods to train machine learning algorithms today is the stochastic gradient descent (SGD). In a distributed setting, SGD-based algorithms have been shown to converge theoretically under specific circumstances. A substantial number of works in the distributed SGD setting assume a fixed topology for the edge devices. These papers also assume that the contribution of nodes to the global model is uniform. However, experiments have shown that such assumptions are suboptimal and a non uniform aggregation strategy coupled with a dynamically shifting topology and client selection can significantly improve the performance of such models. This paper details a unified framework that covers several Local-Update SGD-based distributed algorithms with dynamic topologies and provides improved or matching theoretical guarantees on convergence compared to existing work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14565v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soumya Sarkar, Shweta Jain</dc:creator>
    </item>
    <item>
      <title>Federated Distillation on Edge Devices: Efficient Client-Side Filtering for Non-IID Data</title>
      <link>https://arxiv.org/abs/2508.14769</link>
      <description>arXiv:2508.14769v1 Announce Type: cross 
Abstract: Federated distillation has emerged as a promising collaborative machine learning approach, offering enhanced privacy protection and reduced communication compared to traditional federated learning by exchanging model outputs (soft logits) rather than full model parameters. However, existing methods employ complex selective knowledge-sharing strategies that require clients to identify in-distribution proxy data through computationally expensive statistical density ratio estimators. Additionally, server-side filtering of ambiguous knowledge introduces latency to the process. To address these challenges, we propose a robust, resource-efficient EdgeFD method that reduces the complexity of the client-side density ratio estimation and removes the need for server-side filtering. EdgeFD introduces an efficient KMeans-based density ratio estimator for effectively filtering both in-distribution and out-of-distribution proxy data on clients, significantly improving the quality of knowledge sharing. We evaluate EdgeFD across diverse practical scenarios, including strong non-IID, weak non-IID, and IID data distributions on clients, without requiring a pre-trained teacher model on the server for knowledge distillation. Experimental results demonstrate that EdgeFD outperforms state-of-the-art methods, consistently achieving accuracy levels close to IID scenarios even under heterogeneous and challenging conditions. The significantly reduced computational overhead of the KMeans-based estimator is suitable for deployment on resource-constrained edge devices, thereby enhancing the scalability and real-world applicability of federated distillation. The code is available online for reproducibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14769v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahmed Mujtaba, Gleb Radchenko, Radu Prodan, Marc Masana</dc:creator>
    </item>
    <item>
      <title>Action Engine: Automatic Workflow Generation in FaaS</title>
      <link>https://arxiv.org/abs/2411.19485</link>
      <description>arXiv:2411.19485v2 Announce Type: replace 
Abstract: Function as a Service (FaaS) is poised to become the foundation of the next generation of cloud systems due to its inherent advantages in scalability, cost-efficiency, and ease of use. However, challenges such as the need for specialized knowledge, platform dependence, and difficulty in scalability in building functional workflows persist for cloud-native application developers. To overcome these challenges and mitigate the burden of developing FaaS-based applications, in this paper, we propose a mechanism called Action Engine, that makes use of tool-augmented large language models (LLMs) at its kernel to interpret human language queries and automates FaaS workflow generation, thereby, reducing the need for specialized expertise and manual design. Action Engine includes modules to identify relevant functions from the FaaS repository and seamlessly manage the data dependency between them, ensuring the developer's query is processed and resolved. Beyond that, Action Engine can execute the generated workflow by injecting the user-provided arguments. On another front, this work addresses a gap in tool-augmented LLM research via adopting an Automatic FaaS Workflow Generation perspective to systematically evaluate methodologies across four fundamental sub-processes. Through benchmarking various parameters, this research provides critical insights into streamlining workflow automation for real-world applications, specifically in the FaaS continuum. Our evaluations demonstrate that the Action Engine achieves comparable performance to the few-shot learning approach while maintaining platform- and language-agnosticism, thereby, mitigating provider-specific dependencies in workflow generation. We notice that Action Engine can unlock FaaS workflow generation for non-cloud-savvy developers and expedite the development cycles of cloud-native applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19485v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akiharu Esashi, Pawissanutt Lertpongrujikorn, Shinji Kato, Mohsen Amini Salehi</dc:creator>
    </item>
    <item>
      <title>On the $h$-majority dynamics with many opinions</title>
      <link>https://arxiv.org/abs/2506.20218</link>
      <description>arXiv:2506.20218v2 Announce Type: replace 
Abstract: We present the first upper bound on the convergence time to consensus of the well-known $h$-majority dynamics with $k$ opinions, in the synchronous setting, for $h$ and $k$ that are both non-constant values. We suppose that, at the beginning of the process, there is some initial additive bias towards some plurality opinion, that is, there is an opinion that is supported by $x$ nodes while any other opinion is supported by strictly fewer nodes. We prove that, with high probability, if the bias is $\omega(\sqrt{x})$ and the initial plurality opinion is supported by at least $x = \omega(\log n)$ nodes, then the process converges to plurality consensus in $O(\log n)$ rounds whenever $h = \omega(n \log n / x)$. A main corollary is the following: if $k = o(n / \log n)$ and the process starts from an almost-balanced configuration with an initial bias of magnitude $\omega(\sqrt{n/k})$ towards the initial plurality opinion, then any function $h = \omega(k \log n)$ suffices to guarantee convergence to consensus in $O(\log n)$ rounds, with high probability. Our upper bound shows that the lower bound of $\Omega(k / h^2)$ rounds to reach consensus given by Becchetti et al.\ (2017) cannot be pushed further than $\widetilde{\Omega}(k / h)$. Moreover, the bias we require is asymptotically smaller than the $\Omega(\sqrt{n\log n})$ bias that guarantees plurality consensus in the $3$-majority dynamics: in our case, the required bias is at most any (arbitrarily small) function in $\omega(\sqrt{x})$ for any value of $k \ge 2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20218v2</guid>
      <category>cs.DC</category>
      <category>cs.MA</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesco d'Amore, Niccol\`o D'Archivio, George Giakkoupis, Emanuele Natale</dc:creator>
    </item>
    <item>
      <title>PROV-AGENT: Unified Provenance for Tracking AI Agent Interactions in Agentic Workflows</title>
      <link>https://arxiv.org/abs/2508.02866</link>
      <description>arXiv:2508.02866v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) and other foundation models are increasingly used as the core of AI agents. In agentic workflows, these agents plan tasks, interact with humans and peers, and influence scientific outcomes across federated and heterogeneous environments. However, agents can hallucinate or reason incorrectly, propagating errors when one agent's output becomes another's input. Thus, assuring that agents' actions are transparent, traceable, reproducible, and reliable is critical to assess hallucination risks and mitigate their workflow impacts. While provenance techniques have long supported these principles, existing methods fail to capture and relate agent-centric metadata such as prompts, responses, and decisions with the broader workflow context and downstream outcomes. In this paper, we introduce PROV-AGENT, a provenance model that extends W3C PROV and leverages the Model Context Protocol (MCP) and data observability to integrate agent interactions into end-to-end workflow provenance. Our contributions include: (1) a provenance model tailored for agentic workflows, (2) a near real-time, open-source system for capturing agentic provenance, and (3) a cross-facility evaluation spanning edge, cloud, and HPC environments, demonstrating support for critical provenance queries and agent reliability analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02866v3</guid>
      <category>cs.DC</category>
      <category>cs.DB</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renan Souza, Amal Gueroudji, Stephen DeWitt, Daniel Rosendo, Tirthankar Ghosal, Robert Ross, Prasanna Balaprakash, Rafael Ferreira da Silva</dc:creator>
    </item>
    <item>
      <title>Extending DD-$\alpha$AMG on heterogeneous machines</title>
      <link>https://arxiv.org/abs/2407.08092</link>
      <description>arXiv:2407.08092v4 Announce Type: replace-cross 
Abstract: Multigrid solvers are the standard in modern scientific computing simulations. Domain Decomposition Aggregation-Based Algebraic Multigrid, also known as the DD-$\alpha$AMG solver, is a successful realization of an algebraic multigrid solver for lattice quantum chromodynamics. Its CPU implementation has made it possible to construct, for some particular discretizations, simulations otherwise computationally unfeasible, and furthermore it has motivated the development and improvement of other algebraic multigrid solvers in the area. From an existing version of DD-$\alpha$AMG already partially ported via CUDA to run some finest-level operations of the multigrid solver on Nvidia GPUs, we translate the CUDA code here by using HIP to run on the ORISE supercomputer. We moreover extend the smoothers available in DD-$\alpha$AMG, paying particular attention to Richardson smoothing, which in our numerical experiments has led to a multigrid solver faster than smoothing with GCR and only 10% slower compared to SAP smoothing. Then we port the odd-even-preconditioned versions of GMRES and Richardson via CUDA. Finally, we extend some computationally intensive coarse-grid operations via advanced vectorization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08092v4</guid>
      <category>hep-lat</category>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gustavo Ramirez-Hidalgo, Lianhua He, Ke-Long Zhang</dc:creator>
    </item>
    <item>
      <title>Benchmarking Vector, Graph and Hybrid Retrieval Augmented Generation (RAG) Pipelines for Open Radio Access Networks (ORAN)</title>
      <link>https://arxiv.org/abs/2507.03608</link>
      <description>arXiv:2507.03608v2 Announce Type: replace-cross 
Abstract: Generative AI (GenAI) is expected to play a pivotal role in enabling autonomous optimization in future wireless networks. Within the ORAN architecture, Large Language Models (LLMs) can be specialized to generate xApps and rApps by leveraging specifications and API definitions from the RAN Intelligent Controller (RIC) platform. However, fine-tuning base LLMs for telecom-specific tasks remains expensive and resource-intensive. Retrieval-Augmented Generation (RAG) offers a practical alternative through in-context learning, enabling domain adaptation without full retraining. While traditional RAG systems rely on vector-based retrieval, emerging variants such as GraphRAG and Hybrid GraphRAG incorporate knowledge graphs or dual retrieval strategies to support multi-hop reasoning and improve factual grounding. Despite their promise, these methods lack systematic, metric-driven evaluations, particularly in high-stakes domains such as ORAN. In this study, we conduct a comparative evaluation of Vector RAG, GraphRAG, and Hybrid GraphRAG using ORAN specifications. We assess performance across varying question complexities using established generation metrics: faithfulness, answer relevance, context relevance, and factual correctness. Results show that both GraphRAG and Hybrid GraphRAG outperform traditional RAG. Hybrid GraphRAG improves factual correctness by 8%, while GraphRAG improves context relevance by 11%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03608v2</guid>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarat Ahmad, Zeinab Nezami, Maryam Hafeez, Syed Ali Raza Zaidi</dc:creator>
    </item>
  </channel>
</rss>
