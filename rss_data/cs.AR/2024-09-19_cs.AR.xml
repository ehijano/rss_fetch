<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Sep 2024 04:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Development of High-Performance DSP Algorithms on the European Rad-Hard NG-ULTRA SoC FPGA</title>
      <link>https://arxiv.org/abs/2409.12253</link>
      <description>arXiv:2409.12253v1 Announce Type: new 
Abstract: The emergence of demanding space applications has modified the traditional landscape of computing systems in space. When reliability is a first-class concern, in addition to enhanced performance-per-Watt, radiation-hardened FPGAs are favored. In this context, the current paper evaluates the first European radiation-hardened SoC FPGA, i.e., NanoXplore's NG-ULTRA, for accelerating high-performance DSP algorithms from space applications. The proposed development &amp; testing methodologies provide efficient implementations, while they also aim to test the new NG-ULTRA hardware and its associated software tools. The results show that NG-ULTRA achieves competitive resource utilization and performance, constituting it as a very promising device for space missions, especially for Europe.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12253v1</guid>
      <category>cs.AR</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vasileios Leon, Anastasios Xynos, Dimitrios Soudris, George Lentaris, Ruben Domingo, Arturo Perez, David Gonzalez-Arjona, Isabelle Conway, David Merodio Codinachs</dc:creator>
    </item>
    <item>
      <title>MPAI: A Co-Processing Architecture with MPSoC &amp; AI Accelerators for Vision Applications in Space</title>
      <link>https://arxiv.org/abs/2409.12258</link>
      <description>arXiv:2409.12258v1 Announce Type: new 
Abstract: The emerging need for fast and power-efficient AI/ML deployment on-board spacecraft has forced the space industry to examine specialized accelerators, which have been successfully used in terrestrial applications. Towards this direction, the current work introduces a very heterogeneous co-processing architecture that is built around UltraScale+ MPSoC and its programmable DPU, as well as commercial AI/ML accelerators such as MyriadX VPU and Edge TPU. The proposed architecture, called MPAI, handles networks of different size/complexity and accommodates speed-accuracy-energy trade-offs by exploiting the diversity of accelerators in precision and computational power. This brief provides technical background and reports preliminary experimental results and outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12258v1</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vasileios Leon, Panagiotis Minaidis, Dimitrios Soudris, George Lentaris</dc:creator>
    </item>
    <item>
      <title>A High-Throughput Hardware Accelerator for Lempel-Ziv 4 Compression Algorithm</title>
      <link>https://arxiv.org/abs/2409.12433</link>
      <description>arXiv:2409.12433v1 Announce Type: new 
Abstract: This paper delves into recent hardware implementations of the Lempel-Ziv 4 (LZ4) algorithm, highlighting two key factors that limit the throughput of single-kernel compressors. Firstly, the actual parallelism exhibited in single-kernel designs falls short of the theoretical potential. Secondly, the clock frequency is constrained due to the presence of the feedback loops. To tackle these challenges, we propose a novel scheme that restricts each parallelization window to a single match, thus elevating the level of actual parallelism. Furthermore, by restricting the maximum match length, we eliminate the feedback loops within the architecture, enabling a significant boost in throughput. Finally, we present a high-speed hardware architecture. The implementation results demonstrate that the proposed architecture achieves a throughput of up to 16.10 Gb/s, exhibiting a 2.648x improvement over the start-of-the-art. The new design only results in an acceptable compression ratio reduction ranging from 4.93% to 11.68% with various numbers of hash table entries, compared to the LZ4 compression ratio achieved by official software implementations disclosed on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12433v1</guid>
      <category>cs.AR</category>
      <category>eess.SP</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tao Chen, Suwen Song, Zhongfeng Wang</dc:creator>
    </item>
    <item>
      <title>Accelerating AI and Computer Vision for Satellite Pose Estimation on the Intel Myriad X Embedded SoC</title>
      <link>https://arxiv.org/abs/2409.12939</link>
      <description>arXiv:2409.12939v1 Announce Type: new 
Abstract: The challenging deployment of Artificial Intelligence (AI) and Computer Vision (CV) algorithms at the edge pushes the community of embedded computing to examine heterogeneous System-on-Chips (SoCs). Such novel computing platforms provide increased diversity in interfaces, processors and storage, however, the efficient partitioning and mapping of AI/CV workloads still remains an open issue. In this context, the current paper develops a hybrid AI/CV system on Intel's Movidius Myriad X, which is an heterogeneous Vision Processing Unit (VPU), for initializing and tracking the satellite's pose in space missions. The space industry is among the communities examining alternative computing platforms to comply with the tight constraints of on-board data processing, while it is also striving to adopt functionalities from the AI domain. At algorithmic level, we rely on the ResNet-50-based UrsoNet network along with a custom classical CV pipeline. For efficient acceleration, we exploit the SoC's neural compute engine and 16 vector processors by combining multiple parallelization and low-level optimization techniques. The proposed single-chip, robust-estimation, and real-time solution delivers a throughput of up to 5 FPS for 1-MegaPixel RGB images within a limited power envelope of 2W.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12939v1</guid>
      <category>cs.AR</category>
      <category>cs.CV</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.micpro.2023.104947</arxiv:DOI>
      <arxiv:journal_reference>Elsevier Microprocessors and Microsystems, Vol. 103, Nov. 2023</arxiv:journal_reference>
      <dc:creator>Vasileios Leon, Panagiotis Minaidis, George Lentaris, Dimitrios Soudris</dc:creator>
    </item>
    <item>
      <title>Allspark: Workload Orchestration for Visual Transformers on Processing In-Memory Systems</title>
      <link>https://arxiv.org/abs/2403.15069</link>
      <description>arXiv:2403.15069v2 Announce Type: replace 
Abstract: The advent of Transformers has revolutionized computer vision, offering a powerful alternative to convolutional neural networks (CNNs), especially with the local attention mechanism that excels at capturing local structures within the input and achieve state-of-the-art performance. Processing in-memory (PIM) architecture offers extensive parallelism, low data movement costs, and scalable memory bandwidth, making it a promising solution to accelerate Transformer with memory-intensive operations. However, the crucial issue lies in efficiently deploying an entire model onto resource-limited PIM system while parallelizing each transformer block with potentially many computational branches based on local-attention mechanisms.
  We present Allspark, which focuses on workload orchestration for visual Transformers on PIM systems, aiming at minimizing inference latency. Firstly, to fully utilize the massive parallelism of PIM, Allspark employs a fine-grained partitioning scheme for computational branches, and formats a systematic layout and interleaved dataflow with maximized data locality and reduced data movement. Secondly, Allspark formulates the scheduling of the complete model on a resource-limited distributed PIM system as an integer linear programming (ILP) problem. Thirdly, as local-global data interactions exhibit complex yet regular dependencies, Allspark provides a two-stage placement method, which simplifies the challenging placement of computational branches on the PIM system into the structured layout and greedy-based binding, to minimize NoC communication costs. Extensive experiments on 3D-stacked DRAM-based PIM systems show that Allspark brings 1.2x-24.0x inference speedup for various visual Transformers over baselines. Compared to Nvidia V100 GPU, Allspark-enriched PIM system yields average speedups of 2.3x and energy savings of 20x-55x.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15069v2</guid>
      <category>cs.AR</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengke Ge, Junpeng Wang, Binhan Chen, Yingjian Zhong, Haitao Du, Song Chen, Yi Kang</dc:creator>
    </item>
  </channel>
</rss>
