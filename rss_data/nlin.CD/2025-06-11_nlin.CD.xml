<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>nlin.CD updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/nlin.CD</link>
    <description>nlin.CD updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/nlin.CD" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Jun 2025 01:38:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Rogue waves collision under incident momentum modulation in two-component Bose-Einstein condensates</title>
      <link>https://arxiv.org/abs/2506.08420</link>
      <description>arXiv:2506.08420v1 Announce Type: cross 
Abstract: The collision dynamics of two first-order rogue waves (RWs) with opposite incident momentum in two-component Bose-Einstein condensates (BECs) is studied by solving the two-component one-dimensional Gross-Pitaevskii (GP) equation. It is demonstrated that the introduction of appropriate incident momentum successfully promotes the generation of second-order RWs in the case of relatively weaker interspecies interactions compared to intraspecific interactions. The range of incident momentum that can facilitate the generation of second-order RWs under different interspecies interaction strengths is determined, and machine learning is employed to find and analyze relationships among the interspecies interaction, the incident momentum, and the offset that can lead to the generation of second-order RWs. It shows that any two parameters above exhibit a positive or negative correlation when the third parameter is fixed. These findings provide additional possibilities for generating and controlling high-order RWs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08420v1</guid>
      <category>cond-mat.quant-gas</category>
      <category>nlin.CD</category>
      <category>quant-ph</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhihao Zhang, Tiantian Li, Xiao-Dong Bai, Yunbo Zhang, Denglong Wang</dc:creator>
    </item>
    <item>
      <title>Leveraging chaos in the training of artificial neural networks</title>
      <link>https://arxiv.org/abs/2506.08523</link>
      <description>arXiv:2506.08523v1 Announce Type: cross 
Abstract: Traditional algorithms to optimize artificial neural networks when confronted with a supervised learning task are usually exploitation-type relaxational dynamics such as gradient descent (GD). Here, we explore the dynamics of the neural network trajectory along training for unconventionally large learning rates. We show that for a region of values of the learning rate, the GD optimization shifts away from purely exploitation-like algorithm into a regime of exploration-exploitation balance, as the neural network is still capable of learning but the trajectory shows sensitive dependence on initial conditions -- as characterized by positive network maximum Lyapunov exponent --. Interestingly, the characteristic training time required to reach an acceptable accuracy in the test set reaches a minimum precisely in such learning rate region, further suggesting that one can accelerate the training of artificial neural networks by locating at the onset of chaos. Our results -- initially illustrated for the MNIST classification task -- qualitatively hold for a range of supervised learning tasks, learning architectures and other hyperparameters, and showcase the emergent, constructive role of transient chaotic dynamics in the training of artificial neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08523v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>nlin.CD</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pedro Jim\'enez-Gonz\'alez, Miguel C. Soriano, Lucas Lacasa</dc:creator>
    </item>
    <item>
      <title>Eigenstate Thermalization Hypothesis and Random Matrix Theory Universality in Few-Body Systems</title>
      <link>https://arxiv.org/abs/2506.09011</link>
      <description>arXiv:2506.09011v1 Announce Type: cross 
Abstract: In this paper, we study the Feingold-Peres model as an example, which is a well-known paradigm of quantum chaos. Using semiclassical analysis and numerical simulations, we study the statistical properties of observables in few-body systems with chaotic classical limits and the emergence of random matrix theory universality. More specifically, we focus on: 1) the applicability of the eigenstate thermalization hypothesis in few-body systems and the dependence of its form on the effective Planck constant and 2) the existence of a universal random matrix theory description of observables when truncated to a small microcanonical energy window. Our results provide new insights into the established field of few-body quantum chaos and help bridge it to modern perspectives, such as the general eigenstate thermalization hypothesis (ETH).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09011v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>nlin.CD</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaozi Wang, Hua Yan, Robin Steinigeweg, Jochen Gemmer</dc:creator>
    </item>
  </channel>
</rss>
