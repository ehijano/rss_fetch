<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 Oct 2024 03:31:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Digesting Gibbs Sampling Using R</title>
      <link>https://arxiv.org/abs/2410.14073</link>
      <description>arXiv:2410.14073v1 Announce Type: new 
Abstract: This work aims to provide an environment for all users who are beginner in the context of the statistical simulation approaches. These techniques are known as the Monte Carlo methods as a whole nowadays. Indeed, the Monte Carlo, as a statistical simulation technique, itself involves the Markov chain Monte Carlo that attracts the attention of researchers from a wide variety of study fields. One may see the Markov chain Monte Carlo as statistical simulation approaches that work based on the iterative algorithms and so the others that are not based on iterative algorithm are the Monte Carlo approaches. We would recommend the reader(s) to learn the elementary undergraduate courses in calculus, probability, and statistics before studying or applying this report for practical purposes. The required topics may include, but not limited to, concept of mathematical function, limit, derivative, partial derivative, simple integrals, probability axioms, discrete and continuous random variables, probability distributions, concept of central tendency and variance, multivariate probability distributions, functions of random variables, and the central limit theorem (CLT).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14073v1</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahdi Teimouri</dc:creator>
    </item>
    <item>
      <title>Tensor Decomposition with Unaligned Observations</title>
      <link>https://arxiv.org/abs/2410.14046</link>
      <description>arXiv:2410.14046v1 Announce Type: cross 
Abstract: This paper presents a canonical polyadic (CP) tensor decomposition that addresses unaligned observations. The mode with unaligned observations is represented using functions in a reproducing kernel Hilbert space (RKHS). We introduce a versatile loss function that effectively accounts for various types of data, including binary, integer-valued, and positive-valued types. Additionally, we propose an optimization algorithm for computing tensor decompositions with unaligned observations, along with a stochastic gradient method to enhance computational efficiency. A sketching algorithm is also introduced to further improve efficiency when using the $\ell_2$ loss function. To demonstrate the efficacy of our methods, we provide illustrative examples using both synthetic data and an early childhood human microbiome dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14046v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Runshi Tang, Tamara Kolda, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>WeSpeR: Population spectrum retrieval and spectral density estimation of weighted sample covariance</title>
      <link>https://arxiv.org/abs/2410.14413</link>
      <description>arXiv:2410.14413v1 Announce Type: cross 
Abstract: The spectrum of the weighted sample covariance shows a asymptotic non random behavior when the dimension grows with the number of samples. In this setting, we prove that the asymptotic spectral distribution $F$ of the weighted sample covariance has a continuous density on $\mathbb{R}^*$. We address then the practical problem of numerically finding this density. We propose a procedure to compute it, to determine the support of $F$ and define an efficient grid on it. We use this procedure to design the $\textit{WeSpeR}$ algorithm, which estimates the spectral density and retrieves the true spectral covariance spectrum. Empirical tests confirm the good properties of the $\textit{WeSpeR}$ algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14413v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Benoit Oriol</dc:creator>
    </item>
    <item>
      <title>On the convergence of dynamic implementations of Hamiltonian Monte Carlo and No U-Turn Samplers</title>
      <link>https://arxiv.org/abs/2307.03460</link>
      <description>arXiv:2307.03460v2 Announce Type: replace 
Abstract: There is substantial empirical evidence about the success of dynamic implementations of Hamiltonian Monte Carlo (HMC), such as the No U-Turn Sampler (NUTS), in many challenging inference problems but theoretical results about their behavior are scarce. The aim of this paper is to fill this gap. More precisely, we consider a general class of MCMC algorithms we call dynamic HMC. We show that this general framework encompasses NUTS as a particular case, implying the invariance of the target distribution as a by-product. Second, we establish conditions under which NUTS is irreducible and aperiodic and as a corrolary ergodic. Under conditions similar to the ones existing for HMC, we also show that NUTS is geometrically ergodic. Finally, we improve existing convergence results for HMC showing that this method is ergodic without any boundedness condition on the stepsize and the number of leapfrog steps, in the case where the target is a perturbation of a Gaussian distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.03460v2</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alain Durmus, Samuel Gruffaz, Miika Kailas, Eero Saksman, Matti Vihola</dc:creator>
    </item>
    <item>
      <title>Proximal Interacting Particle Langevin Algorithms</title>
      <link>https://arxiv.org/abs/2406.14292</link>
      <description>arXiv:2406.14292v2 Announce Type: replace 
Abstract: We introduce a class of algorithms, termed Proximal Interacting Particle Langevin Algorithms (PIPLA), for inference and learning in latent variable models whose joint probability density is non-differentiable. Leveraging proximal Markov chain Monte Carlo (MCMC) techniques and the recently introduced interacting particle Langevin algorithm (IPLA), we propose several variants within the novel proximal IPLA family, tailored to the problem of estimating parameters in a non-differentiable statistical model. We prove nonasymptotic bounds for the parameter estimates produced by multiple algorithms in the strongly log-concave setting and provide comprehensive numerical experiments on various models to demonstrate the effectiveness of the proposed methods. In particular, we demonstrate the utility of the proposed family of algorithms on a toy hierarchical example where our assumptions can be checked, as well as on the problems of sparse Bayesian logistic regression, sparse Bayesian neural network, and sparse matrix completion. Our theory and experiments together show that PIPLA family can be the de facto choice for parameter estimation problems in latent variable models for non-differentiable models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14292v2</guid>
      <category>stat.CO</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paula Cordero Encinar, Francesca R. Crucinio, O. Deniz Akyildiz</dc:creator>
    </item>
    <item>
      <title>Gaussian Approximation and Output Analysis for High-Dimensional MCMC</title>
      <link>https://arxiv.org/abs/2407.05492</link>
      <description>arXiv:2407.05492v2 Announce Type: replace 
Abstract: The widespread use of Markov Chain Monte Carlo (MCMC) methods for high-dimensional applications has motivated research into the scalability of these algorithms with respect to the dimension of the problem. Despite this, numerous problems concerning output analysis in high-dimensional settings have remained unaddressed. We present novel quantitative Gaussian approximation results for a broad range of MCMC algorithms. Notably, we analyse the dependency of the obtained approximation errors on the dimension of both the target distribution and the feature space. We demonstrate how these Gaussian approximations can be applied in output analysis. This includes determining the simulation effort required to guarantee Markov chain central limit theorems and consistent estimation of the variance and effective sample size in high-dimensional settings. We give quantitative convergence bounds for termination criteria and show that the termination time of a wide class of MCMC algorithms scales polynomially in dimension while ensuring a desired level of precision. Our results offer guidance to practitioners for obtaining appropriate standard errors and deciding the minimum simulation effort of MCMC algorithms in both multivariate and high-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05492v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ardjen Pengel, Jun Yang, Zhou Zhou</dc:creator>
    </item>
    <item>
      <title>A Data-Adaptive Prior for Bayesian Learning of Kernels in Operators</title>
      <link>https://arxiv.org/abs/2212.14163</link>
      <description>arXiv:2212.14163v2 Announce Type: replace-cross 
Abstract: Kernels are efficient in representing nonlocal dependence and they are widely used to design operators between function spaces. Thus, learning kernels in operators from data is an inverse problem of general interest. Due to the nonlocal dependence, the inverse problem can be severely ill-posed with a data-dependent singular inversion operator. The Bayesian approach overcomes the ill-posedness through a non-degenerate prior. However, a fixed non-degenerate prior leads to a divergent posterior mean when the observation noise becomes small, if the data induces a perturbation in the eigenspace of zero eigenvalues of the inversion operator. We introduce a data-adaptive prior to achieve a stable posterior whose mean always has a small noise limit. The data-adaptive prior's covariance is the inversion operator with a hyper-parameter selected adaptive to data by the L-curve method. Furthermore, we provide a detailed analysis on the computational practice of the data-adaptive prior, and demonstrate it on Toeplitz matrices and integral operators. Numerical tests show that a fixed prior can lead to a divergent posterior mean in the presence of any of the four types of errors: discretization error, model error, partial observation and wrong noise assumption. In contrast, the data-adaptive prior always attains posterior means with small noise limits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.14163v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Neil K. Chada, Quanjun Lang, Fei Lu, Xiong Wang</dc:creator>
    </item>
    <item>
      <title>Models for spatiotemporal data with some missing locations and application to emergency calls models calibration</title>
      <link>https://arxiv.org/abs/2410.11103</link>
      <description>arXiv:2410.11103v2 Announce Type: replace-cross 
Abstract: We consider two classes of models for spatiotemporal data: one without covariates and one with covariates. If $\mathcal{T}$ is a partition of time and $\mathcal{I}$ a partition of the studied area into zones and if $\mathcal{C}$ is the set of arrival types, we assume that the process of arrivals for time interval $t \in \mathcal{T}$, zone $i \in \mathcal{I}$, and arrival type $c \in \mathcal{C}$ is Poisson with some intensity $\lambda_{c,i,t}$. We discussed the calibration and implementation of such models in \cite{laspatedpaper, laspatedmanual} with corresponding software LASPATED (Library for the Analysis of SPAtioTEmporal Discrete data) available on GitHub at https://github.com/vguigues/LASPATED. In this paper, we discuss the extension of these models when some of the locations are missing in the historical data. We propose three models to deal with missing locations and implemented them both in Matlab and C++. The corresponding code is available on GitHub as an extension of LASPATED at https://github.com/vguigues/LASPATED/Missing_Data. We tested our implementation using the process of emergency calls to an Emergency Health Service where many calls come with missing locations and show the importance and benefit of using models that consider missing locations, rather than discarding the calls with missing locations for the calibration of statistical models for such calls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11103v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Guigues, Anton Kleywegt, Victor Hugo Nascimento, Lucas Lucas Rafael de Andrade</dc:creator>
    </item>
  </channel>
</rss>
