<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 27 Oct 2025 04:00:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Geometric Analysis of PCA</title>
      <link>https://arxiv.org/abs/2510.20978</link>
      <description>arXiv:2510.20978v1 Announce Type: new 
Abstract: What property of the data distribution determines the excess risk of principal component analysis? In this paper, we provide a precise answer to this question. We establish a central limit theorem for the error of the principal subspace estimated by PCA, and derive the asymptotic distribution of its excess risk under the reconstruction loss. We obtain a non-asymptotic upper bound on the excess risk of PCA that recovers, in the large sample limit, our asymptotic characterization. Underlying our contributions is the following result: we prove that the negative block Rayleigh quotient, defined on the Grassmannian, is generalized self-concordant along geodesics emanating from its minimizer of maximum rotation less than $\pi/4$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20978v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ayoub El Hanchi, Murat Erdogdu, Chris Maddison</dc:creator>
    </item>
    <item>
      <title>Limiting Spectral Distribution of High-dimensional Multivariate Kendall-$\tau$</title>
      <link>https://arxiv.org/abs/2510.21077</link>
      <description>arXiv:2510.21077v1 Announce Type: new 
Abstract: The multivariate Kendall-$\tau$ statistic, denoted by $K_n$, plays a significant role in robust statistical analysis. This paper establishes the limiting properties of the empirical spectral distribution (ESD) of $K_n$. We demonstrate that the ESD of $\frac{1}{2}pK_n$ converges almost surely to the Mar\v{c}enko--Pastur law with variance parameter $\frac{1}{2}$, analogous to the classical result for sample covariance matrices.
  Using Stieltjes transform techniques, we extend these results to the independent component model, deriving a fixed-point equation that characterizes the limiting spectral distribution of $\frac{1}{2}tr\Sigma K_n$. The theoretical findings are validated through comprehensive simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21077v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruoyu Wu</dc:creator>
    </item>
    <item>
      <title>Kriging measure-valued data with sparse observations: application to nuclear safety studies</title>
      <link>https://arxiv.org/abs/2510.21277</link>
      <description>arXiv:2510.21277v1 Announce Type: new 
Abstract: This work addresses the interpolation of probability measures within a spatial statistics framework. We develop a Kriging approach in the Wasserstein space, leveraging the quantile function representation of the one-dimensional Wasserstein distance. To mitigate the inaccuracies in semivariogram estimation that arise from sparse datasets, we combine this formulation with cross-validation techniques. In particular, we introduce a variant of the virtual cross-validation formulas tailored to quantile functions. The effectiveness of the proposed method is demonstrated on a controlled toy problem as well as on a real-world application from nuclear safety.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21277v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Gossard (IMT), Fran\c{c}ois Bachoc (LPP), Jean Baccou (ASNR), Thibaut Le Gouic (I2M), Jacques Liandrat (I2M), Tony Glantz (ASNR)</dc:creator>
    </item>
    <item>
      <title>Conditional Forecasts and Proper Scoring Rules for Reliable and Accurate Performative Predictions</title>
      <link>https://arxiv.org/abs/2510.21335</link>
      <description>arXiv:2510.21335v1 Announce Type: new 
Abstract: Performative predictions are forecasts which influence the outcomes they aim to predict, undermining the existence of correct forecasts and standard methods of elicitation and estimation. We show that conditioning forecasts on covariates that separate them from the outcome renders the target distribution forecast-invariant, guaranteeing well-posedness of the forecasting problem. However, even under this condition, classical proper scoring rules fail to elicit correct forecasts. We prove a general impossibility result and identify two solutions: (i) in decision-theoretic settings, elicitation of correct and incentive-compatible forecasts is possible if forecasts are separating; (ii) scoring with unbiased estimates of the divergence between the forecast and the induced distribution of the target variable yields correct forecasts. Applying these insights to parameter estimation, conditional forecasts and proper scoring rules enable performatively stable estimation of performatively correct parameters, resolving the issues raised by Perdomo et al. (2020). Our results expose fundamental limits of classical forecast evaluation and offer new tools for reliable and accurate forecasting in performative settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21335v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philip Boeken, Onno Zoeter, Joris M. Mooij</dc:creator>
    </item>
    <item>
      <title>Sparse estimation for the drift of high-dimensional Ornstein--Uhlenbeck processes with i.i.d. paths</title>
      <link>https://arxiv.org/abs/2510.21505</link>
      <description>arXiv:2510.21505v1 Announce Type: new 
Abstract: We study sparsity-regularized maximum likelihood estimation for the drift parameter of high-dimensional non-stationary Ornstein--Uhlenbeck processes given repeated measurements of i.i.d. paths. In particular, we show that Lasso and Slope estimators can achieve the minimax optimal rate of convergence. We exhibit numerical experiments for sparse estimation methods and show their performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21505v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shogo Nakakita</dc:creator>
    </item>
    <item>
      <title>Autocorrelation Test under Frequent Mean Shifts</title>
      <link>https://arxiv.org/abs/2510.21047</link>
      <description>arXiv:2510.21047v1 Announce Type: cross 
Abstract: Testing for the presence of autocorrelation is a fundamental problem in time series analysis. Classical methods such as the Box-Pierce test rely on the assumption of stationarity, necessitating the removal of non-stationary components such as trends or shifts in the mean prior to application. However, this is not always practical, particularly when the mean structure is complex, such as being piecewise constant with frequent shifts. In this work, we propose a new inferential framework for autocorrelation in time series data under frequent mean shifts. In particular, we introduce a Shift-Immune Portmanteau (SIP) test that reliably tests for autocorrelation and is robust against mean shifts. We illustrate an application of our method to nanopore sequencing data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21047v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyang Liu, Ning Hao, Yue Selena Niu, Han Xiao, Hongxu Ding</dc:creator>
    </item>
    <item>
      <title>Instance-Adaptive Hypothesis Tests with Heterogeneous Agents</title>
      <link>https://arxiv.org/abs/2510.21178</link>
      <description>arXiv:2510.21178v1 Announce Type: cross 
Abstract: We study hypothesis testing over a heterogeneous population of strategic agents with private information. Any single test applied uniformly across the population yields statistical error that is sub-optimal relative to the performance of an oracle given access to the private information. We show how it is possible to design menus of statistical contracts that pair type-optimal tests with payoff structures, inducing agents to self-select according to their private information. This separating menu elicits agent types and enables the principal to match the oracle performance even without a priori knowledge of the agent type. Our main result fully characterizes the collection of all separating menus that are instance-adaptive, matching oracle performance for an arbitrary population of heterogeneous agents. We identify designs where information elicitation is essentially costless, requiring negligible additional expense relative to a single-test benchmark, while improving statistical performance. Our work establishes a connection between proper scoring rules and menu design, showing how the structure of the hypothesis test constrains the elicitable information. Numerical examples illustrate the geometry of separating menus and the improvements they deliver in error trade-offs. Overall, our results connect statistical decision theory with mechanism design, demonstrating how heterogeneity and strategic participation can be harnessed to improve efficiency in hypothesis testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21178v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Flora C. Shi, Martin J. Wainwright, Stephen Bates</dc:creator>
    </item>
    <item>
      <title>On the flow matching interpretability</title>
      <link>https://arxiv.org/abs/2510.21210</link>
      <description>arXiv:2510.21210v1 Announce Type: cross 
Abstract: Generative models based on flow matching have demonstrated remarkable success in various domains, yet they suffer from a fundamental limitation: the lack of interpretability in their intermediate generation steps. In fact these models learn to transform noise into data through a series of vector field updates, however the meaning of each step remains opaque. We address this problem by proposing a general framework constraining each flow step to be sampled from a known physical distribution. Flow trajectories are mapped to (and constrained to traverse) the equilibrium states of the simulated physical process. We implement this approach through the 2D Ising model in such a way that flow steps become thermal equilibrium points along a parametric cooling schedule.
  Our proposed architecture includes an encoder that maps discrete Ising configurations into a continuous latent space, a flow-matching network that performs temperature-driven diffusion, and a projector that returns to discrete Ising states while preserving physical constraints.
  We validate this framework across multiple lattice sizes, showing that it preserves physical fidelity while outperforming Monte Carlo generation in speed as the lattice size increases. In contrast with standard flow matching, each vector field represents a meaningful stepwise transition in the 2D Ising model's latent space. This demonstrates that embedding physical semantics into generative flows transforms opaque neural trajectories into interpretable physical processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21210v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>physics.app-ph</category>
      <category>physics.comp-ph</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesco Pivi, Simone Gazza, Davide Evangelista, Roberto Amadini, Maurizio Gabbrielli</dc:creator>
    </item>
    <item>
      <title>Saddle Point Approximation and Central Limit Theorem for Densities in high dimensions</title>
      <link>https://arxiv.org/abs/2510.21545</link>
      <description>arXiv:2510.21545v1 Announce Type: cross 
Abstract: We study the saddlepoint approximation (SPA) for sums of $n$ i.i.d. random vectors $X_i\in\mathbb R^d$ in growing dimensions. SPA provides highly accurate approximations to probability densities and distribution functions via the moment generating function. Recent work by Tang and Reid extended SPA to cases where the dimension $d$ increases with $n$, obtaining an error rate of order $O(d^3/n)$. We refine this analysis and improve the SPA error rate to $O(d^2/n)$. We obtain a non-asymptotic bound for the multiplicative SPA error. As a corollary, we establish the first local central limit theorem for densities in growing dimensions, under the condition $d^2/n \to 0$, and provide explicit multiplicative error bounds. An example involving Gaussian mixtures illustrates our results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21545v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Katsevich</dc:creator>
    </item>
    <item>
      <title>On Uncertainty Calibration for Equivariant Functions</title>
      <link>https://arxiv.org/abs/2510.21691</link>
      <description>arXiv:2510.21691v1 Announce Type: cross 
Abstract: Data-sparse settings such as robotic manipulation, molecular physics, and galaxy morphology classification are some of the hardest domains for deep learning. For these problems, equivariant networks can help improve modeling across undersampled parts of the input space, and uncertainty estimation can guard against overconfidence. However, until now, the relationships between equivariance and model confidence, and more generally equivariance and model calibration, has yet to be studied. Since traditional classification and regression error terms show up in the definitions of calibration error, it is natural to suspect that previous work can be used to help understand the relationship between equivariance and calibration error. In this work, we present a theory relating equivariance to uncertainty estimation. By proving lower and upper bounds on uncertainty calibration errors (ECE and ENCE) under various equivariance conditions, we elucidate the generalization limits of equivariant models and illustrate how symmetry mismatch can result in miscalibration in both classification and regression. We complement our theoretical framework with numerical experiments that clarify the relationship between equivariance and uncertainty using a variety of real and simulated datasets, and we comment on trends with symmetry mismatch, group size, and aleatoric and epistemic uncertainties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21691v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edward Berman, Jacob Ginesin, Marco Pacini, Robin Walters</dc:creator>
    </item>
    <item>
      <title>Nonparametric estimation of a factorizable density using diffusion models</title>
      <link>https://arxiv.org/abs/2501.01783</link>
      <description>arXiv:2501.01783v2 Announce Type: replace 
Abstract: In recent years, diffusion models, and more generally score-based deep generative models, have achieved remarkable success in various applications, including image and audio generation. In this paper, we view diffusion models as an implicit approach to nonparametric density estimation and study them within a statistical framework to analyze their surprising performance. A key challenge in high-dimensional statistical inference is leveraging low-dimensional structures inherent in the data to mitigate the curse of dimensionality. We assume that the underlying density exhibits a low-dimensional structure by factorizing into low-dimensional components, a property common in examples such as Bayesian networks and Markov random fields. Under suitable assumptions, we demonstrate that an implicit density estimator constructed from diffusion models adapts to the factorization structure and achieves the minimax optimal rate with respect to the total variation distance. In constructing the estimator, we design a sparse weight-sharing neural network architecture, where sparsity and weight-sharing are key features of practical architectures such as convolutional neural networks and recurrent neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01783v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyeok Kyu Kwon, Dongha Kim, Ilsang Ohn, Minwoo Chae</dc:creator>
    </item>
    <item>
      <title>Robustified Gaussian quasi-likelihood inference for volatility</title>
      <link>https://arxiv.org/abs/2510.02666</link>
      <description>arXiv:2510.02666v2 Announce Type: replace 
Abstract: We consider statistical inference for a class of continuous regression models contaminated by finite-activity jumps and spike noises. We propose an $M$-estimator through some easy-to-implement one-parameter robustifications of the conventional Gaussian quasi-likelihood function, and prove its asymptotic mixed normality at the standard rate $\sqrt{n}$. It is theoretically shown that the estimator is simultaneously robust against the contaminations in both the covariate process and the objective process. Additionally, we prove that, under suitable design conditions on the tuning parameter, the proposed estimators can enjoy the same asymptotic distribution as in the case of no contamination. Some illustrative simulation results are presented, highlighting the estimator's insensitivity to fine-tuning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02666v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shoichi Eguchi, Hiroki Masuda</dc:creator>
    </item>
    <item>
      <title>Minimum Hellinger Distance Estimators for Complex Survey Designs</title>
      <link>https://arxiv.org/abs/2510.14055</link>
      <description>arXiv:2510.14055v2 Announce Type: replace 
Abstract: Reliable inference from complex survey samples can be derailed by outliers and high-leverage observations induced by unequal inclusion probabilities and calibration. We develop a minimum Hellinger distance estimator (MHDE) for parametric superpopulation models under complex designs, including Poisson PPS and fixed-size SRS/PPS without replacement, with possibly stochastic post-stratified or calibrated weights. Using a Horvitz-Thompson-adjusted kernel density plug-in, we show: (i) $L^1$-consistency of the KDE with explicit large-deviation tail bounds driven by a variance-adaptive effective sample size; (ii) uniform exponential bounds for the Hellinger affinity that yield MHDE consistency under mild identifiability; (iii) an asymptotic Normal distribution for the MHDE with covariance $\mathbf A^{-1}\boldsymbol\Sigma \mathbf A^{\intercal}$ (and a finite-population correction under without-replacement designs); and (iv) robustness via the influence function and $\alpha$-influence curves in the Hellinger topology. Simulations under Gamma and lognormal superpopulation models quantify efficiency-robustness trade-offs relative to weighted MLE under independent and high-leverage contamination. An application to NHANES 2021-2023 total water consumption shows that the MHDE remains stable despite extreme responses that markedly bias the MLE. The estimator is simple to implement via quadrature over a fixed grid and is extensible to other divergence families.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14055v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>David Kepplinger, Anand N. Vidyashankar</dc:creator>
    </item>
    <item>
      <title>Incomplete U-Statistics of Equireplicate Designs: Berry-Esseen Bound and Efficient Construction</title>
      <link>https://arxiv.org/abs/2510.20755</link>
      <description>arXiv:2510.20755v2 Announce Type: replace 
Abstract: U-statistics are a fundamental class of estimators that generalize the sample mean and underpin much of nonparametric statistics. Although extensively studied in both statistics and probability, key challenges remain: their high computational cost - addressed partly through incomplete U-statistics - and their non-standard asymptotic behavior in the degenerate case, which typically requires resampling methods for hypothesis testing. This paper presents a novel perspective on U-statistics, grounded in hypergraph theory and combinatorial designs. Our approach bypasses the traditional Hoeffding decomposition, the main analytical tool in this literature but one highly sensitive to degeneracy. By characterizing the dependence structure of a U-statistic, we derive a Berry-Esseen bound valid for incomplete U-statistics of deterministic designs, yielding conditions under which Gaussian limiting distributions can be established even in degenerate cases and when the order diverges. We also introduce efficient algorithms to construct incomplete U-statistics of equireplicate designs, a subclass of deterministic designs that, in certain cases, achieve minimum variance. Finally, we apply our framework to kernel-based tests that use Maximum Mean Discrepancy (MMD) and Hilbert-Schmidt Independence Criterion. In a real data example with the CIFAR-10 dataset, our permutation-free MMD test delivers substantial computational gains while retaining power and type I error control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20755v2</guid>
      <category>math.ST</category>
      <category>math.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cesare Miglioli, Jordan Awan</dc:creator>
    </item>
    <item>
      <title>Regret Distribution in Stochastic Bandits: Optimal Trade-off between Expectation and Tail Risk</title>
      <link>https://arxiv.org/abs/2304.04341</link>
      <description>arXiv:2304.04341v2 Announce Type: replace-cross 
Abstract: We study the optimal trade-off between expectation and tail risk for regret distribution in the stochastic multi-armed bandit model. We fully characterize the interplay among three desired properties for policy design: worst-case optimality, instance-dependent consistency, and light-tailed risk. New policies are proposed to characterize the optimal regret tail probability for any regret threshold. In particular, we discover an intrinsic gap of the optimal tail rate depending on whether the time horizon $T$ is known a priori or not. Interestingly, when it comes to the purely worst-case scenario, this gap disappears. Our results reveal insights on how to design policies that balance between efficiency and safety, and highlight extra insights on policy robustness with regard to policy hyper-parameters and model mis-specification. We also conduct a simulation study to validate our theoretical insights and provide practical amendment to our policies. Finally, we discuss extensions of our results to (i) general sub-exponential environments and (ii) general stochastic linear bandits. Furthermore, we find that a special case of our policy design surprisingly coincides with what was adopted in AlphaGo Monte Carlo Tree Search. Our theory provides high-level insights to why their engineered solution is successful and should be advocated in complex decision-making environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.04341v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Simchi-Levi, Zeyu Zheng, Feng Zhu</dc:creator>
    </item>
    <item>
      <title>Maximal Inequalities for Empirical Processes under General Mixing Conditions with an Application to Strong Approximations</title>
      <link>https://arxiv.org/abs/2402.11394</link>
      <description>arXiv:2402.11394v3 Announce Type: replace-cross 
Abstract: This paper provides a bound for the supremum of sample averages over a class of functions for a general class of mixing stochastic processes with arbitrary mixing rates. Regardless of the speed of mixing, the bound is comprised of a concentration rate and a novel measure of complexity. The speed of mixing, however, affects the former quantity implying a phase transition. Fast mixing leads to the standard root-n concentration rate, while slow mixing leads to a slower concentration rate, its speed depends on the mixing structure. Our findings are applied to obtain new Glivenko-Cantelli type results and to derive strong approximation results for a general class of mixing processes with arbitrary mixing rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11394v3</guid>
      <category>math.PR</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Demian Pouzo</dc:creator>
    </item>
    <item>
      <title>Inference for Deep Neural Network Estimators in Generalized Nonparametric Models</title>
      <link>https://arxiv.org/abs/2504.09347</link>
      <description>arXiv:2504.09347v3 Announce Type: replace-cross 
Abstract: While deep neural networks (DNNs) are used for prediction, inference on DNN-estimated subject-specific means for categorical or exponential family outcomes remains underexplored. We address this by proposing a DNN estimator under generalized nonparametric regression models (GNRMs) and developing a rigorous inference framework. Unlike existing approaches that assume independence between estimation errors and inputs to establish the error bound, a condition often violated in GNRMs, we allow for dependence and our theoretical analysis demonstrates the feasibility of drawing inference under GNRMs. To implement inference, we consider an Ensemble Subsampling Method (ESM) that leverages U-statistics and the Hoeffding decomposition to construct reliable confidence intervals for DNN estimates. We show that, under GNRM settings, ESM enables model-free variance estimation and accounts for heterogeneity among individuals in the population. Through simulations under nonparametric logistic, Poisson, and binomial regression models, we demonstrate the effectiveness and efficiency of our method. We further apply the method to the electronic Intensive Care Unit (eICU) dataset, a large scale collection of anonymized health records from ICU patients, to predict ICU readmission risk and offer patient-centric insights for clinical decision making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09347v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Xuran Meng, Yi Li</dc:creator>
    </item>
    <item>
      <title>CLT and Edgeworth Expansion for m-out-of-n Bootstrap Estimators of The Studentized Median</title>
      <link>https://arxiv.org/abs/2505.11725</link>
      <description>arXiv:2505.11725v2 Announce Type: replace-cross 
Abstract: The m-out-of-n bootstrap, originally proposed by Bickel, Gotze, and Zwet (1992), approximates the distribution of a statistic by repeatedly drawing m subsamples (with m much smaller than n) without replacement from an original sample of size n. It is now routinely used for robust inference with heavy-tailed data, bandwidth selection, and other large-sample applications. Despite its broad applicability across econometrics, biostatistics, and machine learning, rigorous parameter-free guarantees for the soundness of the m-out-of-n bootstrap when estimating sample quantiles have remained elusive.
  This paper establishes such guarantees by analyzing the estimator of sample quantiles obtained from m-out-of-n resampling of a dataset of size n. We first prove a central limit theorem for a fully data-driven version of the estimator that holds under a mild moment condition and involves no unknown nuisance parameters. We then show that the moment assumption is essentially tight by constructing a counter-example in which the CLT fails. Strengthening the assumptions slightly, we derive an Edgeworth expansion that provides exact convergence rates and, as a corollary, a Berry Esseen bound on the bootstrap approximation error. Finally, we illustrate the scope of our results by deriving parameter-free asymptotic distributions for practical statistics, including the quantiles for random walk Metropolis-Hastings and the rewards of ergodic Markov decision processes, thereby demonstrating the usefulness of our theory in modern estimation and learning tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11725v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Imon Banerjee, Sayak Chakrabarty</dc:creator>
    </item>
    <item>
      <title>Anytime-valid, Bayes-assisted, Prediction-Powered Inference</title>
      <link>https://arxiv.org/abs/2505.18000</link>
      <description>arXiv:2505.18000v2 Announce Type: replace-cross 
Abstract: Given a large pool of unlabelled data and a smaller amount of labels, prediction-powered inference (PPI) leverages machine learning predictions to increase the statistical efficiency of confidence interval procedures based solely on labelled data, while preserving fixed-time validity. In this paper, we extend the PPI framework to the sequential setting, where labelled and unlabelled datasets grow over time. Exploiting Ville's inequality and the method of mixtures, we propose prediction-powered confidence sequence procedures that are asymptotically valid uniformly over time and naturally accommodate prior knowledge on the quality of the predictions to further boost efficiency. We carefully illustrate the design choices behind our method and demonstrate its effectiveness in real and synthetic examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18000v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Valentin Kilian, Stefano Cortinovis, Fran\c{c}ois Caron</dc:creator>
    </item>
    <item>
      <title>On Transferring Transferability: Towards a Theory for Size Generalization</title>
      <link>https://arxiv.org/abs/2505.23599</link>
      <description>arXiv:2505.23599v2 Announce Type: replace-cross 
Abstract: Many modern learning tasks require models that can take inputs of varying sizes. Consequently, dimension-independent architectures have been proposed for domains where the inputs are graphs, sets, and point clouds. Recent work on graph neural networks has explored whether a model trained on low-dimensional data can transfer its performance to higher-dimensional inputs. We extend this body of work by introducing a general framework for transferability across dimensions. We show that transferability corresponds precisely to continuity in a limit space formed by identifying small problem instances with equivalent large ones. This identification is driven by the data and the learning task. We instantiate our framework on existing architectures, and implement the necessary changes to ensure their transferability. Finally, we provide design principles for designing new transferable models. Numerical experiments support our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23599v2</guid>
      <category>cs.LG</category>
      <category>math.RT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eitan Levin, Yuxin Ma, Mateo D\'iaz, Soledad Villar</dc:creator>
    </item>
    <item>
      <title>Learning Survival Models with Right-Censored Reporting Delays</title>
      <link>https://arxiv.org/abs/2510.04421</link>
      <description>arXiv:2510.04421v2 Announce Type: replace-cross 
Abstract: Survival analysis is a statistical technique used to estimate the time until an event occurs. Although it is applied across a wide range of fields, adjusting for reporting delays under practical constraints remains a significant challenge in the insurance industry. Such delays render event occurrences unobservable when their reports are subject to right censoring. This issue becomes particularly critical when estimating hazard rates for newly enrolled cohorts with limited follow-up due to administrative censoring. Our study addresses this challenge by jointly modeling the parametric hazard functions of event occurrences and report timings. The joint probability distribution is marginalized over the latent event occurrence status. We construct an estimator for the proposed survival model and establish its asymptotic consistency. Furthermore, we develop an expectation-maximization algorithm to compute its estimates. Using these findings, we propose a two-stage estimation procedure based on a parametric proportional hazards model to evaluate observations subject to administrative censoring. Experimental results demonstrate that our method effectively improves the timeliness of risk evaluation for newly enrolled cohorts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04421v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuta Shikuri, Hironori Fujisawa</dc:creator>
    </item>
    <item>
      <title>Testing Most Influential Sets</title>
      <link>https://arxiv.org/abs/2510.20372</link>
      <description>arXiv:2510.20372v2 Announce Type: replace-cross 
Abstract: Small subsets of data with disproportionate influence on model outcomes can have dramatic impacts on conclusions, with a few data points sometimes overturning key findings. While recent work has developed methods to identify these most influential sets, no formal theory exists to determine when their influence reflects genuine problems rather than natural sampling variation. We address this gap by developing a principled framework for assessing the statistical significance of most influential sets. Our theoretical results characterize the extreme value distributions of maximal influence and enable rigorous hypothesis tests for excessive influence, replacing current ad-hoc sensitivity checks. We demonstrate the practical value of our approach through applications across economics, biology, and machine learning benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20372v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lucas Darius Konrad, Nikolas Kuschnig</dc:creator>
    </item>
  </channel>
</rss>
