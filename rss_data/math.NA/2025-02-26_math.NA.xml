<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.NA</link>
    <description>math.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Feb 2025 02:53:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Preconditioned normal equations for solving discretised partial differential equations</title>
      <link>https://arxiv.org/abs/2502.17626</link>
      <description>arXiv:2502.17626v1 Announce Type: new 
Abstract: This paper explores preconditioning the normal equation for non-symmetric square linear systems arising from PDE discretization, focusing on methods like CGNE and LSQR. The concept of ``normal'' preconditioning is introduced and a strategy to construct preconditioners studying the associated ``normal'' PDE is presented. Numerical experiments on convection-diffusion problems demonstrate the effectiveness of this approach in achieving fast and stable convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17626v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Lazzarino, Yuji Nakatsukasa, Umberto Zerbinati</dc:creator>
    </item>
    <item>
      <title>Stable algorithms for general linear systems by preconditioning the normal equations</title>
      <link>https://arxiv.org/abs/2502.17767</link>
      <description>arXiv:2502.17767v1 Announce Type: new 
Abstract: This paper studies the solution of nonsymmetric linear systems by preconditioned Krylov methods based on the normal equations, LSQR in particular. On some examples, preconditioned LSQR is seen to produce errors many orders of magnitude larger than classical direct methods; this paper demonstrates that the attainable accuracy of preconditioned LSQR can be greatly improved by applying iterative refinement or restarting when the accuracy stalls. This observation is supported by rigorous backward error analysis. This paper also provides a discussion of the relative merits of GMRES and LSQR for solving nonsymmetric linear systems, demonstrates stability for left-preconditioned LSQR without iterative refinement, and shows that iterative refinement can also improve the accuracy of preconditioned conjugate gradient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17767v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ethan N. Epperly, Anne Greenbaum, Yuji Nakatsukasa</dc:creator>
    </item>
    <item>
      <title>Strategies for Feature-Assisted Development of Topology Agnostic Planar Antennas Using Variable-Fidelity Models</title>
      <link>https://arxiv.org/abs/2502.18275</link>
      <description>arXiv:2502.18275v1 Announce Type: new 
Abstract: Design of antennas for contemporary applications presents a complex challenge that integrates cognitive-driven topology development with the meticulous adjustment of parameters through rigorous numerical optimization. Nevertheless, the process can be streamlined by emphasizing the automatic determination of structure geometry, potentially reducing the reliance on traditional methods that heavily rely on engineering insight in the course of antenna development. In this work, which is an extension of our conference paper [1], a specification-oriented design of topologically agnostic antennas is considered by utilizing two strategies focused on bandwidth-specific design and bandwidth-enhanced optimization. The process is embedded within a variable-fidelity framework, where the low-fidelity optimization involves classification of randomly generated topologies, followed by their local tuning using a trust-region algorithm applied to a feature-based representation of structure response. The final result is then tuned using just a handful of high-fidelity simulations. The strategies under consideration were verified on a case study basis concerning automatic generation of three radiators with varying parameters. Benchmarks of the algorithm against more standard optimization methods, as well as comparisons of the obtained topologies with respect to state-of-the-art antennas from literature have also been considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18275v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jocs.2024.102521</arxiv:DOI>
      <arxiv:journal_reference>Journal of Computational Science, vol. 85, art. no. 102521, 2025</arxiv:journal_reference>
      <dc:creator>Adrian Bekasiewicz, Khadijeh Askaripour, Mariusz Dzwonkowski, Tom Dhaene, Ivo Couckuyt</dc:creator>
    </item>
    <item>
      <title>Polynomial Approximation to the Inverse of a Large Matrix</title>
      <link>https://arxiv.org/abs/2502.18317</link>
      <description>arXiv:2502.18317v1 Announce Type: new 
Abstract: The inverse of a large matrix can often be accurately approximated by a polynomial of degree significantly lower than the order of the matrix. The iteration polynomial generated by a run of the GMRES algorithm is a good candidate, and its approximation to the inverse often seems to track the accuracy of the GMRES iteration. We investigate the quality of this approximation through theory and experiment, noting the practical need to add copies of some polynomial terms to improve stability. To mitigate storage and orthogonalization costs, other approaches have appeal, such as polynomial preconditioned GMRES and deflation of problematic eigenvalues. Applications of such polynomial approximations include solving systems of linear equations with multiple right-hand sides (where the solutions to subsequent problems come simply by multiplying the polynomial against the new right-hand sides) and variance reduction in multilevel Monte Carlo methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18317v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mark Embree, Joel A. Henningsen, Jordan Jackson, Ronald B. Morgan</dc:creator>
    </item>
    <item>
      <title>Accuracy of Wearable ECG Parameter Calculation Method for Long QT and First-Degree A-V Block Detection: A Multi-Center Real-World Study with External Validations Compared to Standard ECG Machines and Cardiologist Assessments</title>
      <link>https://arxiv.org/abs/2502.17499</link>
      <description>arXiv:2502.17499v1 Announce Type: cross 
Abstract: In recent years, wearable devices have revolutionized cardiac monitoring by enabling continuous, non-invasive ECG recording in real-world settings. Despite these advances, the accuracy of ECG parameter calculations (PR interval, QRS interval, QT interval, etc.) from wearables remains to be rigorously validated against conventional ECG machines and expert clinician assessments. In this large-scale, multicenter study, we evaluated FeatureDB, a novel algorithm for automated computation of ECG parameters from wearable single-lead signals Three diverse datasets were employed: the AHMU-FH dataset (n=88,874), the CSE dataset (n=106), and the HeartVoice-ECG-lite dataset (n=369) with annotations provided by two experienced cardiologists. FeatureDB demonstrates a statistically significant correlation with key parameters (PR interval, QRS duration, QT interval, and QTc) calculated by standard ECG machines and annotated by clinical doctors. Bland-Altman analysis confirms a high level of agreement.Moreover,FeatureDB exhibited robust diagnostic performance in detecting Long QT syndrome (LQT) and atrioventricular block interval abnormalities (AVBI),with excellent area under the ROC curve (LQT: 0.836, AVBI: 0.861),accuracy (LQT: 0.856, AVBI: 0.845),sensitivity (LQT: 0.815, AVBI: 0.877),and specificity (LQT: 0.856, AVBI: 0.845).This further validates its clinical reliability. These results validate the clinical applicability of FeatureDB for wearable ECG analysis and highlight its potential to bridge the gap between traditional diagnostic methods and emerging wearable technologies.Ultimately,this study supports integrating wearable ECG devices into large-scale cardiovascular disease management and early intervention strategies,and it highlights the potential of wearable ECG technologies to deliver accurate,clinically relevant cardiac monitoring while advancing broader applications in cardiovascular care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17499v1</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sumei Fan, Deyun Zhang, Yue Wang, Shijia Geng, Kun Lu, Meng Sang, Weilun Xu, Haixue Wang, Qinghao Zhao, Chuandong Cheng, Peng Wang, Shenda Hong</dc:creator>
    </item>
    <item>
      <title>Synergizing Deep Learning and Full-Waveform Inversion: Bridging Data-Driven and Theory-Guided Approaches for Enhanced Seismic Imaging</title>
      <link>https://arxiv.org/abs/2502.17585</link>
      <description>arXiv:2502.17585v1 Announce Type: cross 
Abstract: This review explores the integration of deep learning (DL) with full-waveform inversion (FWI) for enhanced seismic imaging and subsurface characterization. It covers FWI and DL fundamentals, geophysical applications (velocity estimation, deconvolution, tomography), and challenges (model complexity, data quality). The review also outlines future research directions, including hybrid, generative, and physics-informed models for improved accuracy, efficiency, and reliability in subsurface property estimation. The synergy between DL and FWI has the potential to transform geophysics, providing new insights into Earth's subsurface.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17585v1</guid>
      <category>physics.geo-ph</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher Zerafa, Pauline Galea, Cristiana Sebu</dc:creator>
    </item>
    <item>
      <title>Mesoscale Modeling of an Active Colloid's Motion</title>
      <link>https://arxiv.org/abs/2502.17641</link>
      <description>arXiv:2502.17641v1 Announce Type: cross 
Abstract: This paper uses Cahn-Hilliard equations as a mesoscale model of the motion of active colloids. The model attempts to capture the driving mechanisms and qualitative behavior of the isotropic colloids originally proposed by J. Decayeaux in 2021. We compare our model against the single colloid behavior presented in that work, as well as against multi-colloid systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17641v1</guid>
      <category>cond-mat.soft</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew Dobson, David Masse</dc:creator>
    </item>
    <item>
      <title>Optimal Recovery Meets Minimax Estimation</title>
      <link>https://arxiv.org/abs/2502.17671</link>
      <description>arXiv:2502.17671v1 Announce Type: cross 
Abstract: A fundamental problem in statistics and machine learning is to estimate a function $f$ from possibly noisy observations of its point samples. The goal is to design a numerical algorithm to construct an approximation $\hat f$ to $f$ in a prescribed norm that asymptotically achieves the best possible error (as a function of the number $m$ of observations and the variance $\sigma^2$ of the noise). This problem has received considerable attention in both nonparametric statistics (noisy observations) and optimal recovery (noiseless observations). Quantitative bounds require assumptions on $f$, known as model class assumptions. Classical results assume that $f$ is in the unit ball of a Besov space. In nonparametric statistics, the best possible performance of an algorithm for finding $\hat f$ is known as the minimax rate and has been studied in this setting under the assumption that the noise is Gaussian. In optimal recovery, the best possible performance of an algorithm is known as the optimal recovery rate and has also been determined in this setting. While one would expect that the minimax rate recovers the optimal recovery rate when the noise level $\sigma$ tends to zero, it turns out that the current results on minimax rates do not carefully determine the dependence on $\sigma$ and the limit cannot be taken. This paper handles this issue and determines the noise-level-aware (NLA) minimax rates for Besov classes when error is measured in an $L_q$-norm with matching upper and lower bounds. The end result is a reconciliation between minimax rates and optimal recovery rates. The NLA minimax rate continuously depends on the noise level and recovers the optimal recovery rate when $\sigma$ tends to zero.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17671v1</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ronald DeVore, Robert D. Nowak, Rahul Parhi, Guergana Petrova, Jonathan W. Siegel</dc:creator>
    </item>
    <item>
      <title>Sufficient Conditions for the Energy Balance for the Stochastic Incompressible Euler Equations with Additive Noise in two Space Dimensions</title>
      <link>https://arxiv.org/abs/2502.17732</link>
      <description>arXiv:2502.17732v1 Announce Type: cross 
Abstract: We consider vanishing viscosity approximations to solutions of the stochastic incompressible Euler equations in two space dimensions with additive noise. We identify sufficient and necessary conditions under which martingale solutions of the stochastic Euler equations satisfy an exact energy balance in mean. We find that the tightness of the laws of the approximating sequence of solutions of the stochastic Navier-Stokes equations in $L^2([0,T]\times D)$ is equivalent to the limiting martingale solution satisfying an energy balance in mean. Numerical simulations illustrate the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17732v1</guid>
      <category>math.PR</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tobias Rohner, Franziska Weber</dc:creator>
    </item>
    <item>
      <title>Local iterative algorithms for approximate symmetry guided by network centralities</title>
      <link>https://arxiv.org/abs/2502.18155</link>
      <description>arXiv:2502.18155v1 Announce Type: cross 
Abstract: Recently, the influence of potentially present symmetries has begun to be studied in complex networks. A typical way of studying symmetries is via the automorphism group of the corresponding graph. Since complex networks are often subject to uncertainty and automorphisms are very sensitive to small changes, this characterization needs to be modified to an approximate version for successful application. This paper considers a recently introduced approximate symmetry of complex networks computed as an automorphism with acceptance of small edge preservation error, see Liu 2020. This problem is generally very hard with respect to the large space of candidate permutations, and hence the corresponding computation methods typically lead to the utilization of local algorithms such as the simulated annealing used in the original work. This paper proposes a new heuristic algorithm extending such iterative search algorithm method by using network centralities as heuristics. Centralities are shown to be a good tool to navigate the local search towards more appropriate permutations and lead to better search results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18155v1</guid>
      <category>cs.SI</category>
      <category>cs.DM</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Hartman, Jaroslav Hlinka, Anna Pidnebesna, Franti\v{s}ek Szczepanik</dc:creator>
    </item>
    <item>
      <title>Adaptive Quasicontinuum Methods and Simulations for Crystal Defects with a Theory based Unified a Posteriori Error Estimate</title>
      <link>https://arxiv.org/abs/2309.13255</link>
      <description>arXiv:2309.13255v2 Announce Type: replace 
Abstract: Adaptive quasicontinuum (QC) methods are important methodologies in molecular mechanics for the simulations of materials with defects, intending to achieve the optimal balance of accuracy and efficiency on the fly. In this study, we propose a residual-force based a posteriori error estimate that is simple and is unified for consistent quasicontinuum methods, as opposed to the widely adopted residual-stress based a posteriori error estimates which are complicated and need to be derived for the particular QC method under consideration. The simple and unified formulation of the estimator, together with certain sampling techniques, leads to a highly efficient and adaptable implementation. We also prove in theory that the unified error estimator provides an upper bound of the true error. We develop adaptive algorithms based on this unified estimator and validate the algorithms by several representative quasicontinuum methods for various types of crystalline defects, in terms of convergence and efficiency. In particular, the adaptive simulations of the anti-plane crack, of which we possess little a priori knowledge, demonstrate the necessity and significance of the proposed a posteriori estimates and the adaptive strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.13255v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Hao Wang, Yangshuai Wang</dc:creator>
    </item>
    <item>
      <title>Parameter identification in PDEs by the solution of monotone inclusion problems</title>
      <link>https://arxiv.org/abs/2403.04557</link>
      <description>arXiv:2403.04557v2 Announce Type: replace 
Abstract: In this paper we consider the solution of monotone inverse problems using the particular example of a parameter identification problem for a semilinear parabolic PDE. For the regularized solution of this problem, we introduce a total variation based regularization method requiring the solution of a monotone inclusion problem. We show well-posedness in the sense of inverse problems of the resulting regularization scheme. In addition, we introduce and analyze a numerical algorithm for the solution of this inclusion problem using a nested inertial primal dual method. We demonstrate by means of numerical examples the convergence of both the numerical algorithm and the regularization method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04557v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pankaj Gautam, Markus Grasmair</dc:creator>
    </item>
    <item>
      <title>High-order limiting methods using maximum principle bounds derived from the Boltzmann equation I: Euler equations</title>
      <link>https://arxiv.org/abs/2407.20966</link>
      <description>arXiv:2407.20966v3 Announce Type: replace 
Abstract: The use of limiting methods for high-order numerical approximations of hyperbolic conservation laws generally requires defining an admissible region/bounds for the solution. In this work, we present a novel approach for computing solution bounds and limiting for the Euler equations through the kinetic representation provided by the Boltzmann equation, which allows for extending limiters designed for linear advection directly to the Euler equations. Given an arbitrary set of solution values to compute bounds over (e.g., numerical stencil) and a desired linear advection limiter, the proposed approach yields an analytic expression for the admissible region of particle distribution function values, which may be numerically integrated to yield a set of bounds for the density, momentum, and total energy. These solution bounds are shown to preserve positivity of density/pressure/internal energy and, when paired with a limiting technique, can robustly resolve strong discontinuities while recovering high-order accuracy in smooth regions without any ad hoc corrections (e.g., relaxing the bounds). This approach is demonstrated in the context of an explicit unstructured high-order discontinuous Galerkin/flux reconstruction scheme for a variety of difficult problems in gas dynamics, including cases with extreme shocks and shock-vortex interactions. Furthermore, this work presents a foundation for limiting techniques for more complex macroscopic governing equations that can be derived from an underlying kinetic representation for which admissible solution bounds are not well-understood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20966v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>Journal of Computational Physics, 530, 113895, 2025</arxiv:journal_reference>
      <dc:creator>Tarik Dzanic, Luigi Martinelli</dc:creator>
    </item>
    <item>
      <title>Probabilistic error analysis of limited-precision stochastic rounding</title>
      <link>https://arxiv.org/abs/2408.03069</link>
      <description>arXiv:2408.03069v3 Announce Type: replace 
Abstract: Classical probabilistic rounding error analysis is particularly well suited to stochastic rounding (SR), and it yields strong results when dealing with floating-point algorithms that rely heavily on summation. For many numerical linear algebra algorithms, one can prove probabilistic error bounds that grow as O(nu), where n is the problem size and u is the unit roundoff. These probabilistic bounds are asymptotically tighter than the worst-case ones, which grow as O(nu). For certain classes of algorithms, SR has been shown to be unbiased. However, all these results were derived under the assumption that SR is implemented exactly, which typically requires a number of random bits that is too large to be suitable for practical implementations. We investigate the effect of the number of random bits on the probabilistic rounding error analysis of SR. To this end, we introduce a new rounding mode, limited-precision SR. By taking into account the number r of random bits used, this new rounding mode matches hardware implementations accurately, unlike the ideal SR operator generally used in the literature. We show that this new rounding mode is biased and that the bias is a function of r. As r approaches infinity, however, the bias disappears, and limited-precision SR converges to the ideal, unbiased SR operator. We develop a novel model for probabilistic error analysis of algorithms employing SR. Several numerical examples corroborate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03069v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>El-Mehdi El Arar (TARAN), Massimiliano Fasi (TARAN), Silviu-Ioan Filip (TARAN), Mantas Mikaitis</dc:creator>
    </item>
    <item>
      <title>Two-level hybrid Schwarz Preconditioners for The Helmholtz Equation with high wave number</title>
      <link>https://arxiv.org/abs/2408.07669</link>
      <description>arXiv:2408.07669v2 Announce Type: replace 
Abstract: In this work, we propose and analyze two two-level hybrid Schwarz preconditioners for solving the Helmholtz equation with high wave number in two and three dimensions. Both preconditioners are defined over a set of overlapping subdomains, with each preconditioner formed by a global coarse solver and one local solver on each subdomain. The global coarse solver is based on the localized orthogonal decomposition (LOD) technique, which was proposed in [30,31] originally for the discretization schemes for elliptic multiscale problems with heterogeneous and highly oscillating coefficients and Helmholtz problems with high wave number to eliminate the pollution effect. The local subproblems are Helmholtz problems in subdomains with homogeneous boundary conditions (the first preconditioner) or impedance boundary conditions (the second preconditioner). Both preconditioners are shown to be optimal under some reasonable conditions, that is, a uniform upper bound of the preconditioned operator norm and a uniform lower bound of the field of values are established in terms of all the key parameters, such as the fine mesh size, the coarse mesh size, the subdomain size and the wave numbers. It is the first time to show that the LOD solver can be a very effective coarse solver when it is used appropriately in the Schwarz method with multiple overlapping subdomains. Numerical experiments are presented to confirm the optimality and efficiency of the two proposed preconditioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07669v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peipei Lu, Xuejun Xu, Bowen Zheng, Jun Zou</dc:creator>
    </item>
    <item>
      <title>Finite Element Theory for PHIMATS</title>
      <link>https://arxiv.org/abs/2502.16283</link>
      <description>arXiv:2502.16283v2 Announce Type: replace 
Abstract: This document summarizes the main ideas of the finite element method (FEM) theory and constitutive relations as implemented in the PHIMATS code (\href{https://github.com/ahcomat/PHIMATS.git}{GitHub Repository}). Rather than detailing the derivations or specific models, this document focuses on the key mathematical foundations and numerical strategies used within the implementation. For in-depth theoretical discussions, the reader is encouraged to consult the references. For citing this document, please use ... Hands-on examples can be found in CaseStudies directory on the GitHub repository. .</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16283v2</guid>
      <category>math.NA</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.NA</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdelrahman Hussein</dc:creator>
    </item>
    <item>
      <title>Reconstruction of dynamical systems from data without time labels</title>
      <link>https://arxiv.org/abs/2312.04038</link>
      <description>arXiv:2312.04038v3 Announce Type: replace-cross 
Abstract: In this paper, we study the method to reconstruct dynamical systems from data without time labels. Data without time labels appear in many applications, such as molecular dynamics, single-cell RNA sequencing etc. Reconstruction of dynamical system from time sequence data has been studied extensively. However, these methods do not apply if time labels are unknown. Without time labels, sequence data becomes distribution data. Based on this observation, we propose to treat the data as samples from a probability distribution and try to reconstruct the underlying dynamical system by minimizing the distribution loss, sliced Wasserstein distance more specifically. Extensive experiment results demonstrate the effectiveness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04038v3</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.NA</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhijun Zeng, Pipi Hu, Chenglong Bao, Yi Zhu, Zuoqiang Shi</dc:creator>
    </item>
    <item>
      <title>SUperman: Efficient Permanent Computation on GPUs</title>
      <link>https://arxiv.org/abs/2502.16577</link>
      <description>arXiv:2502.16577v2 Announce Type: replace-cross 
Abstract: The permanent is a function, defined for a square matrix, with applications in various domains including quantum computing, statistical physics, complexity theory, combinatorics, and graph theory. Its formula is similar to that of the determinant, however unlike the determinant, its exact computation is #P-complete, i.e., there is no algorithm to compute the permanent in polynomial time unless P=NP. For an $n \times n$ matrix, the fastest algorithm has a time complexity of $O(2^{n-1}n)$. Although supercomputers have been employed for permanent computation before, there is no work and more importantly, no publicly available software that leverages cutting-edge, yet widely accessible, High-Performance Computing accelerators such as GPUs. In this work, we designed, developed, and investigated the performance of SUperman, a complete software suite that can compute matrix permanents on multiple nodes/GPUs on a cluster while handling various matrix types, e.g., real/complex/binary and sparse/dense etc., with a unique treatment for each type. Compared to a state-of-the-art parallel algorithm on 44 cores, SUperman can be $86\times$ faster on a single Nvidia A100 GPU. Combining multiple GPUs, we also showed that SUperman can compute the permanent of a $56 \times 56$ matrix which is the largest reported in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16577v2</guid>
      <category>cs.DC</category>
      <category>cs.DM</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deniz Elbek, Fatih Ta\c{s}yaran, Bora U\c{c}ar, Kamer Kaya</dc:creator>
    </item>
  </channel>
</rss>
