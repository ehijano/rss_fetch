<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Mar 2025 02:15:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Generative Caching System for Large Language Models</title>
      <link>https://arxiv.org/abs/2503.17603</link>
      <description>arXiv:2503.17603v1 Announce Type: new 
Abstract: Caching has the potential to be of significant benefit for accessing large language models (LLMs) due to their high latencies which typically range from a small number of seconds to well over a minute. Furthermore, many LLMs charge money for queries; caching thus has a clear monetary benefit. This paper presents a new caching system for improving user experiences with LLMs. In addition to reducing both latencies and monetary costs for accessing LLMs, our system also provides important features that go beyond the performance benefits typically associated with caches. A key feature we provide is generative caching, wherein multiple cached responses can be synthesized to provide answers to queries which have never been seen before. Our generative caches function as repositories of valuable information which can be mined and analyzed. We also improve upon past semantic caching techniques by tailoring the caching algorithms to optimally balance cost and latency reduction with the quality of responses provided. Performance tests indicate that our caches are considerably faster than GPTcache.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17603v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arun Iyengar, Ashish Kundu, Ramana Kompella, Sai Nandan Mamidi</dc:creator>
    </item>
    <item>
      <title>Revisiting Page Migration for Main-Memory Database Systems</title>
      <link>https://arxiv.org/abs/2503.17685</link>
      <description>arXiv:2503.17685v1 Announce Type: new 
Abstract: Modern NUMA servers and tiered memory systems have significantly improved the DBMS performance, and are poised to deliver further improvements in the future. However, realizing this potential depends on the DBMS's ability to efficiently migrate pages among different NUMA nodes, and/or memory chips as the workload evolves. Modern DBMSs blindly offload the migration procedure to the operating system without accounting for the workload and its migration characteristics. In this paper, we propose a custom system call move_pages2 as an alternate to Linux's own move_pages system call. Experiments on a B$^+$-Tree for a YCSB-like workload show that the proposed move\_pages2 custom system call outperforms the native Linux system call by up to 1.84$\times$ and 2.09$\times$, in terms of query and page migration throughputs, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17685v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yeasir Rayhan, Walid G. Aref</dc:creator>
    </item>
    <item>
      <title>VSAG: An Optimized Search Framework for Graph-based Approximate Nearest Neighbor Search</title>
      <link>https://arxiv.org/abs/2503.17911</link>
      <description>arXiv:2503.17911v1 Announce Type: new 
Abstract: Approximate nearest neighbor search (ANNS) is a fundamental problem in vector databases and AI infrastructures. Recent graph-based ANNS algorithms have achieved high search accuracy with practical efficiency. Despite the advancements, these algorithms still face performance bottlenecks in production, due to the random memory access patterns of graph-based search and the high computational overheads of vector distance. In addition, the performance of a graph-based ANNS algorithm is highly sensitive to parameters, while selecting the optimal parameters is cost-prohibitive, e.g., manual tuning requires repeatedly re-building the index.
  This paper introduces VSAG, an open-source framework that aims to enhance the in production performance of graph-based ANNS algorithms. VSAG has been deployed at scale in the services of Ant Group, and it incorporates three key optimizations: (i) efficient memory access: it reduces L3 cache misses with pre-fetching and cache-friendly vector organization; (ii) automated parameter tuning: it automatically selects performance-optimal parameters without requiring index rebuilding; (iii) efficient distance computation: it leverages modern hardware, scalar quantization, and smartly switches to low-precision representation to dramatically reduce the distance computation costs. We evaluate VSAG on real-world datasets. The experimental results show that VSAG achieves the state-of-the-art performance and provides up to 4x speedup over HNSWlib (an industry-standard library) while ensuring the same accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17911v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoyao Zhong, Haotian Li, Jiabao Jin, Mingyu Yang, Deming Chu, Xiangyu Wang, Zhitao Shen, Wei Jia, George Gu, Yi Xie, Xuemin Lin, Heng Tao Shen, Jingkuan Song, Peng Cheng</dc:creator>
    </item>
    <item>
      <title>Bag Semantics Conjunctive Query Containment. Four Small Steps Towards Undecidability</title>
      <link>https://arxiv.org/abs/2503.18003</link>
      <description>arXiv:2503.18003v1 Announce Type: new 
Abstract: Query Containment Problem (QCP) is one of the most fundamental decision problems in database query processing and optimization. Complexity of QCP for conjunctive queries (QCP-CQ) has been fully understood since 1970s. But, as Chaudhuri and Vardi noticed in their classical 1993 paper [1], this understanding is based on the assumption that query answers are sets of tuples, and it does not transfer to the situation when multi-set (bag) semantics is considered. Now, 30 years after [1] was written, decidability of QCP-CQ for bag semantics remains an open question, one of the most intriguing open questions in database theory. In this paper we show a series of undecidability results for some generalizations of bag-semantics QCP-CQ. We show, for example, that the problem whether, for given two boolean conjunctive queries Q and Q' , and a linear function F, the inequality F(Q(D)) =&lt; Q'(D) holds for each database instance D, is undecidable</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18003v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3651604</arxiv:DOI>
      <arxiv:journal_reference>Proc. ACM Manag. Data 2, 2 (PODS), Article 103 (May 2024),</arxiv:journal_reference>
      <dc:creator>Jerzy Marcinkowski, Mateusz Orda</dc:creator>
    </item>
    <item>
      <title>On the feasibility of semantic query metrics</title>
      <link>https://arxiv.org/abs/2503.18214</link>
      <description>arXiv:2503.18214v1 Announce Type: new 
Abstract: We consider the problem of defining semantic metrics for relational database queries. Informally, a semantic query metric for a query language $L$ is a metric function $\delta:L\times L\to \mathbb{N}$ where $\delta(Q_1, Q_2)$ represents the length of a shortest path between queries $Q_1$ and $Q_2$ in a graph. In this graph, nodes are queries from $L$, and edges connect semantically distinct queries where one query is maximally semantically contained in the other. Since query containment is undecidable for first-order queries, we focus on the simpler language of conjunctive queries. We establish that defining a semantic query metric is impossible even for conjunctive queries. Given this impossibility result, we identify a significant subclass of conjunctive queries where such a metric is feasible, and we establish the computational complexity of calculating distances within this language.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18214v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>George Fletcher, Peter Wood, Nikolay Yakovets</dc:creator>
    </item>
    <item>
      <title>SynchroStore: A Cost-Based Fine-Grained Incremental Compaction for Hybrid Workloads</title>
      <link>https://arxiv.org/abs/2503.18688</link>
      <description>arXiv:2503.18688v1 Announce Type: new 
Abstract: This study proposes a novel storage engine, SynchroStore, designed to address the inefficiency of update operations in columnar storage systems based on Log-Structured Merge Trees (LSM-Trees) under hybrid workload scenarios. While columnar storage formats demonstrate significant query performance advantages when handling large-scale datasets, traditional columnar storage systems face challenges such as high update complexity and poor real-time performance in data-intensive applications. SynchroStore introduces an incremental row storage mechanism and a fine-grained row-to-column transformation and compaction strategy, effectively balancing data update efficiency and query performance. The storage system employs an in-memory row storage structure to support efficient update operations, and the data is converted to a columnar format after freezing to support high-performance read operations. The core innovations of SynchroStore are reflected in the following aspects:(1) the organic combination of incremental row storage and columnar storage; (2) a fine-grained row-to-column transformation and compaction mechanism; (3) a cost-based scheduling strategy. These innovative features allow SynchroStore to leverage background computational resources for row-to-column transformation and compaction operations, while ensuring query performance is unaffected, thus effectively solving the update performance bottleneck of columnar storage under hybrid workloads. Experimental evaluation results show that, compared to existing columnar storage systems like DuckDB, SynchroStore exhibits significant advantages in update performance under hybrid workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18688v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinan Zhang, Huiqi Hu, Xuan Zhou</dc:creator>
    </item>
    <item>
      <title>Transformer-based Ranking Approaches for Keyword Queries over Relational Databases</title>
      <link>https://arxiv.org/abs/2503.18768</link>
      <description>arXiv:2503.18768v1 Announce Type: new 
Abstract: Relational Keyword Search (R-KwS) systems enable naive/informal users to explore and retrieve information from relational databases without requiring schema knowledge or query-language proficiency. Although numerous R-KwS methods have been proposed, most still focus on queries referring only to attribute values or primarily address performance enhancements, providing limited support for queries referencing schema elements. We previously introduced Lathe, a system that accommodates schema-based keyword queries and employs an eager CJN evaluation strategy to filter out spurious Candidate Joining Networks (CJNs). However, Lathe still faces challenges in accurately ranking CJNs when queries are ambiguous. In this work, we propose a new transformer-based ranking approach that provides a more context-aware evaluation of Query Matches (QMs) and CJNs. Our solution introduces a linearization process to convert relational structures into textual sequences suitable for transformer models. It also includes a data augmentation strategy aimed at handling diverse and ambiguous queries more effectively. Experimental results, comparing our transformer-based ranking to Lathe's original Bayesian-based method, show significant improvements in recall and R@k, demonstrating the effectiveness of our neural approach in delivering the most relevant query results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18768v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paulo Martins, Altigran da Silva, Johny Moreira, Edleno de Moura</dc:creator>
    </item>
    <item>
      <title>A Predictive Services Architecture for Efficient Airspace Operations</title>
      <link>https://arxiv.org/abs/2503.17515</link>
      <description>arXiv:2503.17515v1 Announce Type: cross 
Abstract: Predicting air traffic congestion and flow management is essential for airlines and Air Navigation Service Providers (ANSP) to enhance operational efficiency. Accurate estimates of future airport capacity and airspace density are vital for better airspace management, reducing air traffic controller workload and fuel consumption, ultimately promoting sustainable aviation. While existing literature has addressed these challenges, data management and query processing remain complex due to the vast volume of high-rate air traffic data. Many analytics use cases require a common pre-processing infrastructure, as ad-hoc approaches are insufficient. Additionally, linear prediction models often fall short, necessitating more advanced techniques.
  This paper presents a data processing and predictive services architecture that ingests large, uncorrelated, and noisy streaming data to forecast future airspace system states. The system continuously collects raw data, periodically compresses it, and stores it in NoSQL databases for efficient query processing. For prediction, the system learns from historical traffic by extracting key features such as airport arrival and departure events, sector boundary crossings, weather parameters, and other air traffic data. These features are input into various regression models, including linear, non-linear, and ensemble models, with the best-performing model selected for predictions. We evaluate this infrastructure across three prediction use cases in the US National Airspace System (NAS) and a segment of European airspace, using extensive real operations data, confirming that our system can predict future system states efficiently and accurately.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17515v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\'Italo Romani de Oliveira, Samet Ayhan, Glaucia Balvedi, Michael Biglin, Pablo Costas, Euclides C. Pinto Neto, Alexandre Leite, Felipe C. F. de Azevedo</dc:creator>
    </item>
    <item>
      <title>Feather-SQL: A Lightweight NL2SQL Framework with Dual-Model Collaboration Paradigm for Small Language Models</title>
      <link>https://arxiv.org/abs/2503.17811</link>
      <description>arXiv:2503.17811v1 Announce Type: cross 
Abstract: Natural Language to SQL (NL2SQL) has seen significant advancements with large language models (LLMs). However, these models often depend on closed-source systems and high computational resources, posing challenges in data privacy and deployment. In contrast, small language models (SLMs) struggle with NL2SQL tasks, exhibiting poor performance and incompatibility with existing frameworks. To address these issues, we introduce Feather-SQL, a new lightweight framework tailored for SLMs. Feather-SQL improves SQL executability and accuracy through 1) schema pruning and linking, 2) multi-path and multi-candidate generation. Additionally, we introduce the 1+1 Model Collaboration Paradigm, which pairs a strong general-purpose chat model with a fine-tuned SQL specialist, combining strong analytical reasoning with high-precision SQL generation. Experimental results on BIRD demonstrate that Feather-SQL improves NL2SQL performance on SLMs, with around 10% boost for models without fine-tuning. The proposed paradigm raises the accuracy ceiling of SLMs to 54.76%, highlighting its effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17811v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenqi Pei, Hailing Xu, Hengyuan Zhao, Shizheng Hou, Han Chen, Zining Zhang, Pingyi Luo, Bingsheng He</dc:creator>
    </item>
    <item>
      <title>Identifying and Characterising Higher Order Interactions in Mobility Networks Using Hypergraphs</title>
      <link>https://arxiv.org/abs/2503.18572</link>
      <description>arXiv:2503.18572v1 Announce Type: cross 
Abstract: Understanding human mobility is essential for applications ranging from urban planning to public health. Traditional mobility models such as flow networks and colocation matrices capture only pairwise interactions between discrete locations, overlooking higher-order relationships among locations (i.e., mobility flow among two or more locations). To address this, we propose co-visitation hypergraphs, a model that leverages temporal observation windows to extract group interactions between locations from individual mobility trajectory data. Using frequent pattern mining, our approach constructs hypergraphs that capture dynamic mobility behaviors across different spatial and temporal scales. We validate our method on a publicly available mobility dataset and demonstrate its effectiveness in analyzing city-scale mobility patterns, detecting shifts during external disruptions such as extreme weather events, and examining how a location's connectivity (degree) relates to the number of points of interest (POIs) within it. Our results demonstrate that our hypergraph-based mobility analysis framework is a valuable tool with potential applications in diverse fields such as public health, disaster resilience, and urban planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18572v1</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prathyush Sambaturu, Bernardo Gutierrez, Moritz U. G. Kraemer</dc:creator>
    </item>
    <item>
      <title>SwiftSpatial: Spatial Joins on Modern Hardware</title>
      <link>https://arxiv.org/abs/2309.16520</link>
      <description>arXiv:2309.16520v2 Announce Type: replace 
Abstract: Spatial joins are among the most time-consuming spatial queries, remaining costly even in parallel and distributed systems. In this paper, we explore hardware acceleration for spatial joins by proposing SwiftSpatial, an FPGA-based accelerator that can be deployed in data centers and at the edge. SwiftSpatial contains multiple high-performance join units with innovative hybrid parallelism, several efficient memory management units, and an extensible on-chip join scheduler that supports the popular R-tree synchronous traversal and partition-based spatial-merge (PBSM) algorithms. Benchmarked against various CPU and GPU-based spatial data processing systems, SwiftSpatial demonstrates a latency reduction of up to 41.03x relative to the best-performing baseline, while requiring 6.16x less power. The performance and energy efficiency of SwiftSpatial demonstrate its potential to be used in a variety of configurations (e.g., as an accelerator, near storage, in-network) as well as on different devices (e.g., data centers where FPGAs are widely available or mobile devices, which also contain FPGAs for specialized processing).</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.16520v2</guid>
      <category>cs.DB</category>
      <category>cs.AR</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wenqi Jiang, Oleh-Yevhen Khavrona, Martin Parvanov, Gustavo Alonso</dc:creator>
    </item>
    <item>
      <title>A semantic approach to mapping the Provenance Ontology to Basic Formal Ontology</title>
      <link>https://arxiv.org/abs/2408.03866</link>
      <description>arXiv:2408.03866v2 Announce Type: replace 
Abstract: The Provenance Ontology (PROV-O) is a World Wide Web Consortium (W3C) recommended ontology used to structure data about provenance across a wide variety of domains. Basic Formal Ontology (BFO) is a top-level ontology ISO/IEC standard used to structure a wide variety of ontologies, such as the OBO Foundry ontologies and the Common Core Ontologies (CCO). To enhance interoperability between these two ontologies, their extensions, and data organized by them, a mapping methodology and set of alignments are presented according to specific criteria which prioritize semantic and logical principles. The ontology alignments are evaluated by checking their logical consistency with canonical examples of PROV-O instances and querying terms that do not satisfy the alignment criteria as formalized in SPARQL. A variety of semantic web technologies are used in support of FAIR (Findable, Accessible, Interoperable, Reusable) principles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03866v2</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1038/s41597-025-04580-1</arxiv:DOI>
      <arxiv:journal_reference>Sci Data 12, 282 (2025)</arxiv:journal_reference>
      <dc:creator>Tim Prudhomme, Giacomo De Colle, Austin Liebers, Alec Sculley, Peihong "Karl" Xie, Sydney Cohen, John Beverley</dc:creator>
    </item>
    <item>
      <title>LLM+KG@VLDB'24 Workshop Summary</title>
      <link>https://arxiv.org/abs/2410.01978</link>
      <description>arXiv:2410.01978v2 Announce Type: replace 
Abstract: The unification of large language models (LLMs) and knowledge graphs (KGs) has emerged as a hot topic. At the LLM+KG'24 workshop, held in conjunction with VLDB 2024 in Guangzhou, China, one of the key themes explored was important data management challenges and opportunities due to the effective interaction between LLMs and KGs. This report outlines the major directions and approaches presented by various speakers during the LLM+KG'24 workshop.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01978v2</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arijit Khan, Tianxing Wu, Xi Chen</dc:creator>
    </item>
    <item>
      <title>Resilience for Regular Path Queries: Towards a Complexity Classification</title>
      <link>https://arxiv.org/abs/2412.09411</link>
      <description>arXiv:2412.09411v2 Announce Type: replace 
Abstract: The resilience problem for a query and an input set or bag database is to compute the minimum number of facts to remove from the database to make the query false. In this paper, we study how to compute the resilience of Regular Path Queries (RPQs) over graph databases. Our goal is to characterize the regular languages $L$ for which it is tractable to compute the resilience of the existentially-quantified RPQ built from $L$.
  We show that computing the resilience in this sense is tractable (even in combined complexity) for all RPQs defined from so-called local languages. By contrast, we show hardness in data complexity for RPQs defined from the following language classes (after reducing the languages to eliminate redundant words): all finite languages featuring a word containing a repeated letter, and all languages featuring a specific kind of counterexample to being local (which we call four-legged languages). The latter include in particular all languages that are not star-free. Our results also imply hardness for all non-local languages with a so-called neutral letter. We last highlight some remaining obstacles towards a full dichotomy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09411v2</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antoine Amarilli, Wolfgang Gatterbauer, Neha Makhija, Mika\"el Monet</dc:creator>
    </item>
    <item>
      <title>AnDB: Breaking Boundaries with an AI-Native Database for Universal Semantic Analysis</title>
      <link>https://arxiv.org/abs/2502.13805</link>
      <description>arXiv:2502.13805v2 Announce Type: replace 
Abstract: In this demonstration, we present AnDB, an AI-native database that supports traditional OLTP workloads and innovative AI-driven tasks, enabling unified semantic analysis across structured and unstructured data. While structured data analytics is mature, challenges remain in bridging the semantic gap between user queries and unstructured data. AnDB addresses these issues by leveraging cutting-edge AI-native technologies, allowing users to perform semantic queries using intuitive SQL-like statements without requiring AI expertise. This approach eliminates the ambiguity of traditional text-to-SQL systems and provides a seamless end-to-end optimization for analyzing all data types. AnDB automates query processing by generating multiple execution plans and selecting the optimal one through its optimizer, which balances accuracy, execution time, and financial cost based on user policies and internal optimizing mechanisms. AnDB future-proofs data management infrastructure, empowering users to effectively and efficiently harness the full potential of all kinds of data without starting from scratch.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13805v2</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianqing Wang, Xun Xue, Guoliang Li, Yong Wang</dc:creator>
    </item>
    <item>
      <title>Smart Routing: Cost-Effective Multi-LLM Serving in AIOS</title>
      <link>https://arxiv.org/abs/2502.20576</link>
      <description>arXiv:2502.20576v3 Announce Type: replace 
Abstract: As large language models (LLMs) are increasingly deployed as service endpoints in systems, the surge in query volume creates significant scheduling challenges. Existing scheduling frameworks mainly target at latency optimization while neglecting the capability of LLMs to serve different level of queries, which could lead to computational resource waste. For example, those simple queries can be safely handled by small, fast and cheap LLMs, while those complex and difficult queries need to be handled by large, slow, and expensive LLMs. This paper addresses this challenge by proposing an efficient capability-cost coordinated scheduling framework, ECCOS, for multi-LLM serving, which explicitly constrains response quality and workload to optimize LLM inference cost. Specifically, it introduces the two-stage scheduling by designing a multi-objective predictor and a constrained optimizer. The predictor estimates both model capabilities and computational costs through training-based and retrieval-based approaches, while the optimizer determines cost-optimal assignments under quality and workload constraints. It also introduces QAServe, a dataset for sample-wise response quality and costs collected by zero-shot prompting different LLMs on knowledge QA and mathematical reasoning. Extensive experiments demonstrate that ECCOS improves success rates by 6.30% while reducing costs by 10.15% compared to existing methods, consuming less than 0.5% of LLM response time. The code is available at: https://github.com/agiresearch/ECCOS, and the proposed smart routing mechanism has been integrated into AIOS, the AI Agent Operating System, at https://github.com/agiresearch/AIOS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20576v3</guid>
      <category>cs.DB</category>
      <category>cs.CL</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kai Mei, Wujiang Xu, Shuhang Lin, Yongfeng Zhang</dc:creator>
    </item>
    <item>
      <title>DeepInnovation AI: A Global Dataset Mapping the AI innovation from Academic Research to Industrial Patents</title>
      <link>https://arxiv.org/abs/2503.09257</link>
      <description>arXiv:2503.09257v3 Announce Type: replace 
Abstract: In the rapidly evolving field of artificial intelligence (AI), mapping innovation patterns and understanding effective technology transfer from research to applications are essential for economic growth. However, existing data infrastructures suffer from fragmentation, incomplete coverage, and insufficient evaluative capacity. Here, we present DeepInnovationAI, a comprehensive global dataset containing three structured files. DeepPatentAI.csv: Contains 2,356,204 patent records with 8 field-specific attributes. DeepDiveAI.csv: Encompasses 3,511,929 academic publications with 13 metadata fields. These two datasets leverage large language models, multilingual text analysis and dual-layer BERT classifiers to accurately identify AI-related content, while utilizing hypergraph analysis to create robust innovation metrics. Additionally, DeepCosineAI.csv: By applying semantic vector proximity analysis, this file presents approximately one hundred million calculated paper-patent similarity pairs to enhance understanding of how theoretical advancements translate into commercial technologies. DeepInnovationAI enables researchers, policymakers, and industry leaders to anticipate trends and identify collaboration opportunities. With extensive temporal and geographical scope, it supports detailed analysis of technological development patterns and international competition dynamics, establishing a foundation for modeling AI innovation and technology transfer processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09257v3</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.DL</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haixing Gong, Hui Zou, Xingzhou Liang, Shiyuan Meng, Pinlong Cai, Xingcheng Xu, Jingjing Qu</dc:creator>
    </item>
    <item>
      <title>An Algebraic Foundation for Knowledge Graph Construction (Extended Version)</title>
      <link>https://arxiv.org/abs/2503.10385</link>
      <description>arXiv:2503.10385v2 Announce Type: replace 
Abstract: Although they exist since more than ten years already, have attracted diverse implementations, and have been used successfully in a significant number of applications, declarative mapping languages for constructing knowledge graphs from heterogeneous types of data sources still lack a solid formal foundation. This makes it impossible to introduce implementation and optimization techniques that are provably correct and, in fact, has led to discrepancies between different implementations. Moreover, it precludes studying fundamental properties of different languages (e.g., expressive power). To address this gap, this paper introduces a language-agnostic algebra for capturing mapping definitions. As further contributions, we show that the popular mapping language RML can be translated into our algebra (by which we also provide a formal definition of the semantics of RML) and we prove several algebraic rewriting rules that can be used to optimize mapping plans based on our algebra.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10385v2</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sitt Min Oo, Olaf Hartig</dc:creator>
    </item>
    <item>
      <title>Balancing Content Size in RAG-Text2SQL System</title>
      <link>https://arxiv.org/abs/2502.15723</link>
      <description>arXiv:2502.15723v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have emerged as a promising solution for converting natural language queries into SQL commands, enabling seamless database interaction. However, these Text-to-SQL (Text2SQL) systems face inherent limitations, hallucinations, outdated knowledge, and untraceable reasoning. To address these challenges, the integration of retrieval-augmented generation (RAG) with Text2SQL models has gained traction. RAG serves as a retrieval mechanism, providing essential contextual information, such as table schemas and metadata, to enhance the query generation process. Despite their potential, RAG + Text2SQL systems are susceptible to the quality and size of retrieved documents. While richer document content can improve schema relevance and retrieval accuracy, it also introduces noise, increasing the risk of hallucinations and reducing query fidelity as the prompt size of the Text2SQL model increases. This research investigates the nuanced trade-off between document size and quality, aiming to strike a balance that optimizes system performance. Key thresholds are identified where performance degradation occurs, along with actionable strategies to mitigate these challenges. Additionally, we explore the phenomenon of hallucinations in Text2SQL models, emphasizing the critical role of curated document presentation in minimizing errors. Our findings provide a roadmap for enhancing the robustness of RAG + Text2SQL systems, offering practical insights for real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15723v3</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Prakhar Gurawa, Anjali Dharmik</dc:creator>
    </item>
  </channel>
</rss>
