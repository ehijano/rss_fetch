<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Aug 2025 04:04:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Semi-discrete Active Flux as a Petrov-Galerkin method</title>
      <link>https://arxiv.org/abs/2508.15017</link>
      <description>arXiv:2508.15017v1 Announce Type: new 
Abstract: Active Flux (AF) is a recent numerical method for hyperbolic conservation laws, whose degrees of freedom are averages/moments and (shared) point values at cell interfaces. It has been noted previously in a heuristic fashion that it thus combines ideas from Finite Volume/Discontinuous Galerkin (DG) methods with a continuous approximation common in continuous Finite Element (CG) methods. This work shows that the semi-discrete Active Flux method on Cartesian meshes can be obtained from a variational formulation through a particular choice of (biorthogonal) test functions. These latter being discontinuous, the new formulation emphasizes the intermediate nature of AF between DG and CG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15017v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Wasilij Barsukow</dc:creator>
    </item>
    <item>
      <title>Error Estimation for Adaptive Mesh Refinement in Droplet Simulations</title>
      <link>https://arxiv.org/abs/2508.15081</link>
      <description>arXiv:2508.15081v1 Announce Type: new 
Abstract: We present a one-dimensional shear force driven droplet formation model with a flux-based error estimation. The presented model is derived using asymptotic expansion and a front-tracking method to simulate the droplet interface. The model is then discretized using the Galerkin finite element method in the mixed form. However, the jumps in the solution gradients are discontinuous and can grow faster due to the highly convective pinch-off process. This leads to an erroneous droplet interface and incorrect curvature. Therefore, the mesh must be sufficiently refined to capture the interface accurately. The mixed form of the governing equation naturally provides smooth interface gradients that can be used to compute the error estimate. The computed error estimate is then used to drive the adaptive mesh refinement algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15081v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.flu-dyn</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Darsh Nathawani, Matthew Knepley</dc:creator>
    </item>
    <item>
      <title>A Note on the Convergence of Symmetric Triangle Quadrature Rules</title>
      <link>https://arxiv.org/abs/2508.15133</link>
      <description>arXiv:2508.15133v1 Announce Type: new 
Abstract: Symmetric polynomial quadrature rules for triangles are commonly used to efficiently integrate two-dimensional domains in finite-element-type problems. While the development of such rules focuses on the maximum degree a given number of points can exactly integrate, smooth integrands are generally not polynomials of finite degree. Therefore, for such integrands, one needs to balance integration accuracy and computational cost. A natural approach to this balance is to choose the number of points such that the convergence rate with respect to the mesh size $h$ matches that of the other properties of the scheme, such as the planar or curved triangles that approximate the geometry or the basis functions that approximate the solution.
  In general, it is expected that a quadrature rule capable of integrating polynomials up to degree $d$ yields an integration error that is $\mathcal{O}(h^p)$, where $p=d+1$. However, as we describe in this paper, for symmetric triangle quadrature rules, when $d$ is even, $p=d+2$; therefore, for a $p^\text{th}$-order-accurate quadrature rule, fewer quadrature points are necessary, reducing the time required for matrix assembly in finite-element-type problems. This reduction in cost is modest for local differential operators that yield sparse matrices but appreciable for global integral operators that yield dense matrices.
  In this paper, we briefly summarize the details of symmetric triangle quadrature rules, discuss error implications for quadrature rules for one dimension and triangles, and we provide numerical examples that support our observation that polynomials that exactly integrate even maximum degrees converge faster than the conventional expectation for sequences of regular meshes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15133v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian A. Freno, Neil R. Matula, Joseph E. Bishop</dc:creator>
    </item>
    <item>
      <title>Reduced basis solvers for unfitted methods on parameterized domains</title>
      <link>https://arxiv.org/abs/2508.15320</link>
      <description>arXiv:2508.15320v1 Announce Type: new 
Abstract: In this paper, we present a unified framework for reduced basis approximations of parametrized partial differential equations defined on parameter-dependent domains. Our approach combines unfitted finite element methods with both classical and tensor-based reduced basis techniques -- particularly the tensor-train reduced basis method -- to enable efficient and accurate model reduction on general geometries. To address the challenge of reconciling geometric variability with fixed-dimensional snapshot representations, we adopt a deformation-based strategy that maps a reference configuration to each parameterized domain. Furthermore, we introduce a localization procedure to construct dictionaries of reduced subspaces and hyper-reduction approximations, which are obtained via matrix discrete empirical interpolation in our work. We extend the proposed framework to saddle-point problems by adapting the supremizer enrichment strategy to unfitted methods and deformed configurations, demonstrating that the supremizer operator can be defined on the reference configuration without loss of stability. Numerical experiments on two- and three-dimensional problems -- including Poisson, linear elasticity, incompressible Stokes and Navier-Stokes equations -- demonstrate the flexibility, accuracy and efficiency of the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15320v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas Mueller, Santiago Badia, Yiran Zhao</dc:creator>
    </item>
    <item>
      <title>Eig-PIELM: A Mesh-Free Approach for Efficient Eigen-Analysis with Physics-Informed Extreme Learning Machines</title>
      <link>https://arxiv.org/abs/2508.15343</link>
      <description>arXiv:2508.15343v1 Announce Type: new 
Abstract: In this work, a novel Eig-PIELM framework is proposed that extends physics-informed extreme learning machine for an efficient and accurate solution of linear eigenvalue problems. The method reformulates the governing differential equations into a compact algebraic system solvable in a single step. Boundary conditions are enforced exactly via an algebraic projection onto the boundary-admissible subspace, eliminating the computational overhead of penalty parameters, and backpropagation while preserving the computational advantages of extreme learning machines. The proposed framework is mesh-free and yields both eigenvalues and mode shapes simultaneously in one linear solve. The robustness and accuracy of the proposed framework is demonstrated through a range of benchmark problems. We believe that the mesh-free nature, solution structure and accuracy of Eig-PIELM makes it particularly valuable for parametric studies in mechanical, acoustic, and electromechanical systems where rapid frequency spectrum analysis is critical.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15343v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rishi Mishra,  Smriti, Ganapathy Krishnamurthi, Balaji Srinivasan, Sundararajan Natarajan</dc:creator>
    </item>
    <item>
      <title>Implementation of Milstein Schemes for Stochastic Delay-Differential Equations with Arbitrary Fixed Delays</title>
      <link>https://arxiv.org/abs/2508.15365</link>
      <description>arXiv:2508.15365v1 Announce Type: new 
Abstract: This paper develops methods for numerically solving stochastic delay-differential equations (SDDEs) with multiple fixed delays that do not align with a uniform time mesh. We focus on numerical schemes of strong convergence orders $1/2$ and $1$, such as the Euler--Maruyama and Milstein schemes, respectively. Although numerical schemes for SDDEs with delays $\tau_1,\ldots,\tau_K$ are theoretically established, their implementations require evaluations at both present times such as $t_n$, and also at delayed times such as $t_n-\tau_k$ and $t_n-\tau_l-\tau_k$. As a result, previous simulations of these schemes have been largely restricted to the case of divisible delays. We develop simulation techniques for the general case of indivisible delays where delayed times such as $t_n-\tau_k$ are not restricted to a uniform time mesh. To achieve order of convergence (OoC) $1/2$, we implement the schemes with a fixed step size while using linear interpolation to approximate delayed scheme values. To achieve OoC $1$, we construct an augmented time mesh that includes all time points required to evaluate the schemes, which necessitates using a varying step size. We also introduce a technique to simulate delayed iterated stochastic integrals on the augmented time mesh, by extending an established method from the divisible-delays setting. We then confirm that the numerical schemes achieve their theoretical convergence orders with computational examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15365v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mitchell T. Griggs, Kevin Burrage, Pamela M. Burrage</dc:creator>
    </item>
    <item>
      <title>Numerical Analysis of Unsupervised Learning Approaches for Parameter Identification in PDEs</title>
      <link>https://arxiv.org/abs/2508.15381</link>
      <description>arXiv:2508.15381v1 Announce Type: new 
Abstract: Identifying parameters in partial differential equations (PDEs) represents a very broad class of applied inverse problems. In recent years, several unsupervised learning approaches using (deep) neural networks have been developed to solve PDE parameter identifications. These approaches employ neural networks as ansatz functions to approximate the parameters and / or the states, and have demonstrated impressive empirical performance. In this paper, we provide a comprehensive survey on these unsupervised learning techniques on one model problem, diffusion coefficient identification, from the classical numerical analysis perspective, and outline a general framework for deriving rigorous error bounds on the discrete approximations obtained using the Galerkin finite element method, hybrid method and deep neural networks. Throughout we highlight the crucial role of conditional stability estimates in the error analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15381v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyu Cen, Bangti Jin, Qimeng Quan, Zhi Zhou</dc:creator>
    </item>
    <item>
      <title>Conditional Stability and Numerical Reconstruction of a Parabolic Inverse Source Problem Using Carleman Estimates</title>
      <link>https://arxiv.org/abs/2508.15406</link>
      <description>arXiv:2508.15406v1 Announce Type: new 
Abstract: In this work we develop a new numerical approach for recovering a spatially dependent source component in a standard parabolic equation from partial interior measurements. We establish novel conditional Lipschitz stability and H\"{o}lder stability for the inverse problem with and without boundary conditions, respectively, using suitable Carleman estimates. Then we propose a numerical approach for solving the inverse problem using conforming finite element approximations in both time and space. Moreover, by utilizing the conditional stability estimates, we prove rigorous error bounds on the discrete approximation. We present several numerical experiments to illustrate the effectiveness of the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15406v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianhao Hu, Xinchi Huang, Bangti Jin, Qimeng Quan, Zhi Zhou</dc:creator>
    </item>
    <item>
      <title>A Structure-Preserving Scheme for the Euler System with Potential Temperature Transport</title>
      <link>https://arxiv.org/abs/2508.15416</link>
      <description>arXiv:2508.15416v1 Announce Type: new 
Abstract: We consider the compressible Euler equations with potential temperature transport, a system widely used in atmospheric modelling to describe adiabatic, inviscid flows. In the low Mach number regime, the equations become stiff and pose significant numerical challenges. We develop an all-speed, semi-implicit finite volume scheme that is asymptotic preserving (AP) in the low Mach limit and strictly positivity preserving for density and potential temperature. The scheme ensures stability and accuracy across a broad range of Mach numbers, from fully compressible to nearly incompressible regimes. We rigorously establish consistency with both the compressible system and its incompressible, density-dependent limit. Numerical experiments confirm that the method robustly captures complex flow features while preserving the essential physical and mathematical structures of the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15416v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>K. R. Arun, Rahuldev Ghorai</dc:creator>
    </item>
    <item>
      <title>Exponential decay of the discrete energy for the wave-wave coupled system</title>
      <link>https://arxiv.org/abs/2508.15514</link>
      <description>arXiv:2508.15514v1 Announce Type: new 
Abstract: In this article, a numerical analysis of the asymptotic behavior of the discrete energy associated to a dissipative coupled wave system is conducted. The numerical approximation of the system is constructed using the P1 finite element method for spatial discretization, combined with the implicit Euler scheme for time integration. An a priori error analysis is established, showing that, under extra regularity assumptions on the continuous solution, the numerical scheme exhibits linear convergence. Then, for the first time in the literature, the exponential decay of the fully discrete energy is shown using the energy method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15514v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Toni Sayah, Toufic El Arwadi</dc:creator>
    </item>
    <item>
      <title>Weighted finite difference methods for the semiclassical nonlinear Schr\"odinger equation with multiphase oscillatory initial data</title>
      <link>https://arxiv.org/abs/2508.15683</link>
      <description>arXiv:2508.15683v1 Announce Type: new 
Abstract: This paper introduces weighted finite difference methods for numerically solving dispersive evolution equations with solutions that are highly oscillatory in both space and time. We consider a semiclassically scaled cubic nonlinear Schr\"odinger equation with highly oscillatory initial data, first in the single-phase case and then in the general multiphase case. The proposed methods do not need to resolve high-frequency oscillations in both space and time by prohibitively fine grids as would be required by standard finite difference methods. The approach taken here modifies traditional finite difference methods by appropriate exponential weights. Specifically, we propose the weighted leapfrog and weighted Crank--Nicolson methods, both of which achieve second-order accuracy with time steps and mesh sizes that are not restricted in magnitude by the small semiclassical parameter. Numerical experiments illustrate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15683v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanyan Shi, Christian Lubich</dc:creator>
    </item>
    <item>
      <title>Generative Neural Operators of Log-Complexity Can Simultaneously Solve Infinitely Many Convex Programs</title>
      <link>https://arxiv.org/abs/2508.14995</link>
      <description>arXiv:2508.14995v1 Announce Type: cross 
Abstract: Neural operators (NOs) are a class of deep learning models designed to simultaneously solve infinitely many related problems by casting them into an infinite-dimensional space, whereon these NOs operate. A significant gap remains between theory and practice: worst-case parameter bounds from universal approximation theorems suggest that NOs may require an unrealistically large number of parameters to solve most operator learning problems, which stands in direct opposition to a slew of experimental evidence. This paper closes that gap for a specific class of {NOs}, generative {equilibrium operators} (GEOs), using (realistic) finite-dimensional deep equilibrium layers, when solving families of convex optimization problems over a separable Hilbert space $X$. Here, the inputs are smooth, convex loss functions on $X$, and outputs are the associated (approximate) solutions to the optimization problem defined by each input loss.
  We show that when the input losses lie in suitable infinite-dimensional compact sets, our GEO can uniformly approximate the corresponding solutions to arbitrary precision, with rank, depth, and width growing only logarithmically in the reciprocal of the approximation error. We then validate both our theoretical results and the trainability of GEOs on three applications: (1) nonlinear PDEs, (2) stochastic optimal control problems, and (3) hedging problems in mathematical finance under liquidity constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14995v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>q-fin.CP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anastasis Kratsios, Ariel Neufeld, Philipp Schmocker</dc:creator>
    </item>
    <item>
      <title>Statistical conservation laws for scalar model problems: Hierarchical evolution equations</title>
      <link>https://arxiv.org/abs/2508.15359</link>
      <description>arXiv:2508.15359v1 Announce Type: cross 
Abstract: The probability density functions (PDFs) for the solution of the incompressible Navier-Stokes equation can be represented by a hierarchy of linear equations. This article develops new hierarchical evolution equations for PDFs of a scalar conservation law with random initial data as a model problem. Two frameworks are developed, including multi-point PDFs and single-point higher-order derivative PDFs. These hierarchies capture statistical correlations and guide closure strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15359v1</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qian Huang, Christian Rohde</dc:creator>
    </item>
    <item>
      <title>Hybrid Least Squares/Gradient Descent Methods for DeepONets</title>
      <link>https://arxiv.org/abs/2508.15394</link>
      <description>arXiv:2508.15394v1 Announce Type: cross 
Abstract: We propose an efficient hybrid least squares/gradient descent method to accelerate DeepONet training. Since the output of DeepONet can be viewed as linear with respect to the last layer parameters of the branch network, these parameters can be optimized using a least squares (LS) solve, and the remaining hidden layer parameters are updated by means of gradient descent form. However, building the LS system for all possible combinations of branch and trunk inputs yields a prohibitively large linear problem that is infeasible to solve directly. To address this issue, our method decomposes the large LS system into two smaller, more manageable subproblems $\unicode{x2014}$ one for the branch network and one for the trunk network $\unicode{x2014}$ and solves them separately. This method is generalized to a broader type of $L^2$ loss with a regularization term for the last layer parameters, including the case of unsupervised learning with physics-informed loss.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15394v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jun Choi, Chang-Ock Lee, Minam Moon</dc:creator>
    </item>
    <item>
      <title>A least-squares space-time approach for parabolic equations</title>
      <link>https://arxiv.org/abs/2305.03402</link>
      <description>arXiv:2305.03402v2 Announce Type: replace 
Abstract: We propose a least squares formulation for abstract parabolic equations in the natural $L^2(0,T;V^\star)\times H$ norm which only relies on natural regularity assumptions on the data of the problem. The resulting bilinear form then is symmetric, coercive and continuous. We provide two space-time Galerkin frameworks for the numerical approximation. The first one uses a conformal discretization of the underlying bilinear system and relies on the fact that the $V^*-$norm of basis functions can be evaluated exactly. The second approach is nonconforming an replaces the evaluation of the $V^*-$norm by a discrete pendant. We prove convergence for both approaches and illustrate our analytical findings by selected numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.03402v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Michael Hinze, Christian Kahle, Michael Stahl</dc:creator>
    </item>
    <item>
      <title>Tensor Train Decomposition for Adversarial Attacks on Computer Vision Models</title>
      <link>https://arxiv.org/abs/2312.12556</link>
      <description>arXiv:2312.12556v2 Announce Type: replace 
Abstract: Deep neural networks (DNNs) are widely used today, but they are vulnerable to adversarial attacks. To develop effective methods of defense, it is important to understand the potential weak spots of DNNs. Often attacks are organized taking into account the architecture of models (white-box approach) and based on gradient methods, but for real-world DNNs this approach in most cases is impossible. At the same time, several gradient-free optimization algorithms are used to attack black-box models. However, classical methods are often ineffective in the multidimensional case. To organize black-box attacks for computer vision models, in this work, we propose the use of an optimizer based on the low-rank tensor train (TT) format, which has gained popularity in various practical multidimensional applications in recent years. Combined with the attribution of the target image, which is built by the auxiliary (white-box) model, the TT-based optimization method makes it possible to organize an effective black-box attack by small perturbation of pixels in the target image. The superiority of the proposed approach over three popular baselines is demonstrated for seven modern DNNs on the ImageNet dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12556v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andrei Chertkov, Ivan Oseledets</dc:creator>
    </item>
    <item>
      <title>Parallel transport on matrix manifolds and Exponential Action</title>
      <link>https://arxiv.org/abs/2408.06054</link>
      <description>arXiv:2408.06054v2 Announce Type: replace 
Abstract: We express parallel transport for several common matrix Lie groups with a family of pseudo-Riemannian metrics in terms of matrix exponential and exponential actions. The metrics are constructed from a deformation of a bi-invariant metric and are naturally reductive. There is a similar picture for homogeneous spaces when taking quotients satisfying a general condition. In particular, for a Stiefel manifold of orthogonal matrices of size $n\times d$, we give an expression for parallel transport along a geodesic from time zero to $t$, that could be computed with time complexity of $O(n d^2)$ for small $t$, and of $O(td^3)$ for large $t$, contributing a step in a long-standing open problem in matrix manifolds. A similar result holds for {\it flag manifolds} with the canonical metric. We also show the parallel transport formulas for the {\it general linear group} and the {\it special orthogonal group} under these metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06054v2</guid>
      <category>math.NA</category>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Du Nguyen, Stefan Sommer</dc:creator>
    </item>
    <item>
      <title>A Method of Fundamental Solutions for Large-Scale 3D Elastance and Mobility Problems</title>
      <link>https://arxiv.org/abs/2409.04215</link>
      <description>arXiv:2409.04215v2 Announce Type: replace 
Abstract: The method of fundamental solutions (MFS) is known to be effective for solving 3D Laplace and Stokes Dirichlet boundary value problems in the exterior of a large collection of simple smooth objects. Here we present new scalable MFS formulations for the corresponding elastance and mobility problems. The elastance problem computes the potentials of conductors with given net charges, while the mobility problem -- crucial to rheology and complex fluid applications -- computes rigid body velocities given net forces and torques on the particles. The key idea is orthogonal projection of the net charge (or forces and torques) in a rectangular variant of a "completion flow". The proposal is compatible with one-body preconditioning, resulting in well-conditioned square linear systems amenable to fast multipole accelerated iterative solution, thus a cost linear in the particle number. For large suspensions with moderate lubrication forces, MFS sources on inner proxy-surfaces give accuracy on par with a well-resolved boundary integral formulation. Our several numerical tests include a suspension of 10000 nearby ellipsoids, using 26 million total preconditioned degrees of freedom, where GMRES converges to five digits of accuracy in under two hours on one workstation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04215v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.flu-dyn</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Broms, Alex H. Barnett, Anna-Karin Tornberg</dc:creator>
    </item>
    <item>
      <title>Rationally presented metric spaces and complexity, the case of the space of uniformly continuous real functions on a compact interval</title>
      <link>https://arxiv.org/abs/2502.13768</link>
      <description>arXiv:2502.13768v2 Announce Type: replace 
Abstract: We define the notion of {\em rational presentation of a complete metric space} in order to study metric spaces from the algorithmic complexity point of view. In this setting, we study some presentations of the space $\czu$ of uniformly continuous real functions over [0,1] with the usual norm: $\norme{f}_{\infty} = {\bf Sup} \{ \abs{f(x)} ; \;0 \leq x \leq 1\}.$ This allows us to have a comparison of a global kind between complexity notions attached to these presentations. In particular, we get a generalisation of Hoover's results concerning the {\sl Weierstrass approximation theorem in polynomial time}. We get also a generalisation of previous results on analytic functions which are computable in polynomial time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13768v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Theoretical Computer Science, 250 (1-2) 265--332, (2001)</arxiv:journal_reference>
      <dc:creator>Henri Lombardi, Salah Labhalla, E. Moutai</dc:creator>
    </item>
    <item>
      <title>Scalable augmented Lagrangian preconditioners for fictitious domain problems</title>
      <link>https://arxiv.org/abs/2504.11339</link>
      <description>arXiv:2504.11339v2 Announce Type: replace 
Abstract: We present preconditioning techniques to solve linear systems of equations with a block two-by-two and three-by-three structure arising from finite element discretizations of the fictitious domain method with Lagrange multipliers. In particular, we propose two augmented Lagrangian-based preconditioners to accelerate the convergence of iterative solvers for such classes of linear systems. We consider two relevant examples to illustrate the performance of these preconditioners when used in conjunction with flexible GMRES: the Poisson and the Stokes fictitious domain problems. A spectral analysis is established for both exact and inexact versions of the preconditioners. We show the effectiveness of the proposed approach and the robustness of our preconditioning strategy through extensive numerical tests in both two and three dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11339v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michele Benzi, Marco Feder, Luca Heltai, Federica Mugnaioni</dc:creator>
    </item>
    <item>
      <title>Deep regularization networks for inverse problems with noisy operators</title>
      <link>https://arxiv.org/abs/2506.07008</link>
      <description>arXiv:2506.07008v2 Announce Type: replace 
Abstract: A supervised learning approach is proposed for regularization of large inverse problems where the main operator is built from noisy data. This is germane to superresolution imaging via the sampling indicators of the inverse scattering theory. We aim to accelerate the spatiotemporal regularization process for this class of inverse problems to enable real-time imaging. In this approach, a neural operator maps each pattern on the right-hand side of the scattering equation to its affiliated regularization parameter. The network is trained in two steps which entails: (1) training on low-resolution regularization maps furnished by the Morozov discrepancy principle with nonoptimal thresholds, and (2) optimizing network predictions through minimization of the Tikhonov loss function regulated by the validation loss. Step 2 allows for tailoring of the approximate maps of Step 1 toward construction of higher quality images. This approach enables direct learning from test data and dispenses with the need for a-priori knowledge of the optimal regularization maps. The network, trained on low-resolution data, quickly generates dense regularization maps for high-resolution imaging. We highlight the importance of the training loss function on the network's generalizability. In particular, we demonstrate that networks informed by the logic of discrepancy principle lead to images of higher contrast. In this case, the training process involves many-objective optimization. We propose a new method to adaptively select the appropriate loss weights during training without requiring an additional optimization process. The proposed approach is synthetically examined for imaging damage evolution in an elastic plate. The results indicate that the discrepancy-informed regularization networks not only accelerate the imaging process, but also remarkably enhance the image quality in complex environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07008v2</guid>
      <category>math.NA</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>eess.SP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fatemeh Pourahmadian, Yang Xu</dc:creator>
    </item>
    <item>
      <title>Mirror Descent for Stochastic Control Problems with Measure-valued Controls</title>
      <link>https://arxiv.org/abs/2401.01198</link>
      <description>arXiv:2401.01198v2 Announce Type: replace-cross 
Abstract: This paper studies the convergence of the mirror descent algorithm for finite horizon stochastic control problems with measure-valued control processes. The control objective involves a convex regularisation function, denoted as $h$, with regularisation strength determined by the weight $\tau\ge 0$. The setting covers regularised relaxed control problems. Under suitable conditions, we establish the relative smoothness and convexity of the control objective with respect to the Bregman divergence of $h$, and prove linear convergence of the algorithm for $\tau=0$ and exponential convergence for $\tau&gt;0$. The results apply to common regularisers including relative entropy, $\chi^2$-divergence, and entropic Wasserstein costs. This validates recent reinforcement learning heuristics that adding regularisation accelerates the convergence of gradient methods. The proof exploits careful regularity estimates of backward stochastic differential equations in the bounded mean oscillation norm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01198v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bekzhan Kerimkulov, David \v{S}i\v{s}ka, {\L}ukasz Szpruch, Yufei Zhang</dc:creator>
    </item>
  </channel>
</rss>
