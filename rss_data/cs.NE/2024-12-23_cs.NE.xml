<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Dec 2024 03:45:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Functional connectomes of neural networks</title>
      <link>https://arxiv.org/abs/2412.15279</link>
      <description>arXiv:2412.15279v1 Announce Type: new 
Abstract: The human brain is a complex system, and understanding its mechanisms has been a long-standing challenge in neuroscience. The study of the functional connectome, which maps the functional connections between different brain regions, has provided valuable insights through various advanced analysis techniques developed over the years. Similarly, neural networks, inspired by the brain's architecture, have achieved notable success in diverse applications but are often noted for their lack of interpretability. In this paper, we propose a novel approach that bridges neural networks and human brain functions by leveraging brain-inspired techniques. Our approach, grounded in the insights from the functional connectome, offers scalable ways to characterize topology of large neural networks using stable statistical and machine learning techniques. Our empirical analysis demonstrates its capability to enhance the interpretability of neural networks, providing a deeper understanding of their underlying mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15279v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tananun Songdechakraiwut, Yutong Wu</dc:creator>
    </item>
    <item>
      <title>Variable Metric Evolution Strategies for High-dimensional Multi-Objective Optimization</title>
      <link>https://arxiv.org/abs/2412.15647</link>
      <description>arXiv:2412.15647v1 Announce Type: new 
Abstract: We design a class of variable metric evolution strategies well suited for high-dimensional problems. We target problems with many variables, not (necessarily) with many objectives. The construction combines two independent developments: efficient algorithms for scaling covariance matrix adaptation to high dimensions, and evolution strategies for multi-objective optimization. In order to design a specific instance of the class we first develop a (1+1) version of the limited memory matrix adaptation evolution strategy and then use an established standard construction to turn a population thereof into a state-of-the-art multi-objective optimizer with indicator-based selection. The method compares favorably to adaptation of the full covariance matrix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15647v1</guid>
      <category>cs.NE</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tobias Glasmachers</dc:creator>
    </item>
    <item>
      <title>Stalling in Space: Attractor Analysis for any Algorithm</title>
      <link>https://arxiv.org/abs/2412.15848</link>
      <description>arXiv:2412.15848v1 Announce Type: new 
Abstract: Network-based representations of fitness landscapes have grown in popularity in the past decade; this is probably because of growing interest in explainability for optimisation algorithms. Local optima networks (LONs) have been especially dominant in the literature and capture an approximation of local optima and their connectivity in the landscape. However, thus far, LONs have been constructed according to a strict definition of what a local optimum is: the result of local search. Many evolutionary approaches do not include this, however. Popular algorithms such as CMA-ES have therefore never been subject to LON analysis. Search trajectory networks (STNs) offer a possible alternative: nodes can be any search space location. However, STNs are not typically modelled in such a way that models temporal stalls: that is, a region in the search space where an algorithm fails to find a better solution over a defined period of time. In this work, we approach this by systematically analysing a special case of STN which we name attractor networks. These offer a coarse-grained view of algorithm behaviour with a singular focus on stall locations. We construct attractor networks for CMA-ES, differential evolution, and random search for 24 noiseless black-box optimisation benchmark problems. The properties of attractor networks are systematically explored. They are also visualised and compared to traditional LONs and STN models. We find that attractor networks facilitate insights into algorithm behaviour which other models cannot, and we advocate for the consideration of attractor analysis even for algorithms which do not include local search.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15848v1</guid>
      <category>cs.NE</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah L. Thomson, Quentin Renau, Diederick Vermetten, Emma Hart, Niki van Stein, Anna V. Kononova</dc:creator>
    </item>
    <item>
      <title>Fair Distributed Machine Learning with Imbalanced Data as a Stackelberg Evolutionary Game</title>
      <link>https://arxiv.org/abs/2412.16079</link>
      <description>arXiv:2412.16079v1 Announce Type: cross 
Abstract: Decentralised learning enables the training of deep learning algorithms without centralising data sets, resulting in benefits such as improved data privacy, operational efficiency and the fostering of data ownership policies. However, significant data imbalances pose a challenge in this framework. Participants with smaller datasets in distributed learning environments often achieve poorer results than participants with larger datasets. Data imbalances are particularly pronounced in medical fields and are caused by different patient populations, technological inequalities and divergent data collection practices.
  In this paper, we consider distributed learning as an Stackelberg evolutionary game. We present two algorithms for setting the weights of each node's contribution to the global model in each training round: the Deterministic Stackelberg Weighting Model (DSWM) and the Adaptive Stackelberg Weighting Model (ASWM). We use three medical datasets to highlight the impact of dynamic weighting on underrepresented nodes in distributed learning. Our results show that the ASWM significantly favours underrepresented nodes by improving their performance by 2.713% in AUC. Meanwhile, nodes with larger datasets experience only a modest average performance decrease of 0.441%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16079v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.GT</category>
      <category>cs.NE</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Niehaus, Ingo Roeder, Nico Scherf</dc:creator>
    </item>
    <item>
      <title>A Differentiable Model for Optimizing the Genetic Drivers of Synaptogenesis</title>
      <link>https://arxiv.org/abs/2402.07242</link>
      <description>arXiv:2402.07242v2 Announce Type: replace 
Abstract: There is a growing consensus among neuroscientists that many neural circuits critical for survival result from a process of genomic decompression, hence are constructed based on the information contained within the genome. Aligning with this perspective, we introduce SynaptoGen, a novel computational framework designed to bring the advent of synthetic biological intelligence closer, facilitating the development of neural biological agents through the precise control of genetic factors governing synaptogenesis. SynaptoGen represents the first model in the well-established family of Connectome Models (CMs) to offer a possible mechanistic explanation of synaptic multiplicity based on genetic expression and protein interaction probabilities, modeling connectivity with unprecedented granularity. Furthermore, SynaptoGen connects these genetic factors through a differentiable function, effectively working as a neural network in which each synaptic weight is computed as the average number of synapses between neurons, multiplied by its corresponding conductance, and derived from a specific genetic profile. Differentiability is a critical feature of the framework, enabling its integration with gradient-based optimization techniques. This allows SynaptoGen to generate patterns of genetic expression and/or genetic rules capable of producing pre-wired biological agents tailored to specific tasks. The framework is validated in simulated synaptogenesis scenarios with varying degrees of biological plausibility. It successfully produces biological agents capable of solving tasks in four different reinforcement learning benchmarks, consistently outperforming the state-of-the-art and a control baseline designed to represent populations of neurons where synapses form freely, i.e., without guided manipulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07242v2</guid>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tommaso Boccato, Matteo Ferrante, Nicola Toschi</dc:creator>
    </item>
    <item>
      <title>LayerAct: Advanced Activation Mechanism for Robust Inference of CNNs</title>
      <link>https://arxiv.org/abs/2306.04940</link>
      <description>arXiv:2306.04940v4 Announce Type: replace-cross 
Abstract: In this work, we propose a novel activation mechanism called LayerAct for CNNs. This approach is motivated by our theoretical and experimental analyses, which demonstrate that Layer Normalization (LN) can mitigate a limitation of existing activation functions regarding noise robustness. However, LN is known to be disadvantageous in CNNs due to its tendency to make activation outputs homogeneous. The proposed method is designed to be more robust than existing activation functions by reducing the upper bound of influence caused by input shifts without inheriting LN's limitation. We provide analyses and experiments showing that LayerAct functions exhibit superior robustness compared to ElementAct functions. Experimental results on three clean and noisy benchmark datasets for image classification tasks indicate that LayerAct functions outperform other activation functions in handling noisy datasets while achieving superior performance on clean datasets in most cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.04940v4</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.NE</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kihyuk Yoon, Chiehyeon Lim</dc:creator>
    </item>
    <item>
      <title>Faithful and Accurate Self-Attention Attribution for Message Passing Neural Networks via the Computation Tree Viewpoint</title>
      <link>https://arxiv.org/abs/2406.04612</link>
      <description>arXiv:2406.04612v2 Announce Type: replace-cross 
Abstract: The self-attention mechanism has been adopted in various popular message passing neural networks (MPNNs), enabling the model to adaptively control the amount of information that flows along the edges of the underlying graph. Such attention-based MPNNs (Att-GNNs) have also been used as a baseline for multiple studies on explainable AI (XAI) since attention has steadily been seen as natural model interpretations, while being a viewpoint that has already been popularized in other domains (e.g., natural language processing and computer vision). However, existing studies often use naive calculations to derive attribution scores from attention, undermining the potential of attention as interpretations for Att-GNNs. In our study, we aim to fill the gap between the widespread usage of Att-GNNs and their potential explainability via attention. To this end, we propose GATT, edge attribution calculation method for self-attention MPNNs based on the computation tree, a rooted tree that reflects the computation process of the underlying model. Despite its simplicity, we empirically demonstrate the effectiveness of GATT in three aspects of model explanation: faithfulness, explanation accuracy, and case studies by using both synthetic and real-world benchmark datasets. In all cases, the results demonstrate that GATT greatly improves edge attribution scores, especially compared to the previous naive approach. Our code is available at https://github.com/jordan7186/GAtt.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04612v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>cs.NE</category>
      <category>cs.SI</category>
      <category>math.IT</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yong-Min Shin, Siqing Li, Xin Cao, Won-Yong Shin</dc:creator>
    </item>
  </channel>
</rss>
