<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Dec 2025 05:00:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Identifying and extracting Data Access Statements from full-text academic articles</title>
      <link>https://arxiv.org/abs/2512.00001</link>
      <description>arXiv:2512.00001v1 Announce Type: new 
Abstract: A Data Access Statement (DAS) is a formal declaration detailing how and where the underlying research data associated with a publication can be accessed. It promotes transparency, reproducibility, and compliance with funder and publisher data-sharing requirements. Funders such as Plan S, the European Union, UKRI, and NIH emphasise the inclusion of DAS in publications, underscoring its growing importance. While a DAS enhances research by increasing transparency, discoverability, and data quality while clarifying access protocols and elevating datasets as first-class research outputs, the repository community faces challenges in managing and curating DAS as a standard metadata component. Manual DAS curation remains labour-intensive and time-consuming, hindering efficient data-sharing practices. CORE has co-designed with the repository community a module that uses machine learning to identify and extract DAS from full-text articles. This tool facilitates the automated encoding, curation, and validation of DAS within metadata, reducing manual workload and improving metadata quality. This integration aligns with CORE's objective to enhance repository services by providing enriched metadata and supporting compliance with funder requirements. By streamlining DAS management and expanding metadata frameworks, CORE contributes to a more accessible and interconnected scholarly ecosystem, fostering data discoverability and reuse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00001v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Pride, Matteo Cancellieri, Petr Knoth</dc:creator>
    </item>
    <item>
      <title>Prompt perturbation and fraction facilitation sometimes strengthen Large Language Model scores</title>
      <link>https://arxiv.org/abs/2512.01330</link>
      <description>arXiv:2512.01330v1 Announce Type: new 
Abstract: Large Language Models (LLMs) can be tasked with scoring texts according to pre-defined criteria and on a defined scale, but there is no recognised optimal prompting strategy for this. This article focuses on the task of LLMs scoring journal articles for research quality on a four-point scale, testing how user prompt design can enhance this ability. Based primarily on 1.7 million Gemma3 27b queries for 2780 health and life science articles with 58 similar prompts, the results show that improvements can be obtained by (a) testing semantically equivalent prompt variations, (b) averaging scores from semantically equivalent prompts, (c) specifying that fractional scores are allowed, and possibly also (d) not drawing attention to the input being partial. Whilst (a) and (d) suggests that models can be sensitive to how a task is phrased, (b) and (c) suggest that strategies to leverage more of the model's knowledge are helpful, such as by perturbing prompts and facilitating fractions. Perhaps counterintuitively, encouraging incorrect answers (fractions for this task) releases useful information about the model's certainty about its answers. Mixing semantically equivalent prompts also reduces the chance of getting no score for an input. Additional testing showed that the best prompts vary between LLMs, however, and were almost the opposite for ChatGPT 4o-mini, weakly aligned for Llama4 Scout and Magistral, and made little difference to Qwen3 32b and DeepSeek R1 32b. Overall, whilst there is no single best prompt, a good strategy for all models was to average the scores from a range of different semantically equivalent or similar prompts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01330v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mike Thelwall</dc:creator>
    </item>
    <item>
      <title>Estimating the prevalence of LLM-assisted text in scholarly writing</title>
      <link>https://arxiv.org/abs/2512.01560</link>
      <description>arXiv:2512.01560v1 Announce Type: new 
Abstract: The use of large language models (LLMs) in scholarly publications has grown dramatically since the launch of ChatGPT in late 2022. This usage is often undisclosed, and it can be challenging for readers and reviewers to identify human written but LLM-revised or translated text, or predominantly LLM-generated text. Given the known quality and reliability issues connected with LLM-generated text, their potential growth poses an increasing problem for research integrity, and for public trust in research.
  This study presents a simple and easily reproducible methodology to show the growth in the full text of published papers, across the full range of research, as indexed in the Dimensions database. It uses this to demonstrate that LLM tools are likely to have been involved in the production of more than 10% of all published papers in 2024, based on disproportionate use of specific indicative words, and draws together earlier studies to confirm that this is a plausible overall estimate.
  It then discusses the implications of this for the integrity of scholarly publishing, highlighting evidence that use of LLMs for text generation is still being concealed or downplayed by authors, and presents an argument that more comprehensive disclosure requirements are urgently required to address this.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01560v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Gray</dc:creator>
    </item>
    <item>
      <title>Mapping the Landscape of Open Access Dashboards - A Dataset for Research and Infrastructure Development</title>
      <link>https://arxiv.org/abs/2512.01669</link>
      <description>arXiv:2512.01669v1 Announce Type: new 
Abstract: As Open Access continues to gain importance in science policy, understanding the proportion of Open Access publications relative to the total research output of research-performing organizations, individual countries, or even globally has become increasingly relevant. In response, dashboards are being developed to capture and communicate progress in this area. To provide an overview of these dashboards and their characteristics, an extensive survey was conducted, resulting in the identification of nearly 60 dashboards. To support a detailed and structured description, a dedicated metadata schema was developed, and the identified dashboards were systematically indexed accordingly. To foster community engagement and ensure ongoing development, a participatory process was launched, allowing interested stakeholders to contribute to the dataset. The dataset is particularly relevant for researchers in Library and Information Science (LIS) and Science and Technology Studies (STS), supporting both empirical analyses of Open Access and the methodological refinement of indicators and policy instruments in the context of Open Science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01669v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johannes Schneider, Heinz Pampel</dc:creator>
    </item>
    <item>
      <title>SHRAG: AFrameworkfor Combining Human-Inspired Search with RAG</title>
      <link>https://arxiv.org/abs/2512.00772</link>
      <description>arXiv:2512.00772v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) is gaining recognition as one of the key technological axes for next generation information retrieval, owing to its ability to mitigate the hallucination phenomenon in Large Language
  Models (LLMs)and effectively incorporate up-to-date information. However, specialized expertise is necessary to
  construct ahigh-quality retrieval system independently; moreover, RAGdemonstratesrelativelyslowerprocessing
  speeds compared to conventional pure retrieval systems because it involves both retrieval and generation stages.
  Accordingly, this study proposes SHRAG, a novel framework designed to facilitate the seamless integration of
  Information Retrieval and RAG while simultaneously securing precise retrieval performance. SHRAG utilizes a
  Large Language Model as a Query Strategist to automatically transform unstructured natural language queries
  into logically structured search queries, subsequently performing Boolean retrieval to emulate the search process
  of an expert human searcher. Furthermore, it incorporates multilingual query expansion and a multilingual
  embedding model, enabling it to perform efficient cross-lingual question answering within the multilingual
  dataset environment of the ScienceON Challenge. Experimental results demonstrate that the proposed method,
  combining logical retrieval capabilities and generative reasoning, can significantly enhance the accuracy and
  reliability of RAG systems. Furthermore, SHRAG movesbeyondconventionaldocument-centric retrieval methods,
  presenting the potential for a new search paradigm capable of providing direct and reliable responses to queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00772v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.DL</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hyunseok Ryu, Wonjune Shin, Hyun Park</dc:creator>
    </item>
    <item>
      <title>A Core Ontology for Particle Accelerators: Interoperable Data and Workflows Across Facilities</title>
      <link>https://arxiv.org/abs/2512.00868</link>
      <description>arXiv:2512.00868v1 Announce Type: cross 
Abstract: We propose a small, shared core ontology for particle accelerators that provides a semantic backbone for interoperable data and workflows across facilities. The ontology names key device types, signals, parameters, and regions, and relates them through explicit properties (e.g., hasSetpoint, hasReadback, partOf). Each site contributes a lightweight facility bundle, a profile that maps local conventions into the shared vocabulary plus data slices that instantiate those mappings, without renaming channel addresses or changing existing systems. Using standard W3C technologies, the approach supports both sparse and rich descriptions. We demonstrate the idea on two beamline segments at different laboratories. A single semantic query is expressed once and evaluated against both knowledge bases, returning the locally correct PVs. The ontology thereby enables not only portable workflows but also interoperable data, since measurements and catalogs are annotated with shared semantics rather than facility-specific names. The framework complements, rather than replaces, existing middle layers and lattice/data standards, and it creates a stable foundation for reusable tools and agentic workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00868v1</guid>
      <category>physics.acc-ph</category>
      <category>cs.DL</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chris Tennant</dc:creator>
    </item>
    <item>
      <title>OpenDORS: A dataset of openly referenced open research software</title>
      <link>https://arxiv.org/abs/2512.01570</link>
      <description>arXiv:2512.01570v1 Announce Type: cross 
Abstract: In many academic disciplines, software is created during the research process or for a research purpose. The crucial role of software for research is increasingly acknowledged. The application of software engineering to research software has been formalized as research software engineering, to create better software that enables better research. Despite this, large-scale studies of research software and its development are still lacking. To enable such studies, we present a dataset of 134,352 unique open research software projects and 134,154 source code repositories referenced in open access literature. Each dataset record identifies the referencing publication and lists source code repositories of the software project. For 122,425 source code repositories, the dataset provides metadata on latest versions, license information, programming languages and descriptive metadata files. We summarize the distributions of these features in the dataset and describe additional software metadata that extends the dataset in future work. Finally, we suggest examples of research that could use the dataset to develop a better understanding of research software practice in RSE research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01570v1</guid>
      <category>cs.SE</category>
      <category>cs.DL</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephan Druskat, Lars Grunske</dc:creator>
    </item>
    <item>
      <title>A Formalization of the Ionescu-Tulcea Theorem in Mathlib</title>
      <link>https://arxiv.org/abs/2506.18616</link>
      <description>arXiv:2506.18616v4 Announce Type: replace-cross 
Abstract: We describe the formalization of the Ionescu-Tulcea theorem, showing the existence of a probability measure on the space of trajectories of a Markov chain, in the proof assistant Lean using the integrated library Mathlib. We first present a mathematical proof before exposing the difficulties which arise when trying to formalize it, and how they were overcome. We then build on this work to formalize the construction of the product of an arbitrary family of probability measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18616v4</guid>
      <category>math.PR</category>
      <category>cs.DL</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Etienne Marion (ENS de Lyon)</dc:creator>
    </item>
  </channel>
</rss>
