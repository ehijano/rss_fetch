<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 Aug 2024 04:00:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 01 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Dataset for Multi-intensity Continuous Human Activity Recognition through Passive Sensing</title>
      <link>https://arxiv.org/abs/2407.21125</link>
      <description>arXiv:2407.21125v1 Announce Type: new 
Abstract: Human activity recognition (HAR) is essential in healthcare, elder care, security, and human-computer interaction. The use of precise sensor data to identify activities passively and continuously makes HAR accessible and ubiquitous. Specifically, millimeter wave (mmWave) radar is promising for passive and continuous HAR due to its ability to penetrate non-metallic materials and provide high-resolution wireless sensing. Although mmWave sensors are effective at capturing macro-scale activities, like exercising, they fail to capture micro-scale activities, such as typing. In this paper, we introduce mmDoppler, a novel dataset that utilizes off-the-shelf (COTS) mmWave radar in order to capture both macro and micro-scale human movements using a machine-learning driven signal processing pipeline. The dataset includes seven subjects performing 19 distinct activities and employs adaptive doppler resolution to enhance activity recognition. By adjusting the radar's doppler resolution based on the activity type, our system captures subtle movements more precisely. mmDoppler includes range-doppler heatmaps, offering detailed motion dynamics, with data collected in a controlled environment with single as well as multiple subjects performing activities simultaneously. The dataset aims to bridge the gap in HAR systems by providing a more comprehensive and detailed resource for improving the robustness and accuracy of mmWave radar activity recognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21125v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Argha Sen, Anirban Das, Swadhin Pradhan, Sandip Chakraborty</dc:creator>
    </item>
    <item>
      <title>A Survey on Exploratory Spatiotemporal Visual Analytics Approaches for Climate Science</title>
      <link>https://arxiv.org/abs/2407.21199</link>
      <description>arXiv:2407.21199v1 Announce Type: new 
Abstract: Climate science produces a wealth of complex, high-dimensional, multivariate data from observations and numerical models. These data are critical for understanding climate changes and their socioeconomic impacts. Climate scientists are continuously evaluating output from numerical models against observations. This model evaluation process provides useful guidance to improve the numerical models and subsequent climate projections. Exploratory visual analytics systems possess the potential to significantly reduce the burden on scientists for traditional spatiotemporal analyses. In addition, technology and infrastructure advancements are further facilitating broader access to climate data. Climate scientists today can access climate data in distributed analytic environments and render exploratory visualizations for analyses. Efforts are ongoing to optimize the computational efficiency of spatiotemporal analyses to enable efficient exploration of massive data. These advances present further opportunities for the visualization community to innovate over the full landscape of challenges and requirements raised by scientists. In this report, we provide a comprehensive review of the challenges, requirements, and current approaches for exploratory spatiotemporal visual analytics solutions for climate data. We categorize the visual analytic techniques, systems, and tools presented in the relevant literature based on task requirements, data sources, statistical techniques, interaction methods, visualization techniques, performance evaluation methods, and application domains. Moreover, our analytic review identifies trends, limitations, and key challenges in visual analysis. This report will advance future research activities in climate visualizations and enables the end-users of climate data to identify effective climate change mitigation strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21199v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdullah-Al-Raihan Nayeem, Dongyun Han, Huikyo Lee, Donghoon Kim, Daniel Feldman, William J. Tolone, Daniel Crichton, Isaac Cho</dc:creator>
    </item>
    <item>
      <title>Algorithm-Assisted Decision Making and Racial Disparities in Housing: A Study of the Allegheny Housing Assessment Tool</title>
      <link>https://arxiv.org/abs/2407.21209</link>
      <description>arXiv:2407.21209v1 Announce Type: new 
Abstract: The demand for housing assistance across the United States far exceeds the supply, leaving housing providers the task of prioritizing clients for receipt of this limited resource. To be eligible for federal funding, local homelessness systems are required to implement assessment tools as part of their prioritization processes. The Vulnerability Index Service Prioritization Decision Assistance Tool (VI-SPDAT) is the most commonly used assessment tool nationwide. Recent studies have criticized the VI-SPDAT as exhibiting racial bias, which may lead to unwarranted racial disparities in housing provision. Such criticisms have led certain jurisdictions to develop alternative tools. Using data from one such prioritization tool, called the Allegheny Housing Assessment (AHA), we use descriptive and quantitative analysis to assess whether the replacement of the VI-SPDAT with the AHA impacts racial disparities in housing allocation. We find that the VI-SPDAT tended to assign higher risk scores to white clients and lower risk scores to Black clients, and that white clients were served at a higher rates pre-AHA deployment. While post-deployment service decisions became better aligned with the AHA score, and the distribution of AHA scores is similar across racial groups, we do not find evidence of a corresponding decrease in disparities in service rates. We attribute the persistent disparity to the use of Alt-AHA, a survey-based tool that is used in cases of low data quality, as well as group differences in eligibility-related factors, such as chronic homelessness and veteran status. We discuss the implications for housing service systems seeking to reduce racial disparities in their service delivery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21209v1</guid>
      <category>cs.HC</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lingwei Cheng, Cameron Drayton, Alexandra Chouldechova, Rhema Vaithianathan</dc:creator>
    </item>
    <item>
      <title>Mixing Linters with GUIs: A Color Palette Design Probe</title>
      <link>https://arxiv.org/abs/2407.21285</link>
      <description>arXiv:2407.21285v1 Announce Type: new 
Abstract: Visualization linters are end-user facing evaluators that automatically identify potential chart issues. These spell-checker like systems offer a blend of interpretability and customization that is not found in other forms of automated assistance. However, existing linters do not model context and have primarily targeted users who do not need assistance, resulting in obvious -- even annoying -- advice. We investigate these issues within the domain of color palette design, which serves as a microcosm of visualization design concerns. We contribute a GUI-based color palette linter as a design probe that covers perception, accessibility, context, and other design criteria, and use it to explore visual explanations, integrated fixes, and user defined linting rules. Through a formative interview study and theory-driven analysis, we find that linters can be meaningfully integrated into graphical contexts thereby addressing many of their core issues. We discuss implications for integrating linters into visualization tools, developing improved assertion languages, and supporting end-user tunable advice -- all laying the groundwork for more effective visualization linters in any context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21285v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew McNutt, Maureen C. Stone, Jeffrey Heer</dc:creator>
    </item>
    <item>
      <title>Who should I trust? A Visual Analytics Approach for Comparing Net Load Forecasting Models</title>
      <link>https://arxiv.org/abs/2407.21299</link>
      <description>arXiv:2407.21299v1 Announce Type: new 
Abstract: Net load forecasting is crucial for energy planning and facilitating informed decision-making regarding trade and load distributions. However, evaluating forecasting models' performance against benchmark models remains challenging, thereby impeding experts' trust in the model's performance. In this context, there is a demand for technological interventions that allow scientists to compare models across various timeframes and solar penetration levels. This paper introduces a visual analytics-based application designed to compare the performance of deep-learning-based net load forecasting models with other models for probabilistic net load forecasting. This application employs carefully selected visual analytic interventions, enabling users to discern differences in model performance across different solar penetration levels, dataset resolutions, and hours of the day over multiple months. We also present observations made using our application through a case study, demonstrating the effectiveness of visualizations in aiding scientists in making informed decisions and enhancing trust in net load forecasting models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21299v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kaustav Bhattacharjee, Soumya Kundu, Indrasis Chakraborty, Aritra Dasgupta</dc:creator>
    </item>
    <item>
      <title>Exploring the Role of Social Support when Integrating Generative AI into Small Business Workflows</title>
      <link>https://arxiv.org/abs/2407.21404</link>
      <description>arXiv:2407.21404v1 Announce Type: new 
Abstract: Small business owners stand to benefit from generative AI technologies due to limited resources, yet they must navigate increasing legal and ethical risks. In this paper, we interview 11 entrepreneurs and support personnel to investigate existing practices of how entrepreneurs integrate generative AI technologies into their business workflows. Specifically, we build on scholarship in HCI which emphasizes the role of small, offline networks in supporting entrepreneurs' technology maintenance. We detail how entrepreneurs resourcefully leveraged their local networks to discover new use cases of generative AI (e.g., by sharing accounts), assuage heightened techno-anxieties (e.g., by recruiting trusted confidants), overcome barriers to sustained use (e.g., by receiving wrap-around support), and establish boundaries of use. Further, we suggest how generative AI platforms may be redesigned to better support entrepreneurs, such as by taking into account the benefits and tensions of use in a social context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21404v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3678884.3681895</arxiv:DOI>
      <dc:creator>Quentin Romero Lauro, Jeffrey P. Bigham, Yasmine Kotturi</dc:creator>
    </item>
    <item>
      <title>Designing Harvesting Tools for Olive Trees: Methodological Reflections on Exploring and Incorporating Plant Perspectives in the Early Stages of Design Process</title>
      <link>https://arxiv.org/abs/2407.21481</link>
      <description>arXiv:2407.21481v1 Announce Type: new 
Abstract: Sustainability-focused design research is witnessing a change in approach with the emergence of More-than-human Design (MTHD), which challenges human-centered thinking by incorporating nonhuman perspectives into the design process. However, implementing MTHD presents challenges for design researchers and practitioners, such as understanding non-verbal species. Despite the techniques developed to facilitate such an understanding (e.g. contact zone), the growing literature on MTHD lacks studies reflecting on how these techniques are utilized in the design process. In this paper, we present a case study on designing olive harvesting tools from a MTH lens, where designers used contact zone, plant interviews, plant persona, and experience map to explore the perspectives of olive trees and incorporate them into ideas in collaboration with farmers and agricultural engineers. The results indicate the significance of reconsidering decentralization in MTHD from the standpoint of entanglements among techniques and incorporating various knowledge types to manage tensions arising from perspective shifts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21481v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Berre Su Yanl{\i}\c{c}, Aykut Co\c{s}kun</dc:creator>
    </item>
    <item>
      <title>The Impacts of AI Avatar Appearance and Disclosure on User Motivation</title>
      <link>https://arxiv.org/abs/2407.21521</link>
      <description>arXiv:2407.21521v1 Announce Type: new 
Abstract: This study examines the influence of perceived AI features on user motivation in virtual interactions. AI avatars, being disclosed as being an AI, or embodying specific genders, could be used in user-AI interactions. Leveraging insights from AI and avatar research, we explore how AI disclosure and gender affect user motivation. We conducted a game-based experiment involving over 72,500 participants who solved search problems alone or with an AI companion. Different groups experienced varying AI appearances and disclosures. We measured play intensity. Results revealed that the presence of another avatar led to less intense play compared to solo play. Disclosure of the avatar as AI heightened effort intensity compared to non-disclosed AI companions. Additionally, a masculine AI appearance reduced effort intensity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21521v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Boele Visser, Peter van der Putten, Amirhossein Zohrehvand</dc:creator>
    </item>
    <item>
      <title>LLM-for-X: Application-agnostic Integration of Large Language Models to Support Personal Writing Workflows</title>
      <link>https://arxiv.org/abs/2407.21593</link>
      <description>arXiv:2407.21593v1 Announce Type: new 
Abstract: To enhance productivity and to streamline workflows, there is a growing trend to embed large language model (LLM) functionality into applications, from browser-based web apps to native apps that run on personal computers. Here, we introduce LLM-for-X, a system-wide shortcut layer that seamlessly augments any application with LLM services through a lightweight popup dialog. Our native layer seamlessly connects front-end applications to popular LLM backends, such as ChatGPT and Gemini, using their uniform chat front-ends as the programming interface or their custom API calls. We demonstrate the benefits of LLM-for-X across a wide variety of applications, including Microsoft Office, VSCode, and Adobe Acrobat as well as popular web apps such as Overleaf. In our evaluation, we compared LLM-for-X with ChatGPT's web interface in a series of tasks, showing that our approach can provide users with quick, efficient, and easy-to-use LLM assistance without context switching to support writing and reading tasks that is agnostic of the specific application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21593v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas Teufelberger, Xintong Liu, Zhipeng Li, Max Moebus, Christian Holz</dc:creator>
    </item>
    <item>
      <title>A State-of-the-Art Review of Computational Models for Analyzing Longitudinal Wearable Sensor Data in Healthcare</title>
      <link>https://arxiv.org/abs/2407.21665</link>
      <description>arXiv:2407.21665v1 Announce Type: new 
Abstract: Wearable devices are increasingly used as tools for biomedical research, as the continuous stream of behavioral and physiological data they collect can provide insights about our health in everyday contexts. Long-term tracking, defined in the timescale of months of year, can provide insights of patterns and changes as indicators of health changes. These insights can make medicine and healthcare more predictive, preventive, personalized, and participative (The 4P's). However, the challenges in modeling, understanding and processing longitudinal data are a significant barrier to their adoption in research studies and clinical settings. In this paper, we review and discuss three models used to make sense of longitudinal data: routines, rhythms and stability metrics. We present the challenges associated with the processing and analysis of longitudinal wearable sensor data, with a special focus on how to handle the different temporal dynamics at various granularities. We then discuss current limitations and identify directions for future work. This review is essential to the advancement of computational modeling and analysis of longitudinal sensor data for pervasive healthcare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21665v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paula Lago</dc:creator>
    </item>
    <item>
      <title>Decomposed Prompting to Answer Questions on a Course Discussion Board</title>
      <link>https://arxiv.org/abs/2407.21170</link>
      <description>arXiv:2407.21170v1 Announce Type: cross 
Abstract: We propose and evaluate a question-answering system that uses decomposed prompting to classify and answer student questions on a course discussion board. Our system uses a large language model (LLM) to classify questions into one of four types: conceptual, homework, logistics, and not answerable. This enables us to employ a different strategy for answering questions that fall under different types. Using a variant of GPT-3, we achieve $81\%$ classification accuracy. We discuss our system's performance on answering conceptual questions from a machine learning course and various failure modes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21170v1</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-36336-8_33</arxiv:DOI>
      <arxiv:journal_reference>In: Artificial Intelligence in Education. AIED 2023. Communications in Computer and Information Science, vol 1831. Springer, Cham</arxiv:journal_reference>
      <dc:creator>Brandon Jaipersaud, Paul Zhang, Jimmy Ba, Andrew Petersen, Lisa Zhang, Michael R. Zhang</dc:creator>
    </item>
    <item>
      <title>FedBChain: A Blockchain-enabled Federated Learning Framework for Improving DeepConvLSTM with Comparative Strategy Insights</title>
      <link>https://arxiv.org/abs/2407.21282</link>
      <description>arXiv:2407.21282v1 Announce Type: cross 
Abstract: Recent research in the field of Human Activity Recognition has shown that an improvement in prediction performance can be achieved by reducing the number of LSTM layers. However, this kind of enhancement is only significant on monolithic architectures, and when it runs on large-scale distributed training, data security and privacy issues will be reconsidered, and its prediction performance is unknown. In this paper, we introduce a novel framework: FedBChain, which integrates the federated learning paradigm based on a modified DeepConvLSTM architecture with a single LSTM layer. This framework performs comparative tests of prediction performance on three different real-world datasets based on three different hidden layer units (128, 256, and 512) combined with five different federated learning strategies, respectively. The results show that our architecture has significant improvements in Precision, Recall and F1-score compared to the centralized training approach on all datasets with all hidden layer units for all strategies: FedAvg strategy improves on average by 4.54%, FedProx improves on average by 4.57%, FedTrimmedAvg improves on average by 4.35%, Krum improves by 4.18% on average, and FedAvgM improves by 4.46% on average. Based on our results, it can be seen that FedBChain not only improves in performance, but also guarantees the security and privacy of user data compared to centralized training methods during the training process. The code for our experiments is publicly available (https://github.com/Glen909/FedBChain).</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21282v1</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gaoxuan Li, Chern Hong Lim, Qiyao Ma, Xinyu Tang, Hwa Hui Tew</dc:creator>
    </item>
    <item>
      <title>Does the Source of a Warning Matter? Examining the Effectiveness of Veracity Warning Labels Across Warners</title>
      <link>https://arxiv.org/abs/2407.21592</link>
      <description>arXiv:2407.21592v1 Announce Type: cross 
Abstract: In this study, we conducted an online, between-subjects experiment (N = 2,049) to better understand the impact of warning label sources on information trust and sharing intentions. Across four warners (the social media platform, other social media users, Artificial Intelligence (AI), and fact checkers), we found that all significantly decreased trust in false information relative to control, but warnings from AI were modestly more effective. All warners significantly decreased the sharing intentions of false information, except warnings from other social media users. AI was again the most effective. These results were moderated by prior trust in media and the information itself. Most noteworthy, we found that warning labels from AI were significantly more effective than all other warning labels for participants who reported a low trust in news organizations, while warnings from AI were no more effective than any other warning label for participants who reported a high trust in news organizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21592v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin D. Horne</dc:creator>
    </item>
    <item>
      <title>Between the AI and Me: Analysing Listeners' Perspectives on AI- and Human-Composed Progressive Metal Music</title>
      <link>https://arxiv.org/abs/2407.21615</link>
      <description>arXiv:2407.21615v1 Announce Type: cross 
Abstract: Generative AI models have recently blossomed, significantly impacting artistic and musical traditions. Research investigating how humans interact with and deem these models is therefore crucial. Through a listening and reflection study, we explore participants' perspectives on AI- vs human-generated progressive metal, in symbolic format, using rock music as a control group. AI-generated examples were produced by ProgGP, a Transformer-based model. We propose a mixed methods approach to assess the effects of generation type (human vs. AI), genre (progressive metal vs. rock), and curation process (random vs. cherry-picked). This combines quantitative feedback on genre congruence, preference, creativity, consistency, playability, humanness, and repeatability, and qualitative feedback to provide insights into listeners' experiences. A total of 32 progressive metal fans completed the study. Our findings validate the use of fine-tuning to achieve genre-specific specialization in AI music generation, as listeners could distinguish between AI-generated rock and progressive metal. Despite some AI-generated excerpts receiving similar ratings to human music, listeners exhibited a preference for human compositions. Thematic analysis identified key features for genre and AI vs. human distinctions. Finally, we consider the ethical implications of our work in promoting musical data diversity within MIR research by focusing on an under-explored genre.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21615v1</guid>
      <category>cs.SD</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>eess.AS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Pedro Sarmento, Jackson Loth, Mathieu Barthet</dc:creator>
    </item>
    <item>
      <title>Does empirical evidence from healthy aging studies predict a practical difference between visualizations for different age groups?</title>
      <link>https://arxiv.org/abs/2407.21767</link>
      <description>arXiv:2407.21767v1 Announce Type: cross 
Abstract: When communicating critical information to decision-makers, one of the major challenges in visualization is whether the communication is affected by different perceptual or cognitive abilities, one major influencing factor is age. We review both visualization and psychophysics literature to understand where quantitative evidence exists on age differences in visual perception. Using contrast sensitivity data from the literature we show how the differences between visualizations for different age groups can be predicted using a new model of visible frequency range with age. The model assumed that at threshold values some visual data will not be visible to older people (spatial frequency &gt; 2 and contrast &lt;=0.01). We apply this result to a practical visualization and show an example that at higher levels of contrast, the visual signal should be perceivable by all viewers over 20. Universally usable visualization should use a contrast of 0.02 or higher and be designed to avoid spatial frequencies greater than eight cycles per degree to accommodate all ages. There remains much research to do on to translate psychophysics results to practical quantitative guidelines for visualization producers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21767v1</guid>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S. Shao, Y. Li, A. I. Meso, N. Holliman</dc:creator>
    </item>
    <item>
      <title>EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos</title>
      <link>https://arxiv.org/abs/2406.10750</link>
      <description>arXiv:2406.10750v2 Announce Type: replace 
Abstract: Self-recording eating behaviors is a step towards a healthy lifestyle recommended by many health professionals. However, the current practice of manually recording eating activities using paper records or smartphone apps is often unsustainable and inaccurate. Smart glasses have emerged as a promising wearable form factor for tracking eating behaviors, but existing systems primarily identify when eating occurs without capturing details of the eating activities (E.g., what is being eaten). In this paper, we present EchoGuide, an application and system pipeline that leverages low-power active acoustic sensing to guide head-mounted cameras to capture egocentric videos, enabling efficient and detailed analysis of eating activities. By combining active acoustic sensing for eating detection with video captioning models and large-scale language models for retrieval augmentation, EchoGuide intelligently clips and analyzes videos to create concise, relevant activity records on eating. We evaluated EchoGuide with 9 participants in naturalistic settings involving eating activities, demonstrating high-quality summarization and significant reductions in video data needed, paving the way for practical, scalable eating activity tracking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10750v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vineet Parikh, Saif Mahmud, Devansh Agarwal, Ke Li, Fran\c{c}ois Guimbreti\`ere, Cheng Zhang</dc:creator>
    </item>
    <item>
      <title>V-RECS, a Low-Cost LLM4VIS Recommender with Explanations, Captioning and Suggestions</title>
      <link>https://arxiv.org/abs/2406.15259</link>
      <description>arXiv:2406.15259v2 Announce Type: replace 
Abstract: NL2VIS (natural language to visualization) is a promising and recent research area that involves interpreting natural language queries and translating them into visualizations that accurately represent the underlying data. As we navigate the era of big data, NL2VIS holds considerable application potential since it greatly facilitates data exploration by non-expert users. Following the increasingly widespread usage of generative AI in NL2VIS applications, in this paper we present V-RECS, the first LLM-based Visual Recommender augmented with explanations(E), captioning(C), and suggestions(S) for further data exploration. V-RECS' visualization narratives facilitate both response verification and data exploration by non-expert users. Furthermore, our proposed solution mitigates computational, controllability, and cost issues associated with using powerful LLMs by leveraging a methodology to effectively fine-tune small models. To generate insightful visualization narratives, we use Chain-of-Thoughts (CoT), a prompt engineering technique to help LLM identify and generate the logical steps to produce a correct answer. Since CoT is reported to perform poorly with small LLMs, we adopted a strategy in which a large LLM (GPT-4), acting as a Teacher, generates CoT-based instructions to fine-tune a small model, Llama-2-7B, which plays the role of a Student. Extensive experiments-based on a framework for the quantitative evaluation of AI-based visualizations and on manual assessment by a group of participants-show that V-RECS achieves performance scores comparable to GPT-4, at a much lower cost. The efficacy of the V-RECS teacher-student paradigm is also demonstrated by the fact that the un-tuned Llama fails to perform the task in the vast majority of test cases. We release V-RECS for the visualization community to assist visualization designers throughout the entire visualization generation process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15259v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Luca Podo, Marco Angelini, Paola Velardi</dc:creator>
    </item>
    <item>
      <title>Steps Towards an Infrastructure for Scholarly Synthesis</title>
      <link>https://arxiv.org/abs/2407.20666</link>
      <description>arXiv:2407.20666v2 Announce Type: replace 
Abstract: Sharing, reusing, and synthesizing knowledge is central to the research process, both individually, and with others. These core functions are not supported by our formal scholarly publishing infrastructure: instead of the smooth functioning of functional infrastructure, researchers resort to laborious "hacks" and workarounds to "mine" publications for what they need, and struggle to efficiently share the resulting information with others. Information scientists have proposed an alternative infrastructure based on the more appropriately granular model of a discourse graph of claims, and evidence, along with key rhetorical relationships between them. However, despite significant technical progress on standards and platforms, the predominant infrastructure remains steadfastly document-based. Drawing from infrastructure studies, we locate the current infrastructural bottlenecks in the lack of local systems that integrate discourse-centric models to augment synthesis work, from which an infrastructure for synthesis can be grown. Through 3 years of research through design and field deployment in a distributed community of hypertext notebook users, we elaborate a design vision of what can and should be built in order to grow a discourse-centric synthesis infrastructure: a thriving "installed base" of researchers authoring local, shareable discourse graphs to improve synthesis work, enhance primary research and research training, and augment collaborative research. We discuss how this design vision -- and our empirical work -- contributes steps towards a new infrastructure for synthesis, and increases HCI's capacity to advance collective intelligence and solve infrastructure-level problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20666v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joel Chan, Matthew Akamatsu, David Vargas, Lukas Kawerau, Michael Gartner</dc:creator>
    </item>
    <item>
      <title>Knowledge Mechanisms in Large Language Models: A Survey and Perspective</title>
      <link>https://arxiv.org/abs/2407.15017</link>
      <description>arXiv:2407.15017v2 Announce Type: replace-cross 
Abstract: Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial for advancing towards trustworthy AGI. This paper reviews knowledge mechanism analysis from a novel taxonomy including knowledge utilization and evolution. Knowledge utilization delves into the mechanism of memorization, comprehension and application, and creation. Knowledge evolution focuses on the dynamic progression of knowledge within individual and group LLMs. Moreover, we discuss what knowledge LLMs have learned, the reasons for the fragility of parametric knowledge, and the potential dark knowledge (hypothesis) that will be challenging to address. We hope this work can help understand knowledge in LLMs and provide insights for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15017v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengru Wang, Yunzhi Yao, Ziwen Xu, Shuofei Qiao, Shumin Deng, Peng Wang, Xiang Chen, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang</dc:creator>
    </item>
  </channel>
</rss>
