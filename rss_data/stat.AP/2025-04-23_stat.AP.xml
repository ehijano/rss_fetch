<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 24 Apr 2025 01:40:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>ORKM: An R Package for Online Multi-View Data</title>
      <link>https://arxiv.org/abs/2504.15402</link>
      <description>arXiv:2504.15402v1 Announce Type: new 
Abstract: We introduce a software package, denoted as ORKM, that incorporates the Online Regu larized K-Means Clustering (ORKMC) algorithm for processing online multi/single-view data. The
  function ORKMeans of the ORKMC utilizes a regularization term to address multi-view clustering
  problems with online updates. The package ORKM is capable of computing the classification results,
  cluster center matrices, and weights for each view of the multi-view data sets. Furthermore, it can
  handle branch multi/single-view data by transforming the online RKMC algorithm into an offline
  version, referred to as Regularized K-Means Clustering (RKMC). We demonstrate the effectiveness
  of the package through simulations and real data analysis, comparing it with several methods and
  related R packages. Our results indicate that the package is stable and produces good clustering
  outcomes</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15402v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miao Yu, Guangbao Guo</dc:creator>
    </item>
    <item>
      <title>Spatiotemporal Assessment of Aircraft Noise Exposure Using Mobile Phone-Derived Population Estimates and High-Resolution Noise Measurements</title>
      <link>https://arxiv.org/abs/2504.15617</link>
      <description>arXiv:2504.15617v1 Announce Type: new 
Abstract: Aircraft noise exposure has traditionally been assessed using static residential population data and long-term average noise metrics, often overlooking the dynamic nature of human mobility and temporal variations in operational conditions. This study proposes a data-driven framework that integrates high-resolution noise measurements from airport monitoring terminals with mobile phone-derived de facto population estimates to evaluate noise exposure with fine spatio-temporal resolution. We develop hourly noise exposure profiles and quantify the number of individuals affected across regions and time windows, using both absolute counts and inequality metrics such as Gini coefficients. This enables a nuanced examination of not only who is exposed, but when and where the burden is concentrated. At our case study airport, operational runway patterns resulted in recurring spatial shifts in noise exposure. By incorporating de facto population data, we demonstrate that identical noise operations can yield unequal impacts depending on the time and location of population presence, highlighting the importance of accounting for population dynamics in exposure assessment. Our approach offers a scalable basis for designing population-sensitive noise abatement strategies, contributing to more equitable and transparent aviation noise management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15617v1</guid>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soohwan Oh, Hyunsoo Cho, Jungwoo Cho</dc:creator>
    </item>
    <item>
      <title>Bayesian sample size calculations for external validation studies of risk prediction models</title>
      <link>https://arxiv.org/abs/2504.15923</link>
      <description>arXiv:2504.15923v1 Announce Type: new 
Abstract: Summary: Contemporary sample size calculations for external validation of risk prediction models require users to specify fixed values of assumed model performance metrics alongside target precision levels (e.g., 95% CI widths). However, due to the finite samples of previous studies, our knowledge of true model performance in the target population is uncertain, and so choosing fixed values represents an incomplete picture. As well, for net benefit (NB) as a measure of clinical utility, the relevance of conventional precision-based inference is doubtful. In this work, we propose a general Bayesian algorithm for constructing the joint distribution of predicted risks and response values based on summary statistics of model performance in previous studies. For statistical metrics of performance, we propose sample size determination rules that either target desired expected precision, or a desired assurance probability that the precision criteria will be satisfied. For NB, we propose rules based on optimality assurance (the probability that the planned study correctly identifies the most beneficial strategy) and the Expected Value of Sample Information (EVSI), the expected gain in NB from the planned validation study. We showcase these developments in a case study on the validation of a risk prediction model for deterioration of hospitalized COVID-19 patients. Compared to the conventional sample size calculation methods, a Bayesian approach requires explicit quantification of uncertainty around model performance, but thereby enables various sample size rules based on expected precision, assurance probabilities, and value of information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15923v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Sadatsafavi, Paul Gustafson, Solmaz Setayeshgar, Laure Wynants, Richard Riley</dc:creator>
    </item>
    <item>
      <title>Residual lifetime prediction for heterogeneous degradation data by Bayesian semi-parametric method</title>
      <link>https://arxiv.org/abs/2504.15794</link>
      <description>arXiv:2504.15794v1 Announce Type: cross 
Abstract: Degradation data are considered for assessing reliability in highly reliable systems. The usual assumption is that degradation units come from a homogeneous population. But in presence of high variability in the manufacturing process, this assumption is not true in general; that is different sub-populations are involved in the study. Predicting residual lifetime of a functioning unit is a major challenge in the degradation modeling especially in heterogeneous environment. To account for heterogeneous degradation data, we have proposed a Bayesian semi-parametric approach to relax the conventional modeling assumptions. We model the degradation path using Dirichlet process mixture of normal distributions. Based on the samples obtained from posterior distribution of model parameters we obtain residual lifetime distribution for individual unit. Transformation based MCMC technique is used for simulating values from the derived residual lifetime distribution for prediction of residual lifetime. A simulation study is undertaken to check performance of the proposed semi-parametric model compared with parametric model. Fatigue Crack Size data is analyzed to illustrate the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15794v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Barin Karmakar, Biswabrata Pradhan</dc:creator>
    </item>
    <item>
      <title>A Statistical Framework and Analysis for Perfect Radar Pulse Compression</title>
      <link>https://arxiv.org/abs/2308.07597</link>
      <description>arXiv:2308.07597v2 Announce Type: replace-cross 
Abstract: Perfect radar pulse compression coding is a potential emerging field which aims at providing rigorous analysis and fundamental limit radar experiments. It is based on finding non-trivial pulse codes, which we can make statistically equivalent, to the radar experiments carried out with elementary pulses of some shape. A common engineering-based radar experiment design, regarding pulse-compression, often omits the rigorous theory and mathematical limitations. In this work our aim is to develop a mathematical theory which coincides with understanding the radar experiment in terms of the theory of comparison of statistical experiments. We review and generalize some properties of the It\^{o} measure. We estimate the unknown i.e. the structure function in the context of Bayesian statistical inverse problems. We study the posterior for generalized $d$-dimensional inverse problems, where we consider both real-valued and complex-valued inputs for posteriori analysis. Finally this is then extended to the infinite dimensional setting, where our analysis suggests the underlying posterior is non-Gaussian.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.07597v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Neil K. Chada, Petteri Piiroinen, Lassi Roininen</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Genomic Data with Multiple Heterogeneous Outcomes</title>
      <link>https://arxiv.org/abs/2404.09119</link>
      <description>arXiv:2404.09119v4 Announce Type: replace-cross 
Abstract: With the evolution of single-cell RNA sequencing techniques into a standard approach in genomics, it has become possible to conduct cohort-level causal inferences based on single-cell-level measurements. However, the individual gene expression levels of interest are not directly observable; instead, only repeated proxy measurements from each individual's cells are available, providing a derived outcome to estimate the underlying outcome for each of many genes. In this paper, we propose a generic semiparametric inference framework for doubly robust estimation with multiple derived outcomes, which also encompasses the usual setting of multiple outcomes when the response of each unit is available. To reliably quantify the causal effects of heterogeneous outcomes, we specialize the analysis to standardized average treatment effects and quantile treatment effects. Through this, we demonstrate the use of the semiparametric inferential results for doubly robust estimators derived from both Von Mises expansions and estimating equations. A multiple testing procedure based on Gaussian multiplier bootstrap is tailored for doubly robust estimators to control the false discovery exceedance rate. Applications in single-cell CRISPR perturbation analysis and individual-level differential expression analysis demonstrate the utility of the proposed methods and offer insights into the usage of different estimands for causal inference in genomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09119v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin-Hong Du, Zhenghao Zeng, Edward H. Kennedy, Larry Wasserman, Kathryn Roeder</dc:creator>
    </item>
    <item>
      <title>Sampling effects on Lasso estimation of drift functions in high-dimensional diffusion processes</title>
      <link>https://arxiv.org/abs/2408.08638</link>
      <description>arXiv:2408.08638v2 Announce Type: replace-cross 
Abstract: In this paper, we address high-dimensional parametric estimation of the drift function in diffusion models, specifically focusing on a $d$-dimensional ergodic diffusion process observed at discrete time points. We consider both a general linear form for the drift function and the particular case of the Ornstein-Uhlenbeck (OU) process. Assuming sparsity of the parameter vector, we examine the statistical behavior of the Lasso estimator for the unknown parameter. Our primary contribution is the proof of an oracle inequality for the Lasso estimator, which holds on the intersection of three specific sets defined for our analysis. We carefully control the probability of these sets, tackling the central challenge of our study. This approach allows us to derive error bounds for the $l_1$ and $l_2$ norms, assessing the performance of the proposed Lasso estimator. Our results demonstrate that, under certain conditions, the discretization error becomes negligible, enabling us to achieve the same optimal rate of convergence as if the continuous trajectory of the process were observed. We validate our theoretical findings through numerical experiments, which show that the Lasso estimator significantly outperforms the maximum likelihood estimator (MLE) in terms of support recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08638v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Amorino, Francisco Pina, Mark Podolskij</dc:creator>
    </item>
    <item>
      <title>Estimation of conditional inequality curves and measures via estimating the conditional quantile function</title>
      <link>https://arxiv.org/abs/2412.20228</link>
      <description>arXiv:2412.20228v3 Announce Type: replace-cross 
Abstract: The classical concept of inequality curves and measures is extended to conditional inequality curves and measures and a curve of conditional inequality measures is introduced. This extension provides a more nuanced analysis of inequality in relation to covariates. In particular, this enables comparison of inequalities between subpopulations, conditioned on certain values of covariates. To estimate the curves and measures, a novel method for estimating the conditional quantile function is proposed. The method incorporates a modified quantile regression framework that employs isotonic regression to ensure that there is no quantile crossing. The consistency of the proposed estimators is proved while their finite sample performance is evaluated through simulation studies and compared with existing quantile regression approaches. Finally, practical application is demonstrated by analysing salary inequality across different employee age groups, highlighting the potential of conditional inequality measures in empirical research. The code used to prepare the results presented in this article is available in a dedicated GitHub repository.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20228v3</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alicja Jokiel-Rokita, Sylwester Pi\k{a}tek, Rafa{\l} Topolnicki</dc:creator>
    </item>
    <item>
      <title>A conceptual synthesis of causal assumptions for causal discovery and inference</title>
      <link>https://arxiv.org/abs/2504.11035</link>
      <description>arXiv:2504.11035v2 Announce Type: replace-cross 
Abstract: This work presents a conceptual synthesis of causal discovery and inference frameworks, with a focus on how foundational assumptions -- causal sufficiency, causal faithfulness, and the causal Markov condition -- are formalized and operationalized across methodological traditions. Through structured tables and comparative summaries, I map core assumptions, tasks, and analytical choices from multiple causal frameworks, highlighting their connections and differences. The synthesis provides practical guidance for researchers designing causal studies, especially in settings where observational or experimental constraints challenge standard approaches. This guide spans all phases of causal analysis, including question formulation, formalization of background knowledge, selection of appropriate frameworks, choice of study design or algorithm, and interpretation. It is intended as a tool to support rigorous causal reasoning across diverse empirical domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11035v2</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hannah E. Correia</dc:creator>
    </item>
  </channel>
</rss>
