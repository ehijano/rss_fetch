<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 Jan 2025 03:45:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Feature-based Evolutionary Diversity Optimization of Discriminating Instances for Chance-constrained Optimization Problems</title>
      <link>https://arxiv.org/abs/2501.14284</link>
      <description>arXiv:2501.14284v1 Announce Type: new 
Abstract: Algorithm selection is crucial in the field of optimization, as no single algorithm performs perfectly across all types of optimization problems. Finding the best algorithm among a given set of algorithms for a given problem requires a detailed analysis of the problem's features. To do so, it is important to have a diverse set of benchmarking instances highlighting the difference in algorithms' performance. In this paper, we evolve diverse benchmarking instances for chance-constrained optimization problems that contain stochastic components characterized by their expected values and variances. These instances clearly differentiate the performance of two given algorithms, meaning they are easy to solve by one algorithm and hard to solve by the other. We introduce a $(\mu+1)~EA$ for feature-based diversity optimization to evolve such differentiating instances. We study the chance-constrained maximum coverage problem with stochastic weights on the vertices as an example of chance-constrained optimization problems. The experimental results demonstrate that our method successfully generates diverse instances based on different features while effectively distinguishing the performance between a pair of algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14284v1</guid>
      <category>cs.NE</category>
      <category>math.OC</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saba Sadeghi Ahouei, Denis Antipov, Aneta Neumann, Frank Neumann</dc:creator>
    </item>
    <item>
      <title>Cascaded Large-Scale TSP Solving with Unified Neural Guidance: Bridging Local and Population-based Search</title>
      <link>https://arxiv.org/abs/2501.14285</link>
      <description>arXiv:2501.14285v1 Announce Type: new 
Abstract: The traveling salesman problem (TSP) is a fundamental NP-hard optimization problem. This work presents UNiCS, a novel unified neural-guided cascaded solver for solving large-scale TSP instances. UNiCS comprises a local search (LS) phase and a population-based search (PBS) phase, both guided by a learning component called unified neural guidance (UNG). Specifically, UNG guides solution generation across both phases and determines appropriate phase transition timing to effectively combine the complementary strengths of LS and PBS. While trained only on simple distributions with relatively small-scale TSP instances, UNiCS generalizes effectively to challenging TSP benchmarks containing much larger instances (10,000-71,009 nodes) with diverse node distributions entirely unseen during training. Experimental results on the large-scale TSP instances demonstrate that UNiCS consistently outperforms state-of-the-art methods, with its advantage remaining consistent across various runtime budgets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14285v1</guid>
      <category>cs.NE</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shengcai Liu, Haoze Lv, Zhiyuan Wang, Ke Tang</dc:creator>
    </item>
    <item>
      <title>$SpikePack$: Enhanced Information Flow in Spiking Neural Networks with High Hardware Compatibility</title>
      <link>https://arxiv.org/abs/2501.14484</link>
      <description>arXiv:2501.14484v1 Announce Type: new 
Abstract: Spiking Neural Networks (SNNs) hold promise for energy-efficient, biologically inspired computing. We identify substantial informatio loss during spike transmission, linked to temporal dependencies in traditional Leaky Integrate-and-Fire (LIF) neuron-a key factor potentially limiting SNN performance. Existing SNN architectures also underutilize modern GPUs, constrained by single-bit spike storage and isolated weight-spike operations that restrict computational efficiency. We introduce ${SpikePack}$, a neuron model designed to reduce transmission loss while preserving essential features like membrane potential reset and leaky integration. ${SpikePack}$ achieves constant $\mathcal{O}(1)$ time and space complexity, enabling efficient parallel processing on GPUs and also supporting serial inference on existing SNN hardware accelerators. Compatible with standard Artificial Neural Network (ANN) architectures, ${SpikePack}$ facilitates near-lossless ANN-to-SNN conversion across various networks. Experimental results on tasks such as image classification, detection, and segmentation show ${SpikePack}$ achieves significant gains in accuracy and efficiency for both directly trained and converted SNNs over state-of-the-art models. Tests on FPGA-based platforms further confirm cross-platform flexibility, delivering high performance and enhanced sparsity. By enhancing information flow and rethinking SNN-ANN integration, ${SpikePack}$ advances efficient SNN deployment across diverse hardware platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14484v1</guid>
      <category>cs.NE</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guobin Shen, Jindong Li, Tenglong Li, Dongcheng Zhao, Yi Zeng</dc:creator>
    </item>
    <item>
      <title>Channel-wise Parallelizable Spiking Neuron with Multiplication-free Dynamics and Large Temporal Receptive Fields</title>
      <link>https://arxiv.org/abs/2501.14490</link>
      <description>arXiv:2501.14490v1 Announce Type: new 
Abstract: Spiking Neural Networks (SNNs) are distinguished from Artificial Neural Networks (ANNs) for their sophisticated neuronal dynamics and sparse binary activations (spikes) inspired by the biological neural system. Traditional neuron models use iterative step-by-step dynamics, resulting in serial computation and slow training speed of SNNs. Recently, parallelizable spiking neuron models have been proposed to fully utilize the massive parallel computing ability of graphics processing units to accelerate the training of SNNs. However, existing parallelizable spiking neuron models involve dense floating operations and can only achieve high long-term dependencies learning ability with a large order at the cost of huge computational and memory costs. To solve the dilemma of performance and costs, we propose the mul-free channel-wise Parallel Spiking Neuron, which is hardware-friendly and suitable for SNNs' resource-restricted application scenarios. The proposed neuron imports the channel-wise convolution to enhance the learning ability, induces the sawtooth dilations to reduce the neuron order, and employs the bit shift operation to avoid multiplications. The algorithm for design and implementation of acceleration methods is discussed meticulously. Our methods are validated in neuromorphic Spiking Heidelberg Digits voices, sequential CIFAR images, and neuromorphic DVS-Lip vision datasets, achieving the best accuracy among SNNs. Training speed results demonstrate the effectiveness of our acceleration methods, providing a practical reference for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14490v1</guid>
      <category>cs.NE</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peng Xue, Wei Fang, Zhengyu Ma, Zihan Huang, Zhaokun Zhou, Yonghong Tian, Timoth\'ee Masquelier, Huihui Zhou</dc:creator>
    </item>
    <item>
      <title>Benchmarking global optimization techniques for unmanned aerial vehicle path planning</title>
      <link>https://arxiv.org/abs/2501.14503</link>
      <description>arXiv:2501.14503v1 Announce Type: new 
Abstract: The Unmanned Aerial Vehicle (UAV) path planning problem is a complex optimization problem in the field of robotics. In this paper, we investigate the possible utilization of this problem in benchmarking global optimization methods. We devise a problem instance generator and pick 56 representative instances, which we compare to established benchmarking suits through Exploratory Landscape Analysis to show their uniqueness. For the computational comparison, we select twelve well-performing global optimization techniques from both subfields of stochastic algorithms (evolutionary computation methods) and deterministic algorithms (Dividing RECTangles, or DIRECT-type methods). The experiments were conducted in settings with varying dimensionality and computational budgets. The results were analyzed through several criteria (number of best-found solutions, mean relative error, Friedman ranks) and utilized established statistical tests. The best-ranking methods for the UAV problems were almost universally the top-performing evolutionary techniques from recent competitions on numerical optimization at the Institute of Electrical and Electronics Engineers Congress on Evolutionary Computation. Lastly, we discussed the variable dimension characteristics of the studied UAV problems that remain still largely under-investigated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14503v1</guid>
      <category>cs.NE</category>
      <category>cs.RO</category>
      <category>math.OC</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mhd Ali Shehadeh, Jakub Kudela</dc:creator>
    </item>
    <item>
      <title>A Recurrent Spiking Network with Hierarchical Intrinsic Excitability Modulation for Schema Learning</title>
      <link>https://arxiv.org/abs/2501.14539</link>
      <description>arXiv:2501.14539v1 Announce Type: new 
Abstract: Schema, a form of structured knowledge that promotes transfer learning, is attracting growing attention in both neuroscience and artificial intelligence (AI). Current schema research in neural computation is largely constrained to a single behavioral paradigm and relies heavily on recurrent neural networks (RNNs) which lack the neural plausibility and biological interpretability. To address these limitations, this work first constructs a generalized behavioral paradigm framework for schema learning and introduces three novel cognitive tasks, thus supporting a comprehensive schema exploration. Second, we propose a new model using recurrent spiking neural networks with hierarchical intrinsic excitability modulation (HM-RSNNs). The top level of the model selects excitability properties for task-specific demands, while the bottom level fine-tunes these properties for intra-task problems. Finally, extensive visualization analyses of HM-RSNNs are conducted to showcase their computational advantages, track the intrinsic excitability evolution during schema learning, and examine neural coordination differences across tasks. Biologically inspired lesion studies further uncover task-specific distributions of intrinsic excitability within schemas. Experimental results show that HM-RSNNs significantly outperform RSNN baselines across all tasks and exceed RNNs in three novel cognitive tasks. Additionally, HM-RSNNs offer deeper insights into neural dynamics underlying schema learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14539v1</guid>
      <category>cs.NE</category>
      <category>cs.LG</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingchao Yu, Yaochu Jin, Yuchen Xiao, Yuping Yan</dc:creator>
    </item>
    <item>
      <title>Random-Key Algorithms for Optimizing Integrated Operating Room Scheduling</title>
      <link>https://arxiv.org/abs/2501.10243</link>
      <description>arXiv:2501.10243v2 Announce Type: replace 
Abstract: Efficient surgery room scheduling is essential for hospital efficiency, patient satisfaction, and resource utilization. This study addresses this challenge by introducing a novel concept of Random-Key Optimizer (RKO), rigorously tested on literature and new, real-world inspired instances. Our combinatorial optimization problem incorporates multi-room scheduling, equipment scheduling, and complex availability constraints for rooms, patients, and surgeons, facilitating rescheduling and enhancing operational flexibility. The RKO approach represents solutions as points in a continuous space, which are then mapped in the problem solution space via a deterministic function known as a decoder. The core idea is to operate metaheuristics and heuristics in the random-key space, unaware of the original solution space. We design the Biased Random-Key Genetic Algorithm with $Q$-Learning, Simulated Annealing, and Iterated Local Search for use within an RKO framework, employing a single decoder function. The proposed metaheuristics are complemented by lower-bound formulations, providing optimal gaps for evaluating the effectiveness of the heuristic results. Our results demonstrate significant lower and upper bounds improvements for the literature instances, notably proving one optimal result. Furthermore, the best-proposed metaheuristic efficiently generates schedules for the newly introduced instances, even in highly constrained scenarios. This research offers valuable insights and practical solutions for improving surgery scheduling processes, offering tangible benefits to hospitals by optimising resource allocation, reducing patient wait times, and enhancing overall operational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10243v2</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>math.CO</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bruno Salezze Vieira, Eduardo Machado Silva, Antonio Augusto Chaves</dc:creator>
    </item>
    <item>
      <title>QGAIC: Quantum Inspired Genetic Algorithm for Image Classification</title>
      <link>https://arxiv.org/abs/2501.11477</link>
      <description>arXiv:2501.11477v3 Announce Type: replace 
Abstract: This study uses two meta-heuristics methodologies to introduce two novel quantum-inspired meta heuristic approaches: quantum-inspired genetic algorithm (QIGA1) and quantum-inspired genetic algorithm with dynamic approach (QIGA2). The two suggested methods combine a classical and quantum genetic algorithm approach. Both approaches use The correlation coefficient as an assessment function to identify the best (optimal) values for binary image. In quantum computing, they use simple ideas like qubits and state superposition. Due to these characteristics, parallelism which uses the time discreteness of quantum mechanical systems, is exhibited. For five distinct MNIST datasets, the performance of all participating algorithms has been assessed by comparing the suggested approaches first with their traditional approach counterparts and then with the proposed methods QIGA1 and QIGA2. Each method's ideal threshold value, associated fitness value (best and average), loss, and accuracy for each MNIST dataset have all been published. The outcomes demonstrate the superior efficiency of the suggested approaches over their traditional equivalents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11477v3</guid>
      <category>cs.NE</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Akhilesh Kumar Singh, Kirankumar R. Hiremath</dc:creator>
    </item>
  </channel>
</rss>
