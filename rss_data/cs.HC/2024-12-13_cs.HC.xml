<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Dec 2024 05:00:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Audible Artefact: Promoting Cultural Exploration and Engagement with Audio Augmented Reality</title>
      <link>https://arxiv.org/abs/2412.08676</link>
      <description>arXiv:2412.08676v1 Announce Type: new 
Abstract: This paper introduces two ongoing projects where audio augmented reality is implemented as a means of engaging museum and gallery visitors with audio archive material and associated objects, artworks and artefacts. It outlines some of the issues surrounding the presentation and engagement with sound based material within the context of the cultural institution, discusses some previous and related work on approaches to the cultural application of audio augmented reality, and describes the research approach and methodology currently engaged with in developing an increased understanding in this area. Additionally, it discusses the project within the context of related cultural and sound studies literature, presents some initial conclusions as a result of a practice-based approach, and outlines the next steps for the project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08676v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3356590.3356617</arxiv:DOI>
      <arxiv:journal_reference>Audio Mostly 2019, Nottingham, UK (2019, September)</arxiv:journal_reference>
      <dc:creator>Laurence Cliffe, James Mansell, Joanne Cormac, Chris Greenhalgh, Adrian Hazzard</dc:creator>
    </item>
    <item>
      <title>Data Analysis on Speeding Behavior: The Impact of Auditory Warnings and Demographic Factors</title>
      <link>https://arxiv.org/abs/2412.08745</link>
      <description>arXiv:2412.08745v1 Announce Type: new 
Abstract: Speeding significantly contributes to traffic accidents, posing ongoing risks despite advancements in automotive safety technologies. This study investigates how auditory alerts influence speeding behavior across different demographic groups, focusing on drivers' age and experience levels. Using a mobile application to collect real-time driving data, we conducted a field study in Copenhagen/Denmark that included various driving environments and controlled auditory warnings for speed limit violations. Our results revealed that auditory alerts were unexpectedly associated with an increased frequency and duration of speeding incidents. The impact of these alerts varied by experience level: intermediate drivers showed reduced speeding duration in response to alerts, whereas novice and highly experienced drivers tended to speed for more extended periods after receiving alerts. These findings underscore the potential benefits of adaptive, experience-sensitive alert systems tailored to driver demographics, suggesting that personalized alerts may enhance safety more effectively than standardized approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08745v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Bank Lauridsen, Mads Greve Andersen, Max-Emil Smith Thorius, Fabricio Batista Narcizo</dc:creator>
    </item>
    <item>
      <title>Self-regulated Learning Processes in Secondary Education: A Network Analysis of Trace-based Measures</title>
      <link>https://arxiv.org/abs/2412.08921</link>
      <description>arXiv:2412.08921v1 Announce Type: new 
Abstract: While the capacity to self-regulate has been found to be crucial for secondary school students, prior studies often rely on self-report surveys and think-aloud protocols that present notable limitations in capturing self-regulated learning (SRL) processes. This study advances the understanding of SRL in secondary education by using trace data to examine SRL processes during multi-source writing tasks, with higher education participants included for comparison. We collected fine-grained trace data from 66 secondary school students and 59 university students working on the same writing tasks within a shared SRL-oriented learning environment. The data were labelled using Bannert's validated SRL coding scheme to reflect specific SRL processes, and we examined the relationship between these processes, essay performance, and educational levels. Using epistemic network analysis (ENA) to model and visualise the interconnected SRL processes in Bannert's coding scheme, we found that: (a) secondary school students predominantly engaged in three SRL processes -- Orientation, Re-reading, and Elaboration/Organisation; (b) high-performing secondary students engaged more in Re-reading, while low-performing students showed more Orientation process; and (c) higher education students exhibited more diverse SRL processes such as Monitoring and Evaluation than their secondary education counterparts, who heavily relied on following task instructions and rubrics to guide their writing. These findings highlight the necessity of designing scaffolding tools and developing teacher training programs to enhance awareness and development of SRL skills for secondary school learners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08921v1</guid>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706468.3706502</arxiv:DOI>
      <dc:creator>Yixin Cheng, Rui Guan, Tongguang Li, Mladen Rakovi\'c, Xinyu Li, Yizhou Fan, Flora Jin, Yi-Shan Tsai, Dragan Ga\v{s}evi\'c, Zachari Swiecki</dc:creator>
    </item>
    <item>
      <title>Predicting Quality of Video Gaming Experience Using Global-Scale Telemetry Data and Federated Learning</title>
      <link>https://arxiv.org/abs/2412.08950</link>
      <description>arXiv:2412.08950v1 Announce Type: new 
Abstract: Frames Per Second (FPS) significantly affects the gaming experience. Providing players with accurate FPS estimates prior to purchase benefits both players and game developers. However, we have a limited understanding of how to predict a game's FPS performance on a specific device. In this paper, we first conduct a comprehensive analysis of a wide range of factors that may affect game FPS on a global-scale dataset to identify the determinants of FPS. This includes player-side and game-side characteristics, as well as country-level socio-economic statistics. Furthermore, recognizing that accurate FPS predictions require extensive user data, which raises privacy concerns, we propose a federated learning-based model to ensure user privacy. Each player and game is assigned a unique learnable knowledge kernel that gradually extracts latent features for improved accuracy. We also introduce a novel training and prediction scheme that allows these kernels to be dynamically plug-and-play, effectively addressing cold start issues. To train this model with minimal bias, we collected a large telemetry dataset from 224 countries and regions, 100,000 users, and 835 games. Our model achieved a mean Wasserstein distance of 0.469 between predicted and ground truth FPS distributions, outperforming all baseline methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08950v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongyang Zhang, Jinhe Wen, Zixi Chen, Dara Arbab, Sruti Sahani, Bijan Arbab, Haojian Jin, Tauhidur Rahman</dc:creator>
    </item>
    <item>
      <title>The AI Interface: Designing for the Ideal Machine-Human Experience (Editorial)</title>
      <link>https://arxiv.org/abs/2412.09000</link>
      <description>arXiv:2412.09000v1 Announce Type: new 
Abstract: As artificial intelligence (AI) becomes increasingly embedded in daily life, designing intuitive, trustworthy, and emotionally resonant AI-human interfaces has emerged as a critical challenge. This editorial introduces a Special Issue that explores the psychology of AI experience design, focusing on how interfaces can foster seamless collaboration between humans and machines. Drawing on insights from diverse fields (healthcare, consumer technology, workplace dynamics, and cultural sector), the papers in this collection highlight the complexities of trust, transparency, and emotional sensitivity in human-AI interaction. Key themes include designing AI systems that align with user perceptions and expectations, overcoming resistance through transparency and trust, and framing AI capabilities to reduce user anxiety. By synthesizing findings from eight diverse studies, this editorial underscores the need for AI interfaces to balance efficiency with empathy, addressing both functional and emotional dimensions of user experience. Ultimately, it calls for actionable frameworks to bridge research and practice, ensuring that AI systems enhance human lives through thoughtful, human-centered design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09000v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aparna Sundar, Tony Russell-Rose, Udo Kruschwitz, Karen Machleit</dc:creator>
    </item>
    <item>
      <title>MindScratch: A Visual Programming Support Tool for Classroom Learning Based on Multimodal Generative AI</title>
      <link>https://arxiv.org/abs/2412.09001</link>
      <description>arXiv:2412.09001v1 Announce Type: new 
Abstract: Programming has become an essential component of K-12 education and serves as a pathway for developing computational thinking skills. Given the complexity of programming and the advanced skills it requires, previous research has introduced user-friendly tools to support young learners. However, our interviews with six programming educators revealed that current tools often fail to reflect classroom learning objectives, offer flexible, high-quality guidance, and foster student creativity. This highlights the need for more adaptive and reflective tools. Therefore, we introduced MindScratch, a multimodal generative AI (GAI) powered visual programming support tool. MindScratch aims to balance structured classroom activities with free programming creation, supporting students in completing creative programming projects based on teacher-set learning objectives while also providing programming scaffolding. Our user study results indicate that, compared to the baseline, MindScratch more effectively helps students achieve high-quality projects aligned with learning objectives. It also enhances students' computational thinking skills and creative thinking. Overall, we believe that GAI-driven educational tools like MindScratch offer students a focused and engaging learning experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09001v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunnong Chen, Shuhong Xiao, Yaxuan Song, Zejian Li, Lingyun Sun, Liuqing Chen</dc:creator>
    </item>
    <item>
      <title>Motor Imagery Classification for Asynchronous EEG-Based Brain-Computer Interfaces</title>
      <link>https://arxiv.org/abs/2412.09006</link>
      <description>arXiv:2412.09006v1 Announce Type: new 
Abstract: Motor imagery (MI) based brain-computer interfaces (BCIs) enable the direct control of external devices through the imagined movements of various body parts. Unlike previous systems that used fixed-length EEG trials for MI decoding, asynchronous BCIs aim to detect the user's MI without explicit triggers. They are challenging to implement, because the algorithm needs to first distinguish between resting-states and MI trials, and then classify the MI trials into the correct task, all without any triggers. This paper proposes a sliding window prescreening and classification (SWPC) approach for MI-based asynchronous BCIs, which consists of two modules: a prescreening module to screen MI trials out of the resting-state, and a classification module for MI classification. Both modules are trained with supervised learning followed by self-supervised learning, which refines the feature extractors. Within-subject and cross-subject asynchronous MI classifications on four different EEG datasets validated the effectiveness of SWPC, i.e., it always achieved the highest average classification accuracy, and outperformed the best state-of-the-art baseline on each dataset by about 2%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09006v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TNSRE.2024.3356916</arxiv:DOI>
      <arxiv:journal_reference>IEEE Trans. on Neural Systems and Rehabilitation Engineering, 32:527-536, 2024</arxiv:journal_reference>
      <dc:creator>Huanyu Wu, Siyang Li, Dongrui Wu</dc:creator>
    </item>
    <item>
      <title>Front-end Replication Dynamic Window (FRDW) for Online Motor Imagery Classification</title>
      <link>https://arxiv.org/abs/2412.09015</link>
      <description>arXiv:2412.09015v1 Announce Type: new 
Abstract: Motor imagery (MI) is a classical paradigm in electroencephalogram (EEG) based brain-computer interfaces (BCIs). Online accurate and fast decoding is very important to its successful applications. This paper proposes a simple yet effective front-end replication dynamic window (FRDW) algorithm for this purpose. Dynamic windows enable the classification based on a test EEG trial shorter than those used in training, improving the decision speed; front-end replication fills a short test EEG trial to the length used in training, improving the classification accuracy. Within-subject and cross-subject online MI classification experiments on three public datasets, with three different classifiers and three different data augmentation approaches, demonstrated that FRDW can significantly increase the information transfer rate in MI decoding. Additionally, FR can also be used in training data augmentation. FRDW helped win national champion of the China BCI Competition in 2022.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09015v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TNSRE.2023.3321640</arxiv:DOI>
      <arxiv:journal_reference>IEEE Trans. on Neural Systems and Rehabilitation Engineering, 31:3906-3914, 2023</arxiv:journal_reference>
      <dc:creator>X. Chen, J. An, H. Wu, S. Li, B. Liu, D. Wu</dc:creator>
    </item>
    <item>
      <title>Understanding Opportunities and Risks of Synthetic Relationships: Leveraging the Power of Longitudinal Research with Customised AI Tools</title>
      <link>https://arxiv.org/abs/2412.09086</link>
      <description>arXiv:2412.09086v1 Announce Type: new 
Abstract: This position paper discusses the benefits of longitudinal behavioural research with customised AI tools for exploring the opportunities and risks of synthetic relationships. Synthetic relationships are defined as "continuing associations between humans and AI tools that interact with one another wherein the AI tool(s) influence(s) humans' thoughts, feelings, and/or actions." (Starke et al., 2024). These relationships can potentially improve health, education, and the workplace, but they also bring the risk of subtle manipulation and privacy and autonomy concerns. To harness the opportunities of synthetic relationships and mitigate their risks, we outline a methodological approach that complements existing findings. We propose longitudinal research designs with self-assembled AI agents that enable the integration of detailed behavioural and self-reported data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09086v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>CONVERSATIONS 2024 - the 8th International Workshop on Chatbots and Human-Centred AI, hosted by CERTH, Thessaloniki, Greece</arxiv:journal_reference>
      <dc:creator>Alfio Ventura, Nils K\"obis</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Study on Dark Patterns</title>
      <link>https://arxiv.org/abs/2412.09147</link>
      <description>arXiv:2412.09147v1 Announce Type: new 
Abstract: As digital interfaces become increasingly prevalent, certain manipulative design elements have emerged that may harm user interests, raising associated ethical concerns and bringing dark patterns into focus as a significant research topic. Manipulative design strategies are widely used in user interfaces (UI) primarily to guide user behavior in ways that favor service providers, often at the cost of the users themselves. This paper addresses three main challenges in dark pattern research: inconsistencies and incompleteness in classification, limitations of detection tools, and insufficient comprehensiveness in existing datasets. In this study, we propose a comprehensive analytical framework--the Dark Pattern Analysis Framework (DPAF). Using this framework, we developed a taxonomy comprising 68 types of dark patterns, each annotated in detail to illustrate its impact on users, potential scenarios, and real-world examples, validated through industry surveys. Furthermore, we evaluated the effectiveness of current detection tools and assessed the completeness of available datasets. Our findings indicate that, among the 8 detection tools studied, only 31 types of dark patterns are identifiable, resulting in a coverage rate of just 45.5%. Similarly, our analysis of four datasets, encompassing 5,561 instances, reveals coverage of only 30 types of dark patterns, with an overall coverage rate of 44%. Based on the available datasets, we standardized classifications and merged datasets to form a unified image dataset and a unified text dataset. These results highlight significant room for improvement in the field of dark pattern detection. This research not only deepens our understanding of dark pattern classification and detection tools but also offers valuable insights for future research and practice in this domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09147v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Meng Li, Xiang Wang, Liming Nie, Chenglin Li, Yang Liu, Yangyang Zhao, Lei Xue, Kabir Sulaiman Said</dc:creator>
    </item>
    <item>
      <title>LIVE-GS: LLM Powers Interactive VR by Enhancing Gaussian Splatting</title>
      <link>https://arxiv.org/abs/2412.09176</link>
      <description>arXiv:2412.09176v1 Announce Type: new 
Abstract: Recently, radiance field rendering, such as 3D Gaussian Splatting (3DGS), has shown immense potential in VR content creation due to its high-quality rendering and efficient production process. However, existing physics-based interaction systems for 3DGS can only perform simple and non-realistic simulations or demand extensive user input for complex scenes, primarily due to the absence of scene understanding. In this paper, we propose LIVE-GS, a highly realistic interactive VR system powered by LLM. After object-aware GS reconstruction, we prompt GPT-4o to analyze the physical properties of objects in the scene, which are used to guide physical simulations consistent with real phenomena. We also design a GPT-assisted GS inpainting module to fill the unseen area covered by manipulative objects. To perform a precise segmentation of Gaussian kernels, we propose a feature-mask segmentation strategy. To enable rich interaction, we further propose a computationally efficient physical simulation framework through an PBD-based unified interpolation method, supporting various physical forms such as rigid body, soft body, and granular materials. Our experimental results show that with the help of LLM's understanding and enhancement of scenes, our VR system can support complex and realistic interactions without additional manual design and annotation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09176v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haotian Mao, Zhuoxiong Xu, Siyue Wei, Yule Quan, Nianchen Deng, Xubo Yang</dc:creator>
    </item>
    <item>
      <title>Imperceptible Gaze Guidance Through Ocularity in Virtual Reality</title>
      <link>https://arxiv.org/abs/2412.09204</link>
      <description>arXiv:2412.09204v1 Announce Type: new 
Abstract: We introduce to VR a novel imperceptible gaze guidance technique from a recent discovery that human gaze can be attracted to a cue that contrasts from the background in its perceptually non-distinctive ocularity, defined as the relative difference between inputs to the two eyes. This cue pops out in the saliency map in the primary visual cortex without being overtly visible. We tested this method in an odd-one-out visual search task using eye tracking with 15 participants in VR. When the target was rendered as an ocularity singleton, participants' gaze was drawn to the target faster. Conversely, when a background object served as the ocularity singleton, it distracted gaze from the target. Since ocularity is nearly imperceptible, our method maintains user immersion while guiding attention without noticeable scene alterations and can render object's depth in 3D scenes, creating new possibilities for immersive user experience across diverse VR applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09204v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Virmarie Maquiling, Li Zhaoping, Enkelejda Kasneci</dc:creator>
    </item>
    <item>
      <title>Emotion AI in Workplace Environments: A Case Study</title>
      <link>https://arxiv.org/abs/2412.09251</link>
      <description>arXiv:2412.09251v1 Announce Type: new 
Abstract: Emotion AI is an emerging field of artificial intelligence intended to be utilized by organizations to manage and monitor employees emotional states supporting employee wellbeing and organizational goals. The current paper presents a case study that took place in a Finnish research institute in which 11 research participants were interviewed about their experiences of working in an Emotion AI environment. Our findings indicate that employees have a positive predisposition towards wellbeing monitoring in the workplace when benefits are perceived firsthand. Concerns however, manifest even in settings where there is existing familiarity with the technology how it operates and who is conducting the data collection, these are discussed in the findings. We additionally note that employee concerns can be mitigated via robust organizational policies transparency and open communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09251v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Joni-Roy Piispanen, Rebekah Rousi</dc:creator>
    </item>
    <item>
      <title>Distinguishing Scams and Fraud with Ensemble Learning</title>
      <link>https://arxiv.org/abs/2412.08680</link>
      <description>arXiv:2412.08680v1 Announce Type: cross 
Abstract: Users increasingly query LLM-enabled web chatbots for help with scam defense. The Consumer Financial Protection Bureau's complaints database is a rich data source for evaluating LLM performance on user scam queries, but currently the corpus does not distinguish between scam and non-scam fraud. We developed an LLM ensemble approach to distinguishing scam and fraud CFPB complaints and describe initial findings regarding the strengths and weaknesses of LLMs in the scam defense context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08680v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Isha Chadalavada, Tianhui Huang, Jessica Staddon</dc:creator>
    </item>
    <item>
      <title>MS2Mesh-XR: Multi-modal Sketch-to-Mesh Generation in XR Environments</title>
      <link>https://arxiv.org/abs/2412.09008</link>
      <description>arXiv:2412.09008v1 Announce Type: cross 
Abstract: We present MS2Mesh-XR, a novel multi-modal sketch-to-mesh generation pipeline that enables users to create realistic 3D objects in extended reality (XR) environments using hand-drawn sketches assisted by voice inputs. In specific, users can intuitively sketch objects using natural hand movements in mid-air within a virtual environment. By integrating voice inputs, we devise ControlNet to infer realistic images based on the drawn sketches and interpreted text prompts. Users can then review and select their preferred image, which is subsequently reconstructed into a detailed 3D mesh using the Convolutional Reconstruction Model. In particular, our proposed pipeline can generate a high-quality 3D mesh in less than 20 seconds, allowing for immersive visualization and manipulation in run-time XR scenes. We demonstrate the practicability of our pipeline through two use cases in XR settings. By leveraging natural user inputs and cutting-edge generative AI capabilities, our approach can significantly facilitate XR-based creative production and enhance user experiences. Our code and demo will be available at: https://yueqiu0911.github.io/MS2Mesh-XR/</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09008v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuqi Tong, Yue Qiu, Ruiyang Li, Shi Qiu, Pheng-Ann Heng</dc:creator>
    </item>
    <item>
      <title>Dialogue Language Model with Large-Scale Persona Data Engineering</title>
      <link>https://arxiv.org/abs/2412.09034</link>
      <description>arXiv:2412.09034v1 Announce Type: cross 
Abstract: Maintaining persona consistency is paramount in the application of open-domain dialogue systems, as exemplified by models like ChatGPT. Despite significant advancements, the limited scale and diversity of current persona dialogue datasets remain challenges to achieving robust persona-consistent dialogue models. In this study, drawing inspiration from the success of large-scale pre-training, we introduce PPDS, an open-domain persona dialogue system that employs extensive generative pre-training on a persona dialogue dataset to enhance persona consistency. Specifically, we present a persona extraction model designed to autonomously and precisely generate vast persona dialogue datasets. Additionally, we unveil a pioneering persona augmentation technique to address the invalid persona bias inherent in the constructed dataset. Both quantitative and human evaluations consistently highlight the superior response quality and persona consistency of our proposed model, underscoring its effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09034v1</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengze Hong, Chen Zhang, Chaotao Chen, Rongzhong Lian, Di Jiang</dc:creator>
    </item>
    <item>
      <title>herakoi: a sonification experiment for astronomical data</title>
      <link>https://arxiv.org/abs/2412.09152</link>
      <description>arXiv:2412.09152v1 Announce Type: cross 
Abstract: Recent research is revealing data-sonification as a promising complementary approach to vision, benefiting both data perception and interpretation. We present herakoi, a novel open-source software that uses machine learning to allow real-time image sonification, with a focus on astronomical data. By tracking hand movements via a webcam and mapping them to image coordinates, herakoi translates visual properties into sound, enabling users to "hear" images. Its swift responsiveness allows users to access information in astronomical images with short training, demonstrating high reliability and effectiveness. The software has shown promise in educational and outreach settings, making complex astronomical concepts more engaging and accessible to diverse audiences, including blind and visually impaired individuals. We also discuss future developments, such as the integration of large language and vision models to create a more interactive experience in interpreting astronomical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09152v1</guid>
      <category>astro-ph.IM</category>
      <category>cs.HC</category>
      <category>physics.ed-ph</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Michele Ginolfi, Luca Di Mascolo, Anita Zanella</dc:creator>
    </item>
    <item>
      <title>Beware of Metacognitive Laziness: Effects of Generative Artificial Intelligence on Learning Motivation, Processes, and Performance</title>
      <link>https://arxiv.org/abs/2412.09315</link>
      <description>arXiv:2412.09315v1 Announce Type: cross 
Abstract: With the continuous development of technological and educational innovation, learners nowadays can obtain a variety of support from agents such as teachers, peers, education technologies, and recently, generative artificial intelligence such as ChatGPT. The concept of hybrid intelligence is still at a nascent stage, and how learners can benefit from a symbiotic relationship with various agents such as AI, human experts and intelligent learning systems is still unknown. The emerging concept of hybrid intelligence also lacks deep insights and understanding of the mechanisms and consequences of hybrid human-AI learning based on strong empirical research. In order to address this gap, we conducted a randomised experimental study and compared learners' motivations, self-regulated learning processes and learning performances on a writing task among different groups who had support from different agents (ChatGPT, human expert, writing analytics tools, and no extra tool). A total of 117 university students were recruited, and their multi-channel learning, performance and motivation data were collected and analysed. The results revealed that: learners who received different learning support showed no difference in post-task intrinsic motivation; there were significant differences in the frequency and sequences of the self-regulated learning processes among groups; ChatGPT group outperformed in the essay score improvement but their knowledge gain and transfer were not significantly different. Our research found that in the absence of differences in motivation, learners with different supports still exhibited different self-regulated learning processes, ultimately leading to differentiated performance. What is particularly noteworthy is that AI technologies such as ChatGPT may promote learners' dependence on technology and potentially trigger metacognitive laziness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09315v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1111/bjet.13544</arxiv:DOI>
      <dc:creator>Yizhou Fan, Luzhen Tang, Huixiao Le, Kejie Shen, Shufang Tan, Yueying Zhao, Yuan Shen, Xinyu Li, Dragan Ga\v{s}evi\'c</dc:creator>
    </item>
    <item>
      <title>Towards better social crisis data with HERMES: Hybrid sensing for EmeRgency ManagEment System</title>
      <link>https://arxiv.org/abs/1912.02182</link>
      <description>arXiv:1912.02182v2 Announce Type: replace 
Abstract: People involved in mass emergencies increasingly publish information-rich contents in online social networks (OSNs), thus acting as a distributed and resilient network of human sensors. In this work we present HERMES, a system designed to enrich the information spontaneously disclosed by OSN users in the aftermath of disasters. HERMES leverages a mixed data collection strategy, called hybrid sensing, and state-of-the-art AI techniques. Evaluated in real-world emergencies, HERMES proved to increase: (i) the amount of the available damage information; (ii) the density (up to 7x) and the variety (up to 18x) of the retrieved geographic information; (iii) the geographic coverage (up to 30%) and granularity.</description>
      <guid isPermaLink="false">oai:arXiv.org:1912.02182v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.pmcj.2020.101225</arxiv:DOI>
      <arxiv:journal_reference>Pervasive and Mobile Computing 67:101225, 2020</arxiv:journal_reference>
      <dc:creator>Marco Avvenuti, Salvatore Bellomo, Stefano Cresci, Leonardo Nizzoli, Maurizio Tesconi</dc:creator>
    </item>
    <item>
      <title>TimeLighting: Guided Exploration of 2D Temporal Network Projections</title>
      <link>https://arxiv.org/abs/2308.12628</link>
      <description>arXiv:2308.12628v3 Announce Type: replace 
Abstract: In temporal ( event-based ) networks, time is a continuous axis, with real-valued time coordinates for each node and edge. Computing a layout for such graphs means embedding the node trajectories and edge surfaces over time in a 2D+t space, known as the space-time cube. Currently, these space-time cube layouts are visualized through animation or by slicing the cube at regular intervals. However, both techniques present problems such as below-average performance on tasks as well as loss of precision and difficulties in selecting timeslice intervals. In this paper, we present TimeLighting , a novel visual analytics approach to visualize and explore temporal graphs embedded in the space-time cube. Our interactive approach highlights node trajectories and their movement over time, visualizes node "aging", and provides guidance to support users during exploration by indicating interesting time intervals ("when") and network elements ("where") are located for a detail-oriented investigation. This combined focus helps to gain deeper insights into the temporal network's underlying behavior. We assess the utility and efficacy of our approach through two case studies and qualitative expert evaluation. The results demonstrate how TimeLighting supports identifying temporal patterns, extracting insights from nodes with high activity, and guiding the exploration and analysis process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.12628v3</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TVCG.2024.3514858</arxiv:DOI>
      <dc:creator>Velitchko Filipov, Davide Ceneda, Daniel Archambault, Alessio Arleo</dc:creator>
    </item>
    <item>
      <title>"Living Within Four Walls": Exploring Emotional and Social Dynamics in Mobile Usage During Home Confinement</title>
      <link>https://arxiv.org/abs/2310.13304</link>
      <description>arXiv:2310.13304v3 Announce Type: replace 
Abstract: Home confinement, a situation experienced by individuals for reasons ranging from medical quarantines, rehabilitation needs, disability accommodations, and remote working, is a common yet impactful aspect of modern life. While essential in various scenarios, confinement within the home environment can profoundly influence mental well-being and digital device usage. Using the COVID-19 lockdown as a case study, this research explores the emotional and social effects of prolonged home confinement on mobile device usage. We conducted an in-situ study with 32 participants, analyzing three weeks of mobile usage data to assess emotional well-being and social dynamics in restricted environments. Our findings reveal that app usage patterns serve as strong indicators of emotional states, offering insights into how digital interactions can reflect and influence well-being during isolation. This study highlights the potential for developing targeted interventions and support systems for individuals in long-term home confinement, including those with chronic illness, recovery needs, or permanent remote work situations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13304v3</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nan Gao, Sam Nolan, Kaixin Ji, Shakila Khan Rumi, Judith Simone Heinisch, Christoph Anderson, Klaus David, Flora D. Salim</dc:creator>
    </item>
    <item>
      <title>TeachTune: Reviewing Pedagogical Agents Against Diverse Student Profiles with Simulated Students</title>
      <link>https://arxiv.org/abs/2410.04078</link>
      <description>arXiv:2410.04078v2 Announce Type: replace 
Abstract: Large language models (LLMs) can empower teachers to build pedagogical conversational agents (PCAs) customized for their students. As students have different prior knowledge and motivation levels, teachers must review the adaptivity of their PCAs to diverse students. Existing chatbot reviewing methods (e.g., direct chat and benchmarks) are either manually intensive for multiple iterations or limited to testing only single-turn interactions. We present TeachTune, where teachers can create simulated students and review PCAs by observing automated chats between PCAs and simulated students. Our technical pipeline instructs an LLM-based student to simulate prescribed knowledge levels and traits, helping teachers explore diverse conversation patterns. Our pipeline could produce simulated students whose behaviors correlate highly to their input knowledge and motivation levels within 5% and 10% accuracy gaps. Thirty science teachers designed PCAs in a between-subjects study, and using TeachTune resulted in a lower task load and higher student profile coverage over a baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04078v2</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hyoungwook Jin, Minju Yoo, Jeongeon Park, Yokyung Lee, Xu Wang, Juho Kim</dc:creator>
    </item>
    <item>
      <title>Mixed or Misperceived Reality? Flusserian Media Freedom through Surreal Me</title>
      <link>https://arxiv.org/abs/2410.12171</link>
      <description>arXiv:2410.12171v2 Announce Type: replace 
Abstract: This paper delves into Vil\'em Flusser's critique of media as mediators that distort the human perception of reality and diminish freedom, particularly within the Mixed Reality context, i.e., Misperceived Reality. It introduces an artistic inquiry through Surreal Me, which engages participants to experience a two-phase virtual embodying process and reveal the "Misperceived Reality." The process examines the obfuscating nature of media; as the Sense of Embodiment inevitably breaks down, users can discover the constructed nature of media-projected reality. When users reflect on reality's authentic and mediated experiences in MR, this work fosters a critical discourse on Flusserian media freedom addressing emerging immersive technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12171v2</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3689050.3705992</arxiv:DOI>
      <dc:creator>Aven-Le Zhou, Lei Xi, Kang Zhang</dc:creator>
    </item>
    <item>
      <title>"Create a Fear of Missing Out" -- ChatGPT Implements Unsolicited Deceptive Designs in Generated Websites Without Warning</title>
      <link>https://arxiv.org/abs/2411.03108</link>
      <description>arXiv:2411.03108v2 Announce Type: replace 
Abstract: With the recent advancements in Large Language Models (LLMs), web developers increasingly apply their code-generation capabilities to website design. However, since these models are trained on existing designerly knowledge, they may inadvertently replicate bad or even illegal practices, especially deceptive designs (DD). This paper examines whether users can accidentally create DD for a fictitious webshop using GPT-4. We recruited 20 participants, asking them to use ChatGPT to generate functionalities (product overview or checkout) and then modify these using neutral prompts to meet a business goal (e.g., "increase the likelihood of us selling our product"). We found that all 20 generated websites contained at least one DD pattern (mean: 5, max: 9), with GPT-4 providing no warnings. When reflecting on the designs, only 4 participants expressed concerns, while most considered the outcomes satisfactory and not morally problematic, despite the potential ethical and legal implications for end-users and those adopting ChatGPT's recommendations</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03108v2</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Veronika Krau{\ss}, Mark McGill, Thomas Kosch, Yolanda Thiel, Dominik Sch\"on, Jan Gugenheimer</dc:creator>
    </item>
    <item>
      <title>Goetterfunke: Creativity in Machinae Sapiens. About the Qualitative Shift in Generative AI with a Focus on Text-To-Image</title>
      <link>https://arxiv.org/abs/2411.10448</link>
      <description>arXiv:2411.10448v3 Announce Type: replace 
Abstract: The year 2022 marks a watershed in technology, and arguably in human history, with the release of powerful generative AIs capable of convincingly performing creative tasks. With the help of these systems, anyone can create something that would previously have been considered a remarkable work of art. In human-AI collaboration, the computer seems to have become more than a tool. Many who have made their first contact with current generative AIs see them as "creativity machines" while for others the term "machine creativity" remains an oxymoron. This article is about (the possibility of) creativity in computers within the current Machine Learning paradigm. It outlines some of the key concepts behind the technologies and the innovations that have contributed to this qualitative shift, with a focus on text-to-image systems. The nature of Artificial Creativity as such is discussed, as well as what this might mean for art. AI may become a responsible collaborator with elements of independent machine authorship in the artistic process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10448v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jens Knappe</dc:creator>
    </item>
    <item>
      <title>Advancing Music Therapy: Integrating Eastern Five-Element Music Theory and Western Techniques with AI in the Novel Five-Element Harmony System</title>
      <link>https://arxiv.org/abs/2412.06600</link>
      <description>arXiv:2412.06600v2 Announce Type: replace 
Abstract: In traditional medical practices, music therapy has proven effective in treating various psychological and physiological ailments. Particularly in Eastern traditions, the Five Elements Music Therapy (FEMT), rooted in traditional Chinese medicine, possesses profound cultural significance and unique therapeutic philosophies. With the rapid advancement of Information Technology and Artificial Intelligence, applying these modern technologies to FEMT could enhance the personalization and cultural relevance of the therapy and potentially improve therapeutic outcomes. In this article, we developed a music therapy system for the first time by applying the theory of the five elements in music therapy to practice. This innovative approach integrates advanced Information Technology and Artificial Intelligence with Five-Element Music Therapy (FEMT) to enhance personalized music therapy practices. As traditional music therapy predominantly follows Western methodologies, the unique aspects of Eastern practices, specifically the Five-Element theory from traditional Chinese medicine, should be considered. This system aims to bridge this gap by utilizing computational technologies to provide a more personalized, culturally relevant, and therapeutically effective music therapy experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06600v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yubo Zhou, Weizhen Bian, Kaitai Zhang, Xiaohan Gu</dc:creator>
    </item>
    <item>
      <title>Learning About Algorithm Auditing in Five Steps: Scaffolding How High School Youth Can Systematically and Critically Evaluate Machine Learning Applications</title>
      <link>https://arxiv.org/abs/2412.06989</link>
      <description>arXiv:2412.06989v2 Announce Type: replace 
Abstract: While there is widespread interest in supporting young people to critically evaluate machine learning-powered systems, there is little research on how we can support them in inquiring about how these systems work and what their limitations and implications may be. Outside of K-12 education, an effective strategy in evaluating black-boxed systems is algorithm auditing-a method for understanding algorithmic systems' opaque inner workings and external impacts from the outside in. In this paper, we review how expert researchers conduct algorithm audits and how end users engage in auditing practices to propose five steps that, when incorporated into learning activities, can support young people in auditing algorithms. We present a case study of a team of teenagers engaging with each step during an out-of-school workshop in which they audited peer-designed generative AI TikTok filters. We discuss the kind of scaffolds we provided to support youth in algorithm auditing and directions and challenges for integrating algorithm auditing into classroom activities. This paper contributes: (a) a conceptualization of five steps to scaffold algorithm auditing learning activities, and (b) examples of how youth engaged with each step during our pilot study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06989v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luis Morales-Navarro, Yasmin B. Kafai, Lauren Vogelstein, Evelyn Yu, Dana\"e Metaxa</dc:creator>
    </item>
    <item>
      <title>From Lived Experience to Insight: Unpacking the Psychological Risks of Using AI Conversational Agents</title>
      <link>https://arxiv.org/abs/2412.07951</link>
      <description>arXiv:2412.07951v2 Announce Type: replace 
Abstract: Recent gain in popularity of AI conversational agents has led to their increased use for improving productivity and supporting well-being. While previous research has aimed to understand the risks associated with interactions with AI conversational agents, these studies often fall short in capturing the lived experiences. Additionally, psychological risks have often been presented as a sub-category within broader AI-related risks in past taxonomy works, leading to under-representation of the impact of psychological risks of AI use. To address these challenges, our work presents a novel risk taxonomy focusing on psychological risks of using AI gathered through lived experience of individuals. We employed a mixed-method approach, involving a comprehensive survey with 283 individuals with lived mental health experience and workshops involving lived experience experts to develop a psychological risk taxonomy. Our taxonomy features 19 AI behaviors, 21 negative psychological impacts, and 15 contexts related to individuals. Additionally, we propose a novel multi-path vignette based framework for understanding the complex interplay between AI behaviors, psychological impacts, and individual user contexts. Finally, based on the feedback obtained from the workshop sessions, we present design recommendations for developing safer and more robust AI agents. Our work offers an in-depth understanding of the psychological risks associated with AI conversational agents and provides actionable recommendations for policymakers, researchers, and developers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07951v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohit Chandra, Suchismita Naik, Denae Ford, Ebele Okoli, Munmun De Choudhury, Mahsa Ershadi, Gonzalo Ramos, Javier Hernandez, Ananya Bhattacharjee, Shahed Warreth, Jina Suh</dc:creator>
    </item>
    <item>
      <title>Putting the Count Back Into Accountability: An Analysis of Transparency Data About the Sexual Exploitation of Minors</title>
      <link>https://arxiv.org/abs/2402.14625</link>
      <description>arXiv:2402.14625v2 Announce Type: replace-cross 
Abstract: Alarmist and sensationalist statements about the "explosion" of online child sexual exploitation or CSE dominate much of the public discourse about the topic. Based on a new dataset collecting the transparency disclosures for 16 US-based internet platforms and the national clearinghouse collecting legally mandated reports about CSE, this study seeks answers to two research questions: First, what does the data tell us about the growth of online CSE? Second, how reliable and trustworthy is that data? To answer the two questions, this study proceeds in three parts. First, we leverage a critical literature review to synthesize a granular model for CSE reporting. Second, we analyze the growth in CSE reports over the last 25 years and correlate it with the growth of social media user accounts. Third, we use two comparative audits to assess the quality of transparency data. Critical findings include: First, US law increasingly threatens the very population it claims to protect, i.e., children and adolescents. Second, the rapid growth of CSE report over the last decade is linear and largely driven by an equivalent growth in social media user accounts. Third, the Covid-19 pandemic had no statistically relevant impact on report volume. Fourth, while half of surveyed organizations release meaningful and reasonably accurate transparency data, the other half either fail to make disclosures or release data with severe quality issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14625v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert Grimm</dc:creator>
    </item>
    <item>
      <title>Using GPT-4 to guide causal machine learning</title>
      <link>https://arxiv.org/abs/2407.18607</link>
      <description>arXiv:2407.18607v2 Announce Type: replace-cross 
Abstract: Since its introduction to the public, ChatGPT has had an unprecedented impact. While some experts praised AI advancements and highlighted their potential risks, others have been critical about the accuracy and usefulness of Large Language Models (LLMs). In this paper, we are interested in the ability of LLMs to identify causal relationships. We focus on the well-established GPT-4 (Turbo) and evaluate its performance under the most restrictive conditions, by isolating its ability to infer causal relationships based solely on the variable labels without being given any other context by humans, demonstrating the minimum level of effectiveness one can expect when it is provided with label-only information. We show that questionnaire participants judge the GPT-4 graphs as the most accurate in the evaluated categories, closely followed by knowledge graphs constructed by domain experts, with causal Machine Learning (ML) far behind. We use these results to highlight the important limitation of causal ML, which often produces causal graphs that violate common sense, affecting trust in them. However, we show that pairing GPT-4 with causal ML overcomes this limitation, resulting in graphical structures learnt from real data that align more closely with those identified by domain experts, compared to structures learnt by causal ML alone. Overall, our findings suggest that despite GPT-4 not being explicitly designed to reason causally, it can still be a valuable tool for causal representation, as it improves the causal discovery process of causal ML algorithms that are designed to do just that.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18607v2</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anthony C. Constantinou, Neville K. Kitson, Alessio Zanga</dc:creator>
    </item>
    <item>
      <title>Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million Images</title>
      <link>https://arxiv.org/abs/2410.08926</link>
      <description>arXiv:2410.08926v2 Announce Type: replace-cross 
Abstract: We explore the transformative potential of SAM 2, a vision foundation model, in advancing gaze estimation and eye tracking technologies. By significantly reducing annotation time, lowering technical barriers through its ease of deployment, and enhancing segmentation accuracy, SAM 2 addresses critical challenges faced by researchers and practitioners. Utilizing its zero-shot segmentation capabilities with minimal user input-a single click per video-we tested SAM 2 on over 14 million eye images from diverse datasets, including virtual reality setups and the world's largest unified dataset recorded using wearable eye trackers. Remarkably, in pupil segmentation tasks, SAM 2 matches the performance of domain-specific models trained solely on eye images, achieving competitive mean Intersection over Union (mIoU) scores of up to 93% without fine-tuning. Additionally, we provide our code and segmentation masks for these widely used datasets to promote further research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08926v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Virmarie Maquiling, Sean Anthony Byrne, Diederick C. Niehorster, Marco Carminati, Enkelejda Kasneci</dc:creator>
    </item>
    <item>
      <title>MT3DNet: Multi-Task learning Network for 3D Surgical Scene Reconstruction</title>
      <link>https://arxiv.org/abs/2412.03928</link>
      <description>arXiv:2412.03928v2 Announce Type: replace-cross 
Abstract: In image-assisted minimally invasive surgeries (MIS), understanding surgical scenes is vital for real-time feedback to surgeons, skill evaluation, and improving outcomes through collaborative human-robot procedures. Within this context, the challenge lies in accurately detecting, segmenting, and estimating the depth of surgical scenes depicted in high-resolution images, while simultaneously reconstructing the scene in 3D and providing segmentation of surgical instruments along with detection labels for each instrument. To address this challenge, a novel Multi-Task Learning (MTL) network is proposed for performing these tasks concurrently. A key aspect of this approach involves overcoming the optimization hurdles associated with handling multiple tasks concurrently by integrating a Adversarial Weight Update into the MTL framework, the proposed MTL model achieves 3D reconstruction through the integration of segmentation, depth estimation, and object detection, thereby enhancing the understanding of surgical scenes, which marks a significant advancement compared to existing studies that lack 3D capabilities. Comprehensive experiments on the EndoVis2018 benchmark dataset underscore the adeptness of the model in efficiently addressing all three tasks, demonstrating the efficacy of the proposed techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03928v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mithun Parab, Pranay Lendave, Jiyoung Kim, Thi Quynh Dan Nguyen, Palash Ingle</dc:creator>
    </item>
    <item>
      <title>Advancing Extended Reality with 3D Gaussian Splatting: Innovations and Prospects</title>
      <link>https://arxiv.org/abs/2412.06257</link>
      <description>arXiv:2412.06257v2 Announce Type: replace-cross 
Abstract: 3D Gaussian Splatting (3DGS) has attracted significant attention for its potential to revolutionize 3D representation, rendering, and interaction. Despite the rapid growth of 3DGS research, its direct application to Extended Reality (XR) remains underexplored. Although many studies recognize the potential of 3DGS for XR, few have explicitly focused on or demonstrated its effectiveness within XR environments. In this paper, we aim to synthesize innovations in 3DGS that show specific potential for advancing XR research and development. We conduct a comprehensive review of publicly available 3DGS papers, with a focus on those referencing XR-related concepts. Additionally, we perform an in-depth analysis of innovations explicitly relevant to XR and propose a taxonomy to highlight their significance. Building on these insights, we propose several prospective XR research areas where 3DGS can make promising contributions, yet remain rarely touched. By investigating the intersection of 3DGS and XR, this paper provides a roadmap to push the boundaries of XR using cutting-edge 3DGS techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06257v2</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shi Qiu, Binzhu Xie, Qixuan Liu, Pheng-Ann Heng</dc:creator>
    </item>
  </channel>
</rss>
