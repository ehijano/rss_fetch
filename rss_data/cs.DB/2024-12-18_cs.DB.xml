<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Dec 2024 05:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Simple and Fast Way to Handle Semantic Errors in Transactions</title>
      <link>https://arxiv.org/abs/2412.12493</link>
      <description>arXiv:2412.12493v1 Announce Type: new 
Abstract: Many computer systems are now being redesigned to incorporate LLM-powered agents, enabling natural language input and more flexible operations. This paper focuses on handling database transactions created by large language models (LLMs). Transactions generated by LLMs may include semantic errors, requiring systems to treat them as long-lived. This allows for human review and, if the transaction is incorrect, removal from the database history. Any removal action must ensure the database's consistency (the "C" in ACID principles) is maintained throughout the process.
  We propose a novel middleware framework based on Invariant Satisfaction (I-Confluence), which ensures consistency by identifying and coordinating dependencies between long-lived transactions and new transactions. This middleware buffers suspicious or compensating transactions to manage coordination states. Using the TPC-C benchmark, we evaluate how transaction generation frequency, user reviews, and invariant completeness impact system performance. For system researchers, this study establishes an interactive paradigm between LLMs and database systems, providing an "undoing" mechanism for handling incorrect operations while guaranteeing database consistency. For system engineers, this paper offers a middleware design that integrates removable LLM-generated transactions into existing systems with minimal modifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12493v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jinghan Zeng, Eugene Wu, Sanjay Krishnan</dc:creator>
    </item>
    <item>
      <title>UniEntrezDB: Large-scale Gene Ontology Annotation Dataset and Evaluation Benchmarks with Unified Entrez Gene Identifiers</title>
      <link>https://arxiv.org/abs/2412.12688</link>
      <description>arXiv:2412.12688v1 Announce Type: new 
Abstract: Gene studies are crucial for fields such as protein structure prediction, drug discovery, and cancer genomics, yet they face challenges in fully utilizing the vast and diverse information available. Gene studies require clean, factual datasets to ensure reliable results. Ontology graphs, neatly organized domain terminology graphs, provide ideal sources for domain facts. However, available gene ontology annotations are currently distributed across various databases without unified identifiers for genes and gene products. To address these challenges, we introduce Unified Entrez Gene Identifier Dataset and Benchmarks (UniEntrezDB), the first systematic effort to unify large-scale public Gene Ontology Annotations (GOA) from various databases using unique gene identifiers. UniEntrezDB includes a pre-training dataset and four downstream tasks designed to comprehensively evaluate gene embedding performance from gene, protein, and cell levels, ultimately enhancing the reliability and applicability of LLMs in gene research and other professional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12688v1</guid>
      <category>cs.DB</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuwei Miao, Yuzhi Guo, Hehuan Ma, Jingquan Yan, Feng Jiang, Weizhi An, Jean Gao, Junzhou Huang</dc:creator>
    </item>
    <item>
      <title>A Conceptual Model of Intelligent Multimedia Data Rendered using Flying Light Specks</title>
      <link>https://arxiv.org/abs/2412.12938</link>
      <description>arXiv:2412.12938v1 Announce Type: new 
Abstract: A Flying Light Speck, FLS, is a miniature sized drone configured with light sources to illuminate 3D multimedia objects in a fixed volume, an FLS display. A swarm of FLSs may provide haptic interactions by exerting force back at a user's touch. This paper presents a conceptual model for the multimedia data to enable content-based queries. The model empowers users of an FLS display to annotate the illuminations by adding semantics to the data, extending a multimedia repository with information and knowledge. We present a core conceptual model and demonstrate its extensions for two diverse applications, authoring tools with entertainment and MRI scans with healthcare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12938v1</guid>
      <category>cs.DB</category>
      <category>cs.ET</category>
      <category>cs.MM</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.61981/ZFSH2309</arxiv:DOI>
      <dc:creator>Nima Yazdani, Hamed Alimohammadzadeh, Shahram Ghandeharizadeh</dc:creator>
    </item>
    <item>
      <title>The Temporal Vadalog System: Temporal Datalog-based Reasoning</title>
      <link>https://arxiv.org/abs/2412.13019</link>
      <description>arXiv:2412.13019v1 Announce Type: new 
Abstract: In the wake of the recent resurgence of the Datalog language of databases, together with its extensions for ontological reasoning settings, this work aims to bridge the gap between the theoretical studies of DatalogMTL (Datalog extended with metric temporal logic) and the development of production-ready reasoning systems. In particular, we lay out the functional and architectural desiderata of a modern reasoner and propose our system, Temporal Vadalog. Leveraging the vast amount of experience from the database community, we go beyond the typical chase-based implementations of reasoners, and propose a set of novel techniques and a system that adopts a modern data pipeline architecture. We discuss crucial architectural choices, such as how to guarantee termination when infinitely many time intervals are possibly generated, how to merge intervals, and how to sustain a limited memory footprint. We discuss advanced features of the system, such as the support for time series, and present an extensive experimental evaluation. This paper is a substantially extended version of "The Temporal Vadalog System" as presented at RuleML+RR '22. Under consideration in Theory and Practice of Logic Programming (TPLP).</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13019v1</guid>
      <category>cs.DB</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luigi Bellomarini, Livia Blasi, Markus Nissl, Emanuel Sallinger</dc:creator>
    </item>
    <item>
      <title>Intermediate Relation Size Bounds for Select-Project-Join Query Plans: Asymptotically Tight Characterizations</title>
      <link>https://arxiv.org/abs/2412.13104</link>
      <description>arXiv:2412.13104v1 Announce Type: new 
Abstract: We study the problem of statically optimizing select-project-join (SPJ) plans where unary key constraints are allowed. A natural measure of a plan, which we call the output degree and which has been studied previously, is the minimum degree of a polynomial bounding the plan's output relation, as a function of the input database's maximum relation size. This measure is, by definition, invariant under passing from a plan to another plan that is semantically equivalent to the first. In this article, we consider a plan measure which we call the intermediate degree; this measure is defined to be the minimum degree bounding the size of all intermediate relations computed during a plan's execution -- again, as a function of the input database's maximum relation size. We present an algorithm that, given an SPJ plan $q$ and a set $\Sigma$ of unary keys, computes an SPJ plan $q'$ that is semantically equivalent to $q$ (over databases satisfying $\Sigma$) and that has the minimum intermediate degree over all such semantically equivalent plans. For the types of plans considered, we thus obtain a complete and effective understanding of intermediate degree.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13104v1</guid>
      <category>cs.DB</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Hubie Chen, Markus Schneider</dc:creator>
    </item>
    <item>
      <title>Graph Learning in the Era of LLMs: A Survey from the Perspective of Data, Models, and Tasks</title>
      <link>https://arxiv.org/abs/2412.12456</link>
      <description>arXiv:2412.12456v1 Announce Type: cross 
Abstract: With the increasing prevalence of cross-domain Text-Attributed Graph (TAG) Data (e.g., citation networks, recommendation systems, social networks, and ai4science), the integration of Graph Neural Networks (GNNs) and Large Language Models (LLMs) into a unified Model architecture (e.g., LLM as enhancer, LLM as collaborators, LLM as predictor) has emerged as a promising technological paradigm. The core of this new graph learning paradigm lies in the synergistic combination of GNNs' ability to capture complex structural relationships and LLMs' proficiency in understanding informative contexts from the rich textual descriptions of graphs. Therefore, we can leverage graph description texts with rich semantic context to fundamentally enhance Data quality, thereby improving the representational capacity of model-centric approaches in line with data-centric machine learning principles. By leveraging the strengths of these distinct neural network architectures, this integrated approach addresses a wide range of TAG-based Task (e.g., graph learning, graph reasoning, and graph question answering), particularly in complex industrial scenarios (e.g., supervised, few-shot, and zero-shot settings). In other words, we can treat text as a medium to enable cross-domain generalization of graph learning Model, allowing a single graph model to effectively handle the diversity of downstream graph-based Task across different data domains. This work serves as a foundational reference for researchers and practitioners looking to advance graph learning methodologies in the rapidly evolving landscape of LLM. We consistently maintain the related open-source materials at \url{https://github.com/xkLi-Allen/Awesome-GNN-in-LLMs-Papers}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12456v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.DB</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Xunkai Li, Zhengyu Wu, Jiayi Wu, Hanwen Cui, Jishuo Jia, Rong-Hua Li, Guoren Wang</dc:creator>
    </item>
    <item>
      <title>Are Data Experts Buying into Differentially Private Synthetic Data? Gathering Community Perspectives</title>
      <link>https://arxiv.org/abs/2412.13030</link>
      <description>arXiv:2412.13030v1 Announce Type: cross 
Abstract: Data privacy is a core tenet of responsible computing, and in the United States, differential privacy (DP) is the dominant technical operationalization of privacy-preserving data analysis. With this study, we qualitatively examine one class of DP mechanisms: private data synthesizers. To that end, we conducted semi-structured interviews with data experts: academics and practitioners who regularly work with data. Broadly, our findings suggest that quantitative DP benchmarks must be grounded in practitioner needs, while communication challenges persist. Participants expressed a need for context-aware DP solutions, focusing on parity between research outcomes on real and synthetic data. Our analysis led to three recommendations: (1) improve existing insufficient sanitized benchmarks; successful DP implementations require well-documented, partner-vetted use cases, (2) organizations using DP synthetic data should publish discipline-specific standards of evidence, and (3) tiered data access models could allow researchers to gradually access sensitive data based on demonstrated competence with high-privacy, low-fidelity synthetic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13030v1</guid>
      <category>cs.HC</category>
      <category>cs.CR</category>
      <category>cs.DB</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Rosenblatt, Bill Howe, Julia Stoyanovich</dc:creator>
    </item>
    <item>
      <title>Diversity of Answers to Conjunctive Queries</title>
      <link>https://arxiv.org/abs/2301.08848</link>
      <description>arXiv:2301.08848v4 Announce Type: replace 
Abstract: Enumeration problems aim at outputting, without repetition, the set of solutions to a given problem instance. However, outputting the entire solution set may be prohibitively expensive if it is too big. In this case, outputting a small, sufficiently diverse subset of the solutions would be preferable. This leads to the Diverse-version of the original enumeration problem, where the goal is to achieve a certain level d of diversity by selecting k solutions. In this paper, we look at the Diverse-version of the query answering problem for Conjunctive Queries and extensions thereof. That is, we study the problem if it is possible to achieve a certain level d of diversity by selecting k answers to the given query and, in the positive case, to actually compute such k answers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.08848v4</guid>
      <category>cs.DB</category>
      <category>cs.CC</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timo Camillo Merkl, Reinhard Pichler, Sebastian Skritek</dc:creator>
    </item>
    <item>
      <title>RetClean: Retrieval-Based Data Cleaning Using Foundation Models and Data Lakes</title>
      <link>https://arxiv.org/abs/2303.16909</link>
      <description>arXiv:2303.16909v2 Announce Type: replace 
Abstract: Can foundation models (such as ChatGPT) clean your data? In this proposal, we demonstrate that indeed ChatGPT can assist in data cleaning by suggesting corrections for specific cells in a data table (scenario 1). However, ChatGPT may struggle with datasets it has never encountered before (e.g., local enterprise data) or when the user requires an explanation of the source of the suggested clean values. To address these issues, we developed a retrieval-based method that complements ChatGPT's power with a user-provided data lake. The data lake is first indexed, we then retrieve the top-k relevant tuples to the user's query tuple and finally leverage ChatGPT to infer the correct value (scenario 2). Nevertheless, sharing enterprise data with ChatGPT, an externally hosted model, might not be feasible for privacy reasons. To assist with this scenario, we developed a custom RoBERTa-based foundation model that can be locally deployed. By fine-tuning it on a small number of examples, it can effectively make value inferences based on the retrieved tuples (scenario 3). Our proposed system, RetClean, seamlessly supports all three scenarios and provides a user-friendly GUI that enables the VLDB audience to explore and experiment with the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.16909v2</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zan Ahmad Naeem, Mohammad Shahmeer Ahmad, Mohamed Eltabakh, Mourad Ouzzani, Nan Tang</dc:creator>
    </item>
    <item>
      <title>XiYan-SQL: A Multi-Generator Ensemble Framework for Text-to-SQL</title>
      <link>https://arxiv.org/abs/2411.08599</link>
      <description>arXiv:2411.08599v2 Announce Type: replace-cross 
Abstract: To tackle the challenges of large language model performance in natural language to SQL tasks, we introduce XiYan-SQL, an innovative framework that employs a multi-generator ensemble strategy to improve candidate generation. We introduce M-Schema, a semi-structured schema representation method designed to enhance the understanding of database structures. To enhance the quality and diversity of generated candidate SQL queries, XiYan-SQL integrates the significant potential of in-context learning (ICL) with the precise control of supervised fine-tuning. On one hand, we propose a series of training strategies to fine-tune models to generate high-quality candidates with diverse preferences. On the other hand, we implement the ICL approach with an example selection method based on named entity recognition to prevent overemphasis on entities. The refiner optimizes each candidate by correcting logical or syntactical errors. To address the challenge of identifying the best candidate, we fine-tune a selection model to distinguish nuances of candidate SQL queries. The experimental results on multiple dialect datasets demonstrate the robustness of XiYan-SQL in addressing challenges across different scenarios. Overall, our proposed XiYan-SQL achieves the state-of-the-art execution accuracy of 75.63% on Bird benchmark, 89.65% on the Spider test set, 69.86% on SQL-Eval, 41.20% on NL2GQL. The proposed framework not only enhances the quality and diversity of SQL queries but also outperforms previous methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08599v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingqi Gao, Yifu Liu, Xiaoxia Li, Xiaorong Shi, Yin Zhu, Yiming Wang, Shiqi Li, Wei Li, Yuntao Hong, Zhiling Luo, Jinyang Gao, Liyu Mou, Yu Li</dc:creator>
    </item>
  </channel>
</rss>
