<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Mar 2025 04:00:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Dynamically Learning to Integrate in Recurrent Neural Networks</title>
      <link>https://arxiv.org/abs/2503.18754</link>
      <description>arXiv:2503.18754v1 Announce Type: new 
Abstract: Learning to remember over long timescales is fundamentally challenging for recurrent neural networks (RNNs). While much prior work has explored why RNNs struggle to learn long timescales and how to mitigate this, we still lack a clear understanding of the dynamics involved when RNNs learn long timescales via gradient descent. Here we build a mathematical theory of the learning dynamics of linear RNNs trained to integrate white noise. We show that when the initial recurrent weights are small, the dynamics of learning are described by a low-dimensional system that tracks a single outlier eigenvalue of the recurrent weights. This reveals the precise manner in which the long timescale associated with white noise integration is learned. We extend our analyses to RNNs learning a damped oscillatory filter, and find rich dynamical equations for the evolution of a conjugate pair of outlier eigenvalues. Taken together, our analyses build a rich mathematical framework for studying dynamical learning problems salient for both machine learning and neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18754v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Blake Bordelon, Jordan Cotler, Cengiz Pehlevan, Jacob A. Zavatone-Veth</dc:creator>
    </item>
    <item>
      <title>Vision: looking and seeing through our brain's information bottleneck</title>
      <link>https://arxiv.org/abs/2503.18804</link>
      <description>arXiv:2503.18804v1 Announce Type: new 
Abstract: Our brain recognizes only a tiny fraction of sensory input, due to an information processing bottleneck. This blinds us to most visual inputs. Since we are blind to this blindness, only a recent framework highlights this bottleneck by formulating vision as mainly looking and seeing. Looking selects a tiny fraction of visual information for progression through the bottleneck, mainly by shifting gaze to center an attentional spotlight. Seeing decodes, i.e., recognizes, objects within the selected information. Since looking often occurs before seeing and evokes limited awareness, humans have the impression of seeing whole scenes clearly. According to the new framework, the bottleneck starts from the output of the primary visual cortex (V1) to downstream brain areas. This is motivated by the evidence-backed V1 Saliency Hypothesis (V1SH) that V1 creates a saliency map of the visual field to guide looking. Massive visual information loss downstream from V1 makes seeing vulnerable to ambiguity and illusions (errors). To overcome this, feedback from downstream to upstream areas such as V1 queries for additional relevant information. An integral part of this framework is the central-peripheral dichotomy (CPD) theory proposing that vision in the peripheral and central visual fields are specialized for looking (deciding where to shift the gaze) and seeing, respectively, and that the feedback query to aid seeing is mainly directed to the central visual field. This V1SH-Bottleneck-CPD framework predicts that the peripheral visual field, lacking feedback queries, is more vulnerable to illusions, and that such illusions become visible in the central visual field when the feedback query is compromised. We present theoretical predictions, experimental confirmations, a Feedforward-Feedback-Verify-and-reWeight (FFVW) algorithm for seeing through the bottleneck.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18804v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Li Zhaoping</dc:creator>
    </item>
    <item>
      <title>Communities in the Kuramoto Model: Dynamics and Detection via Path Signatures</title>
      <link>https://arxiv.org/abs/2503.17546</link>
      <description>arXiv:2503.17546v1 Announce Type: cross 
Abstract: The behavior of multivariate dynamical processes is often governed by underlying structural connections that relate the components of the system. For example, brain activity which is often measured via time series is determined by an underlying structural graph, where nodes represent neurons or brain regions and edges represent cortical connectivity. Existing methods for inferring structural connections from observed dynamics, such as correlation-based or spectral techniques, may fail to fully capture complex relationships in high-dimensional time series in an interpretable way. Here, we propose the use of path signatures a mathematical framework that encodes geometric and temporal properties of continuous paths to address this problem. Path signatures provide a reparametrization-invariant characterization of dynamical data and, in particular, can be used to compute the lead matrix which reveals lead-lag phenomena. We showcase our approach on time series from coupled oscillators in the Kuramoto model defined on a stochastic block model graph, termed the Kuramoto stochastic block model (KSBM). Using mean-field theory and Gaussian approximations, we analytically derive reduced models of KSBM dynamics in different temporal regimes and theoretically characterize the lead matrix in these settings. Leveraging these insights, we propose a novel signature-based community detection algorithm, achieving exact recovery of structural communities from observed time series in multiple KSBM instances. Our results demonstrate that path signatures provide a novel perspective on analyzing complex neural data and other high-dimensional systems, explicitly exploiting temporal functional relationships to infer underlying structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17546v1</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>nlin.AO</category>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>T\^am Johan Nguy\^en, Darrick Lee, Bernadette Jana Stolz</dc:creator>
    </item>
    <item>
      <title>Feature Learning beyond the Lazy-Rich Dichotomy: Insights from Representational Geometry</title>
      <link>https://arxiv.org/abs/2503.18114</link>
      <description>arXiv:2503.18114v1 Announce Type: cross 
Abstract: The ability to integrate task-relevant information into neural representations is a fundamental aspect of both biological and artificial intelligence. To enable theoretical analysis, recent work has examined whether a network learns task-relevant features (rich learning) or resembles a random feature model (or a kernel machine, i.e., lazy learning). However, this simple lazy-versus-rich dichotomy overlooks the possibility of various subtypes of feature learning that emerge from different architectures, learning rules, and data properties. Furthermore, most existing approaches emphasize weight matrices or neural tangent kernels, limiting their applicability to neuroscience because they do not explicitly characterize representations.
  In this work, we introduce an analysis framework based on representational geometry to study feature learning. Instead of analyzing what are the learned features, we focus on characterizing how task-relevant representational manifolds evolve during the learning process. In both theory and experiment, we find that when a network learns features useful for solving a task, the task-relevant manifolds become increasingly untangled. Moreover, by tracking changes in the underlying manifold geometry, we uncover distinct learning stages throughout training, as well as different learning strategies associated with training hyperparameters, uncovering subtypes of feature learning beyond the lazy-versus-rich dichotomy. Applying our method to neuroscience and machine learning, we gain geometric insights into the structural inductive biases of neural circuits solving cognitive tasks and the mechanisms underlying out-of-distribution generalization in image classification. Our framework provides a novel geometric perspective for understanding and quantifying feature learning in both artificial and biological neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18114v1</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chi-Ning Chou, Hang Le, Yichen Wang, SueYeon Chung</dc:creator>
    </item>
    <item>
      <title>MammAlps: A multi-view video behavior monitoring dataset of wild mammals in the Swiss Alps</title>
      <link>https://arxiv.org/abs/2503.18223</link>
      <description>arXiv:2503.18223v1 Announce Type: cross 
Abstract: Monitoring wildlife is essential for ecology and ethology, especially in light of the increasing human impact on ecosystems. Camera traps have emerged as habitat-centric sensors enabling the study of wildlife populations at scale with minimal disturbance. However, the lack of annotated video datasets limits the development of powerful video understanding models needed to process the vast amount of fieldwork data collected. To advance research in wild animal behavior monitoring we present MammAlps, a multimodal and multi-view dataset of wildlife behavior monitoring from 9 camera-traps in the Swiss National Park. MammAlps contains over 14 hours of video with audio, 2D segmentation maps and 8.5 hours of individual tracks densely labeled for species and behavior. Based on 6135 single animal clips, we propose the first hierarchical and multimodal animal behavior recognition benchmark using audio, video and reference scene segmentation maps as inputs. Furthermore, we also propose a second ecology-oriented benchmark aiming at identifying activities, species, number of individuals and meteorological conditions from 397 multi-view and long-term ecological events, including false positive triggers. We advocate that both tasks are complementary and contribute to bridging the gap between machine learning and ecology. Code and data are available at: https://github.com/eceo-epfl/MammAlps</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18223v1</guid>
      <category>cs.CV</category>
      <category>cs.IR</category>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Valentin Gabeff, Haozhe Qi, Brendan Flaherty, Gencer Sumb\"ul, Alexander Mathis, Devis Tuia</dc:creator>
    </item>
    <item>
      <title>Reading Decisions from Gaze Direction during Graphics Turing Test of Gait Animation</title>
      <link>https://arxiv.org/abs/2503.18619</link>
      <description>arXiv:2503.18619v1 Announce Type: cross 
Abstract: We investigated gaze direction during movement observation. The eye movement data were collected during an experiment, in which different models of movement production (based on movement primitives, MPs) were compared in a two alternatives forced choice task (2AFC).
  Participants observed side-by-side presentation of two naturalistic 3D-rendered human movement videos, where one video was based on motion captured gait sequence, the other one was generated by recombining the machine-learned MPs to approximate the same movement. The task was to discriminate between these movements while their eye movements were recorded. We are complementing previous binary decision data analyses with eye tracking data. Here, we are investigating the role of gaze direction during task execution. We computed the shared information between gaze features and decisions of the participants, and between gaze features and correct answers.
  We found that eye movements reflect the decision of participants during the 2AFC task, but not the correct answer. This result is important for future experiments, which should take advantage of eye tracking to complement binary decision data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18619v1</guid>
      <category>cs.HC</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Knopp, Daniel Auras, Alexander C. Sch\"utz, Dominik Endres</dc:creator>
    </item>
    <item>
      <title>Adaptive Intelligence: leveraging insights from adaptive behavior in animals to build flexible AI systems</title>
      <link>https://arxiv.org/abs/2411.15234</link>
      <description>arXiv:2411.15234v3 Announce Type: replace 
Abstract: Biological intelligence is inherently adaptive -- animals continually adjust their actions based on environmental feedback. However, creating adaptive artificial intelligence (AI) remains a major challenge. The next frontier is to go beyond traditional AI to develop "adaptive intelligence," defined here as harnessing insights from biological intelligence to build agents that can learn online, generalize, and rapidly adapt to changes in their environment. Recent advances in neuroscience offer inspiration through studies that increasingly focus on how animals naturally learn and adapt their world models. In this Perspective, I will review the behavioral and neural foundations of adaptive biological intelligence, the parallel progress in AI, and explore brain-inspired approaches for building more adaptive algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15234v3</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mackenzie Weygandt Mathis</dc:creator>
    </item>
    <item>
      <title>Inferring collective synchrony observing spiking of one or several neurons</title>
      <link>https://arxiv.org/abs/2501.07696</link>
      <description>arXiv:2501.07696v2 Announce Type: replace 
Abstract: We tackle a quantification of synchrony in a large ensemble of interacting neurons from the observation of spiking events. In a simulation study, we efficiently infer the synchrony level in a neuronal population from a point process reflecting spiking of a small number of units and even from a single neuron. We introduce a synchrony measure (order parameter) based on the Bartlett covariance density; this quantity can be easily computed from the recorded point process. This measure is robust concerning missed spikes and, if computed from observing several neurons, does not require spike sorting. We illustrate the approach by modeling populations of spiking or bursting neurons, including the case of sparse synchrony.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07696v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arkady Pikovsky, Michael Rosenblum</dc:creator>
    </item>
    <item>
      <title>When neural implant meets multimodal LLM: A dual-loop system for neuromodulation and naturalistic neuralbehavioral research</title>
      <link>https://arxiv.org/abs/2503.12334</link>
      <description>arXiv:2503.12334v2 Announce Type: replace 
Abstract: We propose a novel dual-loop system that synergistically combines responsive neurostimulation (RNS) implants with artificial intelligence-driven wearable devices for treating post-traumatic stress disorder (PTSD) and enabling naturalistic brain research. In PTSD Therapy Mode, an implanted closed-loop neural device monitors amygdala activity and provides on-demand stimulation upon detecting pathological theta oscillations, while an ensemble of wearables (smart glasses, smartwatches, smartphones) uses multimodal large language model (LLM) analysis of sensory data to detect environmental or physiological PTSD triggers and deliver timely audiovisual interventions. Logged events from both the neural and wearable loops are analyzed to personalize trigger detection and progressively transition patients to non-invasive interventions. In Neuroscience Research Mode, the same platform is adapted for real-world brain activity capture. Wearable-LLM systems recognize naturalistic events (social interactions, emotional situations, compulsive behaviors, decision making) and signal implanted RNS devices (via wireless triggers) to record synchronized intracranial data during these moments. This approach builds on recent advances in mobile intracranial EEG recording and closed-loop neuromodulation in humans (BRAIN Initiative, 2023) (Mobbs et al., 2021). We discuss how our interdisciplinary system could revolutionize PTSD therapy and cognitive neuroscience by enabling 24/7 monitoring, context-aware intervention, and rich data collection outside traditional labs. The vision is a future where AI-enhanced devices continuously collaborate with the human brain, offering therapeutic support and deep insights into neural function, with the resulting real-world context rich neural data, in turn, accelerating the development of more biologically-grounded and human-centric AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12334v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Edward Hong Wang, Cynthia Xin Wen</dc:creator>
    </item>
    <item>
      <title>Oscillatory Signatures of Parkinson's Disease: Central and Parietal EEG Alterations Across Multiple Frequency Bands</title>
      <link>https://arxiv.org/abs/2503.12392</link>
      <description>arXiv:2503.12392v2 Announce Type: replace 
Abstract: This study investigates EEG as a potential early biomarker by applying deep learning techniques to resting-state EEG recordings from 31 subjects (15 with PD and 16 healthy controls). EEG signals underwent preprocessing to remove tremor artifacts before classification with CNNs using wavelet-based electrode triplet images. Our analysis across different brain regions and frequency bands showed distinct spatial-spectral patterns of PD-related neural oscillations. We identified high classification accuracy (76%) using central electrodes (C3, Cz, C4) with full-spectrum 0.4-62.4 Hz analysis and 74% accuracy in right parietal regions (P8, CP6, P4) with 10-second windows. Bilateral centro-parietal regions showed strong performance (67%) in the theta band (4.0-7.79 Hz), while multiple areas demonstrated some sensitivity (65%) in the alpha band (7.8-15.59 Hz). We also observed a distinctive topographical pattern of gamma band (40-62.4 Hz) alterations specifically localized to central-parietal regions, which remained consistent across different temporal windows. In particular, we observed pronounced right-hemisphere involvement across several frequency bands. Unlike previous studies that achieved higher accuracies by potentially including tremor artifacts, our approach isolates genuine neurophysiological alterations in cortical activity. These findings suggest that specific EEG-based oscillatory patterns, especially in central and parietal regions and across multiple frequency bands, may provide diagnostic information for PD, potentially before the onset of motor symptoms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12392v2</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Artem Lensky</dc:creator>
    </item>
    <item>
      <title>Expressivity of Neural Networks with Random Weights and Learned Biases</title>
      <link>https://arxiv.org/abs/2407.00957</link>
      <description>arXiv:2407.00957v3 Announce Type: replace-cross 
Abstract: Landmark universal function approximation results for neural networks with trained weights and biases provided the impetus for the ubiquitous use of neural networks as learning models in neuroscience and Artificial Intelligence (AI). Recent work has extended these results to networks in which a smaller subset of weights (e.g., output weights) are tuned, leaving other parameters random. However, it remains an open question whether universal approximation holds when only biases are learned, despite evidence from neuroscience and AI that biases significantly shape neural responses. The current paper answers this question. We provide theoretical and numerical evidence demonstrating that feedforward neural networks with fixed random weights can approximate any continuous function on compact sets. We further show an analogous result for the approximation of dynamical systems with recurrent neural networks. Our findings are relevant to neuroscience, where they demonstrate the potential for behaviourally relevant changes in dynamics without modifying synaptic weights, as well as for AI, where they shed light on recent fine-tuning methods for large language models, like bias and prefix-based approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00957v3</guid>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ezekiel Williams, Alexandre Payeur, Avery Hee-Woon Ryoo, Thomas Jiralerspong, Matthew G. Perich, Luca Mazzucato, Guillaume Lajoie</dc:creator>
    </item>
  </channel>
</rss>
