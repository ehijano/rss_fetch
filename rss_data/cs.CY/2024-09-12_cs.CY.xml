<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Sep 2024 04:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Ensuring Fairness with Transparent Auditing of Quantitative Bias in AI Systems</title>
      <link>https://arxiv.org/abs/2409.06708</link>
      <description>arXiv:2409.06708v1 Announce Type: new 
Abstract: With the rapid advancement of AI, there is a growing trend to integrate AI into decision-making processes. However, AI systems may exhibit biases that lead decision-makers to draw unfair conclusions. Notably, the COMPAS system used in the American justice system to evaluate recidivism was found to favor racial majority groups; specifically, it violates a fairness standard called equalized odds. Various measures have been proposed to assess AI fairness. We present a framework for auditing AI fairness, involving third-party auditors and AI system providers, and we have created a tool to facilitate systematic examination of AI systems. The tool is open-sourced and publicly available. Unlike traditional AI systems, we advocate a transparent white-box and statistics-based approach. It can be utilized by third-party auditors, AI developers, or the general public for reference when judging the fairness criterion of AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06708v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Chih-Cheng Rex Yuan, Bow-Yaw Wang</dc:creator>
    </item>
    <item>
      <title>A Meta-analysis of College Students' Intention to Use Generative Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2409.06712</link>
      <description>arXiv:2409.06712v1 Announce Type: new 
Abstract: It is of critical importance to analyse the factors influencing college students' intention to use generative artificial intelligence (GenAI) to understand and predict learners' learning behaviours and academic outcomes. Nevertheless, a lack of congruity has been shown in extant research results. This study, therefore, conducted a meta-analysis of 27 empirical studies under an integrated theoretical framework, including 87 effect sizes of independent research and 33,833 sample data. The results revealed that the main variables are strongly correlated with students' behavioural intention to use GenAI. Among them, performance expectancy (r = 0.389) and attitudes (r = 0.576) play particularly critical roles, and effort expectancy and habit are moderated by locational factors. Gender, notably, only moderated attitudes on students' behavioural intention to use GenAI. This study provides valuable insights for addressing the debate regarding students' intention to use GenAI in existed research, improving educational technology, as well as offering support for school decision-makers and educators to apply GenAI in school settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06712v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifei Diao, Ziyi Li, Jiateng Zhou, Wei Gao, Xin Gong</dc:creator>
    </item>
    <item>
      <title>Tailoring Chatbots for Higher Education: Some Insights and Experiences</title>
      <link>https://arxiv.org/abs/2409.06717</link>
      <description>arXiv:2409.06717v1 Announce Type: new 
Abstract: The general availability of powerful Large Language Models had a powerful impact on higher education, yet general models may not always be useful for the associated specialized tasks. When using these models, oftentimes the need for particular domain knowledge becomes quickly apparent, and the desire for customized bots arises. Customization holds the promise of leading to more accurate and contextually relevant responses, enhancing the educational experience. The purpose of this short technical experience report is to describe what "customizing" Large Language Models means in practical terms for higher education institutions. This report thus relates insights and experiences from one particular technical university in Switzerland, ETH Zurich.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06717v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gerd Kortemeyer</dc:creator>
    </item>
    <item>
      <title>Evolutionary Game Dynamics Applied to Strategic Adoption of Immersive Technologies in Cultural Heritage and Tourism</title>
      <link>https://arxiv.org/abs/2409.06720</link>
      <description>arXiv:2409.06720v1 Announce Type: new 
Abstract: Immersive technologies such as Metaverse, AR, and VR are at a crossroads, with many actors pondering their adoption and potential sectors interested in integration. The cultural and tourism industries are particularly impacted, facing significant pressure to make decisions that could shape their future landscapes. Stakeholders' perceptions play a crucial role in this process, influencing the speed and extent of technology adoption. As immersive technologies promise to revolutionize experiences, stakeholders in these fields weigh the benefits and challenges of embracing such innovations. The current choices will likely determine the trajectory of cultural preservation and tourism enhancement, potentially transforming how we engage with history, art, and travel. Starting from a decomposition of stakeholders' perceptions into principal components using Q-methodology, this article employs an evolutionary game model to attempt to map possible scenarios and highlight potential decision-making trajectories. The proposed approach highlights how evolutionary dynamics lead to identifying a dominant long-term strategy that emerges from the complex system of coexistence among various stakeholders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06720v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>econ.TH</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gioacchino Fazio, Stefano Fricano, Claudio Pirrone</dc:creator>
    </item>
    <item>
      <title>Students' Perceived Roles, Opportunities, and Challenges of a Generative AI-powered Teachable Agent: A Case of Middle School Math Class</title>
      <link>https://arxiv.org/abs/2409.06721</link>
      <description>arXiv:2409.06721v1 Announce Type: new 
Abstract: Ongoing advancements in Generative AI (GenAI) have boosted the potential of applying long-standing learning-by-teaching practices in the form of a teachable agent (TA). Despite the recognized roles and opportunities of TAs, less is known about how GenAI could create synergy or introduce challenges in TAs and how students perceived the application of GenAI in TAs. This study explored middle school students perceived roles, benefits, and challenges of GenAI-powered TAs in an authentic mathematics classroom. Through classroom observation, focus-group interviews, and open-ended surveys of 108 sixth-grade students, we found that students expected the GenAI-powered TA to serve as a learning companion, facilitator, and collaborative problem-solver. Students also expressed the benefits and challenges of GenAI-powered TAs. This study provides implications for the design of educational AI and AI-assisted instruction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06721v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yukyeong Song, Jinhee Kim, Zifeng Liu, Chenglu Li, Wanli Xing</dc:creator>
    </item>
    <item>
      <title>Elementary School Students' and Teachers' Perceptions Towards Creative Mathematical Writing with Generative AI</title>
      <link>https://arxiv.org/abs/2409.06723</link>
      <description>arXiv:2409.06723v1 Announce Type: new 
Abstract: While mathematical creative writing can potentially engage students in expressing mathematical ideas in an imaginative way, some elementary school-age students struggle in this process. Generative AI (GenAI) offers possibilities for supporting creative writing activities, such as providing story generation. However, the design of GenAI-powered learning technologies requires careful consideration of the technology reception in the actual classrooms. This study explores students' and teachers' perceptions of creative mathematical writing with the developed GenAI-powered technology. The study adopted a qualitative thematic analysis of the interviews, triangulated with open-ended survey responses and classroom observation of 79 elementary school students, resulting in six themes and 19 subthemes. This study contributes by investigating the lived experience of GenAI-supported learning and the design considerations for GenAI-powered learning technologies and instructions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06723v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yukyeong Song, Jinhee Kim, Wanli Xing, Zifeng Liu, Chenglu Li, Hyunju Oh</dc:creator>
    </item>
    <item>
      <title>How will advanced AI systems impact democracy?</title>
      <link>https://arxiv.org/abs/2409.06729</link>
      <description>arXiv:2409.06729v1 Announce Type: new 
Abstract: Advanced AI systems capable of generating humanlike text and multimodal content are now widely available. In this paper, we discuss the impacts that generative artificial intelligence may have on democratic processes. We consider the consequences of AI for citizens' ability to make informed choices about political representatives and issues (epistemic impacts). We ask how AI might be used to destabilise or support democratic mechanisms like elections (material impacts). Finally, we discuss whether AI will strengthen or weaken democratic principles (foundational impacts). It is widely acknowledged that new AI systems could pose significant challenges for democracy. However, it has also been argued that generative AI offers new opportunities to educate and learn from citizens, strengthen public discourse, help people find common ground, and to reimagine how democracies might work better.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06729v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher Summerfield, Lisa Argyle, Michiel Bakker, Teddy Collins, Esin Durmus, Tyna Eloundou, Iason Gabriel, Deep Ganguli, Kobi Hackenburg, Gillian Hadfield, Luke Hewitt, Saffron Huang, Helene Landemore, Nahema Marchal, Aviv Ovadya, Ariel Procaccia, Mathias Risse, Bruce Schneier, Elizabeth Seger, Divya Siddarth, Henrik Skaug S{\ae}tra, MH Tessler, Matthew Botvinick</dc:creator>
    </item>
    <item>
      <title>Urban context and delivery performance: Modelling service time for cargo bikes and vans across diverse urban environments</title>
      <link>https://arxiv.org/abs/2409.06730</link>
      <description>arXiv:2409.06730v1 Announce Type: new 
Abstract: Light goods vehicles (LGV) used extensively in the last mile of delivery are one of the leading polluters in cities. Cargo-bike logistics and Light Electric Vehicles (LEVs) have been put forward as a high impact candidate for replacing LGVs. Studies have estimated over half of urban van deliveries being replaceable by cargo-bikes, due to their faster speeds, shorter parking times and more efficient routes across cities. However, the logistics sector suffers from a lack of publicly available data, particularly pertaining to cargo-bike deliveries, thus limiting the understanding of their potential benefits. Specifically, service time (which includes cruising for parking, and walking to destination) is a major, but often overlooked component of delivery time modelling. The aim of this study is to establish a framework for measuring the performance of delivery vehicles, with an initial focus on modelling service times of vans and cargo-bikes across diverse urban environments. We introduce two datasets that allow for in-depth analysis and modelling of service times of cargo bikes and use existing datasets to reason about differences in delivery performance across vehicle types. We introduce a modelling framework to predict the service times of deliveries based on urban context. We employ Uber's H3 index to divide cities into hexagonal cells and aggregate OpenStreetMap tags for each cell, providing a detailed assessment of urban context. Leveraging this spatial grid, we use GeoVex to represent micro-regions as points in a continuous vector space, which then serve as input for predicting vehicle service times. We show that geospatial embeddings can effectively capture urban contexts and facilitate generalizations to new contexts and cities. Our methodology addresses the challenge of limited comparative data available for different vehicle types within the same urban settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06730v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maxwell Schrader, Navish Kumar, Esben S{\o}rig, Soonmyeong Yoon, Akash Srivastava, Kai Xu, Maria Astefanoaei, Nicolas Collignon</dc:creator>
    </item>
    <item>
      <title>Understanding and Mitigating the Impacts of Differentially Private Census Data on State Level Redistricting</title>
      <link>https://arxiv.org/abs/2409.06801</link>
      <description>arXiv:2409.06801v1 Announce Type: new 
Abstract: Data from the Decennial Census is published only after applying a disclosure avoidance system (DAS). Data users were shaken by the adoption of differential privacy in the 2020 DAS, a radical departure from past methods. The change raises the question of whether redistricting law permits, forbids, or requires taking account of the effect of disclosure avoidance. Such uncertainty creates legal risks for redistricters, as Alabama argued in a lawsuit seeking to prevent the 2020 DAS's deployment. We consider two redistricting settings in which a data user might be concerned about the impacts of privacy preserving noise: drawing equal population districts and litigating voting rights cases. What discrepancies arise if the user does nothing to account for disclosure avoidance? How might the user adapt her analyses to mitigate those discrepancies? We study these questions by comparing the official 2010 Redistricting Data to the 2010 Demonstration Data -- created using the 2020 DAS -- in an analysis of millions of algorithmically generated state legislative redistricting plans. In both settings, we observe that an analyst may come to incorrect conclusions if they do not account for noise. With minor adaptations, though, the underlying policy goals remain achievable: tweaking selection criteria enables a redistricter to draw balanced plans, and illustrative plans can still be used as evidence of the maximum number of majority-minority districts that are possible in a geography. At least for state legislatures, Alabama's claim that differential privacy ``inhibits a State's right to draw fair lines'' appears unfounded.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06801v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Cianfarani, Aloni Cohen</dc:creator>
    </item>
    <item>
      <title>Personalized Knowledge Tracing through Student Representation Reconstruction and Class Imbalance Mitigation</title>
      <link>https://arxiv.org/abs/2409.06745</link>
      <description>arXiv:2409.06745v1 Announce Type: cross 
Abstract: Knowledge tracing is a technique that predicts students' future performance by analyzing their learning process through historical interactions with intelligent educational platforms, enabling a precise evaluation of their knowledge mastery. Recent studies have achieved significant progress by leveraging powerful deep neural networks. These models construct complex input representations using questions, skills, and other auxiliary information but overlook individual student characteristics, which limits the capability for personalized assessment. Additionally, the available datasets in the field exhibit class imbalance issues. The models that simply predict all responses as correct without substantial effort can yield impressive accuracy. In this paper, we propose PKT, a novel approach for personalized knowledge tracing. PKT reconstructs representations from sequences of interactions with a tutoring platform to capture latent information about the students. Moreover, PKT incorporates focal loss to improve prioritize minority classes, thereby achieving more balanced predictions. Extensive experimental results on four publicly available educational datasets demonstrate the advanced predictive performance of PKT in comparison with 16 state-of-the-art models. To ensure the reproducibility of our research, the code is publicly available at https://anonymous.4open.science/r/PKT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06745v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiyu Chen, Wei Ji, Jing Xiao, Zitao Liu</dc:creator>
    </item>
    <item>
      <title>Minimum Viable Ethics: From Institutionalizing Industry AI Governance to Product Impact</title>
      <link>https://arxiv.org/abs/2409.06926</link>
      <description>arXiv:2409.06926v1 Announce Type: cross 
Abstract: Across the technology industry, many companies have expressed their commitments to AI ethics and created dedicated roles responsible for translating high-level ethics principles into product. Yet it is unclear how effective this has been in leading to meaningful product changes. Through semi-structured interviews with 26 professionals working on AI ethics in industry, we uncover challenges and strategies of institutionalizing ethics work along with translation into product impact. We ultimately find that AI ethics professionals are highly agile and opportunistic, as they attempt to create standardized and reusable processes and tools in a corporate environment in which they have little traditional power. In negotiations with product teams, they face challenges rooted in their lack of authority and ownership over product, but can push forward ethics work by leveraging narratives of regulatory response and ethics as product quality assurance. However, this strategy leaves us with a minimum viable ethics, a narrowly scoped industry AI ethics that is limited in its capacity to address normative issues separate from compliance or product quality. Potential future regulation may help bridge this gap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06926v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Archana Ahlawat, Amy Winecoff, Jonathan Mayer</dc:creator>
    </item>
    <item>
      <title>Legal Fact Prediction: Task Definition and Dataset Construction</title>
      <link>https://arxiv.org/abs/2409.07055</link>
      <description>arXiv:2409.07055v1 Announce Type: cross 
Abstract: Legal facts refer to the facts that can be proven by acknowledged evidence in a trial. They form the basis for the determination of court judgments. This paper introduces a novel NLP task: legal fact prediction, which aims to predict the legal fact based on a list of evidence. The predicted facts can instruct the parties and their lawyers involved in a trial to strengthen their submissions and optimize their strategies during the trial. Moreover, since real legal facts are difficult to obtain before the final judgment, the predicted facts also serve as an important basis for legal judgment prediction. We construct a benchmark dataset consisting of evidence lists and ground-truth legal facts for real civil loan cases, LFPLoan. Our experiments on this dataset show that this task is non-trivial and requires further considerable research efforts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07055v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junkai Liu, Yujie Tong, Hui Huang, Shuyuan Zheng, Muyun Yang, Peicheng Wu, Makoto Onizuka, Chuan Xiao</dc:creator>
    </item>
    <item>
      <title>Regulatory Requirements Engineering in Large Enterprises: An Interview Study on the European Accessibility Act</title>
      <link>https://arxiv.org/abs/2409.07313</link>
      <description>arXiv:2409.07313v1 Announce Type: cross 
Abstract: Context: Regulations, such as the European Accessibility Act (EAA), impact the engineering of software products and services. Managing that impact while providing meaningful inputs to development teams is one of the emerging requirements engineering (RE) challenges.
  Problem: Enterprises conduct Regulatory Impact Analysis (RIA) to consider the effects of regulations on software products offered and formulate requirements at an enterprise level. Despite its practical relevance, we are unaware of any studies on this large-scale regulatory RE process.
  Methodology: We conducted an exploratory interview study of RIA in three large enterprises. We focused on how they conduct RIA, emphasizing cross-functional interactions, and using the EAA as an example.
  Results: RIA, as a regulatory RE process, is conducted to address the needs of executive management and central functions. It involves coordination between different functions and levels of enterprise hierarchy. Enterprises use artifacts to support interpretation and communication of the results of RIA. Challenges to RIA are mainly related to the execution of such coordination and managing the knowledge involved.
  Conclusion: RIA in large enterprises demands close coordination of multiple stakeholders and roles. Applying interpretation and compliance artifacts is one approach to support such coordination. However, there are no established practices for creating and managing such artifacts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07313v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Oleksandr Kosenkov, Michael Unterkalmsteiner, Daniel Mendez, Jannik Fischbach</dc:creator>
    </item>
    <item>
      <title>GitSEED: A Git-backed Automated Assessment Tool for Software Engineering and Programming Education</title>
      <link>https://arxiv.org/abs/2409.07362</link>
      <description>arXiv:2409.07362v1 Announce Type: cross 
Abstract: Due to the substantial number of enrollments in programming courses, a key challenge is delivering personalized feedback to students. The nature of this feedback varies significantly, contingent on the subject and the chosen evaluation method. However, tailoring current Automated Assessment Tools (AATs) to integrate other program analysis tools is not straightforward. Moreover, AATs usually support only specific programming languages, providing feedback exclusively through dedicated websites based on test suites.
  This paper introduces GitSEED, a language-agnostic automated assessment tool designed for Programming Education and Software Engineering (SE) and backed by GitLab. The students interact with GitSEED through GitLab. Using GitSEED, students in Computer Science (CS) and SE can master the fundamentals of git while receiving personalized feedback on their programming assignments and projects. Furthermore, faculty members can easily tailor GitSEED's pipeline by integrating various code evaluation tools (e.g., memory leak detection, fault localization, program repair, etc.) to offer personalized feedback that aligns with the needs of each CS/SE course. Our experiments assess GitSEED's efficacy via comprehensive user evaluation, examining the impact of feedback mechanisms and features on student learning outcomes. Findings reveal positive correlations between GitSEED usage and student engagement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07362v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Pedro Orvalho, Mikol\'a\v{s} Janota, Vasco Manquinho</dc:creator>
    </item>
    <item>
      <title>Towards Fairer Health Recommendations: finding informative unbiased samples via Word Sense Disambiguation</title>
      <link>https://arxiv.org/abs/2409.07424</link>
      <description>arXiv:2409.07424v1 Announce Type: cross 
Abstract: There have been growing concerns around high-stake applications that rely on models trained with biased data, which consequently produce biased predictions, often harming the most vulnerable. In particular, biased medical data could cause health-related applications and recommender systems to create outputs that jeopardize patient care and widen disparities in health outcomes. A recent framework titled Fairness via AI posits that, instead of attempting to correct model biases, researchers must focus on their root causes by using AI to debias data. Inspired by this framework, we tackle bias detection in medical curricula using NLP models, including LLMs, and evaluate them on a gold standard dataset containing 4,105 excerpts annotated by medical experts for bias from a large corpus. We build on previous work by coauthors which augments the set of negative samples with non-annotated text containing social identifier terms. However, some of these terms, especially those related to race and ethnicity, can carry different meanings (e.g., "white matter of spinal cord"). To address this issue, we propose the use of Word Sense Disambiguation models to refine dataset quality by removing irrelevant sentences. We then evaluate fine-tuned variations of BERT models as well as GPT models with zero- and few-shot prompting. We found LLMs, considered SOTA on many NLP tasks, unsuitable for bias detection, while fine-tuned BERT models generally perform well across all evaluated metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07424v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gavin Butts, Pegah Emdad, Jethro Lee, Shannon Song, Chiman Salavati, Willmar Sosa Diaz, Shiri Dori-Hacohen, Fabricio Murai</dc:creator>
    </item>
    <item>
      <title>The Effect of Crypto Rewards in Fundraising: From a Quasi-Experiment to a Dictator Game</title>
      <link>https://arxiv.org/abs/2207.07490</link>
      <description>arXiv:2207.07490v3 Announce Type: replace 
Abstract: Conditional thank-you gifts are one of the most widely used incentives for charitable giving. Past studies explored non-monetary thank-you gifts (e.g., mugs and shirts) and monetary thank-you gifts (e.g., rebates that return some of the donations to the giver). Following the rapid growth of blockchain technology, a novel form of thank-you gifts emerged: the crypto rewards. Through two studies, we analyze crypto thank-you gifts to shed light on fundraising designs in the digital world. In Study I, we examine the Ukrainian government's crypto fundraising plea that accepts donations in both Ethereum and Bitcoin. We find that Ethereum is substantially more effective in enticing giving than Bitcoin, as the hourly donation count increased 706.07% more for Ethereum than for Bitcoin when crypto rewards are present. This is likely because the crypto rewards are more likely to be issued on Ethereum than Bitcoin. However, the decrease in contribution sizes is also more substantial in Ethereum than in Bitcoin in response to the crypto rewards. In Study II, we conducted a laboratory experiment following a dictator game design to investigate the impact of crypto rewards in a more general scenario, with the crypto rewards specified as non-fungible tokens (NFTs). The crypto rewards in Study II carry no monetary value but only serve to recognize donors symbolically. As such, the NFT thank-you gifts did not effectively induce people to donate; a traditional 1:1 donation matching strictly outperforms both the condition without thank-you gifts and the condition with NFT thank-you gifts. Nevertheless, the NFT thank-you gifts effectively increased the contribution sizes, conditional on the choice to give, when the NFT's graphic design primes donor identity and encompasses the charity recipient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.07490v3</guid>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator> Jane (Xue),  Tan, Yong Tan</dc:creator>
    </item>
    <item>
      <title>Macro Ethics Principles for Responsible AI Systems: Taxonomy and Future Directions</title>
      <link>https://arxiv.org/abs/2208.12616</link>
      <description>arXiv:2208.12616v4 Announce Type: replace 
Abstract: Responsible AI must be able to make or support decisions that consider human values and can be justified by human morals. Accommodating values and morals in responsible decision making is supported by adopting a perspective of macro ethics, which views ethics through a holistic lens incorporating social context. Normative ethical principles inferred from philosophy can be used to methodically reason about ethics and make ethical judgements in specific contexts. Operationalising normative ethical principles thus promotes responsible reasoning under the perspective of macro ethics. We survey AI and computer science literature and develop a taxonomy of 21 normative ethical principles which can be operationalised in AI. We describe how each principle has previously been operationalised, highlighting key themes that AI practitioners seeking to implement ethical principles should be aware of. We envision that this taxonomy will facilitate the development of methodologies to incorporate normative ethical principles in reasoning capacities of responsible AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.12616v4</guid>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3672394</arxiv:DOI>
      <arxiv:journal_reference>ACM Comput. Surv. 56, 11, Article 289 (November 2024), 37 pages</arxiv:journal_reference>
      <dc:creator>Jessica Woodgate, Nirav Ajmeri</dc:creator>
    </item>
    <item>
      <title>Programming Skills are Not Enough: a Greedy Strategy to Attract More Girls to Study Computer Science</title>
      <link>https://arxiv.org/abs/2302.06304</link>
      <description>arXiv:2302.06304v2 Announce Type: replace 
Abstract: It has been observed in many studies that female students in general are unwilling to undertake a course of study in ICT. Recent literature has also pointed out that undermining the prejudices of girls with respect to these disciplines is very difficult in adolescence, suggesting that, to be effective, awareness programs on computer disciplines should be offered in pre-school or lower school age. On the other hand, even assuming that large-scale computer literacy programs can be immediately activated in lower schools and kindergartens, we can't wait for &gt;15-20 years before we can appreciate the effectiveness of these programs. The scarcity of women in ICT has a tangible negative impact on countries' technological innovation, which requires immediate action.
  In this paper, we describe a strategy, and the details of a number of programs coordinated by the Engineering and Computer Science Departments at Sapienza University, to make high school girl students aware of the importance of new technologies and ICT. In addition to describing the theoretical approach, the paper offers some project examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.06304v2</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tiziana Catarci, Luca Podo, Daniel Raffini, Paola Velardi</dc:creator>
    </item>
    <item>
      <title>Towards a methodology to consider the environmental impacts of digital agriculture</title>
      <link>https://arxiv.org/abs/2305.09250</link>
      <description>arXiv:2305.09250v2 Announce Type: replace 
Abstract: Agriculture affects global warming, while its yields are threatened by it. Information and communication technology (ICT) is often considered as a potential lever to mitigate this tension, through monitoring and process optimization. However, while agricultural ICT is actively promoted, its environmental impact appears to be overlooked. Possible rebound effects could put at stake its net expected benefits and hamper agriculture sustainability. By adapting environmental footprint assessment methods to digital agriculture context, this research aims at defining a methodology taking into account the environmental footprint of agricultural ICT systems and their required infrastructures. The expected contribution is to propose present and prospective models based on possible digitalization scenarios, in order to assess effects and consequences of different technological paths on agriculture sustainability, sufficiency and resilience. The final results could be useful to enlighten societal debates and political decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.09250v2</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre La Rocca (UB, LaBRI, MANAO)</dc:creator>
    </item>
    <item>
      <title>Moderating Model Marketplaces: Platform Governance Puzzles for AI Intermediaries</title>
      <link>https://arxiv.org/abs/2311.12573</link>
      <description>arXiv:2311.12573v3 Announce Type: replace 
Abstract: The AI development community is increasingly making use of hosting intermediaries such as Hugging Face provide easy access to user-uploaded models and training data. These model marketplaces lower technical deployment barriers for hundreds of thousands of users, yet can be used in numerous potentially harmful and illegal ways. In this article, we explain ways in which AI systems, which can both `contain' content and be open-ended tools, present one of the trickiest platform governance challenges seen to date. We provide case studies of several incidents across three illustrative platforms -- Hugging Face, GitHub and Civitai -- to examine how model marketplaces moderate models. Building on this analysis, we outline important (and yet nevertheless limited) practices that industry has been developing to respond to moderation demands: licensing, access and use restrictions, automated content moderation, and open policy development. While the policy challenge at hand is a considerable one, we conclude with some ideas as to how platforms could better mobilize resources to act as a careful, fair, and proportionate regulatory access point.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.12573v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/17579961.2024.2388914</arxiv:DOI>
      <arxiv:journal_reference>(2024) 16(2) Law Innovation and Technology</arxiv:journal_reference>
      <dc:creator>Robert Gorwa, Michael Veale</dc:creator>
    </item>
    <item>
      <title>Foundational guidelines for enhancing neurotechnology research and development through end-user involvement</title>
      <link>https://arxiv.org/abs/2404.00047</link>
      <description>arXiv:2404.00047v2 Announce Type: replace-cross 
Abstract: Neurotechnologies are increasingly becoming integrated with our everyday lives, our bodies and our mental states. As the popularity and impact of neurotechnology grows, so does our responsibility to ensure we understand its particular implications on its end users, as well as broader ethical and societal implications. Enabling end-users and stakeholders to participate in the development of neurotechnology, from its earliest stages of conception, will help us better navigate our design around these considerations and deliver more impactful technologies. There are many terms and frameworks to articulate the concept of involving end users in the technology development lifecycle, for example: 'Public and Patient Involvement and Engagement' (PPIE), 'lived experience' and 'co-design'. Here we utilise the PPIE framework to develop clear guidelines for implementing a robust involvement process of current and future end-users in neurotechnology. We present best practice guidance for researchers and engineers who are interested in developing and conducting a PPI strategy for their neurotechnology. We provide advice from various online sources to orient individual teams (and funders) to carve up their own approach to meaningful involvement. After an introduction that coveys the tangible and conceptual benefits of user involvement, we guide the reader to develop a general strategy towards setting up their own process. We then help the reader map out their relevant stakeholders and provide advice on how to consider user diversity and representation. We also provide advice on how to quantify the outcomes of the engagement, as well as a check-list to ensure transparency and accountability at various stages. The aim is the establishment of gold-standard methodologies for ensuring that patient and public insights are at the forefront of our scientific inquiry and product development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00047v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Amparo G\"uemes, Tiago da Silva Costa, Tamar Makin</dc:creator>
    </item>
    <item>
      <title>AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts</title>
      <link>https://arxiv.org/abs/2404.05993</link>
      <description>arXiv:2404.05993v2 Announce Type: replace-cross 
Abstract: As Large Language Models (LLMs) and generative AI become more widespread, the content safety risks associated with their use also increase. We find a notable deficiency in high-quality content safety datasets and benchmarks that comprehensively cover a wide range of critical safety areas. To address this, we define a broad content safety risk taxonomy, comprising 13 critical risk and 9 sparse risk categories. Additionally, we curate AEGISSAFETYDATASET, a new dataset of approximately 26, 000 human-LLM interaction instances, complete with human annotations adhering to the taxonomy. We plan to release this dataset to the community to further research and to help benchmark LLM models for safety. To demonstrate the effectiveness of the dataset, we instruction-tune multiple LLM-based safety models. We show that our models (named AEGISSAFETYEXPERTS), not only surpass or perform competitively with the state-of-the-art LLM-based safety models and general purpose LLMs, but also exhibit robustness across multiple jail-break attack categories. We also show how using AEGISSAFETYDATASET during the LLM alignment phase does not negatively impact the performance of the aligned models on MT Bench scores. Furthermore, we propose AEGIS, a novel application of a no-regret online adaptation framework with strong theoretical guarantees, to perform content moderation with an ensemble of LLM content safety experts in deployment</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05993v2</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaona Ghosh, Prasoon Varshney, Erick Galinkin, Christopher Parisien</dc:creator>
    </item>
    <item>
      <title>A Principled Approach for a New Bias Measure</title>
      <link>https://arxiv.org/abs/2405.12312</link>
      <description>arXiv:2405.12312v2 Announce Type: replace-cross 
Abstract: The widespread use of machine learning and data-driven algorithms for decision making has been steadily increasing over many years. The areas in which this is happening are diverse: healthcare, employment, finance, education, the legal system to name a few; and the associated negative side effects are being increasingly harmful for society. Negative data \emph{bias} is one of those, which tends to result in harmful consequences for specific groups of people. Any mitigation strategy or effective policy that addresses the negative consequences of bias must start with awareness that bias exists, together with a way to understand and quantify it. However, there is a lack of consensus on how to measure data bias and oftentimes the intended meaning is context dependent and not uniform within the research community. The main contributions of our work are: (1) The definition of Uniform Bias (UB), the first bias measure with a clear and simple interpretation in the full range of bias values. (2) A systematic study to characterize the flaws of existing measures in the context of anti employment discrimination rules used by the Office of Federal Contract Compliance Programs, additionally showing how UB solves open problems in this domain. (3) A framework that provides an efficient way to derive a mathematical formula for a bias measure based on an algorithmic specification of bias addition. Our results are experimentally validated using nine publicly available datasets and theoretically analyzed, which provide novel insights about the problem. Based on our approach, we also design a bias mitigation model that might be useful to policymakers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12312v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bruno Scarone, Alfredo Viola, Ren\'ee J. Miller, Ricardo Baeza-Yates</dc:creator>
    </item>
    <item>
      <title>The Human Factor in AI Red Teaming: Perspectives from Social and Collaborative Computing</title>
      <link>https://arxiv.org/abs/2407.07786</link>
      <description>arXiv:2407.07786v2 Announce Type: replace-cross 
Abstract: Rapid progress in general-purpose AI has sparked significant interest in "red teaming," a practice of adversarial testing originating in military and cybersecurity applications. AI red teaming raises many questions about the human factor, such as how red teamers are selected, biases and blindspots in how tests are conducted, and harmful content's psychological effects on red teamers. A growing body of HCI and CSCW literature examines related practices-including data labeling, content moderation, and algorithmic auditing. However, few, if any have investigated red teaming itself. Future studies may explore topics ranging from fairness to mental health and other areas of potential harm. We aim to facilitate a community of researchers and practitioners who can begin to meet these challenges with creativity, innovation, and thoughtful reflection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07786v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3678884.3687147</arxiv:DOI>
      <dc:creator>Alice Qian Zhang, Ryland Shaw, Jacy Reese Anthis, Ashlee Milton, Emily Tseng, Jina Suh, Lama Ahmad, Ram Shankar Siva Kumar, Julian Posada, Benjamin Shestakofsky, Sarah T. Roberts, Mary L. Gray</dc:creator>
    </item>
    <item>
      <title>Mpox Narrative on Instagram: A Labeled Multilingual Dataset of Instagram Posts on Mpox for Sentiment, Hate Speech, and Anxiety Analysis</title>
      <link>https://arxiv.org/abs/2409.05292</link>
      <description>arXiv:2409.05292v2 Announce Type: replace-cross 
Abstract: The world is currently experiencing an outbreak of mpox, which has been declared a Public Health Emergency of International Concern by WHO. No prior work related to social media mining has focused on the development of a dataset of Instagram posts about the mpox outbreak. The work presented in this paper aims to address this research gap and makes two scientific contributions to this field. First, it presents a multilingual dataset of 60,127 Instagram posts about mpox, published between July 23, 2022, and September 5, 2024. The dataset, available at https://dx.doi.org/10.21227/7fvc-y093, contains Instagram posts about mpox in 52 languages. For each of these posts, the Post ID, Post Description, Date of publication, language, and translated version of the post (translation to English was performed using the Google Translate API) are presented as separate attributes in the dataset. After developing this dataset, sentiment analysis, hate speech detection, and anxiety or stress detection were performed. This process included classifying each post into (i) one of the sentiment classes, i.e., fear, surprise, joy, sadness, anger, disgust, or neutral, (ii) hate or not hate, and (iii) anxiety/stress detected or no anxiety/stress detected. These results are presented as separate attributes in the dataset. Second, this paper presents the results of performing sentiment analysis, hate speech analysis, and anxiety or stress analysis. The variation of the sentiment classes - fear, surprise, joy, sadness, anger, disgust, and neutral were observed to be 27.95%, 2.57%, 8.69%, 5.94%, 2.69%, 1.53%, and 50.64%, respectively. In terms of hate speech detection, 95.75% of the posts did not contain hate and the remaining 4.25% of the posts contained hate. Finally, 72.05% of the posts did not indicate any anxiety/stress, and the remaining 27.95% of the posts represented some form of anxiety/stress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05292v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nirmalya Thakur</dc:creator>
    </item>
  </channel>
</rss>
