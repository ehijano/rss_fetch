<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 Jan 2025 05:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>SyNPar: Synthetic Null Data Parallelism for High-Power False Discovery Rate Control in High-Dimensional Variable Selection</title>
      <link>https://arxiv.org/abs/2501.05012</link>
      <description>arXiv:2501.05012v1 Announce Type: new 
Abstract: Balancing false discovery rate (FDR) and statistical power to ensure reliable discoveries is a key challenge in high-dimensional variable selection. Although several FDR control methods have been proposed, most involve perturbing the original data, either by concatenating knockoff variables or splitting the data into two halves, both of which can lead to a loss of power. In this paper, we introduce a novel approach called Synthetic Null Parallelism (SyNPar), which controls the FDR in high-dimensional variable selection while preserving the original data. SyNPar generates synthetic null data from a model fitted to the original data and modified to reflect the null hypothesis. It then applies the same estimation procedure in parallel to both the original and synthetic null data to estimate coefficients that indicate feature importance. By comparing the coefficients estimated from the null data with those from the original data, SyNPar effectively identifies false positives, functioning as a numerical analog of a likelihood ratio test. We provide theoretical guarantees for FDR control at any desired level while ensuring that the power approaches one with high probability asymptotically. SyNPar is straightforward to implement and can be applied to a wide range of statistical models, including high-dimensional linear regression, generalized linear models, Cox models, and Gaussian graphical models. Through extensive simulations and real data applications, we demonstrate that SyNPar outperforms state-of-the-art methods, including knockoffs and data-splitting methods, in terms of FDR control, power, and computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05012v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Changhu Wang, Ziheng Zhang, Jingyi Jessica Li</dc:creator>
    </item>
    <item>
      <title>Testing Equality of Medians for Multiple Samples</title>
      <link>https://arxiv.org/abs/2501.05136</link>
      <description>arXiv:2501.05136v1 Announce Type: new 
Abstract: In this paper, we construct a consistent non-parametric test for testing the equality of population medians for different samples when the observations in each sample are independent and identically distributed. This test can be further used to test the equality of unknown location parameters for different samples. The method discussed in this paper can be extended to any quantile level instead of the median. We present the theoretical results and also demonstrate the performance of this test through simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05136v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Swapnaneel Bhattacharyya</dc:creator>
    </item>
    <item>
      <title>Improved MCMC with active subspaces</title>
      <link>https://arxiv.org/abs/2501.05144</link>
      <description>arXiv:2501.05144v1 Announce Type: new 
Abstract: Constantine et al. (2016) introduced a Metropolis-Hastings (MH) approach that target the active subspace of a posterior distribution: a linearly projected subspace that is informed by the likelihood. Schuster et al. (2017) refined this approach to introduce a pseudo-marginal Metropolis-Hastings, integrating out inactive variables through estimating a marginal likelihood at every MH iteration. In this paper we show empirically that the effectiveness of these approaches is limited in the case where the linearity assumption is violated, and suggest a particle marginal Metropolis-Hastings algorithm as an alternative for this situation. Finally, the high computational cost of these approaches leads us to consider alternative approaches to using active subspaces in MCMC that avoid the need to estimate a marginal likelihood: we introduce Metropolis-within-Gibbs and Metropolis-within-particle Gibbs methods that provide a more computationally efficient use of the active subspace.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05144v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leonardo Ripoli, Richard G. Everitt</dc:creator>
    </item>
    <item>
      <title>Entropy Adjusted Graphical Lasso for Sparse Precision Matrix Estimation</title>
      <link>https://arxiv.org/abs/2501.05308</link>
      <description>arXiv:2501.05308v1 Announce Type: new 
Abstract: The estimation of a precision matrix is a crucial problem in various research fields, particularly when working with high dimensional data. In such settings, the most common approach is to use the penalized maximum likelihood. The literature typically employs Lasso, Ridge and Elastic Net norms, which effectively shrink the entries of the estimated precision matrix. Although these shrinkage approaches provide well-conditioned precision matrix estimates, they do not explicitly address the uncertainty associated with these estimated matrices. In fact, as the matrix becomes sparser, the precision matrix imposes fewer restrictions, leading to greater variability in the distribution, and thus, to higher entropy. In this paper, we introduce an entropy-adjusted extension of widely used Graphical Lasso using an additional log-determinant penalty term. The objective of the proposed technique is to impose sparsity on the precision matrix estimate and adjust the uncertainty through the log-determinant term. The advantage of the proposed method compared to the existing ones in the literature is evaluated through comprehensive numerical analyses, including both simulated and real-world datasets. The results demonstrate its benefits compared to existing approaches in the literature, with respect to several evaluation metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05308v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vahe Avagyan</dc:creator>
    </item>
    <item>
      <title>RieszBoost: Gradient Boosting for Riesz Regression</title>
      <link>https://arxiv.org/abs/2501.04871</link>
      <description>arXiv:2501.04871v1 Announce Type: cross 
Abstract: Answering causal questions often involves estimating linear functionals of conditional expectations, such as the average treatment effect or the effect of a longitudinal modified treatment policy. By the Riesz representation theorem, these functionals can be expressed as the expected product of the conditional expectation of the outcome and the Riesz representer, a key component in doubly robust estimation methods. Traditionally, the Riesz representer is estimated indirectly by deriving its explicit analytical form, estimating its components, and substituting these estimates into the known form (e.g., the inverse propensity score). However, deriving or estimating the analytical form can be challenging, and substitution methods are often sensitive to practical positivity violations, leading to higher variance and wider confidence intervals. In this paper, we propose a novel gradient boosting algorithm to directly estimate the Riesz representer without requiring its explicit analytical form. This method is particularly suited for tabular data, offering a flexible, nonparametric, and computationally efficient alternative to existing methods for Riesz regression. Through simulation studies, we demonstrate that our algorithm performs on par with or better than indirect estimation techniques across a range of functionals, providing a user-friendly and robust solution for estimating causal quantities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04871v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaitlyn J. Lee, Alejandro Schuler</dc:creator>
    </item>
    <item>
      <title>A Look into How Machine Learning is Reshaping Engineering Models: the Rise of Analysis Paralysis, Optimal yet Infeasible Solutions, and the Inevitable Rashomon Paradox</title>
      <link>https://arxiv.org/abs/2501.04894</link>
      <description>arXiv:2501.04894v1 Announce Type: cross 
Abstract: The widespread acceptance of empirically derived codal provisions and equations in civil engineering stands in stark contrast to the skepticism facing machine learning (ML) models, despite their shared statistical foundations. This paper examines this philosophical tension through the lens of structural engineering and explores how integrating ML challenges traditional engineering philosophies and professional identities. Recent efforts have documented how ML enhances predictive accuracy, optimizes designs, and analyzes complex behaviors. However, one might also raise concerns about the diminishing role of human intuition and the interpretability of algorithms. To showcase this rarely explored front, this paper presents how ML can be successfully integrated into various engineering problems by means of formulation via deduction, induction, and abduction. Then, this paper identifies three principal paradoxes that could arise when adopting ML: analysis paralysis (increased prediction accuracy leading to a reduced understanding of physical mechanisms), infeasible solutions (optimization resulting in unconventional designs that challenge engineering intuition), and the Rashomon effect (where contradictions in explainability methods and physics arise). This paper concludes by addressing these paradoxes and arguing the need to rethink epistemological shifts in engineering and engineering education and methodologies to harmonize traditional principles with ML.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04894v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>MZ Naser</dc:creator>
    </item>
    <item>
      <title>Quantum-enhanced causal discovery for a small number of samples</title>
      <link>https://arxiv.org/abs/2501.05007</link>
      <description>arXiv:2501.05007v1 Announce Type: cross 
Abstract: The discovery of causal relationships from observed data has attracted significant interest from disciplines such as economics, social sciences, epidemiology, and biology. In practical applications, considerable knowledge of the underlying systems is often unavailable, and real data are often associated with nonlinear causal structures, which make the direct use of most conventional causality analysis methods difficult. This study proposes a novel quantum Peter-Clark (qPC) algorithm for causal discovery that does not assume any underlying model structures. Based on the independence conditional tests in a class of reproducing kernel Hilbert spaces characterized by quantum circuits, the proposed qPC algorithm can explore causal relationships from the observed data drawn from arbitrary distributions. We conducted systematic experiments on fundamental graph parts of causal structures, demonstrating that the qPC algorithm exhibits a significantly better performance, particularly with smaller sample sizes compared to its classical counterpart. Furthermore, we proposed a novel optimization approach based on Kernel Target Alignment (KTA) for determining hyperparameters of quantum kernels. This method effectively reduced the risk of false positives in causal discovery, enabling more reliable inference. Our theoretical and experimental results demonstrate that the proposed quantum algorithm can empower classical algorithms for robust and accurate inference in causal discovery, supporting them in regimes where classical algorithms typically fail. Additionally, the effectiveness of this method was validated using the Boston Housing dataset as a real-world application. These findings demonstrate the new potential of quantum circuit-based causal discovery methods in addressing practical challenges, particularly in small-sample scenarios where traditional approaches have shown limitations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05007v1</guid>
      <category>quant-ph</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yota Maeda, Ken Arai, Yu Tanaka, Yu Terada, Hiroshi Ueno, Hiroyuki Tezuka</dc:creator>
    </item>
    <item>
      <title>An Algorithmic Approach for Causal Health Equity: A Look at Race Differentials in Intensive Care Unit (ICU) Outcomes</title>
      <link>https://arxiv.org/abs/2501.05197</link>
      <description>arXiv:2501.05197v1 Announce Type: cross 
Abstract: The new era of large-scale data collection and analysis presents an opportunity for diagnosing and understanding the causes of health inequities. In this study, we describe a framework for systematically analyzing health disparities using causal inference. The framework is illustrated by investigating racial and ethnic disparities in intensive care unit (ICU) outcome between majority and minority groups in Australia (Indigenous vs. Non-Indigenous) and the United States (African-American vs. White). We demonstrate that commonly used statistical measures for quantifying inequity are insufficient, and focus on attributing the observed disparity to the causal mechanisms that generate it. We find that minority patients are younger at admission, have worse chronic health, are more likely to be admitted for urgent and non-elective reasons, and have higher illness severity. At the same time, however, we find a protective direct effect of belonging to a minority group, with minority patients showing improved survival compared to their majority counterparts, with all other variables kept equal. We demonstrate that this protective effect is related to the increased probability of being admitted to ICU, with minority patients having an increased risk of ICU admission. We also find that minority patients, while showing improved survival, are more likely to be readmitted to ICU. Thus, due to worse access to primary health care, minority patients are more likely to end up in ICU for preventable conditions, causing a reduction in the mortality rates and creating an effect that appears to be protective. Since the baseline risk of ICU admission may serve as proxy for lack of access to primary care, we developed the Indigenous Intensive Care Equity (IICE) Radar, a monitoring system for tracking the over-utilization of ICU resources by the Indigenous population of Australia across geographical areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05197v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Drago Plecko, Paul Secombe, Andrea Clarke, Amelia Fiske, Samarra Toby, Donisha Duff, David Pilcher, Leo Anthony Celi, Rinaldo Bellomo, Elias Bareinboim</dc:creator>
    </item>
    <item>
      <title>Comparing latent inequality with ordinal data</title>
      <link>https://arxiv.org/abs/2501.05338</link>
      <description>arXiv:2501.05338v1 Announce Type: cross 
Abstract: We propose new ways to compare two latent distributions when only ordinal data are available and without imposing parametric assumptions on the underlying continuous distributions. First, we contribute identification results. We show how certain ordinal conditions provide evidence of between-group inequality, quantified by particular quantiles being higher in one latent distribution than in the other. We also show how other ordinal conditions provide evidence of higher within-group inequality in one distribution than in the other, quantified by particular interquantile ranges being wider in one latent distribution than in the other. Second, we propose an "inner" confidence set for the quantiles that are higher for the first latent distribution. We also describe frequentist and Bayesian inference on features of the ordinal distributions relevant to our identification results. Our contributions are illustrated by empirical examples with mental health and general health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05338v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/ectj/utac030</arxiv:DOI>
      <arxiv:journal_reference>The Econometrics Journal 26 (2023) 189-214</arxiv:journal_reference>
      <dc:creator>David M. Kaplan, Wei Zhao</dc:creator>
    </item>
    <item>
      <title>Fast Nonseparable Gaussian Stochastic Process with Application to Methylation Level Interpolation</title>
      <link>https://arxiv.org/abs/1711.11501</link>
      <description>arXiv:1711.11501v5 Announce Type: replace 
Abstract: Gaussian stochastic process (GaSP) has been widely used as a prior over functions due to its flexibility and tractability in modeling. However, the computational cost in evaluating the likelihood is $O(n^3)$, where $n$ is the number of observed points in the process, as it requires to invert the covariance matrix. This bottleneck prevents GaSP being widely used in large-scale data. We propose a general class of nonseparable GaSP models for multiple functional observations with a fast and exact algorithm, in which the computation is linear ($O(n)$) and exact, requiring no approximation to compute the likelihood. We show that the commonly used linear regression and separable models are special cases of the proposed nonseparable GaSP model. Through the study of an epigenetic application, the proposed nonseparable GaSP model can accurately predict the genome-wide DNA methylation levels and compares favorably to alternative methods, such as linear regression, random forest and localized Kriging method. The algorithm for fast computation is implemented in the ${\tt FastGaSP}$ R package on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:1711.11501v5</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/10618600.2019.1665534</arxiv:DOI>
      <arxiv:journal_reference>Journal of Computational and Graphical Statistics, 29:2, 250-260 (2020)</arxiv:journal_reference>
      <dc:creator>Mengyang Gu, Yanxun Xu</dc:creator>
    </item>
    <item>
      <title>Likelihood Inference for Possibly Non-Stationary Processes via Adaptive Overdifferencing</title>
      <link>https://arxiv.org/abs/2011.04168</link>
      <description>arXiv:2011.04168v4 Announce Type: replace 
Abstract: We make an observation that facilitates exact likelihood-based inference for the parameters of the popular ARFIMA model without requiring stationarity by allowing the upper bound $\bar{d}$ for the memory parameter $d$ to exceed $0.5$: estimating the parameters of a single non-stationary ARFIMA model is equivalent to estimating the parameters of a sequence of stationary ARFIMA models. This allows for the use of existing methods for evaluating the likelihood for an invertible and stationary ARFIMA model. This enables improved inference because many standard methods perform poorly when estimates are close to the boundary of the parameter space. It also allows us to leverage the wealth of likelihood approximations that have been introduced for estimating the parameters of a stationary process. We explore how estimation of the memory parameter $d$ depends on the upper bound $\bar{d}$ and introduce adaptive procedures for choosing $\bar{d}$. We show via simulation how our adaptive procedures estimate the memory parameter well, relative to existing alternatives, when the true value is as large as 2.5.</description>
      <guid isPermaLink="false">oai:arXiv.org:2011.04168v4</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maryclare Griffin, Gennady Samorodnitsky, David S. Matteson</dc:creator>
    </item>
    <item>
      <title>Stable Distillation and High-Dimensional Hypothesis Testing</title>
      <link>https://arxiv.org/abs/2212.12539</link>
      <description>arXiv:2212.12539v4 Announce Type: replace 
Abstract: While powerful methods have been developed for high-dimensional hypothesis testing assuming orthogonal parameters, current approaches struggle to generalize to the more common non-orthogonal case. We propose Stable Distillation (SD), a simple paradigm for iteratively extracting independent pieces of information from observed data, assuming a parametric model. When applied to hypothesis testing for large regression models, SD orthogonalizes the effect estimates of non-orthogonal predictors by judiciously introducing noise into the observed outcomes vector, yielding mutually independent p-values across predictors. Generic regression and gene-testing simulations show that SD yields a scalable approach for non-orthogonal designs that exceeds or matches the power of existing methods against sparse alternatives. While we only present explicit SD algorithms for hypothesis testing in ordinary least squares and logistic regression, we provide general guidance for deriving and improving the power of SD procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.12539v4</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryan Christ, Ira Hall, David Steinsaltz</dc:creator>
    </item>
    <item>
      <title>Quantile balancing inverse probability weighting for non-probability samples</title>
      <link>https://arxiv.org/abs/2403.09726</link>
      <description>arXiv:2403.09726v4 Announce Type: replace 
Abstract: The use of non-probability data sources for statistical purposes and for official statistics has become increasingly popular in recent years. However, statistical inference based on non-probability samples is made more difficult by nature of their biasedness and lack of representativity. In this paper we propose quantile balancing inverse probability weighting estimator (QBIPW) for non-probability samples. We apply the idea of Harms and Duchesne (2006) allowing the use of quantile information in the estimation process to reproduce known totals and the distribution of auxiliary variables. We discuss the estimation of the QBIPW probabilities and its variance. Our simulation study has demonstrated that the proposed estimators are robust against model mis-specification and, as a result, help to reduce bias and mean squared error. Finally, we applied the proposed methods to estimate the share of job vacancies aimed at Ukrainian workers in Poland using an integrated set of administrative and survey data about job vacancies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09726v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maciej Ber\k{e}sewicz, Marcin Szymkowiak, Piotr Chlebicki</dc:creator>
    </item>
    <item>
      <title>Unlocking the Power of Time-Since-Infection Models: Data Augmentation for Improved Instantaneous Reproduction Number Estimation</title>
      <link>https://arxiv.org/abs/2403.12243</link>
      <description>arXiv:2403.12243v2 Announce Type: replace 
Abstract: The Time Since Infection (TSI) models, which use disease surveillance data to model infectious diseases, have become increasingly popular due to their flexibility and capacity to address complex disease control questions. However, a notable limitation of TSI models is their primary reliance on incidence data. Even when hospitalization data are available, existing TSI models have not been crafted to improve the estimation of disease transmission or to estimate hospitalization-related parameters - metrics crucial for understanding a pandemic and planning hospital resources. Moreover, their dependence on reported infection data makes them vulnerable to variations in data quality. In this study, we advance TSI models by integrating hospitalization data, marking a significant step forward in modeling with TSI models. We introduce hospitalization propensity parameters to jointly model incidence and hospitalization data. We use a composite likelihood function to accommodate complex data structure and an Monte Carlo expectation-maximization algorithm to estimate model parameters. We analyze COVID-19 data to estimate disease transmission, assess risk factor impacts, and calculate hospitalization propensity. Our model improves the accuracy of estimating the instantaneous reproduction number in TSI models, particularly when hospitalization data is of higher quality than incidence data. It enables the estimation of key infectious disease parameters without relying on contact tracing data and provides a foundation for integrating TSI models with other infectious disease models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12243v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiasheng Shi, Yizhao Zhou, Jing Huang</dc:creator>
    </item>
    <item>
      <title>A spatial-correlated multitask linear mixed-effects model for imaging genetics</title>
      <link>https://arxiv.org/abs/2407.04530</link>
      <description>arXiv:2407.04530v2 Announce Type: replace 
Abstract: Imaging genetics aims to uncover the hidden relationship between imaging quantitative traits (QTs) and genetic markers (e.g. single nucleotide polymorphism (SNP)), and brings valuable insights into the pathogenesis of complex diseases, such as cancers and cognitive disorders (e.g. the Alzheimer's Disease). However, most linear models in imaging genetics didn't explicitly model the inner relationship among QTs, which might miss some potential efficiency gains from information borrowing across brain regions. In this work, we developed a novel Bayesian regression framework for identifying significant associations between QTs and genetic markers while explicitly modeling spatial dependency between QTs, with the main contributions as follows. Firstly, we developed a spatial-correlated multitask linear mixed-effects model (LMM) to account for dependencies between QTs. We incorporated a population-level mixed effects term into the model, taking full advantage of the dependent structure of brain imaging-derived QTs. Secondly, we implemented the model in the Bayesian framework and derived a Markov chain Monte Carlo (MCMC) algorithm to achieve the model inference. Further, we incorporated the MCMC samples with the Cauchy combination test (CCT) to examine the association between SNPs and QTs, which avoided computationally intractable multi-test issues. The simulation studies indicated improved power of our proposed model compared to classic models where inner dependencies of QTs were not modeled. We also applied the new spatial model to an imaging dataset obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04530v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhibin Pu, Shufei Ge</dc:creator>
    </item>
    <item>
      <title>Improving Causal Estimation by Mixing Samples to Address Weak Overlap in Observational Studies</title>
      <link>https://arxiv.org/abs/2411.10801</link>
      <description>arXiv:2411.10801v2 Announce Type: replace 
Abstract: Sufficient overlap of propensity scores is one of the most critical assumptions in observational studies. Researchers have found that violation of the assumption can result in substantial bias or increase in variability of estimated treatment effects. To overcome this, we introduce a simple yet novel strategy, mixing, generating a new treated group by mixing the original treated and control units to estimate causal effects. Our strategy has three key advantages: (1) Improvement in estimators' accuracy, regardless of level of positivity, (2) Identical targeting population of treatment effects, and (3) High adaptability for various estimation methods. The mixed sample incorporates propensity scores that are robust to weak overlap and is shown to be useful in balancing covariates with both traditional and modern weighting methods. The estimation of propensity score weighting is done within the M-estimation theory. Implementation into a broader class of weighting estimators is derived through a variation of a resampling algorithm. We illustrate this with several extensive simulation studies and guide the reader with a real-data analysis for practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10801v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaehyuk Jang, Suehyun Kim, Kwonsang Lee</dc:creator>
    </item>
    <item>
      <title>Multimodal Symmetric Circular Distributions Based on Nonnegative Trigonometric Sums and a Likelihood Ratio Test for Reflective Symmetry</title>
      <link>https://arxiv.org/abs/2412.19501</link>
      <description>arXiv:2412.19501v2 Announce Type: replace 
Abstract: Fern\'andez-Dur\'an (2004) developed a family of circular distributions based on nonnegative trigonometric sums (NNTS) which is flexible for modeling datasets exhibiting multimodality and asymmetry. Many datasets involving angles in the natural sciences, such as animal movement in biology, are expected to exhibit reflective symmetry with respect to a central angle (axis) of symmetry. Testing for symmetry in the underlying circular density from which these angles are generated is crucial. Additionally, such densities often display multimodality. This paper identifies the conditions under which NNTS distributions are reflective symmetric and develops a likelihood ratio test for reflective symmetry. The proposed methodology is demonstrated through applications to simulated and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19501v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juan Jos\'e Fern\'andez-Dur\'an, Mar\'ia Mercedes Gregorio-Dom\'inguez</dc:creator>
    </item>
    <item>
      <title>Markov-switching State Space Models for Uncovering Musical Interpretation</title>
      <link>https://arxiv.org/abs/1907.06244</link>
      <description>arXiv:1907.06244v2 Announce Type: replace-cross 
Abstract: For concertgoers, musical interpretation is the most important factor in determining whether or not we enjoy a classical performance. Every performance includes mistakes -- intonation issues, a lost note, an unpleasant sound -- but these are all easily forgotten (or unnoticed) when a performer engages her audience, imbuing a piece with novel emotional content beyond the vague instructions inscribed on the printed page. In this research, we use data from the CHARM Mazurka Project -- forty-six professional recordings of Chopin's Mazurka Op. 68 No. 3 by consummate artists -- with the goal of elucidating musically interpretable performance decisions. We focus specifically on each performer's use musical tempo by examining the inter-onset intervals of the note attacks in the recording. To explain these tempo decisions, we develop a switching state space model and estimate it by maximum likelihood combined with prior information gained from music theory and performance practice. We use the estimated parameters to quantitatively describe individual performance decisions and compare recordings. These comparisons suggest methods for informing music instruction, discovering listening preferences, and analyzing performances.</description>
      <guid isPermaLink="false">oai:arXiv.org:1907.06244v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1214/21-AOAS1457</arxiv:DOI>
      <arxiv:journal_reference>Ann. Appl. Stat. 15(3): 1147-1170 (September 2021)</arxiv:journal_reference>
      <dc:creator>Daniel J. McDonald, Michael McBride, Yupeng Gu, Christopher Raphael</dc:creator>
    </item>
    <item>
      <title>Space-Time Smoothing of Survey Outcomes using the R Package SUMMER</title>
      <link>https://arxiv.org/abs/2007.05117</link>
      <description>arXiv:2007.05117v2 Announce Type: replace-cross 
Abstract: The increasing availability of complex survey data, and the continued need for estimates of demographic and health indicators at a fine spatial and temporal scale, which leads to issues of data sparsity, has led to the need for spatio-temporal smoothing methods that acknowledge the manner in which the data were collected. The open source R package SUMMER implements a variety of methods for spatial or spatio-temporal smoothing of survey data. The emphasis is on small-area estimation. We focus primarily on indicators in a low and middle-income countries context. Our methods are particularly useful for data from Demographic Health Surveys and Multiple Indicator Cluster Surveys. We build upon functions within the survey package, and use INLA for fast Bayesian computation. This paper includes a brief overview of these methods and illustrates the workflow of accessing and processing surveys, estimating subnational child mortality rates, and visualizing results with both simulated data and DHS surveys.</description>
      <guid isPermaLink="false">oai:arXiv.org:2007.05117v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zehang Richard Li, Bryan D Martin, Tracy Qi Dong, Geir-Arne Fuglstad, John Paige, Andrea Riebler, Samuel Clark, Jon Wakefield</dc:creator>
    </item>
    <item>
      <title>Linear Multidimensional Regression with Interactive Fixed-Effects</title>
      <link>https://arxiv.org/abs/2209.11691</link>
      <description>arXiv:2209.11691v5 Announce Type: replace-cross 
Abstract: This paper studies a linear and additively separable regression model for multidimensional panel data of three or more dimensions with unobserved interactive fixed effects. The main estimator follows a double debias approach, and requires two preliminary steps to control unobserved heterogeneity. First, the model is embedded within the standard two-dimensional panel framework and restrictions are formed under which the factor structure methods in Bai (2009) lead to consistent estimation of model parameters, but at slow rates of convergence. The second step develops a weighted fixed-effects method that is robust to the multidimensional nature of the problem and achieves the parametric rate of consistency. This second step is combined with a double debias procedure for asymptotically normal slope estimates. The methods are implemented to estimate the demand elasticity for beer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.11691v5</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hugo Freeman</dc:creator>
    </item>
    <item>
      <title>Doubly Robust Uniform Confidence Bands for Group-Time Conditional Average Treatment Effects in Difference-in-Differences</title>
      <link>https://arxiv.org/abs/2305.02185</link>
      <description>arXiv:2305.02185v3 Announce Type: replace-cross 
Abstract: We consider a panel data analysis to examine the heterogeneity in treatment effects with respect to a pre-treatment covariate of interest in the staggered difference-in-differences setting of Callaway and Sant'Anna (2021). Under standard identification conditions, a doubly robust estimand conditional on the covariate identifies the group-time conditional average treatment effect given the covariate. Focusing on the case of a continuous covariate, we propose a three-step estimation procedure based on nonparametric local polynomial regressions and parametric estimation methods. Using uniformly valid distributional approximation results for empirical processes and multiplier bootstrapping, we develop doubly robust inference methods to construct uniform confidence bands for the group-time conditional average treatment effect function and a variety of useful summary parameters. The accompanying R package didhetero allows for easy implementation of our methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.02185v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shunsuke Imai, Lei Qin, Takahide Yanagi</dc:creator>
    </item>
    <item>
      <title>Learning cross-layer dependence structure in multilayer networks</title>
      <link>https://arxiv.org/abs/2307.14982</link>
      <description>arXiv:2307.14982v2 Announce Type: replace-cross 
Abstract: We propose a novel class of separable multilayer network models to capture cross-layer dependencies in multilayer networks, enabling the analysis of how interactions in one or more layers may influence interactions in other layers. Our approach separates the network formation process from the layer formation process, and is able to extend existing single-layer network models to multilayer network models that accommodate cross-layer dependence. We establish non-asymptotic and minimax-optimal error bounds for maximum likelihood estimators and demonstrate the convergence rate in scenarios of increasing parameter dimension. Additionally, we establish non-asymptotic error bounds for multivariate normal approximations and propose a model selection method that controls the false discovery rate. Simulation studies and an application to the Lazega lawyers network show that our framework and method perform well in realistic settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.14982v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaheng Li, Jonathan R. Stewart</dc:creator>
    </item>
    <item>
      <title>Bayesian Joint Additive Factor Models for Multiview Learning</title>
      <link>https://arxiv.org/abs/2406.00778</link>
      <description>arXiv:2406.00778v2 Announce Type: replace-cross 
Abstract: It is increasingly common in a wide variety of applied settings to collect data of multiple different types on the same set of samples. Our particular focus in this article is on studying relationships between such multiview features and responses. A motivating application arises in the context of precision medicine where multi-omics data are collected to correlate with clinical outcomes. It is of interest to infer dependence within and across views while combining multimodal information to improve the prediction of outcomes. The signal-to-noise ratio can vary substantially across views, motivating more nuanced statistical tools beyond standard late and early fusion. This challenge comes with the need to preserve interpretability, select features, and obtain accurate uncertainty quantification. We propose a joint additive factor regression model (JAFAR) with a structured additive design, accounting for shared and view-specific components. We ensure identifiability via a novel dependent cumulative shrinkage process (D-CUSP) prior. We provide an efficient implementation via a partially collapsed Gibbs sampler and extend our approach to allow flexible feature and outcome distributions. Prediction of time-to-labor onset from immunome, metabolome, and proteome data illustrates performance gains against state-of-the-art competitors. Our open-source software (R package) is available at https://github.com/niccoloanceschi/jafar.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00778v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niccolo Anceschi, Federico Ferrari, David B. Dunson, Himel Mallick</dc:creator>
    </item>
    <item>
      <title>Sequentializing a Test: Anytime Validity is Free</title>
      <link>https://arxiv.org/abs/2501.03982</link>
      <description>arXiv:2501.03982v2 Announce Type: replace-cross 
Abstract: An anytime valid sequential test permits us to peek at observations as they arrive. This means we can stop, continue or adapt the testing process based on the current data, without invalidating the inference. Given a maximum number of observations $N$, one may believe that this benefit must be paid for in terms of power when compared to a conventional test that waits until all $N$ observations have arrived. Our key contribution is to show that this is false: for any valid test based on $N$ observations, we derive an anytime valid sequential test that matches it after $N$ observations. In addition, we show that the value of the sequential test before a rejection is attained can be directly used as a significance level for a subsequent test. We illustrate this for the $z$-test. There, we find that the current state-of-the-art based on log-optimal $e$-values can be obtained as a special limiting case that replicates a $z$-test with level $\alpha \to 0$ as $N \to \infty$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03982v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick W. Koning, Sam van Meer</dc:creator>
    </item>
  </channel>
</rss>
