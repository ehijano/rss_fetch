<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Dec 2024 05:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Methods for differential network estimation: an empirical comparison</title>
      <link>https://arxiv.org/abs/2412.17922</link>
      <description>arXiv:2412.17922v1 Announce Type: new 
Abstract: We provide a review and a comparison of methods for differential network estimation in Gaussian graphical models with focus on structure learning. We consider the case of two datasets from distributions associated with two graphical models. In our simulations, we use five different methods to estimate differential networks. We vary graph structure and sparsity to explore their influence on performance in terms of power and false discovery rate. We demonstrate empirically that presence of hubs proves to be a challenge for all the methods, as well as increased density. We suggest local and global properties that are associated with this challenge. Direct estimation with lasso penalized D-trace loss is shown to perform the best across all combinations of network structure and sparsity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17922v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Plaksienko, Magne Thoresen, Vera Djordjilovi\'c</dc:creator>
    </item>
    <item>
      <title>Supervised centrality Cvia sparse network influence regression: an application to the 2021 henan floods' social network</title>
      <link>https://arxiv.org/abs/2412.18145</link>
      <description>arXiv:2412.18145v1 Announce Type: new 
Abstract: The social characteristics of players in a social network are closely associated with their network positions and relational importance. Identifying those influential players in a network is of great importance as it helps to understand how ties are formed, how information is propagated, and, in turn, can guide the dissemination of new information. Motivated by a Sina Weibo social network analysis of the 2021 Henan Floods, where response variables for each Sina Weibo user are available, we propose a new notion of supervised centrality that emphasizes the task-specific nature of a player's centrality. To estimate the supervised centrality and identify important players, we develop a novel sparse network influence regression by introducing individual heterogeneity for each user. To overcome the computational difficulties in fitting the model for large social networks, we further develop a forward-addition algorithm and show that it can consistently identify a superset of the influential Sina Weibo users. We apply our method to analyze three responses in the Henan Floods data: the number of comments, reposts, and likes, and obtain meaningful results. A further simulation study corroborates the developed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18145v1</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yingying Ma, Wei Lan, Chenlei Leng, Ting Li, Hansheng Wang</dc:creator>
    </item>
    <item>
      <title>PCM Selector: Penalized Covariate-Mediator Selection Operator for Evaluating Linear Causal Effects</title>
      <link>https://arxiv.org/abs/2412.18180</link>
      <description>arXiv:2412.18180v1 Announce Type: new 
Abstract: For a data-generating process for random variables that can be described with a linear structural equation model, we consider a situation in which (i) a set of covariates satisfying the back-door criterion cannot be observed or (ii) such a set can be observed, but standard statistical estimation methods cannot be applied to estimate causal effects because of multicollinearity/high-dimensional data problems. We propose a novel two-stage penalized regression approach, the penalized covariate-mediator selection operator (PCM Selector), to estimate the causal effects in such scenarios. Unlike existing penalized regression analyses, when a set of intermediate variables is available, PCM Selector provides a consistent or less biased estimator of the causal effect. In addition, PCM Selector provides a variable selection procedure for intermediate variables to obtain better estimation accuracy of the causal effects than does the back-door criterion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18180v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hisayoshi Nanmo, Manabu Kuroki</dc:creator>
    </item>
    <item>
      <title>CARL-GT: Evaluating Causal Reasoning Capabilities of Large Language Models</title>
      <link>https://arxiv.org/abs/2412.17970</link>
      <description>arXiv:2412.17970v1 Announce Type: cross 
Abstract: Causal reasoning capabilities are essential for large language models (LLMs) in a wide range of applications, such as education and healthcare. But there is still a lack of benchmarks for a better understanding of such capabilities. Current LLM benchmarks are mainly based on conversational tasks, academic math tests, and coding tests. Such benchmarks evaluate LLMs in well-regularized settings, but they are limited in assessing the skills and abilities to solve real-world problems. In this work, we provide a benchmark, named by CARL-GT, which evaluates CAusal Reasoning capabilities of large Language models using Graphs and Tabular data. The benchmark has a diverse range of tasks for evaluating LLMs from causal graph reasoning, knowledge discovery, and decision-making aspects. In addition, effective zero-shot learning prompts are developed for the tasks. In our experiments, we leverage the benchmark for evaluating open-source LLMs and provide a detailed comparison of LLMs for causal reasoning abilities. We found that LLMs are still weak in casual reasoning, especially with tabular data to discover new insights. Furthermore, we investigate and discuss the relationships of different benchmark tasks by analyzing the performance of LLMs. The experimental results show that LLMs have different strength over different tasks and that their performance on tasks in different categories, i.e., causal graph reasoning, knowledge discovery, and decision-making, shows stronger correlation than tasks in the same category.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17970v1</guid>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ruibo Tu, Hedvig Kjellstr\"om, Gustav Eje Henter, Cheng Zhang</dc:creator>
    </item>
    <item>
      <title>Conditional Influence Functions</title>
      <link>https://arxiv.org/abs/2412.18080</link>
      <description>arXiv:2412.18080v1 Announce Type: cross 
Abstract: There are many nonparametric objects of interest that are a function of a conditional distribution. One important example is an average treatment effect conditional on a subset of covariates. Many of these objects have a conditional influence function that generalizes the classical influence function of a functional of a (unconditional) distribution. Conditional influence functions have important uses analogous to those of the classical influence function. They can be used to construct Neyman orthogonal estimating equations for conditional objects of interest that depend on high dimensional regressions. They can be used to formulate local policy effects and describe the effect of local misspecification on conditional objects of interest. We derive conditional influence functions for functionals of conditional means and other features of the conditional distribution of an outcome variable. We show how these can be used for locally linear estimation of conditional objects of interest. We give rate conditions for first step machine learners to have no effect on asymptotic distributions of locally linear estimators. We also give a general construction of Neyman orthogonal estimating equations for conditional objects of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18080v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Chernozhukov, Whitney K. Newey, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>HNCI: High-Dimensional Network Causal Inference</title>
      <link>https://arxiv.org/abs/2412.18568</link>
      <description>arXiv:2412.18568v1 Announce Type: cross 
Abstract: The problem of evaluating the effectiveness of a treatment or policy commonly appears in causal inference applications under network interference. In this paper, we suggest the new method of high-dimensional network causal inference (HNCI) that provides both valid confidence interval on the average direct treatment effect on the treated (ADET) and valid confidence set for the neighborhood size for interference effect. We exploit the model setting in Belloni et al. (2022) and allow certain type of heterogeneity in node interference neighborhood sizes. We propose a linear regression formulation of potential outcomes, where the regression coefficients correspond to the underlying true interference function values of nodes and exhibit a latent homogeneous structure. Such a formulation allows us to leverage existing literature from linear regression and homogeneity pursuit to conduct valid statistical inferences with theoretical guarantees. The resulting confidence intervals for the ADET are formally justified through asymptotic normalities with estimable variances. We further provide the confidence set for the neighborhood size with theoretical guarantees exploiting the repro samples approach. The practical utilities of the newly suggested methods are demonstrated through simulation and real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18568v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenqin Du, Rundong Ding, Yingying Fan, Jinchi Lv</dc:creator>
    </item>
    <item>
      <title>A Comparative Study of Imputation Methods for Multivariate Ordinal Data</title>
      <link>https://arxiv.org/abs/2010.10471</link>
      <description>arXiv:2010.10471v5 Announce Type: replace 
Abstract: Missing data remains a very common problem in large datasets, including survey and census data containing many ordinal responses, such as political polls and opinion surveys. Multiple imputation (MI) is usually the go-to approach for analyzing such incomplete datasets, and there are indeed several implementations of MI, including methods using generalized linear models, tree-based models, and Bayesian non-parametric models. However, there is limited research on the statistical performance of these methods for multivariate ordinal data. In this article, we perform an empirical evaluation of several MI methods, including MI by chained equations (MICE) using multinomial logistic regression models, MICE using proportional odds logistic regression models, MICE using classification and regression trees, MICE using random forest, MI using Dirichlet process (DP) mixtures of products of multinomial distributions, and MI using DP mixtures of multivariate normal distributions. We evaluate the methods using simulation studies based on ordinal variables selected from the 2018 American Community Survey (ACS). Under our simulation settings, the results suggest that MI using proportional odds logistic regression models, classification and regression trees and DP mixtures of multinomial distributions generally outperform the other methods. In certain settings, MI using multinomial logistic regression models is able to achieve comparable performance, depending on the missing data mechanism and amount of missing data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2010.10471v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/jssam/smab028</arxiv:DOI>
      <arxiv:journal_reference>Journal of Survey Statistics and Methodology, Volume 11, Issue 1, February 2023, Pages 189-212</arxiv:journal_reference>
      <dc:creator>Chayut Wongkamthong, Olanrewaju Akande</dc:creator>
    </item>
    <item>
      <title>Modelling multivariate extreme value distributions via Markov trees</title>
      <link>https://arxiv.org/abs/2208.02627</link>
      <description>arXiv:2208.02627v2 Announce Type: replace 
Abstract: Multivariate extreme value distributions are a common choice for modelling multivariate extremes. In high dimensions, however, the construction of flexible and parsimonious models is challenging. We propose to combine bivariate max-stable distributions into a Markov random field with respect to a tree. Although in general not max-stable itself, this Markov tree is attracted by a multivariate max-stable distribution. The latter serves as a tree-based approximation to an unknown max-stable distribution with the given bivariate distributions as margins. Given data, we learn an appropriate tree structure by Prim's algorithm with estimated pairwise upper tail dependence coefficients as edge weights. The distributions of pairs of connected variables can be fitted in various ways. The resulting tree-structured max-stable distribution allows for inference on rare event probabilities, as illustrated on river discharge data from the upper Danube basin.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.02627v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1111/sjos.12698</arxiv:DOI>
      <arxiv:journal_reference>Scandinavian Journal of Statistics (2024), volume 51, pages 760-800</arxiv:journal_reference>
      <dc:creator>Shuang Hu, Zuoxiang Peng, Johan Segers</dc:creator>
    </item>
    <item>
      <title>Generative Causal Inference</title>
      <link>https://arxiv.org/abs/2306.16096</link>
      <description>arXiv:2306.16096v2 Announce Type: replace 
Abstract: Generative Bayesian Computation (GBC) methods are developed for Casual Inference. Generative methods are simulation-based methods that use a large training dataset to represent posterior distributions as a map (a.k.a. optimal transport) to a base distribution. They avoid the use of MCMC by replacing the conditional posterior inference problem with a supervised learning problem. We further propose the use Quantile ReLU networks which are density free and hence apply in a variety of Econometric settings where data generating processes are specified by deterministic latent variables updates or as moment constraints. Generative approaches directly simulate large samples of observables and unobservable (parameters, latent variables) and then apply high-dimensional quantile regression to learn a nonlinear transport map from base distribution to parameter inference. We illustrate our methodology in the field of causal inference. Our approach can also handle nonlinearity and heterogeneity. Finally, we conclude with the directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16096v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Maria Nareklishvili, Nicholas Polson, Vadim Sokolov</dc:creator>
    </item>
    <item>
      <title>Smoothing Variances Across Time: Adaptive Stochastic Volatility</title>
      <link>https://arxiv.org/abs/2408.11315</link>
      <description>arXiv:2408.11315v3 Announce Type: replace 
Abstract: We introduce a novel Bayesian framework for estimating time-varying volatility by extending the Random Walk Stochastic Volatility (RWSV) model with a new Dynamic Shrinkage Process (DSP) in (log) variances. Unlike classical Stochastic Volatility or GARCH-type models with restrictive parametric stationarity assumptions, our proposed Adaptive Stochastic Volatility (ASV) model provides smooth yet dynamically adaptive estimates of evolving volatility and its uncertainty (vol of vol). We derive the theoretical properties of the proposed global-local shrinkage prior. Through simulation studies, we demonstrate that ASV exhibits remarkable misspecification resilience with low prediction error across various data generating scenarios in simulation. Furthermore, ASV's capacity to yield locally smooth and interpretable estimates facilitates a clearer understanding of underlying patterns and trends in volatility. Additionally, we propose and illustrate an extension for Bayesian Trend Filtering simultaneously in both mean and variance. Finally, we show that this attribute makes ASV a robust tool applicable across a wide range of disciplines, including in finance, environmental science, epidemiology, and medicine, among others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11315v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason B. Cho, David S. Matteson</dc:creator>
    </item>
    <item>
      <title>Fast Bayesian Functional Principal Components Analysis</title>
      <link>https://arxiv.org/abs/2412.11340</link>
      <description>arXiv:2412.11340v2 Announce Type: replace 
Abstract: Functional Principal Components Analysis (FPCA) is one of the most successful and widely used analytic tools for exploration and dimension reduction of functional data. Standard implementations of FPCA estimate the principal components from the data but ignore their sampling variability in subsequent inferences. To address this problem, we propose the Fast Bayesian Functional Principal Components Analysis (Fast BayesFPCA), that treats principal components as parameters on the Stiefel manifold. To ensure efficiency, stability, and scalability we introduce three innovations: (1) project all eigenfunctions onto an orthonormal spline basis, reducing modeling considerations to a smaller-dimensional Stiefel manifold; (2) induce a uniform prior on the Stiefel manifold of the principal component spline coefficients via the polar representation of a matrix with entries following independent standard Normal priors; and (3) constrain sampling using the assumed FPCA structure to improve stability. We demonstrate the application of Fast BayesFPCA to characterize the variability in mealtime glucose from the Dietary Approaches to Stop Hypertension for Diabetes Continuous Glucose Monitoring (DASH4D CGM) study. All relevant STAN code and simulation routines are available as supplementary material.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11340v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Joseph Sartini, Xinkai Zhou, Liz Selvin, Scott Zeger, Ciprian Crainiceanu</dc:creator>
    </item>
    <item>
      <title>An Empirical Study: Extensive Deep Temporal Point Process</title>
      <link>https://arxiv.org/abs/2110.09823</link>
      <description>arXiv:2110.09823v5 Announce Type: replace-cross 
Abstract: Temporal point process as the stochastic process on continuous domain of time is commonly used to model the asynchronous event sequence featuring with occurrence timestamps. Thanks to the strong expressivity of deep neural networks, they are emerging as a promising choice for capturing the patterns in asynchronous sequences, in the context of temporal point process. In this paper, we first review recent research emphasis and difficulties in modeling asynchronous event sequences with deep temporal point process, which can be concluded into four fields: encoding of history sequence, formulation of conditional intensity function, relational discovery of events and learning approaches for optimization. We introduce most of recently proposed models by dismantling them into the four parts, and conduct experiments by remodularizing the first three parts with the same learning strategy for a fair empirical evaluation. Besides, we extend the history encoders and conditional intensity function family, and propose a Granger causality discovery framework for exploiting the relations among multi-types of events. Because the Granger causality can be represented by the Granger causality graph, discrete graph structure learning in the framework of Variational Inference is employed to reveal latent structures of the graph. Further experiments show that the proposed framework with latent graph discovery can both capture the relations and achieve an improved fitting and predicting performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.09823v5</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haitao Lin, Cheng Tan, Lirong Wu, Zhangyang Gao, Zicheng Liu, Stan. Z. Li</dc:creator>
    </item>
    <item>
      <title>Enhancing the Performance of Neural Networks Through Causal Discovery and Integration of Domain Knowledge</title>
      <link>https://arxiv.org/abs/2311.17303</link>
      <description>arXiv:2311.17303v3 Announce Type: replace-cross 
Abstract: In this paper, we develop a generic methodology to encode hierarchical causality structure among observed variables into a neural network in order to improve its predictive performance. The proposed methodology, called causality-informed neural network (CINN), leverages three coherent steps to systematically map the structural causal knowledge into the layer-to-layer design of neural network while strictly preserving the orientation of every causal relationship. In the first step, CINN discovers causal relationships from observational data via directed acyclic graph (DAG) learning, where causal discovery is recast as a continuous optimization problem to avoid the combinatorial nature. In the second step, the discovered hierarchical causality structure among observed variables is systematically encoded into neural network through a dedicated architecture and customized loss function. By categorizing variables in the causal DAG as root, intermediate, and leaf nodes, the hierarchical causal DAG is translated into CINN with a one-to-one correspondence between nodes in the causal DAG and units in the CINN while maintaining the relative order among these nodes. Regarding the loss function, both intermediate and leaf nodes in the DAG graph are treated as target outputs during CINN training so as to drive co-learning of causal relationships among different types of nodes. As multiple loss components emerge in CINN, we leverage the projection of conflicting gradients to mitigate gradient interference among the multiple learning tasks. Computational experiments across a broad spectrum of UCI data sets demonstrate substantial advantages of CINN in predictive performance over other state-of-the-art methods. In addition, an ablation study underscores the value of integrating structural and quantitative causal knowledge in enhancing the neural network's predictive performance incrementally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.17303v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoge Zhang, Xiao-Lin Wang, Fenglei Fan, Yiu-Ming Cheung, Indranil Bose</dc:creator>
    </item>
    <item>
      <title>Computationally efficient variational-like approximations of possibilistic inferential models</title>
      <link>https://arxiv.org/abs/2404.19224</link>
      <description>arXiv:2404.19224v2 Announce Type: replace-cross 
Abstract: Inferential models (IMs) offer provably reliable, data-driven, possibilistic statistical inference. But despite IMs' theoretical and foundational advantages, efficient computation is often a challenge. This paper presents a simple and powerful numerical strategy for approximating the IM's possibility contour, or at least its $\alpha$-cut for a specified $\alpha \in (0,1)$. Our proposal starts with the specification a parametric family that, in a certain sense, approximately covers the credal set associated with the IM's possibility measure. Then the parameters of that parametric family are tuned in such a way that the family's $100(1-\alpha)\%$ credible set roughly matches the IM contour's $\alpha$-cut. This is reminiscent of the variational approximations now widely used in Bayesian statistics, hence the name variational-like IM approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19224v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonardo Cella, Ryan Martin</dc:creator>
    </item>
  </channel>
</rss>
