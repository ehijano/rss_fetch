<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Jun 2024 04:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 27 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Investigating the effect of non-resonant background variation on the CARS data analysis and classification</title>
      <link>https://arxiv.org/abs/2406.17829</link>
      <description>arXiv:2406.17829v1 Announce Type: cross 
Abstract: : Non-resonant background (NRB) plays a significant role in coherent anti-Stokes Raman scattering (CARS) spectroscopic applications. All the recent works primarily focused on removing the NRB using different deep learning methods, and only one study explored the effect of NRB. Hence, in this work, we systematically investigated the impact of NRB variation on Raman signal retrieval. The NRB is simulated as a linear function with different strengths relative to the resonant Raman signal, and the variance also changed for each NRB strength. The resonant part of nonlinear susceptibility is extracted from real experimental Raman data; hence, the simulated CARS data better approximate the experimental CARS spectra. Then, the corresponding Raman signal is retrieved by four different methods: maximum entropy method (MEM), Kramers-Kronig (KK), convolutional neural network (CNN), and long short-term memory (LSTM) network. Pearson correlation measurements and principal component analysis combined with linear discriminant analysis (PCA-LDA) modelling revealed that MEM and KK methods have an edge over LSTM and CNN for higher NRB strengths. It is also demonstrated that normalizing the input data favors LSTM and CNN predictions. In contrast, background removal from the predictions significantly influenced Pearson correlation but not the classification accuracies for MEM and KK. This comprehensive study is done for the first time to the best of our knowledge and has the potential to impact the CARS spectroscopy and microscopy applications in different areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17829v1</guid>
      <category>physics.optics</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rajendhar Junjuri, Tobias Meyer-Zedler, J\"urgen Popp, Thomas Bocklitz</dc:creator>
    </item>
    <item>
      <title>Emergence of social hierarchies in a society with two competitive classes</title>
      <link>https://arxiv.org/abs/2406.18168</link>
      <description>arXiv:2406.18168v1 Announce Type: cross 
Abstract: Agent-based models describing social interactions among individuals can help to better understand emerging macroscopic patterns in societies. One of the topics which is worth tackling is the formation of different kinds of hierarchies that emerge in social spaces such as cities. Here we propose a Bonabeau-like model by adding a second class of agents. The fundamental particularity of our model is that only a pairwise interaction between agents of the opposite class is allowed. Agent fitness can thus only change by competition among the two classes, while the total fitness in the society remains constant. The main result is that for a broad range of values of the model parameters, the fitness of the agents of each class show a decay in time except for one or very few agents which capture almost all the fitness in the society. Numerical simulations also reveal a singular shift from egalitarian to hierarchical society for each class. This behaviour depends on the control parameter $\eta$, playing the role of the inverse of the temperature of the system. Results are invariant with regard to the system size, contingent solely on the quantity of agents within each class. Finally, a couple of scaling laws are provided thus showing a data collapse from different model parameters and they follow a shape which can be related to the presence of a phase transition in the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18168v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CC</category>
      <category>cs.MA</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Marc Sadurn\'i, Josep Perell\'o, Miquel Montero</dc:creator>
    </item>
    <item>
      <title>Similarity-Based Analysis of Atmospheric Organic Compounds for Machine Learning Applications</title>
      <link>https://arxiv.org/abs/2406.18171</link>
      <description>arXiv:2406.18171v1 Announce Type: cross 
Abstract: The formation of aerosol particles in the atmosphere impacts air quality and climate change, but many of the organic molecules involved remain unknown. Machine learning could aid in identifying these compounds through accelerated analysis of molecular properties and detection characteristics. However, such progress is hindered by the current lack of curated datasets for atmospheric molecules and their associated properties. To tackle this challenge, we propose a similarity analysis that connects atmospheric compounds to existing large molecular datasets used for machine learning development. We find a small overlap between atmospheric and non-atmospheric molecules using standard molecular representations in machine learning applications. The identified out-of-domain character of atmospheric compounds is related to their distinct functional groups and atomic composition. Our investigation underscores the need for collaborative efforts to gather and share more molecular-level atmospheric chemistry data. The presented similarity based analysis can be used for future dataset curation for machine learning development in the atmospheric sciences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18171v1</guid>
      <category>physics.ao-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Hilda Sandstr\"om, Patrick Rinke</dc:creator>
    </item>
    <item>
      <title>ViPErLEED package II: Spot tracking, extraction and processing of I(V) curves</title>
      <link>https://arxiv.org/abs/2406.18413</link>
      <description>arXiv:2406.18413v1 Announce Type: cross 
Abstract: As part of the ViPErLEED project (Vienna package for Erlangen LEED, low-energy electron diffraction), computer programs have been developed for facile and user-friendly data extraction from movies of LEED images. The programs make use of some concepts from astronomical image processing and analysis. As a first step, flat-field and dark-frame corrections reduce the effects of inhomogeneities of the camera and screen. In a second step, for identifying all diffraction maxima ("spots"), it is sufficient to manually mark and label a single spot or very few spots. Then the program can automatically identify all other spots and determine the distortions of the image. This forms the basis for automatic spot tracking (following the "beams" as they move across the LEED screen) and intensity measurement. Even for complex structures with hundreds to a few thousand diffraction beams, this step takes less than a minute. The package also includes a program for further processing of these I(V) curves (averaging of equivalent beams, manual and/or automatic selection, smoothing) as well as several utilities. The software is implemented as a set of plugins for the public-domain image processing program ImageJ and provided as an open-source package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18413v1</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Schmid, Florian Kraushofer, Alexander M. Imre, Tilman Ki{\ss}linger, Lutz Hammer, Ulrike Diebold, Michele Riva</dc:creator>
    </item>
    <item>
      <title>Lectures on Statistics in Theory: Prelude to Statistics in Practice</title>
      <link>https://arxiv.org/abs/1807.05996</link>
      <description>arXiv:1807.05996v4 Announce Type: replace 
Abstract: This is a writeup of lectures on "statistics" that have evolved from the initial version for the 2009 Hadron Collider Physics Summer School at CERN to versions for other venues and, most recently, for the African School of Fundamental Physics and Applications in 2024. The emphasis is on foundations, using simple examples to illustrate the points that are still debated in the professional statistics literature. The three main approaches to interval estimation (Neyman confidence, Bayesian, likelihood ratio) are discussed and compared in detail, with and without nuisance parameters. Hypothesis testing is discussed mainly from the frequentist point of view, with pointers to the Bayesian literature. Various foundational issues are emphasized, including the conditionality principle and the likelihood principle.</description>
      <guid isPermaLink="false">oai:arXiv.org:1807.05996v4</guid>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert D. Cousins</dc:creator>
    </item>
    <item>
      <title>Analysis of neutron time-of-flight spectra with a Bayesian unfolding methodology</title>
      <link>https://arxiv.org/abs/2401.17348</link>
      <description>arXiv:2401.17348v2 Announce Type: replace 
Abstract: We have developed an innovative methodology for obtaining the neutron energy distribution from a time-of-flight (TOF) measurement based on the iterative Bayesian unfolding method and accurate Monte Carlo simulations. This methodology has been validated through the analysis of a realistic virtual $\beta$-decay experiment, including the most relevant systematic effects in a real experiment. The proposed methodology allowed for obtaining accurate results over the energy range above the neutron detection threshold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.17348v2</guid>
      <category>physics.data-an</category>
      <category>nucl-ex</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>A. P\'erez de Rada Fiol, D. Cano-Ott, T. Mart\'inez, V. Alcayne, E. Mendoza, J. Plaza, A. Sanchez-Caballero, D. Villamar\'in</dc:creator>
    </item>
    <item>
      <title>Autoencoder-based Anomaly Detection System for Online Data Quality Monitoring of the CMS Electromagnetic Calorimeter</title>
      <link>https://arxiv.org/abs/2309.10157</link>
      <description>arXiv:2309.10157v2 Announce Type: replace-cross 
Abstract: The CMS detector is a general-purpose apparatus that detects high-energy collisions produced at the LHC. Online Data Quality Monitoring of the CMS electromagnetic calorimeter is a vital operational tool that allows detector experts to quickly identify, localize, and diagnose a broad range of detector issues that could affect the quality of physics data. A real-time autoencoder-based anomaly detection system using semi-supervised machine learning is presented enabling the detection of anomalies in the CMS electromagnetic calorimeter data. A novel method is introduced which maximizes the anomaly detection performance by exploiting the time-dependent evolution of anomalies as well as spatial variations in the detector response. The autoencoder-based system is able to efficiently detect anomalies, while maintaining a very low false discovery rate. The performance of the system is validated with anomalies found in 2018 and 2022 LHC collision data. Additionally, the first results from deploying the autoencoder-based system in the CMS online Data Quality Monitoring workflow during the beginning of Run 3 of the LHC are presented, showing its ability to detect issues missed by the existing system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.10157v2</guid>
      <category>physics.ins-det</category>
      <category>cs.LG</category>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s41781-024-00118-z</arxiv:DOI>
      <arxiv:journal_reference>Comput Softw Big Sci 8, 11 (2024)</arxiv:journal_reference>
      <dc:creator> The CMS ECAL Collaboration</dc:creator>
    </item>
    <item>
      <title>Coarse graining correlation matrices according to macrostructures: Financial markets as a paradigm</title>
      <link>https://arxiv.org/abs/2402.05364</link>
      <description>arXiv:2402.05364v2 Announce Type: replace-cross 
Abstract: We analyze correlation structures in financial markets by coarse graining the Pearson correlation matrices according to market sectors to obtain Guhr matrices using Guhr's correlation method according to Ref. [P. Rinn {\it et. al.}, Europhysics Letters 110, 68003 (2015)]. We compare the results for the evolution of market states and the corresponding transition matrices with those obtained using Pearson correlation matrices. The behavior of market states is found to be similar for both the coarse grained and Pearson matrices. However, the number of relevant variables is reduced by orders of magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.05364v2</guid>
      <category>q-fin.ST</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. Mija\'il Mart\'inez-Ramos, Parisa Majari, Andres R. Cruz-Hern\'andez, Hirdesh K. Pharasi, Manan Vyas</dc:creator>
    </item>
    <item>
      <title>Gaussian Framework and Optimal Projection of Weather Fields for Prediction of Extreme Events</title>
      <link>https://arxiv.org/abs/2405.20903</link>
      <description>arXiv:2405.20903v2 Announce Type: replace-cross 
Abstract: Extreme events are the major weather related hazard for humanity. It is then of crucial importance to have a good understanding of their statistics and to be able to forecast them. However, lack of sufficient data makes their study particularly challenging.
  In this work we provide a simple framework to study extreme events that tackles the lack of data issue by using the whole dataset available, rather than focusing on the extremes in the dataset. To do so, we make the assumption that the set of predictors and the observable used to define the extreme event follow a jointly Gaussian distribution. This naturally gives the notion of an optimal projection of the predictors for forecasting the event.
  We take as a case study extreme heatwaves over France, and we test our method on an 8000-year-long intermediate complexity climate model time series and on the ERA5 reanalysis dataset.
  For a-posteriori statistics, we observe and motivate the fact that composite maps of very extreme events look similar to less extreme ones.
  For prediction, we show that our method is competitive with off-the-shelf neural networks on the long dataset and outperforms them on reanalysis.
  The optimal projection pattern, which makes our forecast intrinsically interpretable, highlights the importance of soil moisture deficit and quasi-stationary Rossby waves as precursors to extreme heatwaves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20903v2</guid>
      <category>physics.ao-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Valeria Mascolo, Alessandro Lovo, Corentin Herbert, Freddy Bouchet</dc:creator>
    </item>
  </channel>
</rss>
