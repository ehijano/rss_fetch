<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 03 May 2024 04:04:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 03 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Asymptotic Properties of the Distributional Synthetic Controls</title>
      <link>https://arxiv.org/abs/2405.00953</link>
      <description>arXiv:2405.00953v1 Announce Type: new 
Abstract: This paper enhances our comprehension of the Distributional Synthetic Control (DSC) proposed by Gunsilius (2023), focusing on its asymptotic properties. We first establish the DSC estimator's asymptotic optimality. The essence of this optimality lies in the treatment effect estimator given by DSC achieves the lowest possible squared prediction error among all potential treatment effect estimators that depend on an average of quantiles of control units. We also establish the convergence of the DSC weights when some requirements are met, as well as the convergence rate. A significant aspect of our research is that we find DSC synthesis forms an optimal weighted average, particularly in situations where it is impractical to perfectly fit the treated unit's quantiles through the weighted average of the control units' quantiles. To corroborate our theoretical insights, we provide empirical evidence derived from simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00953v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lu Zhang, Xiaomeng Zhang, Xinyu Zhang</dc:creator>
    </item>
    <item>
      <title>Dynamic Local Average Treatment Effects</title>
      <link>https://arxiv.org/abs/2405.01463</link>
      <description>arXiv:2405.01463v1 Announce Type: new 
Abstract: We consider Dynamic Treatment Regimes (DTRs) with one sided non-compliance that arise in applications such as digital recommendations and adaptive medical trials. These are settings where decision makers encourage individuals to take treatments over time, but adapt encouragements based on previous encouragements, treatments, states, and outcomes. Importantly, individuals may choose to (not) comply with a treatment recommendation, whenever it is made available to them, based on unobserved confounding factors. We provide non-parametric identification, estimation, and inference for Dynamic Local Average Treatment Effects, which are expected values of multi-period treatment contrasts among appropriately defined complier subpopulations. Under standard assumptions in the Instrumental Variable and DTR literature, we show that one can identify local average effects of contrasts that correspond to offering treatment at any single time step. Under an additional cross-period effect-compliance independence assumption, which is satisfied in Staggered Adoption settings and a generalization of them, which we define as Staggered Compliance settings, we identify local average treatment effects of treating in multiple time periods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01463v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ravi B. Sojitra, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>De-Biasing Models of Biased Decisions: A Comparison of Methods Using Mortgage Application Data</title>
      <link>https://arxiv.org/abs/2405.00910</link>
      <description>arXiv:2405.00910v1 Announce Type: cross 
Abstract: Prediction models can improve efficiency by automating decisions such as the approval of loan applications. However, they may inherit bias against protected groups from the data they are trained on. This paper adds counterfactual (simulated) ethnic bias to real data on mortgage application decisions, and shows that this bias is replicated by a machine learning model (XGBoost) even when ethnicity is not used as a predictive variable. Next, several other de-biasing methods are compared: averaging over prohibited variables, taking the most favorable prediction over prohibited variables (a novel method), and jointly minimizing errors as well as the association between predictions and prohibited variables. De-biasing can recover some of the original decisions, but the results are sensitive to whether the bias is effected through a proxy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00910v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>econ.EM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicholas Tenev</dc:creator>
    </item>
    <item>
      <title>Demistifying Inference after Adaptive Experiments</title>
      <link>https://arxiv.org/abs/2405.01281</link>
      <description>arXiv:2405.01281v1 Announce Type: cross 
Abstract: Adaptive experiments such as multi-arm bandits adapt the treatment-allocation policy and/or the decision to stop the experiment to the data observed so far. This has the potential to improve outcomes for study participants within the experiment, to improve the chance of identifying best treatments after the experiment, and to avoid wasting data. Seen as an experiment (rather than just a continually optimizing system) it is still desirable to draw statistical inferences with frequentist guarantees. The concentration inequalities and union bounds that generally underlie adaptive experimentation algorithms can yield overly conservative inferences, but at the same time the asymptotic normality we would usually appeal to in non-adaptive settings can be imperiled by adaptivity. In this article we aim to explain why, how, and when adaptivity is in fact an issue for inference and, when it is, understand the various ways to fix it: reweighting to stabilize variances and recover asymptotic normality, always-valid inference based on joint normality of an asymptotic limiting sequence, and characterizing and inverting the non-normal distributions induced by adaptivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01281v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aur\'elien Bibaut, Nathan Kallus</dc:creator>
    </item>
    <item>
      <title>Designing Algorithmic Recommendations to Achieve Human-AI Complementarity</title>
      <link>https://arxiv.org/abs/2405.01484</link>
      <description>arXiv:2405.01484v1 Announce Type: cross 
Abstract: Algorithms frequently assist, rather than replace, human decision-makers. However, the design and analysis of algorithms often focus on predicting outcomes and do not explicitly model their effect on human decisions. This discrepancy between the design and role of algorithmic assistants becomes of particular concern in light of empirical evidence that suggests that algorithmic assistants again and again fail to improve human decisions. In this article, we formalize the design of recommendation algorithms that assist human decision-makers without making restrictive ex-ante assumptions about how recommendations affect decisions. We formulate an algorithmic-design problem that leverages the potential-outcomes framework from causal inference to model the effect of recommendations on a human decision-maker's binary treatment choice. Within this model, we introduce a monotonicity assumption that leads to an intuitive classification of human responses to the algorithm. Under this monotonicity assumption, we can express the human's response to algorithmic recommendations in terms of their compliance with the algorithm and the decision they would take if the algorithm sends no recommendation. We showcase the utility of our framework using an online experiment that simulates a hiring task. We argue that our approach explains the relative performance of different recommendation algorithms in the experiment, and can help design solutions that realize human-AI complementarity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01484v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bryce McLaughlin, Jann Spiess</dc:creator>
    </item>
    <item>
      <title>Identification with possibly invalid IVs</title>
      <link>https://arxiv.org/abs/2401.03990</link>
      <description>arXiv:2401.03990v2 Announce Type: replace 
Abstract: This paper proposes a novel identification strategy relying on quasi-instrumental variables (quasi-IVs). A quasi-IV is a relevant but possibly invalid IV because it is not exogenous or not excluded. We show that a variety of models with discrete or continuous endogenous treatment which are usually identified with an IV - quantile models with rank invariance, additive models with homogenous treatment effects, and local average treatment effect models - can be identified under the joint relevance of two complementary quasi-IVs instead. To achieve identification, we complement one excluded but possibly endogenous quasi-IV (e.g., "relevant proxies" such as lagged treatment choice) with one exogenous (conditional on the excluded quasi-IV) but possibly included quasi-IV (e.g., random assignment or exogenous market shocks). Our approach also holds if any of the two quasi-IVs turns out to be a valid IV. In practice, being able to address endogeneity with complementary quasi-IVs instead of IVs is convenient since there are many applications where quasi-IVs are more readily available. Difference-in-differences is a notable example: time is an exogenous quasi-IV while the group assignment acts as a complementary excluded quasi-IV.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03990v2</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christophe Bruneel-Zupanc, Jad Beyhum</dc:creator>
    </item>
    <item>
      <title>On the testability of common trends in panel data without placebo periods</title>
      <link>https://arxiv.org/abs/2404.16961</link>
      <description>arXiv:2404.16961v2 Announce Type: replace 
Abstract: We demonstrate and discuss the testability of the common trend assumption imposed in Difference-in-Differences (DiD) estimation in panel data when not relying on multiple pre-treatment periods for running placebo tests. Our testing approach involves two steps: (i) constructing a control group of non-treated units whose pre-treatment outcome distribution matches that of treated units, and (ii) verifying if this control group and the original non-treated group share the same time trend in average outcomes. Testing is motivated by the fact that in several (but not all) panel data models, a common trend violation across treatment groups implies and is implied by a common trend violation across pre-treatment outcomes. For this reason, the test verifies a sufficient, but (depending on the model) not necessary condition for DiD-based identification. We investigate the finite sample performance of a testing procedure that is based on double machine learning, which permits controlling for covariates in a data-driven manner, in a simulation study and also apply it to labor market data from the National Supported Work Demonstration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16961v2</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Huber</dc:creator>
    </item>
  </channel>
</rss>
