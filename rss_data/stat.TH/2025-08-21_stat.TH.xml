<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Aug 2025 04:02:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Gaussian Multiplier Bootstrap Procedure for the $\kappa$th Largest Coordinate of High-Dimensional Statistics</title>
      <link>https://arxiv.org/abs/2508.14400</link>
      <description>arXiv:2508.14400v1 Announce Type: new 
Abstract: We consider the problem of Gaussian and bootstrap approximations for the distribution of the $\kappa$th-largest statistic in high dimensions. This statistic, defined as the $\kappa$th-largest component of the sum of independent random vectors, is critical in numerous high-dimensional estimation and testing problems. Such a problem has been studied previously for $\kappa=1$ (i.e., maxima). However, in many applications, a general $\kappa\geq1$ is of great interest, which is addressed in this paper. By invoking the iterative randomized Lindeberg method, we provide bounds for the errors in distributional approximations. These bounds generalize existing results and extend the applicability to a wider range of bootstrap methods. All these results allow the dimension $p$ of random vectors to be as large as or much larger than the sample size $n$. Extensive simulation results and real data analysis demonstrate the effectiveness and advantages of the Gaussian multiplier bootstrap procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14400v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yixi Ding, Qizhai Li, Yuke Shi, Liuquan Sun</dc:creator>
    </item>
    <item>
      <title>Ordering results for random maxima and minima from two dependent Kumaraswamy generalized distributed samples</title>
      <link>https://arxiv.org/abs/2508.14855</link>
      <description>arXiv:2508.14855v1 Announce Type: new 
Abstract: Let $\{X_{1},\ldots,X_{N_1}\}$ and $\{Y_{1},\ldots,Y_{N_2}\}$ be two sequences of interdependent heterogeneous samples, where for $i=1,\ldots,N_{1},$ $X_{i}\sim \text{Kw-G}(x, \alpha_{i}, \gamma_{i};G)$ and for $i=1,\ldots,N_{2},$ $Y_{i}\sim \text{Kw-G}(x, \beta_{i}, \delta_{i};H),$ where $G$ and $H$ are baseline distributions in the Kumaraswamy generalized model and $N_1$ and $N_2$ are two positive integer-valued random variables, independently of $X_{i}'$s and $Y_{i}'$s, respectively. In this article, we establish several stochastic orders such as usual stochastic, hazard rate, reversed hazard rate, dispersive and likelihood ratio orders between the random maxima ($X_{{N_1}:{N_1}}$ and $Y_{{N_2}:{N_2}}$) and the random minima ($X_{{1}:{N_1}}$ and $X_{{1}:{N_2}}$), when the sample sizes are different and random (positive).</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14855v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Statistics (2025)</arxiv:journal_reference>
      <dc:creator>Sangita Das, Narayanaswamy Balakrishnan</dc:creator>
    </item>
    <item>
      <title>Sequential Correct Screening and Post-Screening Inference</title>
      <link>https://arxiv.org/abs/2508.14596</link>
      <description>arXiv:2508.14596v1 Announce Type: cross 
Abstract: Selecting the top-$m$ variables with the $m$ largest population parameters from a larger set of candidates is a fundamental problem in statistics. In this paper, we propose a novel methodology called Sequential Correct Screening (SCS), which sequentially screens out variables that are not among the top-$m$. A key feature of our method is its anytime validity; it provides a sequence of variable subsets that, with high probability, always contain the true top-$m$ variables. Furthermore, we develop a post-screening inference (PSI) procedure to construct confidence intervals for the selected parameters. Importantly, this procedure is designed to control the false coverage rate (FCR) whenever it is conducted -- an aspect that has been largely overlooked in the existing literature. We establish theoretical guarantees for both SCS and PSI, and demonstrate their performance through simulation studies and an application to a real-world dataset on suicide rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14596v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masaki Toyoda, Yoshimasa Uematsu</dc:creator>
    </item>
    <item>
      <title>Single-index models for extreme value index regression</title>
      <link>https://arxiv.org/abs/2203.05758</link>
      <description>arXiv:2203.05758v4 Announce Type: replace 
Abstract: Since the extreme value index (EVI) controls the tail behaviour of the distribution function, the estimation of EVI is a very important topic in extreme value theory. Recent developments in the estimation of EVI along with covariates have been in the context of nonparametric regression. However, for the large dimension of covariates, the fully nonparametric estimator faces the problem of the curse of dimensionality. To avoid this, we apply the single index model to EVI regression under Pareto-type tailed distribution. We study the penalized maximum likelihood estimation of the single index model. The asymptotic properties of the estimator are also developed. Numerical studies are presented to show the efficiency of the proposed model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.05758v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takuma Yoshida</dc:creator>
    </item>
    <item>
      <title>Non-asymptotic bounds for forward processes in denoising diffusions: Ornstein-Uhlenbeck is hard to beat</title>
      <link>https://arxiv.org/abs/2408.13799</link>
      <description>arXiv:2408.13799v2 Announce Type: replace 
Abstract: Denoising diffusion probabilistic models (DDPMs) represent a recent advance in generative modelling that has delivered state-of-the-art results across many domains of applications. Despite their success, a rigorous theoretical understanding of the error within DDPMs, particularly the non-asymptotic bounds required for the comparison of their efficiency, remain scarce. Making minimal assumptions on the initial data distribution, allowing for example the manifold hypothesis, this paper presents explicit non-asymptotic bounds on the forward diffusion error in total variation (TV), expressed as a function of the terminal time $T$.
  We parametrise multi-modal data distributions in terms of the distance $R$ to their furthest modes and consider forward diffusions with additive and multiplicative noise. Our analysis rigorously proves that, under mild assumptions, the canonical choice of the Ornstein-Uhlenbeck (OU) process cannot be significantly improved in terms of reducing the terminal time $T$ as a function of $R$ and error tolerance $\varepsilon&gt;0$. Motivated by data distributions arising in generative modelling, we also establish a cut-off like phenomenon (as $R\to\infty$) for the convergence to its invariant measure in TV of an OU process, initialized at a multi-modal distribution with maximal mode distance $R$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13799v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miha Bre\v{s}ar, Aleksandar Mijatovi\'c</dc:creator>
    </item>
    <item>
      <title>Regularized e-processes: anytime valid inference with knowledge-based efficiency gains</title>
      <link>https://arxiv.org/abs/2410.01427</link>
      <description>arXiv:2410.01427v4 Announce Type: replace 
Abstract: Classical statistical methods have theoretical justification when the sample size is predetermined. In applications, however, it's often the case that sample sizes are data-dependent rather than predetermined. The aforementioned methods aren't reliable in this latter case, hence the recent interest in e-processes and methods that are anytime valid, i.e., reliable for any dynamic data-collection plan. But if the investigator has relevant-yet-incomplete prior information about the quantity of interest, then there's an opportunity for efficiency gain. This paper proposes a regularized e-process framework featuring a knowledge-based, imprecise-probabilistic regularization with improved efficiency. A generalized version of Ville's inequality is established, ensuring that inference based on the regularized e-process are anytime valid in a novel, knowledge-dependent sense. Regularized e-processes also facilitate possibility-theoretic uncertainty quantification with strong frequentist-like calibration properties and other Bayesian-like properties: satisfies the likelihood principle, avoids sure-loss, and offers formal decision-making with reliability guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01427v4</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Martin</dc:creator>
    </item>
    <item>
      <title>Hybrid estimation for a mixed fractional Black-Scholes model with random effects from discrete time observations</title>
      <link>https://arxiv.org/abs/2508.07936</link>
      <description>arXiv:2508.07936v2 Announce Type: replace 
Abstract: We propose a hybrid estimation procedure to estimate global fixed parameters and subject-specific random effects in a mixed fractional Black-Scholes model based on discrete time observations. Specifically, we consider $N$ independent stochastic processes, each driven by a linear combination of standard Brownian motion and an independent fractional Brownian motion, and governed by a drift term that depends on an unobserved random effect with unknown distribution. Based on discrete-time statistics of process increments, we construct parametric estimators for the Brownian motion volatility, the scaling parameter for the fractional Brownian motion, and the Hurst parameter using a generalized method of moments. We establish strong consistency and joint asymptotic normality of these estimators. Then, from one trajectory, we consistently estimate the random effects, using a plug-in approach, and we study their asymptotic behavior under different asymptotic regimes as $N$ and $n$ grow. Finally, we construct a nonparametric estimator for the distribution function of these random effects using a Lagrange interpolation at Chebyshev-Gauss nodes based method, and we analyze its asymptotic properties as both the number of subjects $N$ and the number of observations per-subject $n$ increase. A numerical simulation framework is also investigated to illustrate the theoretical results of the estimators behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07936v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nesrine Chebli, Hamdi Fathallah, Yousri Slaoui</dc:creator>
    </item>
    <item>
      <title>Better bootstrap t confidence intervals for the mean</title>
      <link>https://arxiv.org/abs/2508.10083</link>
      <description>arXiv:2508.10083v2 Announce Type: replace 
Abstract: This article explores combinations of weighted bootstraps, like the Bayesian bootstrap, with the bootstrap $t$ method for setting approximate confidence intervals for the mean of a random variable in small samples. For this problem the usual bootstrap $t$ has good coverage but provides intervals with long and highly variable lengths. Those intervals can have infinite length not just for tiny $n$, when the data have a discrete distribution. The BC$_a$ bootstrap produces shorter intervals but tends to severely under-cover the mean. Bootstrapping the studentized mean with weights from a Beta$(1/2,3/2)$ distribution is shown to attain second order accuracy. It never yields infinite length intervals and the mean square bootstrap $t$ statistic is finite when there are at least three distinct values in the data, or two distinct values appearing at least three times each. In a range of small sample settings, the beta bootstrap $t$ intervals have closer to nominal coverage than the BC$_a$ and shorter length than the multinomial bootstrap $t$. The paper includes a lengthy discussion of the difficulties in constructing a utility function to evaluate nonparametric approximate confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10083v2</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Art B. Owen</dc:creator>
    </item>
    <item>
      <title>The Kikuchi Hierarchy and Tensor PCA</title>
      <link>https://arxiv.org/abs/1904.03858</link>
      <description>arXiv:1904.03858v3 Announce Type: replace-cross 
Abstract: For the tensor PCA (principal component analysis) problem, we propose a new hierarchy of increasingly powerful algorithms with increasing runtime. Our hierarchy is analogous to the sum-of-squares (SOS) hierarchy but is instead inspired by statistical physics and related algorithms such as belief propagation and AMP (approximate message passing). Our level-$\ell$ algorithm can be thought of as a linearized message-passing algorithm that keeps track of $\ell$-wise dependencies among the hidden variables. Specifically, our algorithms are spectral methods based on the Kikuchi Hessian, which generalizes the well-studied Bethe Hessian to the higher-order Kikuchi free energies.
  It is known that AMP, the flagship algorithm of statistical physics, has substantially worse performance than SOS for tensor PCA. In this work we 'redeem' the statistical physics approach by showing that our hierarchy gives a polynomial-time algorithm matching the performance of SOS. Our hierarchy also yields a continuum of subexponential-time algorithms, and we prove that these achieve the same (conjecturally optimal) tradeoff between runtime and statistical power as SOS. Our proofs are much simpler than prior work, and also apply to the related problem of refuting random $k$-XOR formulas. The results we present here apply to tensor PCA for tensors of all orders, and to $k$-XOR when $k$ is even.
  Our methods suggest a new avenue for systematically obtaining optimal algorithms for Bayesian inference problems, and our results constitute a step toward unifying the statistical physics and sum-of-squares approaches to algorithm design.</description>
      <guid isPermaLink="false">oai:arXiv.org:1904.03858v3</guid>
      <category>cs.DS</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander S. Wein, Ahmed El Alaoui, Cristopher Moore</dc:creator>
    </item>
    <item>
      <title>Exact threshold for approximate ellipsoid fitting of random points</title>
      <link>https://arxiv.org/abs/2310.05787</link>
      <description>arXiv:2310.05787v2 Announce Type: replace-cross 
Abstract: We consider the problem $(\rm P)$ of exactly fitting an ellipsoid (centered at $0$) to $n$ standard Gaussian random vectors in $\mathbb{R}^d$, as $n, d \to \infty$ with $n / d^2 \to \alpha &gt; 0$. This problem is conjectured to undergo a sharp transition: with high probability, $(\rm P)$ has a solution if $\alpha &lt; 1/4$, while $(\rm P)$ has no solutions if $\alpha &gt; 1/4$. So far, only a trivial bound $\alpha &gt; 1/2$ is known to imply the absence of solutions, while the sharpest results on the positive side assume $\alpha \leq \eta$ (for $\eta &gt; 0$ a small constant) to prove that $(\rm P)$ is solvable. In this work we show a universality property for the minimal fitting error achievable by ellipsoids: we show that, to leading order, it coincides with the minimal error in a so-called "Gaussian equivalent" problem, for which the satisfiability transition can be rigorously analyzed. Our main results follow from this finding, and they are twofold. On the positive side, we prove that if $\alpha &lt; 1/4$, there exists an ellipsoid fitting all the points up to a small error, and that the lengths of its principal axes are bounded above and below. On the other hand, for $\alpha &gt; 1/4$, we show that achieving small fitting error is not possible if the length of the ellipsoid's shortest axis does not approach $0$ as $d \to \infty$ (and in particular there does not exist any ellipsoid fit whose shortest axis length is bounded away from $0$ as $d \to \infty$). To the best of our knowledge, our work is the first rigorous result characterizing the expected phase transition in ellipsoid fitting at $\alpha = 1/4$. In a companion non-rigorous work, the second author and D. Kunisky give a general analysis of ellipsoid fitting using the replica method of statistical physics, which inspired the present work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.05787v2</guid>
      <category>math.PR</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1214/25-EJP1378</arxiv:DOI>
      <arxiv:journal_reference>Electron. J. Probab. 30: 1-46 (2025)</arxiv:journal_reference>
      <dc:creator>Afonso S. Bandeira, Antoine Maillard</dc:creator>
    </item>
    <item>
      <title>Tame sparse exponential random graphs</title>
      <link>https://arxiv.org/abs/2406.17390</link>
      <description>arXiv:2406.17390v3 Announce Type: replace-cross 
Abstract: In this paper, we obtain a precise estimate of the probability that the sparse binomial random graph contains a large number of vertices in a triangle. The estimate of log of this probability is correct up to second order, and enables us to propose an exponential random graph model based on the number of vertices in a triangle. Specifically, by tuning a single parameter, we can with high probability induce any given fraction of vertices in a triangle. Moreover, in the proposed exponential random graph model we derive the large deviation principle for the number of edges. As a byproduct, we propose a consistent estimator of the tuning parameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17390v3</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suman Chakraborty, Remco van der Hofstad, Frank den Hollander</dc:creator>
    </item>
    <item>
      <title>Sharp Generalization for Nonparametric Regression in Interpolation Space by Over-Parameterized Neural Networks Trained with Preconditioned Gradient Descent and Early-Stopping</title>
      <link>https://arxiv.org/abs/2407.11353</link>
      <description>arXiv:2407.11353v3 Announce Type: replace-cross 
Abstract: We study nonparametric regression using an over-parameterized two-layer neural networks trained with algorithmic guarantees in this paper. We consider the setting where the training features are drawn uniformly from the unit sphere in $\mathbb{R}^d$, and the target function lies in an interpolation space commonly studied in statistical learning theory. We demonstrate that training the neural network with a novel Preconditioned Gradient Descent (PGD) algorithm, equipped with early stopping, achieves a sharp regression rate of $\mathcal O(n^{-\frac{2\alpha s'}{2\alpha s'+1}})$ when the target function is in the interpolation space $[\mathcal H_K]^{s'}$ with $s' \ge 3$. This rate is even sharper than the currently known nearly-optimal rate of $\mathcal O(n^{-\frac{2\alpha s'}{2\alpha s'+1}})\log^2(1/\delta)$~\citep{Li2024-edr-general-domain}, where $n$ is the size of the training data and $\delta \in (0,1)$ is a small probability. This rate is also sharper than the standard kernel regression rate of $\mathcal O(n^{-\frac{2\alpha}{2\alpha+1}})$ obtained under the regular Neural Tangent Kernel (NTK) regime when training the neural network with the vanilla gradient descent (GD), where $2\alpha = d/(d-1)$. Our analysis is based on two key technical contributions. First, we present a principled decomposition of the network output at each PGD step into a function in the reproducing kernel Hilbert space (RKHS) of a newly induced integral kernel, and a residual function with small $L^{\infty}$-norm. Second, leveraging this decomposition, we apply local Rademacher complexity theory to tightly control the complexity of the function class comprising all the neural network functions obtained in the PGD iterates. Our results further suggest that PGD enables the neural network to escape the linear NTK regime and achieve improved generalization by inducing a new integral kernel of lower kernel complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11353v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingzhen Yang, Ping Li</dc:creator>
    </item>
    <item>
      <title>Poisson Midpoint Method for Log Concave Sampling: Beyond the Strong Error Lower Bounds</title>
      <link>https://arxiv.org/abs/2506.07614</link>
      <description>arXiv:2506.07614v3 Announce Type: replace-cross 
Abstract: We study the problem of sampling from strongly log-concave distributions over $\mathbb{R}^d$ using the Poisson midpoint discretization (a variant of the randomized midpoint method) for overdamped/underdamped Langevin dynamics. We prove its convergence in the 2-Wasserstein distance ($W_2$), achieving a cubic speedup in dependence on the target accuracy ($\epsilon$) over the Euler-Maruyama discretization, surpassing existing bounds for randomized midpoint methods. Notably, in the case of underdamped Langevin dynamics, we demonstrate the complexity of $W_2$ convergence is much smaller than the complexity lower bounds for convergence in $L^2$ strong error established in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07614v3</guid>
      <category>math.PR</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rishikesh Srinivasan, Dheeraj Nagaraj</dc:creator>
    </item>
    <item>
      <title>Joint Quantile Shrinkage: A State-Space Approach toward Non-Crossing Bayesian Quantile Models</title>
      <link>https://arxiv.org/abs/2506.13257</link>
      <description>arXiv:2506.13257v2 Announce Type: replace-cross 
Abstract: Crossing of fitted conditional quantiles is a prevalent problem for quantile regression models. We propose a new Bayesian modelling framework that penalises multiple quantile regression functions toward the desired non-crossing space. We achieve this by estimating multiple quantiles jointly with a prior on variation across quantiles, a fused shrinkage prior with quantile adaptivity. The posterior is derived from a decision-theoretic general Bayes perspective, whose form yields a natural state-space interpretation aligned with Time-Varying Parameter (TVP) models. Taken together our approach leads to a Quantile-Varying Parameter (QVP) model, for which we develop efficient sampling algorithms. We demonstrate that our proposed modelling framework provides superior parameter recovery and predictive performance compared to competing Bayesian and frequentist quantile regression estimators in simulated experiments and a real-data application to multivariate quantile estimation in macroeconomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13257v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Kohns, Tibor Szendrei</dc:creator>
    </item>
    <item>
      <title>Kernel Two-Sample Testing via Directional Components Analysis</title>
      <link>https://arxiv.org/abs/2508.08564</link>
      <description>arXiv:2508.08564v2 Announce Type: replace-cross 
Abstract: We propose a novel kernel-based two-sample test that leverages the spectral decomposition of the maximum mean discrepancy (MMD) statistic to identify and utilize well-estimated directional components in reproducing kernel Hilbert space (RKHS). Our approach is motivated by the observation that the estimation quality of these components varies significantly, with leading eigen-directions being more reliably estimated in finite samples. By focusing on these directions and aggregating information across multiple kernels, the proposed test achieves higher power and improved robustness, especially in high-dimensional and unbalanced sample settings. We further develop a computationally efficient multiplier bootstrap procedure for approximating critical values, which is theoretically justified and significantly faster than permutation-based alternatives. Extensive simulations and empirical studies on microarray datasets demonstrate that our method maintains the nominal Type I error rate and delivers superior power compared to other existing MMD-based tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08564v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rui Cui, Yuhao Li, Xiaojun Song</dc:creator>
    </item>
  </channel>
</rss>
