<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 Oct 2025 04:00:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Zero-Inflated Bayesian Multi-Study Infinite Non-Negative Matrix Factorization</title>
      <link>https://arxiv.org/abs/2510.07518</link>
      <description>arXiv:2510.07518v1 Announce Type: new 
Abstract: Understanding the association between dietary patterns and health outcomes, such as the cancer risk, is crucial to inform public health guidelines and shaping future dietary interventions. However, dietary intake data present several statistical challenges: they are high-dimensional, often sparse with excess zeros, and exhibit heterogeneity driven by individual-level covariates. Non-Negative Matrix Factorization (NMF), commonly used to estimate patterns in high-dimensional count data, typically relies on Poisson assumptions and lacks the flexibility to fully address these complexities. Additionally, integrating data across multiple studies, such as case-control studies on cancer risk, requires models that can share information across sources while preserving study-specific structure.
  In this paper, we introduce a novel Bayesian NMF model that (i) jointly models multi-study count data to enable cross-study information sharing, (ii) incorporate a mixture component to account for zero inflation, and (iii) leverage flexible Bayesian non-parametric priors for characterizing the heterogeneity in pattern scores induced by the individual covariates. This structure allows for clustering of individuals based on dietary profiles, enabling downstream association analyses with health outcomes. Through extensive simulation studies, we demonstrate that our model significantly improves estimation accuracy compared to existing Bayesian NMF methods.
  We further illustrate its utility through an application to multiple case-control studies on diet and upper aero-digestive tract cancers, identifying nutritionally meaningful dietary patterns. An R package implementing our approach is available at https://github.com/blhansen/ZIMultiStudyNMF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07518v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Blake Hansen, Dafne Zorzetto, Valeria Edefonti, Roberta De Vito</dc:creator>
    </item>
    <item>
      <title>Integrating smart surveys with traditional surveys</title>
      <link>https://arxiv.org/abs/2510.07521</link>
      <description>arXiv:2510.07521v1 Announce Type: new 
Abstract: Smart surveys are surveys that make use of sensors and machine intelligence to reduce respondent burden and increase data quality. Smart surveys have been tests as a way to improve diary surveys in official statistics, where data are collected on topics such as travel, time use and household expenditures. There are often inherent differences both in measurement and representation between smart surveys and traditional diaries, which makes it difficult to integrate both data sources in producing statistics over time, or within a mixed- or multi-source context. This paper distinguishes two different approaches to integration: the mixed-mode approach, which prioritizes outcome alignment and minimizes measurement differences for straightforward data merging, and the multisource approach, which maintains inherent mode differences and integrates data at the modeling stage, allowing exploitation of the strengths of each source. Using travel surveys as an illustrative example, we explore the benefits and drawbacks of each approach, and propose a decision framework to guide researchers in selecting the appropriate integration strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07521v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Danielle Mccool, Peter Lugtig, Bella Struminskaya</dc:creator>
    </item>
    <item>
      <title>Density estimation for compositional data using nonparametric mixtures</title>
      <link>https://arxiv.org/abs/2510.07608</link>
      <description>arXiv:2510.07608v1 Announce Type: new 
Abstract: Compositional data, representing proportions constrained to the simplex, arise in diverse fields such as geosciences, ecology, genomics, and microbiome research. Existing nonparametric density estimation methods often rely on transformations, which may induce substantial bias near the simplex boundary. We propose a nonparametric mixture-based framework for density estimation on compositions. Nonparametric Dirichlet mixtures are employed to naturally accommodate boundary values, thereby avoiding the transformation or zero-replacement, while also identifying components supported on the boundary, providing reliable estimates for data with zero or near-zero values. Bandwidth selection and initialization schemes are addressed. For comparison, nonparametric Gaussian mixtures, coupled with log-ratio transformations, are also considered. Extensive simulations show that the proposed estimators outperform existing approaches. Three real data applications, including GDP data analysis, handwritten digit recognition, and skin detection, demonstrate the usefulness of nonparametric Dirichlet mixtures in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07608v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiajin Xie, Yong Wang, Eduardo Garc\'ia-Portugu\'es</dc:creator>
    </item>
    <item>
      <title>Adjusted Random Effect Block Bootstraps for Highly Unbalanced Clustered Data</title>
      <link>https://arxiv.org/abs/2510.07770</link>
      <description>arXiv:2510.07770v1 Announce Type: new 
Abstract: Clustered data arise naturally in many scientific and applied research settings where units are grouped within clusters. They are commonly analyzed using linear mixed models to account for within-cluster correlations. This article focuses on the scenario in which cluster sizes might be highly unbalanced and proposes a proportional random effect block bootstrap and a modified random effect block bootstrap, which are applicable in such cases and accommodate general distributions of random effects and error terms. These methods generalize the random effect block bootstrap, originally designed for the balanced case, and can be used for inference on parameters of linear mixed models or functions thereof. Both proposed bootstraps are shown to enjoy Fisher consistency under general cluster sizes, while the original random effect block bootstrap is consistent only for balanced clusters. Simulations demonstrate strong finite sample inferential performance of the proposed bootstraps relative to the random effect block bootstrap and other existing bootstrap methods for clustered data. Application to the Oman rainfall enhancement trial dataset, with cluster sizes ranging from 1 to 58, shows improved bootstrap confidence intervals using the proposed bootstraps over the random effect block bootstrap and a statistically significant effect of the ionization technology on rainfall.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07770v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhi Yang Tho, Raymond Chambers, A. H. Welsh</dc:creator>
    </item>
    <item>
      <title>Detection of mean changes in partially observed functional data</title>
      <link>https://arxiv.org/abs/2510.07854</link>
      <description>arXiv:2510.07854v1 Announce Type: new 
Abstract: We propose a test for a change in the mean for a sequence of functional observations that are only partially observed on subsets of the domain, with no information available on the complement. The framework accommodates important scenarios, including both abrupt and gradual changes. The significance of the test statistic is assessed via a permutation test. In addition to the classical permutation approach with a fixed number of permutation samples, we also discuss a variant with controlled resampling risk that relies on a random (data-driven) number of permutation samples. The small sample performance of the proposed methodology is illustrated in a Monte Carlo simulation study and an application to real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07854v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\v{S}\'arka Hudecov\'a, Claudia Kirch</dc:creator>
    </item>
    <item>
      <title>Fitting sparse high-dimensional varying-coefficient models with Bayesian regression tree ensembles</title>
      <link>https://arxiv.org/abs/2510.08204</link>
      <description>arXiv:2510.08204v1 Announce Type: new 
Abstract: By allowing the effects of $p$ covariates in a linear regression model to vary as functions of $R$ additional effect modifiers, varying-coefficient models (VCMs) strike a compelling balance between interpretable-but-rigid parametric models popular in classical statistics and flexible-but-opaque methods popular in machine learning. But in high-dimensional settings where $p$ and/or $R$ exceed the number of observations, existing approaches to fitting VCMs fail to identify which covariates have a non-zero effect and which effect modifiers drive these effects. We propose sparseVCBART, a fully Bayesian model that approximates each coefficient function in a VCM with a regression tree ensemble and encourages sparsity with a global--local shrinkage prior on the regression tree leaf outputs and a hierarchical prior on the splitting probabilities of each tree. We show that the sparseVCBART posterior contracts at a near-minimax optimal rate, automatically adapting to the unknown sparsity structure and smoothness of the true coefficient functions. Compared to existing state-of-the-art methods, sparseVCBART achieved competitive predictive accuracy and substantially narrower and better-calibrated uncertainty intervals, especially for null covariate effects. We use sparseVCBART to investigate how the effects of interpersonal conversations on prejudice could vary according to the political and demographic characteristics of the respondents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08204v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soham Ghosh, Saloni Bhogale, Sameer K. Deshpande</dc:creator>
    </item>
    <item>
      <title>Bayesian Profile Regression with Linear Mixed Models (Profile-LMM) applied to Longitudinal Exposome Data</title>
      <link>https://arxiv.org/abs/2510.08304</link>
      <description>arXiv:2510.08304v1 Announce Type: new 
Abstract: Exposure to diverse non-genetic factors, known as the exposome, is a critical determinant of health outcomes. However, analyzing the exposome presents significant methodological challenges, including: high collinearity among exposures, the longitudinal nature of repeated measurements, and potential complex interactions with individual characteristics. In this paper, we address these challenges by proposing a novel statistical framework that extends Bayesian profile regression. Our method integrates profile regression, which handles collinearity by clustering exposures into latent profiles, into a linear mixed model (LMM), a framework for longitudinal data analysis. This profile-LMM approach effectively accounts for within-person variability over time while also incorporating interactions between the latent exposure clusters and individual characteristics. We validate our method using simulated data, demonstrating its ability to accurately identify model parameters and recover the true latent exposure cluster structure. Finally, we apply this approach to a large longitudinal data set from the Lifelines cohort to identify combinations of exposures that are significantly associated with diastolic blood pressure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08304v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Amestoy, Mark van de Wiel, Jeroen Lakerveld, Wessel van Wieringen</dc:creator>
    </item>
    <item>
      <title>Doubly Robust Estimation with Stabilized Weights for Binary Proximal Outcomes in Micro-Randomized Trials</title>
      <link>https://arxiv.org/abs/2510.08359</link>
      <description>arXiv:2510.08359v1 Announce Type: new 
Abstract: Micro-randomized trials (MRTs) are increasingly used to evaluate mobile health interventions with binary proximal outcomes. Standard inverse probability weighting (IPW) estimators are unbiased but unstable in small samples or under extreme randomization. Estimated mean excursion effect (EMEE) improves efficiency but lacks double robustness. We propose a doubly robust EMEE (DR-EMEE) with stabilized and truncated weights, combining per-decision IPW and outcome regression. We prove double robustness, asymptotic efficiency, and provide finite-sample variance corrections, with extensions to machine learning nuisance estimators. In simulations, DR-EMEE reduces root mean squared error, improves coverage, and achieves up to twofold efficiency gains over IPW and five to ten percent over EMEE. Applications to HeartSteps, PAMAP2, and mHealth datasets confirm stable and efficient inference across both randomized and observational settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08359v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinho Cha, Eunchan Cha</dc:creator>
    </item>
    <item>
      <title>Estimands and doubly robust estimation for cluster-randomized trials with survival outcomes</title>
      <link>https://arxiv.org/abs/2510.08438</link>
      <description>arXiv:2510.08438v1 Announce Type: new 
Abstract: Cluster-randomized trials (CRTs) are experimental designs where groups or clusters of participants, rather than the individual participants themselves, are randomized to intervention groups. Analyzing CRT requires distinguishing between treatment effects at the cluster level and the individual level, which requires a clear definition of the estimands under the potential outcomes framework. For analyzing survival outcomes, it is common to assess the treatment effect by comparing survival functions or restricted mean survival times between treatment groups. In this article, we formally characterize cluster-level and individual-level treatment effect estimands with right-censored survival outcomes in CRTs and propose doubly robust estimators for targeting such estimands. Under covariate-dependent censoring, our estimators ensure consistency when either the censoring model or the outcome model is correctly specified, but not necessarily both. We explore different modeling options for the censoring and outcome models to estimate the censoring and survival distributions, and investigate a deletion-based jackknife method for variance and interval estimation. Extensive simulations demonstrate that the proposed methods perform adequately in finite samples. Finally, we illustrate our method by analyzing a completed CRT with survival endpoints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08438v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xi Fang, Bingkai Wang, Liangyuan Hu, Fan Li</dc:creator>
    </item>
    <item>
      <title>Evaluating and Learning Optimal Dynamic Treatment Regimes under Truncation by Death</title>
      <link>https://arxiv.org/abs/2510.07501</link>
      <description>arXiv:2510.07501v1 Announce Type: cross 
Abstract: Truncation by death, a prevalent challenge in critical care, renders traditional dynamic treatment regime (DTR) evaluation inapplicable due to ill-defined potential outcomes. We introduce a principal stratification-based method, focusing on the always-survivor value function. We derive a semiparametrically efficient, multiply robust estimator for multi-stage DTRs, demonstrating its robustness and efficiency. Empirical validation and an application to electronic health records showcase its utility for personalized treatment optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07501v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sihyung Park (North Carolina State University), Wenbin Lu (North Carolina State University), Shu Yang (North Carolina State University)</dc:creator>
    </item>
    <item>
      <title>A coupling-based approach to f-divergences diagnostics for Markov chain Monte Carlo</title>
      <link>https://arxiv.org/abs/2510.07559</link>
      <description>arXiv:2510.07559v1 Announce Type: cross 
Abstract: A long-standing gap exists between the theoretical analysis of Markov chain Monte Carlo convergence, which is often based on statistical divergences, and the diagnostics used in practice. We introduce the first general convergence diagnostics for Markov chain Monte Carlo based on any f-divergence, allowing users to directly monitor, among others, the Kullback--Leibler and the $\chi^2$ divergences as well as the Hellinger and the total variation distances. Our first key contribution is a coupling-based `weight harmonization' scheme that produces a direct, computable, and consistent weighting of interacting Markov chains with respect to their target distribution. The second key contribution is to show how such consistent weightings of empirical measures can be used to provide upper bounds to f-divergences in general. We prove that these bounds are guaranteed to tighten over time and converge to zero as the chains approach stationarity, providing a concrete diagnostic. Numerical experiments demonstrate that our method is a practical and competitive diagnostic tool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07559v1</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrien Corenflos, Hai-Dang Dau</dc:creator>
    </item>
    <item>
      <title>A Honest Cross-Validation Estimator for Prediction Performance</title>
      <link>https://arxiv.org/abs/2510.07649</link>
      <description>arXiv:2510.07649v1 Announce Type: cross 
Abstract: Cross-validation is a standard tool for obtaining a honest assessment of the performance of a prediction model. The commonly used version repeatedly splits data, trains the prediction model on the training set, evaluates the model performance on the test set, and averages the model performance across different data splits. A well-known criticism is that such cross-validation procedure does not directly estimate the performance of the particular model recommended for future use. In this paper, we propose a new method to estimate the performance of a model trained on a specific (random) training set. A naive estimator can be obtained by applying the model to a disjoint testing set. Surprisingly, cross-validation estimators computed from other random splits can be used to improve this naive estimator within a random-effects model framework. We develop two estimators -- a hierarchical Bayesian estimator and an empirical Bayes estimator -- that perform similarly to or better than both the conventional cross-validation estimator and the naive single-split estimator. Simulations and a real-data example demonstrate the superior performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07649v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyu Pan, Vincent Z. Yu, Viswanath Devanarayan, Lu Tian</dc:creator>
    </item>
    <item>
      <title>Adaptive Thresholds for Monitoring and Screening in Imbalanced Samples: Optimality and Boosting Sensitivity</title>
      <link>https://arxiv.org/abs/2510.08035</link>
      <description>arXiv:2510.08035v1 Announce Type: cross 
Abstract: Suppose (standardized) measurements or statistics are monitored to raise an alarm when a threshold is exceeded. Often, the underlying population is heterogenous with respect to important discrete variables and thus samples may consist of imbalanced classes. We propose to use thresholds which depend on such covariates to boost the sensitivity for rare classes, which otherwise tend to be ignored. Under mild conditions, we identify optimal threshold functions and develop a feasible procedure for their computation. Further, for the proportional rule a nonparametric estimator of the threshold function is proposed and a central limit theorem is shown, including the case that conditional mean and variance used for standardization are estimated. For feasible uncertainty quantification a bootstrap scheme is proposed. The approach is illustrated and evaluated by a real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08035v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ansgar Steland</dc:creator>
    </item>
    <item>
      <title>Two-Stage Trigonometric Regression for Modeling Circadian Rhythms</title>
      <link>https://arxiv.org/abs/2510.08309</link>
      <description>arXiv:2510.08309v1 Announce Type: cross 
Abstract: Gene expression levels, hormone secretion, and internal body temperature each oscillate over an approximately 24-hour cycle, or display circadian rhythms. Many circadian biology studies have investigated how these rhythms vary across cohorts, uncovering associations between atypical rhythms and diseases such as cancer, metabolic syndrome, and sleep disorders. A challenge in analyzing circadian biology data is that the oscillation peak and trough times for a phenomenon differ across individuals. If these individual-level differences are not accounted for in trigonometric regression, which is prevalent in circadian biology studies, then estimates of the population-level amplitude parameters can suffer from attenuation bias. This attenuation bias could lead to inaccurate study conclusions. To address attenuation bias, we propose a refined two-stage (RTS) method for trigonometric regression given longitudinal data obtained from each individual participating in a study. In the first stage, the parameters of individual-level models are estimated. In the second stage, transformations of these individual-level estimates are aggregated to produce population-level parameter estimates for inference. Simulation studies show that our RTS method mitigates bias in parameter estimation, obtains greater statistical power, and maintains appropriate type I error control when compared to the standard two-stage (STS) method, which ignores individual-level differences in peak and trough times. The only exception for parameter estimation and statistical power occurs when the oscillation amplitudes are weak relative to random variability in the data and the sample size is small. Illustrations with cortisol level data and heart rate data show that our RTS method obtains larger population-level amplitude parameter estimates and smaller $p$-values for multiple hypothesis tests when compared to the STS method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08309v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael T. Gorczyca, Jenna D. Li, Charissa M. Newkirk, Arjun S. Srivatsa, Hugo F. M. Milan</dc:creator>
    </item>
    <item>
      <title>GARCH copulas, v-transforms and D-vines for stochastic volatility</title>
      <link>https://arxiv.org/abs/2408.07025</link>
      <description>arXiv:2408.07025v2 Announce Type: replace 
Abstract: The bivariate copulas that describe the dependencies and partial dependencies of lagged variables in strictly stationary, first-order GARCH-type processes are investigated. It is shown that the copulas of symmetric GARCH processes are jointly symmetric but non-exchangeable, while the copulas of processes with symmetric innovation distributions and asymmetric leverage effects have weaker h-symmetry; copulas with asymmetric innovation distributions have neither form of symmetry. Since the true bivariate copulas are typically inaccessible, due to the unknown functional forms of the marginal distributions of GARCH processes, a new class of approximating copulas is proposed. These rely on copula density constructions that combine standard bivariate copula densities for positive dependence with two uniformity-preserving transformations known as v-transforms. The construction is shown to be particularly effective when applied to the density of the copula of the absolute values of a spherical t distribution. Tractable simplified D-vines incorporating the new pair copulas are developed for applications to time series showing stochastic volatility. The resulting models are shown to provide better fits to simulated data from GARCH processes, and to a dataset of financial exchange-rate returns, than have previously been obtained using vine copulas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07025v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexandra Dias, Jialing Han, Alexander J. McNeil</dc:creator>
    </item>
    <item>
      <title>On testing for independence between generalized error models of several time series</title>
      <link>https://arxiv.org/abs/2410.24003</link>
      <description>arXiv:2410.24003v3 Announce Type: replace 
Abstract: We define generalized innovations associated with generalized error models having arbitrary distributions, that is, distributions that can be mixtures of continuous and discrete distributions. These models include stochastic volatility models and regime-switching models. We also propose statistics for testing independence between the generalized errors of these models, extending previous results of Duchesne, Ghoudi and Remillard (2012) obtained for stochastic volatility models. We define families of empirical processes constructed from lagged generalized errors, and we show that their joint asymptotic distributions are Gaussian and independent of the estimated parameters of the individual time series. Moebius transformations of the empirical processes are used to obtain tractable covariances. Several tests statistics are then proposed, based on Cramer-von Mises statistics and dependence measures, as well as graphical methods to visualize the dependence. In addition, numerical experiments are performed to assess the power of the proposed tests. Finally, to show the usefulness of our methodologies, examples of applications for financial data and crime data are given to cover both discrete and continuous cases. ll developed methodologies are implemented in the CRAN package IndGenErrors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.24003v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kilani Ghoudi, Bouchra R. Nasri, Bruno N. Remillard</dc:creator>
    </item>
    <item>
      <title>Learning to Partially Defer for Sequences</title>
      <link>https://arxiv.org/abs/2502.01459</link>
      <description>arXiv:2502.01459v2 Announce Type: replace 
Abstract: In the Learning to Defer (L2D) framework, a prediction model can either make a prediction or defer it to an expert, as determined by a rejector. Current L2D methods train the rejector to decide whether to reject the {\em entire prediction}, which is not desirable when the model predicts long sequences. We present an L2D setting for sequence outputs where the system can defer \textit{specific outputs} of the whole model prediction to an expert in an effort to interleave the expert and machine throughout the prediction. We propose two types of model-based post-hoc rejectors for pre-trained predictors: a token-level rejector, which defers specific token predictions to experts with next token prediction capabilities, and a one-time rejector for experts without such abilities, which defers the remaining sequence from a specific point onward. In the experiments, we also empirically demonstrate that such granular deferrals achieve better cost-accuracy tradeoffs than whole deferrals on Traveling salesman solvers, News summarization, and Weather prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01459v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sahana Rayan, Ambuj Tewari</dc:creator>
    </item>
    <item>
      <title>Quantile Forecast Matching with a Bayesian Quantile Gaussian Process Model</title>
      <link>https://arxiv.org/abs/2502.06605</link>
      <description>arXiv:2502.06605v2 Announce Type: replace 
Abstract: A set of probabilities along with corresponding quantiles are often used to define predictive distributions or probabilistic forecasts. These quantile predictions offer easily interpreted uncertainty of an event, and quantiles are generally straightforward to estimate using standard statistical and machine learning methods. However, compared to a distribution defined by a probability density or cumulative distribution function, a set of quantiles has less distributional information. When given estimated quantiles, it may be desirable to estimate a fully defined continuous distribution function. Many researchers do so to make evaluation or ensemble modeling simpler. Most existing methods for fitting a distribution to quantiles lack accurate representation of the inherent uncertainty from quantile estimation or are limited in their applications. In this manuscript, we present a Gaussian process model, the quantile Gaussian process, which is based on established theory of quantile functions and sample quantiles, to construct a probability distribution given estimated quantiles. A Bayesian application of the quantile Gaussian process is evaluated for parameter inference and distribution approximation in simulation studies. The quantile Gaussian process is used to approximate the distributions of quantile forecasts from the 2023-24 US Centers for Disease Control collaborative flu forecasting initiative. The simulation studies and data analysis show that the quantile Gaussian process leads to accurate inference on model parameters, estimation of a continuous distribution, and uncertainty quantification of sample quantiles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06605v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Spencer Wadsworth, Jarad Niemi</dc:creator>
    </item>
    <item>
      <title>Efficient Inference in First Passage Time Models</title>
      <link>https://arxiv.org/abs/2503.18381</link>
      <description>arXiv:2503.18381v2 Announce Type: replace 
Abstract: First passage time models describe the time it takes for a random process to exit a region of interest and are widely used across various scientific fields. Fast and accurate numerical methods for computing the likelihood function in these models are essential for efficient statistical inference of model parameters. Specifically, in computational cognitive neuroscience, generalized drift diffusion models (GDDMs) are an important class of first passage time models that describe the latent psychological processes underlying simple decision-making scenarios. GDDMs model the joint distribution over choices and response times as the first hitting time of a one-dimensional stochastic differential equation (SDE) to possibly time-varying upper and lower boundaries. They are widely applied to extract parameters associated with distinct cognitive and neural mechanisms. However, current likelihood computation methods struggle in common application scenarios in which drift rates dynamically vary within trials as a function of exogenous covariates (e.g., brain activity in specific regions or visual fixations). In this work, we propose a fast and flexible algorithm for computing the likelihood function of GDDMs based on a large class of SDEs satisfying the Cherkasov condition. Our method divides each trial into discrete stages, employs fast analytical results to compute stage-wise densities, and integrates these to compute the overall trial-wise likelihood. Numerical examples demonstrate that our method not only yields accurate likelihood evaluations for efficient statistical inference, but also considerably outperforms existing approaches in terms of speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18381v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sicheng Liu, Alexander Fengler, Michael J. Frank, Matthew T. Harrison</dc:creator>
    </item>
    <item>
      <title>Exact, Nonparametric Sensitivity Analysis for Observational Studies of Contingency Tables</title>
      <link>https://arxiv.org/abs/2507.17207</link>
      <description>arXiv:2507.17207v2 Announce Type: replace 
Abstract: In observational studies, contingency tables provide a simple and intuitive approach to study associations between categorical variables. However, any test of association in contingency tables may be biased due to unmeasured confounders. Existing sensitivity analyses that assess the impact of unmeasured confounding on association tests typically assume a binary treatment variable or impose strong parametric assumptions on the non-binary treatment variable. We overcome these limitations with an exact (i.e., non-asymptotic) and nonparametric sensitivity analysis for unmeasured confounding in $I \times J$ or $I \times J \times K$ contingency tables, accommodating both non-binary treatment and non-binary outcome. Specifically, we extend Rosenbaum's sensitivity model for generic bias and develop a general method to calculate the exact worst-case null distribution for any permutation-invariant test, which includes the chi-square test and many likelihood-based or score-based tests of association. We also propose specialized results for sub-families of permutation-invariant tests that lead to more efficient computation of the worst-case null distribution. Finally, we show that sensitivity analyses based on tests that utilize all treatment and outcome levels typically have higher power than "naive" approaches that dichotomize the categorical variables. We conclude with a re-analysis of the effect of pre-kindergarten care on math achievement using data from the Early Childhood Longitudinal Study, Kindergarten Class of 1998-1999. An R package, sensitivityIxJ, implements the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17207v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elaine K. Chiu, Hyunseung Kang</dc:creator>
    </item>
    <item>
      <title>Conformal Robust Control of Linear Systems</title>
      <link>https://arxiv.org/abs/2405.16250</link>
      <description>arXiv:2405.16250v4 Announce Type: replace-cross 
Abstract: End-to-end engineering design pipelines, in which designs are evaluated using concurrently defined optimal controllers, are becoming increasingly common in practice. To discover designs that perform well even under the misspecification of system dynamics, such end-to-end pipelines have now begun evaluating designs with a robust control objective in place of the nominal optimal control setup. Current approaches of specifying such robust control subproblems, however, rely on hand specification of perturbations anticipated to be present upon deployment or margin methods that ignore problem structure, resulting in a lack of theoretical guarantees and overly conservative empirical performance. We, instead, propose a novel methodology for LQR systems that leverages conformal prediction to specify such uncertainty regions in a data-driven fashion. Such regions have distribution-free coverage guarantees on the true system dynamics, in turn allowing for a probabilistic characterization of the regret of the resulting robust controller. We then demonstrate that such a controller can be efficiently produced via a novel policy gradient method that has convergence guarantees. We finally demonstrate the superior empirical performance of our method over alternate robust control specifications, such as $H_{\infty}$ and LQR with multiplicative noise, across a collection of engineering control systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16250v4</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yash Patel, Sahana Rayan, Ambuj Tewari</dc:creator>
    </item>
    <item>
      <title>Average Controlled and Average Natural Micro Direct Effects in Summary Causal Graphs</title>
      <link>https://arxiv.org/abs/2410.23975</link>
      <description>arXiv:2410.23975v2 Announce Type: replace-cross 
Abstract: In this paper, we investigate the identifiability of average controlled direct effects and average natural direct effects in causal systems represented by summary causal graphs, which are abstractions of full causal graphs, often used in dynamic systems where cycles and omitted temporal information complicate causal inference. Unlike in the traditional linear setting, where direct effects are typically easier to identify and estimate, non-parametric direct effects, which are crucial for handling real-world complexities, particularly in epidemiological contexts where relationships between variables (e.g, genetic, environmental, and behavioral factors) are often non-linear, are much harder to define and identify. In particular, we give sufficient conditions for identifying average controlled micro direct effect and average natural micro direct effect from summary causal graphs in the presence of hidden confounding. Furthermore, we show that the conditions given for the average controlled micro direct effect become also necessary in the setting where there is no hidden confounding and where we are only interested in identifiability by adjustment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23975v2</guid>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Ferreira, Charles K. Assaad</dc:creator>
    </item>
    <item>
      <title>A Necessary and Sufficient Condition for Size Controllability of Heteroskedasticity Robust Test Statistics</title>
      <link>https://arxiv.org/abs/2412.17470</link>
      <description>arXiv:2412.17470v2 Announce Type: replace-cross 
Abstract: We revisit size controllability results in P\"otscher and Preinerstorfer (2025) concerning heteroskedasticity robust test statistics in regression models. For the special, but important, case of testing a single restriction (e.g., a zero restriction on a single coefficient), we povide a necessary and sufficient condition for size controllability, whereas the condition in P\"otscher and Preinerstorfer (2025) is, in general, only sufficient (even in the case of testing a single restriction).</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17470v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Benedikt M. P\"otscher, David Preinerstorfer</dc:creator>
    </item>
    <item>
      <title>Learning General Causal Structures with Hidden Dynamic Process for Climate Analysis</title>
      <link>https://arxiv.org/abs/2501.12500</link>
      <description>arXiv:2501.12500v2 Announce Type: replace-cross 
Abstract: Understanding climate dynamics requires going beyond correlations in observational data to uncover their underlying causal process. Latent drivers, such as atmospheric processes, play a critical role in temporal dynamics, while direct causal influences also exist among geographically proximate observed variables. Traditional Causal Representation Learning (CRL) typically focuses on latent factors but overlooks such observable-to-observable causal relations, limiting its applicability to climate analysis. In this paper, we introduce a unified framework that jointly uncovers (i) causal relations among observed variables and (ii) latent driving forces together with their interactions. We establish conditions under which both the hidden dynamic processes and the causal structure among observed variables are simultaneously identifiable from time-series data. Remarkably, our guarantees hold even in the nonparametric setting, leveraging contextual information to recover latent variables and causal relations. Building on these insights, we propose CaDRe (Causal Discovery and Representation learning), a time-series generative model with structural constraints that integrates CRL and causal discovery. Experiments on synthetic datasets validate our theoretical results. On real-world climate datasets, CaDRe not only delivers competitive forecasting accuracy but also recovers visualized causal graphs aligned with domain expertise, thereby offering interpretable insights into climate systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12500v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minghao Fu, Biwei Huang, Zijian Li, Yujia Zheng, Ignavier Ng, Guangyi Chen, Yingyao Hu, Kun Zhang</dc:creator>
    </item>
  </channel>
</rss>
