<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 16 Oct 2025 01:45:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Thin Trees via $k$-Respecting Cut Identities</title>
      <link>https://arxiv.org/abs/2510.12050</link>
      <description>arXiv:2510.12050v1 Announce Type: new 
Abstract: Thin spanning trees lie at the intersection of graph theory, approximation algorithms, and combinatorial optimization. They are central to the long-standing \emph{thin tree conjecture}, which asks whether every $k$-edge-connected graph contains an $O(1/k)$-thin tree, and they underpin algorithmic breakthroughs such as the $O(\log n/\log\log n)$-approximation for ATSP. Yet even the basic algorithmic task of \emph{verifying} that a given tree is thin has remained elusive: checking thinness requires reasoning about exponentially many cuts, and no efficient certificates have been known.
  We introduce a new machinery of \emph{$k$-respecting cut identities}, which express the weight of every cut that crosses a spanning tree in at most $k$ edges as a simple function of pairwise ($2$-respecting) cuts. This yields a tree-local oracle that, after $O(n^2)$ preprocessing, evaluates such cuts in $O_k(1)$ time. Building on this oracle, we give the first procedure to compute the exact $k$-thinness certificate $\Theta_k(T)$ of any spanning tree for fixed $k$ in time $\tilde O(n^2+n^k)$, outputting both the certificate value and a witnessing cut.
  Beyond general graphs, our framework yields sharper guarantees in structured settings. In planar graphs, duality with cycles and dual girth imply that every spanning tree admits a verifiable certificate $\Theta_k(T)\le k/\lambda$ (hence $O(1/\lambda)$ for constant $k$). In graphs embedded on a surface of genus $\gamma$, refined counting gives certified (per-cut) bounds $O((\log n+\gamma)/\lambda)$ via the same ensemble coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12050v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohit Daga</dc:creator>
    </item>
    <item>
      <title>Engineering Dominating Patterns: A Fine-grained Case Study</title>
      <link>https://arxiv.org/abs/2510.12232</link>
      <description>arXiv:2510.12232v1 Announce Type: new 
Abstract: The \emph{Dominating $H$-Pattern} problem generalizes the classical $k$-Dominating Set problem: for a fixed \emph{pattern} $H$ and a given graph $G$, the goal is to find an induced subgraph $S$ of $G$ such that (1) $S$ is isomorphic to $H$, and (2) $S$ forms a dominating set in $G$. Fine-grained complexity results show that on worst-case inputs, any significant improvement over the naive brute-force algorithm is unlikely, as this would refute the Strong Exponential Time Hypothesis. Nevertheless, a recent work by Dransfeld et al. (ESA 2025) reveals some significant improvement potential particularly in \emph{sparse} graphs.
  We ask: Can algorithms with conditionally almost-optimal worst-case performance solve the Dominating $H$-Pattern, for selected patterns $H$, efficiently on practical inputs? We develop and experimentally evaluate several approaches on a large benchmark of diverse datasets, including baseline approaches using the Glasgow Subgraph Solver (GSS), the SAT solver Kissat, and the ILP solver Gurobi.
  Notably, while a straightforward implementation of the algorithms -- with conditionally close-to-optimal worst-case guarantee -- performs comparably to existing solvers, we propose a tailored Branch-\&amp;-Bound approach -- supplemented with careful pruning techniques -- that achieves improvements of up to two orders of magnitude on our test instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12232v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Dransfeld, Marvin K\"unnemann, Mirza Redzic, Marcus Wunderlich</dc:creator>
    </item>
    <item>
      <title>Exact Matching and Top-k Perfect Matching Parameterized by Neighborhood Diversity or Bandwidth</title>
      <link>https://arxiv.org/abs/2510.12552</link>
      <description>arXiv:2510.12552v1 Announce Type: new 
Abstract: The Exact Matching (EM) problem asks whether there exists a perfect matching which uses a prescribed number of red edges in a red/blue edge-colored graph. While there exists a randomized polynomial-time algorithm for the problem, only some special cases admit a deterministic one so far, making it a natural candidate for testing the P=RP hypothesis. A polynomial-time equivalent problem, Top-k Perfect Matching (TkPM), asks for a perfect matching maximizing the weight of the $k$ heaviest edges.
  We study the above problems, mainly the latter, in the scenario where the input is a blown-up graph, meaning a graph which had its vertices replaced by cliques or independent sets. We describe an FPT algorithm for TkPM parameterized by $k$ and the neighborhood diversity of the input graph, which is essentially the size of the graph before the blow-up; this graph is also called the prototype. We extend this algorithm into an approximation scheme with a much softer dependency on the aforementioned parameters, time-complexity wise. Moreover, for prototypes with bounded bandwidth but unbounded size, we develop a recursive algorithm that runs in subexponential time. Utilizing another algorithm for EM on bounded neighborhood diversity graphs, we adapt this recursive subexponential algorithm to EM.
  Our approach is similar to the use of dynamic programming on e.g. bounded treewidth instances for various problems. The main point is that the existence of many disjoint separators is utilized to avoid including in the separator any of a set of ``bad'' vertices during the split phase.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12552v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas El Maalouly, Kostas Lakis</dc:creator>
    </item>
    <item>
      <title>Lossless Derandomization for Undirected Single-Source Shortest Paths and Approximate Distance Oracles</title>
      <link>https://arxiv.org/abs/2510.12598</link>
      <description>arXiv:2510.12598v1 Announce Type: new 
Abstract: A common step in algorithms related to shortest paths in undirected graphs is that, we select a subset of vertices as centers, then grow a ball around each vertex until a center is reached. We want the balls to be as small as possible. A randomized algorithm can uniformly sample $r$ centers to achieve the optimal (expected) ball size of $\Theta(n/r)$. A folklore derandomization is to use the $O(\log n)$ approximation for the set cover problem in the hitting set version where we want to hit all the balls with the centers.
  However, the extra $O(\log n)$ factor is sometimes too expensive. For example, the recent $O(m\sqrt{\log n\log\log n})$ undirected single-source shortest path algorithm [DMSY23] beats Dijkstra's algorithm in sparse graphs, but the folklore derandomization would make it dominated by Dijkstra's.
  In this paper, we exploit the fact that the sizes of these balls can be adaptively chosen by the algorithm instead of fixed by the input. We propose a simple deterministic algorithm achieving the optimal ball size of $\Theta(n/r)$ on average. Furthermore, given any polynomially large cost function of the ball size, we can still achieve the optimal cost on average. It allows us to derandomize [DMSY23], resulting in a deterministic $O(m\sqrt{\log n\log\log n})$ algorithm for undirected single-source shortest path.
  In addition, we show that the same technique can also be used to derandomize the seminal Thorup-Zwick approximate distance oracle [TZ05], also without any loss in the time/space complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12598v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuyi Yan</dc:creator>
    </item>
    <item>
      <title>Vizing's Theorem in Deterministic Almost-Linear Time</title>
      <link>https://arxiv.org/abs/2510.12619</link>
      <description>arXiv:2510.12619v1 Announce Type: new 
Abstract: Vizing's theorem states that any $n$-vertex $m$-edge graph of maximum degree $\Delta$ can be edge colored using at most $\Delta + 1$ different colors. Vizing's original proof is easily translated into a deterministic $O(mn)$ time algorithm. This deterministic time bound was subsequently improved to $\tilde O(m \sqrt n)$ time, independently by [Arjomandi, 1982] and by [Gabow et al., 1985].
  A series of recent papers improved the time bound of $\tilde O(m\sqrt{n})$ using randomization, culminating in the randomized near-linear time $(\Delta+1)$-coloring algorithm by [Assadi, Behnezhad, Bhattacharya, Costa, Solomon, and Zhang, 2025]. At the heart of all of these recent improvements, there is some form of a sublinear time algorithm. Unfortunately, sublinear time algorithms as a whole almost always require randomization. This raises a natural question: can the deterministic time complexity of the problem be reduced below the $\tilde O(m\sqrt{n})$ barrier?
  In this paper, we answer this question in the affirmative. We present a deterministic almost-linear time $(\Delta+1)$-coloring algorithm, namely, an algorithm running in $m \cdot 2^{O(\sqrt{\log \Delta})} \cdot \log n = m^{1+o(1)}$ time. Our main technical contribution is to entirely forego sublinear time algorithms. We do so by presenting a new deterministic color-type sparsification approach that runs in almost-linear (instead of sublinear) time, but can be used to color a much larger set of edges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12619v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sepehr Assadi, Soheil Behnezhad, Sayan Bhattacharya, Mart\'in Costa, Shay Solomon, Tianyi Zhang</dc:creator>
    </item>
    <item>
      <title>On the Complexity of Nucleolus Computation for Bipartite b-Matching Games</title>
      <link>https://arxiv.org/abs/2105.07161</link>
      <description>arXiv:2105.07161v3 Announce Type: cross 
Abstract: We explore the complexity of nucleolus computation in b-matching games on bipartite graphs. We show that computing the nucleolus of a simple b-matching game is NP-hard even on bipartite graphs of maximum degree 7. We complement this with partial positive results in the special case where b values are bounded by 2. In particular, we describe an efficient algorithm when a constant number of vertices satisfy b(v) = 2 as well as an efficient algorithm for computing the non-simple b-matching nucleolus when b = 2.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.07161v3</guid>
      <category>cs.GT</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.tcs.2024.114476</arxiv:DOI>
      <dc:creator>Jochen Koenemann, Justin Toth, Felix Zhou</dc:creator>
    </item>
    <item>
      <title>Replicable Learning of Large-Margin Halfspaces</title>
      <link>https://arxiv.org/abs/2402.13857</link>
      <description>arXiv:2402.13857v2 Announce Type: cross 
Abstract: We provide efficient replicable algorithms for the problem of learning large-margin halfspaces. Our results improve upon the algorithms provided by Impagliazzo, Lei, Pitassi, and Sorrell [STOC, 2022]. We design the first dimension-independent replicable algorithms for this task which runs in polynomial time, is proper, and has strictly improved sample complexity compared to the one achieved by Impagliazzo et al. [2022] with respect to all the relevant parameters. Moreover, our first algorithm has sample complexity that is optimal with respect to the accuracy parameter $\epsilon$. We also design an SGD-based replicable algorithm that, in some parameters' regimes, achieves better sample and time complexity than our first algorithm. Departing from the requirement of polynomial time algorithms, using the DP-to-Replicability reduction of Bun, Gaboardi, Hopkins, Impagliazzo, Lei, Pitassi, Sorrell, and Sivakumar [STOC, 2023], we show how to obtain a replicable algorithm for large-margin halfspaces with improved sample complexity with respect to the margin parameter $\tau$, but running time doubly exponential in $1/\tau^2$ and worse sample complexity dependence on $\epsilon$ than one of our previous algorithms. We then design an improved algorithm with better sample complexity than all three of our previous algorithms and running time exponential in $1/\tau^{2}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13857v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alkis Kalavasis, Amin Karbasi, Kasper Green Larsen, Grigoris Velegkas, Felix Zhou</dc:creator>
    </item>
    <item>
      <title>High-Probability Bounds For Heterogeneous Local Differential Privacy</title>
      <link>https://arxiv.org/abs/2510.11895</link>
      <description>arXiv:2510.11895v1 Announce Type: cross 
Abstract: We study statistical estimation under local differential privacy (LDP) when users may hold heterogeneous privacy levels and accuracy must be guaranteed with high probability. Departing from the common in-expectation analyses, and for one-dimensional and multi-dimensional mean estimation problems, we develop finite sample upper bounds in $\ell_2$-norm that hold with probability at least $1-\beta$. We complement these results with matching minimax lower bounds, establishing the optimality (up to constants) of our guarantees in the heterogeneous LDP regime. We further study distribution learning in $\ell_\infty$-distance, designing an algorithm with high-probability guarantees under heterogeneous privacy demands. Our techniques offer principled guidance for designing mechanisms in settings with user-specific privacy levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11895v1</guid>
      <category>stat.ML</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maryam Aliakbarpour, Alireza Fallah, Swaha Roy, Ria Stevens</dc:creator>
    </item>
    <item>
      <title>The Structure of In-Place Space-Bounded Computation</title>
      <link>https://arxiv.org/abs/2510.12005</link>
      <description>arXiv:2510.12005v1 Announce Type: cross 
Abstract: In the standard model of computing multi-output functions in logspace ($\mathsf{FL}$), we are given a read-only tape holding $x$ and a logarithmic length worktape, and must print $f(x)$ to a dedicated write-only tape. However, there has been extensive work (both in theory and in practice) on algorithms that transform $x$ into $f(x)$ in-place on a single read-write tape with limited (in our case $O(\log n)$) additional workspace. We say $f\in \mathsf{inplaceFL}$ if $f$ can be computed in this model. We initiate the study of in-place computation from a structural complexity perspective, proving upper and lower bounds on the power of $\mathsf{inplaceFL}$. We show the following:
  i) Unconditionally, $\mathsf{FL}\not\subseteq \mathsf{inplaceFL}$.
  ii) The problems of integer multiplication and evaluating $\mathsf{NC}^0_4$ circuits lie outside $\mathsf{inplaceFL}$ under cryptographic assumptions. However, evaluating $\mathsf{NC}^0_2$ circuits can be done in $\mathsf{inplaceFL}$.
  iii) We have $\mathsf{FL} \subseteq \mathsf{inplaceFL}^{\mathsf{STP}}.$ Consequently, proving $\mathsf{inplaceFL} \not\subseteq \mathsf{FL}$ would imply $\mathsf{SAT} \not\in \mathsf{L}$.
  We also consider the analogous catalytic class ($\mathsf{inplaceFCL}$), where the in-place algorithm has a large additional worktape tape that it must reset at the end of the computation. We give $\mathsf{inplaceFCL}$ algorithms for matrix multiplication and inversion over polynomial-sized finite fields. We furthermore use our results and techniques to show two novel barriers to proving $\mathsf{CL} \subseteq \mathsf{P}$. First, we show that any proof of $\mathsf{CL}\subseteq \mathsf{P}$ must be non-relativizing, by giving an oracle relative to which $\mathsf{CL}^O=\mathsf{EXP}^O$. Second, we identify a search problem in $\mathsf{searchCL}$ but not known to be in $\mathsf{P}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12005v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Cook, Surendra Ghentiyala, Ian Mertz, Edward Pyne, Nathan S. Sheffield</dc:creator>
    </item>
    <item>
      <title>Planted clique recovery in random geometric graphs</title>
      <link>https://arxiv.org/abs/2510.12365</link>
      <description>arXiv:2510.12365v1 Announce Type: cross 
Abstract: We investigate the problem of identifying planted cliques in random geometric graphs, focusing on two distinct algorithmic approaches: the first based on vertex degrees (VD) and the other on common neighbors (CN). We analyze the performance of these methods under varying regimes of key parameters, namely the average degree of the graph and the size of the planted clique. We demonstrate that exact recovery is achieved with high probability as the graph size increases, in a specific set of parameters. Notably, our results reveal that the CN-algorithm significantly outperforms the VD-algorithm. In particular, in the connectivity regime, tiny planted cliques (even edges) are correctly identified by the CN-algorithm, yielding a significant impact on anomaly detection. Finally, our results are confirmed by a series of numerical experiments, showing that the devised algorithms are effective in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12365v1</guid>
      <category>math.PR</category>
      <category>cs.DS</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Konstantin Avrachenkov, Andrei Bobu, Nelly Litvak, Riccardo Michielan</dc:creator>
    </item>
    <item>
      <title>Single-Deviation Stability in Additively Separable Hedonic Games with Constrained Coalition Sizes</title>
      <link>https://arxiv.org/abs/2510.12641</link>
      <description>arXiv:2510.12641v1 Announce Type: cross 
Abstract: We study stability in additively separable hedonic games when coalition sizes have to respect fixed size bounds. We consider four classic notions of stability based on single-agent deviations, namely, Nash stability, individual stability, contractual Nash stability, and contractual individual stability. For each stability notion, we consider two variants: in one, the coalition left behind by a deviator must still be of a valid size, and in the other there is no such constraint. We provide a full picture of the existence of stable outcomes with respect to given size parameters. Additionally, when there are only upper bounds, we fully characterize the computational complexity of the associated existence problem. In particular, we obtain polynomial-time algorithms for contractual individual stability and contractual Nash stability, where the latter requires an upper bound of 2. We obtain further results for Nash stability and contractual individual stability, when the lower bound is at least 2.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12641v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Bullinger, Adam Dunajski, Edith Elkind, Matan Gilboa</dc:creator>
    </item>
    <item>
      <title>Structure-Aware Spectral Sparsification via Uniform Edge Sampling</title>
      <link>https://arxiv.org/abs/2510.12669</link>
      <description>arXiv:2510.12669v1 Announce Type: cross 
Abstract: Spectral clustering is a fundamental method for graph partitioning, but its reliance on eigenvector computation limits scalability to massive graphs. Classical sparsification methods preserve spectral properties by sampling edges proportionally to their effective resistances, but require expensive preprocessing to estimate these resistances. We study whether uniform edge sampling-a simple, structure-agnostic strategy-can suffice for spectral clustering. Our main result shows that for graphs admitting a well-separated $k$-clustering, characterized by a large structure ratio $\Upsilon(k) = \lambda_{k+1} / \rho_G(k)$, uniform sampling preserves the spectral subspace used for clustering. Specifically, we prove that uniformly sampling $O(\gamma^2 n \log n / \epsilon^2)$ edges, where $\gamma$ is the Laplacian condition number, yields a sparsifier whose top $(n-k)$-dimensional eigenspace is approximately orthogonal to the cluster indicators. This ensures that the spectral embedding remains faithful, and clustering quality is preserved. Our analysis introduces new resistance bounds for intra-cluster edges, a rank-$(n-k)$ effective resistance formulation, and a matrix Chernoff bound adapted to the dominant eigenspace. These tools allow us to bypass importance sampling entirely. Conceptually, our result connects recent coreset-based clustering theory to spectral sparsification, showing that under strong clusterability, even uniform sampling is structure-aware. This provides the first provable guarantee that uniform edge sampling suffices for structure-preserving spectral clustering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12669v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaiwen He, Petros Drineas, Rajiv Khanna</dc:creator>
    </item>
    <item>
      <title>Performance of Gaussian Boson Sampling on Planted Bipartite Clique Detection</title>
      <link>https://arxiv.org/abs/2510.12774</link>
      <description>arXiv:2510.12774v1 Announce Type: cross 
Abstract: We investigate whether Gaussian Boson Sampling (GBS) can provide a computational advantage for solving the planted biclique problem, which is a graph problem widely believed to be classically hard when the planted structure is small. Although GBS has been heuristically and experimentally observed to favor sampling dense subgraphs, its theoretical performance on this classically hard problem remains largely unexplored. We focus on a natural statistic derived from GBS output: the frequency with which a node appears in GBS samples, referred to as the node weight. We rigorously analyze whether this signal is strong enough to distinguish planted biclique nodes from background nodes. Our analysis characterizes the distribution of node weights under GBS and quantifies the bias introduced by the planted structure. The results reveal a sharp limitation: when the planted biclique size falls within the conjectured hard regime, the natural fluctuations in node weights dominate the bias signal, making detection unreliable using simple ranking strategies. These findings provide the first rigorous evidence that planted biclique detection may remain computationally hard even under GBS-based quantum computing, and they motivate further investigation into more advanced GBS-based algorithms or other quantum approaches for this problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12774v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yu-Zhen Janice Chen, Laurent Massouli\'e, Don Towsley</dc:creator>
    </item>
    <item>
      <title>Upper and Lower Bounds on the Smoothed Complexity of the Simplex Method</title>
      <link>https://arxiv.org/abs/2211.11860</link>
      <description>arXiv:2211.11860v3 Announce Type: replace 
Abstract: The simplex method for linear programming is known to be highly efficient in practice, and understanding its performance from a theoretical perspective is an active research topic. The framework of smoothed analysis, first introduced by Spielman and Teng (JACM '04) for this purpose, defines the smoothed complexity of solving a linear program with $d$ variables and $n$ constraints as the expected running time when Gaussian noise of variance $\sigma^2$ is added to the LP data. We prove that the smoothed complexity of the simplex method is $O(\sigma^{-3/2} d^{13/4}\log^{7/4} n)$, improving the dependence on $1/\sigma$ compared to the previous bound of $O(\sigma^{-2} d^2\sqrt{\log n})$. We accomplish this through a new analysis of the \emph{shadow bound}, key to earlier analyses as well. Illustrating the power of our new method, we use our method to prove a nearly tight upper bound on the smoothed complexity of two-dimensional polygons.
  We also establish the first non-trivial lower bound on the smoothed complexity of the simplex method, proving that the \emph{shadow vertex simplex method} requires at least $\Omega \Big(\min \big(\sigma^{-1/2} d^{-1/2}\log^{-1/4} d,2^d \big) \Big)$ pivot steps with high probability. A key part of our analysis is a new variation on the extended formulation for the regular $2^k$-gon. We end with a numerical experiment that suggests this analysis could be further improved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.11860v3</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.46298/theoretics.25.23</arxiv:DOI>
      <arxiv:journal_reference>TheoretiCS, Volume 4 (2025), Article 23, 1-56</arxiv:journal_reference>
      <dc:creator>Sophie Huiberts, Yin Tat Lee, Xinzhi Zhang</dc:creator>
    </item>
    <item>
      <title>Vizing's Theorem in Near-Linear Time</title>
      <link>https://arxiv.org/abs/2410.05240</link>
      <description>arXiv:2410.05240v3 Announce Type: replace 
Abstract: Vizing's theorem states that any $n$-vertex $m$-edge graph of maximum degree $\Delta$ can be edge colored using at most $\Delta + 1$ different colors [Vizing, 1964]. Vizing's original proof is algorithmic and shows that such an edge coloring can be found in $O(mn)$ time. This was subsequently improved to $\tilde O(m\sqrt{n})$ time, independently by [Arjomandi, 1982] and by [Gabow et al., 1985].
  Very recently, independently and concurrently, using randomization, this runtime bound was further improved to $\tilde{O}(n^2)$ by [Assadi, 2024] and $\tilde O(mn^{1/3})$ by [Bhattacharya, Carmon, Costa, Solomon and Zhang, 2024] (and subsequently to $\tilde O(mn^{1/4})$ time by [Bhattacharya, Costa, Solomon and Zhang, 2024]).
  In this paper, we present a randomized algorithm that computes a $(\Delta+1)$-edge coloring in near-linear time -- in fact, only $O(m\log{\Delta})$ time -- with high probability, giving a near-optimal algorithm for this fundamental problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05240v3</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sepehr Assadi, Soheil Behnezhad, Sayan Bhattacharya, Mart\'in Costa, Shay Solomon, Tianyi Zhang</dc:creator>
    </item>
    <item>
      <title>Solving the Correlation Cluster LP in Sublinear Time</title>
      <link>https://arxiv.org/abs/2503.20883</link>
      <description>arXiv:2503.20883v3 Announce Type: replace 
Abstract: Correlation Clustering is a fundamental and widely-studied problem in unsupervised learning and data mining. The input is a graph and the goal is to construct a clustering minimizing the number of inter-cluster edges plus the number of missing intra-cluster edges.
  CCL+24 introduced the cluster LP for Correlation Clustering, which they argued captures the problem much more succinctly than previous linear programming formulations. However, the cluster LP has exponential size, with a variable for every possible set of vertices in the input graph. Nevertheless, CCL+24 showed how to find a feasible solution for the cluster LP in time $O(n^{\text{poly}(1/\eps)})$ with objective value at most $(1+\epsilon)$ times the value of an optimal solution for the respective Correlation Clustering instance. Furthermore, they showed how to round a solution to the cluster LP, yielding a $(1.485+\eps)$-approximation algorithm for the Correlation Clustering problem.
  The main technical result of this paper is a new approach to find a feasible solution for the cluster LP with objective value at most $(1+\epsilon)$ of the optimum in time $\widetilde O(2^{\text{poly}(1/\eps)} n)$, where $n$ is the number of vertices in the graph. We also show how to implement the rounding within the same time bounds, thus achieving a fast $(1.485+\eps)$-approximation algorithm for the Correlation Clustering problem. This bridges the gap between state-of-the-art methods for approximating Correlation Clustering and the recent focus on fast algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20883v3</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3717823.3718181</arxiv:DOI>
      <dc:creator>Nairen Cao, Vincent Cohen-Addad, Shi Li, Euiwoong Lee, David Rasmussen Lolck, Alantha Newman, Mikkel Thorup, Lukas Vogl, Shuyi Yan, Hanwen Zhang</dc:creator>
    </item>
    <item>
      <title>Retroactive Monotonic Priority Queues via Range Searching</title>
      <link>https://arxiv.org/abs/2508.09892</link>
      <description>arXiv:2508.09892v2 Announce Type: replace 
Abstract: The best-known fully retroactive priority queue costs $O(\log^2 m \log \log m)$ time per operation, where $m$ is the number of operations performed on the data structure. In contrast, standard (non-retroactive) and partially retroactive priority queues can cost $O(\log m)$ time per operation. So far, it is unknown whether this $O(\log m)$ bound can be achieved for fully retroactive priority queues.
  In this work, we study a restricted variant of priority queues known as monotonic priority queues. First, we show that finding the minimum in a retroactive monotonic priority queue is a special case of the range-searching problem. Then, we design a fully retroactive monotonic priority queue with a cost of $O(\log m + T(m))$ time per operation, where $T(m)$ is the maximum between the query and the update time of a specific range-searching data structure with $m$ elements. Finally, we design a fully retroactive monotonic priority queue that costs $O(\log m \log \log m)$ time per operation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09892v2</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas Castro, Rosiane de Freitas</dc:creator>
    </item>
    <item>
      <title>A Generalization of von Neumann's Reduction from the Assignment Problem to Zero-Sum Games</title>
      <link>https://arxiv.org/abs/2410.10767</link>
      <description>arXiv:2410.10767v2 Announce Type: replace-cross 
Abstract: The equivalence between von Neumann's Minimax Theorem for zero-sum games and the LP Duality Theorem connects cornerstone problems of the two fields of game theory and optimization, respectively, and has been the subject of intense scrutiny for seven decades. Yet, as observed in this paper, the proof of the difficult direction of this equivalence is unsatisfactory: It does not assign distinct roles to the two players of the game, as is natural from the definition of a zero-sum game.
  In retrospect, a partial resolution to this predicament was provided in another brilliant paper of von Neumann, which reduced the assignment problem to zero-sum games. However, the underlying LP is highly specialized; all entries of its objective function vector are strictly positive, the constraint vector is all ones, and the constraint matrix is 0/1.
  We generalize von Neumann's result along two directions, each allowing negative entries in certain parts of the LP. Our reductions make explicit the roles of the two players of the reduced game, namely their maximin strategies are to play optimal solutions to the primal and dual LPs. Furthermore, unlike previous reductions, the value of the reduced game reveals the value of the given LP. Our generalizations encompass several basic economic scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10767v2</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>econ.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilan Adler, Martin Bullinger, Vijay V. Vazirani</dc:creator>
    </item>
    <item>
      <title>A Customized SAT-based Solver for Graph Coloring</title>
      <link>https://arxiv.org/abs/2504.04821</link>
      <description>arXiv:2504.04821v2 Announce Type: replace-cross 
Abstract: We introduce ZykovColor, a novel SAT-based algorithm to solve the graph coloring problem working on top of an encoding that mimics the Zykov tree. Our method is based on an approach of H\'ebrard and Katsirelos (2020) that employs a propagator to enforce transitivity constraints, incorporate lower bounds for search tree pruning, and enable inferred propagations.
  We leverage the recently introduced IPASIR-UP interface for CaDiCaL to implement these techniques with a SAT solver. Furthermore, we propose new features that take advantage of the underlying SAT solver. These include modifying the integrated decision strategy with vertex domination hints and using incremental bottom-up search that allows to reuse learned clauses from previous calls. Additionally, we integrate a more effective clique computation and an algorithm for computing the fractional chromatic number to improve the lower bounds used for pruning during the search.
  We validate the effectiveness of each new feature through an experimental analysis. ZykovColor outperforms other state-of-the-art graph coloring implementations on the DIMACS benchmark set. Further experiments on random Erd\H{o}s-R\'enyi graphs show that our new approach matches or outperforms state-of-the-art SAT-based methods for both very sparse and highly dense graphs. We give an additional configuration of ZykovColor that dominates other SAT-based methods on the Erd\H{o}s-R\'enyi graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04821v2</guid>
      <category>cs.DM</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>cs.LO</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timo Brand, Daniel Faber, Stephan Held, Petra Mutzel</dc:creator>
    </item>
  </channel>
</rss>
