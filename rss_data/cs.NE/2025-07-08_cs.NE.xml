<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Jul 2025 04:01:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Large Language Model-Driven Surrogate-Assisted Evolutionary Algorithm for Expensive Optimization</title>
      <link>https://arxiv.org/abs/2507.02892</link>
      <description>arXiv:2507.02892v1 Announce Type: new 
Abstract: Surrogate-assisted evolutionary algorithms (SAEAs) are a key tool for addressing costly optimization tasks, with their efficiency being heavily dependent on the selection of surrogate models and infill sampling criteria. However, designing an effective dynamic selection strategy for SAEAs is labor-intensive and requires substantial domain knowledge. To address this challenge, this paper proposes LLM-SAEA, a novel approach that integrates large language models (LLMs) to configure both surrogate models and infill sampling criteria online. Specifically, LLM-SAEA develops a collaboration-of-experts framework, where one LLM serves as a scoring expert (LLM-SE), assigning scores to surrogate models and infill sampling criteria based on their optimization performance, while another LLM acts as a decision expert (LLM-DE), selecting the appropriate configurations by analyzing their scores along with the current optimization state. Experimental results demonstrate that LLM-SAEA outperforms several state-of-the-art algorithms across standard test cases. The source code is publicly available at https://github.com/ForrestXie9/LLM-SAEA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02892v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lindong Xie, Genghui Li, Zhenkun Wang, Edward Chung, Maoguo Gong</dc:creator>
    </item>
    <item>
      <title>Particle Swarm Optimization for Quantum Circuit Synthesis: Performance Analysis and Insights</title>
      <link>https://arxiv.org/abs/2507.02898</link>
      <description>arXiv:2507.02898v1 Announce Type: new 
Abstract: This paper discusses how particle swarm optimization (PSO) can be used to generate quantum circuits to solve an instance of the MaxOne problem. It then analyzes previous studies on evolutionary algorithms for circuit synthesis. With a brief introduction to PSO, including its parameters and algorithm flow, the paper focuses on a method of quantum circuit encoding and representation as PSO parameters. The fitness evaluation used in this paper is the MaxOne problem. The paper presents experimental results that compare different learning abilities and inertia weight variations in the PSO algorithm. A comparison is further made between the PSO algorithm and a genetic algorithm for quantum circuit synthesis. The results suggest PSO converges more quickly to the optimal solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02898v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mirza Hizriyan Nubli Hidayat, Tan Chye Cheah</dc:creator>
    </item>
    <item>
      <title>Online Continual Learning via Spiking Neural Networks with Sleep Enhanced Latent Replay</title>
      <link>https://arxiv.org/abs/2507.02901</link>
      <description>arXiv:2507.02901v1 Announce Type: new 
Abstract: Edge computing scenarios necessitate the development of hardware-efficient online continual learning algorithms to be adaptive to dynamic environment. However, existing algorithms always suffer from high memory overhead and bias towards recently trained tasks. To tackle these issues, this paper proposes a novel online continual learning approach termed as SESLR, which incorporates a sleep enhanced latent replay scheme with spiking neural networks (SNNs). SESLR leverages SNNs' binary spike characteristics to store replay features in single bits, significantly reducing memory overhead. Furthermore, inspired by biological sleep-wake cycles, SESLR introduces a noise-enhanced sleep phase where the model exclusively trains on replay samples with controlled noise injection, effectively mitigating classification bias towards new classes. Extensive experiments on both conventional (MNIST, CIFAR10) and neuromorphic (NMNIST, CIFAR10-DVS) datasets demonstrate SESLR's effectiveness. On Split CIFAR10, SESLR achieves nearly 30% improvement in average accuracy with only one-third of the memory consumption compared to baseline methods. On Split CIFAR10-DVS, it improves accuracy by approximately 10% while reducing memory overhead by a factor of 32. These results validate SESLR as a promising solution for online continual learning in resource-constrained edge computing scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02901v1</guid>
      <category>cs.NE</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erliang Lin, Wenbin Luo, Wei Jia, Yu Chen, Shaofu Yang</dc:creator>
    </item>
    <item>
      <title>Experiment on creating a neural network with weights determined by the potential of a simulated electrostatic field</title>
      <link>https://arxiv.org/abs/2507.02933</link>
      <description>arXiv:2507.02933v1 Announce Type: new 
Abstract: This paper explores the possibility of determining the weights and thresholds of a neural network using the potential -- a parameter of an electrostatic field -- without analytical calculations and without applying training algorithms. The work is based on neural network architectures employing metric recognition methods. The electrostatic field is simulated in the Builder C++ environment. In the same environment, a neural network based on metric recognition methods is constructed, with the weights of the first-layer neurons determined by the values of the potentials of the simulated electrostatic field. The effectiveness of the resulting neural network within the simulated system is evaluated using the MNIST test dataset under various initial conditions of the simulated system. The results demonstrated functional viability. The implementation of this approach shows that a neural network can obtain weight values almost instantaneously from the electrostatic field, without the need for analytical computations, lengthy training procedures, or massive training datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02933v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3103/S0147688222050161</arxiv:DOI>
      <arxiv:journal_reference>Scientific and Technical Information Processing 49 (2022)</arxiv:journal_reference>
      <dc:creator>Geidarov Polad</dc:creator>
    </item>
    <item>
      <title>SPEAR: Structured Pruning for Spiking Neural Networks via Synaptic Operation Estimation and Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2507.02945</link>
      <description>arXiv:2507.02945v1 Announce Type: new 
Abstract: While deep spiking neural networks (SNNs) demonstrate superior performance, their deployment on resource-constrained neuromorphic hardware still remains challenging. Network pruning offers a viable solution by reducing both parameters and synaptic operations (SynOps) to facilitate the edge deployment of SNNs, among which search-based pruning methods search for the SNNs structure after pruning. However, existing search-based methods fail to directly use SynOps as the constraint because it will dynamically change in the searching process, resulting in the final searched network violating the expected SynOps target. In this paper, we introduce a novel SNN pruning framework called SPEAR, which leverages reinforcement learning (RL) technique to directly use SynOps as the searching constraint. To avoid the violation of SynOps requirements, we first propose a SynOps prediction mechanism called LRE to accurately predict the final SynOps after search. Observing SynOps cannot be explicitly calculated and added to constrain the action in RL, we propose a novel reward called TAR to stabilize the searching. Extensive experiments show that our SPEAR framework can effectively compress SNN under specific SynOps constraint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02945v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hui Xie, Yuhe Liu, Shaoqi Yang, Jinyang Guo, Yufei Guo, Yuqing Ma, Jiaxin Chen, Jiaheng Liu, Xianglong Liu</dc:creator>
    </item>
    <item>
      <title>Strategies for Resource Allocation of Two Competing Companies using Genetic Algorithm</title>
      <link>https://arxiv.org/abs/2507.02952</link>
      <description>arXiv:2507.02952v1 Announce Type: new 
Abstract: We investigate various strategic locations of shops in shopping malls in a metropolis with the aim of finding the best strategy for final dominance of market share by a company in a competing environment. The problem is posed in the context of two competing supermarket chains in a metropolis, described in the framework of the two-dimensional Ising model. Evolutionary Algorithm is used to encode the ensemble of initial configurations and Monte Carlo method is used to evolve the pattern. Numerical simulation indicates that initial patterns with certain topological properties do evolve faster to market dominance. The description of these topological properties is given and suggestions are made on the initial pattern so as to evolve faster to market dominance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02952v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wing Keung Cheung, Kwok Yip Szeto</dc:creator>
    </item>
    <item>
      <title>Optimization of Low-Latency Spiking Neural Networks Utilizing Historical Dynamics of Refractory Periods</title>
      <link>https://arxiv.org/abs/2507.02960</link>
      <description>arXiv:2507.02960v1 Announce Type: new 
Abstract: The refractory period controls neuron spike firing rate, crucial for network stability and noise resistance. With advancements in spiking neural network (SNN) training methods, low-latency SNN applications have expanded. In low-latency SNNs, shorter simulation steps render traditional refractory mechanisms, which rely on empirical distributions or spike firing rates, less effective. However, omitting the refractory period amplifies the risk of neuron over-activation and reduces the system's robustness to noise. To address this challenge, we propose a historical dynamic refractory period (HDRP) model that leverages membrane potential derivative with historical refractory periods to estimate an initial refractory period and dynamically adjust its duration. Additionally, we propose a threshold-dependent refractory kernel to mitigate excessive neuron state accumulation. Our approach retains the binary characteristics of SNNs while enhancing both noise resistance and overall performance. Experimental results show that HDRP-SNN significantly reduces redundant spikes compared to traditional SNNs, and achieves state-of-the-art (SOTA) accuracy both on static datasets and neuromorphic datasets. Moreover, HDRP-SNN outperforms artificial neural networks (ANNs) and traditional SNNs in noise resistance, highlighting the crucial role of the HDRP mechanism in enhancing the performance of low-latency SNNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02960v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Liying Tao, Zonglin Yang, Delong Shang</dc:creator>
    </item>
    <item>
      <title>A Novel Hybrid Grey Wolf Differential Evolution Algorithm</title>
      <link>https://arxiv.org/abs/2507.03022</link>
      <description>arXiv:2507.03022v1 Announce Type: new 
Abstract: Grey wolf optimizer (GWO) is a nature-inspired stochastic meta-heuristic of the swarm intelligence field that mimics the hunting behavior of grey wolves. Differential evolution (DE) is a popular stochastic algorithm of the evolutionary computation field that is well suited for global optimization. In this part, we introduce a new algorithm based on the hybridization of GWO and two DE variants, namely the GWO-DE algorithm. We evaluate the new algorithm by applying various numerical benchmark functions. The numerical results of the comparative study are quite satisfactory in terms of performance and solution quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03022v1</guid>
      <category>cs.NE</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>physics.app-ph</category>
      <category>physics.comp-ph</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ioannis D. Bougas, Pavlos Doanis, Maria S. Papadopoulou, Achilles D. Boursianis, Sotirios P. Sotiroudis, Zaharias D. Zaharis, George Koudouridis, Panagiotis Sarigiannidis, Mohammad Abdul Matint, George Karagiannidis, Sotirios K. Goudos</dc:creator>
    </item>
    <item>
      <title>Behaviour Space Analysis of LLM-driven Meta-heuristic Discovery</title>
      <link>https://arxiv.org/abs/2507.03605</link>
      <description>arXiv:2507.03605v1 Announce Type: new 
Abstract: We investigate the behaviour space of meta-heuristic optimisation algorithms automatically generated by Large Language Model driven algorithm discovery methods. Using the Large Language Evolutionary Algorithm (LLaMEA) framework with a GPT o4-mini LLM, we iteratively evolve black-box optimisation heuristics, evaluated on 10 functions from the BBOB benchmark suite. Six LLaMEA variants, featuring different mutation prompt strategies, are compared and analysed. We log dynamic behavioural metrics including exploration, exploitation, convergence and stagnation measures, for each run, and analyse these via visual projections and network-based representations. Our analysis combines behaviour-based
  projections, Code Evolution Graphs built from static code features, performance convergence curves, and behaviour-based Search Trajectory Networks. The results reveal clear differences in search dynamics and algorithm structures across LLaMEA configurations. Notably, the variant that employs both a code simplification prompt and a random perturbation prompt in a 1+1 elitist evolution strategy, achieved the best performance, with the highest Area Over the Convergence Curve. Behaviour-space visualisations show that higher-performing algorithms exhibit more intensive exploitation behaviour and faster convergence with less stagnation. Our findings demonstrate how behaviour-space analysis can explain why certain LLM-designed heuristics outperform others and how LLM-driven algorithm discovery navigates the open-ended and complex search space of algorithms. These findings provide insights to guide the future design of adaptive LLM-driven algorithm generators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03605v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niki van Stein, Haoran Yin, Anna V. Kononova, Thomas B\"ack, Gabriela Ochoa</dc:creator>
    </item>
    <item>
      <title>A First Runtime Analysis of the PAES-25: An Enhanced Variant of the Pareto Archived Evolution Strategy</title>
      <link>https://arxiv.org/abs/2507.03666</link>
      <description>arXiv:2507.03666v1 Announce Type: new 
Abstract: This paper presents a first mathematical runtime analysis of PAES-25, an enhanced version of the original Pareto Archived Evolution Strategy (PAES) coming from the study of telecommunication problems over two decades ago to understand the dynamics of local search of MOEAs on many-objective fitness landscapes. We derive tight expected runtime bounds of PAES-25 with one-bit mutation on $m$-LOTZ until the entire Pareto front is found: $\Theta(n^3)$ iterations if $m=2$, $\Theta(n^3 \log^2(n))$ iterations if $m=4$ and $\Theta(n(2n/m)^{m/2} \log(n/m))$ iterations if $m&gt;4$ where $n$ is the problem size and $m$ the number of objectives. To the best of our knowledge, these are the first known tight runtime bounds for an MOEA outperforming the best known upper bound of $O(n^{m+1})$ for (G)SEMO on $m$-LOTZ when $m$ is at least $4$. We also show that archivers, such as the Adaptive Grid Archiver (AGA), Hypervolume Archiver (HVA) or Multi-Level Grid Archiver (MGA), help to distribute the set of solutions across the Pareto front of $m$-LOTZ efficiently. We also show that PAES-25 with standard bit mutation optimizes the bi-objective LOTZ benchmark in expected $O(n^4)$ iterations, and we discuss its limitations on other benchmarks such as OMM or COCZ.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03666v1</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Andre Opris</dc:creator>
    </item>
    <item>
      <title>A Better Multi-Objective GP-GOMEA -- But do we Need it?</title>
      <link>https://arxiv.org/abs/2507.03777</link>
      <description>arXiv:2507.03777v1 Announce Type: new 
Abstract: In Symbolic Regression (SR), achieving a proper balance between accuracy and interpretability remains a key challenge. The Genetic Programming variant of the Gene-pool Optimal Mixing Evolutionary Algorithm (GP-GOMEA) is of particular interest as it achieves state-of-the-art performance using a template that limits the size of expressions. A recently introduced expansion, modular GP-GOMEA, is capable of decomposing expressions using multiple subexpressions, further increasing chances of interpretability. However, modular GP-GOMEA may create larger expressions, increasing the need to balance size and accuracy. A multi-objective variant of GP-GOMEA exists, which can be used, for instance, to optimize for size and accuracy simultaneously, discovering their trade-off. However, even with enhancements that we propose in this paper to improve the performance of multi-objective modular GP-GOMEA, when optimizing for size and accuracy, the single-objective version in which a multi-objective archive is used only for logging, still consistently finds a better average hypervolume. We consequently analyze when a single-objective approach should be preferred. Additionally, we explore an objective that stimulates re-use in multi-objective modular GP-GOMEA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03777v1</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joe Harrison, Tanja Alderliesten. Peter A. N. Bosman</dc:creator>
    </item>
    <item>
      <title>A Non-Dominated Sorting Evolutionary Algorithm Updating When Required</title>
      <link>https://arxiv.org/abs/2507.03864</link>
      <description>arXiv:2507.03864v1 Announce Type: new 
Abstract: The NSGA-III algorithm relies on uniformly distributed reference points to promote diversity in many-objective optimization problems. However, this strategy may underperform when facing irregular Pareto fronts, where certain vectors remain unassociated with any optimal solutions. While adaptive schemes such as A-NSGA-III address this issue by dynamically modifying reference points, they may introduce unnecessary complexity in regular scenarios. This paper proposes NSGA-III with Update when Required (NSGA-III-UR), a hybrid algorithm that selectively activates reference vector adaptation based on the estimated regularity of the Pareto front. Experimental results on benchmark suites (DTLZ1-7, IDTLZ1-2) and real-world problems demonstrate that NSGA-III-UR consistently outperforms NSGA-III and A-NSGA-III across diverse problem landscapes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03864v1</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas R. C. Farias, Abimael J. F. Santos, Matheus R. B. Nobre</dc:creator>
    </item>
    <item>
      <title>Bridging Expressivity and Scalability with Adaptive Unitary SSMs</title>
      <link>https://arxiv.org/abs/2507.05238</link>
      <description>arXiv:2507.05238v1 Announce Type: new 
Abstract: Recent work has revealed that state space models (SSMs), while efficient for long-sequence processing, are fundamentally limited in their ability to represent formal languages particularly due to time-invariant and real-valued recurrence structures. In this work, we draw inspiration from adaptive and structured dynamics observed in biological neural systems and introduce the Adaptive Unitary State Space Model (AUSSM)- a novel class of SSMs that leverages skew-symmetric, input-dependent recurrence to achieve unitary evolution and high expressive power. Using algebraic automata theory, we prove that AUSSM can perform modulo counting and simulate solvable group automata at finite precision, enabling SSMs to model a broad class of regular languages that are out of reach for other SSM architectures. To overcome the practical inefficiencies of adaptive recurrence, we develop a separable convolution formulation and a CUDA implementation that enables scalable parallel training. Empirically, we show that AUSSM when interleaved with Mamba outperform prior SSMs on formal algorithmic tasks such as parity and modular arithmetic, and achieve competent performance on real-world long time-series classification benchmarks. Our results demonstrate that adaptive unitary recurrence provides a powerful and efficient inductive bias for both symbolic and continuous sequence modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05238v1</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Arjun Karuvally, Franz Nowak, Anderson T. Keller, Carmen Amo Alonso, Terrence J. Sejnowski, Hava T. Siegelmann</dc:creator>
    </item>
    <item>
      <title>Evolution, Future of AI, and Singularity</title>
      <link>https://arxiv.org/abs/2507.02876</link>
      <description>arXiv:2507.02876v1 Announce Type: cross 
Abstract: This article critically examines the foundational principles of contemporary AI methods, exploring the limitations that hinder its potential. We draw parallels between the modern AI landscape and the 20th-century Modern Synthesis in evolutionary biology, and highlight how advancements in evolutionary theory that augmented the Modern Synthesis, particularly those of Evolutionary Developmental Biology, offer insights that can inform a new design paradigm for AI. By synthesizing findings across AI and evolutionary theory, we propose a pathway to overcome existing limitations, enabling AI to achieve its aspirational goals. We also examine how this perspective transforms the idea of an AI-driven technological singularity from speculative futurism into a grounded prospect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02876v1</guid>
      <category>cs.OH</category>
      <category>cs.NE</category>
      <category>nlin.AO</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zeki Doruk Erden</dc:creator>
    </item>
    <item>
      <title>A Census-Based Genetic Algorithm for Target Set Selection Problem in Social Networks</title>
      <link>https://arxiv.org/abs/2410.02011</link>
      <description>arXiv:2410.02011v2 Announce Type: replace 
Abstract: This paper considers the Target Set Selection (TSS) Problem in social networks, a fundamental problem in viral marketing. In the TSS problem, a graph and a threshold value for each vertex of the graph are given. We need to find a minimum size vertex subset to "activate" such that all graph vertices are activated at the end of the propagation process. Specifically, we propose a novel approach called "a census-based genetic algorithm" for the TSS problem. In our algorithm, we use the idea of a census to gather and store information about each individual in a population and collect census data from the individuals constructed during the algorithm's execution so that we can achieve greater diversity and avoid premature convergence at locally optimal solutions. We use two distinct census information: (a) for each individual, the algorithm stores how many times it has been identified during the execution (b) for each network node, the algorithm counts how many times it has been included in a solution. The proposed algorithm can also self-adjust by using a parameter specifying the aggressiveness employed in each reproduction method. Additionally, the algorithm is designed to run in a parallelized environment to minimize the computational cost and check each individual's feasibility. Moreover, our algorithm finds the optimal solution in all cases while experimenting on random graphs. Furthermore, we execute the proposed algorithm on 14 large graphs of real-life social network instances from the literature, improving around 9.57 solution size (on average) and 134 vertices (in total) compared to the best solutions obtained in previous studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02011v2</guid>
      <category>cs.NE</category>
      <category>cs.SI</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md. Samiur Rahman, Mohammad Shamim Ahsan, Cheng-Wu Chen, Vijayakumar Varadarajan</dc:creator>
    </item>
    <item>
      <title>An approach to the timetabling problem in deregulated railway markets based on metaheuristic algorithms</title>
      <link>https://arxiv.org/abs/2504.17455</link>
      <description>arXiv:2504.17455v2 Announce Type: replace 
Abstract: The train timetabling problem in liberalized railway markets represents a challenge to the coordination between infrastructure managers and railway undertakings. Efficient scheduling is critical to maximizing infrastructure capacity and utilization while adhering as closely as possible to the requests of railway undertakings. These objectives ultimately contribute to maximizing the infrastructure manager's revenues. This paper sets out a modular simulation framework to reproduce the dynamics of deregulated railway systems. Ten metaheuristic algorithms using the MEALPY Python library are then evaluated in order to optimize train schedules in the liberalized Spanish railway market. In addition, an analysis of the scalability of the problem has been carried out by comparing the results with those obtained with a classical mathematical model such as SCIP in Pyomo. The results show that the Genetic Algorithm outperforms others in revenue optimization, convergence speed, and schedule adherence. Alternatives, such as Particle Swarm Optimization and Ant Colony Optimization Continuous, show slower convergence and higher variability. The results emphasize the trade-off between scheduling more trains and adhering to requested times, providing insights into solving complex scheduling problems in deregulated railway systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17455v2</guid>
      <category>cs.NE</category>
      <category>cs.CE</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Mu\~noz-Valero, Juan Moreno-Garcia, Julio Alberto L\'opez-G\'omez, Enrique Adrian Villarrubia-Martin, Luis Rodriguez-Benitez</dc:creator>
    </item>
    <item>
      <title>Phase codes emerge in recurrent neural networks optimized for modular arithmetic</title>
      <link>https://arxiv.org/abs/2310.07908</link>
      <description>arXiv:2310.07908v2 Announce Type: replace-cross 
Abstract: Recurrent neural networks (RNNs) can implement complex computations by leveraging a range of dynamics, such as oscillations, attractors, and transient trajectories. A growing body of work has highlighted the emergence of phase codes, a type of oscillatory activity where information is encoded in the relative phase of network activity, in RNNs trained for working memory tasks. However, these studies rely on architectural constraints or regularization schemes that explicitly promote oscillatory solutions. Here, we investigate whether phase coding can emerge purely from task optimization by training continuous-time RNNs to perform a simple modular arithmetic task without oscillatory-promoting biases. We find that in the absence of such biases, RNNs can learn phase code solutions. Surprisingly, we also uncover a rich diversity of alternative solutions that solve our modular arithmetic task via qualitatively distinct dynamics and dynamical mechanisms. We map the solution space for our task and show that the phase code solution occupies a distinct region. These results suggest that phase coding can be a natural but not inevitable outcome of training RNNs on modular arithmetic, and highlight the diversity of solutions RNNs can learn to solve simple tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07908v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keith T. Murray</dc:creator>
    </item>
    <item>
      <title>Transformers Can Solve Non-Linear and Non-Markovian Filtering Problems in Continuous Time For Conditionally Gaussian Signals</title>
      <link>https://arxiv.org/abs/2310.19603</link>
      <description>arXiv:2310.19603v3 Announce Type: replace-cross 
Abstract: The use of attention-based deep learning models in stochastic filtering, e.g.\ transformers and deep Kalman filters, has recently come into focus; however, the potential for these models to solve stochastic filtering problems remains largely unknown. The paper provides an affirmative answer to this open problem in the theoretical foundations of machine learning by showing that a class of continuous-time transformer models, called \textit{filterformers}, can approximately implement the conditional law of a broad class of non-Markovian and conditionally Gaussian signal processes given noisy continuous-time (possibly non-Gaussian) measurements. Our approximation guarantees hold uniformly over sufficiently regular compact subsets of continuous-time paths, where the worst-case 2-Wasserstein distance between the true optimal filter and our deep learning model quantifies the approximation error. Our construction relies on two new customizations of the standard attention mechanism: The first can losslessly adapt to the characteristics of a broad range of paths since we show that the attention mechanism implements bi-Lipschitz embeddings of sufficiently regular sets of paths into low-dimensional Euclidean spaces; thus, it incurs no ``dimension reduction error''. The latter attention mechanism is tailored to the geometry of Gaussian measures in the $2$-Wasserstein space. Our analysis relies on new stability estimates of robust optimal filters in the conditionally Gaussian setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.19603v3</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>cs.NE</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Blanka Horvath, Anastasis Kratsios, Yannick Limmer, Xuwei Yang</dc:creator>
    </item>
    <item>
      <title>PIP-Net: Pedestrian Intention Prediction in the Wild</title>
      <link>https://arxiv.org/abs/2402.12810</link>
      <description>arXiv:2402.12810v3 Announce Type: replace-cross 
Abstract: Accurate pedestrian intention prediction (PIP) by Autonomous Vehicles (AVs) is one of the current research challenges in this field. In this article, we introduce PIP-Net, a novel framework designed to predict pedestrian crossing intentions by AVs in real-world urban scenarios. We offer two variants of PIP-Net designed for different camera mounts and setups. Leveraging both kinematic data and spatial features from the driving scene, the proposed model employs a recurrent and temporal attention-based solution, outperforming state-of-the-art performance. To enhance the visual representation of road users and their proximity to the ego vehicle, we introduce a categorical depth feature map, combined with a local motion flow feature, providing rich insights into the scene dynamics. Additionally, we explore the impact of expanding the camera's field of view, from one to three cameras surrounding the ego vehicle, leading to an enhancement in the model's contextual perception. Depending on the traffic scenario and road environment, the model excels in predicting pedestrian crossing intentions up to 4 seconds in advance, which is a breakthrough in current research studies in pedestrian intention prediction. Finally, for the first time, we present the Urban-PIP dataset, a customised pedestrian intention prediction dataset, with multi-camera annotations in real-world automated driving scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12810v3</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <category>eess.IV</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TITS.2025.3570794</arxiv:DOI>
      <arxiv:journal_reference>in IEEE Transactions on Intelligent Transportation Systems, vol. 26, no. 7, pp. 9824-9837, July 2025</arxiv:journal_reference>
      <dc:creator>Mohsen Azarmi, Mahdi Rezaei, He Wang</dc:creator>
    </item>
    <item>
      <title>How more data can hurt: Instability and regularization in next-generation reservoir computing</title>
      <link>https://arxiv.org/abs/2407.08641</link>
      <description>arXiv:2407.08641v3 Announce Type: replace-cross 
Abstract: It has been found recently that more data can, counter-intuitively, hurt the performance of deep neural networks. Here, we show that a more extreme version of the phenomenon occurs in data-driven models of dynamical systems. To elucidate the underlying mechanism, we focus on next-generation reservoir computing (NGRC) -- a popular framework for learning dynamics from data. We find that, despite learning a better representation of the flow map with more training data, NGRC can adopt an ill-conditioned ``integrator'' and lose stability. We link this data-induced instability to the auxiliary dimensions created by the delayed states in NGRC. Based on these findings, we propose simple strategies to mitigate the instability, either by increasing regularization strength in tandem with data size, or by carefully introducing noise during training. Our results highlight the importance of proper regularization in data-driven modeling of dynamical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08641v3</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>math.DS</category>
      <category>nlin.AO</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1063/5.0262977</arxiv:DOI>
      <arxiv:journal_reference>Chaos 35, 073102 (2025)</arxiv:journal_reference>
      <dc:creator>Yuanzhao Zhang, Edmilson Roque dos Santos, Huixin Zhang, Sean P. Cornelius</dc:creator>
    </item>
    <item>
      <title>Universal approximation results for neural networks with non-polynomial activation function over non-compact domains</title>
      <link>https://arxiv.org/abs/2410.14759</link>
      <description>arXiv:2410.14759v4 Announce Type: replace-cross 
Abstract: This paper extends the universal approximation property of single-hidden-layer feedforward neural networks beyond compact domains, which is of particular interest for the approximation within weighted $C^k$-spaces and weighted Sobolev spaces over unbounded domains. More precisely, by assuming that the activation function is non-polynomial, we establish universal approximation results within function spaces defined over non-compact subsets of a Euclidean space, including $L^p$-spaces, weighted $C^k$-spaces, and weighted Sobolev spaces, where the latter two include the approximation of the (weak) derivatives. Moreover, we provide some dimension-independent rates for approximating a function with sufficiently regular and integrable Fourier transform by neural networks with non-polynomial activation function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14759v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>math.CA</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ariel Neufeld, Philipp Schmocker</dc:creator>
    </item>
    <item>
      <title>CODE-CL: Conceptor-Based Gradient Projection for Deep Continual Learning</title>
      <link>https://arxiv.org/abs/2411.15235</link>
      <description>arXiv:2411.15235v3 Announce Type: replace-cross 
Abstract: Continual learning (CL) - the ability to progressively acquire and integrate new concepts - is essential to intelligent systems to adapt to dynamic environments. However, deep neural networks struggle with catastrophic forgetting (CF) when learning tasks sequentially, as training for new tasks often overwrites previously learned knowledge. To address this, recent approaches constrain updates to orthogonal subspaces using gradient projection, effectively preserving important gradient directions for previous tasks. While effective in reducing forgetting, these approaches inadvertently hinder forward knowledge transfer (FWT), particularly when tasks are highly correlated. In this work, we propose Conceptor-based gradient projection for Deep Continual Learning (CODE-CL), a novel method that leverages conceptor matrix representations, a form of regularized reconstruction, to adaptively handle highly correlated tasks. CODE-CL mitigates CF by projecting gradients onto pseudo-orthogonal subspaces of previous task feature spaces while simultaneously promoting FWT. It achieves this by learning a linear combination of shared basis directions, allowing efficient balance between stability and plasticity and transfer of knowledge between overlapping input feature representations. Extensive experiments on continual learning benchmarks validate CODE-CL's efficacy, demonstrating superior performance, reduced forgetting, and improved FWT as compared to state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15235v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.NE</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Paul E. Apolinario, Sakshi Choudhary, Kaushik Roy</dc:creator>
    </item>
    <item>
      <title>Role of scrambling and noise in temporal information processing with quantum systems</title>
      <link>https://arxiv.org/abs/2505.10080</link>
      <description>arXiv:2505.10080v2 Announce Type: replace-cross 
Abstract: Scrambling quantum systems have attracted attention as effective substrates for temporal information processing. Here we consider a quantum reservoir processing framework that captures a broad range of physical computing models with quantum systems. We examine the scalability and memory retention of the model with scrambling reservoirs modelled by high-order unitary designs in both noiseless and noisy settings. In the former regime, we show that measurement readouts become exponentially concentrated with increasing reservoir size, yet strikingly do not worsen with the reservoir iterations. Thus, while repeatedly reusing a small scrambling reservoir with quantum data might be viable, scaling up the problem size deteriorates generalization unless one can afford an exponential shot overhead. In contrast, the memory of early inputs and initial states decays exponentially in both reservoir size and reservoir iterations. In the noisy regime, we also prove that memory decays exponentially in time for local noisy channels. These results required us to introduce new proof techniques for bounding concentration in temporal quantum models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10080v2</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weijie Xiong, Zo\"e Holmes, Armando Angrisani, Yudai Suzuki, Thiparat Chotibut, Supanut Thanasilp</dc:creator>
    </item>
    <item>
      <title>GLU Attention Improve Transformer</title>
      <link>https://arxiv.org/abs/2507.00022</link>
      <description>arXiv:2507.00022v2 Announce Type: replace-cross 
Abstract: Gated Linear Units (GLU) have shown great potential in enhancing neural network performance. In this paper, I introduce a novel attention mechanism called GLU Attention, which introduces nonlinearity into the values of Attention. My experiments demonstrate that GLU Attention improves both model performance and convergence speed across text and vision modalities with zero additional parameters and negligible computational costs. GLU Attention is lightweight and can seamlessly integrate with other technologies, such as Flash Attention, Rotary Position Embedding (RoPE), and various Multi-Head Attention (MHA) variants such as Grouped-Query Attention (GQA). This project is open-sourced at github.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00022v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.NE</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Zehao Wang</dc:creator>
    </item>
    <item>
      <title>Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2507.02211</link>
      <description>arXiv:2507.02211v2 Announce Type: replace-cross 
Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement learning have shown that static agents can learn to cooperate through a diverse sort of mechanisms, including noise injection, different types of learning algorithms and neighbours' payoff knowledge. In this work, using an independent multi-agent Q-learning algorithm, we study the effects of dilution and mobility in the spatial version of the prisoner's dilemma. Within this setting, different possible actions for the algorithm are defined, connecting with previous results on the classical, non-reinforcement learning spatial prisoner's dilemma, showcasing the versatility of the algorithm in modeling different game-theoretical scenarios and the benchmarking potential of this approach. As a result, a range of effects is observed, including evidence that games with fixed update rules can be qualitatively equivalent to those with learned ones, as well as the emergence of a symbiotic mutualistic effect between populations that forms when multiple actions are defined.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02211v2</guid>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <category>physics.comp-ph</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gustavo C. Mangold, Heitor C. M. Fernandes, Mendeli H. Vainstein</dc:creator>
    </item>
  </channel>
</rss>
