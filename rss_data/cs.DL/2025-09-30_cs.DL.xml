<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 01 Oct 2025 02:07:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Overview of SCIDOCA 2025 Shared Task on Citation Prediction, Discovery, and Placement</title>
      <link>https://arxiv.org/abs/2509.24283</link>
      <description>arXiv:2509.24283v1 Announce Type: new 
Abstract: We present an overview of the SCIDOCA 2025 Shared Task, which focuses on citation discovery and prediction in scientific documents. The task is divided into three subtasks: (1) Citation Discovery, where systems must identify relevant references for a given paragraph; (2) Masked Citation Prediction, which requires selecting the correct citation for masked citation slots; and (3) Citation Sentence Prediction, where systems must determine the correct reference for each cited sentence. We release a large-scale dataset constructed from the Semantic Scholar Open Research Corpus (S2ORC), containing over 60,000 annotated paragraphs and a curated reference set. The test set consists of 1,000 paragraphs from distinct papers, each annotated with ground-truth citations and distractor candidates. A total of seven teams registered, with three submitting results. We report performance metrics across all subtasks and analyze the effectiveness of submitted systems. This shared task provides a new benchmark for evaluating citation modeling and encourages future research in scientific document understanding. The dataset and task materials are publicly available at https://github.com/daotuanan/scidoca2025-shared-task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24283v1</guid>
      <category>cs.DL</category>
      <category>cs.CL</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>An Dao, Vu Tran, Le-Minh Nguyen, Yuji Matsumoto</dc:creator>
    </item>
    <item>
      <title>The Landscape of problematic papers in the field of non-coding RNA</title>
      <link>https://arxiv.org/abs/2509.24511</link>
      <description>arXiv:2509.24511v1 Announce Type: new 
Abstract: In recent years, the surge in retractions has been accompanied by numerous papers receiving comments that raise concerns about their reliability. The prevalence of problematic papers undermines the reliability of scientific research and threatens the foundation of evidence-based medicine. In this study,we focus on the field of non-coding RNA(ncRNA) as a case study to explore the typical characteristics of problematic papers from various perspectives, aiming to provide insights for addressing large-scale fraudulent publications. Research on under-investigated ncRNAs is more likely to yield problematic papers. These problematic papers often exhibit significant textual similarity, and many others sharing this similarity also display suspicious instances of image duplication. Healthcare institutions are particularly prone to publishing problematic papers, especially those with a low publication volume. Most problematic papers are found in a limited number of journals, and many journals inadequately address the commented papers. Our findings suggest that numerous problematic papers may still remain unidentified. The revealed characteristics offer valuable insights for formulating strategies to address the issue of fraudulent papers at scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24511v1</guid>
      <category>cs.DL</category>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying Lou, Zhengyi Zhou, Guosheng Wang, Zhesi Shen, Menghui Li</dc:creator>
    </item>
    <item>
      <title>Automatically Advancing LLM Expertise in Technology Judgment</title>
      <link>https://arxiv.org/abs/2505.12452</link>
      <description>arXiv:2505.12452v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are rapidly becoming core tools for science, engineering, and innovation. Their promise lies not just in remembering facts, but in putting knowledge to work. Despite their impressive ability to answer increasingly difficult questions, it remains unclear whether LLMs truly use their knowledge when confronted with new and challenging tasks. We address this question with a patent classification task that requires deep conceptual understanding: distinguishing objectively different but semantically similar patents. To evaluate this approach, we introduce a challenging new benchmark of 1.3 million post-2015 computer science patent pairs, characterized by dense technical jargon and strategically complex writing. We find that LLMs often fail our benchmark and struggle to distinguish among semantically similar patents. To probe this failure, we introduce a novel framework that decomposes model errors into two sources: missing and unused knowledge. Our approach asks models to generate clarifying questions to improve their understanding, and then compares three settings: raw performance, self-answered questions, and externally supplied answers. This decomposition reveals that LLMs often possess the relevant knowledge internally but fail to deploy it, while a smaller share of errors arises from genuine knowledge gaps. We then ask whether the ability of models to construct a task-specific database of questions and answers differs across models. We find that smaller models generate simpler, broadly transferable questions, while larger models propose more complex but less generalizable ones. This suggests new strategies for combining strengths across models. Our findings highlight a critical limitation of current LLMs and their evaluation: models often know more than they can use. LLM evaluation should shift from recall of static facts to application of dynamic knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12452v3</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.IR</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siyang Wu, Honglin Bao, Nadav Kunievsky, James A. Evans</dc:creator>
    </item>
  </channel>
</rss>
