<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 24 Apr 2025 01:40:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Transport f divergences</title>
      <link>https://arxiv.org/abs/2504.15515</link>
      <description>arXiv:2504.15515v2 Announce Type: new 
Abstract: We define a class of divergences to measure differences between probability density functions in one-dimensional sample space. The construction is based on the convex function with the Jacobi operator of mapping function that pushforwards one density to the other. We call these information measures transport f-divergences. We present several properties of transport $f$-divergences, including invariances, convexities, variational formulations, and Taylor expansions in terms of mapping functions. Examples of transport f-divergences in generative models are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15515v2</guid>
      <category>math.ST</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wuchen Li</dc:creator>
    </item>
    <item>
      <title>Dynamical mean-field analysis of adaptive Langevin diffusions: Propagation-of-chaos and convergence of the linear response</title>
      <link>https://arxiv.org/abs/2504.15556</link>
      <description>arXiv:2504.15556v1 Announce Type: new 
Abstract: Motivated by an application to empirical Bayes learning in high-dimensional regression, we study a class of Langevin diffusions in a system with random disorder, where the drift coefficient is driven by a parameter that continuously adapts to the empirical distribution of the realized process up to the current time. The resulting dynamics take the form of a stochastic interacting particle system having both a McKean-Vlasov type interaction and a pairwise interaction defined by the random disorder. We prove a propagation-of-chaos result, showing that in the large system limit over dimension-independent time horizons, the empirical distribution of sample paths of the Langevin process converges to a deterministic limit law that is described by dynamical mean-field theory. This law is characterized by a system of dynamical fixed-point equations for the limit of the drift parameter and for the correlation and response kernels of the limiting dynamics. Using a dynamical cavity argument, we verify that these correlation and response kernels arise as the asymptotic limits of the averaged correlation and linear response functions of single coordinates of the system. These results enable an asymptotic analysis of an empirical Bayes Langevin dynamics procedure for learning an unknown prior parameter in a linear regression model, which we develop in a companion paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15556v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Zhou Fan, Justin Ko, Bruno Loureiro, Yue M. Lu, Yandi Shen</dc:creator>
    </item>
    <item>
      <title>Dynamical mean-field analysis of adaptive Langevin diffusions: Replica-symmetric fixed point and empirical Bayes</title>
      <link>https://arxiv.org/abs/2504.15558</link>
      <description>arXiv:2504.15558v1 Announce Type: new 
Abstract: In many applications of statistical estimation via sampling, one may wish to sample from a high-dimensional target distribution that is adaptively evolving to the samples already seen. We study an example of such dynamics, given by a Langevin diffusion for posterior sampling in a Bayesian linear regression model with i.i.d. regression design, whose prior continuously adapts to the Langevin trajectory via a maximum marginal-likelihood scheme. Results of dynamical mean-field theory (DMFT) developed in our companion paper establish a precise high-dimensional asymptotic limit for the joint evolution of the prior parameter and law of the Langevin sample. In this work, we carry out an analysis of the equations that describe this DMFT limit, under conditions of approximate time-translation-invariance which include, in particular, settings where the posterior law satisfies a log-Sobolev inequality. In such settings, we show that this adaptive Langevin trajectory converges on a dimension-independent time horizon to an equilibrium state that is characterized by a system of scalar fixed-point equations, and the associated prior parameter converges to a critical point of a replica-symmetric limit for the model free energy. As a by-product of our analyses, we obtain a new dynamical proof that this replica-symmetric limit for the free energy is exact, in models having a possibly misspecified prior and where a log-Sobolev inequality holds for the posterior law.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15558v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Zhou Fan, Justin Ko, Bruno Loureiro, Yue M. Lu, Yandi Shen</dc:creator>
    </item>
    <item>
      <title>Deep learning of point processes for modeling high-frequency data</title>
      <link>https://arxiv.org/abs/2504.15944</link>
      <description>arXiv:2504.15944v1 Announce Type: new 
Abstract: We investigate applications of deep neural networks to a point process having an intensity with mixing covariates processes as input. Our generic model includes Cox-type models and marked point processes as well as multivariate point processes. An oracle inequality and a rate of convergence are derived for the prediction error. A simulation study shows that the marked point process can be superior to the simple multivariate model in prediction. We apply the marked ratio model to real limit order book data</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15944v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoshihiro Gyotoku, Ioane Muni Toke, Nakahiro Yoshida</dc:creator>
    </item>
    <item>
      <title>The e-Partitioning Principle of False Discovery Rate Control</title>
      <link>https://arxiv.org/abs/2504.15946</link>
      <description>arXiv:2504.15946v1 Announce Type: new 
Abstract: We present a novel necessary and sufficient principle for False Discovery Rate (FDR) control. This e-Partitioning Principle says that a procedure controls FDR if and only if it is a special case of a general e-Partitioning procedure. By writing existing methods as special cases of this procedure, we can achieve uniform improvements of these methods, and we show this in particular for the eBH, BY and Su methods. We also show that methods developed using the $e$-Partitioning Principle have several valuable properties. They generally control FDR not just for one rejected set, but simultaneously over many, allowing post hoc flexibility for the researcher in the final choice of the rejected hypotheses. Under some conditions, they also allow for post hoc adjustment of the error rate, choosing the FDR level $\alpha$ post hoc, or switching to familywise error control after seeing the data. In addition, e-Partitioning allows FDR control methods to exploit logical relationships between hypotheses to gain power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15946v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jelle Goeman, Rianne de Heide, Aldo Solari</dc:creator>
    </item>
    <item>
      <title>On the minimax optimality of Flow Matching through the connection to kernel density estimation</title>
      <link>https://arxiv.org/abs/2504.13336</link>
      <description>arXiv:2504.13336v1 Announce Type: cross 
Abstract: Flow Matching has recently gained attention in generative modeling as a simple and flexible alternative to diffusion models, the current state of the art. While existing statistical guarantees adapt tools from the analysis of diffusion models, we take a different perspective by connecting Flow Matching to kernel density estimation. We first verify that the kernel density estimator matches the optimal rate of convergence in Wasserstein distance up to logarithmic factors, improving existing bounds for the Gaussian kernel. Based on this result, we prove that for sufficiently large networks, Flow Matching also achieves the optimal rate up to logarithmic factors, providing a theoretical foundation for the empirical success of this method. Finally, we provide a first justification of Flow Matching's effectiveness in high-dimensional settings by showing that rates improve when the target distribution lies on a lower-dimensional linear subspace.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13336v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lea Kunkel, Mathias Trabs</dc:creator>
    </item>
    <item>
      <title>Deep learning with missing data</title>
      <link>https://arxiv.org/abs/2504.15388</link>
      <description>arXiv:2504.15388v1 Announce Type: cross 
Abstract: In the context of multivariate nonparametric regression with missing covariates, we propose Pattern Embedded Neural Networks (PENNs), which can be applied in conjunction with any existing imputation technique. In addition to a neural network trained on the imputed data, PENNs pass the vectors of observation indicators through a second neural network to provide a compact representation. The outputs are then combined in a third neural network to produce final predictions. Our main theoretical result exploits an assumption that the observation patterns can be partitioned into cells on which the Bayes regression function behaves similarly, and belongs to a compositional H\"older class. It provides a finite-sample excess risk bound that holds for an arbitrary missingness mechanism, and in combination with a complementary minimax lower bound, demonstrates that our PENN estimator attains in typical cases the minimax rate of convergence as if the cells of the partition were known in advance, up to a poly-logarithmic factor in the sample size. Numerical experiments on simulated, semi-synthetic and real data confirm that the PENN estimator consistently improves, often dramatically, on standard neural networks without pattern embedding. Code to reproduce our experiments, as well as a tutorial on how to apply our method, is publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15388v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyi Ma, Tengyao Wang, Richard J. Samworth</dc:creator>
    </item>
    <item>
      <title>A stochastic method to estimate a zero-inflated two-part mixed model for human microbiome data</title>
      <link>https://arxiv.org/abs/2504.15411</link>
      <description>arXiv:2504.15411v1 Announce Type: cross 
Abstract: Human microbiome studies based on genetic sequencing techniques produce compositional longitudinal data of the relative abundances of microbial taxa over time, allowing to understand, through mixed-effects modeling, how microbial communities evolve in response to clinical interventions, environmental changes, or disease progression. In particular, the Zero-Inflated Beta Regression (ZIBR) models jointly and over time the presence and abundance of each microbe taxon, considering the compositional nature of the data, its skewness, and the over-abundance of zeros. However, as for other complex random effects models, maximum likelihood estimation suffers from the intractability of likelihood integrals. Available estimation methods rely on log-likelihood approximation, which is prone to potential limitations such as biased estimates or unstable convergence. In this work we develop an alternative maximum likelihood estimation approach for the ZIBR model, based on the Stochastic Approximation Expectation Maximization (SAEM) algorithm. The proposed methodology allows to model unbalanced data, which is not always possible in existing approaches. We also provide estimations of the standard errors and the log-likelihood of the fitted model. The performance of the algorithm is established through simulation, and its use is demonstrated on two microbiome studies, showing its ability to detect changes in both presence and abundance of bacterial taxa over time and in response to treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15411v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>John Barrera, Cristian Meza, Ana Arribas-Gil</dc:creator>
    </item>
    <item>
      <title>Markov Kernels, Distances and Optimal Control: A Parable of Linear Quadratic Non-Gaussian Distribution Steering</title>
      <link>https://arxiv.org/abs/2504.15753</link>
      <description>arXiv:2504.15753v1 Announce Type: cross 
Abstract: For a controllable linear time-varying (LTV) pair $(\boldsymbol{A}_t,\boldsymbol{B}_t)$ and $\boldsymbol{Q}_{t}$ positive semidefinite, we derive the Markov kernel for the It\^{o} diffusion ${\mathrm{d}}\boldsymbol{x}_{t}=\boldsymbol{A}_{t}\boldsymbol{x}_t {\mathrm{d}} t + \sqrt{2}\boldsymbol{B}_{t}{\mathrm{d}}\boldsymbol{w}_{t}$ with an accompanying killing of probability mass at rate $\frac{1}{2}\boldsymbol{x}^{\top}\boldsymbol{Q}_{t}\boldsymbol{x}$. This Markov kernel is the Green's function for an associated linear reaction-advection-diffusion partial differential equation. Our result generalizes the recently derived kernel for the special case $\left(\boldsymbol{A}_t,\boldsymbol{B}_t\right)=\left(\boldsymbol{0},\boldsymbol{I}\right)$, and depends on the solution of an associated Riccati matrix ODE. A consequence of this result is that the linear quadratic non-Gaussian Schr\"{o}dinger bridge is exactly solvable. This means that the problem of steering a controlled LTV diffusion from a given non-Gaussian distribution to another over a fixed deadline while minimizing an expected quadratic cost can be solved using dynamic Sinkhorn recursions performed with the derived kernel. Our derivation for the $\left(\boldsymbol{A}_t,\boldsymbol{B}_t,\boldsymbol{Q}_t\right)$-parametrized kernel pursues a new idea that relies on finding a state-time dependent distance-like functional given by the solution of a deterministic optimal control problem. This technique breaks away from existing methods, such as generalizing Hermite polynomials or Weyl calculus, which have seen limited success in the reaction-diffusion context. Our technique uncovers a new connection between Markov kernels, distances, and optimal control. This connection is of interest beyond its immediate application in solving the linear quadratic Schr\"{o}dinger bridge problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15753v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexis M. H. Teter, Wenqing Wang, Sachin Shivakumar, Abhishek Halder</dc:creator>
    </item>
    <item>
      <title>Multiscale detection of practically significant changes in a gradually varying time series</title>
      <link>https://arxiv.org/abs/2504.15872</link>
      <description>arXiv:2504.15872v1 Announce Type: cross 
Abstract: In many change point problems it is reasonable to assume that compared to a benchmark at a given time point $t_0$ the properties of the observed stochastic process change gradually over time for $t &gt;t_0$. Often, these gradual changes are not of interest as long as they are small (nonrelevant), but one is interested in the question if the deviations are practically significant in the sense that the deviation of the process compared to the time $t_0$ (measured by an appropriate metric) exceeds a given threshold, which is of practical significance (relevant change).
  In this paper we develop novel and powerful change point analysis for detecting such deviations in a sequence of gradually varying means, which is compared with the average mean from a previous time period. Current approaches to this problem suffer from low power, rely on the selection of smoothing parameters and require a rather regular (smooth) development for the means. We develop a multiscale procedure that alleviates all these issues, validate it theoretically and demonstrate its good finite sample performance on both synthetic and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15872v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Bastian, Holger Dette</dc:creator>
    </item>
    <item>
      <title>Bayesian Parameter Identification in the Landau-de Gennes Theory for Nematic Liquid Crystals</title>
      <link>https://arxiv.org/abs/2504.16029</link>
      <description>arXiv:2504.16029v1 Announce Type: cross 
Abstract: This manuscript establishes a pathway to reconstruct material parameters from measurements within the Landau-de Gennes model for nematic liquid crystals. We present a Bayesian approach to this inverse problem and analyse its properties using given, simulated data for benchmark problems of a planar bistable nematic device. In particular, we discuss the accuracy of the Markov chain Monte Carlo approximations, confidence intervals and the limits of identifiability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16029v1</guid>
      <category>math.NA</category>
      <category>cond-mat.soft</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heiko Gimperlein, Ruma R. Maity, Apala Majumdar, Michael Oberguggenberger</dc:creator>
    </item>
    <item>
      <title>A Statistical Framework and Analysis for Perfect Radar Pulse Compression</title>
      <link>https://arxiv.org/abs/2308.07597</link>
      <description>arXiv:2308.07597v2 Announce Type: replace 
Abstract: Perfect radar pulse compression coding is a potential emerging field which aims at providing rigorous analysis and fundamental limit radar experiments. It is based on finding non-trivial pulse codes, which we can make statistically equivalent, to the radar experiments carried out with elementary pulses of some shape. A common engineering-based radar experiment design, regarding pulse-compression, often omits the rigorous theory and mathematical limitations. In this work our aim is to develop a mathematical theory which coincides with understanding the radar experiment in terms of the theory of comparison of statistical experiments. We review and generalize some properties of the It\^{o} measure. We estimate the unknown i.e. the structure function in the context of Bayesian statistical inverse problems. We study the posterior for generalized $d$-dimensional inverse problems, where we consider both real-valued and complex-valued inputs for posteriori analysis. Finally this is then extended to the infinite dimensional setting, where our analysis suggests the underlying posterior is non-Gaussian.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.07597v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Neil K. Chada, Petteri Piiroinen, Lassi Roininen</dc:creator>
    </item>
    <item>
      <title>Sampling effects on Lasso estimation of drift functions in high-dimensional diffusion processes</title>
      <link>https://arxiv.org/abs/2408.08638</link>
      <description>arXiv:2408.08638v2 Announce Type: replace 
Abstract: In this paper, we address high-dimensional parametric estimation of the drift function in diffusion models, specifically focusing on a $d$-dimensional ergodic diffusion process observed at discrete time points. We consider both a general linear form for the drift function and the particular case of the Ornstein-Uhlenbeck (OU) process. Assuming sparsity of the parameter vector, we examine the statistical behavior of the Lasso estimator for the unknown parameter. Our primary contribution is the proof of an oracle inequality for the Lasso estimator, which holds on the intersection of three specific sets defined for our analysis. We carefully control the probability of these sets, tackling the central challenge of our study. This approach allows us to derive error bounds for the $l_1$ and $l_2$ norms, assessing the performance of the proposed Lasso estimator. Our results demonstrate that, under certain conditions, the discretization error becomes negligible, enabling us to achieve the same optimal rate of convergence as if the continuous trajectory of the process were observed. We validate our theoretical findings through numerical experiments, which show that the Lasso estimator significantly outperforms the maximum likelihood estimator (MLE) in terms of support recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08638v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Amorino, Francisco Pina, Mark Podolskij</dc:creator>
    </item>
    <item>
      <title>Transition of $\alpha$-mixing in Random Iterations with Applications in Queuing Theory</title>
      <link>https://arxiv.org/abs/2410.05056</link>
      <description>arXiv:2410.05056v3 Announce Type: replace 
Abstract: Nonlinear time series models with exogenous regressors are essential in econometrics, queuing theory, and machine learning, though their statistical analysis remains incomplete. Key results, such as the law of large numbers and the functional central limit theorem, are known for weakly dependent variables. We demonstrate the transfer of mixing properties from the exogenous regressor to the response via coupling arguments. Additionally, we study Markov chains in random environments with drift and minorization conditions, even under non-stationary environments with favorable mixing properties, and apply this framework to single-server queuing models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05056v3</guid>
      <category>math.ST</category>
      <category>cs.AI</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Attila Lovas</dc:creator>
    </item>
    <item>
      <title>Estimation of conditional inequality curves and measures via estimating the conditional quantile function</title>
      <link>https://arxiv.org/abs/2412.20228</link>
      <description>arXiv:2412.20228v3 Announce Type: replace 
Abstract: The classical concept of inequality curves and measures is extended to conditional inequality curves and measures and a curve of conditional inequality measures is introduced. This extension provides a more nuanced analysis of inequality in relation to covariates. In particular, this enables comparison of inequalities between subpopulations, conditioned on certain values of covariates. To estimate the curves and measures, a novel method for estimating the conditional quantile function is proposed. The method incorporates a modified quantile regression framework that employs isotonic regression to ensure that there is no quantile crossing. The consistency of the proposed estimators is proved while their finite sample performance is evaluated through simulation studies and compared with existing quantile regression approaches. Finally, practical application is demonstrated by analysing salary inequality across different employee age groups, highlighting the potential of conditional inequality measures in empirical research. The code used to prepare the results presented in this article is available in a dedicated GitHub repository.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20228v3</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alicja Jokiel-Rokita, Sylwester Pi\k{a}tek, Rafa{\l} Topolnicki</dc:creator>
    </item>
    <item>
      <title>Semiparametric plug-in estimation, sup-norm risk bounds, marginal optimization, and inference in BTL model</title>
      <link>https://arxiv.org/abs/2503.15045</link>
      <description>arXiv:2503.15045v2 Announce Type: replace 
Abstract: The recent paper \cite{GSZ2023} on estimation and inference for top-ranking problem in Bradley-Terry-Lice (BTL) model presented a surprising result: component-wise estimation and inference can be done under much weaker conditions on the number of comparison then it is required for the full dimensional estimation. The present paper revisits this finding from completely different viewpoint. Namely, we show how a theoretical study of \emph{estimation in sup-norm} can be reduced to the analysis of \emph{plug-in semiparametric estimation}. For the latter, we adopt and extend the general approach from \cite{Sp2024} to high-dimensional estimation and inference. The main tool of the analysis is a theory of \emph{perturbed marginal optimization} when an objective function depends on a low-dimensional target parameter along with a high-dimensional nuisance parameter. A particular focus of the study is the critical dimension condition. Full-dimensional estimation requires in general the condition \( \mathbbmsl{N} \gg \mathbb{p} \) between the effective parameter dimension \( \mathbb{p} \) and the effective sample size \( \mathbbmsl{N} \) corresponding to the smallest eigenvalue of the Fisher information matrix \( \mathbbmsl{F} \). Inference on the estimated parameter is even more demanding: the condition \( \mathbbmsl{N} \gg \mathbb{p}^{2} \) cannot be generally avoided; see \cite{Sp2024}. However, for the sup-norm estimation, the critical dimension condition can be reduced to \( \mathbbmsl{N} \geq C \log p \).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15045v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladimir Spokoiny</dc:creator>
    </item>
    <item>
      <title>Low solution rank of the matrix LASSO under RIP with consequences for rank-constrained algorithms</title>
      <link>https://arxiv.org/abs/2404.12828</link>
      <description>arXiv:2404.12828v2 Announce Type: replace-cross 
Abstract: We show that solutions to the popular convex matrix LASSO problem (nuclear-norm--penalized linear least-squares) have low rank under similar assumptions as required by classical low-rank matrix sensing error bounds. Although the purpose of the nuclear norm penalty is to promote low solution rank, a proof has not yet (to our knowledge) been provided outside very specific circumstances. Furthermore, we show that this result has significant theoretical consequences for nonconvex rank-constrained optimization approaches. Specifically, we show that if (a) the ground truth matrix has low rank, (b) the (linear) measurement operator has the matrix restricted isometry property (RIP), and (c) the measurement error is small enough relative to the nuclear norm penalty, then the (unique) LASSO solution has rank (approximately) bounded by that of the ground truth. From this, we show (a) that a low-rank--projected proximal gradient descent algorithm will converge linearly to the LASSO solution from any initialization, and (b) that the nonconvex landscape of the low-rank Burer-Monteiro--factored problem formulation is benign in the sense that all second-order critical points are globally optimal and yield the LASSO solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12828v2</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew D. McRae</dc:creator>
    </item>
    <item>
      <title>Benign overfitting in Fixed Dimension via Physics-Informed Learning with Smooth Inductive Bias</title>
      <link>https://arxiv.org/abs/2406.09194</link>
      <description>arXiv:2406.09194v3 Announce Type: replace-cross 
Abstract: Recent advances in machine learning have inspired a surge of research into reconstructing specific quantities of interest from measurements that comply with certain physical laws. These efforts focus on inverse problems that are governed by partial differential equations (PDEs). In this work, we develop an asymptotic Sobolev norm learning curve for kernel ridge(less) regression when addressing (elliptical) linear inverse problems. Our results show that the PDE operators in the inverse problem can stabilize the variance and even behave benign overfitting for fixed-dimensional problems, exhibiting different behaviors from regression problems. Besides, our investigation also demonstrates the impact of various inductive biases introduced by minimizing different Sobolev norms as a form of implicit regularization. For the regularized least squares estimator, we find that all considered inductive biases can achieve the optimal convergence rate, provided the regularization parameter is appropriately chosen. The convergence rate is actually independent to the choice of (smooth enough) inductive bias for both ridge and ridgeless regression. Surprisingly, our smoothness requirement recovered the condition found in Bayesian setting and extend the conclusion to the minimum norm interpolation estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09194v3</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Honam Wong, Wendao Wu, Fanghui Liu, Yiping Lu</dc:creator>
    </item>
    <item>
      <title>Bringing closure to FDR control: beating the e-Benjamini-Hochberg procedure</title>
      <link>https://arxiv.org/abs/2504.11759</link>
      <description>arXiv:2504.11759v2 Announce Type: replace-cross 
Abstract: False discovery rate (FDR) has been a key metric for error control in multiple hypothesis testing, and many methods have developed for FDR control across a diverse cross-section of settings and applications. We develop a closure principle for all FDR controlling procedures, i.e., we provide a characterization based on e-values for all admissible FDR controlling procedures. A general version of this closure principle can recover any multiple testing error metric and allows one to choose the error metric post-hoc. We leverage this idea to formulate the closed eBH procedure, a (usually strict) improvement over the eBH procedure for FDR control when provided with e-values. This also yields a closed BY procedure that dominates the Benjamini-Yekutieli (BY) procedure for FDR control with arbitrarily dependent p-values, thus proving that the latter is inadmissibile. We demonstrate the practical performance of our new procedures in simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11759v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyu Xu, Lasse Fischer, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>How Much Weak Overlap Can Doubly Robust T-Statistics Handle?</title>
      <link>https://arxiv.org/abs/2504.13273</link>
      <description>arXiv:2504.13273v2 Announce Type: replace-cross 
Abstract: In the presence of sufficiently weak overlap, it is known that no regular root-n-consistent estimators exist and standard estimators may fail to be asymptotically normal. This paper shows that a thresholded version of the standard doubly robust estimator is asymptotically normal with well-calibrated Wald confidence intervals even when constructed using nonparametric estimates of the propensity score and conditional mean outcome. The analysis implies a cost of weak overlap in terms of black-box nuisance rates, borne when the semiparametric bound is infinite, and the contribution of outcome smoothness to the outcome regression rate, which is incurred even when the semiparametric bound is finite. As a byproduct of this analysis, I show that under weak overlap, the optimal global regression rate is the same as the optimal pointwise regression rate, without the usual polylogarithmic penalty. The high-level conditions yield new rules of thumb for thresholding in practice. In simulations, thresholded AIPW can exhibit moderate overrejection in small samples, but I am unable to reject a null hypothesis of exact coverage in large samples. In an empirical application, the clipped AIPW estimator that targets the standard average treatment effect yields similar precision to a heuristic 10% fixed-trimming approach that changes the target sample.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13273v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jacob Dorn</dc:creator>
    </item>
  </channel>
</rss>
