<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Dec 2024 02:52:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Copernican Argument for Alien Consciousness; The Mimicry Argument Against Robot Consciousness</title>
      <link>https://arxiv.org/abs/2412.00008</link>
      <description>arXiv:2412.00008v1 Announce Type: new 
Abstract: On broadly Copernican grounds, we are entitled to default assume that apparently behaviorally sophisticated extraterrestrial entities ("aliens") would be conscious. Otherwise, we humans would be inexplicably, implausibly lucky to have consciousness, while similarly behaviorally sophisticated entities elsewhere would be mere shells, devoid of consciousness. However, this Copernican default assumption is canceled in the case of behaviorally sophisticated entities designed to mimic superficial features associated with consciousness in humans ("consciousness mimics"), and in particular a broad class of current, near-future, and hypothetical robots. These considerations, which we formulate, respectively, as the Copernican and Mimicry Arguments, jointly defeat an otherwise potentially attractive parity principle, according to which we should apply the same types of behavioral or cognitive tests to aliens and robots, attributing or denying consciousness similarly to the extent they perform similarly. Instead of grounding speculations about alien and robot consciousness in metaphysical or scientific theories about the physical or functional bases of consciousness, our approach appeals directly to the epistemic principles of Copernican mediocrity and inference to the best explanation. This permits us to justify certain default assumptions about consciousness while remaining to a substantial extent neutral about specific metaphysical and scientific theories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00008v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Eric Schwitzgebel, Jeremy Pober</dc:creator>
    </item>
    <item>
      <title>New Graphs at the braingraph.org Website for Studying the Aging Brain Circuitry</title>
      <link>https://arxiv.org/abs/2412.01418</link>
      <description>arXiv:2412.01418v1 Announce Type: new 
Abstract: Human braingraphs or connectomes are widely studied in the last decade to understand the structural and functional properties of our brain. In the last several years our research group has computed and deposited thousands of human braingraphs to the braingraph.org site, by applying public structural (diffusion) MRI data from young and healthy subjects. Here we describe a recent addition to the {\tt braingraph.org} site, which contains connectomes from healthy and demented subjects between 42 and 95 years of age, based on the public release of the OASIS-3 dataset. The diffusion MRI data was processed with the Connectome Mapper Toolkit v.3.1. We believe that the new addition to the braingraph.org site will become a useful resource for enlightening the aging circuitry of the human brain in healthy and diseased subjects, including those with Alzheimer's disease in several stages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01418v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Balint Varga, Vince Grolmusz</dc:creator>
    </item>
    <item>
      <title>Task learning through stimulation-induced plasticity in neural networks</title>
      <link>https://arxiv.org/abs/2412.01683</link>
      <description>arXiv:2412.01683v1 Announce Type: new 
Abstract: Synaptic plasticity dynamically shapes the connectivity of neural systems and is key to learning processes in the brain. To what extent the mechanisms of plasticity can be exploited to drive a neural network and make it perform some kind of computational task remains unclear. This question, relevant in a bioengineering context, can be formulated as a control problem on a high-dimensional system with strongly constrained and non-linear dynamics. We present a self-contained procedure which, through appropriate spatio-temporal stimulations of the neurons, is able to drive rate-based neural networks with arbitrary initial connectivity towards a desired functional state. We illustrate our approach on two different computational tasks: a non-linear association between multiple input stimulations and activity patterns (representing digit images), and the construction of a continuous attractor encoding a collective variable in a neural population. Our work thus provides a proof of principle for emerging paradigms of in vitro computation based on real neurons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01683v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1103/PRXLife.2.043014</arxiv:DOI>
      <arxiv:journal_reference>PRX Life 2, 043014 (2024)</arxiv:journal_reference>
      <dc:creator>Francesco Borra, Simona Cocco, R\'emi Monasson</dc:creator>
    </item>
    <item>
      <title>The Thermodynamic Model to Study the Slow Afterhyperpolarization in a Single Neuron at Different ATP Levels</title>
      <link>https://arxiv.org/abs/2412.01707</link>
      <description>arXiv:2412.01707v1 Announce Type: new 
Abstract: The neuron consumes energy from ATP hydrolysis to maintain a far-from-equilibrium steady state inside the cell, thus all physiological functions inside the cell are modulated by thermodynamics. The neurons that manage information encoding, transferring, and processing with high energy consumption, displaying a phenomenon called slow afterhyperpolarization after burst firing, whose properties are affected by the energy conditions. Here we constructed a thermodynamical model to quantitatively describe the sAHP process generated by $Na^+-K^+$ ATPases(NKA) and the Calcium-activated potassium(K(Ca)) channels. The model simulates how the amplitude of sAHP is effected by the intracellular ATP concentration and ATP hydrolysis free energy $\Delta$ G. The results show a trade-off between NKA and the K(Ca)'s modulation on the sAHP's energy dependence, and also predict an alteration of sAHP's behavior under insufficient ATP supply if the proportion of NKA and K(Ca)'s expression quantities is changed. The research provides insights in understanding the maintenance of neural homeostasis and support furthur researches on metabolism-related and neurodegenerative diseases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01707v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jianwei Li, Simeng Yu, Mingye Guo, Xuewen Shen, Qi Ouyang, Fangting Li</dc:creator>
    </item>
    <item>
      <title>Differential learning kinetics govern the transition from memorization to generalization during in-context learning</title>
      <link>https://arxiv.org/abs/2412.00104</link>
      <description>arXiv:2412.00104v1 Announce Type: cross 
Abstract: Transformers exhibit in-context learning (ICL): the ability to use novel information presented in the context without additional weight updates. Recent work shows that ICL emerges when models are trained on a sufficiently diverse set of tasks and the transition from memorization to generalization is sharp with increasing task diversity. One interpretation is that a network's limited capacity to memorize favors generalization. Here, we examine the mechanistic underpinnings of this transition using a small transformer applied to a synthetic ICL task. Using theory and experiment, we show that the sub-circuits that memorize and generalize can be viewed as largely independent. The relative rates at which these sub-circuits learn explains the transition from memorization to generalization, rather than capacity constraints. We uncover a memorization scaling law, which determines the task diversity threshold at which the network generalizes. The theory quantitatively explains a variety of other ICL-related phenomena, including the long-tailed distribution of when ICL is acquired, the bimodal behavior of solutions close to the task diversity threshold, the influence of contextual and data distributional statistics on ICL, and the transient nature of ICL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00104v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Nguyen, Gautam Reddy</dc:creator>
    </item>
    <item>
      <title>Stochastic Dynamics and Probability Analysis for a Generalized Epidemic Model with Environmental Noise</title>
      <link>https://arxiv.org/abs/2412.00405</link>
      <description>arXiv:2412.00405v1 Announce Type: cross 
Abstract: In this paper we consider a stochastic SEIQR (susceptible-exposed-infected-quarantined-recovered) epidemic model with a generalized incidence function. Using the Lyapunov method, we establish the existence and uniqueness of a global positive solution to the model, ensuring that it remains well-defined over time. Through the application of Young's inequality and Chebyshev's inequality, we demonstrate the concepts of stochastic ultimate boundedness and stochastic permanence, providing insights into the long-term behavior of the epidemic dynamics under random perturbations. Furthermore, we derive conditions for stochastic extinction, which describe scenarios where the epidemic may eventually die out, and V-geometric ergodicity, which indicates the rate at which the system's state converges to its equilibrium. Finally, we perform numerical simulations to verify our theoretical results and assess the model's behavior under different parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00405v1</guid>
      <category>q-bio.PE</category>
      <category>math.DS</category>
      <category>physics.data-an</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brahim Boukanjimea, Mohamed Maama</dc:creator>
    </item>
    <item>
      <title>Simplified derivations for high-dimensional convex learning problems</title>
      <link>https://arxiv.org/abs/2412.01110</link>
      <description>arXiv:2412.01110v1 Announce Type: cross 
Abstract: Statistical physics provides tools for analyzing high-dimensional problems in machine learning and theoretical neuroscience. These calculations, particularly those using the replica method, often involve lengthy derivations that can obscure physical interpretation. We give concise, non-replica derivations of several key results and highlight their underlying similarities. Specifically, we introduce a cavity approach to analyzing high-dimensional learning problems and apply it to three cases: perceptron classification of points, perceptron classification of manifolds, and kernel ridge regression. These problems share a common structure -- a bipartite system of interacting feature and datum variables -- enabling a unified analysis. For perceptron-capacity problems, we identify a symmetry that allows derivation of correct capacities through a na\"ive method. These results match those obtained through the replica method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01110v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David G. Clark, Haim Sompolinsky</dc:creator>
    </item>
    <item>
      <title>Estimating the contribution of early and late noise in vision from psychophysical data</title>
      <link>https://arxiv.org/abs/2012.06608</link>
      <description>arXiv:2012.06608v4 Announce Type: replace 
Abstract: In many psychophysical detection and discrimination tasks human performance is thought to be limited by internal or inner noise when neuronal activity is converted into an overt behavioural response. It is unclear, however, to what extent the behaviourally limiting inner noise arises from early noise in the photoreceptors and the retina, or from late noise in cortex at or immediately prior to the decision stage. Presumably, the behaviourally limiting inner noise is a non-trivial combination of both early and late noises. Here we propose a method to quantify the contributions of early and late noise purely from psychophysical data. Our analysis generalizes classical results for linear systems (Burgess and Colborne, 1988) by combining the theory of noise propagation through a nonlinear network (Ahumada, 1987) with the expressions to obtain the perceptual metric along the nonlinear network (Malo and Simoncelli, 2006; Laparra et al., 2010). We show that from threshold-only data the relative contribution of early and late noise can only be determined if the experiments include substantial external noise in some of the stimuli used during experiments. If experimenters collected full psychometric functions, however, then early and late noise sources can be quantified even in the absence of external noise. Our psychophysical estimate of the magnitude of the early noise assuming a standard cascade of linear and nonlinear model stages is substantially lower than the noise in cone photocurrents computed via an accurate model of retinal physiology (Brainard and Wandell, 2020, ISETBIO). This is consistent with the idea that one of the fundamental tasks of early vision is to reduce the comparatively large retinal noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2012.06608v4</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jesus Malo, Jose Juan Esteve-Taboada, Guillermo Aguilar, Marianne Maertens, Felix A. Wichmann</dc:creator>
    </item>
    <item>
      <title>Multilevel Monte Carlo for a class of Partially Observed Processes in Neuroscience</title>
      <link>https://arxiv.org/abs/2310.06533</link>
      <description>arXiv:2310.06533v2 Announce Type: replace 
Abstract: In this paper we consider Bayesian parameter inference associated to a class of partially observed stochastic differential equations (SDE) driven by jump processes. Such type of models can be routinely found in applications, of which we focus upon the case of neuroscience. The data are assumed to be observed regularly in time and driven by the SDE model with unknown parameters. In practice the SDE may not have an analytically tractable solution and this leads naturally to a time-discretization. We adapt the multilevel Markov chain Monte Carlo method of [11], which works with a hierarchy of time discretizations and show empirically and theoretically that this is preferable to using one single time discretization. The improvement is in terms of the computational cost needed to obtain a pre-specified numerical error. Our approach is illustrated on models that are found in neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.06533v2</guid>
      <category>q-bio.NC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohamed Maama, Ajay Jasra, Kengo Kamatani</dc:creator>
    </item>
    <item>
      <title>Latent Diffusion for Neural Spiking Data</title>
      <link>https://arxiv.org/abs/2407.08751</link>
      <description>arXiv:2407.08751v2 Announce Type: replace 
Abstract: Modern datasets in neuroscience enable unprecedented inquiries into the relationship between complex behaviors and the activity of many simultaneously recorded neurons. While latent variable models can successfully extract low-dimensional embeddings from such recordings, using them to generate realistic spiking data, especially in a behavior-dependent manner, still poses a challenge. Here, we present Latent Diffusion for Neural Spiking data (LDNS), a diffusion-based generative model with a low-dimensional latent space: LDNS employs an autoencoder with structured state-space (S4) layers to project discrete high-dimensional spiking data into continuous time-aligned latents. On these inferred latents, we train expressive (conditional) diffusion models, enabling us to sample neural activity with realistic single-neuron and population spiking statistics. We validate LDNS on synthetic data, accurately recovering latent structure, firing rates, and spiking statistics. Next, we demonstrate its flexibility by generating variable-length data that mimics human cortical activity during attempted speech. We show how to equip LDNS with an expressive observation model that accounts for single-neuron dynamics not mediated by the latent state, further increasing the realism of generated samples. Finally, conditional LDNS trained on motor cortical activity during diverse reaching behaviors can generate realistic spiking data given reach direction or unseen reach trajectories. In summary, LDNS simultaneously enables inference of low-dimensional latents and realistic conditional generation of neural spiking datasets, opening up further possibilities for simulating experimentally testable hypotheses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08751v2</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jaivardhan Kapoor, Auguste Schulz, Julius Vetter, Felix Pei, Richard Gao, Jakob H. Macke</dc:creator>
    </item>
    <item>
      <title>Multi-Resolution Graph Analysis of Dynamic Brain Network for Classification of Alzheimer's Disease and Mild Cognitive Impairment</title>
      <link>https://arxiv.org/abs/2409.04072</link>
      <description>arXiv:2409.04072v2 Announce Type: replace 
Abstract: Alzheimer's disease (AD) is a neurodegenerative disorder marked by memory loss and cognitive decline, making early detection vital for timely intervention. However, early diagnosis is challenging due to the heterogeneous presentation of symptoms. Resting-state functional magnetic resonance imaging (rs-fMRI) captures spontaneous brain activity and functional connectivity, which are known to be disrupted in AD and mild cognitive impairment (MCI). Traditional methods, such as Pearson's correlation, have been used to calculate association matrices, but these approaches often overlook the dynamic and non-stationary nature of brain activity. In this study, we introduce a novel method that integrates discrete wavelet transform (DWT) and graph theory to model the dynamic behavior of brain networks. Our approach captures the time-frequency representation of brain activity, allowing for a more nuanced analysis of the underlying network dynamics. Machine learning was employed to automate the discrimination of different stages of AD based on learned patterns from brain network at different frequency bands. We applied our method to a dataset of rs-fMRI images from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database, demonstrating its potential as an early diagnostic tool for AD and for monitoring disease progression. Our statistical analysis identifies specific brain regions and connections that are affected in AD and MCI, at different frequency bands, offering deeper insights into the disease's impact on brain function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04072v2</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Khazaee, Abdolreza Mohammadi, Ruairi O'Reilly</dc:creator>
    </item>
    <item>
      <title>Image Statistics Predict the Sensitivity of Perceptual Quality Metrics</title>
      <link>https://arxiv.org/abs/2303.09874</link>
      <description>arXiv:2303.09874v4 Announce Type: replace-cross 
Abstract: Previously, Barlow and Attneave hypothesised a link between biological vision and information maximisation. Following Shannon, information was defined using the probability of natural images. Several physiological and psychophysical phenomena have been derived from principles like info-max, efficient coding, or optimal denoising. However, it remains unclear how this link is expressed in mathematical terms from image probability. Classical derivations were subjected to strong assumptions on the probability models and on the behaviour of the sensors. Moreover, the direct evaluation of the hypothesis was limited by the inability of classical image models to deliver accurate estimates of the probability. Here, we directly evaluate image probabilities using a generative model for natural images, and analyse how probability-related factors can be combined to predict the sensitivity of state-of-the-art subjective image quality metrics, a proxy for human perception. We use information theory and regression analysis to find a simple model that when combining just two probability-related factors achieves 0.77 correlation with subjective metrics. This probability-based model is validated in two ways: through direct comparison with the opinion of real observers in a subjective quality experiment, and by reproducing basic trends of classical psychophysical facts such as the Contrast Sensitivity Function, the Weber-law, and contrast masking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.09874v4</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Hepburn, Valero Laparra, Ra\'ul Santos-Rodriguez, Jes\'us Malo</dc:creator>
    </item>
    <item>
      <title>Alternators For Sequence Modeling</title>
      <link>https://arxiv.org/abs/2405.11848</link>
      <description>arXiv:2405.11848v2 Announce Type: replace-cross 
Abstract: This paper introduces alternators, a novel family of non-Markovian dynamical models for sequences. An alternator features two neural networks: the observation trajectory network (OTN) and the feature trajectory network (FTN). The OTN and the FTN work in conjunction, alternating between outputting samples in the observation space and some feature space, respectively, over a cycle. The parameters of the OTN and the FTN are not time-dependent and are learned via a minimum cross-entropy criterion over the trajectories. Alternators are versatile. They can be used as dynamical latent-variable generative models or as sequence-to-sequence predictors. Alternators can uncover the latent dynamics underlying complex sequential data, accurately forecast and impute missing data, and sample new trajectories. We showcase the capabilities of alternators in three applications. We first used alternators to model the Lorenz equations, often used to describe chaotic behavior. We then applied alternators to Neuroscience, to map brain activity to physical activity. Finally, we applied alternators to Climate Science, focusing on sea-surface temperature forecasting. In all our experiments, we found alternators are stable to train, fast to sample from, yield high-quality generated samples and latent variables, and often outperform strong baselines such as Mambas, neural ODEs, and diffusion models in the domains we studied.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11848v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>physics.ao-ph</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Reza Rezaei, Adji Bousso Dieng</dc:creator>
    </item>
    <item>
      <title>Scale-free behavior of weight distributions of connectomes</title>
      <link>https://arxiv.org/abs/2407.17220</link>
      <description>arXiv:2407.17220v2 Announce Type: replace-cross 
Abstract: To determine the precise link between anatomical structure and function, brain studies primarily concentrate on the anatomical wiring of the brain and its topological properties. In this work, we investigate the weighted degree and connection length distributions of the KKI-113 and KKI-18 human connectomes, the fruit fly, and of the mouse retina. We found that the node strength (weighted degree) distribution behavior differs depending on the considered scale. On the global scale, the distributions are found to follow a power-law behavior, with a roughly universal exponent close to 3. However, this behavior breaks at the local scale as the node strength distributions of the KKI-18 follow a stretched exponential, and the fly and mouse retina follow the lognormal distribution, respectively which are indicative of underlying random multiplicative processes and underpins non-locality of learning in a brain close to the critical state. However, for the case of the KKI-113 and the H01 human (1mm$^3$) datasets, the local weighted degree distributions follow an exponentially truncated power-law, which may hint at the fact that the critical learning mechanism may have manifested at the node level too.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17220v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>physics.bio-ph</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michelle Cirunay, G\'eza \'Odor, Istv\'an Papp, Gustavo Deco</dc:creator>
    </item>
    <item>
      <title>Do Mice Grok? Glimpses of Hidden Progress During Overtraining in Sensory Cortex</title>
      <link>https://arxiv.org/abs/2411.03541</link>
      <description>arXiv:2411.03541v2 Announce Type: replace-cross 
Abstract: Does learning of task-relevant representations stop when behavior stops changing? Motivated by recent theoretical advances in machine learning and the intuitive observation that human experts continue to learn from practice even after mastery, we hypothesize that task-specific representation learning can continue, even when behavior plateaus. In a novel reanalysis of recently published neural data, we find evidence for such learning in posterior piriform cortex of mice following continued training on a task, long after behavior saturates at near-ceiling performance ("overtraining"). This learning is marked by an increase in decoding accuracy from piriform neural populations and improved performance on held-out generalization tests. We demonstrate that class representations in cortex continue to separate during overtraining, so that examples that were incorrectly classified at the beginning of overtraining can abruptly be correctly classified later on, despite no changes in behavior during that time. We hypothesize this hidden yet rich learning takes the form of approximate margin maximization; we validate this and other predictions in the neural data, as well as build and interpret a simple synthetic model that recapitulates these phenomena. We conclude by showing how this model of late-time feature learning implies an explanation for the empirical puzzle of overtraining reversal in animal learning, where task-specific representations are more robust to particular task changes because the learned features can be reused.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03541v2</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tanishq Kumar, Blake Bordelon, Cengiz Pehlevan, Venkatesh N. Murthy, Samuel J. Gershman</dc:creator>
    </item>
  </channel>
</rss>
