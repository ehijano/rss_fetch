<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 13 Jan 2025 05:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Propensity score matching in semaglutide retrospective studies</title>
      <link>https://arxiv.org/abs/2501.05533</link>
      <description>arXiv:2501.05533v1 Announce Type: new 
Abstract: Propensity Score Matching (PSM) is a causal inference technique that is used as a substitution for experimental methods when it is not possible to implement them due to logistical and ethical concerns. By using a logistic classifier to calculate the probability of assignment between the control and experimental groups a log odds value or 'logit' score is assigned to each data point. After assignment of a logit score every data point in the treatment group is assigned a comparable control in order to balance the potential confounding variables of an experiment. While a viable inference technique, many implementations of PSM fail to properly outline the methodology used, such as not explaining feature selection and matching techniques. This paper outlines multiple different techniques for both feature selection and matching which then are compared based on their efficiency. Three unique quantitative feature selection methods were utilized including random removal, feature importance calculation, and individual removal. Individual removal was the most efficient in consolidating the overlap between the treatment and control groups. The matching techniques used were bisect, binary insertion, nearest neighbors, and the most efficient, nearest neighbor with a caliper, in order to limit the error percentage and standard mean deviation. Only testing these techniques on a data set that included patients treated with semaglutide makes it not possible to definitively state which technique is the best. However, this paper explores the influence of methodology on the outcome of an experiment while providing ways in which to test efficiency of techniques. It is not only important for researchers to properly document methodology but explore different techniques to maximize results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05533v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elizabeth Mohney, Alexey Shvets</dc:creator>
    </item>
    <item>
      <title>Bayesian Model Selection for Network Discrimination and Risk-informed Decision Making in Material Flow Analysis</title>
      <link>https://arxiv.org/abs/2501.05556</link>
      <description>arXiv:2501.05556v1 Announce Type: new 
Abstract: Material flow analyses (MFAs) provide insight into supply chain level opportunities for resource efficiency. MFAs can be represented as networks with nodes that represent materials, processes, sectors or locations. MFA network structure uncertainty (i.e., the existence or absence of flows between nodes) is pervasive and can undermine the reliability of the flow predictions. This article investigates MFA network structure uncertainty by proposing candidate node-and-flow structures and using Bayesian model selection to identify the most suitable structures and Bayesian model averaging to quantify the parametric mass flow uncertainty. The results of this holistic approach to MFA uncertainty are used in conjunction with the input-output (I/O) method to make risk-informed resource efficiency recommendation. These techniques are demonstrated using a case study on the U.S. steel sector where 16 candidate structures are considered. Model selection highlights 2 networks as most probable based on data collected from the United States Geological Survey and the World Steel Association. Using the I/O method, we then show that the construction sector accounts for the greatest mean share of domestic U.S. steel industry emissions while the automotive and steel products sectors have the highest mean emissions per unit of steel used in the end-use sectors. The uncertainty in the results is used to analyze which end-use sector should be the focus of demand reduction efforts under different appetites for risk. This article's methods generate holistic and transparent MFA uncertainty that account for structural uncertainty, enabling decisions whose outcomes are more robust to the uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05556v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiankan Liao, Xun Huan, Daniel Cooper</dc:creator>
    </item>
    <item>
      <title>The Impact of Question Framing on the Precision of Automatic Occupation Coding</title>
      <link>https://arxiv.org/abs/2501.05584</link>
      <description>arXiv:2501.05584v1 Announce Type: new 
Abstract: Occupational data play a vital role in research, official statistics, and policymaking, yet their collection and accurate classification remain a persistent challenge. This study investigates the effects of occupational question wording on data variability and the performance of automatic coding tools. Through a series of survey experiments conducted and replicated in Germany, we tested two widely-used occupational question formats: one focusing on "job title" (Berufsbezeichnung) and another on "occupational tasks" (berufliche T\"atigkeit). Our analysis reveals that automatic coding tools, such as CASCOT and OccuCoDe, exhibit significant sensitivity to the form and origin of the data. Specifically, these tools performed more efficiently when coding responses to the job title question format compared to the occupational task format. Additionally, we found that including examples of main tasks and duties in the questions led respondents to provide more detailed but less linguistically diverse responses. This reduced diversity may negatively affect the precision of automatic coding. These findings highlight the importance of tailoring automatic coding tools to the specific structure and origin of the data they are applied to. We emphasize the need for further research to optimize question design and coding tools for greater accuracy and applicability in occupational data collection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05584v1</guid>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Olga Kononykhina, Malte Schierholz, Frauke Kreuter</dc:creator>
    </item>
    <item>
      <title>The Geostrategy of Youth Player Recruitment in Portuguese Clubs</title>
      <link>https://arxiv.org/abs/2501.05858</link>
      <description>arXiv:2501.05858v1 Announce Type: new 
Abstract: Portugal's prominent role as a global exporter of football talent is primarily driven by youth academies. Notably, Portugal leads the global ranking in terms of net transfer balance. This study aims to uncover and understand the recruitment strategies of Portuguese clubs for sourcing young talent and evaluate the relative success of different strategies. A comprehensive dataset spanning recent decades of Portuguese youth and professional football provides granular insights, including information such as players' birthplaces and the initial grassroots clubs where they developed. The initial findings suggest a correlation between a club's prominence and the geographic reach of its youth scouting operations, with larger clubs able to cast their net wider. Analysis of the correlation between players' birthplace and high-tier football club location suggests that the performance of senior teams acts as a catalyst for investment in youth teams. Regions without professional clubs are often left underserved. That said, certain clubs have made significant gains by focusing on player recruitment outside their district, such as the Algarve region, demonstrating how geographically targeted strategies can deliver substantial returns on investment. This study underscores data's role in sharpening youth player recruitment operations at football clubs. Clubs have access to in-depth and comprehensive datasets that can be used for resource allocation, territorial coverage planning, and identifying strategic partnerships with other clubs, potentially influencing their future success both on the field and financially. This offers opportunities for growth for individual clubs and holds implications for the continued strength of Portuguese football.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05858v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tiago Mendes-Neves, Lu\'is Meireles, Jo\~ao Mendes-Moreira, Nuno de Almeida</dc:creator>
    </item>
    <item>
      <title>Forecasting Soccer Matches through Distributions</title>
      <link>https://arxiv.org/abs/2501.05873</link>
      <description>arXiv:2501.05873v1 Announce Type: new 
Abstract: Forecasting sporting events encapsulate a compelling intellectual endeavor, underscored by the substantial financial activity of an estimated $80 billion wagered in global sports betting during 2022, a trend that grows yearly. Motivated by the challenges set forth in the Springer Soccer Prediction Challenge, this study presents a method for forecasting soccer match outcomes by forecasting the shot quantity and quality distributions. The methodology integrates established ELO ratings with machine learning models. The empirical findings reveal that, despite the constraints of the challenge, this approach yields positive returns, taking advantage of the established market odds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05873v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tiago Mendes-Neves, Yassine Baghoussi, Lu\'is Meireles, Carlos Soares, Jo\~ao Mendes-Moreira</dc:creator>
    </item>
    <item>
      <title>Statistical Challenges in Analyzing Migrant Backgrounds Among University Students: a Case Study from Italy</title>
      <link>https://arxiv.org/abs/2501.06166</link>
      <description>arXiv:2501.06166v1 Announce Type: new 
Abstract: The methodological issues and statistical complexities of analyzing university students with migrant backgrounds is explored, focusing on Italian data from the University of Milano-Bicocca. With the increasing size of migrant populations and the growth of the second and middle generations, the need has risen for deeper knowledge of the various strata of this population, including university students with migrant backgrounds. This presents challenges due to inconsistent recording in university datasets. By leveraging both administrative records and an original targeted survey we propose a methodology to fully identify the study population of students with migrant histories, and to distinguish relevant subpopulations within it such as second-generation born in Italy. Traditional logistic regression and machine learning random forest models are used and compared to predict migrant status. The primary contribution lies in creating an expanded administrative dataset enriched with indicators of students' migrant backgrounds and status. The expanded dataset provides a critical foundation for analyzing the characteristics of students with migration histories across all variables routinely registered in the administrative data set. Additionally, findings highlight the presence of selection bias in the targeted survey data, underscoring the need of further research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06166v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Giammei, Laura Terzera, Fulvia Mecatti</dc:creator>
    </item>
    <item>
      <title>Probabilistic Forecasts of Load, Solar and Wind for Electricity Price Forecasting</title>
      <link>https://arxiv.org/abs/2501.06180</link>
      <description>arXiv:2501.06180v1 Announce Type: new 
Abstract: Electricity price forecasting is a critical tool for the efficient operation of power systems and for supporting informed decision-making by market participants. This paper explores a novel methodology aimed at improving the accuracy of electricity price forecasts by incorporating probabilistic inputs of fundamental variables. Traditional approaches often rely on point forecasts of exogenous variables such as load, solar, and wind generation. Our method proposes the integration of quantile forecasts of these fundamental variables, providing a new set of exogenous variables that account for a more comprehensive representation of uncertainty. We conducted empirical tests on the German electricity market using recent data to evaluate the effectiveness of this approach. The findings indicate that incorporating probabilistic forecasts of load and renewable energy source generation significantly improves the accuracy of point forecasts of electricity prices. Furthermore, the results clearly show that the highest improvement in forecast accuracy can be achieved with full probabilistic forecast information. This highlights the importance of probabilistic forecasting in research and practice, particularly that the current state-of-the-art in reporting load, wind and solar forecast is insufficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06180v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bartosz Uniejewski, Florian Ziel</dc:creator>
    </item>
    <item>
      <title>Diving Deep: Forecasting Sea Surface Temperatures and Anomalies</title>
      <link>https://arxiv.org/abs/2501.05731</link>
      <description>arXiv:2501.05731v1 Announce Type: cross 
Abstract: This overview paper details the findings from the Diving Deep: Forecasting Sea Surface Temperatures and Anomalies Challenge at the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD) 2024. The challenge focused on the data-driven predictability of global sea surface temperatures (SSTs), a key factor in climate forecasting, ecosystem management, fisheries management, and climate change monitoring. The challenge involved forecasting SST anomalies (SSTAs) three months in advance using historical data and included a special task of predicting SSTAs nine months ahead for the Baltic Sea. Participants utilized various machine learning approaches to tackle the task, leveraging data from ERA5. This paper discusses the methodologies employed, the results obtained, and the lessons learned, offering insights into the future of climate-related predictive modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05731v1</guid>
      <category>cs.LG</category>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ding Ning, Varvara Vetrova, Karin R. Bryan, Yun Sing Koh, Andreas Voskou, N'Dah Jean Kouagou, Arnab Sharma</dc:creator>
    </item>
    <item>
      <title>Empirical Power Analysis of a Statistical Test to Quantify Gerrymandering</title>
      <link>https://arxiv.org/abs/2501.05761</link>
      <description>arXiv:2501.05761v1 Announce Type: cross 
Abstract: Gerrymandering is a pervasive problem within the US political system. In the past decade, methods based on Markov Chain Monte Carlo (MCMC) sampling and statistical outlier tests have been proposed to quantify gerrymandering and were used as evidence in several high-profile legal cases. We perform an empirical power analysis of one such hypothesis test from Chikina et al (2020). We generate a family of biased North Carolina congressional district maps using the 2012 and 2016 presidential elections and assess under which conditions the outlier test fails to flag them at the specified Type I error level. The power of the outlier test is found to be relatively stable across political parties, election years, lengths of the MCMC chain and effect sizes. The main effect on the power of the test is shown to be the choice of the bias metric. This is the first work that computationally verifies the power of statistical tests used in gerrymandering cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05761v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ranthony A. Clark, Susan Glenn, Harlin Lee, Soledad Villar</dc:creator>
    </item>
    <item>
      <title>Explainable Federated Bayesian Causal Inference and Its Application in Advanced Manufacturing</title>
      <link>https://arxiv.org/abs/2501.06077</link>
      <description>arXiv:2501.06077v1 Announce Type: cross 
Abstract: Causal inference has recently gained notable attention across various fields like biology, healthcare, and environmental science, especially within explainable artificial intelligence (xAI) systems, for uncovering the causal relationships among multiple variables and outcomes. Yet, it has not been fully recognized and deployed in the manufacturing systems. In this paper, we introduce an explainable, scalable, and flexible federated Bayesian learning framework, \texttt{xFBCI}, designed to explore causality through treatment effect estimation in distributed manufacturing systems. By leveraging federated Bayesian learning, we efficiently estimate posterior of local parameters to derive the propensity score for each client without accessing local private data. These scores are then used to estimate the treatment effect using propensity score matching (PSM). Through simulations on various datasets and a real-world Electrohydrodynamic (EHD) printing data, we demonstrate that our approach outperforms standard Bayesian causal inference methods and several state-of-the-art federated learning benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06077v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaofeng Xiao, Khawlah Alharbi, Pengyu Zhang, Hantang Qin, Xubo Yue</dc:creator>
    </item>
    <item>
      <title>Maximizing Diver Score by Examining Discrepancies in Diver Competency and Judges' Marks</title>
      <link>https://arxiv.org/abs/2310.08719</link>
      <description>arXiv:2310.08719v2 Announce Type: replace 
Abstract: Central to diving competitions is the diver's ``dive list'', which is the list of dives an athlete will perform during a competition. Creating a dive list that contains enough difficulty to be competitive yet not beyond the capability of the diver is an important consideration in diving. In this work, we examine the discrepancy between a diver's ability and judges' scores in springboard diving meets with the purpose of discovering biases in scoring that might aid a diver in completing a dive list. As a measure of the ability of a diver, we calculate a mean score for all dives and all meets in which the diver has participated. We call this mean score a diver's competency score. We use the difference between judges' scores within a given meet and the diver's competency to define a discrepancy: the difference between a judge's estimation of a diver's ability and their true ability. The notions of competency and discrepancy are applied to a data set, gathered from divemeets.com for high-school one meter diving competitions in the US from 2017 to 2022.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.08719v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Monnie McGee</dc:creator>
    </item>
    <item>
      <title>Bayesian inference for the Markov-modulated Poisson process with an outcome process</title>
      <link>https://arxiv.org/abs/2408.15314</link>
      <description>arXiv:2408.15314v2 Announce Type: replace 
Abstract: In medical research, understanding changes in outcome measurements is crucial for inferring shifts in health conditions. However, traditional methods often struggle with large, irregularly longitudinal data and fail to account for the tendency of individuals in poorer health to interact more frequently with the healthcare system. Additionally, clinical data can lack information on terminating events like death. To address these challenges, we start from the continuous-time hidden Markov model which models observed data as outcomes influenced by latent health states. Our extension incorporates a point process to account for the impact of health states on observation timings and includes a "death" state to model unobserved terminating events through a Poisson process, where transition rates depend on the latent health state. This approach captures both the severity of the disease and the timing of healthcare interactions. We present an exact Gibbs sampler procedure that alternates between sampling the latent health state paths and the model parameters. By including the "death" state, we mitigate biases in parameter estimation that would arise from solely modelling "live" health states. Simulation studies demonstrate that the proposed Gibbs sampler performs effectively. We apply our method to Canadian healthcare data, offering valuable insights for healthcare management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15314v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Luo, Chris Sherlock</dc:creator>
    </item>
    <item>
      <title>Deep Switching State Space Model (DS$^3$M) for Nonlinear Time Series Forecasting with Regime Switching</title>
      <link>https://arxiv.org/abs/2106.02329</link>
      <description>arXiv:2106.02329v3 Announce Type: replace-cross 
Abstract: Modern time series data often display complex nonlinear dependencies along with irregular regime-switching behaviors. These features present technical challenges in modeling, inference, and in offering insightful understanding into the underlying stochastic phenomena. To tackle these challenges, we introduce a novel modeling framework known as the Deep Switching State Space Model (DS$^3$M). This framework is engineered to make accurate forecasts for such time series while adeptly identifying the irregular regimes hidden within the dynamics. These identifications not only have significant economic ramifications but also contribute to a deeper understanding of the underlying phenomena. In DS$^3$M, the architecture employs discrete latent variables to represent regimes and continuous latent variables to account for random driving factors. By melding a Recurrent Neural Network (RNN) with a nonlinear Switching State Space Model (SSSM), we manage to capture the nonlinear dependencies and irregular regime-switching behaviors, governed by a Markov chain and parameterized using multilayer perceptrons. We validate the effectiveness and regime identification capabilities of DS$^3$M through short- and long-term forecasting tests on a wide array of simulated and real-world datasets, spanning sectors such as healthcare, economics, traffic, meteorology, and energy. Experimental results reveal that DS$^3$M outperforms several state-of-the-art models in terms of forecasting accuracy, while providing meaningful regime identifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.02329v3</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiuqin Xu, Hanqiu Peng, Ying Chen</dc:creator>
    </item>
    <item>
      <title>Testing the Fairness-Accuracy Improvability of Algorithms</title>
      <link>https://arxiv.org/abs/2405.04816</link>
      <description>arXiv:2405.04816v4 Announce Type: replace-cross 
Abstract: Many organizations use algorithms that have a disparate impact, i.e., the benefits or harms of the algorithm fall disproportionately on certain social groups. Addressing an algorithm's disparate impact can be challenging, however, because it is often unclear whether it is possible to reduce this impact without sacrificing other objectives of the organization, such as accuracy or profit. Establishing the improvability of algorithms with respect to multiple criteria is of both conceptual and practical interest: in many settings, disparate impact that would otherwise be prohibited under US federal law is permissible if it is necessary to achieve a legitimate business interest. The question is how a policy-maker can formally substantiate, or refute, this "necessity" defense. In this paper, we provide an econometric framework for testing the hypothesis that it is possible to improve on the fairness of an algorithm without compromising on other pre-specified objectives. Our proposed test is simple to implement and can be applied under any exogenous constraint on the algorithm space. We establish the large-sample validity and consistency of our test, and microfound the test's robustness to manipulation based on a game between a policymaker and the analyst. Finally, we apply our approach to evaluate a healthcare algorithm originally considered by Obermeyer et al. (2019), and quantify the extent to which the algorithm's disparate impact can be reduced without compromising the accuracy of its predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04816v4</guid>
      <category>econ.EM</category>
      <category>cs.DS</category>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Auerbach, Annie Liang, Kyohei Okumura, Max Tabord-Meehan</dc:creator>
    </item>
    <item>
      <title>Multi-objective Bayesian optimization for Likelihood-Free inference in sequential sampling models of decision making</title>
      <link>https://arxiv.org/abs/2409.01735</link>
      <description>arXiv:2409.01735v3 Announce Type: replace-cross 
Abstract: Scientifically motivated statistical models can sometimes be defined by a generative process for simulating synthetic data. Models specified this way can have likelihoods which are intractable, and this is the case for many sequential sampling models (SSMs) widely used in psychology and consumer behavior modelling. Researchers have developed likelihood-free inference (LFI) methods to make Bayesian inferences on parameters in models with intractable likelihood. Extending a popular approach to simulation efficient LFI for single-source data, we propose Multi-objective Bayesian Optimization for Likelihood-Free Inference (MOBOLFI) to estimate the parameters of SSMs calibrated using multi-source data, such as those based on response times and choice outcomes. MOBOLFI models a multi-dimensional discrepancy between observed and simulated data, using a discrepancy for each data source. Multi-objective Bayesian Optimization is then used to ensure simulation efficient approximation of the SSM likelihood. The use of a multivariate discrepancy allows for approximations to individual data source likelihoods in addition to the joint likelihood, enabling both the detection of conflicting information and a deeper understanding of the importance of different data sources in estimating individual SSM parameters. We illustrate the advantages of our approach in comparison with the use of a single discrepancy in a simple synthetic data example and an SSM example with real-world data assessing preferences of ride-hailing drivers in Singapore to rent electric vehicles. Although we focus on applications to SSMs, our approach applies to the likelihood-free calibration of other models using multi-source data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01735v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Chen, Xinwei Li, Eui-Jin Kim, Prateek Bansal, David Nott</dc:creator>
    </item>
    <item>
      <title>Exploring the Potential Role of Generative AI in the TRAPD Procedure for Survey Translation</title>
      <link>https://arxiv.org/abs/2411.14472</link>
      <description>arXiv:2411.14472v2 Announce Type: replace-cross 
Abstract: This paper explores and assesses in what ways generative AI can assist in translating survey instruments. Writing effective survey questions is a challenging and complex task, made even more difficult for surveys that will be translated and deployed in multiple linguistic and cultural settings. Translation errors can be detrimental, with known errors rendering data unusable for its intended purpose and undetected errors leading to incorrect conclusions. A growing number of institutions face this problem as surveys deployed by private and academic organizations globalize, and the success of their current efforts depends heavily on researchers' and translators' expertise and the amount of time each party has to contribute to the task. Thus, multilinguistic and multicultural surveys produced by teams with limited expertise, budgets, or time are at significant risk for translation-based errors in their data. We implement a zero-shot prompt experiment using ChatGPT to explore generative AI's ability to identify features of questions that might be difficult to translate to a linguistic audience other than the source language. We find that ChatGPT can provide meaningful feedback on translation issues, including common source survey language, inconsistent conceptualization, sensitivity and formality issues, and nonexistent concepts. In addition, we provide detailed information on the practicality of the approach, including accessing the necessary software, associated costs, and computational run times. Lastly, based on our findings, we propose avenues for future research that integrate AI into survey translation practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14472v2</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erica Ann Metheney, Lauren Yehle</dc:creator>
    </item>
  </channel>
</rss>
