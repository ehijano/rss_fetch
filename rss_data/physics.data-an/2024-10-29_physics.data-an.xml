<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 29 Oct 2024 04:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A totally empirical basis of science</title>
      <link>https://arxiv.org/abs/2410.19866</link>
      <description>arXiv:2410.19866v1 Announce Type: new 
Abstract: Statistical hypothesis testing is the central method to demarcate scientific theories in both exploratory and inferential analyses. However, whether this method befits such purpose remains a matter of debate. Established approaches to hypothesis testing make several assumptions on the data generation process beyond the scientific theory. Most of these assumptions not only remain unmet in realistic datasets, but often introduce unwarranted bias in the analysis. Here, we depart from such restrictive assumptions to propose an alternative framework of total empiricism. We derive the Information-test ($I$-test) which allows for testing versatile hypotheses including non-null effects. To exemplify the adaptability of the $I$-test to application and study design, we revisit the hypothesis of interspecific metabolic scaling in mammals, ultimately rejecting both competing theories of pure allometry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19866v1</guid>
      <category>physics.data-an</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Orestis Loukas, Ho-Ryun Chung</dc:creator>
    </item>
    <item>
      <title>Estimation of Ru-97 Half-Life Using the Most Frequent Value Method and Bootstrapping Techniques</title>
      <link>https://arxiv.org/abs/2410.19988</link>
      <description>arXiv:2410.19988v1 Announce Type: new 
Abstract: A new and robust statistics was applied to previous measurements of the 97Ru half-life. This process incorporates the most frequent value (MFV) technique along with hybrid parametric bootstrap (HPB) method to deliver a more precise estimate of the 97Ru half-life. The derived value is T1/2,MFV(HPB) = 2.8385+0.0022-0.0075 days. This estimate corresponds to a 68.27% confidence interval ranging from 2.8310 to 2.8407 days, and a 95.45% confidence interval ranging from 2.8036 to 2.8485 days, calculated using the percentile method. This level of uncertainty is significantly lower-over 30 times-than the uncertainty in the previously recognized half-life value found in nuclear data sheets. Employing an alternate approach to minimization could further cut down the statistical uncertainty by 44% for the 97Ru half-life. In particular, the HPB method accounts for uncertainties in small datasets when determining the confidence interval. When the HPB method, in combination with the MFV approach, was used to review a four-element dataset of the specific activity of 39Ar based on underground data, the result was SA_MFV(HPB) = 0.966 +0.027 -0.020 Bq/kg_atmAr. This value results in a 68.27% confidence interval of 0.946 to 0.993, along with a 95.45% confidence interval of 0.921 to 1.029, also determined using the percentile method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19988v1</guid>
      <category>physics.data-an</category>
      <category>nucl-ex</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Victor V. Golovko</dc:creator>
    </item>
    <item>
      <title>Simultaneous Dimensionality Reduction for Extracting Useful Representations of Large Empirical Multimodal Datasets</title>
      <link>https://arxiv.org/abs/2410.19867</link>
      <description>arXiv:2410.19867v1 Announce Type: cross 
Abstract: The quest for simplification in physics drives the exploration of concise mathematical representations for complex systems. This Dissertation focuses on the concept of dimensionality reduction as a means to obtain low-dimensional descriptions from high-dimensional data, facilitating comprehension and analysis. We address the challenges posed by real-world data that defy conventional assumptions, such as complex interactions within neural systems or high-dimensional dynamical systems. Leveraging insights from both theoretical physics and machine learning, this work unifies diverse reduction methods under a comprehensive framework, the Deep Variational Multivariate Information Bottleneck. This framework enables the design of tailored reduction algorithms based on specific research questions. We explore and assert the efficacy of simultaneous reduction approaches over their independent reduction counterparts, demonstrating their superiority in capturing covariation between multiple modalities, while requiring less data. We also introduced novel techniques, such as the Deep Variational Symmetric Information Bottleneck, for general nonlinear simultaneous reduction. We show that the same principle of simultaneous reduction is the key to efficient estimation of mutual information. We show that our new method is able to discover the coordinates of high-dimensional observations of dynamical systems. Through analytical investigations and empirical validations, we shed light on the intricacies of dimensionality reduction methods, paving the way for enhanced data analysis across various domains. We underscore the potential of these methodologies to extract meaningful insights from complex datasets, driving advancements in fundamental research and applied sciences. As these methods evolve, they promise to deepen our understanding of complex systems and inform more effective data analysis strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19867v1</guid>
      <category>cs.LG</category>
      <category>physics.bio-ph</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eslam Abdelaleem</dc:creator>
    </item>
    <item>
      <title>SIGMA: Single Interpolated Generative Model for Anomalies</title>
      <link>https://arxiv.org/abs/2410.20537</link>
      <description>arXiv:2410.20537v1 Announce Type: cross 
Abstract: A key step in any resonant anomaly detection search is accurate modeling of the background distribution in each signal region. Data-driven methods like CATHODE accomplish this by training separate generative models on the complement of each signal region, and interpolating them into their corresponding signal regions. Having to re-train the generative model on essentially the entire dataset for each signal region is a major computational cost in a typical sliding window search with many signal regions. Here, we present SIGMA, a new, fully data-driven, computationally-efficient method for estimating background distributions. The idea is to train a single generative model on all of the data and interpolate its parameters in sideband regions in order to obtain a model for the background in the signal region. The SIGMA method significantly reduces the computational cost compared to previous approaches, while retaining a similar high quality of background modeling and sensitivity to anomalous signals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20537v1</guid>
      <category>hep-ph</category>
      <category>cs.LG</category>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ranit Das, David Shih</dc:creator>
    </item>
    <item>
      <title>Evolving interdisciplinary contributions to global societal challenges: A 50-year overview</title>
      <link>https://arxiv.org/abs/2410.20619</link>
      <description>arXiv:2410.20619v1 Announce Type: cross 
Abstract: Addressing global societal challenges necessitates insights and expertise that transcend the boundaries of individual disciplines. In recent decades, interdisciplinary collaboration has been recognised as a vital driver of innovation and effective problem-solving, with the potential to profoundly influence policy and practice worldwide. However, quantitative evidence remains limited regarding how cross-disciplinary efforts contribute to societal challenges, as well as the evolving roles and relevance of specific disciplines in addressing these issues. To fill this gap, this study examines the long-term evolution of interdisciplinary contributions to the United Nations' Sustainable Development Goals (SDGs), drawing on extensive bibliometric data from OpenAlex. By analysing publication and citation trends across 19 research fields from 1970 to 2022, we reveal how the relative presence of different disciplines in addressing particular SDGs has shifted over time. Our results also provide unique evidence of the increasing interconnection between fields since the 2000s, coinciding with the United Nations' initiative to tackle global societal challenges through interdisciplinary efforts. These insights will benefit policymakers and practitioners as they reflect on past progress and plan for future action, particularly with the SDG target deadline approaching in the next five years.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20619v1</guid>
      <category>cs.DL</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <category>physics.data-an</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keisuke Okamura</dc:creator>
    </item>
    <item>
      <title>Reconstructing dynamics from sparse observations with no training on target system</title>
      <link>https://arxiv.org/abs/2410.21222</link>
      <description>arXiv:2410.21222v1 Announce Type: cross 
Abstract: In applications, an anticipated situation is where the system of interest has never been encountered before and sparse observations can be made only once. Can the dynamics be faithfully reconstructed from the limited observations without any training data? This problem defies any known traditional methods of nonlinear time-series analysis as well as existing machine-learning methods that typically require extensive data from the target system for training. We address this challenge by developing a hybrid transformer and reservoir-computing machine-learning scheme. The key idea is that, for a complex and nonlinear target system, the training of the transformer can be conducted not using any data from the target system, but with essentially unlimited synthetic data from known chaotic systems. The trained transformer is then tested with the sparse data from the target system. The output of the transformer is further fed into a reservoir computer for predicting the long-term dynamics or the attractor of the target system. The power of the proposed hybrid machine-learning framework is demonstrated using a large number of prototypical nonlinear dynamical systems, with high reconstruction accuracy even when the available data is only 20% of that required to faithfully represent the dynamical behavior of the underlying system. The framework provides a paradigm of reconstructing complex and nonlinear dynamics in the extreme situation where training data does not exist and the observations are random and sparse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21222v1</guid>
      <category>cs.LG</category>
      <category>nlin.CD</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zheng-Meng Zhai, Jun-Yin Huang, Benjamin D. Stern, Ying-Cheng Lai</dc:creator>
    </item>
    <item>
      <title>Machine Learning based Pointing Models for Radio/Sub-millimeter Telescopes</title>
      <link>https://arxiv.org/abs/2402.08589</link>
      <description>arXiv:2402.08589v2 Announce Type: replace-cross 
Abstract: Radio, sub-millimeter and millimeter ground-based telescopes are powerful instruments for studying the gas and dust-rich regions of the Universe that are invisible at optical wavelengths, but the pointing accuracy is crucial for obtaining high-quality data. Pointing errors are small deviations of the telescope's orientation from its desired direction. The telescopes use linear regression pointing models to correct for these errors, taking into account various factors such as weather conditions, telescope mechanical structure, and the target's position in the sky. However, residual pointing errors can still occur due to factors that are hard to model accurately, such as thermal and gravitational deformation and environmental conditions like humidity and wind. Here we present a proof-of-concept for reducing pointing error for the Atacama Pathfinder EXperiment (APEX) telescope in the high-altitude Atacama Desert in Chile based on machine learning. Using historic pointing data from 2022, we trained eXtreme Gradient Boosting (XGBoost) models that reduced the Root Mean Squared Error (RMSE) for azimuth and elevation (horizontal and vertical angle) pointing corrections by 4.3% and 9.5%, respectively, on hold-out test data. Our results will inform operations of current and future facilities such as the next-generation Atacama Large Aperture Submillimeter Telescope (AtLAST).</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08589v2</guid>
      <category>astro-ph.IM</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bendik Nyheim, Signe Riemer-S{\o}rensen, Rodrigo Parra, Claudia Cicone</dc:creator>
    </item>
    <item>
      <title>Distribution of lowest eigenvalue in $k$-body bosonic random matrix ensembles</title>
      <link>https://arxiv.org/abs/2405.00190</link>
      <description>arXiv:2405.00190v2 Announce Type: replace-cross 
Abstract: We numerically study the distribution of the lowest eigenvalue of finite many-boson systems with $k$-body interactions modeled by Bosonic Embedded Gaussian Orthogonal [BEGOE($k$)] and Unitary [BEGUE($k$)] random matrix Ensembles. Following the recently published result that the $q$-normal describes the smooth form of the eigenvalue density of the $k$-body embedded ensembles, the first four moments of the distribution of lowest eigenvalues have been analyzed as a function of the $q$ parameter, with $q \sim 1$ for $k = 1$ and $q = 0$ for $k = m$; $m$ being the number of bosons. Analytics are difficult as we are dealing with highly correlated variables, however we provide ansatzs for centroids and variances of these distributions. These match very well with the numerical results obtained. Our results show the distribution exhibits a smooth transition from Gaussian like for $q$ close to 1 to a modified Gumbel like for intermediate values of $q$ to the well-known Tracy-Widom distribution for $q=0$. It should be emphasized that this is a new result which numerically demonstrates that the distribution of the lowest eigenvalue of finite many-boson systems with $k$-body interactions exhibits a smooth transition from Gaussian like (for $q$ close to 1) to a modified Gumbel like (for intermediate values of $q$) to the well-known Tracy-Widom distribution (for $q=0$). In addition, we have also studied the distribution of normalized spacing between the lowest and next lowest eigenvalues and it is seen that this distribution exhibits a transition from Wigner's surmise (for $k=1$) to Poisson (for intermediate $k$ values with $k \le m/2$) to Wigner's surmise (starting from $k = m/2$ to $k = m$) with decreasing $q$ value. Thus, the spacings at the spectrum edge behave differently from the spacings inside the spectrum bulk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00190v2</guid>
      <category>quant-ph</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>N. D. Chavda, Priyanka Rao, V. K. B. Kota, Manan Vyas</dc:creator>
    </item>
    <item>
      <title>A Bayesian mixture model approach to quantifying the empirical nuclear saturation point</title>
      <link>https://arxiv.org/abs/2405.02748</link>
      <description>arXiv:2405.02748v2 Announce Type: replace-cross 
Abstract: The equation of state (EOS) in the limit of infinite symmetric nuclear matter exhibits an equilibrium density, $n_0 \approx 0.16 \, \mathrm{fm}^{-3}$, at which the pressure vanishes and the energy per particle attains its minimum, $E_0 \approx -16 \, \mathrm{MeV}$. Although not directly measurable, the saturation point $(n_0,E_0)$ can be extrapolated by density functional theory (DFT), providing tight constraints for microscopic interactions derived from chiral effective field theory (EFT). However, when considering several DFT predictions for $(n_0,E_0)$ from Skyrme and Relativistic Mean Field models together, a discrepancy between these model classes emerges at high confidence levels that each model prediction's uncertainty cannot explain. How can we leverage these DFT constraints to rigorously benchmark saturation properties of chiral interactions? To address this question, we present a Bayesian mixture model that combines multiple DFT predictions for $(n_0,E_0)$ using an efficient conjugate prior approach. The inferred posterior for the saturation point's mean and covariance matrix follows a Normal-inverse-Wishart class, resulting in posterior predictives in the form of correlated, bivariate $t$-distributions. The DFT uncertainty reports are then used to mix these posteriors using an ordinary Monte Carlo approach. At the 95\% credibility level, we estimate $n_0 \approx 0.157 \pm 0.010 \, \mathrm{fm}^{-3}$ and $E_0 \approx -15.97 \pm 0.40 \, \mathrm{MeV}$ for the marginal (univariate) $t$-distributions. Combined with chiral EFT calculations of the pure neutron matter EOS, we obtain bivariate normal distributions for the symmetry energy and its slope parameter at $n_0$: $S_v \approx 32.0 \pm 1.1 \, \mathrm{MeV}$ and $L\approx 52.6\pm 8.1 \, \mathrm{MeV}$ (95\%), respectively. Our Bayesian framework is publicly available, so practitioners can readily use and extend our results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02748v2</guid>
      <category>nucl-th</category>
      <category>nucl-ex</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevC.110.044320</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. C 110, 044320 (2024)</arxiv:journal_reference>
      <dc:creator>C. Drischler, P. G. Giuliani, S. Bezoui, J. Piekarewicz, F. Viens</dc:creator>
    </item>
    <item>
      <title>Marine spatial planning techniques with a case study on wave-powered offshore aquaculture farms</title>
      <link>https://arxiv.org/abs/2410.11926</link>
      <description>arXiv:2410.11926v2 Announce Type: replace-cross 
Abstract: As emerging marine technologies lead to the development of new infrastructure across the ocean, they enter an environment that existing ecosystems and industries already rely on. Although necessary to provide sustainable sources of energy and food, careful planning will be important to make informed decisions and avoid conflicts. This paper examines several techniques used for marine spatial planning, an approach for analyzing and planning the use of marine resources. Using open source software including QGIS and Python, the potential for developing wave-powered offshore aquaculture farms using the RM3 wave energy converter along the Northeast coast of the United States is assessed and several feasible sites are identified. The optimal site, located at 43.7{\deg}N, 68.9{\deg}W along the coast of Maine, has a total cost for a 5-pen farm of \$56.8M, annual fish yield of 676 tonnes, and a levelized cost of fish of \$9.23 per kilogram. Overall trends indicate that the cost greatly decreases with distance to shore due to the greater availability of wave energy and that conflicts and environmental constraints significantly limit the number of feasible sites in this region.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11926v2</guid>
      <category>physics.ao-ph</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gabriel Ewig, Arezoo Hasankhani, Eugene Won, Maha Haji</dc:creator>
    </item>
  </channel>
</rss>
