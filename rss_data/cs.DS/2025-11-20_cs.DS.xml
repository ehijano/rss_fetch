<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Nov 2025 05:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Exact Learning of Weighted Graphs Using Composite Queries</title>
      <link>https://arxiv.org/abs/2511.14882</link>
      <description>arXiv:2511.14882v1 Announce Type: new 
Abstract: In this paper, we study the exact learning problem for weighted graphs, where we are given the vertex set, $V$, of a weighted graph, $G=(V,E,w)$, but we are not given $E$. The problem, which is also known as graph reconstruction, is to determine all the edges of $E$, including their weights, by asking queries about $G$ from an oracle. As we observe, using simple shortest-path length queries is not sufficient, in general, to learn a weighted graph. So we study a number of scenarios where it is possible to learn $G$ using a subquadratic number of composite queries, which combine two or three simple queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14882v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-98740-3_8</arxiv:DOI>
      <dc:creator>Michael T. Goodrich, Songyu Liu, Ioannis Panageas</dc:creator>
    </item>
    <item>
      <title>Intermediate N-Gramming: Deterministic and Fast N-Grams For Large N and Large Datasets</title>
      <link>https://arxiv.org/abs/2511.14955</link>
      <description>arXiv:2511.14955v1 Announce Type: new 
Abstract: The number of n-gram features grows exponentially in n, making it computationally demanding to compute the most frequent n-grams even for n as small as 3. Motivated by our production machine learning system built on n-gram features, we ask: is it possible to accurately, deterministically, and quickly recover the top-k most frequent n-grams? We devise a multi-pass algorithm called Intergrams that constructs candidate n-grams from the preceding (n - 1)-grams. By designing this algorithm with hardware in mind, our approach yields more than an order of magnitude speedup (up to 33x!) over the next known fastest algorithm, even when similar optimizations are applied to the other algorithm. Using the empirical power-law distribution over n-grams, we also provide theory to inform the efficacy of our multi-pass approach. Our code is available at https://github.com/rcurtin/Intergrams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14955v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryan R. Curtin, Fred Lu, Edward Raff, Priyanka Ranade</dc:creator>
    </item>
    <item>
      <title>A Dichotomy for 1-Planarity with Restricted Crossing Types Parameterized by Treewidth</title>
      <link>https://arxiv.org/abs/2511.14975</link>
      <description>arXiv:2511.14975v1 Announce Type: new 
Abstract: A drawing of a graph is 1-planar if each edge participates in at most one crossing and adjacent edges do not cross. Up to symmetry, each crossing in a 1-planar drawing belongs to one out of six possible crossing types, where a type characterizes the subgraph induced by the four vertices of the crossing edges. Each of the 63 possible nonempty subsets $\mathcal{S}$ of crossing types gives a recognition problem: does a given graph admit an $\mathcal{S}$-restricted drawing, that is, a 1-planar drawing where the crossing type of each crossing is in $\mathcal{S}$?
  We show that there is a set $\mathcal{S}_{\rm bad}$ with three crossing types and the following properties: If $\mathcal{S}$ contains no crossing type from $\mathcal{S}_{\rm bad}$, then the recognition of graphs that admit an $\mathcal{S}$-restricted drawing is fixed-parameter tractable with respect to the treewidth of the input graph. If $\mathcal{S}$ contains any crossing type from $\mathcal{S}_{\rm bad}$, then it is NP-hard to decide whether a graph has an $\mathcal{S}$-restricted drawing, even when considering graphs of constant pathwidth.
  We also extend this characterization of crossing types to 1-planar straight-line drawings and show the same complexity behaviour parameterized by treewidth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14975v1</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <category>math.CO</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sergio Cabello, Alexander Dobler, Ga\v{s}per Fijav\v{z}, Thekla Hamm, Mirko H. Wagner</dc:creator>
    </item>
    <item>
      <title>Combinatorial Optimization using Comparison Oracles</title>
      <link>https://arxiv.org/abs/2511.15142</link>
      <description>arXiv:2511.15142v1 Announce Type: new 
Abstract: In a linear combinatorial optimization problem, we are given a family $\mathcal{F} \subseteq 2^U$ of feasible subsets of a ground set $U$ of $n$ elements, and aim to find $S^* = \arg\min_{S \in \mathcal{F}} \langle w, \mathbbm{1}_S \rangle$. Traditionally, the weight vector is given, or a value oracle allows evaluating $w(S) := \langle w, \mathbbm{1}_S \rangle$. Motivated by practical interest in pairwise comparisons, and by the theoretical quest to understand computational models, we study a weaker, more robust comparison oracle that for any $S, T \in \mathcal{F}$ reveals only whether $w(S) &lt;, =, &gt; w(T)$. We ask: when can we find $S^*$ using few comparison queries, and when can this be done efficiently?
  We present three contributions: (1) We establish that the query complexity over any set system $\mathcal{F} \subseteq 2^U$ is $\tilde O(n^2)$, using the inference dimension framework, highlighting a separation between information and computational complexity (runtime may still be exponential for NP-hard problems under ETH). (2) We introduce a Global Subspace Learning (GSL) framework for objective functions with discrete integer weights bounded by $B$, giving an algorithm to sort all feasible sets using $O(nB \log(nB))$ queries, improving the $\tilde O(n^2)$ bound when $B = o(n)$. For linear matroids, algebraic techniques yield efficient algorithms for problems including $k$-SUM, SUBSET-SUM, and $A{+}B$ sorting. (3) We give the first polynomial-time, low-query algorithms for classic combinatorial problems: minimum cuts, minimum weight spanning trees (and matroid bases), bipartite matching (and matroid intersection), and shortest $s$-$t$ paths.
  Our work provides the first general query complexity bounds and efficient algorithms for this model, opening new directions for comparison-based optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15142v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Cohen-Addad, Tommaso d'Orsi, Anupam Gupta, Guru Guruganesh, Euiwoong Lee, Debmalya Panigrahi, Madhusudhan Reddy Pittu, Jon Schneider, David P. Woodruff</dc:creator>
    </item>
    <item>
      <title>Dynamic Matroids: Base Packing and Covering</title>
      <link>https://arxiv.org/abs/2511.15460</link>
      <description>arXiv:2511.15460v1 Announce Type: new 
Abstract: In this paper, we consider dynamic matroids, where elements can be inserted to or deleted from the ground set over time. The independent sets change to reflect the current ground set. As matroids are central to the study of many combinatorial optimization problems, it is a natural next step to also consider them in a dynamic setting. The study of dynamic matroids has the potential to generalize several dynamic graph problems, including, but not limited to, arboricity and maximum bipartite matching. We contribute by providing efficient algorithms for some fundamental matroid questions.
  In particular, we study the most basic question of maintaining a base dynamically, providing an essential building block for future algorithms. We further utilize this result and consider the elementary problems of base packing and base covering. We provide a deterministic algorithm that maintains a $(1\pm \varepsilon)$-approximation of the base packing number $\Phi$ in $O(\Phi \cdot \text{poly}(\log n, \varepsilon^{-1}))$ queries per update. Similarly, we provide a deterministic algorithm that maintains a $(1\pm \varepsilon)$-approximation of the base covering number $\beta$ in $O(\beta \cdot \text{poly}(\log n, \varepsilon^{-1}))$ queries per update. Moreover, we give an algorithm that maintains a $(1\pm \varepsilon)$-approximation of the base covering number $\beta$ in $O(\text{poly}(\log n, \varepsilon^{-1}))$ queries per update against an oblivious adversary.
  These results are obtained by exploring the relationship between base collections, a generalization of tree-packings, and base packing and covering respectively. We provide structural theorems to formalize these connections, and show how they lead to simple dynamic algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15460v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tijn de Vos, Mara Grilnberger</dc:creator>
    </item>
    <item>
      <title>Computing Power Indices in Weighted Majority Games with Formal Power Series</title>
      <link>https://arxiv.org/abs/2511.14995</link>
      <description>arXiv:2511.14995v1 Announce Type: cross 
Abstract: In this paper, we propose fast pseudo-polynomial-time algorithms for computing power indices in weighted majority games. We show that we can compute the Banzhaf index for all players in $O(n+q\log (q))$ time, where $n$ is the number of players and $q$ is a given quota. Moreover, we prove that the Shapley--Shubik index for all players can be computed in $O(nq\log (q))$ time. Our algorithms are faster than existing algorithms when $q=2^{o(n)}$. Our algorithms exploit efficient computation techniques for formal power series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14995v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naonori Kakimura, Yoshihiko Terai</dc:creator>
    </item>
    <item>
      <title>Sample-Adaptivity Tradeoff in On-Demand Sampling</title>
      <link>https://arxiv.org/abs/2511.15507</link>
      <description>arXiv:2511.15507v1 Announce Type: cross 
Abstract: We study the tradeoff between sample complexity and round complexity in on-demand sampling, where the learning algorithm adaptively samples from $k$ distributions over a limited number of rounds. In the realizable setting of Multi-Distribution Learning (MDL), we show that the optimal sample complexity of an $r$-round algorithm scales approximately as $dk^{\Theta(1/r)} / \epsilon$. For the general agnostic case, we present an algorithm that achieves near-optimal sample complexity of $\widetilde O((d + k) / \epsilon^2)$ within $\widetilde O(\sqrt{k})$ rounds. Of independent interest, we introduce a new framework, Optimization via On-Demand Sampling (OODS), which abstracts the sample-adaptivity tradeoff and captures most existing MDL algorithms. We establish nearly tight bounds on the round complexity in the OODS setting. The upper bounds directly yield the $\widetilde O(\sqrt{k})$-round algorithm for agnostic MDL, while the lower bounds imply that achieving sub-polynomial round complexity would require fundamentally new techniques that bypass the inherent hardness of OODS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15507v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nika Haghtalab, Omar Montasser, Mingda Qiao</dc:creator>
    </item>
    <item>
      <title>B+ANN: A Fast Billion-Scale Disk-based Nearest-Neighbor Index</title>
      <link>https://arxiv.org/abs/2511.15557</link>
      <description>arXiv:2511.15557v1 Announce Type: cross 
Abstract: Storing and processing of embedding vectors by specialized Vector databases (VDBs) has become the linchpin in building modern AI pipelines. Most current VDBs employ variants of a graph-based ap- proximate nearest-neighbor (ANN) index algorithm, HNSW, to an- swer semantic queries over stored vectors. Inspite of its wide-spread use, the HNSW algorithm suffers from several issues: in-memory design and implementation, random memory accesses leading to degradation in cache behavior, limited acceleration scope due to fine-grained pairwise computations, and support of only semantic similarity queries. In this paper, we present a novel disk-based ANN index, B+ANN, to address these issues: it first partitions input data into blocks containing semantically similar items, then builds an B+ tree variant to store blocks both in-memory and on disks, and finally, enables hybrid edge- and block-based in-memory traversals. As demonstrated by our experimantal evaluation, the proposed B+ANN disk-based index improves both quality (Recall value), and execution performance (Queries per second/QPS) over HNSW, by improving spatial and temporal locality for semantic operations, reducing cache misses (19.23% relative gain), and decreasing the memory consumption and disk-based build time by 24x over the DiskANN algorithm. Finally, it enables dissimilarity queries, which are not supported by similarity-oriented ANN indices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15557v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Selim Furkan Tekin, Rajesh Bordawekar</dc:creator>
    </item>
    <item>
      <title>Tokenisation over Bounded Alphabets is Hard</title>
      <link>https://arxiv.org/abs/2511.15709</link>
      <description>arXiv:2511.15709v1 Announce Type: cross 
Abstract: Recent works have shown that tokenisation is NP-complete. However, these works assume tokenisation is applied to inputs with unboundedly large alphabets -- an unrealistic assumption, given that in practice tokenisers operate over fixed-size alphabets, such as bytes or Unicode characters. We close this gap by analysing tokenisation over bounded $n$-ary alphabets, considering two natural variants: bottom-up tokenisation and direct tokenisation, where we must, respectively, select a sequence of merge operations or a vocabulary whose application optimally compresses a dataset. First, we note that proving hardness results for an $n$-ary alphabet proves the same results for alphabets of any larger size. We then prove that even with binary alphabets, both variants are not only NP-complete, but admit no polynomial-time approximation scheme (unless P=NP). We further show that direct tokenisation remains NP-complete even when applied to unary alphabets. While unary alphabets may not be practically useful, this result establishes that the computational intractability of tokenisation is not an artifact of large alphabets or complex constructions, but a fundamental barrier. Overall, our results explain why practical algorithms such as BPE and UnigramLM are heuristic, and points toward approximation algorithms being an important path going forward for tokenisation research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15709v1</guid>
      <category>cs.CL</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Violeta Kastreva, Philip Whittington, Dennis Komm, Tiago Pimentel</dc:creator>
    </item>
    <item>
      <title>Weighted Reservoir Sampling With Replacement from Data Streams</title>
      <link>https://arxiv.org/abs/2403.20256</link>
      <description>arXiv:2403.20256v5 Announce Type: replace 
Abstract: In this work, we present a new random sampling method for data streams where the probability of an element's inclusion in the sample is proportional to a weight associated with that element. Our method is based on sampling with replacement, although most of the literature on this topic has focused on sampling without replacement. Our algorithm generates a weighted random sample in one pass over a population of unknown size. At any point in time, the sample is representative of the population seen so far and can be directly used by other modules without requiring any post-processing. We formally prove the correctness and efficiency of our method. An experimental analysis shows the performance of our method in practice when compared to state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20256v5</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adriano Meligrana, Adriano Fazzone</dc:creator>
    </item>
    <item>
      <title>Expander Decomposition for Non-Uniform Vertex Measures</title>
      <link>https://arxiv.org/abs/2510.23913</link>
      <description>arXiv:2510.23913v2 Announce Type: replace 
Abstract: A $(\phi,\epsilon)$-expander-decomposition of a graph $G$ (with $n$ vertices and $m$ edges) is a partition of $V$ into clusters $V_1,\ldots,V_k$ with conductance $\Phi(G[V_i]) \ge \phi$, such that there are $O(\epsilon m)$ inter-cluster edges. Such a decomposition plays a crucial role in many graph algorithms. [Agassy, Dorman, and Kaplan, ICALP 2023] (ADK) gave a randomized $\tilde{O}(m)$ time algorithm for computing a $(\phi, \phi\log^2 {n})$-expander decomposition.
  In this paper we generalize this result for a broader notion of expansion. Let $\mu \in \mathbb{R}_{\ge 0 }^n$ be a vertex measure. A standard generalization of conductance of a cut $(S,\overline{S})$ is its $\mu$-expansion $\Phi^{\mu}_G(S,\overline{S}) = |E(S,\overline{S})|/\min \{\mu(S),\mu(\overline{S})\}$, where $\mu(S) = \sum_{v\in S} \mu(v)$.
  We present a randomized $\tilde{O}(m)$ time algorithm for computing a $(\phi, \phi \log^2 {n}\cdot\frac{\mu(V)}{m})$-expander decomposition with respect to $\mu$-expansion. A substantial portion of the exposition is adapted from ADK, and this work serves as a convenient reference for generalized expander decomposition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23913v2</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Agassy, Dani Dorfman, Haim Kaplan</dc:creator>
    </item>
    <item>
      <title>Tight Bounds for Sampling q-Colorings via Coupling from the Past</title>
      <link>https://arxiv.org/abs/2511.04982</link>
      <description>arXiv:2511.04982v2 Announce Type: replace 
Abstract: The Coupling from the Past (CFTP) paradigm is a canonical method for perfect sampling. For uniform sampling of proper $q$-colorings in graphs with maximum degree $\Delta$, the bounding chains of Huber (STOC 1998) provide a systematic framework for efficiently implementing CFTP algorithms within the classical regime $q \ge (1 + o(1))\Delta^2$. This was subsequently improved to $q &gt; 3\Delta$ by Bhandari and Chakraborty (STOC 2020) and to $q \ge (8/3 + o(1))\Delta$ by Jain, Sah, and Sawhney (STOC 2021).
  In this work, we establish the asymptotically tight threshold for bounding-chain-based CFTP algorithms for graph colorings. We prove a lower bound showing that all such algorithms satisfying the standard contraction property require $q \ge 2.5\Delta$, and we present an efficient CFTP algorithm that achieves this asymptotically optimal threshold $q \ge (2.5 + o(1))\Delta$ via an optimal design of bounding chains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04982v2</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianxing Ding, Hongyang Liu, Yitong Yin, Can Zhou</dc:creator>
    </item>
    <item>
      <title>Acceleration for Distributed Transshipment and Parallel Maximum Flow</title>
      <link>https://arxiv.org/abs/2511.06581</link>
      <description>arXiv:2511.06581v2 Announce Type: replace 
Abstract: We combine several recent advancements to solve $(1+\varepsilon)$-transshipment and $(1+\varepsilon)$-maximum flow with a parallel algorithm with $\tilde{O}(1/\varepsilon)$ depth and $\tilde{O}(m/\varepsilon)$ work. We achieve this by developing and deploying suitable parallel linear cost approximators in conjunction with an accelerated continuous optimization framework known as the box-simplex game by Jambulapati et al. (ICALP 2022). A linear cost approximator is a linear operator that allows us to efficiently estimate the cost of the optimal solution to a given routing problem. Obtaining accelerated $\varepsilon$ dependencies for both problems requires developing a stronger multicommodity cost approximator, one where cancellations between different commodities are disallowed. For maximum flow, we observe that a recent linear cost approximator due to Agarwal et al. (SODA 2024) can be augmented with additional parallel operations and achieve $\varepsilon^{-1}$ dependency via the box-simplex game.
  For transshipment, we also construct a deterministic and distributed approximator. This yields a deterministic CONGEST algorithm that requires $\tilde{O}(\varepsilon^{-1}(D + \sqrt{n}))$ rounds on general networks of hop diameter $D$ and $\tilde{O}(\varepsilon^{-1}D)$ rounds on minor-free networks to compute a $(1+\varepsilon)$-approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06581v2</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Grunau, Rasmus Kyng, Goran Zuzic</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Provably Efficient Algorithms to Estimate Shapley Values</title>
      <link>https://arxiv.org/abs/2506.05216</link>
      <description>arXiv:2506.05216v2 Announce Type: replace-cross 
Abstract: Shapley values have emerged as a critical tool for explaining which features impact the decisions made by machine learning models. However, computing exact Shapley values is difficult, generally requiring an exponential (in the feature dimension) number of model evaluations. To address this, many model-agnostic randomized estimators have been developed, the most influential and widely used being the KernelSHAP method (Lundberg &amp; Lee, 2017). While related estimators such as unbiased KernelSHAP (Covert &amp; Lee, 2021) and LeverageSHAP (Musco &amp; Witter, 2025) are known to satisfy theoretical guarantees, bounds for KernelSHAP have remained elusive. We describe a broad and unified framework that encompasses KernelSHAP and related estimators constructed using both with and without replacement sampling strategies. We then prove strong non-asymptotic theoretical guarantees that apply to all estimators from our framework. This provides, to the best of our knowledge, the first theoretical guarantees for KernelSHAP and sheds further light on tradeoffs between existing estimators. Through comprehensive benchmarking on small and medium dimensional datasets for Decision-Tree models, we validate our approach against exact Shapley values, consistently achieving low mean squared error with modest sample sizes. Furthermore, we make specific implementation improvements to enable scalability of our methods to high-dimensional datasets. Our methods, tested on datasets such MNIST and CIFAR10, provide consistently better results compared to the KernelSHAP library.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05216v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>quant-ph</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tyler Chen, Akshay Seshadri, Mattia J. Villani, Pradeep Niroula, Shouvanik Chakrabarti, Archan Ray, Pranav Deshpande, Romina Yalovetzky, Marco Pistoia, Niraj Kumar</dc:creator>
    </item>
    <item>
      <title>Computational Complexity in Property Testing</title>
      <link>https://arxiv.org/abs/2510.05927</link>
      <description>arXiv:2510.05927v3 Announce Type: replace-cross 
Abstract: We initiate a systematic study of the computational complexity of property testing, focusing on the relationship between query and time complexity. While traditional work in property testing has emphasized query complexity, relatively little is known about the computational hardness of property testers. Our goal is to chart the landscape of time-query interplay and develop tools for proving time complexity lower bounds. Our first contribution is a pair of time-query hierarchy theorems for property testing. For all suitable nondecreasing functions $q(n)$ and $t(n)$ with $t(n)\geq q(n)$, we construct properties with query complexity $\tilde{\Theta}(q(n))$ and time complexity $\tilde\Omega(t(n))$. Our weak hierarchy holds unconditionally, whereas the strong version-assuming the Strong Exponential Time Hypothesis-provides better control over the time complexity of the constructed properties.
  We then turn to halfspaces in $\mathbb{R}^d$, a fundamental class in property testing and learning theory. We study the problem of approximating the distance from the input function to the nearest halfspace within additive error $\epsilon$. For the distribution-free distance approximation problem, known algorithms achieve query complexity $O(d/\epsilon^2)$, but take time $\tilde{\Theta}(1/\epsilon^d)$. We provide a fine-grained justification for this gap: assuming the $k$-SUM conjecture, any algorithm must have running time ${\Omega}(1/\epsilon^{d/2})$. This fine-grained lower bound yields a provable separation between query and time complexity for a natural and well-studied (tolerant) testing problem. We also prove that any Statistical Query (SQ) algorithm under the standard Gaussian distribution requires $(1/\epsilon)^{\Omega(d)}$ queries if the queries are answered with additive error up to $\epsilon^{\Omega(d)}$, revealing a fundamental barrier even in the distribution-specific setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05927v3</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Renato Ferreira Pinto Jr., Diptaksho Palit, Sofya Raskhodnikova</dc:creator>
    </item>
  </channel>
</rss>
