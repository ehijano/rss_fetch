<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Sep 2025 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Trustworthy Second-hand Marketplace for Built Environment</title>
      <link>https://arxiv.org/abs/2509.04085</link>
      <description>arXiv:2509.04085v1 Announce Type: cross 
Abstract: The construction industry faces significant challenges regarding material waste and sustainable practices, necessitating innovative solutions that integrate automation, traceability, and decentralised decision-making to enable efficient material reuse. This paper presents a blockchain-enabled digital marketplace for sustainable construction material reuse, ensuring transparency and traceability using InterPlanetary File System (IPFS). The proposed framework enhances trust and accountability in material exchange, addressing key challenges in industrial automation and circular supply chains. A framework has been developed to demonstrate the operational processes of the marketplace, illustrating its practical application and effectiveness. Our contributions show how the marketplace can facilitate the efficient and trustworthy exchange of reusable materials, representing a substantial step towards more sustainable construction practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04085v1</guid>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stanly Wilson, Kwabena Adu-Duodu, Yinhao Li, Ringo Sham, Yingli Wang, Ellis Solaiman, Charith Perera, Rajiv Ranjan, Omer Rana</dc:creator>
    </item>
    <item>
      <title>How many patients could we save with LLM priors?</title>
      <link>https://arxiv.org/abs/2509.04250</link>
      <description>arXiv:2509.04250v1 Announce Type: cross 
Abstract: Imagine a world where clinical trials need far fewer patients to achieve the same statistical power, thanks to the knowledge encoded in large language models (LLMs). We present a novel framework for hierarchical Bayesian modeling of adverse events in multi-center clinical trials, leveraging LLM-informed prior distributions. Unlike data augmentation approaches that generate synthetic data points, our methodology directly obtains parametric priors from the model. Our approach systematically elicits informative priors for hyperparameters in hierarchical Bayesian models using a pre-trained LLM, enabling the incorporation of external clinical expertise directly into Bayesian safety modeling. Through comprehensive temperature sensitivity analysis and rigorous cross-validation on real-world clinical trial data, we demonstrate that LLM-derived priors consistently improve predictive performance compared to traditional meta-analytical approaches. This methodology paves the way for more efficient and expert-informed clinical trial design, enabling substantial reductions in the number of patients required to achieve robust safety assessment and with the potential to transform drug safety monitoring and regulatory decision making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04250v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.IR</category>
      <category>stat.AP</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shota Arai, David Selby, Andrew Vargo, Sebastian Vollmer</dc:creator>
    </item>
    <item>
      <title>Machine Intelligence on Wireless Edge Networks</title>
      <link>https://arxiv.org/abs/2506.12210</link>
      <description>arXiv:2506.12210v2 Announce Type: replace 
Abstract: Machine intelligence on edge devices enables low-latency processing and improved privacy, but is often limited by the energy and delay of moving and converting data. Current systems frequently avoid local model storage by sending queries to a server, incurring uplink cost, network latency, and privacy risk. We present the opposite approach: broadcasting model weights to clients that perform inference locally using in-physics computation inside the radio receive chain. A base station transmits weights as radio frequency (RF) waveforms; the client encodes activations onto the waveform and computes the result using existing mixer and filter stages, RF components already present in billions of edge devices such as cellphones, eliminating repeated signal conversions and extra hardware. Analysis shows that thermal noise and nonlinearity create an optimal energy window for accurate analog inner products. Hardware-tailored training through a differentiable RF chain preserves accuracy within this regime. Circuit-informed simulations, consistent with a companion experiment, demonstrate reduced memory and conversion overhead while maintaining high accuracy in realistic wireless edge scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12210v2</guid>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sri Krishna Vadlamani, Kfir Sulimany, Zhihui Gao, Tingjun Chen, Dirk Englund</dc:creator>
    </item>
    <item>
      <title>Spin-NeuroMem: A Low-Power Neuromorphic Associative Memory Design Based on Spintronic Devices</title>
      <link>https://arxiv.org/abs/2404.02463</link>
      <description>arXiv:2404.02463v3 Announce Type: replace-cross 
Abstract: Biologically-inspired computing models have made significant progress in recent years, but the conventional von Neumann architecture is inefficient for the large-scale matrix operations and massive parallelism required by these models. This paper presents Spin-NeuroMem, a low-power circuit design of Hopfield network for the function of associative memory. Spin-NeuroMem is equipped with energy-efficient spintronic synapses which utilize magnetic tunnel junctions (MTJs) to store weight matrices of multiple associative memories. The proposed synapse design achieves as low as 17.4% power consumption compared to the state-of-the-art synapse designs. Spin-NeuroMem also encompasses a novel voltage converter with a 53.3% reduction in transistor usage for effective Hopfield network computation. In addition, we propose an associative memory simulator for the first time, which achieves a 5Mx speedup with a comparable associative memory effect. By harnessing the potential of spintronic devices, this work paves the way for the development of energy-efficient and scalable neuromorphic computing systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02463v3</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <category>physics.app-ph</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Siqing Fu, Lizhou Wu, Tiejun Li, Chunyuan Zhang, Jianmin Zhang, Sheng Ma</dc:creator>
    </item>
  </channel>
</rss>
