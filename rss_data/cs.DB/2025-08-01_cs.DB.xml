<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 01 Aug 2025 04:00:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>AutoIndexer: A Reinforcement Learning-Enhanced Index Advisor Towards Scaling Workloads</title>
      <link>https://arxiv.org/abs/2507.23084</link>
      <description>arXiv:2507.23084v1 Announce Type: new 
Abstract: Efficiently selecting indexes is fundamental to database performance optimization, particularly for systems handling large-scale analytical workloads. While deep reinforcement learning (DRL) has shown promise in automating index selection through its ability to learn from experience, few works address how these RL-based index advisors can adapt to scaling workloads due to exponentially growing action spaces and heavy trial and error. To address these challenges, we introduce AutoIndexer, a framework that combines workload compression, query optimization, and specialized RL models to scale index selection effectively. By operating on compressed workloads, AutoIndexer substantially lowers search complexity without sacrificing much index quality. Extensive evaluations show that it reduces end-to-end query execution time by up to 95% versus non-indexed baselines. On average, it outperforms state-of-the-art RL-based index advisors by approximately 20% in workload cost savings while cutting tuning time by over 50%. These results affirm AutoIndexer's practicality for large and diverse workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23084v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Taiyi Wang, Eiko Yoneki</dc:creator>
    </item>
    <item>
      <title>Jelly-Patch: a Fast Format for Recording Changes in RDF Datasets</title>
      <link>https://arxiv.org/abs/2507.23499</link>
      <description>arXiv:2507.23499v1 Announce Type: new 
Abstract: Recording data changes in RDF systems is a crucial capability, needed to support auditing, incremental backups, database replication, and event-driven workflows. In large-scale and low-latency RDF applications, the high volume and frequency of updates can cause performance bottlenecks in the serialization and transmission of changes. To alleviate this, we propose Jelly-Patch -- a high-performance, compressed binary serialization format for changes in RDF datasets. To evaluate its performance, we benchmark Jelly-Patch against existing RDF Patch formats, using two datasets representing different use cases (change data capture and IoT streams). Jelly-Patch is shown to achieve 3.5--8.9x better compression, and up to 2.5x and 4.6x higher throughput in serialization and parsing, respectively. These significant advancements in throughput and compression are expected to improve the performance of large-scale and low-latency RDF systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23499v1</guid>
      <category>cs.DB</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Piotr Sowinski, Kacper Grzymkowski, Anastasiya Danilenka</dc:creator>
    </item>
    <item>
      <title>DataLens: Enhancing Dataset Discovery via Network Topologies</title>
      <link>https://arxiv.org/abs/2507.23515</link>
      <description>arXiv:2507.23515v1 Announce Type: new 
Abstract: The rapid growth of publicly available textual resources, such as lexicons and domain-specific corpora, presents challenges in efficiently identifying relevant resources. While repositories are emerging, they often lack advanced search and exploration features. Most search methods rely on keyword queries and metadata filtering, which require prior knowledge and fail to reveal connections between resources. To address this, we present DataLens, a web-based platform that combines faceted search with advanced visualization techniques to enhance resource discovery. DataLens offers network-based visualizations, where the network structure can be adapted to suit the specific analysis task. It also supports a chained views approach, enabling users to explore data from multiple perspectives. A formative user study involving six data practitioners revealed that users highly value visualization tools-especially network-based exploration-and offered insights to help refine our approach to better support dataset search.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23515v1</guid>
      <category>cs.DB</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ana\"is Ollagnier (CRISAM, CNRS, MARIANNE), Aline Menin (WIMMICS, Laboratoire I3S - SPARKS)</dc:creator>
    </item>
    <item>
      <title>Text-to-SQL Task-oriented Dialogue Ontology Construction</title>
      <link>https://arxiv.org/abs/2507.23358</link>
      <description>arXiv:2507.23358v1 Announce Type: cross 
Abstract: Large language models (LLMs) are widely used as general-purpose knowledge sources, but they rely on parametric knowledge, limiting explainability and trustworthiness. In task-oriented dialogue (TOD) systems, this separation is explicit, using an external database structured by an explicit ontology to ensure explainability and controllability. However, building such ontologies requires manual labels or supervised training. We introduce TeQoDO: a Text-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM autonomously builds a TOD ontology from scratch without supervision using its inherent SQL programming capabilities combined with dialogue theory provided in the prompt. We show that TeQoDO outperforms transfer learning approaches, and its constructed ontology is competitive on a downstream dialogue state tracking task. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also scales to allow construction of much larger ontologies, which we investigate on a Wikipedia and ArXiv dataset. We view this as a step towards broader application of ontologies to increase LLM explainability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23358v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.IR</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renato Vukovic, Carel van Niekerk, Michael Heck, Benjamin Ruppik, Hsien-Chin Lin, Shutong Feng, Nurul Lubis, Milica Gasic</dc:creator>
    </item>
    <item>
      <title>Chatting with your ERP: A Recipe</title>
      <link>https://arxiv.org/abs/2507.23429</link>
      <description>arXiv:2507.23429v1 Announce Type: cross 
Abstract: This paper presents the design, implementation, and evaluation behind a Large Language Model (LLM) agent that chats with an industrial production-grade ERP system. The agent is capable of interpreting natural language queries and translating them into executable SQL statements, leveraging open-weight LLMs. A novel dual-agent architecture combining reasoning and critique stages was proposed to improve query generation reliability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23429v1</guid>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jorge Ruiz G\'omez, Lidia Andr\'es Susinos, Jorge Alamo Oliv\'e, Sonia Rey Osorno, Manuel Luis Gonzalez Hern\'andez</dc:creator>
    </item>
    <item>
      <title>Jelly: a Fast and Convenient RDF Serialization Format</title>
      <link>https://arxiv.org/abs/2506.11298</link>
      <description>arXiv:2506.11298v2 Announce Type: replace 
Abstract: Existing RDF serialization formats such as Turtle, N-Quads, and JSON-LD are widely used for communication and storage in knowledge graph and Semantic Web applications. However, they suffer from limitations in performance, compression ratio, and lack of native support for RDF streams. To address these shortcomings, we introduce Jelly, a fast and convenient binary serialization format for RDF data that supports both batch and streaming use cases. Jelly is designed to maximize serialization throughput, reduce file size with lightweight streaming compression, and minimize compute resource usage. Built on Protocol Buffers, Jelly is easy to integrate with modern programming languages and RDF libraries. To maximize reusability, Jelly has an open protocol specification, open-source implementations in Java and Python integrated with popular RDF libraries, and a versatile command-line tool. To illustrate its usefulness, we outline concrete use cases where Jelly can provide tangible benefits. We consider that by combining practical usability with state-of-the-art efficiency, Jelly is an important contribution to the Semantic Web tool stack.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11298v2</guid>
      <category>cs.DB</category>
      <category>cs.NI</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Piotr Sowinski, Karolina Bogacka, Anastasiya Danilenka, Nikita Kozlov</dc:creator>
    </item>
    <item>
      <title>Towards Serverless Processing of Spatiotemporal Big Data Queries</title>
      <link>https://arxiv.org/abs/2507.06005</link>
      <description>arXiv:2507.06005v2 Announce Type: replace 
Abstract: Spatiotemporal data are being produced in continuously growing volumes by a variety of data sources and a variety of application fields rely on rapid analysis of such data. Existing systems such as PostGIS or MobilityDB usually build on relational database systems, thus, inheriting their scale-out characteristics. As a consequence, big spatiotemporal data scenarios still have limited support even though many query types can easily be parallelized. In this paper, we propose our vision of a native serverless data processing approach for spatiotemporal data: We break down queries into small subqueries which then leverage the near-instant scaling of Function-as-a-Service platforms to execute them in parallel. With this, we partially solve the scalability needs of big spatiotemporal data processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06005v2</guid>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Diana Baumann, Tim C. Rese, David Bermbach</dc:creator>
    </item>
    <item>
      <title>Is SHACL Suitable for Data Quality Assessment?</title>
      <link>https://arxiv.org/abs/2507.22305</link>
      <description>arXiv:2507.22305v2 Announce Type: replace 
Abstract: Knowledge graphs have been widely adopted in both enterprises, such as the Google Knowledge Graph, and open platforms like Wikidata, to represent domain knowledge and support artificial intelligence applications. They model real-world information as nodes and edges. To embrace flexibility, knowledge graphs often lack enforced schemas (i.e., ontologies), leading to potential data quality issues, such as semantically overlapping nodes. Yet ensuring their quality is essential, as issues in the data can affect applications relying on them. To assess the quality of knowledge graphs, existing works propose either high-level frameworks comprising various data quality dimensions without concrete implementations, define tools that measure data quality with ad-hoc SPARQL queries, or promote the usage of constraint languages, such as the Shapes Constraint Language (SHACL), to assess and improve the quality of the graph. Although the latter approaches claim to address data quality assessment, none of them comprehensively tries to cover all data quality dimensions. In this paper, we explore this gap by investigating the extent to which SHACL can be used to assess data quality in knowledge graphs. Specifically, we defined SHACL shapes for 69 data quality metrics proposed by Zaveri et al. [1] and implemented a prototype that automatically instantiates these shapes and computes the corresponding data quality measures from their validation results. All resources are provided for repeatability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22305v2</guid>
      <category>cs.DB</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carolina Cort\'es, Lisa Ehrlinger, Lorena Etcheverry, Felix Naumann</dc:creator>
    </item>
    <item>
      <title>SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases</title>
      <link>https://arxiv.org/abs/2507.22701</link>
      <description>arXiv:2507.22701v2 Announce Type: replace 
Abstract: The co-location of multiple database instances on resource constrained edge nodes creates significant cache contention, where traditional schemes are inefficient and unstable under dynamic workloads. To address this, we present SAM(a Stability-Aware Manager), an autonomic cache manager that establishes decision stability as a first-class design principle. It achieves this through its core control policy, AURA(Autonomic Utility-balancing Resource Allocator), which resolves the classic exploitation-exploration dilemma by synthesizing two orthogonal factors: the H-factor, representing proven historical efficiency (exploitation), and the V-factor, for estimated marginal gain (exploration). Through this practical synthesis and adaptive control, SAM achieves sustained high performance with strategic stability and robustness in volatile conditions.
  Extensive experiments against 14 diverse baselines demonstrate SAM's superiority. It achieves top-tier throughput while being uniquely resilient to complex workload shifts and adversarial workloads like cache pollution. Furthermore, its decision latency is highly scalable, remaining nearly constant as the system grows to 120 databases. Crucially, SAM achieves superior decision stability -- maintaining consistent optimization directions despite noise, avoiding performance oscillations while ensuring predictable Quality of Service. These results prove that a principled, stability-aware design is essential for sustained high performance in real-world, large-scale systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22701v2</guid>
      <category>cs.DB</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoran Zhang, Decheng Zuo, Yu Yan, Zhiyu Liang, Hongzhi Wang</dc:creator>
    </item>
    <item>
      <title>Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and Ethics</title>
      <link>https://arxiv.org/abs/2506.12365</link>
      <description>arXiv:2506.12365v2 Announce Type: replace-cross 
Abstract: This survey paper outlines the key developments in the field of Large Language Models (LLMs), including enhancements to their reasoning skills, adaptability to various tasks, increased computational efficiency, and the ability to make ethical decisions. The techniques that have been most effective in bridging the gap between human and machine communications include the Chain-of-Thought prompting, Instruction Tuning, and Reinforcement Learning from Human Feedback. The improvements in multimodal learning and few-shot or zero-shot techniques have further empowered LLMs to handle complex jobs with minor input. A significant focus is placed on efficiency, detailing scaling strategies, optimization techniques, and the influential Mixture-of-Experts (MoE) architecture, which strategically routes inputs to specialized subnetworks to boost predictive accuracy, while optimizing resource allocation. This survey also offers a broader perspective on recent advancements in LLMs, going beyond isolated aspects such as model architecture or ethical concerns. Additionally, it explores the role of LLMs in Agentic AI and their use as Autonomous Decision-Making Systems, and categorizes emerging methods that enhance LLM reasoning, efficiency, and ethical alignment. The survey also identifies underexplored areas such as interpretability, cross-modal integration, and sustainability. While significant advancements have been made in LLMs, challenges such as high computational costs, biases, and ethical risks remain. Overcoming these requires a focus on bias mitigation, transparent decision-making, and explicit ethical guidelines. Future research will generally focus on enhancing the model's ability to handle multiple inputs, thereby making it more intelligent, safe, and reliable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12365v2</guid>
      <category>cs.CL</category>
      <category>cs.DB</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Asifullah Khan, Muhammad Zaeem Khan, Saleha Jamshed, Sadia Ahmad, Aleesha Zainab, Kaynat Khatib, Faria Bibi, Abdul Rehman</dc:creator>
    </item>
  </channel>
</rss>
