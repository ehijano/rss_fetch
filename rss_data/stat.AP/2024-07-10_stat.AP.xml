<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Jul 2024 01:35:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 10 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Simple, Statistically Robust Test of Discrimination</title>
      <link>https://arxiv.org/abs/2407.06539</link>
      <description>arXiv:2407.06539v1 Announce Type: new 
Abstract: In observational studies of discrimination, the most common statistical approaches consider either the rate at which decisions are made (benchmark tests) or the success rate of those decisions (outcome tests). Both tests, however, have well-known statistical limitations, sometimes suggesting discrimination even when there is none. Despite the fallibility of the benchmark and outcome tests individually, here we prove a surprisingly strong statistical guarantee: under a common non-parametric assumption, at least one of the two tests must be correct; consequently, when both tests agree, they are guaranteed to yield correct conclusions. We present empirical evidence that the underlying assumption holds approximately in several important domains, including lending, education, and criminal justice -- and that our hybrid test is robust to the moderate violations of the assumption that we observe in practice. Applying this approach to 2.8 million police stops across California, we find evidence of widespread racial discrimination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06539v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johann D. Gaebler, Sharad Goel</dc:creator>
    </item>
    <item>
      <title>TCKIN: A Novel Integrated Network Model for Predicting Mortality Risk in Sepsis Patients</title>
      <link>https://arxiv.org/abs/2407.06560</link>
      <description>arXiv:2407.06560v1 Announce Type: new 
Abstract: Sepsis poses a major global health threat, accounting for millions of deaths annually and significant economic costs. Accurate predictions of mortality risk in sepsis patients facilitate the efficient allocation of medical resources, thereby enhancing patient survival and quality of life. Through precise risk assessments, healthcare facilities can effectively distribute intensive care beds, medical equipment, and staff, ensuring high-risk patients receive timely and appropriate care. Early identification and intervention significantly decrease mortality rates and improve patient outcomes. Current methods typically utilize only one type of data--either constant, temporal, or ICD codes. This study introduces the Time-Constant KAN Integrated Network(TCKIN), an innovative model that enhances the accuracy of sepsis mortality risk predictions by integrating both temporal and constant data from electronic health records and ICD codes. Validated against the MIMIC-III and MIMIC-IV datasets, TCKIN surpasses existing machine learning and deep learning methods in accuracy, sensitivity, and specificity. Notably, TCKIN achieved AUCs of 87.76% and 88.07%, demonstrating superior capability in identifying high-risk patients. Additionally, TCKIN effectively combats the prevalent issue of data imbalance in clinical settings, improving the detection of patients at elevated risk of mortality and facilitating timely interventions. These results confirm the model's effectiveness and its potential to transform patient management and treatment optimization in clinical practice. With this advanced risk assessment tool, healthcare providers can devise more tailored treatment plans, optimize resource utilization, and ultimately enhance survival rates and quality of life for sepsis patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06560v1</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fanglin Dong</dc:creator>
    </item>
    <item>
      <title>Extending the blended generalized extreme value distribution</title>
      <link>https://arxiv.org/abs/2407.06875</link>
      <description>arXiv:2407.06875v1 Announce Type: new 
Abstract: The generalized extreme value (GEV) distribution is commonly employed to help estimate the likelihood of extreme events in many geophysical and other application areas. The recently proposed blended generalized extreme value (bGEV) distribution modifies the GEV with positive shape parameter to avoid a hard lower bound that complicates fitting and inference. Here, the bGEV is extended to the GEV with negative shape parameter, avoiding a hard upper bound that is unrealistic in many applications. This extended bGEV is shown to improve on the GEV for forecasting future heat extremes based on past data. Software implementing this bGEV and applying it to the example temperature data is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06875v1</guid>
      <category>stat.AP</category>
      <category>physics.ao-ph</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nir Y. Krakauer</dc:creator>
    </item>
    <item>
      <title>Investigating opioid vulnerability profiles through a spatial Kalman filter: insights from social and environmental factors</title>
      <link>https://arxiv.org/abs/2407.07008</link>
      <description>arXiv:2407.07008v1 Announce Type: new 
Abstract: This study investigates the national opioid crisis by exploring three key social and environmental drivers: opioid-related mortality rates, opioid prescription dispensing rates, and disability rank ordered rates. We introduce a spatial Kalman filter to examine each factor individually and their potential intra-relationships. This study utilizes county level data, spanning the years 2014 through 2020, on the rates of opioid-related mortality, opioid prescription dispensing, and disability. To successfully estimate and predict trends in these opioid-related socioenvironmental factors, we augment the Kalman Filter with a novel spatial component. Through this construction we analyze opioid vulnerability at the national scale, creating heat maps and a hotspot analysis. Our spatial Kalman filter demonstrates strong predictive performance. These predictions are used to uncover pronounced vulnerability profiles for both the rates of opioid-related mortality and disability, finding that the Appalachian region is likely the nation's most vulnerable area to these two factors. This contrasts with a chaotic vulnerability profile found for the dispensing rates. We suggest that the rates of opioid-related mortality and disability are connected and propose that this connection is unlikely to be found in the realm of prescription opioids. Our analysis indicates that prescription opioids are not the primary drivers of opioid-related deaths across the country. We advocate for comprehensive solutions which extend beyond limiting prescription opioids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07008v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Deas, Hashan Fernando, Anuj J Kapadia, Jodie Trafton, Adam Spannaus, Vasileios Maroulas</dc:creator>
    </item>
    <item>
      <title>Assumption Smuggling in Intermediate Outcome Tests of Causal Mechanisms</title>
      <link>https://arxiv.org/abs/2407.07072</link>
      <description>arXiv:2407.07072v1 Announce Type: new 
Abstract: Political scientists are increasingly attuned to the promises and pitfalls of establishing causal effects. But the vital question for many is not if a causal effect exists but why and how it exists. Even so, many researchers avoid causal mediation analyses due to the assumptions required, instead opting to explore causal mechanisms through what we call intermediate outcome tests. These tests use the same research design used to estimate the effect of treatment on the outcome to estimate the effect of the treatment on one or more mediators, with authors often concluding that evidence of the latter is evidence of a causal mechanism. We show in this paper that, without further assumptions, this can neither establish nor rule out the existence of a causal mechanism. Instead, such conclusions about the indirect effect of treatment rely on implicit and usually very strong assumptions that are often unmet. Thus, such causal mechanism tests, though very common in political science, should not be viewed as a free lunch but rather should be used judiciously, and researchers should explicitly state and defend the requisite assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07072v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Matthew Blackwell, Ruofan Ma, Aleksei Opacic</dc:creator>
    </item>
    <item>
      <title>Digital twin with automatic disturbance detection for real-time optimization of a semi-autogenous grinding (SAG) mill</title>
      <link>https://arxiv.org/abs/2407.06216</link>
      <description>arXiv:2407.06216v2 Announce Type: cross 
Abstract: This work describes the development and validation of a digital twin for a semi-autogenous grinding (SAG) mill controlled by an expert system. The digital twin consists of three modules emulating a closed-loop system: fuzzy logic for the expert control, a state-space model for regulatory control, and a recurrent neural network for the SAG mill process. The model was trained with 68 hours of data and validated with 8 hours of test data. It predicts the mill's behavior within a 2.5-minute horizon with a 30-second sampling time. The disturbance detection evaluates the need for retraining, and the digital twin shows promise for supervising the SAG mill with the expert control system. Future work will focus on integrating this digital twin into real-time optimization strategies with industrial validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06216v2</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paulina Quintanilla, Francisco Fern\'andez, Cristobal Mancilla, Mat\'ias Rojas, Mauricio Estrada, Daniel Navia</dc:creator>
    </item>
    <item>
      <title>Impact Evaluation on the European Privacy Laws governing generative-AI models -- Evidence in Relation between Internet Censorship and the Ban of ChatGPT in Italy</title>
      <link>https://arxiv.org/abs/2407.06495</link>
      <description>arXiv:2407.06495v1 Announce Type: cross 
Abstract: We proceed an impact evaluation on the European Privacy Laws governing generative-AI models, especially, focusing on the effects of the Ban of ChatGPT in Italy. We investigate on the causal relationship between Internet Censorship Data and the Ban of ChatGPT in Italy during the period from March 27, 2023 to April 11, 2023. We analyze the relation based on the hidden Markov model with Poisson emissions. We find out that the HTTP Invalid Requests, which decreased during those period, can be explained with seven-state model. Our findings shows the apparent inability for the users in the internet accesses as a result of EU regulations on the generative-AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06495v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>A flexible model for Record Linkage</title>
      <link>https://arxiv.org/abs/2407.06835</link>
      <description>arXiv:2407.06835v1 Announce Type: cross 
Abstract: Combining data from various sources empowers researchers to explore innovative questions, for example those raised by conducting healthcare monitoring studies. However, the lack of a unique identifier often poses challenges. Record linkage procedures determine whether pairs of observations collected on different occasions belong to the same individual using partially identifying variables (e.g. birth year, postal code). Existing methodologies typically involve a compromise between computational efficiency and accuracy. Traditional approaches simplify this task by condensing information, yet they neglect dependencies among linkage decisions and disregard the one-to-one relationship required to establish coherent links. Modern approaches offer a comprehensive representation of the data generation process, at the expense of computational overhead and reduced flexibility. We propose a flexible method, that adapts to varying data complexities, addressing registration errors and accommodating changes of the identifying information over time. Our approach balances accuracy and scalability, estimating the linkage using a Stochastic Expectation Maximisation algorithm on a latent variable model. We illustrate the ability of our methodology to connect observations using large real data applications and demonstrate the robustness of our model to the linking variables quality in a simulation study. The proposed algorithm FlexRL is implemented and available in an open source R package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06835v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kayan\'e Robach, St\'ephanie van der Pas, Mark van de Wiel, Michel H. Hof</dc:creator>
    </item>
    <item>
      <title>Who Goes Next? Optimizing the Allocation of Adherence-Improving Interventions</title>
      <link>https://arxiv.org/abs/2407.06898</link>
      <description>arXiv:2407.06898v1 Announce Type: cross 
Abstract: Long-term adherence to medication is a critical factor in preventing chronic diseases, such as cardiovascular disease. To address poor adherence, physicians may recommend adherence-improving interventions; however, such interventions are costly and limited in their availability. Knowing which patients will stop adhering helps distribute the available resources more effectively. We developed a binary integer program (BIP) model to select patients for adherence-improving intervention under budget constraints. We further studied a long-term adherence prediction model using dynamic logistic regression (DLR) model that uses patients' claim data, medical health factors, demographics, and monitoring frequencies to predict the risk of future non-adherence. We trained and tested our predictive model to longitudinal data for cardiovascular disease in a large cohort of patients taking medication for cholesterol control seen in the national Veterans Affairs health system. Our study shows the importance of including past adherence to increase prediction accuracy. Finally, we assess the potential benefits of using the prediction model by proposing an algorithm that combines the DLR and BIP models to decrease the number of CVD events in a population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06898v1</guid>
      <category>math.OC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Otero-Leon, Mariel Lavieri, Brian Denton, Jeremy Sussman, Rodney Hayward</dc:creator>
    </item>
    <item>
      <title>Traffic Count Data Analysis Using Mixtures of Kato--Jones Distributions</title>
      <link>https://arxiv.org/abs/2206.01355</link>
      <description>arXiv:2206.01355v2 Announce Type: replace 
Abstract: We discuss the modelling of traffic count data that show the variation of traffic volume within a day. For the modelling, we apply mixtures of Kato-Jones distributions in which each component is unimodal and affords a wide range of skewness and kurtosis. We consider two methods for parameter estimation, namely, a modified method of moments and the maximum likelihood method. These methods were seen to be useful for fitting the proposed mixtures to our data. As a result, the variation in traffic volume was classified into the morning and evening traffic whose distributions have different shapes, particularly different degrees of skewness and kurtosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.01355v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kota Nagasaki, Shogo Kato, Wataru Nakanishi, M. C. Jones</dc:creator>
    </item>
    <item>
      <title>Improving subgroup analysis using methods to extend inferences to specific target populations</title>
      <link>https://arxiv.org/abs/2406.08297</link>
      <description>arXiv:2406.08297v2 Announce Type: replace 
Abstract: Subgroup analyses are common in epidemiologic and clinical research. Unfortunately, restriction to subgroup members to test for heterogeneity can yield imprecise effect estimates. If the true effect differs between members and non-members due to different distributions of other measured effect measure modifiers (EMMs), leveraging data from non-members can improve the precision of subgroup effect estimates. We obtained data from the PRIME RCT of panitumumab in patients with metastatic colon and rectal cancer from Project Datasphere(TM) to demonstrate this method. We weighted non-Hispanic White patients to resemble Hispanic patients in measured potential EMMs (e.g., age, KRAS distribution, sex), combined Hispanic and weighted non-Hispanic White patients in one data set, and estimated 1-year differences in progression-free survival (PFS). We obtained percentile-based 95% confidence limits for this 1-year difference in PFS from 2,000 bootstraps. To show when the method is less helpful, we also reweighted male patients to resemble female patients and mutant-type KRAS (no treatment benefit) patients to resemble wild-type KRAS (treatment benefit) patients. The PRIME RCT included 795 non-Hispanic White and 42 Hispanic patients with complete data on EMMs. While the Hispanic-only analysis estimated a one-year PFS change of -17% (95% C.I. -45%, 8.8%) with panitumumab, the combined weighted estimate was more precise (-8.7%, 95% CI -22%, 5.3%) while differing from the full population estimate (1.0%, 95% CI: -5.9%, 7.5%). When targeting wild-type KRAS patients the combined weighted estimate incorrectly suggested no benefit (one-year PFS change: 0.9%, 95% CI: -6.0%, 7.2%). Methods to extend inferences from study populations to specific targets can improve the precision of estimates of subgroup effect estimates when their assumptions are met. Violations of those assumptions can lead to bias, however.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08297v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Webster-Clark, Anthony A. Matthews, Alan R. Ellis, Alan C. Kinlaw, Robert W. Platt</dc:creator>
    </item>
    <item>
      <title>Bayesian Semi-supervised learning under nonparanormality</title>
      <link>https://arxiv.org/abs/2001.03798</link>
      <description>arXiv:2001.03798v2 Announce Type: replace-cross 
Abstract: Semi-supervised learning is a model training method that uses both labeled and unlabeled data. This paper proposes a fully Bayes semi-supervised learning algorithm that can be applied to any binary classification problem. We assume the labels are missing at random when using unlabeled data in a semi-supervised setting. We assume that the observations follow two multivariate normal distributions depending on their true class labels after some common unknown transformation is applied to each component of the observation vector. The function is expanded in a B-splines series and a prior is put on the coefficients. We consider a normal prior on the coefficients and constrain the values to meet the requirement for normality and identifiability constraints. The precision matrices of the two Gaussian distributions have a conjugate Wishart prior, while the means have improper uniform priors. The resulting posterior is still conditionally conjugate, and the Gibbs sampler aided by a data augmentation technique can thus be adopted. An extensive simulation study compares the proposed method with several other available methods. The proposed method is also applied to real datasets on diagnosing breast cancer and classification of signals. We conclude that the proposed method has a better prediction accuracy in various cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2001.03798v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Zhu, Shuvrarghya Ghosh, Subhashis Ghosal</dc:creator>
    </item>
    <item>
      <title>Flexible Covariate Adjustments in Regression Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2107.07942</link>
      <description>arXiv:2107.07942v3 Announce Type: replace-cross 
Abstract: Empirical regression discontinuity (RD) studies often use covariates to increase the precision of their estimates. In this paper, we propose a novel class of estimators that use such covariate information more efficiently than existing methods and can accommodate many covariates. It involves running a standard RD analysis in which a function of the covariates has been subtracted from the original outcome variable. We characterize the function that leads to the estimator with the smallest asymptotic variance, and consider feasible versions of such estimators in which this function is estimated, for example, through modern machine learning techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.07942v3</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Claudia Noack, Tomasz Olma, Christoph Rothe</dc:creator>
    </item>
    <item>
      <title>Inference for bivariate extremes via a semi-parametric angular-radial model</title>
      <link>https://arxiv.org/abs/2401.07259</link>
      <description>arXiv:2401.07259v2 Announce Type: replace-cross 
Abstract: The modelling of multivariate extreme events is important in a wide variety of applications, including flood risk analysis, metocean engineering and financial modelling. A wide variety of statistical techniques have been proposed in the literature; however, many such methods are limited in the forms of dependence they can capture, or make strong parametric assumptions about data structures. In this article, we introduce a novel inference framework for multivariate extremes based on a semi-parametric angular-radial model. This model overcomes the limitations of many existing approaches and provides a unified paradigm for assessing joint tail behaviour. Alongside inferential tools, we also introduce techniques for assessing uncertainty and goodness of fit. Our proposed technique is tested on simulated data sets alongside observed metocean time series', with results indicating generally good performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07259v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Callum John Rowlandson Murphy-Barltrop, Ed Mackay, Philip Jonathan</dc:creator>
    </item>
    <item>
      <title>Estimating Metocean Environments Associated with Extreme Structural Response to Demonstrate the Dangers of Environmental Contour Methods</title>
      <link>https://arxiv.org/abs/2404.16775</link>
      <description>arXiv:2404.16775v3 Announce Type: replace-cross 
Abstract: Extreme value analysis (EVA) uses data to estimate long-term extreme environmental conditions for variables such as significant wave height and period, for the design of marine structures. Together with models for the short-term evolution of the ocean environment and for wave-structure interaction, EVA provides a basis for full probabilistic design analysis. Alternatively, environmental contours provide an approximate approach to estimating structural integrity, without requiring structural knowledge. These contour methods also exploit statistical models, including EVA, but avoid the need for structural modelling by making what are believed to be conservative assumptions about the shape of the structural failure boundary in the environment space. These assumptions, however, may not always be appropriate, or may lead to unnecessary wasted resources from over design. We demonstrate a methodology for efficient fully probabilistic analysis of structural failure. From this, we estimate the joint conditional probability density of the environment (CDE), given the occurrence of an extreme structural response. We use CDE as a diagnostic to highlight the deficiencies of environmental contour methods for design; none of the IFORM environmental contours considered characterise CDE well for three example structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16775v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Speers, David Randell, Jonathan Angus Tawn, Philip Jonathan</dc:creator>
    </item>
  </channel>
</rss>
