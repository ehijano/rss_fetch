<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Dec 2025 02:33:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Seasonal trend assessment of US extreme precipitation via changepoint segmentation</title>
      <link>https://arxiv.org/abs/2512.03513</link>
      <description>arXiv:2512.03513v1 Announce Type: new 
Abstract: Most climate trend studies analyze long-term trends as a proxy for climate dynamics. However, when examining seasonal data, it is unrealistic to assume that long-term trends remain consistent across all seasons. Instead, each season likely experiences distinct trends. Additionally, seasonal climate time series, such as seasonal maximum precipitation, often exhibit nonstationarities, including periodicities and location shifts. Failure to rigorously account for these features in modeling may lead to inaccurate trend estimates. This study quantifies seasonal trends in the contiguous United States' seasonal maximum precipitation series while addressing these nonstationarities. To ensure accurate trend estimation, we identify changepoints where the seasonal maximum precipitation shifts due to factors like measurement device changes, observer differences, or location moves. We employ a penalized likelihood method to estimate multiple changepoints, incorporating a generalized extreme value distribution with periodic features. A genetic algorithm based search algorithm efficiently explores the vast space of potential changepoints in both number and timing. Additionally, we compute seasonal return levels for extreme precipitation. Our methods are illustrated using two selected stations, and the results for the US are summarized through maps. We find that seasonal trends vary more when changepoints are considered than in studies that ignore them. Our findings also reveal distinct regional and seasonal patterns, with increasing trends more prevalent during fall in the South and along the East Coast when changepoints are accounted for.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03513v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaechoul Lee, Mintaek Lee, Thea Sukianto</dc:creator>
    </item>
    <item>
      <title>A decay-adjusted spatio-temporal model to account for the impact of mass drug administration on neglected tropical disease prevalence</title>
      <link>https://arxiv.org/abs/2512.03760</link>
      <description>arXiv:2512.03760v1 Announce Type: new 
Abstract: Prevalence surveys are routinely used to monitor the effectiveness of mass drug administration (MDA) programmes for controlling neglected tropical diseases (NTDs). We propose a decay-adjusted spatio-temporal (DAST) model that explicitly accounts for the time-varying impact of MDA on NTD prevalence, providing a flexible and interpretable framework for estimating intervention effects from sparse survey data. Using case studies on soil-transmitted helminths and lymphatic filariasis, we show that DAST offers a practical alternative to standard geostatistical models when the objective includes quantifying MDA impact and supporting short-term programmatic forecasting. We also discuss extensions and identifiability challenges, advocating for data-driven parsimony over complexity in settings where the available data are too sparse to support the estimation of highly parameterised models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03760v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emanuele Giorgi, Claudio Fronterre, Peter J. Diggle</dc:creator>
    </item>
    <item>
      <title>A Large Scale Heterogeneous Treatment Effect Estimation Framework and Its Applications of Users' Journey at Snap</title>
      <link>https://arxiv.org/abs/2512.03060</link>
      <description>arXiv:2512.03060v1 Announce Type: cross 
Abstract: Heterogeneous Treatment Effect (HTE) and Conditional Average Treatment Effect (CATE) models relax the assumption that treatment effects are the same for every user. We present a large scale industrial framework for estimating HTE using experimental data from hundreds of millions of Snapchat users. By combining results across many experiments, the framework uncovers latent user characteristics that were previously unmeasurable and produces stable treatment effect estimates at scale.
  We describe the core components that enabled this system, including experiment selection, base learner design, and incremental training. We also highlight two applications: user influenceability to ads and user sensitivity to ads. An online A/B test using influenceability scores for targeting showed an improvement on key business metrics that is more than six times larger than what is typically considered significant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03060v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jing Pan, Li Shi, Paul Lo</dc:creator>
    </item>
    <item>
      <title>E-valuator: Reliable Agent Verifiers with Sequential Hypothesis Testing</title>
      <link>https://arxiv.org/abs/2512.03109</link>
      <description>arXiv:2512.03109v1 Announce Type: cross 
Abstract: Agentic AI systems execute a sequence of actions, such as reasoning steps or tool calls, in response to a user prompt. To evaluate the success of their trajectories, researchers have developed verifiers, such as LLM judges and process-reward models, to score the quality of each action in an agent's trajectory. Although these heuristic scores can be informative, there are no guarantees of correctness when used to decide whether an agent will yield a successful output. Here, we introduce e-valuator, a method to convert any black-box verifier score into a decision rule with provable control of false alarm rates. We frame the problem of distinguishing successful trajectories (that is, a sequence of actions that will lead to a correct response to the user's prompt) and unsuccessful trajectories as a sequential hypothesis testing problem. E-valuator builds on tools from e-processes to develop a sequential hypothesis test that remains statistically valid at every step of an agent's trajectory, enabling online monitoring of agents over arbitrarily long sequences of actions. Empirically, we demonstrate that e-valuator provides greater statistical power and better false alarm rate control than other strategies across six datasets and three agents. We additionally show that e-valuator can be used for to quickly terminate problematic trajectories and save tokens. Together, e-valuator provides a lightweight, model-agnostic framework that converts verifier heuristics into decisions rules with statistical guarantees, enabling the deployment of more reliable agentic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03109v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuvom Sadhuka, Drew Prinster, Clara Fannjiang, Gabriele Scalia, Aviv Regev, Hanchen Wang</dc:creator>
    </item>
    <item>
      <title>A Theoretical Framework Bridging Model Validation and Loss Ratio in Insurance</title>
      <link>https://arxiv.org/abs/2512.03242</link>
      <description>arXiv:2512.03242v1 Announce Type: cross 
Abstract: This paper establishes the first analytical relationship between predictive model performance and loss ratio in insurance pricing. We derive a closed-form formula connecting the Pearson correlation between predicted and actual losses to expected loss ratio. The framework proves that model improvements exhibit diminishing marginal returns, analytically confirming the actuarial intuition to prioritize poorly performing models. We introduce the Loss Ratio Error metric for quantifying business impact across frequency, severity, and pure premium models. Simulations show reliable predictions under stated assumptions, with graceful degradation under assumption violations. This framework transforms model investment decisions from qualitative intuition to quantitative cost-benefit analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03242v1</guid>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>C. Evans Hedges</dc:creator>
    </item>
    <item>
      <title>Joint Progression Modeling (JPM): A Probabilistic Framework for Mixed-Pathology Progression</title>
      <link>https://arxiv.org/abs/2512.03475</link>
      <description>arXiv:2512.03475v1 Announce Type: cross 
Abstract: Event-based models (EBMs) infer disease progression from cross-sectional data, and standard EBMs assume a single underlying disease per individual. In contrast, mixed pathologies are common in neurodegeneration. We introduce the Joint Progression Model (JPM), a probabilistic framework that treats single-disease trajectories as partial rankings and builds a prior over joint progressions. We study several JPM variants (Pairwise, Bradley-Terry, Plackett-Luce, and Mallows) and analyze three properties: (i) calibration -- whether lower model energy predicts smaller distance to the ground truth ordering; (ii) separation -- the degree to which sampled rankings are distinguishable from random permutations; and (iii) sharpness -- the stability of sampled aggregate rankings. All variants are calibrated, and all achieve near-perfect separation; sharpness varies by variant and is well-predicted by simple features of the input partial rankings (number and length of rankings, conflict, and overlap). In synthetic experiments, JPM improves ordering accuracy by roughly 21 percent over a strong EBM baseline (SA-EBM) that treats the joint disease as a single condition. Finally, using NACC, we find that the Mallows variant of JPM and the baseline model (SA-EBM) have results that are more consistent with prior literature on the possible disease progression of the mixed pathology of AD and VaD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03475v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongtao Hao, Joseph L. Austerweil</dc:creator>
    </item>
    <item>
      <title>Weighted Conformal Prediction for Survival Analysis under Covariate Shift</title>
      <link>https://arxiv.org/abs/2512.03738</link>
      <description>arXiv:2512.03738v1 Announce Type: cross 
Abstract: Reliable uncertainty quantification is essential in survival prediction, particularly in clinical settings where erroneous decisions carry high risk. Conformal prediction has attracted substantial attention as it offers a model-agnostic framework with finite-sample coverage guarantees. Extending it to right-censored outcomes poses nontrivial challenges. Several adaptations of conformal approaches for survival outcomes have been developed, but they either rely on restrictive censoring settings or substantial computation. A recent conformal approach for right-censored data constructs censoring-adjusted p-values and enables prediction intervals in general survival settings. However, the empirical coverage depends sensitively on heuristic tuning choices and its validity is limited to scenarios without covariate shift. In this paper, we establish theoretical justification for its prediction-set construction, providing a principled basis for defining prediction-set bounds, and extend the approach to covariate-shift settings. Simulation studies and a real data application demonstrate that the proposed method achieves robust coverage and coherent interval structure across varying censoring levels and covariate-shift settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03738v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jaeyoung Shin (Department of Statistics and Data Science, Yonsei University, Seoul, South Korea), Chi Hyun Lee (Department of Statistics and Data Science, Yonsei University, Seoul, South Korea, Department of Applied Statistics, Yonsei University, Seoul, South Korea), Sangwook Kang (Department of Statistics and Data Science, Yonsei University, Seoul, South Korea, Department of Applied Statistics, Yonsei University, Seoul, South Korea)</dc:creator>
    </item>
    <item>
      <title>Using functional information for binary classifications</title>
      <link>https://arxiv.org/abs/2512.03761</link>
      <description>arXiv:2512.03761v1 Announce Type: cross 
Abstract: The adequate use of information measured in a continuous manner along a period of time represents a methodological challenge. In the last decades, most of traditional statistical procedures have been extended for accommodating these functional data. The binary classification problem, which aims to correctly identify units as positive or negative based on marker values, is not aside of this scenario. The crucial point for making binary classifications based on a marker is to establish an order in the marker values, which is not immediate when these values are presented as functions. Here, we argue that if the marker is related to the characteristic under study, a trajectory from a positive participant should be more similar to trajectories from the positive population than to those drawn from the negative. With this criterion, a classification procedure based on the distance between the involved functions is proposed. Besides, we propose a fully non-parametric estimator for this so-called probability-based criterion, PBC. We explore its asymptotic properties, and its finite-sample behavior from an extensive Monte Carlo study. The observed results suggest that the proposed methodology works adequately, and frequently better than its competitors, for a wide variety of situations when the sample size in both the training and the testing cohorts is adequate. The practical use of the proposal is illustrated from real-world dataset. As online supplementary material, the manuscript includes a document with further simulations and additional comments. An R function which wraps up the implemented routines is also provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03761v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pablo Martinez-Camblor</dc:creator>
    </item>
    <item>
      <title>Learning from crises: A new class of time-varying parameter VARs with observable adaptation</title>
      <link>https://arxiv.org/abs/2512.03763</link>
      <description>arXiv:2512.03763v1 Announce Type: cross 
Abstract: We revisit macroeconomic time-varying parameter vector autoregressions (TVP-VARs), whose persistent coefficients may adapt too slowly to large, abrupt shifts such as those during major crises. We explore the performance of an adaptively-varying parameter (AVP) VAR that incorporates deterministic adjustments driven by observable exogenous variables, replacing latent state innovations with linear combinations of macroeconomic and financial indicators. This reformulation collapses the state equation into the measurement equation, enabling simple linear estimation of the model. Simulations show that adaptive parameters are substantially more parsimonious than conventional TVPs, effectively disciplining parameter dynamics without sacrificing flexibility. Using macroeconomic datasets for both the U.S. and the euro area, we demonstrate that AVP-VAR consistently improves out-of-sample forecasts, especially during periods of heightened volatility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03763v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nicolas Hardy, Dimitris Korobilis</dc:creator>
    </item>
    <item>
      <title>A comparison between initialization strategies for the infinite hidden Markov model</title>
      <link>https://arxiv.org/abs/2512.03777</link>
      <description>arXiv:2512.03777v1 Announce Type: cross 
Abstract: Infinite hidden Markov models provide a flexible framework for modelling time series with structural changes and complex dynamics, without requiring the number of latent states to be specified in advance. This flexibility is achieved through the hierarchical Dirichlet process prior, while efficient Bayesian inference is enabled by the beam sampler, which combines dynamic programming with slice sampling to truncate the infinite state space adaptively. Despite extensive methodological developments, the role of initialization in this framework has received limited attention. This study addresses this gap by systematically evaluating initialization strategies commonly used for finite hidden Markov models and assessing their suitability in the infinite setting. Results from both simulated and real datasets show that distance-based clustering initializations consistently outperform model-based and uniform alternatives, the latter being the most widely adopted in the existing literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03777v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Federico P. Cortese, Luca Rossini</dc:creator>
    </item>
    <item>
      <title>Refining Genetic Discoveries of Group Knockoffs via A Feature-level Filter</title>
      <link>https://arxiv.org/abs/2408.12618</link>
      <description>arXiv:2408.12618v3 Announce Type: replace 
Abstract: Identifying variants that carry substantial information on the trait of interest remains a core topic in genetic studies. In analyzing the EADB-UKBB dataset to identify genetic variants associated with Alzheimer's disease (AD), however, we recognize that both existing marginal association tests and conditional independence tests using existing knockoff filters suffer either power loss or lack of informativeness, especially when strong correlations exist among variants. To address these limitations, we propose a new feature-versus-group (FVG) filter that achieves balance between the power and precision in identifying important features from a set of strongly correlated features using group knockoffs. In extensive simulation studies, the FVG filter controls the expected proportion of false discoveries and identifies important features in smaller catching sets without large power loss. Applying the proposed method to the EADB-UKBB dataset, we discover important variants from 89 loci (similar to the most powerful group knockoff filter) with catching sets of substantially smaller size and higher purity and verify the biological informativeness of our discoveries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12618v3</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaqi Gu, Zhaomeng Chen, Zihuai He</dc:creator>
    </item>
    <item>
      <title>Multi-view Bayesian optimisation in an input-output reduced space for engineering design</title>
      <link>https://arxiv.org/abs/2501.01552</link>
      <description>arXiv:2501.01552v2 Announce Type: replace 
Abstract: Bayesian optimisation is an adaptive sampling strategy for constructing a Gaussian process surrogate to efficiently search for the global minimum of a black-box computational model. Gaussian processes have limited applicability in engineering design problems, which usually have many design variables but typically a low intrinsic dimensionality. Their scalability can be significantly improved by identifying a low-dimensional space of latent variables that serve as inputs to the Gaussian process. In this paper, we introduce a multi-view learning strategy that considers both the input design variables and output data representing the objective or constraint functions, to identify a low-dimensional latent subspace. Adopting a fully probabilistic viewpoint, we use probabilistic partial least squares (PPLS) to learn an orthogonal mapping from the design variables to the latent variables using training data consisting of inputs and outputs of the black-box computational model. The latent variables and posterior probability densities of the PPLS and Gaussian process models are determined sequentially and iteratively, with retraining occurring at each adaptive sampling iteration. We compare the proposed probabilistic partial least squares Bayesian optimisation (PPLS-BO) strategy with its deterministic counterpart, partial least squares Bayesian optimisation (PLS-BO), and classical Bayesian optimisation, demonstrating significant improvements in convergence to the global minimum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01552v2</guid>
      <category>stat.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1002/nme.70187.</arxiv:DOI>
      <arxiv:journal_reference>International Journal for Numerical Methods in Engineering, 126(2025)</arxiv:journal_reference>
      <dc:creator>Thomas A. Archbold, Ieva Kazlauskaite, Fehmi Cirak</dc:creator>
    </item>
    <item>
      <title>Marginally interpretable spatial logistic regression with bridge processes</title>
      <link>https://arxiv.org/abs/2412.04744</link>
      <description>arXiv:2412.04744v3 Announce Type: replace-cross 
Abstract: In including random effects to account for dependent observations, the odds ratio interpretation of logistic regression coefficients is changed from population-averaged to subject-specific. This is unappealing in many applications, motivating a rich literature on methods that maintain the marginal logistic regression structure without random effects, such as generalized estimating equations. However, for spatial data, random effect approaches are appealing in providing a full probabilistic characterization of the data that can be used for prediction. We propose a new class of spatial logistic regression models that maintain both population-averaged and subject-specific interpretations through a novel class of bridge processes for spatial random effects. These processes are shown to have appealing computational and theoretical properties, including a scale mixture of normal representation. The new methodology is illustrated with simulations and an analysis of childhood malaria prevalence data in the Gambia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04744v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changwoo J. Lee, David B. Dunson</dc:creator>
    </item>
  </channel>
</rss>
