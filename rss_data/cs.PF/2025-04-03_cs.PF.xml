<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 04 Apr 2025 04:00:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>FB$^+$-tree: A Memory-Optimized B$^+$-tree with Latch-Free Update</title>
      <link>https://arxiv.org/abs/2503.23397</link>
      <description>arXiv:2503.23397v1 Announce Type: cross 
Abstract: B$^+$-trees are prevalent in traditional database systems due to their versatility and balanced structure. While binary search is typically utilized for branch operations, it may lead to inefficient cache utilization in main-memory scenarios. In contrast, trie-based index structures drive branch operations through prefix matching. While these structures generally produce fewer cache misses and are thus increasingly popular, they may underperform in range scans because of frequent pointer chasing. This paper proposes a new high-performance B$^+$-tree variant called \textbf{Feature B$^+$-tree (FB$^+$-tree)}. Similar to employing bit or byte for branch operation in tries, FB$^+$-tree progressively considers several bytes following the common prefix on each level of its inner nodes\textemdash referred to as features, which allows FB$^+$-tree to benefit from prefix skewness. FB$^+$-tree blurs the lines between B$^+$-trees and tries, while still retaining balance. In the best case, FB$^+$-tree almost becomes a trie, whereas in the worst case, it continues to function as a B$^+$-tree. Meanwhile, a crafted synchronization protocol that combines the link technique and optimistic lock is designed to support efficient concurrent index access. Distinctively, FB$^+$-tree leverages subtle atomic operations seamlessly coordinated with optimistic lock to facilitate latch-free updates, which can be easily extended to other structures. Intensive experiments on multiple workload-dataset combinations demonstrate that FB$^+$-tree shows comparable lookup performance to state-of-the-art trie-based indexes and outperforms popular B$^+$-trees by 2.3x$\ \sim\ $3.7x under 96 threads. FB$^+$-tree also exhibits significant potential on other workloads, especially update workloads under contention and scan workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23397v1</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <category>cs.PF</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Chen, Ao Li, Wenhai Li, Lingfeng Deng</dc:creator>
    </item>
    <item>
      <title>Finite-Time Behavior of Erlang-C Model: Mixing Time, Mean Queue Length and Tail Bounds</title>
      <link>https://arxiv.org/abs/2504.02207</link>
      <description>arXiv:2504.02207v1 Announce Type: cross 
Abstract: Service systems like data centers and ride-hailing are popularly modeled as queueing systems in the literature. Such systems are primarily studied in the steady state due to their analytical tractability. However, almost all applications in real life do not operate in a steady state, so there is a clear discrepancy in translating theoretical queueing results to practical applications. To this end, we provide a finite-time convergence for Erlang-C systems (also known as $M/M/n$ queues), providing a stepping stone towards understanding the transient behavior of more general queueing systems. We obtain a bound on the Chi-square distance between the finite time queue length distribution and the stationary distribution for a finite number of servers. We then use these bounds to study the behavior in the many-server heavy-traffic asymptotic regimes. The Erlang-C model exhibits a phase transition at the so-called Halfin-Whitt regime. We show that our mixing rate matches the limiting behavior in the Super-Halfin-Whitt regime, and matches up to a constant factor in the Sub-Halfin-Whitt regime.
  To prove such a result, we employ the Lyapunov-Poincar\'e approach, where we first carefully design a Lyapunov function to obtain a negative drift outside a finite set. Within the finite set, we develop different strategies depending on the properties of the finite set to get a handle on the mixing behavior via a local Poincar\'e inequality. A key aspect of our methodological contribution is in obtaining tight guarantees in these two regions, which when combined give us tight mixing time bounds. We believe that this approach is of independent interest for studying mixing in reversible countable-state Markov chains more generally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02207v1</guid>
      <category>math.PR</category>
      <category>cs.PF</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hoang Huy Nguyen, Sushil Mahavir Varma, Siva Theja Maguluri</dc:creator>
    </item>
    <item>
      <title>SProBench: Stream Processing Benchmark for High Performance Computing Infrastructure</title>
      <link>https://arxiv.org/abs/2504.02364</link>
      <description>arXiv:2504.02364v1 Announce Type: cross 
Abstract: Recent advancements in data stream processing frameworks have improved real-time data handling, however, scalability remains a significant challenge affecting throughput and latency. While studies have explored this issue on local machines and cloud clusters, research on modern high performance computing (HPC) infrastructures is yet limited due to the lack of scalable measurement tools. This work presents SProBench, a novel benchmark suite designed to evaluate the performance of data stream processing frameworks in large-scale computing systems. Building on best practices, SProBench incorporates a modular architecture, offers native support for SLURM-based clusters, and seamlessly integrates with popular stream processing frameworks such as Apache Flink, Apache Spark Streaming, and Apache Kafka Streams. Experiments conducted on HPC clusters demonstrate its exceptional scalability, delivering throughput that surpasses existing benchmarks by more than tenfold. The distinctive features of SProBench, including complete customization options, built-in automated experiment management tools, seamless interoperability, and an open-source license, distinguish it as an innovative benchmark suite tailored to meet the needs of modern data stream processing frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02364v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Apurv Deepak Kulkarni, Siavash Ghiasvand</dc:creator>
    </item>
    <item>
      <title>A Scalable Synthesis Algorithm for Reversible Functions</title>
      <link>https://arxiv.org/abs/2504.02632</link>
      <description>arXiv:2504.02632v1 Announce Type: cross 
Abstract: Reversible computation is an emerging technology that has gained significant attention due to its critical role in quantum circuit synthesis and low-power design. This paper introduces a transformation-based method for exact synthesis of reversible circuits. The proposed approach utilizes a modified Quine-McCluskey algorithm to eliminate input-output discrepancies in the truth table, transforming the permutation matrix into an identity matrix. Furthermore, a novel search space reduction technique is presented which, combined with the primary method, enables the synthesis algorithm to handle high-input reversible functions. This approach combines the influence of multiple control qubits on a target qubit, evaluating their collective impact. This aggregation can decrease the control qubit count within quantum gates. Consequently, it proves beneficial for applications like surface code error correction architectures as well as current Noisy Intermediate-Scale Quantum (NISQ) hardwares. Experimental results demonstrate significant improvements over the state-of-the-art exact synthesis methods, achieving up to 99% improvements in terms of the number of levels of T-gates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02632v1</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <category>cs.PF</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moein Sarvaghad-Moghaddam, Morteza Saheb Zamani, Mehdi Sedighi</dc:creator>
    </item>
    <item>
      <title>Exploiting Unstructured Sparsity in Fully Homomorphic Encrypted DNNs</title>
      <link>https://arxiv.org/abs/2503.09184</link>
      <description>arXiv:2503.09184v2 Announce Type: replace-cross 
Abstract: The deployment of deep neural networks (DNNs) in privacy-sensitive environments is constrained by computational overheads in fully homomorphic encryption (FHE). This paper explores unstructured sparsity in FHE matrix multiplication schemes as a means of reducing this burden while maintaining model accuracy requirements. We demonstrate that sparsity can be exploited in arbitrary matrix multiplication, providing runtime benefits compared to a baseline naive algorithm at all sparsity levels. This is a notable departure from the plaintext domain, where there is a trade-off between sparsity and the overhead of the sparse multiplication algorithm. In addition, we propose three sparse multiplication schemes in FHE based on common plaintext sparse encodings. We demonstrate the performance gain is scheme-invariant; however, some sparse schemes vastly reduce the memory storage requirements of the encrypted matrix at high sparsity values. Our proposed sparse schemes yield an average performance gain of 2.5x at 50% unstructured sparsity, with our multi-threading scheme providing a 32.5x performance increase over the equivalent single-threaded sparse computation when utilizing 64 cores.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09184v2</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aidan Ferguson, Perry Gibson, Lara D'Agata, Parker McLeod, Ferhat Yaman, Amitabh Das, Ian Colbert, Jos\'e Cano</dc:creator>
    </item>
    <item>
      <title>A Pilot Study on Tunable Precision Emulation via Automatic BLAS Offloading</title>
      <link>https://arxiv.org/abs/2503.22875</link>
      <description>arXiv:2503.22875v2 Announce Type: replace-cross 
Abstract: This study explores the use of automatic BLAS offloading and INT8-based emulation for accelerating traditional HPC workloads on modern GPU architectures. Through the use of low-bitwidth integer units and cache-coherent Unified Memory Architecture, we emulate double-precision matrix multiplications in the MuST application without code changes. We find that accuracy depends on both arithmetic precision and the properties of the operator, which can be dealt with through tunable precision emulation. Unlike traditional mixed-precision approaches, this method preserves original algorithms while optimizing hardware utilization. We showcases the potential of improving accuracy and performance at the same time. This work highlights the potential of AI-driven hardware to transform HPC, advocating for adaptive precision strategies in future scientific computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22875v2</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hang Liu, Junjie Li, Yinzhi Wang</dc:creator>
    </item>
    <item>
      <title>Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models</title>
      <link>https://arxiv.org/abs/2503.22879</link>
      <description>arXiv:2503.22879v2 Announce Type: replace-cross 
Abstract: State Space Models (SSMs) are emerging as a compelling alternative to Transformers because of their consistent memory usage and high performance. Despite this, scaling up SSMs on cloud services or limited-resource devices is challenging due to their storage requirements and computational power. To overcome this, quantizing SSMs with low bit-width data formats can reduce model size and benefit from hardware acceleration. As SSMs are prone to quantization-induced errors, recent efforts have focused on optimizing a particular model or bit-width for efficiency without sacrificing performance. However, distinct bit-width configurations are essential for different scenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for enhancing generation speed in short prompt applications for a single user. To this end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both Mamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment on various platforms. Based on the channel order preserving and activation persistence of SSMs, we propose an offline approach to quantize inputs of a linear recurrence in 8-bit by sorting and clustering for input $x$, combined with a per-state-group quantization for input-dependent parameters $B$ and $C$. To ensure compute-invariance in the SSM output, we rearrange weights offline according to the clustering sequence. The experiments show that Quamba2-8B outperforms several state-of-the-art SSM quantization methods and delivers 1.3$\times$ and 3$\times$ speed-ups in the pre-filling and generation stages, respectively, while offering 4$\times$ memory reduction with only a $1.6\%$ average accuracy drop. The evaluation on MMLU shows the generalizability and robustness of our framework. The code and quantized models will be released at: https://github.com/enyac-group/Quamba.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22879v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.PF</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hung-Yueh Chiang, Chi-Chih Chang, Natalia Frumkin, Kai-Chiang Wu, Mohamed S. Abdelfattah, Diana Marculescu</dc:creator>
    </item>
  </channel>
</rss>
