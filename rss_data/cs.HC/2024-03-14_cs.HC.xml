<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 Mar 2024 04:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 15 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Ontologia para monitorar a defici\^encia mental em seus d\'eficts no processamento da informa\c{c}\~ao por decl\'inio cognitivo e evitar agress\~oes psicol\'ogicas e f\'isicas em ambientes educacionais com ajuda da I.A*</title>
      <link>https://arxiv.org/abs/2403.08795</link>
      <description>arXiv:2403.08795v1 Announce Type: new 
Abstract: The intention of this article is to propose the use of artificial intelligence to detect through analysis by UFO ontology the emergence of verbal and physical aggression related to psychosocial deficiencies and their provoking agents, in an attempt to prevent catastrophic consequences within school environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08795v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bruna Ara\'ujo de Castro Oliveira</dc:creator>
    </item>
    <item>
      <title>Gore Diffusion LoRA Model</title>
      <link>https://arxiv.org/abs/2403.08812</link>
      <description>arXiv:2403.08812v1 Announce Type: new 
Abstract: The Emergence of Artificial Intelligence (AI) has significantly impacted our engagement with violence, sparking ethical deliberations regarding the algorithmic creation of violent imagery. This paper scrutinizes the "Gore Diffusion LoRA Model," an innovative AI model proficient in generating hyper-realistic visuals portraying intense violence and bloodshed. Our exploration encompasses the model's technical intricacies, plausible applications, and the ethical quandaries inherent in its utilization. We contend that the creation and implementation of such models warrant a meticulous discourse concerning the convergence of AI, art, and violence. Furthermore, we advocate for a structured framework advocating responsible development and ethical deployment of these potent technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08812v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ayush Thakur, Ashwani Kumar Dubey</dc:creator>
    </item>
    <item>
      <title>A Novel Approach to Personalized Personality Assessment with the Attachment-Caregiving Questionnaire (ACQ): First Evidence in favor of AI-Oriented Inventory Designs</title>
      <link>https://arxiv.org/abs/2403.08823</link>
      <description>arXiv:2403.08823v1 Announce Type: new 
Abstract: Background. Personality is a primary object of interest in clinical psychology and psychiatry. It is most often measured using questionnaires, which rely on Factor Analysis (FA) to identify essential domains corresponding to highly correlated questions/items that define a (sub)scale. This procedure implies the rigid assignment of each question to one scale - giving the item the same meaning regardless of how the respondent may interpret it - arguably affecting the assessment capability of the instrument.
  Methods. To test this hypothesis, we use the Attachment-Caregiving Questionnaire (ACQ), a clinical and personality self-report that - through extra-scale information - allows the clinician to infer the possible different meanings subjects attribute to the items. Considering four psychotherapy patients, we compare the scoring of the ACQ provided by expert clinicians to the detailed information gained from therapy and the patients.
  Results. Our analysis suggests that a question can be interpreted differently - receiving the same score for different (clinically relevant) reasons - potentially impacting personality assessment and clinical decision-making. Moreover, accounting for multiple interpretations requires a specific questionnaire design and a more advanced pattern recognition than FA - which Artificial Intelligence (AI) could provide.
  Conclusion. Our results indicate that a meaning-sensitive, personalized read of a personality self-report can affect profiling and treatment. Since a machine learning model can mimic the interpretative performance of an expert clinician, our results also imply a novel, AI-oriented approach to inventory design, of which we envision the first implementation steps. More evidence is required to support these preliminary findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08823v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcantonio Gagliardi (University of Greenwich UK), Marina Bonadeni (Italian Professional Body of Psychologists &amp; Psychotherapists), Sara Billai (Italian Professional Body of Psychologists &amp; Psychotherapists), Gian Luca Marcialis (University of Cagliari Italy)</dc:creator>
    </item>
    <item>
      <title>Measuring Non-Typical Emotions for Mental Health: A Survey of Computational Approaches</title>
      <link>https://arxiv.org/abs/2403.08824</link>
      <description>arXiv:2403.08824v1 Announce Type: new 
Abstract: Analysis of non-typical emotions, such as stress, depression and engagement is less common and more complex compared to that of frequently discussed emotions like happiness, sadness, fear, and anger. The importance of these non-typical emotions has been increasingly recognized due to their implications on mental health and well-being. Stress and depression impact the engagement in daily tasks, highlighting the need to understand their interplay. This survey is the first to simultaneously explore computational methods for analyzing stress, depression, and engagement. We discuss the most commonly used datasets, input modalities, data processing techniques, and information fusion methods used for the computational analysis of stress, depression and engagement. A timeline and taxonomy of non-typical emotion analysis approaches along with their generic pipeline and categories are presented. Subsequently, we describe state-of-the-art computational approaches for non-typical emotion analysis, including a performance summary on the most commonly used datasets. Following this, we explore the applications, along with the associated challenges, limitations, and future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08824v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.MM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Puneet Kumar, Alexander Vedernikov, Xiaobai Li</dc:creator>
    </item>
    <item>
      <title>A Dataset for the Validation of Truth Inference Algorithms Suitable for Online Deployment</title>
      <link>https://arxiv.org/abs/2403.08826</link>
      <description>arXiv:2403.08826v1 Announce Type: new 
Abstract: For the purpose of efficient and cost-effective large-scale data labeling, crowdsourcing is increasingly being utilized. To guarantee the quality of data labeling, multiple annotations need to be collected for each data sample, and truth inference algorithms have been developed to accurately infer the true labels. Despite previous studies having released public datasets to evaluate the efficacy of truth inference algorithms, these have typically focused on a single type of crowdsourcing task and neglected the temporal information associated with workers' annotation activities. These limitations significantly restrict the practical applicability of these algorithms, particularly in the context of long-term and online truth inference. In this paper, we introduce a substantial crowdsourcing annotation dataset collected from a real-world crowdsourcing platform. This dataset comprises approximately two thousand workers, one million tasks, and six million annotations. The data was gathered over a period of approximately six months from various types of tasks, and the timestamps of each annotation were preserved. We analyze the characteristics of the dataset from multiple perspectives and evaluate the effectiveness of several representative truth inference algorithms on this dataset. We anticipate that this dataset will stimulate future research on tracking workers' abilities over time in relation to different types of tasks, as well as enhancing online truth inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08826v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fei Wang, Haoyu Liu, Haoyang Bi, Xiangzhuang Shen, Renyu Zhu, Runze Wu, Minmin Lin, Tangjie Lv, Changjie Fan, Qi Liu, Zhenya Huang, Enhong Chen</dc:creator>
    </item>
    <item>
      <title>People Attribute Purpose to Autonomous Vehicles When Explaining Their Behavior</title>
      <link>https://arxiv.org/abs/2403.08828</link>
      <description>arXiv:2403.08828v1 Announce Type: new 
Abstract: A hallmark of a good XAI system is explanations that users can understand and act on. In many cases, this requires a system to offer causal or counterfactual explanations that are intelligible. Cognitive science can help us understand what kinds of explanations users might expect, and in which format to frame these explanations. We briefly review relevant literature from the cognitive science of explanation, particularly as it concerns teleology, the tendency to explain a decision in terms of the purpose it was meant to achieve. We then report empirical data on how people generate explanations for the behavior of autonomous vehicles, and how they evaluate these explanations. In a first survey, participants (n=54) were shown videos of a road scene and asked to generate either mechanistic, counterfactual, or teleological verbal explanations for a vehicle's actions. In the second survey, a different set of participants (n=356) rated these explanations along various metrics including quality, trustworthiness, and how much each explanatory mode was emphasized in the explanation. Participants deemed mechanistic and teleological explanations as significantly higher quality than counterfactual explanations. In addition, perceived teleology was the best predictor of perceived quality and trustworthiness. Neither perceived teleology nor quality ratings were affected by whether the car whose actions were being explained was an autonomous vehicle or was being driven by a person. The results show people use and value teleological concepts to evaluate information about both other people and autonomous vehicles, indicating they find the 'intentional stance' a convenient abstraction. We make our dataset of annotated video situations with explanations, called Human Explanations for Autonomous Driving Decisions (HEADD), publicly available, which we hope will prompt further research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08828v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Balint Gyevnar, Stephanie Droop, Tadeg Quillien</dc:creator>
    </item>
    <item>
      <title>Mitigating Biases in Collective Decision-Making: Enhancing Performance in the Face of Fake News</title>
      <link>https://arxiv.org/abs/2403.08829</link>
      <description>arXiv:2403.08829v1 Announce Type: new 
Abstract: Individual and social biases undermine the effectiveness of human advisers by inducing judgment errors which can disadvantage protected groups. In this paper, we study the influence these biases can have in the pervasive problem of fake news by evaluating human participants' capacity to identify false headlines. By focusing on headlines involving sensitive characteristics, we gather a comprehensive dataset to explore how human responses are shaped by their biases. Our analysis reveals recurring individual biases and their permeation into collective decisions. We show that demographic factors, headline categories, and the manner in which information is presented significantly influence errors in human judgment. We then use our collected data as a benchmark problem on which we evaluate the efficacy of adaptive aggregation algorithms. In addition to their improved accuracy, our results highlight the interactions between the emergence of collective intelligence and the mitigation of participant biases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08829v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Axel Abels, Elias Fernandez Domingos, Ann Now\'e, Tom Lenaerts</dc:creator>
    </item>
    <item>
      <title>Making High-Level AI Design Decisions Explicit Using a Binary Stream System-Designation Approach</title>
      <link>https://arxiv.org/abs/2403.08832</link>
      <description>arXiv:2403.08832v1 Announce Type: new 
Abstract: Some crucial decisions in AI design tend to be overlooked or factor choices are assumed implicitly. The question often answered first is what the AI will do, not how it will interact with the rest of the world. This reduces our understanding of the possible types of AI that can be developed and their potential impacts on humanity. As an initial AI taxonomy, I present binary choices for 10 of the subjectively most separable and influential high-level design factors, then give brief examples of several of the 1024 possible systems defined by those choices. This supports a simple binary stream approach to system designation based on translating the stream of choices into decimal notation, giving a short-hand way of referring to systems with different properties that meet specialized needs. Further, underspecified or generic systems can be designated using the binary stream approach as well, a notational feature that supports modeling the impacts of AI systems with selected characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08832v1</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Julia Mossbridge</dc:creator>
    </item>
    <item>
      <title>AcademiaOS: Automating Grounded Theory Development in Qualitative Research with Large Language Models</title>
      <link>https://arxiv.org/abs/2403.08844</link>
      <description>arXiv:2403.08844v1 Announce Type: new 
Abstract: AcademiaOS is a first attempt to automate grounded theory development in qualitative research with large language models. Using recent large language models' language understanding, generation, and reasoning capabilities, AcademiaOS codes curated qualitative raw data such as interview transcripts and develops themes and dimensions to further develop a grounded theoretical model, affording novel insights. A user study (n=19) suggests that the system finds acceptance in the academic community and exhibits the potential to augment humans in qualitative research. AcademiaOS has been made open-source for others to build upon and adapt to their use cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08844v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas \"Ubellacker</dc:creator>
    </item>
    <item>
      <title>Bury Me Here --The New Genre of Narrative Design Game Based on Immersive Storytelling</title>
      <link>https://arxiv.org/abs/2403.08903</link>
      <description>arXiv:2403.08903v1 Announce Type: new 
Abstract: Virtual reality games always provide the player with the most verisimilitude experience. With the advancement of VR hardware, it may become mainstream how people feel and attach to a virtual world. The paper discusses a possible solution to finding a better balance between the two classical genres of VR games, sensory stimulation and storytelling. To this end, we designed a game named "Bury Me Here," in which players can find an emotional bond between the game protagonist and themselves. The game includes four sections, the departure from the hometown, the travel on the train, the work in the office, and the life in the penthouse. At the game's end, the protagonist returns to his country yard and spends the rest of his life there. All the sections are designed to tell a stranger's life story to the player, making them experience someone else's life path and bonding an emotional connection between the player and the protagonist through storytelling. Results show that the game provides an immersive visual experience and has emotive sparks echo in players' minds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08903v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Zhongsheng Li, Wuji Li, Yudong He</dc:creator>
    </item>
    <item>
      <title>A Virtual Environment for Collaborative Inspection in Additive Manufacturing</title>
      <link>https://arxiv.org/abs/2403.08940</link>
      <description>arXiv:2403.08940v1 Announce Type: new 
Abstract: Additive manufacturing (AM) techniques have been used to enhance the design and fabrication of complex components for various applications in the medical, aerospace, energy, and consumer products industries. A defining feature for many AM parts is the complex internal geometry enabled by the printing process. However, inspecting these internal structures requires volumetric imaging, i.e., X-ray CT, leading to the well-known challenge of visualizing complex 3D geometries using 2D desktop interfaces. Furthermore, existing tools are limited to single-user systems making it difficult to jointly discuss or share findings with a larger team, i.e., the designers, manufacturing experts, and evaluation team. In this work, we present a collaborative virtual reality (VR) for the exploration and inspection of AM parts. Geographically separated experts can virtually inspect and jointly discuss data. It also supports VR and non-VR users, who can be spectators in the VR environment. Various features for data exploration and inspection are developed and enhanced via real-time synchronization. We followed usability and interface verification guidelines using Nielsen's heuristics approach. Furthermore, we conducted exploratory and semi-structured interviews with domain experts to collect qualitative feedback. Results reveal potential benefits, applicability, and current limitations. The proposed collaborative VR environment provides a new basis and opens new research directions for virtual inspection and team collaboration in AM settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08940v1</guid>
      <category>cs.HC</category>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Vuthea Chheang, Brian Thomas Weston, Robert William Cerda, Brian Au, Brian Giera, Peer-Timo Bremer, Haichao Miao</dc:creator>
    </item>
    <item>
      <title>Exploring Prompt Engineering Practices in the Enterprise</title>
      <link>https://arxiv.org/abs/2403.08950</link>
      <description>arXiv:2403.08950v1 Announce Type: new 
Abstract: Interaction with Large Language Models (LLMs) is primarily carried out via prompting. A prompt is a natural language instruction designed to elicit certain behaviour or output from a model. In theory, natural language prompts enable non-experts to interact with and leverage LLMs. However, for complex tasks and tasks with specific requirements, prompt design is not trivial. Creating effective prompts requires skill and knowledge, as well as significant iteration in order to determine model behavior, and guide the model to accomplish a particular goal. We hypothesize that the way in which users iterate on their prompts can provide insight into how they think prompting and models work, as well as the kinds of support needed for more efficient prompt engineering. To better understand prompt engineering practices, we analyzed sessions of prompt editing behavior, categorizing the parts of prompts users iterated on and the types of changes they made. We discuss design implications and future directions based on these prompt engineering practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08950v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Desmond, Michelle Brachman</dc:creator>
    </item>
    <item>
      <title>AI coach for badminton</title>
      <link>https://arxiv.org/abs/2403.08956</link>
      <description>arXiv:2403.08956v1 Announce Type: new 
Abstract: In the competitive realm of sports, optimal performance necessitates rigorous management of nutrition and physical conditioning. Specifically, in badminton, the agility and precision required make it an ideal candidate for motion analysis through video analytics. This study leverages advanced neural network methodologies to dissect video footage of badminton matches, aiming to extract detailed insights into player kinetics and biomechanics. Through the analysis of stroke mechanics, including hand-hip coordination, leg positioning, and the execution angles of strokes, the research aims to derive predictive models that can suggest improvements in stance, technique, and muscle orientation. These recommendations are designed to mitigate erroneous techniques, reduce the risk of joint fatigue, and enhance overall performance. Utilizing a vast array of data available online, this research correlates players' physical attributes with their in-game movements to identify muscle activation patterns during play. The goal is to offer personalized training and nutrition strategies that align with the specific biomechanical demands of badminton, thereby facilitating targeted performance enhancements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08956v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/INCET54531.2022.9825164</arxiv:DOI>
      <arxiv:journal_reference>2022 3rd International Conference for Emerging Technology (INCET), Belgaum, India, 2022, pp. 1-7</arxiv:journal_reference>
      <dc:creator>Dhruv Toshniwal, Arpit Patil, Nancy Vachhani</dc:creator>
    </item>
    <item>
      <title>The Full-scale Assembly Simulation Testbed (FAST) Dataset</title>
      <link>https://arxiv.org/abs/2403.08969</link>
      <description>arXiv:2403.08969v1 Announce Type: new 
Abstract: In recent years, numerous researchers have begun investigating how virtual reality (VR) tracking and interaction data can be used for a variety of machine learning purposes, including user identification, predicting cybersickness, and estimating learning gains. One constraint for this research area is the dearth of open datasets. In this paper, we present a new open dataset captured with our VR-based Full-scale Assembly Simulation Testbed (FAST). This dataset consists of data collected from 108 participants (50 females, 56 males, 2 non-binary) learning how to assemble two distinct full-scale structures in VR. In addition to explaining how the dataset was collected and describing the data included, we discuss how the dataset may be used by future researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08969v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alec G. Moore, Tiffany D. Do, Nayan N. Chawla, Antonia Jimenez Iriarte, Ryan P. McMahan</dc:creator>
    </item>
    <item>
      <title>Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset</title>
      <link>https://arxiv.org/abs/2403.09029</link>
      <description>arXiv:2403.09029v1 Announce Type: new 
Abstract: Using vision-language models (VLMs) in web development presents a promising strategy to increase efficiency and unblock no-code solutions: by providing a screenshot or a sketch of a UI, a VLM could generate the code to reproduce it, for instance in a language like HTML. Despite the advancements in VLMs for various tasks, the specific challenge of converting a screenshot into a corresponding HTML has been minimally explored. We posit that this is mainly due to the absence of a suitable, high-quality dataset. This work introduces WebSight, a synthetic dataset consisting of 2 million pairs of HTML codes and their corresponding screenshots. We fine-tune a foundational VLM on our dataset and show proficiency in converting webpage screenshots to functional HTML code. To accelerate the research in this area, we open-source WebSight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09029v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hugo Lauren\c{c}on, L\'eo Tronchon, Victor Sanh</dc:creator>
    </item>
    <item>
      <title>How do Older Adults Set Up Voice Assistants? Lessons Learned from a Deployment Experience for Older Adults to Set Up Standalone Voice Assistants</title>
      <link>https://arxiv.org/abs/2403.09043</link>
      <description>arXiv:2403.09043v1 Announce Type: new 
Abstract: While standalone Voice Assistants (VAs) are promising to support older adults' daily routine and wellbeing management, onboarding and setting up these devices can be challenging. Although some older adults choose to seek assistance from technicians and adult children, easy set up processes that facilitate independent use are still critical, especially for those who do not have access to external resources. We aim to understand the older adults' experience while setting up commercially available voice-only and voice-first screen-based VAs. Rooted in participants observations and semi-structured interviews, we designed a within-subject study with 10 older adults using Amazon Echo Dot and Echo Show. We identified the values of the built-in touchscreen and the instruction documents, as well as the impact of form factors, and outline important directions to support older adult independence with VAs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09043v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3563703.3596640</arxiv:DOI>
      <dc:creator>Chen Chen, Ella T. Lifset, Yichen Han, Arkajyoti Roy, Michael Hogarth, Alison A. Moore, Emilia Farcas, Nadir Weibel</dc:creator>
    </item>
    <item>
      <title>OutlineSpark: Igniting AI-powered Presentation Slides Creation from Computational Notebooks through Outlines</title>
      <link>https://arxiv.org/abs/2403.09121</link>
      <description>arXiv:2403.09121v1 Announce Type: new 
Abstract: Computational notebooks are widely utilized for exploration and analysis. However, creating slides to communicate analysis results from these notebooks is quite tedious and time-consuming. Researchers have proposed automatic systems for generating slides from notebooks, which, however, often do not consider the process of users conceiving and organizing their messages from massive code cells. Those systems ask users to go directly into the slide creation process, which causes potentially ill-structured slides and burdens in further refinement. Inspired by the common and widely recommended slide creation practice: drafting outlines first and then adding concrete content, we introduce OutlineSpark, an AI-powered slide creation tool that generates slides from a slide outline written by the user. The tool automatically retrieves relevant notebook cells based on the outlines and converts them into slide content. We evaluated OutlineSpark with 12 users. Both the quantitative and qualitative feedback from the participants verify its effectiveness and usability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09121v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fengjie Wang, Yanna Lin, Leni Yang, Haotian Li, Mingyang Gu, Min Zhu, Huamin Qu</dc:creator>
    </item>
    <item>
      <title>Towards Proactive Interactions for In-Vehicle Conversational Assistants Utilizing Large Language Models</title>
      <link>https://arxiv.org/abs/2403.09135</link>
      <description>arXiv:2403.09135v1 Announce Type: new 
Abstract: Research demonstrates that the proactivity of in-vehicle conversational assistants (IVCAs) can help to reduce distractions and enhance driving safety, better meeting users' cognitive needs. However, existing IVCAs struggle with user intent recognition and context awareness, which leads to suboptimal proactive interactions. Large language models (LLMs) have shown potential for generalizing to various tasks with prompts, but their application in IVCAs and exploration of proactive interaction remain under-explored. These raise questions about how LLMs improve proactive interactions for IVCAs and influence user perception. To investigate these questions systematically, we establish a framework with five proactivity levels across two dimensions-assumption and autonomy-for IVCAs. According to the framework, we propose a "Rewrite + ReAct + Reflect" strategy, aiming to empower LLMs to fulfill the specific demands of each proactivity level when interacting with users. Both feasibility and subjective experiments are conducted. The LLM outperforms the state-of-the-art model in success rate and achieves satisfactory results for each proactivity level. Subjective experiments with 40 participants validate the effectiveness of our framework and show the proactive level with strong assumptions and user confirmation is most appropriate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09135v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huifang Du, Xuejing Feng, Jun Ma, Meng Wang, Shiyu Tao, Yijie Zhong, Yuan-Fang Li, Haofen Wang</dc:creator>
    </item>
    <item>
      <title>VIVID: Human-AI Collaborative Authoring of Vicarious Dialogues from Lecture Videos</title>
      <link>https://arxiv.org/abs/2403.09168</link>
      <description>arXiv:2403.09168v1 Announce Type: new 
Abstract: The lengthy monologue-style online lectures cause learners to lose engagement easily. Designing lectures in a "vicarious dialogue" format can foster learners' cognitive activities more than monologue-style. However, designing online lectures in a dialogue style catered to the diverse needs of learners is laborious for instructors. We conducted a design workshop with eight educational experts and seven instructors to present key guidelines and the potential use of large language models (LLM) to transform a monologue lecture script into pedagogically meaningful dialogue. Applying these design guidelines, we created VIVID which allows instructors to collaborate with LLMs to design, evaluate, and modify pedagogical dialogues. In a within-subjects study with instructors (N=12), we show that VIVID helped instructors select and revise dialogues efficiently, thereby supporting the authoring of quality dialogues. Our findings demonstrate the potential of LLMs to assist instructors with creating high-quality educational dialogues across various learning stages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09168v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Seulgi Choi, Hyewon Lee, Yoonjoo Lee, Juho Kim</dc:creator>
    </item>
    <item>
      <title>Enabling Waypoint Generation for Collaborative Robots using LLMs and Mixed Reality</title>
      <link>https://arxiv.org/abs/2403.09308</link>
      <description>arXiv:2403.09308v1 Announce Type: new 
Abstract: Programming a robotic is a complex task, as it demands the user to have a good command of specific programming languages and awareness of the robot's physical constraints. We propose a framework that simplifies robot deployment by allowing direct communication using natural language. It uses large language models (LLM) for prompt processing, workspace understanding, and waypoint generation. It also employs Augmented Reality (AR) to provide visual feedback of the planned outcome. We showcase the effectiveness of our framework with a simple pick-and-place task, which we implement on a real robot. Moreover, we present an early concept of expressive robot behavior and skill generation that can be used to communicate with the user and learn new skills (e.g., object grasping).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09308v1</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cathy Mengying Fang, Krzysztof Zieli\'nski, Pattie Maes, Joe Paradiso, Bruce Blumberg, Mikkel Baun Kj{\ae}rgaard</dc:creator>
    </item>
    <item>
      <title>Machine Learning Processes as Sources of Ambiguity: Insights from AI Art</title>
      <link>https://arxiv.org/abs/2403.09374</link>
      <description>arXiv:2403.09374v1 Announce Type: new 
Abstract: Ongoing efforts to turn Machine Learning (ML) into a design material have encountered limited success. This paper examines the burgeoning area of AI art to understand how artists incorporate ML in their creative work. Drawing upon related HCI theories, we investigate how artists create ambiguity by analyzing nine AI artworks that use computer vision and image synthesis. Our analysis shows that, in addition to the established types of ambiguity, artists worked closely with the ML process (dataset curation, model training, and application) and developed various techniques to evoke the ambiguity of processes. Our finding indicates that the current conceptualization of ML as a design material needs to reframe the ML process as design elements, instead of technical details. Finally, this paper offers reflections on commonly held assumptions in HCI about ML uncertainty, dependability, and explainability, and advocates to supplement the artifact-centered design perspective of ML with a process-centered one.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09374v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3613904.3642855</arxiv:DOI>
      <dc:creator>Christian Sivertsen, Guido Salimbeni, Anders Sundnes L{\o}vlie, Steve Benford, Jichen Zhu</dc:creator>
    </item>
    <item>
      <title>Which Artificial Intelligences Do People Care About Most? A Conjoint Experiment on Moral Consideration</title>
      <link>https://arxiv.org/abs/2403.09405</link>
      <description>arXiv:2403.09405v1 Announce Type: new 
Abstract: Many studies have identified particular features of artificial intelligences (AI), such as their autonomy and emotion expression, that affect the extent to which they are treated as subjects of moral consideration. However, there has not yet been a comparison of the relative importance of features as is necessary to design and understand increasingly capable, multi-faceted AI systems. We conducted an online conjoint experiment in which 1,163 participants evaluated descriptions of AIs that varied on these features. All 11 features increased how morally wrong participants considered it to harm the AIs. The largest effects were from human-like physical bodies and prosociality (i.e., emotion expression, emotion recognition, cooperation, and moral judgment). For human-computer interaction designers, the importance of prosociality suggests that, because AIs are often seen as threatening, the highest levels of moral consideration may only be granted if the AI has positive intentions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09405v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Ladak, Jamie Harris, Jacy Reese Anthis</dc:creator>
    </item>
    <item>
      <title>"Like a Nesting Doll": Analyzing Recursion Analogies Generated by CS Students using Large Language Models</title>
      <link>https://arxiv.org/abs/2403.09409</link>
      <description>arXiv:2403.09409v1 Announce Type: new 
Abstract: Grasping complex computing concepts often poses a challenge for students who struggle to anchor these new ideas to familiar experiences and understandings. To help with this, a good analogy can bridge the gap between unfamiliar concepts and familiar ones, providing an engaging way to aid understanding. However, creating effective educational analogies is difficult even for experienced instructors. We investigate to what extent large language models (LLMs), specifically ChatGPT, can provide access to personally relevant analogies on demand. Focusing on recursion, a challenging threshold concept, we conducted an investigation analyzing the analogies generated by more than 350 first-year computing students. They were provided with a code snippet and tasked to generate their own recursion-based analogies using ChatGPT, optionally including personally relevant topics in their prompts. We observed a great deal of diversity in the analogies produced with student-prescribed topics, in contrast to the otherwise generic analogies, highlighting the value of student creativity when working with LLMs. Not only did students enjoy the activity and report an improved understanding of recursion, but they described more easily remembering analogies that were personally and culturally relevant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09409v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seth Bernstein, Paul Denny, Juho Leinonen, Lauren Kan, Arto Hellas, Matt Littlefield Sami Sarsa, Stephen MacNeil</dc:creator>
    </item>
    <item>
      <title>Effect of external characteristics of a virtual human being during the use of a computer-assisted therapy tool</title>
      <link>https://arxiv.org/abs/2403.09544</link>
      <description>arXiv:2403.09544v1 Announce Type: new 
Abstract: Identification within media, whether with real or fictional characters, significantly impacts users, shaping their behavior and enriching their social and emotional experiences. Immersive media, like video games, utilize virtual entities such as agents, avatars, or NPCs to connect users with virtual worlds, fostering a heightened sense of immersion and identification. However, challenges arise in visually representing these entities, with design decisions crucial for enhancing user interaction. Recent research highlights the potential of user-defined design, or customization, which goes beyond mere visual resemblance to the user. Understanding how identification with virtual avatars influences user experiences, especially in psychological interventions, is pivotal. In a study exploring this, 22 participants created virtual agents either similar or dissimilar to themselves, which then addressed their dysfunctional thoughts. Results indicate that similarity between users and virtual agents not only boosts identification but also positively impacts emotions and motivation, enhancing interest and enjoyment. This study sheds light on the significance of customization and identification, particularly in computer-assisted therapy tools, underscoring the importance of visual design for optimizing user experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09544v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Navid Ashrafi, Vanessa Neuhaus, Francesco Vona, Nicolina Laura Peperkorn, Youssef Shiban, Jan-Niklas Voigt-Antons</dc:creator>
    </item>
    <item>
      <title>"Are You Really Sure?" Understanding the Effects of Human Self-Confidence Calibration in AI-Assisted Decision Making</title>
      <link>https://arxiv.org/abs/2403.09552</link>
      <description>arXiv:2403.09552v1 Announce Type: new 
Abstract: In AI-assisted decision-making, it is crucial but challenging for humans to achieve appropriate reliance on AI. This paper approaches this problem from a human-centered perspective, "human self-confidence calibration". We begin by proposing an analytical framework to highlight the importance of calibrated human self-confidence. In our first study, we explore the relationship between human self-confidence appropriateness and reliance appropriateness. Then in our second study, We propose three calibration mechanisms and compare their effects on humans' self-confidence and user experience. Subsequently, our third study investigates the effects of self-confidence calibration on AI-assisted decision-making. Results show that calibrating human self-confidence enhances human-AI team performance and encourages more rational reliance on AI (in some aspects) compared to uncalibrated baselines. Finally, we discuss our main findings and provide implications for designing future AI-assisted decision-making interfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09552v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuai Ma, Xinru Wang, Ying Lei, Chuhan Shi, Ming Yin, Xiaojuan Ma</dc:creator>
    </item>
    <item>
      <title>DungeonMaker: Embedding Tangible Creation and Destruction in Hybrid Board Games through Personal Fabrication Technology</title>
      <link>https://arxiv.org/abs/2403.09592</link>
      <description>arXiv:2403.09592v1 Announce Type: new 
Abstract: Hybrid board games (HBGs) augment their analog origins digitally (e.g., through apps) and are an increasingly popular pastime activity. Continuous world and character development and customization, known to facilitate engagement in video games, remain rare in HBGs. If present, they happen digitally or imaginarily, often leaving physical aspects generic. We developed DungeonMaker, a fabrication-augmented HBG bridging physical and digital game elements: 1) the setup narrates a story and projects a digital game board onto a laser cutter; 2) DungeonMaker assesses player-crafted artifacts; 3) DungeonMaker's modified laser head senses and moves player- and non-player figures, and 4) can physically damage figures. An evaluation (n=4x3) indicated that DungeonMaker provides an engaging experience, may support players' connection to their figures, and potentially spark novices' interest in fabrication. DungeonMaker provides a rich constellation to play HBGs by blending aspects of craft and automation to couple the physical and digital elements of an HBG tightly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09592v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3613904.3642243</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI '24), May 11-16, 2024, Honolulu, HI, USA</arxiv:journal_reference>
      <dc:creator>Evgeny Stemasov, Tobias Wagner, Ali Askari, Jessica Janek, Omid Rajabi, Anja Schikorr, Julian Frommel, Jan Gugenheimer, Enrico Rukzio</dc:creator>
    </item>
    <item>
      <title>pARam: Leveraging Parametric Design in Extended Reality to Support the Personalization of Artifacts for Personal Fabrication</title>
      <link>https://arxiv.org/abs/2403.09607</link>
      <description>arXiv:2403.09607v1 Announce Type: new 
Abstract: Extended Reality (XR) allows in-situ previewing of designs to be manufactured through Personal Fabrication (PF). These in-situ interactions exhibit advantages for PF, like incorporating the environment into the design process. However, design-for-fabrication in XR often happens through either highly complex 3D-modeling or is reduced to rudimentary adaptations of crowd-sourced models. We present pARam, a tool combining parametric designs (PDs) and XR, enabling in-situ configuration of artifacts for PF. In contrast to modeling- or search-focused approaches, pARam supports customization through embodied and practical inputs (e.g., gestures, recommendations) and evaluation (e.g., lighting estimation) without demanding complex 3D-modeling skills. We implemented pARam for HoloLens 2 and evaluated it (n=20), comparing XR and desktop conditions. Users succeeded in choosing context-related parameters and took their environment into account for their configuration using pARam. We reflect on the prospects and challenges of PDs in XR to streamline complex design methods for PF while retaining suitable expressivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09607v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3613904.3642083</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI '24), May 11-16, 2024, Honolulu, HI, USA</arxiv:journal_reference>
      <dc:creator>Evgeny Stemasov, Simon Demharter, Max R\"adler, Jan Gugenheimer, Enrico Rukzio</dc:creator>
    </item>
    <item>
      <title>PrompTHis: Visualizing the Process and Influence of Prompt Editing during Text-to-Image Creation</title>
      <link>https://arxiv.org/abs/2403.09615</link>
      <description>arXiv:2403.09615v1 Announce Type: new 
Abstract: Generative text-to-image models, which allow users to create appealing images through a text prompt, have seen a dramatic increase in popularity in recent years. However, most users have a limited understanding of how such models work and it often requires many trials and errors to achieve satisfactory results. The prompt history contains a wealth of information that could provide users with insights into what have been explored and how the prompt changes impact the output image, yet little research attention has been paid to the visual analysis of such process to support users. We propose the Image Variant Graph, a novel visual representation designed to support comparing prompt-image pairs and exploring the editing history. The Image Variant Graph models prompt differences as edges between corresponding images and presents the distances between images through projection. Based on the graph, we developed the PrompTHis system through co-design with artists. Besides Image Variant Graph, PrompTHis also incorporates a detailed prompt-image history and a navigation mini-map. Based on the review and analysis of the prompting history, users can better understand the impact of prompt changes and have a more effective control of image generation. A quantitative user study with eleven amateur participants and qualitative interviews with five professionals and one amateur user were conducted to evaluate the effectiveness of PrompTHis. The results demonstrate PrompTHis can help users review the prompt history, make sense of the model, and plan their creative process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09615v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuhan Guo, Hanning Shao, Can Liu, Kai Xu, Xiaoru Yuan</dc:creator>
    </item>
    <item>
      <title>Bridging Human Concepts and Computer Vision for Explainable Face Verification</title>
      <link>https://arxiv.org/abs/2403.08789</link>
      <description>arXiv:2403.08789v1 Announce Type: cross 
Abstract: With Artificial Intelligence (AI) influencing the decision-making process of sensitive applications such as Face Verification, it is fundamental to ensure the transparency, fairness, and accountability of decisions. Although Explainable Artificial Intelligence (XAI) techniques exist to clarify AI decisions, it is equally important to provide interpretability of these decisions to humans. In this paper, we present an approach to combine computer and human vision to increase the explanation's interpretability of a face verification algorithm. In particular, we are inspired by the human perceptual process to understand how machines perceive face's human-semantic areas during face comparison tasks. We use Mediapipe, which provides a segmentation technique that identifies distinct human-semantic facial regions, enabling the machine's perception analysis. Additionally, we adapted two model-agnostic algorithms to provide human-interpretable insights into the decision-making processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08789v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miriam Doh (UMons, IRIDIA), Caroline Mazini Rodrigues (LRDE, LIGM), Nicolas Boutry (LRDE), Laurent Najman (LIGM), Matei Mancas (UMONS), Hugues Bersini (IRIDIA)</dc:creator>
    </item>
    <item>
      <title>User Identification via Free Roaming Eye Tracking Data</title>
      <link>https://arxiv.org/abs/2403.09415</link>
      <description>arXiv:2403.09415v1 Announce Type: cross 
Abstract: We present a new dataset of "free roaming" (FR) and "targeted roaming" (TR): a pool of 41 participants is asked to walk around a university campus (FR) or is asked to find a particular room within a library (TR). Eye movements are recorded using a commodity wearable eye tracker (Pupil Labs Neon at 200Hz). On this dataset we investigate the accuracy of user identification using a previously known machine learning pipeline where a Radial Basis Function Network (RBFN) is used as classifier. Our highest accuracies are 87.3% for FR and 89.4% for TR. This should be compared to 95.3% which is the (corresponding) highest accuracy we are aware of (achieved in a laboratory setting using the "RAN" stimulus of the BioEye 2015 competition dataset). To the best of our knowledge, our results are the first that study user identification in a non laboratory setting; such settings are often more feasible than laboratory settings and may include further advantages. The minimum duration of each recording is 263s for FR and 154s for TR. Our best accuracies are obtained when restricting to 120s and 140s for FR and TR respectively, always cut from the end of the trajectories (both for the training and testing sessions). If we cut the same length from the beginning, then accuracies are 12.2% lower for FR and around 6.4% lower for TR. On the full trajectories accuracies are lower by 5% and 52% for FR and TR. We also investigate the impact of including higher order velocity derivatives (such as acceleration, jerk, or jounce).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09415v1</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rishabh Vallabh Varsha Haria, Amin El Abed, Sebastian Maneth</dc:creator>
    </item>
    <item>
      <title>MambaTalk: Efficient Holistic Gesture Synthesis with Selective State Space Models</title>
      <link>https://arxiv.org/abs/2403.09471</link>
      <description>arXiv:2403.09471v1 Announce Type: cross 
Abstract: Gesture synthesis is a vital realm of human-computer interaction, with wide-ranging applications across various fields like film, robotics, and virtual reality. Recent advancements have utilized the diffusion model and attention mechanisms to improve gesture synthesis. However, due to the high computational complexity of these techniques, generating long and diverse sequences with low latency remains a challenge. We explore the potential of state space models (SSMs) to address the challenge, implementing a two-stage modeling strategy with discrete motion priors to enhance the quality of gestures. Leveraging the foundational Mamba block, we introduce MambaTalk, enhancing gesture diversity and rhythm through multimodal integration. Extensive experiments demonstrate that our method matches or exceeds the performance of state-of-the-art models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09471v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zunnan Xu, Yukang Lin, Haonan Han, Sicheng Yang, Ronghui Li, Yachao Zhang, Xiu Li</dc:creator>
    </item>
    <item>
      <title>The COMMOTIONS Urban Interactions Driving Simulator Study Dataset</title>
      <link>https://arxiv.org/abs/2305.11909</link>
      <description>arXiv:2305.11909v2 Announce Type: replace 
Abstract: Accurate modelling of road user interaction has received lot of attention in recent years due to the advent of increasingly automated vehicles. To support such modelling, there is a need to complement naturalistic datasets of road user interaction with targeted, controlled study data. This paper describes a dataset collected in a simulator study conducted in the project COMMOTIONS, addressing urban driving interactions, in a state of the art moving base driving simulator. The study focused on two types of near-crash situations that can arise in urban driving interactions, and also collected data on human driver gap acceptance across a range of controlled gap sequences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.11909v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aravinda Ramakrishnan Srinivasan, Julian Schumann, Yueyang Wang, Yi-Shin Lin, Michael Daly, Albert Solernou, Arkady Zgonnikov, Matteo Leonetti, Jac Billington, Gustav Markkula</dc:creator>
    </item>
    <item>
      <title>Staying at the Roach Motel: Cross-Country Analysis of Manipulative Subscription and Cancellation Flows</title>
      <link>https://arxiv.org/abs/2309.17145</link>
      <description>arXiv:2309.17145v3 Announce Type: replace 
Abstract: Subscribing to online services is typically a straightforward process, but cancelling them can be arduous and confusing -- causing many to resign and continue paying for services they no longer use. Making the cancellation intentionally difficult is recognized as a dark pattern called Roach Motel. This paper characterizes the subscription and cancellation flows of popular news websites from four different countries, and discusses them in the context of recent regulatory changes. We study the design features that make it difficult to cancel a subscription and find several cancellation flows that feature intentional barriers, such as forcing users to type in a phrase or call a representative. Further, we find many subscription flows that do not adequately inform users about recurring charges. Our results point to a growing need for effective regulation of designs that trick, coerce, or manipulate users into paying for subscriptions they do not want.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.17145v3</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashley Sheil, Gunes Acar, Hanna Schraffenberger, Rapha\"el Gellert, David Malone</dc:creator>
    </item>
    <item>
      <title>Luminate: Structured Generation and Exploration of Design Space with Large Language Models for Human-AI Co-Creation</title>
      <link>https://arxiv.org/abs/2310.12953</link>
      <description>arXiv:2310.12953v3 Announce Type: replace 
Abstract: Thanks to their generative capabilities, large language models (LLMs) have become an invaluable tool for creative processes. These models have the capacity to produce hundreds and thousands of visual and textual outputs, offering abundant inspiration for creative endeavors. But are we harnessing their full potential? We argue that current interaction paradigms fall short, guiding users towards rapid convergence on a limited set of ideas, rather than empowering them to explore the vast latent design space in generative models. To address this limitation, we propose a framework that facilitates the structured generation of design space in which users can seamlessly explore, evaluate, and synthesize a multitude of responses. We demonstrate the feasibility and usefulness of this framework through the design and development of an interactive system, Luminate, and a user study with 14 professional writers. Our work advances how we interact with LLMs for creative tasks, introducing a way to harness the creative potential of LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.12953v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3613904.3642400</arxiv:DOI>
      <dc:creator>Sangho Suh, Meng Chen, Bryan Min, Toby Jia-Jun Li, Haijun Xia</dc:creator>
    </item>
    <item>
      <title>Chatbots as social companions: How people perceive consciousness, human likeness, and social health benefits in machines</title>
      <link>https://arxiv.org/abs/2311.10599</link>
      <description>arXiv:2311.10599v3 Announce Type: replace 
Abstract: As artificial intelligence (AI) becomes more widespread, one question that arises is how human-AI interaction might impact human-human interaction. Chatbots, for example, are increasingly used as social companions, and while much is speculated, little is known empirically about how their use impacts human relationships. A common hypothesis is that relationships with companion chatbots are detrimental to social health by harming or replacing human interaction, but this hypothesis may be too simplistic, especially considering the social needs of users and the health of their preexisting human relationships. To understand how relationships with companion chatbots impact social health, we studied people who regularly used companion chatbots and people who did not use them. Contrary to expectations, companion chatbot users indicated that these relationships were beneficial to their social health, whereas non-users viewed them as harmful. Another common assumption is that people perceive conscious, humanlike AI as disturbing and threatening. Among both users and non-users, however, we found the opposite: perceiving companion chatbots as more conscious and humanlike correlated with more positive opinions and more pronounced social health benefits. Detailed accounts from users suggested that these humanlike chatbots may aid social health by supplying reliable and safe interactions, without necessarily harming human relationships, but this may depend on users' preexisting social needs and how they perceive both human likeness and mind in the chatbot.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.10599v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rose Guingrich, Michael S. A. Graziano</dc:creator>
    </item>
    <item>
      <title>Generative artificial intelligence enhances creativity but reduces the diversity of novel content</title>
      <link>https://arxiv.org/abs/2312.00506</link>
      <description>arXiv:2312.00506v3 Announce Type: replace 
Abstract: Creativity is core to being human. Generative artificial intelligence (GenAI) holds promise for humans to be more creative by offering new ideas, or less creative by anchoring on GenAI ideas. We study the causal impact of GenAI on the production of a creative output in an online experimental study where some writers are could obtain ideas for a story from a GenAI platform. Access to GenAI ideas causes an increase in the writer's creativity with stories being evaluated as better written and more enjoyable, especially among less creative writers. However, GenAI-enabled stories are more similar to each other than stories by humans alone. Our results have implications for researchers, policy-makers and practitioners interested in bolstering creativity, but point to potential downstream consequences from over-reliance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.00506v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anil R. Doshi, Oliver P. Hauser</dc:creator>
    </item>
    <item>
      <title>Ergonomic Design of Computer Laboratory Furniture: Mismatch Analysis Utilizing Anthropometric Data of University Students</title>
      <link>https://arxiv.org/abs/2403.05589</link>
      <description>arXiv:2403.05589v2 Announce Type: replace 
Abstract: Many studies have shown how ergonomically designed furniture improves productivity and well-being. As computers have become a part of students' academic lives, they will grow further in the future. We propose anthropometric-based furniture dimensions suitable for university students to improve computer laboratory ergonomics. We collected data from 380 participants and analyzed 11 anthropometric measurements, correlating them to 11 furniture dimensions. Two types of furniture were studied: a non-adjustable chair with a non-adjustable table and an adjustable chair with a non-adjustable table. The mismatch calculation showed a significant difference between furniture dimensions and anthropometric measurements. The one-way ANOVA test with a significance level of 5% also showed a significant difference between proposed and existing furniture dimensions. The proposed dimensions were found to be more compatible and reduced mismatch percentages for both males and females compared to existing furniture. The proposed dimensions of the furniture set with adjustable seat height showed slightly improved results compared to the non-adjustable furniture set. This suggests that the proposed dimensions can improve comfort levels and reduce the risk of musculoskeletal disorders among students. Further studies on the implementation and long-term effects of these proposed dimensions in real-world computer laboratory settings are recommended.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05589v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anik Kumar Saha, Md Abrar Jahin, Md. Rafiquzzaman, M. F. Mridha</dc:creator>
    </item>
    <item>
      <title>Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion</title>
      <link>https://arxiv.org/abs/2403.07721</link>
      <description>arXiv:2403.07721v3 Announce Type: replace 
Abstract: How to decode human vision through neural signals has attracted a long-standing interest in neuroscience and machine learning. Modern contrastive learning and generative models improved the performance of fMRI-based visual decoding and reconstruction. However, the high cost and low temporal resolution of fMRI limit their applications in brain-computer interfaces (BCIs), prompting a high need for EEG-based visual reconstruction. In this study, we present an EEG-based visual reconstruction framework. It consists of a plug-and-play EEG encoder called the Adaptive Thinking Mapper (ATM), which is aligned with image embeddings, and a two-stage EEG guidance image generator that first transforms EEG features into image priors and then reconstructs the visual stimuli with a pre-trained image generator. Our approach allows EEG embeddings to achieve superior performance in image classification and retrieval tasks. Our two-stage image generation strategy vividly reconstructs images seen by humans. Furthermore, we analyzed the impact of signals from different time windows and brain regions on decoding and reconstruction. The versatility of our framework is demonstrated in the magnetoencephalogram (MEG) data modality. We report that EEG-based visual decoding achieves SOTA performance, highlighting the portability, low cost, and high temporal resolution of EEG, enabling a wide range of BCI applications. The code of ATM is available at https://github.com/dongyangli-del/EEG_Image_decode.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07721v3</guid>
      <category>cs.HC</category>
      <category>eess.SP</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongyang Li, Chen Wei, Shiying Li, Jiachen Zou, Quanying Liu</dc:creator>
    </item>
    <item>
      <title>Making Language Models Better Tool Learners with Execution Feedback</title>
      <link>https://arxiv.org/abs/2305.13068</link>
      <description>arXiv:2305.13068v3 Announce Type: replace-cross 
Abstract: Tools serve as pivotal interfaces that enable humans to understand and reshape the environment. With the advent of foundation models, AI systems can utilize tools to expand their capabilities and interact with the real world. Existing tool learning methodologies, encompassing supervised fine-tuning and prompt engineering approaches, often induce large language models to utilize tools indiscriminately, as complex tasks often exceed their own competencies. However, introducing tools for simple tasks, which the models themselves can readily resolve, can inadvertently propagate errors rather than enhance performance. This leads to the research question: can we teach language models when and how to use tools? To meet this need, we propose Tool leaRning wIth exeCution fEedback (TRICE), a two-stage end-to-end framework that enables the model to continually learn through feedback derived from tool execution, thereby learning when and how to use tools effectively. Experimental results, backed by further analysis, show that TRICE can make the large language model selectively use tools by improving the accuracy of tool usage while enhancing insufficient tool learning and mitigating excessive reliance on tools. Code is available at https://github.com/zjunlp/TRICE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.13068v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuofei Qiao, Honghao Gui, Chengfei Lv, Qianghuai Jia, Huajun Chen, Ningyu Zhang</dc:creator>
    </item>
    <item>
      <title>The Impact of Explanations on Fairness in Human-AI Decision-Making: Protected vs Proxy Features</title>
      <link>https://arxiv.org/abs/2310.08617</link>
      <description>arXiv:2310.08617v2 Announce Type: replace-cross 
Abstract: AI systems have been known to amplify biases in real-world data. Explanations may help human-AI teams address these biases for fairer decision-making. Typically, explanations focus on salient input features. If a model is biased against some protected group, explanations may include features that demonstrate this bias, but when biases are realized through proxy features, the relationship between this proxy feature and the protected one may be less clear to a human. In this work, we study the effect of the presence of protected and proxy features on participants' perception of model fairness and their ability to improve demographic parity over an AI alone. Further, we examine how different treatments -- explanations, model bias disclosure and proxy correlation disclosure -- affect fairness perception and parity. We find that explanations help people detect direct but not indirect biases. Additionally, regardless of bias type, explanations tend to increase agreement with model biases. Disclosures can help mitigate this effect for indirect biases, improving both unfairness recognition and decision-making fairness. We hope that our findings can help guide further research into advancing explanations in support of fair human-AI decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.08617v2</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Navita Goyal, Connor Baumler, Tin Nguyen, Hal Daum\'e III</dc:creator>
    </item>
  </channel>
</rss>
