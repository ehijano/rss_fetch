<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 16 Oct 2025 04:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Recent Advances in Debordering Methods</title>
      <link>https://arxiv.org/abs/2510.13049</link>
      <description>arXiv:2510.13049v1 Announce Type: new 
Abstract: Border complexity captures functions that can be approximated by low-complexity ones. Debordering is the task of proving an upper bound on some non-border complexity measure in terms of a border complexity measure, thus getting rid of limits. Debordering lies at the heart of foundational complexity theory questions relating Valiant's determinant versus permanent conjecture (1979) and its geometric complexity theory (GCT) variant proposed by Mulmuley and Sohoni (2001). The debordering of matrix multiplication tensors by Bini (1980) played a pivotal role in the development of efficient matrix multiplication algorithms. Consequently, debordering finds applications in both establishing computational complexity lower bounds and facilitating algorithm design. Recent years have seen notable progress in debordering various restricted border complexity measures. In this survey, we highlight these advances and discuss techniques underlying them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13049v1</guid>
      <category>cs.CC</category>
      <category>cs.SC</category>
      <category>math.AG</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pranjal Dutta, Vladimir Lysikov</dc:creator>
    </item>
    <item>
      <title>Order Retrieval in Compact Storage Systems</title>
      <link>https://arxiv.org/abs/2510.13268</link>
      <description>arXiv:2510.13268v1 Announce Type: new 
Abstract: Growing demand for sustainable logistics and higher space utilization, driven by e-commerce and urbanization, increases the need for storage systems that are both energy- and space-efficient. Compact storage systems aim to maximize space utilization in limited storage areas and are therefore particularly suited in densely-populated urban areas where space is scarce. In this paper, we examine a recently introduced compact storage system in which uniformly shaped bins are stacked directly on top of each other, eliminating the need for aisles used to handle materials. Target bins are retrieved in a fully automated process by first lifting all other bins that block access and then accessing the target bin from the side of the system by a dedicated robot. Consequently, retrieving a bin can require substantial lifting effort, and thus energy. However, this energy can be reduced through smart retrieval strategies. From an operational perspective, we investigate how retrievals can be optimized with respect to energy consumption.
  We model the retrieval problem within a mathematical framework. We show that the problem is strongly NP-complete and derive structural insights. Building on these insights, we propose two exact methods: a mixed-integer programming (MIP) formulation and a dynamic programming algorithm, along with a simple, practitioner-oriented greedy algorithm that yields near-instant solutions. Numerical experiments reveal that dynamic programming consistently outperforms state-of-the-art MIP solvers in small to medium sized instances, while the greedy algorithm delivers satisfactory performance, especially when exact methods become computationally impractical.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13268v1</guid>
      <category>cs.CC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Malte Fliedner, Julian Golak, Ya\u{g}mur G\"ul, Simone Neumann</dc:creator>
    </item>
    <item>
      <title>VC-Dimension vs Degree: An Uncertainty Principle for Boolean Functions</title>
      <link>https://arxiv.org/abs/2510.13705</link>
      <description>arXiv:2510.13705v1 Announce Type: cross 
Abstract: In this paper, we uncover a new uncertainty principle that governs the complexity of Boolean functions. This principle manifests as a fundamental trade-off between two central measures of complexity: a combinatorial complexity of its supported set, captured by its Vapnik-Chervonenkis dimension ($\mathrm{VC}(f)$), and its algebraic structure, captured by its polynomial degree over various fields. We establish two primary inequalities that formalize this trade-off:$\mathrm{VC}(f)+\deg(f)\ge n,$ and $\mathrm{VC}(f)+\deg_{\mathbb{F}_2}(f)\ge n$. In particular, these results recover the classical uncertainty principle on the discrete hypercube, as well as the Sziklai--Weiner's bound in the case of $\mathbb{F}_2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13705v1</guid>
      <category>math.CO</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fan Chang, Yijia Fang</dc:creator>
    </item>
    <item>
      <title>k-SUM Hardness Implies Treewidth-SETH</title>
      <link>https://arxiv.org/abs/2510.08185</link>
      <description>arXiv:2510.08185v2 Announce Type: replace 
Abstract: We show that if k-SUM is hard, in the sense that the standard algorithm is essentially optimal, then a variant of the SETH called the Primal Treewidth SETH is true. Formally: if there is an $\varepsilon&gt;0$ and an algorithm which solves SAT in time $(2-\varepsilon)^{tw}|\phi|^{O(1)}$, where $tw$ is the width of a given tree decomposition of the primal graph of the input, then there exists a randomized algorithm which solves k-SUM in time $n^{(1-\delta)\frac{k}{2}}$ for some $\delta&gt;0$ and all sufficiently large $k$. We also establish an analogous result for the k-XOR problem, where integer addition is replaced by component-wise addition modulo $2$.
  As an application of our reduction we are able to revisit tight lower bounds on the complexity of several fundamental problems parameterized by treewidth (Independent Set, Max Cut, $k$-Coloring). Our results imply that these bounds, which were initially shown under the SETH, also hold if one assumes the k-SUM or k-XOR Hypotheses, arguably increasing our confidence in their validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08185v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Lampis</dc:creator>
    </item>
    <item>
      <title>The Algorithmic Regulator</title>
      <link>https://arxiv.org/abs/2510.10300</link>
      <description>arXiv:2510.10300v3 Announce Type: replace 
Abstract: The regulator theorem states that, under certain conditions, any optimal controller must embody a model of the system it regulates, grounding the idea that controllers embed, explicitly or implicitly, internal models of the controlled. This principle underpins neuroscience and predictive brain theories like the Free-Energy Principle or Kolmogorov/Algorithmic Agent theory. However, the theorem is only proven in limited settings. Here, we treat the deterministic, closed, coupled world-regulator system $(W,R)$ as a single self-delimiting program $p$ via a constant-size wrapper that produces the world output string~$x$ fed to the regulator. We analyze regulation from the viewpoint of the algorithmic complexity of the output, $K(x)$. We define $R$ to be a \emph{good algorithmic regulator} if it \emph{reduces} the algorithmic complexity of the readout relative to a null (unregulated) baseline $\varnothing$, i.e., \[ \Delta = K\big(O_{W,\varnothing}\big) - K\big(O_{W,R}\big) &gt; 0. \] We then prove that the larger $\Delta$ is, the more world-regulator pairs with high mutual algorithmic information are favored. More precisely, a complexity gap $\Delta &gt; 0$ yields \[ \Pr\big((W,R)\mid x\big) \le C\,2^{\,M(W{:}R)}\,2^{-\Delta}, \] making low $M(W{:}R)$ exponentially unlikely as $\Delta$ grows. This is an AIT version of the idea that ``the regulator contains a model of the world.'' The framework is distribution-free, applies to individual sequences, and complements the Internal Model Principle. Beyond this necessity claim, the same coding-theorem calculus singles out a \emph{canonical scalar objective} and implicates a \emph{planner}. On the realized episode, a regulator behaves \emph{as if} it minimized the conditional description length of the readout.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10300v3</guid>
      <category>cs.CC</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.IT</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Giulio Ruffini</dc:creator>
    </item>
    <item>
      <title>On the consistency of stronger lower bounds for NEXP</title>
      <link>https://arxiv.org/abs/2504.03320</link>
      <description>arXiv:2504.03320v3 Announce Type: replace-cross 
Abstract: It was recently shown by Atserias, Buss and Mueller that the standard complexity-theoretic conjecture NEXP not in P / poly is consistent with the relatively strong bounded arithmetic theory V^0_2, which can prove a substantial part of complexity theory. We observe that their approach can be extended to show that the stronger conjectures NEXP not in EXP / poly and NEXP not in coNEXP are consistent with a stronger theory, which includes every true universal number-sort sentence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03320v3</guid>
      <category>cs.LO</category>
      <category>cs.CC</category>
      <category>math.LO</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Neil Thapen</dc:creator>
    </item>
  </channel>
</rss>
