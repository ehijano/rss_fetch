<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PL</link>
    <description>cs.PL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 06 Aug 2025 04:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Compositional Quantum Control Flow with Efficient Compilation in Qunity</title>
      <link>https://arxiv.org/abs/2508.02857</link>
      <description>arXiv:2508.02857v1 Announce Type: new 
Abstract: Most existing quantum programming languages are based on the quantum circuit model of computation, as higher-level abstractions are particularly challenging to implement - especially ones relating to quantum control flow. The Qunity language, proposed by Voichick et al., offered such an abstraction in the form of a quantum control construct, with great care taken to ensure that the resulting language is still realizable. However, Qunity lacked a working implementation, and the originally proposed compilation procedure was very inefficient, with even simple quantum algorithms compiling to unreasonably large circuits.
  In this work, we focus on the efficient compilation of high-level quantum control flow constructs, using Qunity as our starting point. We introduce a wider range of abstractions on top of Qunity's core language that offer compelling trade-offs compared to its existing control construct. We create a complete implementation of a Qunity compiler, which converts high-level Qunity code into the quantum assembly language OpenQASM 3. We develop optimization techniques for multiple stages of the Qunity compilation procedure, including both low-level circuit optimizations as well as methods that consider the high-level structure of a Qunity program, greatly reducing the number of qubits and gates used by the compiler.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02857v1</guid>
      <category>cs.PL</category>
      <category>quant-ph</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mikhail Mints, Finn Voichick, Leonidas Lampropoulos, Robert Rand</dc:creator>
    </item>
    <item>
      <title>SAGE-HLS: Syntax-Aware AST-Guided LLM for High-Level Synthesis Code Generation</title>
      <link>https://arxiv.org/abs/2508.03558</link>
      <description>arXiv:2508.03558v1 Announce Type: new 
Abstract: In today's rapidly evolving field of electronic design automation (EDA), the complexity of hardware designs is increasing, necessitating more sophisticated automation solutions. High-level synthesis (HLS), as a pivotal solution, automates hardware designs from high-level abstractions (e.g., C/C++). However, it faces significant challenges, particularly in design space exploration and optimization. While large language models (LLMs) have shown notable capabilities in code generation, their application to HLS has been limited due to the scarcity of (publicly) available HLS code datasets. Hence, research in this domain has primarily focused on techniques such as prompt engineering and retrieval-augmented generation (RAG). To overcome this limitation, this paper introduces SAGE-HLS, the first-of-its-kind fine-tuned LLM specifically for HLS code generation. Our method includes three key advancements: (i) We implement Verilog-to-C/C++ porting, converting verified and synthesizable Verilog codes into corresponding C, creating a dataset of 16.7K HLS codes; (ii) We implement a fine-tuning strategy, which is based on instruction prompting to code generation guided by abstract syntax tree (AST); (iii) We develop a semi-automated evaluation framework using VerilogEval to assess the functionality of the generated HLS code. Our experiments show that SAGE-HLS, fined-tuned on the QwenCoder (2.5) 7B model, achieves a near 100% success rate in code synthesizability and a 75% success rate in functional correctness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03558v1</guid>
      <category>cs.PL</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M Zafir Sadik Khan, Nowfel Mashnoor, Mohammad Akyash, Kimia Azar, Hadi Kamali</dc:creator>
    </item>
    <item>
      <title>Teaching Introductory Functional Programming Using Haskelite</title>
      <link>https://arxiv.org/abs/2508.03640</link>
      <description>arXiv:2508.03640v1 Announce Type: new 
Abstract: Learning functional programming requires learning a substitution-based computational model. While substitution should be a familiar concept from high-school algebra, students often have difficulty applying it to new settings, such as recursive definitions, algebraic data types and higher-order functions. Step-by-step interpreters have been shown to help beginners by clarifying misconceptions and improving understanding.
  This paper reports on the experience of using a step-by-step tracing interpreter for a subset of Haskell while teaching an introductory functional programming course at the University of Porto.  We describe the use of the interpreter, present some feedback obtained from students, reflect on the lessons learned and point directions for further work.
</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03640v1</guid>
      <category>cs.PL</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.424.3</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 424, 2025, pp. 49-66</arxiv:journal_reference>
      <dc:creator>Pedro Vasconcelos (University of Porto)</dc:creator>
    </item>
    <item>
      <title>Blueprint First, Model Second: A Framework for Deterministic LLM Workflow</title>
      <link>https://arxiv.org/abs/2508.02721</link>
      <description>arXiv:2508.02721v1 Announce Type: cross 
Abstract: While powerful, the inherent non-determinism of large language model (LLM) agents limits their application in structured operational environments where procedural fidelity and predictable execution are strict requirements. This limitation stems from current architectures that conflate probabilistic, high-level planning with low-level action execution within a single generative process. To address this, we introduce the Source Code Agent framework, a new paradigm built on the "Blueprint First, Model Second" philosophy. Our framework decouples the workflow logic from the generative model. An expert-defined operational procedure is first codified into a source code-based Execution Blueprint, which is then executed by a deterministic engine. The LLM is strategically invoked as a specialized tool to handle bounded, complex sub-tasks within the workflow, but never to decide the workflow's path. We conduct a comprehensive evaluation on the challenging tau-bench benchmark, designed for complex user-tool-rule scenarios. Our results demonstrate that the Source Code Agent establishes a new state-of-the-art, outperforming the strongest baseline by 10.1 percentage points on the average Pass^1 score while dramatically improving execution efficiency. Our work enables the verifiable and reliable deployment of autonomous agents in applications governed by strict procedural logic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02721v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.PL</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Libin Qiu, Yuhang Ye, Zhirong Gao, Xide Zou, Junfu Chen, Ziming Gui, Weizhi Huang, Xiaobo Xue, Wenkai Qiu, Kun Zhao</dc:creator>
    </item>
    <item>
      <title>Automated Code Repair for C/C++ Static Analysis Alerts</title>
      <link>https://arxiv.org/abs/2508.02820</link>
      <description>arXiv:2508.02820v1 Announce Type: cross 
Abstract: (Note: This work is a preprint.) Static analysis (SA) tools produce many diagnostic alerts indicating that source code in C or C++ may be defective and potentially vulnerable to security exploits. Many of these alerts are false positives. Identifying the true-positive alerts and repairing the defects in the associated code are huge efforts that automated program repair (APR) tools can help with. Our experience showed us that APR can reduce the number of SA alerts significantly and reduce the manual effort of analysts to review code. This engineering experience paper details the application of design, development, and performance testing to an APR tool we built that repairs C/C++ code associated with 3 categories of alerts produced by multiple SA tools. Its repairs are simple and local. Furthermore, our findings convinced the maintainers of the CERT Coding Standards to re-assess and update the metrics used to assess when violations of guidelines are detectable or repairable. We discuss engineering design choices made to support goals of trustworthiness and acceptability to developers. Our APR tool repaired 8718 out of 9234 alerts produced by one SA tool on one codebase. It can repair 3 flaw categories. For 2 flaw categories, 2 SA tools, and 2 codebases, our tool repaired or dismissed as false positives over 80% of alerts, on average. Tests showed repairs did not appreciably degrade the performance of the code or cause new alerts to appear (with the possible exception of sqlite3.c). This paper describes unique contributions that include a new empirical analysis of SA data, our selection method for flaw categories to repair, publication of our APR tool, and a dataset of SA alerts from open-source SA tools run on open-source codebases. It discusses positive and negative results and lessons learned.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02820v1</guid>
      <category>cs.SE</category>
      <category>cs.PL</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>David Svoboda, Lori Flynn, William Klieber, Michael Duggan, Nicholas Reimer, Joseph Sible</dc:creator>
    </item>
    <item>
      <title>StoneDetector: Conventional and versatile code clone detection for Java</title>
      <link>https://arxiv.org/abs/2508.03435</link>
      <description>arXiv:2508.03435v1 Announce Type: cross 
Abstract: Copy &amp; paste is a widespread practice when developing software and, thus, duplicated and subsequently modified code occurs frequently in software projects. Since such code clones, i.e., identical or similar fragments of code, can bloat software projects and cause issues like bug or vulnerability propagation, their identification is of importance. In this paper, we present the StoneDetector platform and its underlying method for finding code clones in Java source and Bytecode. StoneDetector implements a conventional clone detection approach based upon the textual comparison of paths derived from the code's representation by dominator trees. In this way, the tool does not only find exact and syntactically similar near-miss code clones, but also code clones that are harder to detect due to their larger variety in the syntax. We demonstrate StoneDetector's versatility as a conventional clone detection platform and analyze its various available configuration parameters, including the usage of different string metrics, hashing algorithms, etc. In our exhaustive evaluation with other conventional clone detectors on several state-of-the-art benchmarks, we can show StoneDetector's performance and scalability in finding code clones in both, Java source and Bytecode.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03435v1</guid>
      <category>cs.SE</category>
      <category>cs.PL</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Thomas S. Heinze, Andr\'e Sch\"afer, Wolfram Amme</dc:creator>
    </item>
    <item>
      <title>ReFuzzer: Feedback-Driven Approach to Enhance Validity of LLM-Generated Test Programs</title>
      <link>https://arxiv.org/abs/2508.03603</link>
      <description>arXiv:2508.03603v1 Announce Type: cross 
Abstract: Existing LLM-based compiler fuzzers often produce syntactically or semantically invalid test programs, limiting their effectiveness in exercising compiler optimizations and backend components. We introduce ReFuzzer, a framework for refining LLM-generated test programs by systematically detecting and correcting compilation and runtime violations (e.g. division by zero or array out-of-bounds accesses). ReFuzzer employs a feedback loop with a local LLM to validate and filter erroneous programs before execution, improving fuzzing effectiveness beyond crash detection and enabling the generation of diverse yet valid test programs.
  We evaluated ReFuzzer's effectiveness across black-, grey- and white-box fuzzing approaches targeting LLVM/Clang. ReFuzzer improved test programs' validity from 47.0-49.4% to 96.6-97.3%, with an average processing time of 2.9-3.5 s per test program on a dual-GPU machine. Further, refuzzing significantly increased code coverage in critical optimization and IR generation components. For example, vectorization coverage had an absolute improvement of 9.2%, 2.3%, and 7.1% in black-, grey-, and white-box fuzzing, enhancing testing effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03603v1</guid>
      <category>cs.SE</category>
      <category>cs.PL</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Iti Shree, Karine Even-Mendoz, Tomasz Radzik</dc:creator>
    </item>
    <item>
      <title>Design Support for Multitape Turing Machines</title>
      <link>https://arxiv.org/abs/2508.03638</link>
      <description>arXiv:2508.03638v1 Announce Type: cross 
Abstract: Many Formal Languages and Automata Theory courses introduce students to Turing machine extensions. One of the most widely-used extensions endows Turing machines with multiple tapes. Although multitape Turing machines are an abstraction to simplify Turing machine design, students find them no less challenging. To aid students in understanding these machines, the FSM programming language provides support for their definition and execution. This, however, has proven insufficient for many students to understand the operational semantics of such machines and to understand why such machines accept or reject a word. To address this problem, three visualization tools have been developed. The first is a dynamic visualization tool that simulates machine execution. The second is a static visualization tool that automatically renders a graphic for a multitape Turing machine's transition diagram. The third is a static visualization tool that automatically renders computation graphs for multitape Turing machines. This article presents these tools and illustrates how they are used to help students design and implement multitape Turing machines. In addition, empirical data is presented that suggests these tools are well-received and found useful by students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03638v1</guid>
      <category>cs.FL</category>
      <category>cs.HC</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.424.1</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 424, 2025, pp. 1-24</arxiv:journal_reference>
      <dc:creator>Marco T. Moraz\'an (Seton Hall University), Oliwia Kempinski (University of Maryland), Andr\'es M. Garced (Seton Hall University)</dc:creator>
    </item>
    <item>
      <title>A Design Recipe and Recipe-Based Errors for Regular Expressions</title>
      <link>https://arxiv.org/abs/2508.03639</link>
      <description>arXiv:2508.03639v1 Announce Type: cross 
Abstract: This article presents a novel framework to provide Formal Languages and Automata Theory students design support for the development of regular expressions. This framework includes a design recipe for regular expressions and a customized error messaging system. The error messaging system produces recipe-based errors that include the step of the design recipe not successfully completed. Furthermore, the error messages follow the established practices of being concise, succinct, jargon-free, and nonprescriptive. In addition, a shorthand syntax developed for writing unit tests is described. The in-class use of the design recipe is illustrated, two debugging sessions using the described system are discussed, and the implementation of the error messaging system is briefly sketched.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03639v1</guid>
      <category>cs.FL</category>
      <category>cs.HC</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.424.2</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 424, 2025, pp. 25-48</arxiv:journal_reference>
      <dc:creator>Marco T. Moraz\'an (Seton Hall University), Shamil Dzhatdoyev (Axoni, USA), Josephine Des Rosiers (Penguin Random House), Tijana Mini\'c (University of Washington), Andr\'es M. Garced (Seton Hall University), David Anthony K. Fields (Seton Hall University)</dc:creator>
    </item>
    <item>
      <title>Visual Execution and Validation of Finite-State Machines and Pushdown Automata</title>
      <link>https://arxiv.org/abs/2508.03641</link>
      <description>arXiv:2508.03641v1 Announce Type: cross 
Abstract: In Formal Languages and Automata Theory courses, students find understanding nondeterministic finite-state and pushdown automata difficult. In many cases, this means that it is challenging for them to comprehend the operational semantics of such machines and, as a consequence, determine why a word is accepted or rejected. This is not entirely surprising, because students are mostly trained to design and implement deterministic programs. Comprehension of pushdown automata is further complicated, because reasoning about the stack is necessary. A common difficulty students face, for example, is understanding that two different computations on the same word may reach the same state with different stack values. To aid student understanding, we present two novel dynamic visualization tools for FSM -- a domain-specific programming language for the Automata Theory classroom -- to support the design of such machines. These tools visualize all computations that may be performed, respectively, by a nondeterministic finite-state machine or by a pushdown automata in a stepwise manner. In addition, these tools aid the machine verification process by allowing users to visually validate whether the properties a state represents hold when a machine transitions into it.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03641v1</guid>
      <category>cs.FL</category>
      <category>cs.HC</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.424.5</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 424, 2025, pp. 87-108</arxiv:journal_reference>
      <dc:creator>Marco T. Moraz\'an (Seton Hall University), David Anthony K. Fields (Seton Hall University), Andr\'es M. Garced (Seton Hall University), Tijana Mini\'c (University of Washington)</dc:creator>
    </item>
    <item>
      <title>More Than a Score: Probing the Impact of Prompt Specificity on LLM Code Generation</title>
      <link>https://arxiv.org/abs/2508.03678</link>
      <description>arXiv:2508.03678v1 Announce Type: cross 
Abstract: State-of-the-art Large Language Models (LLMs) achieve high pass@1 on general benchmarks like HumanEval but underperform on specialized suites such as ParEval. Is this due to LLMs missing domain knowledge or insufficient prompt detail is given? To answer this, we introduce PartialOrderEval, which augments any code generation benchmark with a partial order of prompts from minimal to maximally detailed. Applying it to HumanEval and both serial and OpenMP subsets of ParEval, we measure how pass@1 scales with prompt specificity. Our experiments with Llama-3.x and Qwen2.5-Coder demonstrate varying degrees of prompt sensitivity across different tasks, and a qualitative analysis highlights explicit I/O specifications, edge-case handling, and stepwise breakdowns as the key drivers of prompt detail improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03678v1</guid>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.PL</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yangtian Zi, Harshitha Menon, Arjun Guha</dc:creator>
    </item>
    <item>
      <title>Cobblestone: A Divide-and-Conquer Approach for Automating Formal Verification</title>
      <link>https://arxiv.org/abs/2410.19940</link>
      <description>arXiv:2410.19940v3 Announce Type: replace-cross 
Abstract: Formal verification using proof assistants, such as Coq, is an effective way of improving software quality, but requires significant effort and expertise. Machine learning can automatically synthesize proofs, but such tools are able to prove only a fraction of desired software properties. We introduce Cobblestone, a divide-and-conquer approach for proof synthesis. Cobblestone uses a large language model (LLM) to generate potential proofs, uses those proofs to break the problem into simpler parts, automatically identifies which of those parts were successfully proven, and iterates on the remaining parts to build a correct proof that is guaranteed to be sound, despite the reliance on unsound LLMs. We evaluate Cobblestone on four benchmarks of open-source Coq projects, controlling for training data leakage. Fully automatically, Cobblestone outperforms state-of-the-art non-LLM tools, and proves many theorems that other LLM-based tools cannot, and on many benchmarks, outperforms them. Each Cobblestone run costs only $1.25 and takes 14.7 minutes, on average. Cobblestone can also be used with external input, from a user or another tool, providing a proof structure or relevant lemmas. Evaluated with such an oracle, Cobblestone proves up to 58% of theorems. Overall, our research shows that tools can make use of partial progress and external input to more effectively automate formal verification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19940v3</guid>
      <category>cs.LO</category>
      <category>cs.AI</category>
      <category>cs.PL</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saketh Ram Kasibatla, Arpan Agarwal, Yuriy Brun, Sorin Lerner, Talia Ringer, Emily First</dc:creator>
    </item>
  </channel>
</rss>
