<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Dec 2025 02:38:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Reduced rank regression for neural communication: a tutorial for neuroscientists</title>
      <link>https://arxiv.org/abs/2512.12467</link>
      <description>arXiv:2512.12467v1 Announce Type: new 
Abstract: Reduced rank regression (RRR) is a statistical method for finding a low-dimensional linear mapping between a set of high-dimensional inputs and outputs. In recent years, RRR has found numerous applications in neuroscience, in particular for identifying "communication subspaces" governing the interactions between brain regions. This tutorial article seeks to provide an introduction to RRR and its mathematical foundations, with a particular emphasis on neural communication. We discuss RRR's relationship to alternate dimensionality reduction techniques such as singular value decomposition (SVD), principal components analysis (PCA), principal components regression (PCR), and canonical correlation analysis (CCA). We also derive important extensions to RRR, including ridge regularization and non-spherical noise. Finally, we introduce new metrics for quantifying communication strength as well as the alignment between communication axes and the principal modes of neural activity. By the end of this article, readers should have a clear understanding of RRR and the practical considerations involved in applying it to their own data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12467v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bichan Wu, Jonathan Pillow</dc:creator>
    </item>
    <item>
      <title>Random matrix theory of sparse neuronal networks with heterogeneous timescales</title>
      <link>https://arxiv.org/abs/2512.12767</link>
      <description>arXiv:2512.12767v1 Announce Type: new 
Abstract: Training recurrent neuronal networks consisting of excitatory (E) and inhibitory (I) units with additive noise for working memory computation slows and diversifies inhibitory timescales, leading to improved task performance that is attributed to emergent marginally stable equilibria [PNAS 122 (2025) e2316745122]. Yet the link between trained network characteristics and their roles in shaping desirable dynamical landscapes remains unexplored. Here, we investigate the Jacobian matrices describing the dynamics near these equilibria and show that they are sparse, non-Hermitian rectangular-block matrices modified by heterogeneous synaptic decay timescales and activation-function gains. We specify a random matrix ensemble that faithfully captures the spectra of trained Jacobian matrices, arising from the inhibitory core - excitatory periphery network motif (pruned E weights, broadly distributed I weights) observed post-training. An analytic theory of this ensemble is developed using statistical field theory methods: a Hermitized resolvent representation of the spectral density processed with a supersymmetry-based treatment in the style of Fyodorov and Mirlin. In this manner, an analytic description of the spectral edge is obtained, relating statistical parameters of the Jacobians (sparsity, weight variances, E/I ratio, and the distributions of timescales and gains) to near-critical features of the equilibria essential for robust working memory computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12767v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>math.PR</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thiparat Chotibut, Oleg Evnin, Weerawit Horinouchi</dc:creator>
    </item>
    <item>
      <title>A Disproof of Large Language Model Consciousness: The Necessity of Continual Learning for Consciousness</title>
      <link>https://arxiv.org/abs/2512.12802</link>
      <description>arXiv:2512.12802v1 Announce Type: new 
Abstract: The requirements for a falsifiable and non-trivial theory of consciousness significantly constrain such theories. Specifically, recent research on the Unfolding Argument and the Substitution Argument has given us formal tools to analyze requirements for a theory of consciousness. I show via a new Proximity Argument that these requirements especially constrain the potential consciousness of contemporary Large Language Models (LLMs) because of their proximity to systems that are equivalent to LLMs in terms of input/output function; yet, for these functionally equivalent systems, there cannot be any non-trivial theory of consciousness that judges them conscious. This forms the basis of a disproof of contemporary LLM consciousness. I then show a positive result, which is that theories of consciousness based on (or requiring) continual learning do satisfy the stringent formal constraints for a theory of consciousness in humans. Intriguingly, this work supports a hypothesis: If continual learning is linked to consciousness in humans, the current limitations of LLMs (which do not continually learn) are intimately tied to their lack of consciousness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12802v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erik Hoel</dc:creator>
    </item>
    <item>
      <title>Macular: a multi-scale simulation platform for the retina and the primary visual system</title>
      <link>https://arxiv.org/abs/2512.13052</link>
      <description>arXiv:2512.13052v1 Announce Type: new 
Abstract: We developed Macular, a simulation platform with a graphical interface, designed to produce in silico experiment scenarios for the retina and the primary visual system. A scenario consists of generating a three-dimensional structure with interconnected layers, each layer corresponding to a type of 'cell' in the retina or visual cortex. The cells can correspond to neurons or more complex structures (such as cortical columns). The inputs are arbitrary videos. The user can use the cells and synapses provided with the software, or create their own using a graphical interface where they enter the constituent equations in text format (e.g., LaTeX). They also create the three-dimensional structure via the graphical interface. Macular then automatically generates and compiles the C++ code and generates the simulation interface. This allows the user to view the input video and the three-dimensional structure in layers. It also allows the user to select cells and synapses in each layer and view the activity of their state variables. Finally, the user can adjust the phenomenological parameters of the cells or synapses via the interface. We provide several example scenarios, corresponding to published articles, including an example of a retino-cortical model. Macular was designed for neurobiologists and modelers, specialists in the primary visual system, who want to test hypotheses in silico without the need for programming. By design, this tool allows natural or altered conditions (pharmacology, pathology, development) to be simulated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13052v1</guid>
      <category>q-bio.NC</category>
      <category>physics.comp-ph</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Bruno Cessac, Erwan Demairy, J\'er\^ome Emonet, Evgenia Kartsaki, Thibaud Kloczko, C\^ome Le Breton, Nicolas Niclausse, Selma Souihel, Jean-Luc Szpyrka, Julien Wintz</dc:creator>
    </item>
    <item>
      <title>A Deep Learning Model of Mental Rotation Informed by Interactive VR Experiments</title>
      <link>https://arxiv.org/abs/2512.13517</link>
      <description>arXiv:2512.13517v1 Announce Type: new 
Abstract: Mental rotation -- the ability to compare objects seen from different viewpoints -- is a fundamental example of mental simulation and spatial world modelling in humans. Here we propose a mechanistic model of human mental rotation, leveraging advances in deep, equivariant, and neuro-symbolic learning. Our model consists of three stacked components: (1) an equivariant neural encoder, taking images as input and producing 3D spatial representations of objects, (2) a neuro-symbolic object encoder, deriving symbolic descriptions of objects from these spatial representations, and (3) a neural decision agent, comparing these symbolic descriptions to prescribe rotation simulations in 3D latent space via a recurrent pathway. Our model design is guided by the abundant experimental literature on mental rotation, which we complemented with experiments in VR where participants could at times manipulate the objects to compare, providing us with additional insights into the cognitive process of mental rotation. Our model captures well the performance, response times and behavior of participants in our and others' experiments. The necessity of each model component is shown through systematic ablations. Our work adds to a recent collection of deep neural models of human spatial reasoning, further demonstrating the potency of integrating deep, equivariant, and symbolic representations to model the human mind.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13517v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raymond Khazoum, Daniela Fernandes, Aleksandr Krylov, Qin Li, Stephane Deny</dc:creator>
    </item>
    <item>
      <title>Altered oscillatory brain networks during emotional face processing in ADHD: an eLORETA and functional ICA study</title>
      <link>https://arxiv.org/abs/2512.13539</link>
      <description>arXiv:2512.13539v1 Announce Type: new 
Abstract: Attention-deficit/hyperactivity disorder (ADHD) is characterized by executive dysfunction and difficulties in processing emotional facial expressions, yet the large-scale neural dynamics underlying these impairments remain insufficiently understood. This study applied network-based EEG source analysis to examine oscillatory cortical activity during cognitive and emotional Go/NoGo tasks in individuals with ADHD. EEG data from 272 participants (ADHD n equals 102, controls n equals 170, age range 6 to 60 years) were analyzed using exact low-resolution brain electromagnetic tomography combined with functional independent component analysis, yielding ten frequency-resolved cortical networks. Mixed-effects ANCOVAs were conducted on independent component loadings with Group, Task, and Condition as factors and age and sex as covariates. ADHD participants showed statistically significant but small increases in activation across several networks, including a gamma-dominant inferior temporal component showing a Group effect and a Group by Condition interaction with stronger NoGo-related activation in ADHD. Two additional components showed similar but weaker NoGo-selective patterns. A main effect of Task emerged only for one temporal delta component, with higher activation during the VCPT than the ECPT. No Group by Task interactions were observed. Behavioral results replicated the established ADHD performance profile, with slower responses, greater variability, and higher error rates, particularly during the emotional ECPT. Overall, the findings reveal subtle alterations in oscillatory brain networks during inhibitory processing in ADHD, with modest effect sizes embedded within substantial within-group variability. These results support a dimensional view of ADHD neurobiology and highlight the limited discriminative power of network-level EEG markers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13539v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saghar Vosough (Division of Neuropsychology, Department of Psychology, University of Zurich, Zurich, Switzerland), Gian Candrian (Brain,Trauma Foundation Grisons, Chur, Switzerland), Johannes Kasper (Praxisgemeinschaft f\"ur Psychiatrie und Psychotherapie, Lucerne, Switzerland), Hossam Abdel Rehim (Psychiatrie und Psychotherapie Rapperswil, Rapperswil, Switzerland), Dominique Eich (Department of Psychiatry, Psychotherapy,,Psychosomatics, University of Zurich, Zurich, Switzerland), Andreas Mueller (Brain,Trauma Foundation Grisons, Chur, Switzerland), Lutz J\"ancke (Division of Neuropsychology, Department of Psychology, University of Zurich, Zurich, Switzerland)</dc:creator>
    </item>
    <item>
      <title>BaRISTA: Brain Scale Informed Spatiotemporal Representation of Human Intracranial Neural Activity</title>
      <link>https://arxiv.org/abs/2512.12135</link>
      <description>arXiv:2512.12135v1 Announce Type: cross 
Abstract: Intracranial recordings have opened a unique opportunity to simultaneously measure activity across multiregional networks in the human brain. Recent works have focused on developing transformer-based neurofoundation models of such recordings that can generalize across subjects and datasets. However, these recordings exhibit highly complex spatiotemporal interactions across diverse spatial scales, from the single-channel scale to the scale of brain regions. As such, there remain critical open questions regarding how best to encode spatial information and how to design self-supervision tasks that enable the learning of brain network patterns and enhance downstream decoding performance using such high-dimensional, multiregional recordings. To allow for exploring these questions, we propose a new spatiotemporal transformer model of multiregional neural activity and a corresponding self-supervised masked latent reconstruction task, designed to enable flexibility in the spatial scale used for token encoding and masking. Applying this model on publicly available multiregional intracranial electrophysiology (iEEG) data, we demonstrate that adjusting the spatial scale for both token encoding and masked reconstruction significantly impacts downstream decoding. Further, we find that spatial encoding at larger scales than channel-level encoding, which is commonly used in existing iEEG transformer models, improves downstream decoding performance. Finally, we demonstrate that our method allows for region-level token encoding while also maintaining accurate channel-level neural reconstruction. Taken together, our modeling framework enables exploration of the spatial scales used for token encoding and masking, reveals their importance towards self-supervised pretraining of neurofoundation models of multiregional human brain activity, and enhances downstream decoding performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12135v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>NeurIPS 2025</arxiv:journal_reference>
      <dc:creator>Lucine L. Oganesian, Saba Hashemi, Maryam M. Shanechi</dc:creator>
    </item>
    <item>
      <title>Cross-Modal Representational Knowledge Distillation for Enhanced Spike-Informed LFP Modeling</title>
      <link>https://arxiv.org/abs/2512.12461</link>
      <description>arXiv:2512.12461v1 Announce Type: cross 
Abstract: Local field potentials (LFPs) can be routinely recorded alongside spiking activity in intracortical neural experiments, measure a larger complementary spatiotemporal scale of brain activity for scientific inquiry, and can offer practical advantages over spikes, including greater long-term stability, robustness to electrode degradation, and lower power requirements. Despite these advantages, recent neural modeling frameworks have largely focused on spiking activity since LFP signals pose inherent modeling challenges due to their aggregate, population-level nature, often leading to lower predictive power for downstream task variables such as motor behavior. To address this challenge, we introduce a cross-modal knowledge distillation framework that transfers high-fidelity representational knowledge from pretrained multi-session spike transformer models to LFP transformer models. Specifically, we first train a teacher spike model across multiple recording sessions using a masked autoencoding objective with a session-specific neural tokenization strategy. We then align the latent representations of the student LFP model to those of the teacher spike model. Our results show that the Distilled LFP models consistently outperform single- and multi-session LFP baselines in both fully unsupervised and supervised settings, and can generalize to other sessions without additional distillation while maintaining superior performance. These findings demonstrate that cross-modal knowledge distillation is a powerful and scalable approach for leveraging high-performing spike models to develop more accurate LFP models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12461v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>NeurIPS 2025</arxiv:journal_reference>
      <dc:creator>Eray Erturk, Saba Hashemi, Maryam M. Shanechi</dc:creator>
    </item>
    <item>
      <title>Dynamical modeling of nonlinear latent factors in multiscale neural activity with real-time inference</title>
      <link>https://arxiv.org/abs/2512.12462</link>
      <description>arXiv:2512.12462v1 Announce Type: cross 
Abstract: Real-time decoding of target variables from multiple simultaneously recorded neural time-series modalities, such as discrete spiking activity and continuous field potentials, is important across various neuroscience applications. However, a major challenge for doing so is that different neural modalities can have different timescales (i.e., sampling rates) and different probabilistic distributions, or can even be missing at some time-steps. Existing nonlinear models of multimodal neural activity do not address different timescales or missing samples across modalities. Further, some of these models do not allow for real-time decoding. Here, we develop a learning framework that can enable real-time recursive decoding while nonlinearly aggregating information across multiple modalities with different timescales and distributions and with missing samples. This framework consists of 1) a multiscale encoder that nonlinearly aggregates information after learning within-modality dynamics to handle different timescales and missing samples in real time, 2) a multiscale dynamical backbone that extracts multimodal temporal dynamics and enables real-time recursive decoding, and 3) modality-specific decoders to account for different probabilistic distributions across modalities. In both simulations and three distinct multiscale brain datasets, we show that our model can aggregate information across modalities with different timescales and distributions and missing samples to improve real-time target decoding. Further, our method outperforms various linear and nonlinear multimodal benchmarks in doing so.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12462v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>NeurIPS 2025</arxiv:journal_reference>
      <dc:creator>Eray Erturk, Maryam M. Shanechi</dc:creator>
    </item>
    <item>
      <title>Unsupervised learning of multiscale switching dynamical system models from multimodal neural data</title>
      <link>https://arxiv.org/abs/2512.12881</link>
      <description>arXiv:2512.12881v1 Announce Type: cross 
Abstract: Neural population activity often exhibits regime-dependent non-stationarity in the form of switching dynamics. Learning accurate switching dynamical system models can reveal how behavior is encoded in neural activity. Existing switching approaches have primarily focused on learning models from a single neural modality, either continuous Gaussian signals or discrete Poisson signals. However, multiple neural modalities are often recorded simultaneously to measure different spatiotemporal scales of brain activity, and all these modalities can encode behavior. Moreover, regime labels are typically unavailable in training data, posing a significant challenge for learning models of regime-dependent switching dynamics. To address these challenges, we develop a novel unsupervised learning algorithm that learns the parameters of switching multiscale dynamical system models using only multiscale neural observations. We demonstrate our method using both simulations and two distinct experimental datasets with multimodal spike-LFP observations during different motor tasks. We find that our switching multiscale dynamical system models more accurately decode behavior than switching single-scale dynamical models, showing the success of multiscale neural fusion. Further, our models outperform stationary multiscale models, illustrating the importance of tracking regime-dependent non-stationarity in multimodal neural data. The developed unsupervised learning framework enables more accurate modeling of complex multiscale neural dynamics by leveraging information in multimodal recordings while incorporating regime switches. This approach holds promise for improving the performance and robustness of brain-computer interfaces over time and for advancing our understanding of the neural basis of behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12881v1</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>DongKyu Kim, Han-Lin Hsieh, Maryam M. Shanechi</dc:creator>
    </item>
    <item>
      <title>Large language models are not about language</title>
      <link>https://arxiv.org/abs/2512.13441</link>
      <description>arXiv:2512.13441v1 Announce Type: cross 
Abstract: Large Language Models are useless for linguistics, as they are probabilistic models that require a vast amount of data to analyse externalized strings of words. In contrast, human language is underpinned by a mind-internal computational system that recursively generates hierarchical thought structures. The language system grows with minimal external input and can readily distinguish between real language and impossible languages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13441v1</guid>
      <category>cs.CL</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johan J. Bolhuis, Andrea Moro, Stephen Crain, Sandiway Fong</dc:creator>
    </item>
    <item>
      <title>Learning in Focus: Detecting Behavioral and Collaborative Engagement Using Vision Transformers</title>
      <link>https://arxiv.org/abs/2508.15782</link>
      <description>arXiv:2508.15782v2 Announce Type: replace 
Abstract: In early childhood education, accurately detecting collaborative and behavioral engagement is essential to foster meaningful learning experiences. This paper presents an AI driven approach that leverages Vision Transformers (ViTs) to automatically classify children s engagement using visual cues such as gaze direction, interaction, and peer collaboration. Utilizing the ChildPlay gaze dataset, our method is trained on annotated video segments to classify behavioral and collaborative engagement states (e.g., engaged, not engaged, collaborative, not collaborative). We evaluated six state of the art transformer models: Vision Transformer (ViT), Data efficient Image Transformer (DeiT), Swin Transformer, VitGaze, APVit and GazeTR. Among these, the Swin Transformer achieved the highest classification performance with an accuracy of 97.58 percent, demonstrating its effectiveness in modeling local and global attention. Our results highlight the potential of transformer based architectures for scalable, automated engagement analysis in real world educational settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15782v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sindhuja Penchala, Saketh Reddy Kontham, Prachi Bhattacharjee, Nima Mahmoodi, Daniel Fonseca, Sareh Karami, Mehdi Ghahremani, Andy D. Perkins, Shahram Rahimi, Noorbakhsh Amiri Golilarz</dc:creator>
    </item>
    <item>
      <title>Human-computer interactions predict mental health</title>
      <link>https://arxiv.org/abs/2511.20179</link>
      <description>arXiv:2511.20179v2 Announce Type: replace 
Abstract: Scalable assessments of mental illness, the leading driver of disability worldwide, remain a critical roadblock toward accessible and equitable care. Here, we show that human-computer interactions encode mental health with state-of-the-art biomarker precision. We introduce MAILA, a MAchine-learning framework for Inferring Latent mental states from digital Activity. We trained MAILA to predict 1.3 million mental-health self-reports from 20,000 cursor and touchscreen recordings recorded in 9,000 online participants. The dataset includes 2,000 individuals assessed longitudinally, 1,500 diagnosed with depression, and 500 with obsessive-compulsive disorder. MAILA tracks dynamic mental states along three orthogonal dimensions, identifies individuals living with mental illness, and achieves near-ceiling accuracy when predicting group-level mental health. By extracting non-verbal signatures of psychological function that have so far remained untapped, MAILA represents a key step toward foundation models for mental health. The ability to decode mental states at zero marginal cost creates new opportunities in neuroscience, medicine, and public health, while raising urgent questions about privacy, agency, and autonomy online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20179v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Veith Weilnhammer, Jefferson Ortega, David Whitney</dc:creator>
    </item>
    <item>
      <title>Foveated Retinotopy Improves Classification and Localization in Convolutional Neural Networks</title>
      <link>https://arxiv.org/abs/2402.15480</link>
      <description>arXiv:2402.15480v5 Announce Type: replace-cross 
Abstract: From falcons spotting preys to humans recognizing faces, rapid visual abilities depend on a foveated retinal organization which delivers high-acuity central vision while preserving low-resolution periphery. This organization is conserved along early visual pathways but remains underexplored in machine learning. Here we examine how embedding a foveated retinotopic transformation as a preprocessing layer impacts convolutional neural networks (CNNs) for image classification. By applying a log-polar mapping to off-the-shelf models and retraining them, we retain comparable accuracy while improving robustness to scale and rotation. We show that this architecture becomes highly sensitive to fixation-point shifts, and that this sensitivity yields a proxy for defining saliency maps that effectively facilitates object localization. Our results show that foveated retinotopy encodes prior geometric knowledge, offering a solution to visual-search and enhancing both classification and localization. These findings connect biological vision principles with artificial networks, pointing to new, robust and efficient directions for computer-vision systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15480v5</guid>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jean-Nicolas J\'er\'emie, Emmanuel Dauc\'e, Laurent U Perrinet</dc:creator>
    </item>
    <item>
      <title>Stabilizing Fractional Dynamical Networks Suppresses Epileptic Seizures</title>
      <link>https://arxiv.org/abs/2511.20950</link>
      <description>arXiv:2511.20950v2 Announce Type: replace-cross 
Abstract: Medically uncontrolled epileptic seizures affect nearly 15 million people worldwide, resulting in enormous economic and psychological burdens. Treatment of medically refractory epilepsy is essential for patients to achieve remission, improve psychological functioning, and enhance social and vocational outcomes. Here, we show a state-of-the-art method that stabilizes fractional dynamical networks modeled from intracranial EEG data, effectively suppressing seizure activity in 34 out of 35 total spontaneous episodes from patients at the University of Pennsylvania and the Mayo Clinic. We perform a multi-scale analysis and show that the fractal behavior and stability properties of these data distinguish between four epileptic states: interictal, pre-ictal, ictal, and post-ictal. Furthermore, the simulated controlled signals exhibit substantial amplitude reduction ($49\%$ average). These findings highlight the potential of fractional dynamics to characterize seizure-related brain states and demonstrate its capability to suppress epileptic activity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20950v2</guid>
      <category>q-bio.QM</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yaoyue Wang, Arian Ashourvan, Guilherme Ramos, Paul Bogdan, Emily Pereira</dc:creator>
    </item>
  </channel>
</rss>
