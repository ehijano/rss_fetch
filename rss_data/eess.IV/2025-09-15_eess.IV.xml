<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>eess.IV updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/eess.IV</link>
    <description>eess.IV updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/eess.IV" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 15 Sep 2025 04:00:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Automated Tuning for Diffusion Inverse Problem Solvers without Generative Prior Retraining</title>
      <link>https://arxiv.org/abs/2509.09880</link>
      <description>arXiv:2509.09880v1 Announce Type: new 
Abstract: Diffusion/score-based models have recently emerged as powerful generative priors for solving inverse problems, including accelerated MRI reconstruction. While their flexibility allows decoupling the measurement model from the learned prior, their performance heavily depends on carefully tuned data fidelity weights, especially under fast sampling schedules with few denoising steps. Existing approaches often rely on heuristics or fixed weights, which fail to generalize across varying measurement conditions and irregular timestep schedules. In this work, we propose Zero-shot Adaptive Diffusion Sampling (ZADS), a test-time optimization method that adaptively tunes fidelity weights across arbitrary noise schedules without requiring retraining of the diffusion prior. ZADS treats the denoising process as a fixed unrolled sampler and optimizes fidelity weights in a self-supervised manner using only undersampled measurements. Experiments on the fastMRI knee dataset demonstrate that ZADS consistently outperforms both traditional compressed sensing and recent diffusion-based methods, showcasing its ability to deliver high-fidelity reconstructions across varying noise schedules and acquisition settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09880v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>physics.med-ph</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ya\c{s}ar Utku Al\c{c}alar, Junno Yun, Mehmet Ak\c{c}akaya</dc:creator>
    </item>
    <item>
      <title>Accelerating 3D Photoacoustic Computed Tomography with End-to-End Physics-Aware Neural Operators</title>
      <link>https://arxiv.org/abs/2509.09894</link>
      <description>arXiv:2509.09894v1 Announce Type: new 
Abstract: Photoacoustic computed tomography (PACT) combines optical contrast with ultrasonic resolution, achieving deep-tissue imaging beyond the optical diffusion limit. While three-dimensional PACT systems enable high-resolution volumetric imaging for applications spanning transcranial to breast imaging, current implementations require dense transducer arrays and prolonged acquisition times, limiting clinical translation. We introduce Pano (PACT imaging neural operator), an end-to-end physics-aware model that directly learns the inverse acoustic mapping from sensor measurements to volumetric reconstructions. Unlike existing approaches (e.g. universal back-projection algorithm), Pano learns both physics and data priors while also being agnostic to the input data resolution. Pano employs spherical discrete-continuous convolutions to preserve hemispherical sensor geometry, incorporates Helmholtz equation constraints to ensure physical consistency and operates resolutionindependently across varying sensor configurations. We demonstrate the robustness and efficiency of Pano in reconstructing high-quality images from both simulated and real experimental data, achieving consistent performance even with significantly reduced transducer counts and limited-angle acquisition configurations. The framework maintains reconstruction fidelity across diverse sparse sampling patterns while enabling real-time volumetric imaging capabilities. This advancement establishes a practical pathway for making 3D PACT more accessible and feasible for both preclinical research and clinical applications, substantially reducing hardware requirements without compromising image reconstruction quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09894v1</guid>
      <category>eess.IV</category>
      <category>cs.LG</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiayun Wang, Yousuf Aborahama, Arya Khokhar, Yang Zhang, Chuwei Wang, Karteekeya Sastry, Julius Berner, Yilin Luo, Boris Bonev, Zongyi Li, Kamyar Azizzadenesheli, Lihong V. Wang, Anima Anandkumar</dc:creator>
    </item>
    <item>
      <title>Drone-Based Multispectral Imaging and Deep Learning for Timely Detection of Branched Broomrape in Tomato Farms</title>
      <link>https://arxiv.org/abs/2509.09972</link>
      <description>arXiv:2509.09972v1 Announce Type: new 
Abstract: This study addresses the escalating threat of branched broomrape (Phelipanche ramosa) to California's tomato industry, which supplies over 90 percent of U.S. processing tomatoes. The parasite's largely underground life cycle makes early detection difficult, while conventional chemical controls are costly, environmentally harmful, and often ineffective. To address this, we combined drone-based multispectral imagery with Long Short-Term Memory (LSTM) deep learning networks, using the Synthetic Minority Over-sampling Technique (SMOTE) to handle class imbalance. Research was conducted on a known broomrape-infested tomato farm in Woodland, Yolo County, CA, across five key growth stages determined by growing degree days (GDD). Multispectral images were processed to isolate tomato canopy reflectance. At 897 GDD, broomrape could be detected with 79.09 percent overall accuracy and 70.36 percent recall without integrating later stages. Incorporating sequential growth stages with LSTM improved detection substantially. The best-performing scenario, which integrated all growth stages with SMOTE augmentation, achieved 88.37 percent overall accuracy and 95.37 percent recall. These results demonstrate the strong potential of temporal multispectral analysis and LSTM networks for early broomrape detection. While further real-world data collection is needed for practical deployment, this study shows that UAV-based multispectral sensing coupled with deep learning could provide a powerful precision agriculture tool to reduce losses and improve sustainability in tomato production.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09972v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1117/12.3021219.</arxiv:DOI>
      <arxiv:journal_reference>Proc. SPIE 13053, Autonomous Air and Ground Sensing Systems for Agricultural Optimization and Phenotyping IX, 1305304 (7 June 2024)</arxiv:journal_reference>
      <dc:creator>Mohammadreza Narimani, Alireza Pourreza, Ali Moghimi, Mohsen Mesgaran, Parastoo Farajpoor, Hamid Jafarbiglu</dc:creator>
    </item>
    <item>
      <title>Polarization Denoising and Demosaicking: Dataset and Baseline Method</title>
      <link>https://arxiv.org/abs/2509.10098</link>
      <description>arXiv:2509.10098v1 Announce Type: new 
Abstract: A division-of-focal-plane (DoFP) polarimeter enables us to acquire images with multiple polarization orientations in one shot and thus it is valuable for many applications using polarimetric information. The image processing pipeline for a DoFP polarimeter entails two crucial tasks: denoising and demosaicking. While polarization demosaicking for a noise-free case has increasingly been studied, the research for the joint task of polarization denoising and demosaicking is scarce due to the lack of a suitable evaluation dataset and a solid baseline method. In this paper, we propose a novel dataset and method for polarization denoising and demosaicking. Our dataset contains 40 real-world scenes and three noise-level conditions, consisting of pairs of noisy mosaic inputs and noise-free full images. Our method takes a denoising-then-demosaicking approach based on well-accepted signal processing components to offer a reproducible method. Experimental results demonstrate that our method exhibits higher image reconstruction performance than other alternative methods, offering a solid baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10098v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Muhamad Daniel Ariff Bin Abdul Rahman, Yusuke Monno, Masayuki Tanaka, Masatoshi Okutomi</dc:creator>
    </item>
    <item>
      <title>Soft Tissue Simulation and Force Estimation from Heterogeneous Structures using Equivariant Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2509.10125</link>
      <description>arXiv:2509.10125v1 Announce Type: new 
Abstract: Accurately simulating soft tissue deformation is crucial for surgical training, pre-operative planning, and real-time haptic feedback systems. While physics-based models such as the finite element method (FEM) provide high-fidelity results, they are often computationally expensive and require extensive preprocessing. We propose a graph neural network (GNN) architecture that predicts both tissue surface deformation and applied force from sparse point clouds. The model incorporates internal anatomical information through binary tissue profiles beneath each point and leverages E(n)-equivariant message passing to improve robustness. We collected experimental data that comprises a real silicone and bone-like phantom, and complemented it with synthetic simulations generated using FEM. Our model achieves a comparable performance to a baseline GNN on standard test cases and significantly outperforms it in rotated and cross-resolution scenarios, showing a strong generalization to unseen orientations and point densities. It also achieves a significant speed improvement, offering a solution for real-time applications. When fine-tuned on experimental data, the model maintains sub-millimeter deformation accuracy despite limited sample size and measurement noise. The results demonstrate that our approach offers an efficient, data-driven alternative to traditional simulations, capable of generalizing across anatomical configurations and supporting interactive surgical environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10125v1</guid>
      <category>eess.IV</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Madina Kojanazarova, Sidady El Hadramy, Jack Wilkie, Georg Rauter, Philippe C. Cattin</dc:creator>
    </item>
    <item>
      <title>Multi-pathology Chest X-ray Classification with Rejection Mechanisms</title>
      <link>https://arxiv.org/abs/2509.10348</link>
      <description>arXiv:2509.10348v1 Announce Type: new 
Abstract: Overconfidence in deep learning models poses a significant risk in high-stakes medical imaging tasks, particularly in multi-label classification of chest X-rays, where multiple co-occurring pathologies must be detected simultaneously. This study introduces an uncertainty-aware framework for chest X-ray diagnosis based on a DenseNet-121 backbone, enhanced with two selective prediction mechanisms: entropy-based rejection and confidence interval-based rejection. Both methods enable the model to abstain from uncertain predictions, improving reliability by deferring ambiguous cases to clinical experts. A quantile-based calibration procedure is employed to tune rejection thresholds using either global or class-specific strategies. Experiments conducted on three large public datasets (PadChest, NIH ChestX-ray14, and MIMIC-CXR) demonstrate that selective rejection improves the trade-off between diagnostic accuracy and coverage, with entropy-based rejection yielding the highest average AUC across all pathologies. These results support the integration of selective prediction into AI-assisted diagnostic workflows, providing a practical step toward safer, uncertainty-aware deployment of deep learning in clinical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10348v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yehudit Aperstein, Amit Tzahar, Alon Gottlib, Tal Verber, Ravit Shagan Damti, Alexander Apartsin</dc:creator>
    </item>
    <item>
      <title>Human Body Segment Volume Estimation with Two RGB-D Cameras</title>
      <link>https://arxiv.org/abs/2509.10429</link>
      <description>arXiv:2509.10429v1 Announce Type: new 
Abstract: In the field of human biometry, accurately estimating the volume of the whole body and its individual segments is of fundamental importance. Such measurements support a wide range of applications that include assessing health, optimizing ergonomic design, and customizing biomechanical models. In this work, we presented a Body Segment Volume Estimation (BSV) system to automatically compute whole-body and segment volumes using only two RGB-D cameras, thus limiting the system complexity. However, to maintain the accuracy comparable to 3D laser scanners, we enhanced the As-Rigid-As-Possible (ARAP) non-rigid registration techniques, disconnecting its energy from the single triangle mesh. Thus, we improved the geometrical coherence of the reconstructed mesh, especially in the lateral gap areas. We evaluated BSV starting from the RGB-D camera performances, through the results obtained with FAUST dataset human body models, and comparing with a state-of-the-art work, up to real acquisitions. It showed superior ability in accurately estimating human body volumes, and it allows evaluating volume ratios between proximal and distal body segments, which are useful indices in many clinical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10429v1</guid>
      <category>eess.IV</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giulia Bassani, Emilio Maoddi, Usman Asghar, Carlo Alberto Avizzano, Alessandro Filippeschi</dc:creator>
    </item>
    <item>
      <title>Glorbit: A Modular, Web-Based Platform for AI Based Periorbital Measurement in Low-Resource Settings</title>
      <link>https://arxiv.org/abs/2509.09693</link>
      <description>arXiv:2509.09693v1 Announce Type: cross 
Abstract: Periorbital measurements such as margin reflex distances (MRD1/2), palpebral fissure height, and scleral show are essential in diagnosing and managing conditions like ptosis and eyelid disorders. We developed Glorbit, a lightweight, browser-based application for automated periorbital distance measurement using artificial intelligence, designed for use in low-resource clinical settings. The app integrates a DeepLabV3 segmentation model into a modular pipeline with secure, site-specific Google Cloud storage. Glorbit supports offline mode, local preprocessing, and cloud upload via Firebase-authenticated logins. We evaluated usability, cross-platform compatibility, and deployment readiness through a simulated enrollment study of 15 volunteers. The app completed the full workflow -- metadata entry, image capture, segmentation, and upload -- on all tested sessions without error. Glorbit successfully ran on laptops, tablets, and mobile phones across major browsers. The segmentation model succeeded on all images. Average session time was 101.7 seconds (standard deviation: 17.5). Usability survey scores (1-5 scale) were uniformly high: intuitiveness and efficiency (5.0), workflow clarity (4.8), output confidence (4.9), and clinical utility (4.9). Glorbit provides a functional, scalable solution for standardized periorbital measurement in diverse environments. It supports secure data collection and may enable future development of real-time triage tools and multimodal AI-driven oculoplastics. Tool available at: https://glorbit.app</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09693v1</guid>
      <category>q-bio.TO</category>
      <category>eess.IV</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George R. Nahass, Jacob van der Ende, Sasha Hubschman, Benjamin Beltran, Bhavana Kolli, Caitlin Berek, James D. Edmonds, R. V. Paul Chan, Pete Setabutr, James W. Larrick, Darvin Yi, Ann Q. Tran</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Pipeline for Aortic Segmentation and Shape Analysis</title>
      <link>https://arxiv.org/abs/2509.09718</link>
      <description>arXiv:2509.09718v1 Announce Type: cross 
Abstract: Aortic shape analysis plays a key role in cardiovascular diagnostics, treatment planning, and understanding disease progression. We present a robust, fully automated pipeline for aortic shape analysis from cardiac MRI, combining deep learning and statistical techniques across segmentation, 3D surface reconstruction, and mesh registration. We benchmark leading segmentation models including nnUNet, TotalSegmentator, and MedSAM2 highlighting the effectiveness of domain specific training and transfer learning on a curated dataset. Following segmentation, we reconstruct high quality 3D meshes and introduce a DL based mesh registration method that directly optimises vertex displacements. This approach significantly outperforms classical rigid and nonrigid methods in geometric accuracy and anatomical consistency. Using the registered meshes, we perform statistical shape analysis on a cohort of 599 healthy subjects. Principal Component Analysis reveals dominant modes of aortic shape variation, capturing both global morphology and local structural differences under rigid and similarity transformations. Our findings demonstrate the advantages of integrating traditional geometry processing with learning based models for anatomically precise and scalable aortic analysis. This work lays the groundwork for future studies into pathological shape deviations and supports the development of personalised diagnostics in cardiovascular medicine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09718v1</guid>
      <category>q-bio.TO</category>
      <category>eess.IV</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nairouz Shehata, Amr Elsawy, Mohamed Nagy, Muhammad ElMahdy, Mariam Ali, Soha Romeih, Heba Aguib, Magdi Yacoub, Ben Glocker</dc:creator>
    </item>
    <item>
      <title>Australian Supermarket Object Set (ASOS): A Benchmark Dataset of Physical Objects and 3D Models for Robotics and Computer Vision</title>
      <link>https://arxiv.org/abs/2509.09720</link>
      <description>arXiv:2509.09720v1 Announce Type: cross 
Abstract: This paper introduces the Australian Supermarket Object Set (ASOS), a comprehensive dataset comprising 50 readily available supermarket items with high-quality 3D textured meshes designed for benchmarking in robotics and computer vision applications. Unlike existing datasets that rely on synthetic models or specialized objects with limited accessibility, ASOS provides a cost-effective collection of common household items that can be sourced from a major Australian supermarket chain. The dataset spans 10 distinct categories with diverse shapes, sizes, and weights. 3D meshes are acquired by a structure-from-motion techniques with high-resolution imaging to generate watertight meshes. The dataset's emphasis on accessibility and real-world applicability makes it valuable for benchmarking object detection, pose estimation, and robotics applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09720v1</guid>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <category>eess.IV</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akansel Cosgun, Lachlan Chumbley, Benjamin J. Meyer</dc:creator>
    </item>
    <item>
      <title>Adaptive Token Merging for Efficient Transformer Semantic Communication at the Edge</title>
      <link>https://arxiv.org/abs/2509.09955</link>
      <description>arXiv:2509.09955v1 Announce Type: cross 
Abstract: Large-scale transformers are central to modern semantic communication, yet their high computational and communication costs hinder deployment on resource-constrained edge devices. This paper introduces a training-free framework for adaptive token merging, a novel mechanism that compresses transformer representations at runtime by selectively merging semantically redundant tokens under per-layer similarity thresholds. Unlike prior fixed-ratio reduction, our approach couples merging directly to input redundancy, enabling data-dependent adaptation that balances efficiency and task relevance without retraining. We cast the discovery of merging strategies as a multi-objective optimization problem and leverage Bayesian optimization to obtain Pareto-optimal trade-offs between accuracy, inference cost, and communication cost. On ImageNet classification, we match the accuracy of the unmodified transformer with 30\% fewer floating-point operations per second and under 20\% of the original communication cost, while for visual question answering our method achieves performance competitive with the full LLaVA model at less than one-third of the compute and one-tenth of the bandwidth. Finally, we show that our adaptive merging is robust across varying channel conditions and provides inherent privacy benefits, substantially degrading the efficacy of model inversion attacks. Our framework provides a practical and versatile solution for deploying powerful transformer models in resource-limited edge intelligence scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09955v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Omar Erak, Omar Alhussein, Hatem Abou-Zeid, Mehdi Bennis, Sami Muhaidat</dc:creator>
    </item>
    <item>
      <title>Efficient and Accurate Downfacing Visual Inertial Odometry</title>
      <link>https://arxiv.org/abs/2509.10021</link>
      <description>arXiv:2509.10021v1 Announce Type: cross 
Abstract: Visual Inertial Odometry (VIO) is a widely used computer vision method that determines an agent's movement through a camera and an IMU sensor. This paper presents an efficient and accurate VIO pipeline optimized for applications on micro- and nano-UAVs. The proposed design incorporates state-of-the-art feature detection and tracking methods (SuperPoint, PX4FLOW, ORB), all optimized and quantized for emerging RISC-V-based ultra-low-power parallel systems on chips (SoCs). Furthermore, by employing a rigid body motion model, the pipeline reduces estimation errors and achieves improved accuracy in planar motion scenarios. The pipeline's suitability for real-time VIO is assessed on an ultra-low-power SoC in terms of compute requirements and tracking accuracy after quantization. The pipeline, including the three feature tracking methods, was implemented on the SoC for real-world validation. This design bridges the gap between high-accuracy VIO pipelines that are traditionally run on computationally powerful systems and lightweight implementations suitable for microcontrollers. The optimized pipeline on the GAP9 low-power SoC demonstrates an average reduction in RMSE of up to a factor of 3.65x over the baseline pipeline when using the ORB feature tracker. The analysis of the computational complexity of the feature trackers further shows that PX4FLOW achieves on-par tracking accuracy with ORB at a lower runtime for movement speeds below 24 pixels/frame.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10021v1</guid>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <category>eess.IV</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/JIOT.2025.3609011</arxiv:DOI>
      <dc:creator>Jonas K\"uhne, Christian Vogt, Michele Magno, Luca Benini</dc:creator>
    </item>
    <item>
      <title>PL-Net: Progressive Learning Network for Medical Image Segmentation</title>
      <link>https://arxiv.org/abs/2110.14484</link>
      <description>arXiv:2110.14484v3 Announce Type: replace 
Abstract: In recent years, deep convolutional neural network-based segmentation methods have achieved state-of-the-art performance for many medical analysis tasks. However, most of these approaches rely on optimizing the U-Net structure or adding new functional modules, which overlooks the complementation and fusion of coarse-grained and fine-grained semantic information. To address these issues, we propose a 2D medical image segmentation framework called Progressive Learning Network (PL-Net), which comprises Internal Progressive Learning (IPL) and External Progressive Learning (EPL). PL-Net offers the following advantages: (1) IPL divides feature extraction into two steps, allowing for the mixing of different size receptive fields and capturing semantic information from coarse to fine granularity without introducing additional parameters; (2) EPL divides the training process into two stages to optimize parameters and facilitate the fusion of coarse-grained information in the first stage and fine-grained information in the second stage. We conducted comprehensive evaluations of our proposed method on five medical image segmentation datasets, and the experimental results demonstrate that PL-Net achieves competitive segmentation performance. It is worth noting that PL-Net does not introduce any additional learnable parameters compared to other U-Net variants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.14484v3</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kunpeng Mao, Ruoyu Li, Junlong Cheng, Danmei Huang, Zhiping Song, ZeKui Liu</dc:creator>
    </item>
    <item>
      <title>Integrative Variational Autoencoders for Generative Modeling of an Image Outcome with Multiple Input Images</title>
      <link>https://arxiv.org/abs/2402.02734</link>
      <description>arXiv:2402.02734v2 Announce Type: replace 
Abstract: Understanding relationships across multiple imaging modalities is central to neuroimaging research. We introduce the Integrative Variational Autoencoder (InVA), the first hierarchical VAE framework for image-on-image regression in multimodal neuroimaging. Unlike standard VAEs, which are not designed for predictive integration across modalities, InVA models outcome images as functions of both shared and modality-specific features. This flexible, data-driven approach avoids rigid assumptions of classical tensor regression and outperforms conventional VAEs and nonlinear models such as BART. As a key application, InVA accurately predicts costly PET scans from structural MRI, offering an efficient and powerful tool for multimodal neuroimaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02734v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.NE</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bowen Lei, Yeseul Jeon, Rajarshi Guhaniyogi, Aaron Scheffler, Bani Mallick, Alzheimer's Disease Neuroimaging Initiatives</dc:creator>
    </item>
    <item>
      <title>Generalized Ray Tracing with Basis functions for Tomographic Projections</title>
      <link>https://arxiv.org/abs/2503.20907</link>
      <description>arXiv:2503.20907v2 Announce Type: replace 
Abstract: This work aims at the precise and efficient computation of the x-ray projection of an image represented by a linear combination of general shifted basis functions that typically overlap. We achieve this with a suitable adaptation of ray tracing, which is one of the most efficient methods to compute line integrals. In our work, the cases in which the image is expressed as a spline are of particular relevance. The proposed implementation is applicable to any projection geometry as it computes the forward and backward operators over a collection of arbitrary lines. We validate our work with experiments in the context of inverse problems for image reconstruction and maximize the image quality for a given resolution of the reconstruction grid.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20907v2</guid>
      <category>eess.IV</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youssef Haouchat, Sepand Kashani, Philippe Th\'evenaz, Michael Unser</dc:creator>
    </item>
    <item>
      <title>Quanta Diffusion</title>
      <link>https://arxiv.org/abs/2506.06945</link>
      <description>arXiv:2506.06945v2 Announce Type: replace 
Abstract: We present Quanta Diffusion (QuDi), a powerful generative video reconstruction method for single-photon imaging. QuDi is an algorithm supporting the latest Quanta Image Sensors (QIS) and Single Photon Avalanche Diodes (SPADs) for extremely low-light imaging conditions. Compared to existing methods, QuDi overcomes the difficulties of simultaneously managing the motion and the strong shot noise. The core innovation of QuDi is to inject a physics-based forward model into the diffusion algorithm, while keeping the motion estimation in the loop. QuDi demonstrates an average of 2.4 dB PSNR improvement over the best existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06945v2</guid>
      <category>eess.IV</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ICIP55913.2025.11084414</arxiv:DOI>
      <arxiv:journal_reference>IEEE International Conference on Image Processing (IEEE ICIP) 2025</arxiv:journal_reference>
      <dc:creator>Prateek Chennuri, Dongdong Fu, Stanley H. Chan</dc:creator>
    </item>
    <item>
      <title>Hadamard Encoded Row Column Ultrasonic Expansive Scanning (HERCULES) with Bias-Switchable Row-Column Arrays</title>
      <link>https://arxiv.org/abs/2506.11443</link>
      <description>arXiv:2506.11443v2 Announce Type: replace 
Abstract: Top-Orthogonal-to-Bottom-Electrode (TOBE) arrays, also known as bias-switchable row-column arrays (RCAs), allow for imaging techniques otherwise impossible for non-bias-switachable RCAs. Hadamard Encoded Row Column Ultrasonic Expansive Scanning (HERCULES) is a novel imaging technique that allows for expansive 3D scanning by transmitting plane or cylindrical wavefronts and receiving using Hadamard-Encoded-Read-Out (HERO) to perform beamforming on what is effectively a full 2D synthetic receive aperture. This allows imaging beyond the shadow of the aperture of the RCA array, potentially allows for whole organ imaging and 3D visualization of tissue morphology. It additionally enables view large volumes through limited windows. In this work we demonstrated with simulation that we are able to image at comparable resolution to existing RCA imaging methods at tens to hundreds of volumes per second. We validated these simulations by demonstrating an experimental implementation of HERCULES using a custom fabricated TOBE array, custom biasing electronics, and a research ultrasound system. Furthermore, we assess our imaging capabilities by imaging a commercial phantom, and comparing our results to those taken with traditional RCA imaging methods. Finally, we verified our ability to image real tissue by imaging a xenograft mouse model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11443v2</guid>
      <category>eess.IV</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Darren Olufemi Dahunsi, Randy Palmar, Tyler Henry, Mohammad Rahim Sobhani, Negar Majidi, Joy Wang, Afshin Kashani Ilkhechi, Jeremy Brown, Roger Zemp</dc:creator>
    </item>
    <item>
      <title>Uncovering Neuroimaging Biomarkers of Brain Tumor Surgery with AI-Driven Methods</title>
      <link>https://arxiv.org/abs/2507.04881</link>
      <description>arXiv:2507.04881v2 Announce Type: replace 
Abstract: Brain tumor resection is a highly complex procedure with profound implications for survival and quality of life. Predicting patient outcomes is crucial to guide clinicians in balancing oncological control with preservation of neurological function. However, building reliable prediction models is severely limited by the rarity of curated datasets that include both pre- and post-surgery imaging, given the clinical, logistical and ethical challenges of collecting such data. In this study, we develop a novel framework that integrates explainable artificial intelligence (XAI) with neuroimaging-based feature engineering for survival assessment in brain tumor patients. We curated structural MRI data from 49 patients scanned pre- and post-surgery, providing a rare resource for identifying survival-related biomarkers. A key methodological contribution is the development of a global explanation optimizer, which refines survival-related feature attribution in deep learning models, thereby improving both the interpretability and reliability of predictions. From a clinical perspective, our findings provide important evidence that survival after oncological surgery is influenced by alterations in regions related to cognitive and sensory functions. These results highlight the importance of preserving areas involved in decision-making and emotional regulation to improve long-term outcomes. From a technical perspective, the proposed optimizer advances beyond state-of-the-art XAI methods by enhancing both the fidelity and comprehensibility of model explanations, thus reinforcing trust in the recognition patterns driving survival prediction. This work demonstrates the utility of XAI-driven neuroimaging analysis in identifying survival-related variability and underscores its potential to inform precision medicine strategies in brain tumor treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04881v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Carmen Jimenez-Mesa, Yizhou Wan, Guilio Sansone, Francisco J. Martinez-Murcia, Javier Ramirez, Pietro Lio, Juan M. Gorriz, Stephen J. Price, John Suckling, Michail Mamalakis</dc:creator>
    </item>
    <item>
      <title>Out-Of-Distribution Detection for Audio-visual Generalized Zero-Shot Learning: A General Framework</title>
      <link>https://arxiv.org/abs/2408.01284</link>
      <description>arXiv:2408.01284v2 Announce Type: replace-cross 
Abstract: Generalized Zero-Shot Learning (GZSL) is a challenging task requiring accurate classification of both seen and unseen classes. Within this domain, Audio-visual GZSL emerges as an extremely exciting yet difficult task, given the inclusion of both visual and acoustic features as multi-modal inputs. Existing efforts in this field mostly utilize either embedding-based or generative-based methods. However, generative training is difficult and unstable, while embedding-based methods often encounter domain shift problem. Thus, we find it promising to integrate both methods into a unified framework to leverage their advantages while mitigating their respective disadvantages. Our study introduces a general framework employing out-of-distribution (OOD) detection, aiming to harness the strengths of both approaches. We first employ generative adversarial networks to synthesize unseen features, enabling the training of an OOD detector alongside classifiers for seen and unseen classes. This detector determines whether a test feature belongs to seen or unseen classes, followed by classification utilizing separate classifiers for each feature type. We test our framework on three popular audio-visual datasets and observe a significant improvement comparing to existing state-of-the-art works. Codes can be found in https://github.com/liuyuan-wen/AV-OOD-GZSL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01284v2</guid>
      <category>cs.MM</category>
      <category>cs.CV</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <category>eess.IV</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liuyuan Wen</dc:creator>
    </item>
  </channel>
</rss>
