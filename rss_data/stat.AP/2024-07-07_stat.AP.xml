<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 08 Jul 2024 04:01:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 08 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Investigating symptom duration using current status data: a case study of post-acute COVID-19 syndrome</title>
      <link>https://arxiv.org/abs/2407.04214</link>
      <description>arXiv:2407.04214v1 Announce Type: new 
Abstract: For infectious diseases, characterizing symptom duration is of clinical and public health importance. Symptom duration may be assessed by surveying infected individuals and querying symptom status at the time of survey response. For example, in a SARS-CoV-2 testing program at the University of Washington, participants were surveyed at least 28 days after testing positive and asked to report current symptom status. This study design yielded current status data: Outcome measurements for each respondent consisted only of the time of survey response and a binary indicator of whether symptoms had resolved by that time. Such study design benefits from limited risk of recall bias, but analyzing the resulting data necessitates specialized statistical tools. Here, we review methods for current status data and describe a novel application of modern nonparametric techniques to this setting. The proposed approach is valid under weaker assumptions compared to existing methods, allows use of flexible machine learning tools, and handles potential survey nonresponse. From the university study, we estimate that 19% of participants experienced ongoing symptoms 30 days after testing positive, decreasing to 7% at 90 days. Female sex, history of seasonal allergies, fatigue during acute infection, and higher viral load were associated with slower symptom resolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04214v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charles J. Wolock, Susan Jacob, Julia C. Bennett, Anna Elias-Warren, Jessica O'Hanlon, Avi Kenny, Nicholas P. Jewell, Andrea Rotnitzky, Ana A. Weil, Helen Y. Chu, Marco Carone</dc:creator>
    </item>
    <item>
      <title>A Statistical Reduced Complexity Climate Model for Probabilistic Analyses and Projections</title>
      <link>https://arxiv.org/abs/2407.04351</link>
      <description>arXiv:2407.04351v1 Announce Type: new 
Abstract: We propose a new statistical reduced complexity climate model. The centerpiece of the model consists of a set of physical equations for the global climate system which we show how to cast in non-linear state space form. The parameters in the model are estimated using the method of maximum likelihood with the likelihood function being evaluated by the extended Kalman filter. Our statistical framework is based on well-established methodology and is computationally feasible. In an empirical analysis, we estimate the parameters for a data set comprising the period 1959-2022. A likelihood ratio test sheds light on the most appropriate equation for converting the level of atmospheric concentration of carbon dioxide into radiative forcing. Using the estimated model, and different future paths of greenhouse gas emissions, we project global mean surface temperature until the year 2100. Our results illustrate the potential of combining statistical modelling with physical insights to arrive at rigorous statistical analyses of the climate system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04351v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mikkel Bennedsen, Eric Hillebrand, Siem Jan Koopman</dc:creator>
    </item>
    <item>
      <title>An open-source framework for data-driven trajectory extraction from AIS data -- the $\alpha$-method</title>
      <link>https://arxiv.org/abs/2407.04402</link>
      <description>arXiv:2407.04402v1 Announce Type: new 
Abstract: Ship trajectories from Automatic Identification System (AIS) messages are important in maritime safety, domain awareness, and algorithmic testing. Although the specifications for transmitting and receiving AIS messages are fixed, it is well known that technical inaccuracies and lacking seafarer compliance lead to severe data quality impairment. This paper proposes an adaptable, data-driven, $\alpha$-quantile-based framework for decoding, constructing, splitting, and assessing trajectories from raw AIS records to improve transparency in AIS data mining. Results indicate the proposed filtering algorithm robustly extracts clean, long, and uninterrupted trajectories for further processing. An open-source Python implementation of the framework is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04402v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Niklas Paulig, Ostap Okhrin</dc:creator>
    </item>
    <item>
      <title>Learning Patterns from Biological Networks: A Compounded Burr Probability Model</title>
      <link>https://arxiv.org/abs/2407.04465</link>
      <description>arXiv:2407.04465v1 Announce Type: new 
Abstract: Complex biological networks, comprising metabolic reactions, gene interactions, and protein interactions, often exhibit scale-free characteristics with power-law degree distributions. However, empirical studies have revealed discrepancies between observed biological network data and ideal power-law fits, highlighting the need for improved modeling approaches. To address this challenge, we propose a novel family of distributions, building upon the baseline Burr distribution. Specifically, we introduce the compounded Burr (CBurr) distribution, derived from a continuous probability distribution family, enabling flexible and efficient modeling of node degree distributions in biological networks. This study comprehensively investigates the general properties of the CBurr distribution, focusing on parameter estimation using the maximum likelihood method. Subsequently, we apply the CBurr distribution model to large-scale biological network data, aiming to evaluate its efficacy in fitting the entire range of node degree distributions, surpassing conventional power-law distributions and other benchmarks. Through extensive data analysis and graphical illustrations, we demonstrate that the CBurr distribution exhibits superior modeling capabilities compared to traditional power-law distributions. This novel distribution model holds great promise for accurately capturing the complex nature of biological networks and advancing our understanding of their underlying mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04465v1</guid>
      <category>stat.AP</category>
      <category>cs.SI</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Tanujit Chakraborty, Shraddha M. Naik, Swarup Chattopadhyay, Suchismita Das</dc:creator>
    </item>
    <item>
      <title>DCZNMaker: A Web-based Application for Multi-Attribute Utilities Analysis</title>
      <link>https://arxiv.org/abs/2407.04655</link>
      <description>arXiv:2407.04655v1 Announce Type: new 
Abstract: DCZNMaker is a web-based application designed to streamline decision-making processes using Multi-attribute Utility Analysis (MAUA). Built with simplicity and efficiency in mind, DCZNMaker empowers users to make informed decisions among alternatives (options) by making explicit the factors (attributes) to be taken into consideration, as well as the importance (weights) and utility (location) of each attribute. The app offers a user-friendly interface, allowing individuals to input the various attributes and their associated weights and locations effortlessly. Leveraging advanced algorithms, DCZNMaker computes and presents comprehensive analyses, aiding users in understanding the relative importance of each attribute and guiding them towards optimal decisions. Several use cases are demonstrated. Whether for personal, professional, or academic use, DCZNMaker is a versatile tool adaptable to diverse decision-making scenarios. With its intuitive design and robust functionality, DCZNMaker revolutionizes decision-making processes, empowering individuals or groups of users to make well-informed choices with confidence and clarity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04655v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrienne Kline</dc:creator>
    </item>
    <item>
      <title>Balancing events, not patients, maximizes power of the logrank test: and other insights on unequal randomization in survival trials</title>
      <link>https://arxiv.org/abs/2407.03420</link>
      <description>arXiv:2407.03420v1 Announce Type: cross 
Abstract: We revisit the question of what randomization ratio (RR) maximizes power of the logrank test in event-driven survival trials under proportional hazards (PH). By comparing three approximations of the logrank test (Schoenfeld, Freedman, Rubinstein) to empirical simulations, we find that the RR that maximizes power is the RR that balances number of events across treatment arms at the end of the trial. This contradicts the common misconception implied by Schoenfeld's approximation that 1:1 randomization maximizes power. Besides power, we consider other factors that might influence the choice of RR (accrual, trial duration, sample size, etc.). We perform simulations to better understand how unequal randomization might impact these factors in practice. Altogether, we derive 6 insights to guide statisticians in the design of survival trials considering unequal randomization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03420v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Godwin Yung, Kaspar Rufibach, Marcel Wolbers, Ray Lin, Yi Liu</dc:creator>
    </item>
    <item>
      <title>On Large Language Models in National Security Applications</title>
      <link>https://arxiv.org/abs/2407.03453</link>
      <description>arXiv:2407.03453v1 Announce Type: cross 
Abstract: The overwhelming success of GPT-4 in early 2023 highlighted the transformative potential of large language models (LLMs) across various sectors, including national security. This article explores the implications of LLM integration within national security contexts, analyzing their potential to revolutionize information processing, decision-making, and operational efficiency. Whereas LLMs offer substantial benefits, such as automating tasks and enhancing data analysis, they also pose significant risks, including hallucinations, data privacy concerns, and vulnerability to adversarial attacks. Through their coupling with decision-theoretic principles and Bayesian reasoning, LLMs can significantly improve decision-making processes within national security organizations. Namely, LLMs can facilitate the transition from data to actionable decisions, enabling decision-makers to quickly receive and distill available information with less manpower. Current applications within the US Department of Defense and beyond are explored, e.g., the USAF's use of LLMs for wargaming and automatic summarization, that illustrate their potential to streamline operations and support decision-making. However, these applications necessitate rigorous safeguards to ensure accuracy and reliability. The broader implications of LLM integration extend to strategic planning, international relations, and the broader geopolitical landscape, with adversarial nations leveraging LLMs for disinformation and cyber operations, emphasizing the need for robust countermeasures. Despite exhibiting "sparks" of artificial general intelligence, LLMs are best suited for supporting roles rather than leading strategic decisions. Their use in training and wargaming can provide valuable insights and personalized learning experiences for military personnel, thereby improving operational readiness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03453v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William N. Caballero, Phillip R. Jenkins</dc:creator>
    </item>
    <item>
      <title>Population Size Estimation with Many Lists and Heterogeneity: A Conditional Log-Linear Model Among the Unobserved</title>
      <link>https://arxiv.org/abs/2407.03539</link>
      <description>arXiv:2407.03539v1 Announce Type: cross 
Abstract: We contribute a general and flexible framework to estimate the size of a closed population in the presence of $K$ capture-recapture lists and heterogeneous capture probabilities. Our novel identifying strategy leverages the fact that it is sufficient for identification that a subset of the $K$ lists are not arbitrarily dependent \textit{within the subset of the population unobserved by the remaining lists}, conditional on covariates. This identification approach is interpretable and actionable, interpolating between the two predominant approaches in the literature as special cases: (conditional) independence across lists and log-linear models with no highest-order interaction. We derive nonparametric doubly-robust estimators for the resulting identification expression that are nearly optimal and approximately normal for any finite sample size, even when the heterogeneous capture probabilities are estimated nonparametrically using machine learning methods. Additionally, we devise a sensitivity analysis to show how deviations from the identification assumptions affect the resulting population size estimates, allowing for the integration of domain-specific knowledge into the identification and estimation processes more transparently. We empirically demonstrate the advantages of our method using both synthetic data and real data from the Peruvian internal armed conflict to estimate the number of casualties. The proposed methodology addresses recent critiques of capture-recapture models by allowing for a weaker and more interpretable identifying assumption and accommodating complex heterogeneous capture probabilities depending on high-dimensional or continuous covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03539v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mateo Dulce Rubio, Edward Kennedy</dc:creator>
    </item>
    <item>
      <title>Simulation-based Calibration of Uncertainty Intervals under Approximate Bayesian Estimation</title>
      <link>https://arxiv.org/abs/2407.04659</link>
      <description>arXiv:2407.04659v1 Announce Type: cross 
Abstract: The mean field variational Bayes (VB) algorithm implemented in Stan is relatively fast and efficient, making it feasible to produce model-estimated official statistics on a rapid timeline. Yet, while consistent point estimates of parameters are achieved for continuous data models, the mean field approximation often produces inaccurate uncertainty quantification to the extent that parameters are correlated a posteriori. In this paper, we propose a simulation procedure that calibrates uncertainty intervals for model parameters estimated under approximate algorithms to achieve nominal coverages. Our procedure detects and corrects biased estimation of both first and second moments of approximate marginal posterior distributions induced by any estimation algorithm that produces consistent first moments under specification of the correct model. The method generates replicate datasets using parameters estimated in an initial model run. The model is subsequently re-estimated on each replicate dataset, and we use the empirical distribution over the re-samples to formulate calibrated confidence intervals of parameter estimates of the initial model run that are guaranteed to asymptotically achieve nominal coverage. We demonstrate the performance of our procedure in Monte Carlo simulation study and apply it to real data from the Current Employment Statistics survey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04659v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Terrance D. Savitsky, Julie Gershunskaya</dc:creator>
    </item>
    <item>
      <title>Stereographic Projections for Designs on the Sphere</title>
      <link>https://arxiv.org/abs/2401.05931</link>
      <description>arXiv:2401.05931v2 Announce Type: replace 
Abstract: This paper is concerned with the use of the stereographic projection to map the points of a design on the sphere in three dimensions onto a two-dimensional stereogram. Details of the projection and its attendant stereogram are given and the application of the constructs to designs on the sphere is highlighted. Spherical $t$-designs are then introduced and stereograms of selected examples are presented. In addition, examples of regression models with design space the unit ball are considered, stereograms which represent the spherical component of such designs are given and an example in which stereograms are used to elucidate the geometric isomorphism of a suite of designs generated by an exchange algorithm is presented. Emphasis throughout the text is placed on the use of the stereographic projection in teaching and research within the framework of statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.05931v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linda M. Haines</dc:creator>
    </item>
    <item>
      <title>Active sampling: A machine-learning-assisted framework for finite population inference with optimal subsamples</title>
      <link>https://arxiv.org/abs/2212.10024</link>
      <description>arXiv:2212.10024v3 Announce Type: replace-cross 
Abstract: Data subsampling has become widely recognized as a tool to overcome computational and economic bottlenecks in analyzing massive datasets. We contribute to the development of adaptive design for estimation of finite population characteristics, using active learning and adaptive importance sampling. We propose an active sampling strategy that iterates between estimation and data collection with optimal subsamples, guided by machine learning predictions on yet unseen data. The method is illustrated on virtual simulation-based safety assessment of advanced driver assistance systems. Substantial performance improvements are demonstrated compared to traditional sampling methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.10024v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1080/00401706.2024.2374554</arxiv:DOI>
      <dc:creator>Henrik Imberg, Xiaomi Yang, Carol Flannagan, Jonas B\"argman</dc:creator>
    </item>
    <item>
      <title>Gotta match 'em all: Solution diversification in graph matching matched filters</title>
      <link>https://arxiv.org/abs/2308.13451</link>
      <description>arXiv:2308.13451v3 Announce Type: replace-cross 
Abstract: We present a novel approach for finding multiple noisily embedded template graphs in a very large background graph. Our method builds upon the graph-matching-matched-filter technique proposed in Sussman et al., with the discovery of multiple diverse matchings being achieved by iteratively penalizing a suitable node-pair similarity matrix in the matched filter algorithm. In addition, we propose algorithmic speed-ups that greatly enhance the scalability of our matched-filter approach. We present theoretical justification of our methodology in the setting of correlated Erdos-Renyi graphs, showing its ability to sequentially discover multiple templates under mild model conditions. We additionally demonstrate our method's utility via extensive experiments both using simulated models and real-world dataset, include human brain connectomes and a large transactional knowledge base.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.13451v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhirui Li, Ben Johnson, Daniel L. Sussman, Carey E. Priebe, Vince Lyzinski</dc:creator>
    </item>
    <item>
      <title>Detecting influential observations in single-index Fr\'echet regression</title>
      <link>https://arxiv.org/abs/2311.17246</link>
      <description>arXiv:2311.17246v3 Announce Type: replace-cross 
Abstract: Regression with random data objects is becoming increasingly common in modern data analysis. Unfortunately, this novel regression method is not immune to the trouble caused by unusual observations. A metric Cook's distance extending the original Cook's distances of Cook (1977) to regression between metric-valued response objects and Euclidean predictors is proposed. The performance of the metric Cook's distance is demonstrated in regression across four different response spaces in an extensive experimental study. Two real data applications involving the analyses of distributions of COVID-19 transmission in the State of Texas and the analyses of the structural brain connectivity networks are provided to illustrate the utility of the proposed method in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.17246v3</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdul-Nasah Soale</dc:creator>
    </item>
    <item>
      <title>Modelling clusters in network time series with an application to presidential elections in the USA</title>
      <link>https://arxiv.org/abs/2401.09381</link>
      <description>arXiv:2401.09381v2 Announce Type: replace-cross 
Abstract: Network time series are becoming increasingly relevant in the study of dynamic processes characterised by a known or inferred underlying network structure. Generalised Network Autoregressive (GNAR) models provide a parsimonious framework for exploiting the underlying network, even in the high-dimensional setting. We extend the GNAR framework by presenting the $\textit{community}$-$\alpha$ GNAR model that exploits prior knowledge and/or exogenous variables for identifying and modelling dynamic interactions across communities in the network. We further analyse the dynamics of $\textit{ Red, Blue}$ and $\textit{Swing}$ states throughout presidential elections in the USA. Our analysis suggests interesting global and communal effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.09381v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Guy Nason, Daniel Salnikov, Mario Cortina-Borja</dc:creator>
    </item>
    <item>
      <title>Integrating representative and non-representative survey data for efficient inference</title>
      <link>https://arxiv.org/abs/2404.02283</link>
      <description>arXiv:2404.02283v3 Announce Type: replace-cross 
Abstract: Non-representative surveys are commonly used and widely available but suffer from selection bias that generally cannot be entirely eliminated using weighting techniques. Instead, we propose a Bayesian method to synthesize longitudinal representative unbiased surveys with non-representative biased surveys by estimating the degree of selection bias over time. We show using a simulation study that synthesizing biased and unbiased surveys together out-performs using the unbiased surveys alone, even if the selection bias may evolve in a complex manner over time. Using COVID-19 vaccination data, we are able to synthesize two large sample biased surveys with an unbiased survey to reduce uncertainty in now-casting and inference estimates while simultaneously retaining the empirical credible interval coverage. Ultimately, we are able to conceptually obtain the properties of a large sample unbiased survey if the assumed unbiased survey, used to anchor the estimates, is unbiased for all time-points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02283v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathaniel Dyrkton, Paul Gustafson, Harlan Campbell</dc:creator>
    </item>
    <item>
      <title>Testing the Fairness-Accuracy Improvability of Algorithms</title>
      <link>https://arxiv.org/abs/2405.04816</link>
      <description>arXiv:2405.04816v3 Announce Type: replace-cross 
Abstract: Many organizations use algorithms that have a disparate impact, i.e., the benefits or harms of the algorithm fall disproportionately on certain social groups. Addressing an algorithm's disparate impact can be challenging, however, because it is often unclear whether it is possible to reduce this impact without sacrificing other objectives of the organization, such as accuracy or profit. Establishing the improvability of algorithms with respect to multiple criteria is of both conceptual and practical interest: in many settings, disparate impact that would otherwise be prohibited under US federal law is permissible if it is necessary to achieve a legitimate business interest. The question is how a policy-maker can formally substantiate, or refute, this "necessity" defense. In this paper, we provide an econometric framework for testing the hypothesis that it is possible to improve on the fairness of an algorithm without compromising on other pre-specified objectives. Our proposed test is simple to implement and can be applied under any exogenous constraint on the algorithm space. We establish the large-sample validity and consistency of our test, and illustrate its practical application by evaluating a healthcare algorithm originally considered by Obermeyer et al. (2019). In this application, we reject the null hypothesis that it is not possible to reduce the algorithm's disparate impact without compromising the accuracy of its predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04816v3</guid>
      <category>econ.EM</category>
      <category>cs.DS</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Auerbach, Annie Liang, Kyohei Okumura, Max Tabord-Meehan</dc:creator>
    </item>
    <item>
      <title>Volatility Forecasting Using Similarity-based Parameter Correction and Aggregated Shock Information</title>
      <link>https://arxiv.org/abs/2406.08738</link>
      <description>arXiv:2406.08738v2 Announce Type: replace-cross 
Abstract: We develop a procedure for forecasting the volatility of a time series immediately following a news shock. Adapting the similarity-based framework of Lin and Eck (2020), we exploit series that have experienced similar shocks. We aggregate their shock-induced excess volatilities by positing the shocks to be affine functions of exogenous covariates. The volatility shocks are modeled as random effects and estimated as fixed effects. The aggregation of these estimates is done in service of adjusting the $h$-step-ahead GARCH forecast of the time series under study by an additive term. The adjusted and unadjusted forecasts are evaluated using the unobservable but easily-estimated realized volatility (RV). A real-world application is provided, as are simulation results suggesting the conditions and hyperparameters under which our method thrives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08738v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David P. Lundquist, Daniel J. Eck</dc:creator>
    </item>
    <item>
      <title>Towards Statistically Significant Taxonomy Aware Co-location Pattern Detection</title>
      <link>https://arxiv.org/abs/2407.00317</link>
      <description>arXiv:2407.00317v2 Announce Type: replace-cross 
Abstract: Given a collection of Boolean spatial feature types, their instances, a neighborhood relation (e.g., proximity), and a hierarchical taxonomy of the feature types, the goal is to find the subsets of feature types or their parents whose spatial interaction is statistically significant. This problem is for taxonomy-reliant applications such as ecology (e.g., finding new symbiotic relationships across the food chain), spatial pathology (e.g., immunotherapy for cancer), retail, etc. The problem is computationally challenging due to the exponential number of candidate co-location patterns generated by the taxonomy. Most approaches for co-location pattern detection overlook the hierarchical relationships among spatial features, and the statistical significance of the detected patterns is not always considered, leading to potential false discoveries. This paper introduces two methods for incorporating taxonomies and assessing the statistical significance of co-location patterns. The baseline approach iteratively checks the significance of co-locations between leaf nodes or their ancestors in the taxonomy. Using the Benjamini-Hochberg procedure, an advanced approach is proposed to control the false discovery rate. This approach effectively reduces the risk of false discoveries while maintaining the power to detect true co-location patterns. Experimental evaluation and case study results show the effectiveness of the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00317v2</guid>
      <category>cs.IR</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhankar Ghosh, Arun Sharma, Jayant Gupta, Shashi Shekhar</dc:creator>
    </item>
  </channel>
</rss>
