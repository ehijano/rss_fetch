<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 Aug 2024 01:36:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 22 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>High-Dimensional Overdispersed Generalized Factor Model with Application to Single-Cell Sequencing Data Analysis</title>
      <link>https://arxiv.org/abs/2408.11272</link>
      <description>arXiv:2408.11272v1 Announce Type: new 
Abstract: The current high-dimensional linear factor models fail to account for the different types of variables, while high-dimensional nonlinear factor models often overlook the overdispersion present in mixed-type data. However, overdispersion is prevalent in practical applications, particularly in fields like biomedical and genomics studies. To address this practical demand, we propose an overdispersed generalized factor model (OverGFM) for performing high-dimensional nonlinear factor analysis on overdispersed mixed-type data. Our approach incorporates an additional error term to capture the overdispersion that cannot be accounted for by factors alone. However, this introduces significant computational challenges due to the involvement of two high-dimensional latent random matrices in the nonlinear model. To overcome these challenges, we propose a novel variational EM algorithm that integrates Laplace and Taylor approximations. This algorithm provides iterative explicit solutions for the complex variational parameters and is proven to possess excellent convergence properties. We also develop a criterion based on the singular value ratio to determine the optimal number of factors. Numerical results demonstrate the effectiveness of this criterion. Through comprehensive simulation studies, we show that OverGFM outperforms state-of-the-art methods in terms of estimation accuracy and computational efficiency. Furthermore, we demonstrate the practical merit of our method through its application to two datasets from genomics. To facilitate its usage, we have integrated the implementation of OverGFM into the R package GFM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11272v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jinyu Nie, Zhilong Qin, Wei Liu</dc:creator>
    </item>
    <item>
      <title>Locally Adaptive Random Walk Stochastic Volatility</title>
      <link>https://arxiv.org/abs/2408.11315</link>
      <description>arXiv:2408.11315v1 Announce Type: new 
Abstract: We introduce a novel Bayesian framework for estimating time-varying volatility by extending the Random Walk Stochastic Volatility (RWSV) model with a new Dynamic Shrinkage Process (DSP) in (log) variances. Unlike classical Stochastic Volatility or GARCH-type models with restrictive parametric stationarity assumptions, our proposed Adaptive Stochastic Volatility (ASV) model provides smooth yet dynamically adaptive estimates of evolving volatility and its uncertainty (vol of vol). We derive the theoretical properties of the proposed global-local shrinkage prior. Through simulation studies, we demonstrate that ASV exhibits remarkable misspecification resilience with low prediction error across various data generating scenarios in simulation. Furthermore, ASV's capacity to yield locally smooth and interpretable estimates facilitates a clearer understanding of underlying patterns and trends in volatility. Additionally, we propose and illustrate an extension for Bayesian Trend Filtering simultaneously in both mean and variance. Finally, we show that this attribute makes ASV a robust tool applicable across a wide range of disciplines, including in finance, environmental science, epidemiology, and medicine, among others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11315v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason B. Cho, David S. Matteson</dc:creator>
    </item>
    <item>
      <title>Climate Change in Austria: Precipitation and Dry Spells over 50 years</title>
      <link>https://arxiv.org/abs/2408.11497</link>
      <description>arXiv:2408.11497v1 Announce Type: new 
Abstract: We propose a spatio-temporal generalised additive model (GAM) to study if precipitation patterns have changed between two 10-year time periods in the last 50 years in Austria. In particular, we model three scenarios: monthly mean and monthly maximum precipitation as well as the maximum length of a dry spell per month with a gamma, blended generalised extreme value and negative binomial distribution, respectively, over the periods 1973-1982 and 2013-2022. In order to model the spatial dependencies in the data more realistically, we intend to take the mountainous landscape of Austria into account. Therefore, we have chosen a non-stationary version of the Mat\'ern covariance function, which accounts for elevation differences, as a spatial argument of the latent field in the GAM. The temporal part of the latent field is captured by an AR(1) process. We use the stochastic partial differential equation approach in combination with integrated nested Laplace approximation to perform inference computationally efficient. The model outputs are visualised and support existing climate change studies in the Alpine region obtained with, for example, projections from regional climate models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11497v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Corinna Perchtold, Evelyn Buckwar</dc:creator>
    </item>
    <item>
      <title>On the handling of method failure in comparison studies</title>
      <link>https://arxiv.org/abs/2408.11594</link>
      <description>arXiv:2408.11594v1 Announce Type: new 
Abstract: Comparison studies in methodological research are intended to compare methods in an evidence-based manner, offering guidance to data analysts to select a suitable method for their application. To provide trustworthy evidence, they must be carefully designed, implemented, and reported, especially given the many decisions made in planning and running. A common challenge in comparison studies is to handle the ``failure'' of one or more methods to produce a result for some (real or simulated) data sets, such that their performances cannot be measured in those instances. Despite an increasing emphasis on this topic in recent literature (focusing on non-convergence as a common manifestation), there is little guidance on proper handling and interpretation, and reporting of the chosen approach is often neglected. This paper aims to fill this gap and provides practical guidance for handling method failure in comparison studies. In particular, we show that the popular approaches of discarding data sets yielding failure (either for all or the failing methods only) and imputing are inappropriate in most cases. We also discuss how method failure in published comparison studies -- in various contexts from classical statistics and predictive modeling -- may manifest differently, but is often caused by a complex interplay of several aspects. Building on this, we provide recommendations derived from realistic considerations on suitable fallbacks when encountering method failure, hence avoiding the need for discarding data sets or imputation. Finally, we illustrate our recommendations and the dangers of inadequate handling of method failure through two illustrative comparison studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11594v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Milena W\"unsch, Moritz Herrmann, Elisa Noltenius, Mattia Mohr, Tim P. Morris, Anne-Laure Boulesteix</dc:creator>
    </item>
    <item>
      <title>Evidential Analysis: An Alternative to Hypothesis Testing in Normal Linear Models</title>
      <link>https://arxiv.org/abs/2408.11672</link>
      <description>arXiv:2408.11672v1 Announce Type: new 
Abstract: Statistical hypothesis testing, as formalized by 20th Century statisticians and taught in college statistics courses, has been a cornerstone of 100 years of scientific progress. Nevertheless, the methodology is increasingly questioned in many scientific disciplines. We demonstrate in this paper how many of the worrisome aspects of statistical hypothesis testing can be ameliorated with concepts and methods from evidential analysis. The model family we treat is the familiar normal linear model with fixed effects, embracing multiple regression and analysis of variance, a warhorse of everyday science in labs and field stations. Questions about study design, the applicability of the null hypothesis, the effect size, error probabilities, evidence strength, and model misspecification become more naturally housed in an evidential setting. We provide a completely worked example featuring a 2-way analysis of variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11672v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Brian Dennis, Mark L Taper, Jos\'e M Ponciano</dc:creator>
    </item>
    <item>
      <title>Scalable and non-iterative graphical model estimation</title>
      <link>https://arxiv.org/abs/2408.11718</link>
      <description>arXiv:2408.11718v1 Announce Type: new 
Abstract: Graphical models have found widespread applications in many areas of modern statistics and machine learning. Iterative Proportional Fitting (IPF) and its variants have become the default method for undirected graphical model estimation, and are thus ubiquitous in the field. As the IPF is an iterative approach, it is not always readily scalable to modern high-dimensional data regimes. In this paper we propose a novel and fast non-iterative method for positive definite graphical model estimation in high dimensions, one that directly addresses the shortcomings of IPF and its variants. In addition, the proposed method has a number of other attractive properties. First, we show formally that as the dimension p grows, the proportion of graphs for which the proposed method will outperform the state-of-the-art in terms of computational complexity and performance tends to 1, affirming its efficacy in modern settings. Second, the proposed approach can be readily combined with scalable non-iterative thresholding-based methods for high-dimensional sparsity selection. Third, the proposed method has high-dimensional statistical guarantees. Moreover, our numerical experiments also show that the proposed method achieves scalability without compromising on statistical precision. Fourth, unlike the IPF, which depends on the Gaussian likelihood, the proposed method is much more robust.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11718v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kshitij Khare, Syed Rahman, Bala Rajaratnam, Jiayuan Zhou</dc:creator>
    </item>
    <item>
      <title>Bayesian Nonparametric Risk Assessment in Developmental Toxicity Studies with Ordinal Responses</title>
      <link>https://arxiv.org/abs/2408.11803</link>
      <description>arXiv:2408.11803v1 Announce Type: new 
Abstract: We develop a nonparametric Bayesian modeling framework for clustered ordinal responses in developmental toxicity studies, which typically exhibit extensive heterogeneity. The primary focus of these studies is to examine the dose-response relationship, which is depicted by the (conditional) probability of an endpoint across the dose (toxin) levels. Standard parametric approaches, limited in terms of the response distribution and/or the dose-response relationship, hinder reliable uncertainty quantification in this context. We propose nonparametric mixture models that are built from dose-dependent stick-breaking process priors, leveraging the continuation-ratio logits representation of the multinomial distribution to formulate the mixture kernel. We further elaborate the modeling approach, amplifying the mixture models with an overdispersed kernel which offers enhanced control of variability. We conduct a simulation study to demonstrate the benefits of both the discrete nonparametric mixing structure and the overdispersed kernel in delivering coherent uncertainty quantification. Further illustration is provided with different forms of risk assessment, using data from a toxicity experiment on the effects of ethylene glycol.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11803v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jizhou Kang, Athanasios Kottas</dc:creator>
    </item>
    <item>
      <title>Distance Correlation in Multiple Biased Sampling Models</title>
      <link>https://arxiv.org/abs/2408.11808</link>
      <description>arXiv:2408.11808v1 Announce Type: new 
Abstract: Testing the independence between random vectors is a fundamental problem in statistics. Distance correlation, a recently popular dependence measure, is universally consistent for testing independence against all distributions with finite moments. However, when data are subject to selection bias or collected from multiple sources or schemes, spurious dependence may arise. This creates a need for methods that can effectively utilize data from different sources and correct these biases. In this paper, we study the estimation of distance covariance and distance correlation under multiple biased sampling models, which provide a natural framework for addressing these issues. Theoretical properties, including the strong consistency and asymptotic null distributions of the distance covariance and correlation estimators, and the rate at which the test statistic diverges under sequences of alternatives approaching the null, are established. A weighted permutation procedure is proposed to determine the critical value of the independence test. Simulation studies demonstrate that our approach improves both the estimation of distance correlation and the power of the test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11808v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuwei Ke, Hok Kan Ling, Yanglei Song</dc:creator>
    </item>
    <item>
      <title>The Ensemble Epanechnikov Mixture Filter</title>
      <link>https://arxiv.org/abs/2408.11164</link>
      <description>arXiv:2408.11164v1 Announce Type: cross 
Abstract: In the high-dimensional setting, Gaussian mixture kernel density estimates become increasingly suboptimal. In this work we aim to show that it is practical to instead use the optimal multivariate Epanechnikov kernel. We make use of this optimal Epanechnikov mixture kernel density estimate for the sequential filtering scenario through what we term the ensemble Epanechnikov mixture filter (EnEMF). We provide a practical implementation of the EnEMF that is as cost efficient as the comparable ensemble Gaussian mixture filter. We show on a static example that the EnEMF is robust to growth in dimension, and also that the EnEMF has a significant reduction in error per particle on the 40-variable Lorenz '96 system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11164v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrey A. Popov, Renato Zanetti</dc:creator>
    </item>
    <item>
      <title>Small Sample Behavior of Wasserstein Projections, Connections to Empirical Likelihood, and Other Applications</title>
      <link>https://arxiv.org/abs/2408.11753</link>
      <description>arXiv:2408.11753v1 Announce Type: cross 
Abstract: The empirical Wasserstein projection (WP) distance quantifies the Wasserstein distance from the empirical distribution to a set of probability measures satisfying given expectation constraints. The WP is a powerful tool because it mitigates the curse of dimensionality inherent in the Wasserstein distance, making it valuable for various tasks, including constructing statistics for hypothesis testing, optimally selecting the ambiguity size in Wasserstein distributionally robust optimization, and studying algorithmic fairness. While the weak convergence analysis of the WP as the sample size $n$ grows is well understood, higher-order (i.e., sharp) asymptotics of WP remain unknown. In this paper, we study the second-order asymptotic expansion and the Edgeworth expansion of WP, both expressed as power series of $n^{-1/2}$. These expansions are essential to develop improved confidence level accuracy and a power expansion analysis for the WP-based tests for moment equations null against local alternative hypotheses. As a by-product, we obtain insightful criteria for comparing the power of the Empirical Likelihood and Hotelling's $T^2$ tests against the WP-based test. This insight provides the first comprehensive guideline for selecting the most powerful local test among WP-based, empirical-likelihood-based, and Hotelling's $T^2$ tests for a null. Furthermore, we introduce Bartlett-type corrections to improve the approximation to WP distance quantiles and, thus, improve the coverage in WP applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11753v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sirui Lin, Jose Blanchet, Peter Glynn, Viet Anh Nguyen</dc:creator>
    </item>
    <item>
      <title>The matryoshka doll prior: principled penalization in Bayesian selection</title>
      <link>https://arxiv.org/abs/1511.04745</link>
      <description>arXiv:1511.04745v2 Announce Type: replace 
Abstract: This paper introduces a general and principled construction of model space priors with a focus on regression problems. The proposed formulation regards each model as a ``local'' null hypothesis whose alternatives are the set of models that nest it. A simple proportionality principle yields a natural isomorphism of model spaces induced by conditioning on predictor inclusion before or after observing data. This isomorphism produces the Poisson distribution as the unique limiting distribution over model dimension under mild assumptions. We compare this model space prior theoretically and in simulations to widely adopted Beta-Binomial constructions and show that the proposed prior yields a ``just-right'' penalization profile.</description>
      <guid isPermaLink="false">oai:arXiv.org:1511.04745v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Andrew J Womack, Daniel Taylor-Rodriguez, Claudio Fuentes</dc:creator>
    </item>
    <item>
      <title>Estimating Shapley Effects in Big-Data Emulation and Regression Settings using Bayesian Additive Regression Trees</title>
      <link>https://arxiv.org/abs/2304.03809</link>
      <description>arXiv:2304.03809v2 Announce Type: replace 
Abstract: Shapley effects are a particularly interpretable approach to assessing how a function depends on its various inputs. The existing literature contains various estimators for this class of sensitivity indices in the context of nonparametric regression where the function is observed with noise, but there does not seem to be an estimator that is computationally tractable for input dimensions in the hundreds scale. This article provides such an estimator that is computationally tractable on this scale. The estimator uses a metamodel-based approach by first fitting a Bayesian Additive Regression Trees model which is then used to compute Shapley-effect estimates. This article also establishes a theoretical guarantee of posterior consistency on a large function class for this Shapley-effect estimator. Finally, this paper explores the performance of these Shapley-effect estimators on four different test functions for various input dimensions, including $p=500$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.03809v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akira Horiguchi, Matthew T. Pratola</dc:creator>
    </item>
    <item>
      <title>Fatigue detection via sequential testing of biomechanical data using martingale statistic</title>
      <link>https://arxiv.org/abs/2306.01566</link>
      <description>arXiv:2306.01566v2 Announce Type: replace 
Abstract: Injuries to the knee joint are very common for long-distance and frequent runners, an issue which is often attributed to fatigue. We address the problem of fatigue detection from biomechanical data from different sources, consisting of lower extremity joint angles and ground reaction forces from running athletes with the goal of better understanding the impact of fatigue on the biomechanics of runners in general and on an individual level. This is done by sequentially testing for change in a datastream using a simple martingale test statistic. Time-uniform probabilistic martingale bounds are provided which are used as thresholds for the test statistic. Sharp bounds can be developed by a hybrid of a piece-wise linear- and a law of iterated logarithm- bound over all time regimes, where the probability of an early detection is controlled in a uniform way. If the underlying distribution of the data gradually changes over the course of a run, then a timely upcrossing of the martingale over these bounds is expected. The methods are developed for a setting when change sets in gradually in an incoming stream of data. Parameter selection for the bounds are based on simulations and methodological comparison is done with respect to existing advances. The algorithms presented here can be easily adapted to an online change-detection setting. Finally, we provide a detailed data analysis based on extensive measurements of several athletes and benchmark the fatigue detection results with the runners' individual feedback over the course of the data collection. Qualitative conclusions on the biomechanical profiles of the athletes can be made based on the shape of the martingale trajectories even in the absence of an upcrossing of the threshold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.01566v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rupsa Basu, Katharina Proksch</dc:creator>
    </item>
    <item>
      <title>Inference on summaries of a model-agnostic longitudinal variable importance trajectory with application to suicide prevention</title>
      <link>https://arxiv.org/abs/2311.01638</link>
      <description>arXiv:2311.01638v2 Announce Type: replace 
Abstract: Risk of suicide attempt varies over time. Understanding the importance of risk factors measured at a mental health visit can help clinicians evaluate future risk and provide appropriate care during the visit. In prediction settings where data are collected over time, such as in mental health care, it is often of interest to understand both the importance of variables for predicting the response at each time point and the importance summarized over the time series. Building on recent advances in estimation and inference for variable importance measures, we define summaries of variable importance trajectories and corresponding estimators. The same approaches for inference can be applied to these measures regardless of the choice of the algorithm(s) used to estimate the prediction function. We propose a nonparametric efficient estimation and inference procedure as well as a null hypothesis testing procedure that are valid even when complex machine learning tools are used for prediction. Through simulations, we demonstrate that our proposed procedures have good operating characteristics. We use these approaches to analyze electronic health records data from two large health systems to investigate the longitudinal importance of risk factors for suicide attempt to inform future suicide prevention research and clinical workflow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01638v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian D. Williamson, Erica E. M. Moodie, Gregory E. Simon, Rebecca C. Rossom, Susan M. Shortreed</dc:creator>
    </item>
    <item>
      <title>Counterfactual Slopes and Their Applications in Social Stratification</title>
      <link>https://arxiv.org/abs/2401.07000</link>
      <description>arXiv:2401.07000v2 Announce Type: replace 
Abstract: This paper addresses two prominent theses in social stratification research, the great equalizer thesis and Mare's (1980) school transition thesis. Both theses are premised on a descriptive regularity: the association between socioeconomic background and an outcome variable changes when conditioning on an intermediate treatment. However, if the descriptive regularity is driven by differential selection into treatment, then the two theses do not have substantive interpretations. We propose a set of novel counterfactual slope estimands, which capture the two theses under the hypothetical scenario where differential selection into treatment is eliminated. Thus, we use the counterfactual slopes to construct selection-free tests for the two theses. Compared with the existing literature, we are the first to explicitly provide nonparametric and causal estimands, which enable us to conduct more principled analysis. We are also the first to develop flexible, efficient, and robust estimators for the two theses based on efficient influence functions. We apply our framework to a nationally representative dataset in the United States and re-evaluate the two theses. Findings from our selection-free tests show that the descriptive regularity is sometimes misleading for substantive interpretations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07000v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ang Yu, Jiwei Zhao</dc:creator>
    </item>
    <item>
      <title>Gambling-Based Confidence Sequences for Bounded Random Vectors</title>
      <link>https://arxiv.org/abs/2402.03683</link>
      <description>arXiv:2402.03683v2 Announce Type: replace 
Abstract: A confidence sequence (CS) is a sequence of confidence sets that contains a target parameter of an underlying stochastic process at any time step with high probability. This paper proposes a new approach to constructing CSs for means of bounded multivariate stochastic processes using a general gambling framework, extending the recently established coin toss framework for bounded random processes. The proposed gambling framework provides a general recipe for constructing CSs for categorical and probability-vector-valued observations, as well as for general bounded multidimensional observations through a simple reduction. This paper specifically explores the use of the mixture portfolio, akin to Cover's universal portfolio, in the proposed framework and investigates the properties of the resulting CSs. Simulations demonstrate the tightness of these confidence sequences compared to existing methods. When applied to the sampling without-replacement setting for finite categorical data, it is shown that the resulting CS based on a universal gambling strategy is provably tighter than that of the posterior-prior ratio martingale proposed by Waudby-Smith and Ramdas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.03683v2</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. Jon Ryu, Gregory W. Wornell</dc:creator>
    </item>
    <item>
      <title>Bayesian Learning of Relational Graph in Semiparametric High-dimensional Time Series</title>
      <link>https://arxiv.org/abs/2403.04915</link>
      <description>arXiv:2403.04915v3 Announce Type: replace 
Abstract: Time series data arising in many applications nowadays are high-dimensional. A large number of parameters describe features of these time series. We propose a novel approach to modeling a high-dimensional time series through several independent univariate time series, which are then orthogonally rotated and sparsely linearly transformed. With this approach, any specified intrinsic relations among component time series given by a graphical structure can be maintained at all time snapshots. We call the resulting process an Orthogonally-rotated Univariate Time series (OUT). Key structural properties of time series such as stationarity and causality can be easily accommodated in the OUT model. For Bayesian inference, we put suitable prior distributions on the spectral densities of the independent latent times series, the orthogonal rotation matrix, and the common precision matrix of the component times series at every time point. A likelihood is constructed using the Whittle approximation for univariate latent time series. An efficient Markov Chain Monte Carlo (MCMC) algorithm is developed for posterior computation. We study the convergence of the pseudo-posterior distribution based on the Whittle likelihood for the model's parameters upon developing a new general posterior convergence theorem for pseudo-posteriors. We find that the posterior contraction rate for independent observations essentially prevails in the OUT model under very mild conditions on the temporal dependence described in terms of the smoothness of the corresponding spectral densities. Through a simulation study, we compare the accuracy of estimating the parameters and identifying the graphical structure with other approaches. We apply the proposed methodology to analyze a dataset on different industrial components of the US gross domestic product between 2010 and 2019 and predict future observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04915v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arkaprava Roy, Anindya Roy, Subhashis Ghosal</dc:creator>
    </item>
    <item>
      <title>Combining BART and Principal Stratification to estimate the effect of intermediate on primary outcomes with application to estimating the effect of family planning on employment in sub-Saharan Africa</title>
      <link>https://arxiv.org/abs/2408.03777</link>
      <description>arXiv:2408.03777v3 Announce Type: replace 
Abstract: There is interest in learning about the causal effect of family planning (FP) on empowerment related outcomes. Experimental data related to this question are available from trials in which FP programs increase access to FP. While program assignment is unconfounded, FP uptake and subsequent empowerment may share common causes. We use principal stratification to estimate the causal effect of an intermediate FP outcome on a primary outcome of interest, among women affected by a FP program. Within strata defined by the potential reaction to the program, FP uptake is unconfounded. To minimize the need for parametric assumptions, we propose to use Bayesian Additive Regression Trees (BART) for modeling stratum membership and outcomes of interest. We refer to the combined approach as Prince BART. We evaluate Prince BART through a simulation study and use it to assess the causal effect of modern contraceptive use on employment in six cities in Nigeria, based on quasi-experimental data from a FP program trial during the first half of the 2010s. We show that findings differ between Prince BART and alternative modeling approaches based on parametric assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03777v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Godoy Garraza, Ilene Speizer, Leontine Alkema</dc:creator>
    </item>
    <item>
      <title>Censored and extreme losses: functional convergence and applications to tail goodness-of-fit</title>
      <link>https://arxiv.org/abs/2408.05862</link>
      <description>arXiv:2408.05862v2 Announce Type: replace 
Abstract: This paper establishes the functional convergence of the Extreme Nelson--Aalen and Extreme Kaplan--Meier estimators, which are designed to capture the heavy-tailed behaviour of censored losses. The resulting limit representations can be used to obtain the distributions of pathwise functionals with respect to the so-called tail process. For instance, we may recover the convergence of a censored Hill estimator, and we further investigate two goodness-of-fit statistics for the tail of the loss distribution. Using the the latter limit theorems, we propose two rules for selecting a suitable number of order statistics, both based on test statistics derived from the functional convergence results. The effectiveness of these selection rules is investigated through simulations and an application to a real dataset comprised of French motor insurance claim sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05862v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Bladt, Christoffer {\O}hlenschl{\ae}ger</dc:creator>
    </item>
    <item>
      <title>On Confidence Sequences for Bounded Random Processes via Universal Gambling Strategies</title>
      <link>https://arxiv.org/abs/2207.12382</link>
      <description>arXiv:2207.12382v3 Announce Type: replace-cross 
Abstract: This paper considers the problem of constructing a confidence sequence, which is a sequence of confidence intervals that hold uniformly over time, for estimating the mean of bounded real-valued random processes. This paper revisits the gambling-based approach established in the recent literature from a natural \emph{two-horse race} perspective, and demonstrates new properties of the resulting algorithm induced by Cover (1991)'s universal portfolio. The main result of this paper is a new algorithm based on a mixture of lower bounds, which closely approximates the performance of Cover's universal portfolio with constant per-round time complexity. A higher-order generalization of a lower bound on a logarithmic function in (Fan et al., 2015), which is developed as a key technique for the proposed algorithm, may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.12382v3</guid>
      <category>math.PR</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. Jon Ryu, Alankrita Bhatt</dc:creator>
    </item>
    <item>
      <title>Joint Spectral Clustering in Multilayer Degree-Corrected Stochastic Blockmodels</title>
      <link>https://arxiv.org/abs/2212.05053</link>
      <description>arXiv:2212.05053v2 Announce Type: replace-cross 
Abstract: Modern network datasets are often composed of multiple layers, either as different views, time-varying observations, or independent sample units, resulting in collections of networks over the same set of vertices but with potentially different connectivity patterns on each network. These data require models and methods that are flexible enough to capture local and global differences across the networks, while at the same time being parsimonious and tractable to yield computationally efficient and theoretically sound solutions that are capable of aggregating information across the networks. This paper considers the multilayer degree-corrected stochastic blockmodel, where a collection of networks share the same community structure, but degree-corrections and block connection probability matrices are permitted to be different. We establish the identifiability of this model and propose a spectral clustering algorithm for community detection in this setting. Our theoretical results demonstrate that the misclustering error rate of the algorithm improves exponentially with multiple network realizations, even in the presence of significant layer heterogeneity with respect to degree corrections, signal strength, and spectral properties of the block connection probability matrices. Simulation studies show that this approach improves on existing multilayer community detection methods in this challenging regime. Furthermore, in a case study of US airport data through January 2016 -- September 2021, we find that this methodology identifies meaningful community structure and trends in airport popularity influenced by pandemic impacts on travel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.05053v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Agterberg, Zachary Lubberts, Jes\'us Arroyo</dc:creator>
    </item>
    <item>
      <title>Spike-and-slab shrinkage priors for structurally sparse Bayesian neural networks</title>
      <link>https://arxiv.org/abs/2308.09104</link>
      <description>arXiv:2308.09104v2 Announce Type: replace-cross 
Abstract: Network complexity and computational efficiency have become increasingly significant aspects of deep learning. Sparse deep learning addresses these challenges by recovering a sparse representation of the underlying target function by reducing heavily over-parameterized deep neural networks. Specifically, deep neural architectures compressed via structured sparsity (e.g. node sparsity) provide low latency inference, higher data throughput, and reduced energy consumption. In this paper, we explore two well-established shrinkage techniques, Lasso and Horseshoe, for model compression in Bayesian neural networks. To this end, we propose structurally sparse Bayesian neural networks which systematically prune excessive nodes with (i) Spike-and-Slab Group Lasso (SS-GL), and (ii) Spike-and-Slab Group Horseshoe (SS-GHS) priors, and develop computationally tractable variational inference including continuous relaxation of Bernoulli variables. We establish the contraction rates of the variational posterior of our proposed models as a function of the network topology, layer-wise node cardinalities, and bounds on the network weights. We empirically demonstrate the competitive performance of our models compared to the baseline models in prediction accuracy, model compression, and inference latency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.09104v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanket Jantre, Shrijita Bhattacharya, Tapabrata Maiti</dc:creator>
    </item>
    <item>
      <title>Ab initio uncertainty quantification in scattering analysis of microscopy</title>
      <link>https://arxiv.org/abs/2309.02468</link>
      <description>arXiv:2309.02468v3 Announce Type: replace-cross 
Abstract: Estimating parameters from data is a fundamental problem in physics, customarily done by minimizing a loss function between a model and observed statistics. In scattering-based analysis, researchers often employ their domain expertise to select a specific range of wavevectors for analysis, a choice that can vary depending on the specific case. We introduce another paradigm that defines a probabilistic generative model from the beginning of data processing and propagates the uncertainty for parameter estimation, termed ab initio uncertainty quantification (AIUQ). As an illustrative example, we demonstrate this approach with differential dynamic microscopy (DDM) that extracts dynamical information through Fourier analysis at a selected range of wavevectors. We first show that DDM is equivalent to fitting a temporal variogram in the reciprocal space using a latent factor model as the generative model. Then we derive the maximum marginal likelihood estimator, which optimally weighs information at all wavevectors, therefore eliminating the need to select the range of wavevectors. Furthermore, we substantially reduce the computational cost by utilizing the generalized Schur algorithm for Toeplitz covariances without approximation. Simulated studies validate that AIUQ significantly improves estimation accuracy and enables model selection with automated analysis. The utility of AIUQ is also demonstrated by three distinct sets of experiments: first in an isotropic Newtonian fluid, pushing limits of optically dense systems compared to multiple particle tracking; next in a system undergoing a sol-gel transition, automating the determination of gelling points and critical exponent; and lastly, in discerning anisotropic diffusive behavior of colloids in a liquid crystal. These outcomes collectively underscore AIUQ's versatility to capture system dynamics in an efficient and automated manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.02468v3</guid>
      <category>physics.comp-ph</category>
      <category>cond-mat.soft</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengyang Gu, Yue He, Xubo Liu, Yimin Luo</dc:creator>
    </item>
    <item>
      <title>Is Cross-Validation the Gold Standard to Evaluate Model Performance?</title>
      <link>https://arxiv.org/abs/2407.02754</link>
      <description>arXiv:2407.02754v2 Announce Type: replace-cross 
Abstract: Cross-Validation (CV) is the default choice for evaluating the performance of machine learning models. Despite its wide usage, their statistical benefits have remained half-understood, especially in challenging nonparametric regimes. In this paper we fill in this gap and show that in fact, for a wide spectrum of models, CV does not statistically outperform the simple "plug-in" approach where one reuses training data for testing evaluation. Specifically, in terms of both the asymptotic bias and coverage accuracy of the associated interval for out-of-sample evaluation, $K$-fold CV provably cannot outperform plug-in regardless of the rate at which the parametric or nonparametric models converge. Leave-one-out CV can have a smaller bias as compared to plug-in; however, this bias improvement is negligible compared to the variability of the evaluation, and in some important cases leave-one-out again does not outperform plug-in once this variability is taken into account. We obtain our theoretical comparisons via a novel higher-order Taylor analysis that allows us to derive necessary conditions for limit theorems of testing evaluations, which applies to model classes that are not amenable to previously known sufficient conditions. Our numerical results demonstrate that plug-in performs indeed no worse than CV across a wide range of examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02754v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Garud Iyengar, Henry Lam, Tianyu Wang</dc:creator>
    </item>
    <item>
      <title>Online Distributional Regression</title>
      <link>https://arxiv.org/abs/2407.08750</link>
      <description>arXiv:2407.08750v2 Announce Type: replace-cross 
Abstract: Large-scale streaming data are common in modern machine learning applications and have led to the development of online learning algorithms. Many fields, such as supply chain management, weather and meteorology, energy markets, and finance, have pivoted towards using probabilistic forecasts, which yields the need not only for accurate learning of the expected value but also for learning the conditional heteroskedasticity and conditional distribution moments. Against this backdrop, we present a methodology for online estimation of regularized, linear distributional models. The proposed algorithm is based on a combination of recent developments for the online estimation of LASSO models and the well-known GAMLSS framework. We provide a case study on day-ahead electricity price forecasting, in which we show the competitive performance of the incremental estimation combined with strongly reduced computational effort. Our algorithms are implemented in a computationally efficient Python package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08750v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Hirsch, Jonathan Berrisch, Florian Ziel</dc:creator>
    </item>
  </channel>
</rss>
