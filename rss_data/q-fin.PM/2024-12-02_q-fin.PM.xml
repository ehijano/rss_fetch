<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.PM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.PM</link>
    <description>q-fin.PM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.PM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 03 Dec 2024 04:26:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Double Descent in Portfolio Optimization: Dance between Theoretical Sharpe Ratio and Estimation Accuracy</title>
      <link>https://arxiv.org/abs/2411.18830</link>
      <description>arXiv:2411.18830v1 Announce Type: new 
Abstract: We study the relationship between model complexity and out-of-sample performance in the context of mean-variance portfolio optimization. Representing model complexity by the number of assets, we find that the performance of low-dimensional models initially improves with complexity but then declines due to overfitting. As model complexity becomes sufficiently high, the performance improves with complexity again, resulting in a double ascent Sharpe ratio curve similar to the double descent phenomenon observed in artificial intelligence. The underlying mechanisms involve an intricate interaction between the theoretical Sharpe ratio and estimation accuracy. In high-dimensional models, the theoretical Sharpe ratio approaches its upper limit, and the overfitting problem is reduced because there are more parameters than data restrictions, which allows us to choose well-behaved parameters based on inductive bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18830v1</guid>
      <category>q-fin.PM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yonghe Lu, Yanrong Yang, Terry Zhang</dc:creator>
    </item>
    <item>
      <title>Dynamic ETF Portfolio Optimization Using enhanced Transformer-Based Models for Covariance and Semi-Covariance Prediction(Work in Progress)</title>
      <link>https://arxiv.org/abs/2411.19649</link>
      <description>arXiv:2411.19649v1 Announce Type: new 
Abstract: This study explores the use of Transformer-based models to predict both covariance and semi-covariance matrices for ETF portfolio optimization. Traditional portfolio optimization techniques often rely on static covariance estimates or impose strict model assumptions, which may fail to capture the dynamic and non-linear nature of market fluctuations. Our approach leverages the power of Transformer models to generate adaptive, real-time predictions of asset covariances, with a focus on the semi-covariance matrix to account for downside risk. The semi-covariance matrix emphasizes negative correlations between assets, offering a more nuanced approach to risk management compared to traditional methods that treat all volatility equally.
  Through a series of experiments, we demonstrate that Transformer-based predictions of both covariance and semi-covariance significantly enhance portfolio performance. Our results show that portfolios optimized using the semi-covariance matrix outperform those optimized with the standard covariance matrix, particularly in volatile market conditions. Moreover, the use of the Sortino ratio, a risk-adjusted performance metric that focuses on downside risk, further validates the effectiveness of our approach in managing risk while maximizing returns.
  These findings have important implications for asset managers and investors, offering a dynamic, data-driven framework for portfolio construction that adapts more effectively to shifting market conditions. By integrating Transformer-based models with the semi-covariance matrix for improved risk management, this research contributes to the growing field of machine learning in finance and provides valuable insights for optimizing ETF portfolios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19649v1</guid>
      <category>q-fin.PM</category>
      <category>q-fin.CP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiahao Zhu, Hengzhi Wu</dc:creator>
    </item>
    <item>
      <title>BPQP: A Differentiable Convex Optimization Framework for Efficient End-to-End Learning</title>
      <link>https://arxiv.org/abs/2411.19285</link>
      <description>arXiv:2411.19285v1 Announce Type: cross 
Abstract: Data-driven decision-making processes increasingly utilize end-to-end learnable deep neural networks to render final decisions. Sometimes, the output of the forward functions in certain layers is determined by the solutions to mathematical optimization problems, leading to the emergence of differentiable optimization layers that permit gradient back-propagation. However, real-world scenarios often involve large-scale datasets and numerous constraints, presenting significant challenges. Current methods for differentiating optimization problems typically rely on implicit differentiation, which necessitates costly computations on the Jacobian matrices, resulting in low efficiency. In this paper, we introduce BPQP, a differentiable convex optimization framework designed for efficient end-to-end learning. To enhance efficiency, we reformulate the backward pass as a simplified and decoupled quadratic programming problem by leveraging the structural properties of the KKT matrix. This reformulation enables the use of first-order optimization algorithms in calculating the backward pass gradients, allowing our framework to potentially utilize any state-of-the-art solver. As solver technologies evolve, BPQP can continuously adapt and improve its efficiency. Extensive experiments on both simulated and real-world datasets demonstrate that BPQP achieves a significant improvement in efficiency--typically an order of magnitude faster in overall execution time compared to other differentiable optimization layers. Our results not only highlight the efficiency gains of BPQP but also underscore its superiority over differentiable optimization layer baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19285v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-fin.PM</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianming Pan, Zeqi Ye, Xiao Yang, Xu Yang, Weiqing Liu, Lewen Wang, Jiang Bian</dc:creator>
    </item>
  </channel>
</rss>
