<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 14 Jan 2026 02:37:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Building Faculty Expertise Ontology using Protege: Enhancing Academic Library Research Services</title>
      <link>https://arxiv.org/abs/2601.07451</link>
      <description>arXiv:2601.07451v1 Announce Type: new 
Abstract: Academic libraries struggle to find and access faculty expertise across disciplines. This research proposes a faculty expertise ontology with a hierarchical structure based on Prot\'eg\'e to enhance library services and knowledge organisation. The ontology classifies relationships between departments, subject areas, faculty members, and contact data into layers including Top, Middle, and Bottom levels. The academic structure that this tiered form takes enables discovery of expertise in departments. The ontology which answers competency questions generated from the subject matter experts can answer real-world questions like which faculties are in the specific areas, how to collaborate with other disciplines and search contact information and so on. Competency questions act as design and test instruments to show that the ontology will fulfil the information needs of Researchers, Librarians and Administrators. The ontology is able to cope with semantically-enhanced queries, as shown by SPARQL implementations. The model works effectively in initiating referrals to an expert, aligning research with the strength of a department and allowing academics to partner up. The ontology delivers a scalable platform that adapts to institutional change. In the future, we intend to integrate with institutional databases and library systems for automatic API updates, as well as develop user interfaces and visualisations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07451v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Snehasish Paul</dc:creator>
    </item>
    <item>
      <title>The Issue with Special Issues: when Guest Editors Publish in Support of Self</title>
      <link>https://arxiv.org/abs/2601.07563</link>
      <description>arXiv:2601.07563v1 Announce Type: new 
Abstract: The recent exceptional growth in the number of special issues has led to the largest delegation of editorial power in the history of scientific publishing. Has this power been used responsibly? In this article we provide the first systematic analysis of a particular form of abuse of power by guest editors: endogeny, the practice of publishing articles in ones own special issue. While moderate levels of endogeny are common in special issues, excessive endogeny is a blatant case of scientific misconduct. We define special issues containing more than 33% endogeny as Published in Support of Self (PISS). We build a dataset of over 100,000 special issues published between 2015 and 2025 by five leading publishers. The large majority of guest editors engage in endogeny responsibly, if at all. Nonetheless, despite endogeny policies by publishers and indexers, PISS is comparable in magnitude to scientific fraud. All journals heavily relying on special issues host PISS, and more than 1,000 PISS special issues are published each year, hosting tens of thousands of endogenous articles. Extreme PISS abuses are rare, as the majority of PISS occurs at moderate levels of endogeny. Since the scientific literature is a common pool resource this is not good news, as it reflects a widespread normalisation of guest editor misconduct. Fortunately, PISS can be solved by setting easily enforceable commonsense policies. We provide the data and analyses needed for indexers and academic regulators to act.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07563v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paolo Crosetto, Pablo G\'omez Barreiro, Mark Austin Hanson</dc:creator>
    </item>
    <item>
      <title>Data-Driven Framework Development for Public Space Quality Assessment</title>
      <link>https://arxiv.org/abs/2601.06026</link>
      <description>arXiv:2601.06026v1 Announce Type: cross 
Abstract: Public space quality assessment lacks systematic methodologies that integrate factors across diverse spatial typologies while maintaining context-specific relevance. Current approaches remain fragmented within disciplinary boundaries, limiting comprehensive evaluation and comparative analysis across different space types. This study develops a systematic, data-driven framework for assessing public space quality through the algorithmic integration of empirical research findings. Using a 7-phase methodology, we transform 1,207 quality factors extracted from 157 peer-reviewed studies into a validated hierarchical taxonomy spanning six public space typologies: urban spaces, open spaces, green spaces, parks and waterfronts, streets and squares, and public facilities. The methodology combines semantic analysis, cross-typology distribution analysis, and domain knowledge integration to address terminological variations and functional relationships across space types. The resulting framework organizes 1,029 unique quality factors across 14 main categories and 66 subcategories, identifying 278 universal factors applicable across all space types, 397 space-specific factors unique to particular typologies, and 124 cross-cutting factors serving multiple functions. Framework validation demonstrates systematic consistency in factor organization and theoretical alignment with established research on public spaces. This research provides a systematic methodology for transforming empirical public space research into practical assessment frameworks, supporting evidence-based policy development, design quality evaluation, and comparative analysis across diverse urban contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06026v1</guid>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.IR</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sherzod Turaev, Mary John</dc:creator>
    </item>
    <item>
      <title>Loci Similes: A Benchmark for Extracting Intertextualities in Latin Literature</title>
      <link>https://arxiv.org/abs/2601.07533</link>
      <description>arXiv:2601.07533v1 Announce Type: cross 
Abstract: Tracing connections between historical texts is an important part of intertextual research, enabling scholars to reconstruct the virtual library of a writer and identify the sources influencing their creative process. These intertextual links manifest in diverse forms, ranging from direct verbatim quotations to subtle allusions and paraphrases disguised by morphological variation. Language models offer a promising path forward due to their capability of capturing semantic similarity beyond lexical overlap. However, the development of new methods for this task is held back by the scarcity of standardized benchmarks and easy-to-use datasets. We address this gap by introducing Loci Similes, a benchmark for Latin intertextuality detection comprising of a curated dataset of ~172k text segments containing 545 expert-verified parallels linking Late Antique authors to a corpus of classical authors. Using this data, we establish baselines for retrieval and classification of intertextualities with state-of-the-art LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07533v1</guid>
      <category>cs.IR</category>
      <category>cs.DL</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julian Schelb, Michael Wittweiler, Marie Revellio, Barbara Feichtinger, Andreas Spitz</dc:creator>
    </item>
    <item>
      <title>Review of Passenger Flow Modelling Approaches Based on a Bibliometric Analysis</title>
      <link>https://arxiv.org/abs/2511.13742</link>
      <description>arXiv:2511.13742v2 Announce Type: replace 
Abstract: This paper presents a bibliometric analysis of the field of short-term passenger flow forecasting within local public transit, covering 814 publications that span from 1984 to 2024. In addition to common bibliometric analysis tools, a variant of a citation network was developed, and topic modelling was conducted. The analysis reveals that research activity exhibited sporadic patterns prior to 2008, followed by a marked acceleration, characterised by a shift from conventional statistical and machine learning methodologies (e.g., ARIMA, SVM, and basic neural networks) to specialised deep learning architectures. Based on this insight, a connection to more general fields such as machine learning and time series modelling was established. In addition to modelling, spatial, linguistic, and modal biases were identified and findings from existing secondary literature were validated and quantified. This revealed existing gaps, such as constrained data fusion, open (multivariate) data, and underappreciated challenges related to model interpretability, cost-efficiency, and a balance between algorithmic performance and practical deployment considerations. In connection with the superordinate fields, the growth in relevance of foundation models is also noteworthy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13742v2</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan Hecht, Weilian Li, Ziyue Li, Youness Dehbi</dc:creator>
    </item>
    <item>
      <title>A Global Atlas of Digital Dermatology to Map Innovation and Disparities</title>
      <link>https://arxiv.org/abs/2601.00840</link>
      <description>arXiv:2601.00840v2 Announce Type: replace 
Abstract: The adoption of artificial intelligence in dermatology promises democratized access to healthcare, but model reliability depends on the quality and comprehensiveness of the data fueling these models. Despite rapid growth in publicly available dermatology images, the field lacks quantitative key performance indicators to measure whether new datasets expand clinical coverage or merely replicate what is already known. Here we present SkinMap, a multi-modal framework for the first comprehensive audit of the field's entire data basis. We unify the publicly available dermatology datasets into a single, queryable semantic atlas comprising more than 1.1 million images of skin conditions and quantify (i) informational novelty over time, (ii) dataset redundancy, and (iii) representation gaps across demographics and diagnoses. Despite exponential growth in dataset sizes, informational novelty across time has somewhat plateaued: Some clusters, such as common neoplasms on fair skin, are densely populated, while underrepresented skin types and many rare diseases remain unaddressed. We further identify structural gaps in coverage: Darker skin tones (Fitzpatrick V-VI) constitute only 5.8% of images and pediatric patients only 3.0%, while many rare diseases and phenotype combinations remain sparsely represented. SkinMap provides infrastructure to measure blind spots and steer strategic data acquisition toward undercovered regions of clinical space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00840v2</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabian Gr\"oger, Simone Lionetti, Philippe Gottfrois, Alvaro Gonzalez-Jimenez, Lea Habermacher, Labelling Consortium, Ludovic Amruthalingam, Matthew Groh, Marc Pouly, Alexander A. Navarini</dc:creator>
    </item>
    <item>
      <title>LongDA: Benchmarking LLM Agents for Long-Document Data Analysis</title>
      <link>https://arxiv.org/abs/2601.02598</link>
      <description>arXiv:2601.02598v2 Announce Type: replace 
Abstract: We introduce LongDA, a data analysis benchmark for evaluating LLM-based agents under documentation-intensive analytical workflows. In contrast to existing benchmarks that assume well-specified schemas and inputs, LongDA targets real-world settings in which navigating long documentation and complex data is the primary bottleneck. To this end, we manually curate raw data files, long and heterogeneous documentation, and expert-written publications from 17 publicly available U.S. national surveys, from which we extract 505 analytical queries grounded in real analytical practice. Solving these queries requires agents to first retrieve and integrate key information from multiple unstructured documents, before performing multi-step computations and writing executable code, which remains challenging for existing data analysis agents. To support the systematic evaluation under this setting, we develop LongTA, a tool-augmented agent framework that enables document access, retrieval, and code execution, and evaluate a range of proprietary and open-source models. Our experiments reveal substantial performance gaps even among state-of-the-art models, highlighting the challenges researchers should consider before applying LLM agents for decision support in real-world, high-stakes analytical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02598v2</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiyang Li, Zheyuan Zhang, Tianyi Ma, Zehong Wang, Keerthiram Murugesan, Chuxu Zhang, Yanfang Ye</dc:creator>
    </item>
    <item>
      <title>Examining persistence of European open repository infrastructure and its diffusion in the scholarly record</title>
      <link>https://arxiv.org/abs/2601.04015</link>
      <description>arXiv:2601.04015v2 Announce Type: replace 
Abstract: This article seeks to determine the extent to which the principle of persistence is observed by repositories and the organizations that operate them. We also evaluate the impact that negative repository persistence levels may be having on the scholarly record. We do this by interrogating and combining data about European repositories from several repository registries and web scraped sources, including the Internet Archive's Wayback Machine, thereby creating a unique dataset of historic repository locations and their OAI-PMH endpoints. We then use this data as the basis for text mining CORE, a vast corpus of scholarly outputs, to determine the extent to which impersistent European repository content has permeated the scholarly literature. Our findings indicate over a fifth of European repositories (&gt; 20%) could be classified as 'dead', with an even greater proportion (&gt; 40%) of the machine interfaces associated with these repositories similarly dead. Problematically, our analysis indicates that circa 12,000 unique scholarly works cite, refer to, or actively used this repository content, amounting to circa 19,000 unique repository locations, all of which are now unretrievable from their stated resource location. Partly owing to limitations in available repository registry data and the existence of 'zombie' repositories, there are reasons to conclude that the total number of scholarly works referring to dead repository content is far higher. We also find evidence of dead repository content entering the current scholarly record, a phenomenon we describe as 'dead on arrival' referencing. We consider the implications of these observations, proffer explanations, and propose possible policy interventions to address the issue of repository persistence. Our dataset also enables us to make several observations about the nature of impersistent repositories, their profile, and their decay rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04015v2</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Macgregor, Joy Davidson</dc:creator>
    </item>
    <item>
      <title>KARMA: Leveraging Multi-Agent LLMs for Automated Knowledge Graph Enrichment</title>
      <link>https://arxiv.org/abs/2502.06472</link>
      <description>arXiv:2502.06472v2 Announce Type: replace-cross 
Abstract: Maintaining comprehensive and up-to-date knowledge graphs (KGs) is critical for modern AI systems, but manual curation struggles to scale with the rapid growth of scientific literature. This paper presents KARMA, a novel framework employing multi-agent large language models (LLMs) to automate KG enrichment through structured analysis of unstructured text. Our approach employs nine collaborative agents, spanning entity discovery, relation extraction, schema alignment, and conflict resolution that iteratively parse documents, verify extracted knowledge, and integrate it into existing graph structures while adhering to domain-specific schema. Experiments on 1,200 PubMed articles from three different domains demonstrate the effectiveness of KARMA in knowledge graph enrichment, with the identification of up to 38,230 new entities while achieving 83.1\% LLM-verified correctness and reducing conflict edges by 18.6\% through multi-layer assessments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06472v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.DL</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Spotlight paper of NeurIPS 2025</arxiv:journal_reference>
      <dc:creator>Yuxing Lu, Wei Wu, Xukai Zhao, Rui Peng, Jinzhuo Wang</dc:creator>
    </item>
  </channel>
</rss>
