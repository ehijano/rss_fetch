<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 11 Aug 2025 04:04:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Physics-Augmented Machine Learning Constitutive Model for Damage in Solids</title>
      <link>https://arxiv.org/abs/2508.05638</link>
      <description>arXiv:2508.05638v1 Announce Type: cross 
Abstract: We propose a data-driven constitutive framework for anisotropic damage mechanics based on the second-order damage tensor approach for both compressible and incompressible materials. The formulation is thermodynamically consistent and satisfies the Clausius-Duhem inequality. The strain energy density potentials are expressed as isotropic functions of the right Cauchy-Green deformation tensor, along with structural tensors that encode anisotropy either present in the virgin material or resulting from damage. To guarantee the polyconvexity condition, non-decreasing convex neural networks with inputs that ensure polyconvexity are used to parameterize the strain energy density potentials. The model vanishes in the undeformed state, fulfilling the normality condition. In contrast to classical [1-d] damage models, the expressiveness of the new data-driven model is enhanced by employing a family of nonlinear, convex, decreasing functions to capture the effect of damage. Damage evolution is governed through a damage potential, where the corresponding threshold is defined in terms of the damage conjugate forces. As a special case of the general formulation, a new anisotropic generic format is introduced to predict constitutive responses under damage-induced anisotropy in initially isotropic materials. To reduce the computational burden during training, a decoupled training scheme is introduced, and its accuracy is demonstrated in all numerical examples. These include benchmarks for incompressible isotropic, transversely isotropic, and compressible orthotropic materials. The framework is also validated against experimental data capturing anisotropic Mullins-type damage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05638v1</guid>
      <category>physics.app-ph</category>
      <category>physics.data-an</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amirhossein Amiri-Hezaveh, Adrian Buganza Tepole</dc:creator>
    </item>
    <item>
      <title>A Graph Neural Network Approach for Mapping the Conceptual Structure and Inter-Branch Connectivity of Physics</title>
      <link>https://arxiv.org/abs/2508.05724</link>
      <description>arXiv:2508.05724v1 Announce Type: cross 
Abstract: This work introduces a novel framework for representing and analyzing physical laws as a weighted knowledge graph. We constructed a database of 659 distinct physical equations, subjected to rigorous semantic cleaning to resolve notational ambiguities, resulting in a corpus of 400 advanced physics equations. We developed an enhanced graph representation where both physical concepts and equations are nodes, connected by weighted inter-equation bridges. These weights are objectively defined using normalized metrics for variable overlap, physics-informed importance scores, and bibliometric data. A Graph Attention Network (GAT) was trained for link prediction, achieving a test AUC of 0.9742 +/- 0.0018 across five independent runs, significantly outperforming both classical heuristics (best baseline AUC: 0.9487) and established GNN architectures like GraphSAGE (AUC: 0.9504, p = 0.029). Statistical testing confirmed significance of all comparisons (p &lt; 0.05), with 2.7% improvement over the best baseline. Our analysis reveals three key findings: (i) The model autonomously rediscovers the known macroscopic structure of physics, identifying strong conceptual axes between Electromagnetism and Statistical Mechanics. (ii) It identifies central hub equations that serve as critical bridges between multiple physical domains. (iii) The model generates stable, computationally-derived hypotheses for cross-domain relationships, identifying both known principles and suggesting novel mathematical analogies for further theoretical investigation. The framework can generate hundreds of such hypotheses, enabling the creation of specialized datasets for targeted analysis of specific physics subfields. Code and data available at https://github.com/kingelanci/graphysics</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05724v1</guid>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Massimiliano Romiti</dc:creator>
    </item>
    <item>
      <title>Cosmic-ray tomography of shipping containers: A combination of complementary secondary particle and muon information using simulations</title>
      <link>https://arxiv.org/abs/2508.06128</link>
      <description>arXiv:2508.06128v1 Announce Type: cross 
Abstract: Cosmic-ray tomography usually relies on measuring the scattering or transmission of muons produced within cosmic-ray air showers to reconstruct an examined volume of interest (VOI). During the traversing of a VOI, all air shower particles, including muons, interact with the matter within the VOI producing so-called secondary particles. The characteristics of the production of these particles contain additional information about the properties of the examined objects and their materials. However, this approach has not been fully realized practically. Hence, this work aims to study a novel technique to scan shipping containers by comparing and combining the complementary results from stand-alone secondary particles and muon scattering using simulated simplified scenes with a 1 m3 cube made out of five different materials located inside the container. The proposed approach for a statistical combination is based on a multi-step procedure centered around a clustering and segmentation algorithm. This ensures a consistent evaluation and comparison of the results before and after the combination focusing on dedicated properties of the reconstructed object. The findings of this work show a potential improvement over the results obtained solely through muon scattering due to the utilization of secondary particle information by applying this novel dual-channel cosmic-ray tomography analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06128v1</guid>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Maximilian P\'erez Prada, Angel Bueno Rodr\'iguez, Maurice Stephan, Sarah Barnes</dc:creator>
    </item>
    <item>
      <title>Comparative study of ensemble-based uncertainty quantification methods for neural network interatomic potentials</title>
      <link>https://arxiv.org/abs/2508.06456</link>
      <description>arXiv:2508.06456v1 Announce Type: cross 
Abstract: Machine learning interatomic potentials (MLIPs) enable atomistic simulations with near first-principles accuracy at substantially reduced computational cost, making them powerful tools for large-scale materials modeling. The accuracy of MLIPs is typically validated on a held-out dataset of \emph{ab initio} energies and atomic forces. However, accuracy on these small-scale properties does not guarantee reliability for emergent, system-level behavior -- precisely the regime where atomistic simulations are most needed, but for which direct validation is often computationally prohibitive. As a practical heuristic, predictive precision -- quantified as inverse uncertainty -- is commonly used as a proxy for accuracy, but its reliability remains poorly understood, particularly for system-level predictions. In this work, we systematically assess the relationship between predictive precision and accuracy in both in-distribution (ID) and out-of-distribution (OOD) regimes, focusing on ensemble-based uncertainty quantification methods for neural network potentials, including bootstrap, dropout, random initialization, and snapshot ensembles. We use held-out cross-validation for ID assessment and calculate cold curve energies and phonon dispersion relations for OOD testing. These evaluations are performed across various carbon allotropes as representative test systems. We find that uncertainty estimates can behave counterintuitively in OOD settings, often plateauing or even decreasing as predictive errors grow. These results highlight fundamental limitations of current uncertainty quantification approaches and underscore the need for caution when using predictive precision as a stand-in for accuracy in large-scale, extrapolative applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06456v1</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>physics.data-an</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yonatan Kurniawan (Department of Physics and Astronomy, Brigham Young University, Provo, Utah, USA), Mingjian Wen (Institute of Fundamental and Frontier Sciences, University of Electronic Science and Technology of China, Chengdu, China), Ellad B. Tadmor (Department of Aerospace Engineering and Mechanics, University of Minnesota, Minneapolis, Minnesota, USA), Mark K. Transtrum (Department of Physics and Astronomy, Brigham Young University, Provo, Utah, USA)</dc:creator>
    </item>
    <item>
      <title>Converting sWeights to Probabilities with Density Ratios</title>
      <link>https://arxiv.org/abs/2409.08183</link>
      <description>arXiv:2409.08183v2 Announce Type: replace 
Abstract: The use of machine learning approaches continues to have many benefits in experimental nuclear and particle physics. One common issue is generating training data which is sufficiently realistic to give reliable results. Here we advocate using real experimental data as the source of training data and demonstrate how one might subtract background contributions through the use of probabilistic weights which can be readily applied to training data. The sPlot formalism is a common tool used to isolate distributions from different sources. However, the negative sWeights produced by the sPlot technique can cause training problems and poor predictive power. This article demonstrates how density ratio estimation can be applied to convert sWeights to event probabilities, which we call drWeights. The drWeights can then be applied to produce the distributions of interest and are consistent with direct use of the sWeights. This article will also show how decision trees are particularly well suited to convert sWeights, with the benefit of fast prediction rates and adaptability to aspects of experimental data such as the data sample size and proportions of different event sources. We also show that a density ratio product approach in which the initial drWeights are reweighted by an additional converter gives substantially better results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08183v2</guid>
      <category>physics.data-an</category>
      <category>hep-ex</category>
      <category>nucl-ex</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>D. I. Glazier, R. Tyson</dc:creator>
    </item>
    <item>
      <title>Description length of canonical and microcanonical models</title>
      <link>https://arxiv.org/abs/2307.05645</link>
      <description>arXiv:2307.05645v4 Announce Type: replace-cross 
Abstract: The (non-)equivalence of canonical and microcanonical ensembles is a fundamental question in statistical physics, concerning whether the use of soft and hard constraints in the maximum-entropy construction leads to the same description of a system. Despite the fact that maximum-entropy models are also commonly used in statistical inference, pattern detection, and hypothesis testing, a complete understanding of the effects of ensemble non-equivalence on statistical modeling is still missing. Here, we study this problem from a rigorous model selection perspective by comparing canonical and microcanonical models via the Minimum Description Length (MDL) principle, which yields a trade-off between likelihood, measuring model accuracy, and complexity, measuring model flexibility and its potential to overfit data. We compute the Normalized Maximum Likelihood (NML) of both formulations and find that: (i) microcanonical models always achieve higher likelihood but are always more complex; (ii) the optimal model choice depends on the empirical values of the constraints -- the canonical model performs best when its fit to the observed data exceeds its uniform average fit across all realizations; (iii) in the thermodynamic limit, the difference in description length per node vanishes when ensemble equivalence holds but persists otherwise, showing that non-equivalence implies extensive differences between large canonical and microcanonical models. Finally, we compare the NML approach to Bayesian methods, showing that (iv) the choice of priors, practically irrelevant in equivalent models, becomes crucial when an extensive number of constraints is enforced, possibly leading to very different outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.05645v4</guid>
      <category>cond-mat.stat-mech</category>
      <category>physics.data-an</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/1sd2-rxmy</arxiv:DOI>
      <dc:creator>Francesca Giuffrida, Tiziano Squartini, Peter Gr\"unwald, Diego Garlaschelli</dc:creator>
    </item>
  </channel>
</rss>
