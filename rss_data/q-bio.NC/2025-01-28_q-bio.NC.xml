<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 Jan 2025 05:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Localization of Seizure Onset Zone based on Spatio-Temporal Independent Component Analysis on fMRI</title>
      <link>https://arxiv.org/abs/2501.14782</link>
      <description>arXiv:2501.14782v1 Announce Type: new 
Abstract: Localizing the seizure onset zone (SOZ) as a step of presurgical planning leads to higher efficiency in surgical and stimulation treatments. However, the clinical localization including structural, ictal, and invasive data acquisition and assessment is a difficult and long procedure with increasing challenges in patients with complex epileptic foci. The interictal methods are proposed to assist in presurgical planning with simpler data acquisition and higher speed. This study presents a spatiotemporal component classification for the localization of epileptic foci using resting-state functional magnetic resonance imaging data. This method is based on spatiotemporal independent component analysis on rsfMRI with a component-sorting procedure upon dominant power frequency, biophysical constraints, spatial lateralization, local connectivity, temporal energy, and functional non-Gaussianity. This method utilized the rs-fMRI potential to reach a high spatial accuracy in localizing epileptic foci from interictal data while retaining the reliability of results for clinical usage. Thirteen patients with temporal lobe epilepsy who underwent surgical resection and had seizure-free surgical outcomes after a 12-month follow-up were included in this study. All patients had presurgical structural MRI and rsfMRI while postsurgical MRI images were available for ten. Based on the relationship between the localized foci and resection, the results were classified into three groups fully concordant, partially concordant, and discordant. These groups had the resulting cluster aligned with, in the same lobe with, and outside the lobe of the resection area, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14782v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seyyed Mostafa Sadjadi, Elias Ebrahimzadeh, Alireza Fallahi, Jafar Mehvari Habibabadi, Mohammad-Reza Nazem-Zadeh, Hamid Soltanian-Zadeh</dc:creator>
    </item>
    <item>
      <title>Towards Dynamic Neural Communication and Speech Neuroprosthesis Based on Viseme Decoding</title>
      <link>https://arxiv.org/abs/2501.14790</link>
      <description>arXiv:2501.14790v1 Announce Type: new 
Abstract: Decoding text, speech, or images from human neural signals holds promising potential both as neuroprosthesis for patients and as innovative communication tools for general users. Although neural signals contain various information on speech intentions, movements, and phonetic details, generating informative outputs from them remains challenging, with mostly focusing on decoding short intentions or producing fragmented outputs. In this study, we developed a diffusion model-based framework to decode visual speech intentions from speech-related non-invasive brain signals, to facilitate face-to-face neural communication. We designed an experiment to consolidate various phonemes to train visemes of each phoneme, aiming to learn the representation of corresponding lip formations from neural signals. By decoding visemes from both isolated trials and continuous sentences, we successfully reconstructed coherent lip movements, effectively bridging the gap between brain signals and dynamic visual interfaces. The results highlight the potential of viseme decoding and talking face reconstruction from human neural signals, marking a significant step toward dynamic neural communication systems and speech neuroprosthesis for patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14790v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ji-Ha Park, Seo-Hyun Lee, Soowon Kim, Seong-Whan Lee</dc:creator>
    </item>
    <item>
      <title>BOLDreams: Dreaming with pruned in-silico fMRI Encoding Models of the Visual Cortex</title>
      <link>https://arxiv.org/abs/2501.14854</link>
      <description>arXiv:2501.14854v1 Announce Type: new 
Abstract: In this article we use the Natural Scenes Dataset (NSD) to train a family of feature-weighted receptive field neural encoding models. These models use a pre-trained vision or text backbone and map extracted features to the voxel space via receptive field readouts. We comprehensively assess such models, quantifying performance changes based on using different modalities like text or images, toggling finetuning, using different pre-trained backbones, and changing the width of the readout. We also dissect each model using explainable AI (XAI) techniques, such as feature visualization via input optimization, also referred to as ``dreaming'' in the AI literature, and the integrated gradients approach to calculate implicit attention maps to illustrate which features drive the predicted signal in different brain areas. These XAI tools illustrate biologically plausible features that drive the predicted signal. Traversing the model hyperparameter space reveals the existence of a maximally minimal model, balancing simplicity while maintaining performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14854v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Uzair Hussain, Kamil Uludag</dc:creator>
    </item>
    <item>
      <title>Physiologically-Informed Predictability of a Teammate's Future Actions Forecasts Team Performance</title>
      <link>https://arxiv.org/abs/2501.15328</link>
      <description>arXiv:2501.15328v1 Announce Type: new 
Abstract: In collaborative environments, a deep understanding of multi-human teaming dynamics is essential for optimizing performance. However, the relationship between individuals' behavioral and physiological markers and their combined influence on overall team performance remains poorly understood. To explore this, we designed a triadic human collaborative sensorimotor task in virtual reality (VR) and introduced a novel predictability metric to examine team dynamics and performance. Our findings reveal a strong connection between team performance and the predictability of a team member's future actions based on other team members' behavioral and physiological data. Contrary to conventional wisdom that high-performing teams are highly synchronized, our results suggest that physiological and behavioral synchronizations among team members have a limited correlation with team performance. These insights provide a new quantitative framework for understanding multi-human teaming, paving the way for deeper insights into team dynamics and performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15328v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yinuo Qin, Richard T. Lee, Weijia Zhang, Xiaoxiao Sun, Paul Sajda</dc:creator>
    </item>
    <item>
      <title>On Design Choices in Similarity-Preserving Sparse Randomized Embeddings</title>
      <link>https://arxiv.org/abs/2501.14741</link>
      <description>arXiv:2501.14741v1 Announce Type: cross 
Abstract: Expand &amp; Sparsify is a principle that is observed in anatomically similar neural circuits found in the mushroom body (insects) and the cerebellum (mammals). Sensory data are projected randomly to much higher-dimensionality (expand part) where only few the most strongly excited neurons are activated (sparsify part). This principle has been leveraged to design a FlyHash algorithm that forms similarity-preserving sparse embeddings, which have been found useful for such tasks as novelty detection, pattern recognition, and similarity search. Despite its simplicity, FlyHash has a number of design choices to be set such as preprocessing of the input data, choice of sparsifying activation function, and formation of the random projection matrix. In this paper, we explore the effect of these choices on the performance of similarity search with FlyHash embeddings. We find that the right combination of design choices can lead to drastic difference in the search performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14741v1</guid>
      <category>cs.NE</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/IJCNN60899.2024.10651277</arxiv:DOI>
      <arxiv:journal_reference>2024 International Joint Conference on Neural Networks (IJCNN)</arxiv:journal_reference>
      <dc:creator>Denis Kleyko, Dmitri A. Rachkovskij</dc:creator>
    </item>
    <item>
      <title>What if Eye...? Computationally Recreating Vision Evolution</title>
      <link>https://arxiv.org/abs/2501.15001</link>
      <description>arXiv:2501.15001v1 Announce Type: cross 
Abstract: Vision systems in nature show remarkable diversity, from simple light-sensitive patches to complex camera eyes with lenses. While natural selection has produced these eyes through countless mutations over millions of years, they represent just one set of realized evolutionary paths. Testing hypotheses about how environmental pressures shaped eye evolution remains challenging since we cannot experimentally isolate individual factors. Computational evolution offers a way to systematically explore alternative trajectories. Here we show how environmental demands drive three fundamental aspects of visual evolution through an artificial evolution framework that co-evolves both physical eye structure and neural processing in embodied agents. First, we demonstrate computational evidence that task specific selection drives bifurcation in eye evolution - orientation tasks like navigation in a maze leads to distributed compound-type eyes while an object discrimination task leads to the emergence of high-acuity camera-type eyes. Second, we reveal how optical innovations like lenses naturally emerge to resolve fundamental tradeoffs between light collection and spatial precision. Third, we uncover systematic scaling laws between visual acuity and neural processing, showing how task complexity drives coordinated evolution of sensory and computational capabilities. Our work introduces a novel paradigm that illuminates evolutionary principles shaping vision by creating targeted single-player games where embodied agents must simultaneously evolve visual systems and learn complex behaviors. Through our unified genetic encoding framework, these embodied agents serve as next-generation hypothesis testing machines while providing a foundation for designing manufacturable bio-inspired vision systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15001v1</guid>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kushagra Tiwary, Aaron Young, Zaid Tasneem, Tzofi Klinghoffer, Akshat Dave, Tomaso Poggio, Dan Nilsson, Brian Cheung, Ramesh Raskar</dc:creator>
    </item>
    <item>
      <title>A New Approach for Knowledge Generation Using Active Inference</title>
      <link>https://arxiv.org/abs/2501.15105</link>
      <description>arXiv:2501.15105v1 Announce Type: cross 
Abstract: There are various models proposed on how knowledge is generated in the human brain including the semantic networks model. Although this model has been widely studied and even computational models are presented, but, due to various limits and inefficiencies in the generation of different types of knowledge, its application is limited to semantic knowledge because of has been formed according to semantic memory and declarative knowledge and has many limits in explaining various procedural and conditional knowledge. Given the importance of providing an appropriate model for knowledge generation, especially in the areas of improving human cognitive functions or building intelligent machines, improving existing models in knowledge generation or providing more comprehensive models is of great importance. In the current study, based on the free energy principle of the brain, is the researchers proposed a model for generating three types of declarative, procedural, and conditional knowledge. While explaining different types of knowledge, this model is capable to compute and generate concepts from stimuli based on probabilistic mathematics and the action-perception process (active inference). The proposed model is unsupervised learning that can update itself using a combination of different stimuli as a generative model can generate new concepts of unsupervised received stimuli. In this model, the active inference process is used in the generation of procedural and conditional knowledge and the perception process is used to generate declarative knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15105v1</guid>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jamshid Ghasimi, Nazanin Movarraei</dc:creator>
    </item>
    <item>
      <title>Scaling laws for decoding images from brain activity</title>
      <link>https://arxiv.org/abs/2501.15322</link>
      <description>arXiv:2501.15322v1 Announce Type: cross 
Abstract: Generative AI has recently propelled the decoding of images from brain activity. How do these approaches scale with the amount and type of neural recordings? Here, we systematically compare image decoding from four types of non-invasive devices: electroencephalography (EEG), magnetoencephalography (MEG), high-field functional Magnetic Resonance Imaging (3T fMRI) and ultra-high field (7T) fMRI. For this, we evaluate decoding models on the largest benchmark to date, encompassing 8 public datasets, 84 volunteers, 498 hours of brain recording and 2.3 million brain responses to natural images. Unlike previous work, we focus on single-trial decoding performance to simulate real-time settings. This systematic comparison reveals three main findings. First, the most precise neuroimaging devices tend to yield the best decoding performances, when the size of the training sets are similar. However, the gain enabled by deep learning - in comparison to linear models - is obtained with the noisiest devices. Second, we do not observe any plateau of decoding performance as the amount of training data increases. Rather, decoding performance scales log-linearly with the amount of brain recording. Third, this scaling law primarily depends on the amount of data per subject. However, little decoding gain is observed by increasing the number of subjects. Overall, these findings delineate the path most suitable to scale the decoding of images from non-invasive brain recordings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15322v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hubert Banville, Yohann Benchetrit, St\'ephane d'Ascoli, J\'er\'emy Rapin amd Jean-R\'emi King</dc:creator>
    </item>
    <item>
      <title>Perception of an AI Teammate in an Embodied Control Task Affects Team Performance, Reflected in Human Teammates' Behaviors and Physiological Responses</title>
      <link>https://arxiv.org/abs/2501.15332</link>
      <description>arXiv:2501.15332v1 Announce Type: cross 
Abstract: The integration of artificial intelligence (AI) into human teams is widely expected to enhance performance and collaboration. However, our study reveals a striking and counterintuitive result: human-AI teams performed worse than human-only teams, especially when task difficulty increased. Using a virtual reality-based sensorimotor task, we observed that the inclusion of an active human-like AI teammate disrupted team dynamics, leading to elevated arousal, reduced engagement, and diminished communication intensity among human participants. These effects persisted even as the human teammates' perception of the AI teammate improved over time. These findings challenge prevailing assumptions about the benefits of AI in team settings and highlight the critical need for human-centered AI design to mitigate adverse physiological and behavioral impacts, ensuring more effective human-AI collaboration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15332v1</guid>
      <category>cs.HC</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yinuo Qin, Richard T. Lee, Paul Sajda</dc:creator>
    </item>
    <item>
      <title>Efficient Distillation of Deep Spiking Neural Networks for Full-Range Timestep Deployment</title>
      <link>https://arxiv.org/abs/2501.15925</link>
      <description>arXiv:2501.15925v1 Announce Type: cross 
Abstract: Spiking Neural Networks (SNNs) are emerging as a brain-inspired alternative to traditional Artificial Neural Networks (ANNs), prized for their potential energy efficiency on neuromorphic hardware. Despite this, SNNs often suffer from accuracy degradation compared to ANNs and face deployment challenges due to fixed inference timesteps, which require retraining for adjustments, limiting operational flexibility. To address these issues, our work considers the spatio-temporal property inherent in SNNs, and proposes a novel distillation framework for deep SNNs that optimizes performance across full-range timesteps without specific retraining, enhancing both efficacy and deployment adaptability. We provide both theoretical analysis and empirical validations to illustrate that training guarantees the convergence of all implicit models across full-range timesteps. Experimental results on CIFAR-10, CIFAR-100, CIFAR10-DVS, and ImageNet demonstrate state-of-the-art performance among distillation-based SNNs training methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15925v1</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengting Yu, Xiaochen Zhao, Lei Liu, Shu Yang, Gaoang Wang, Erping Li, Aili Wang</dc:creator>
    </item>
    <item>
      <title>Inverse Reinforcement Learning via Convex Optimization</title>
      <link>https://arxiv.org/abs/2501.15957</link>
      <description>arXiv:2501.15957v1 Announce Type: cross 
Abstract: We consider the inverse reinforcement learning (IRL) problem, where an unknown reward function of some Markov decision process is estimated based on observed expert demonstrations. In most existing approaches, IRL is formulated and solved as a nonconvex optimization problem, posing challenges in scenarios where robustness and reproducibility are critical. We discuss a convex formulation of the IRL problem (CIRL) initially proposed by Ng and Russel, and reformulate the problem such that the domain-specific language CVXPY can be applied directly to specify and solve the convex problem. We also extend the CIRL problem to scenarios where the expert policy is not given analytically but by trajectory as state-action pairs, which can be strongly inconsistent with optimality, by augmenting some of the constraints. Theoretical analysis and practical implementation for hyperparameter auto-selection are introduced. This note helps the users to easily apply CIRL for their problems, without background knowledge on convex optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15957v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>math.OC</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Zhu, Yuan Zhang, Joschka Boedecker</dc:creator>
    </item>
    <item>
      <title>Why Do Colors Appear in Benham's Top?</title>
      <link>https://arxiv.org/abs/2303.04624</link>
      <description>arXiv:2303.04624v4 Announce Type: replace 
Abstract: In 2023, I published a paper titled "A Dynamic Interference Model for Benham's Top." Here, I would like to concisely explain the most important aspect of dynamic interference by organizing and integrating the figures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.04624v4</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yutaka Nishiyama</dc:creator>
    </item>
    <item>
      <title>When A Man Says He Is Pregnant: ERP Evidence for A Rational Account of Speaker-contextualized Language Comprehension</title>
      <link>https://arxiv.org/abs/2409.17525</link>
      <description>arXiv:2409.17525v2 Announce Type: replace 
Abstract: Spoken language is often, if not always, understood in a context formed by the identity of the speaker. For example, we can easily make sense of an utterance such as "I'm going to have a manicure this weekend" or "The first time I got pregnant I had a hard time" when spoken by a woman, but it would be harder to understand when it is spoken by a man. Previous event-related potential (ERP) studies have shown mixed results regarding the neurophysiological responses to such speaker-content mismatches, with some reporting an N400 effect and others a P600 effect. In an electroencephalography (EEG) experiment involving 64 participants, we used social and biological mismatches as test cases to demonstrate how these distinct ERP patterns reflect different aspects of rational inference. We showed that when the mismatch involves social stereotypes (e.g., men getting a manicure), listeners can arrive at a "literal" interpretation by integrating the content with their social knowledge, though this integration requires additional effort due to stereotype violations-resulting in an N400 effect. In contrast, when the mismatch involves biological knowledge (e.g., men getting pregnant), a "literal" interpretation becomes impossible, leading listeners to treat the input as potentially containing errors and engage in correction processes-resulting in a P600 effect. Supporting this rational inference framework, we found that the social N400 effect decreased as a function of the listener's personality trait of openness (as more open-minded individuals maintain more flexible social expectations), while the biological P600 effect remained robust (as biological constraints are recognized regardless of individual personalities). Our findings help to reconcile the empirical inconsistencies and show how rational inference shapes speaker-contextualized language comprehension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17525v2</guid>
      <category>q-bio.NC</category>
      <category>cs.CL</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanlin Wu, Zhenguang G. Cai</dc:creator>
    </item>
    <item>
      <title>Accessing the topological properties of human brain functional sub-circuits in Echo State Networks</title>
      <link>https://arxiv.org/abs/2412.14999</link>
      <description>arXiv:2412.14999v2 Announce Type: replace 
Abstract: Recent years have witnessed an emerging trend in neuromorphic computing that centers around the use of brain connectomics as a blueprint for artificial neural networks. Connectomics-based neuromorphic computing has primarily focused on embedding human brain large-scale structural connectomes (SCs), as estimated from diffusion Magnetic Resonance Imaging (dMRI) modality, to echo-state networks (ESNs). A critical step in ESN embedding requires pre-determined read-in and read-out layers constructed by the induced subgraphs of the embedded reservoir. As \textit{a priori} set of functional sub-circuits are derived from functional MRI (fMRI) modality, it is unknown, till this point, whether the embedding of fMRI-induced sub-circuits/networks onto SCs is well justified from the neuro-physiological perspective and ESN performance across a variety of tasks. This paper proposes a pipeline to implement and evaluate ESNs with various embedded topologies and processing/memorization tasks. To this end, we showed that different performance optimums highly depend on the neuro-physiological characteristics of these pre-determined fMRI-induced sub-circuits. In general, fMRI-induced sub-circuit-embedded ESN outperforms simple bipartite and various null models with feed-forward properties commonly seen in MLP for different tasks and reservoir criticality conditions. We provided a thorough analysis of the topological properties of pre-determined fMRI-induced sub-circuits and highlighted their graph-theoretical properties that play significant roles in determining ESN performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14999v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bach Nguyen, Tianlong Chen, Shu Yang, Bojian Hou, Li Shen, Duy Duong-Tran</dc:creator>
    </item>
    <item>
      <title>A Mathematical Framework for the Problem of Security for Cognition in Neurotechnology</title>
      <link>https://arxiv.org/abs/2403.07945</link>
      <description>arXiv:2403.07945v4 Announce Type: replace-cross 
Abstract: The rapid advancement in neurotechnology in recent years has created an emerging critical intersection between neurotechnology and security. Implantable devices, non-invasive monitoring, and non-invasive therapies all carry with them the prospect of violating the privacy and autonomy of individuals' cognition. A growing number of scientists and physicians have made calls to address this issue, but applied efforts have been relatively limited. A major barrier hampering scientific and engineering efforts to address these security issues is the lack of a clear means of describing and analyzing relevant problems. In this paper we develop Cognitive Neurosecurity, a mathematical framework which enables such description and analysis by drawing on methods and results from multiple fields. We demonstrate certain statistical properties which have significant implications for Cognitive Neurosecurity, and then present descriptions of the algorithmic problems faced by attackers attempting to violate privacy and autonomy, and defenders attempting to obstruct such attempts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07945v4</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bryce Allen Bagley, Claudia K Petritsch</dc:creator>
    </item>
    <item>
      <title>Order parameters and phase transitions of continual learning in deep neural networks</title>
      <link>https://arxiv.org/abs/2407.10315</link>
      <description>arXiv:2407.10315v2 Announce Type: replace-cross 
Abstract: Continual learning (CL) enables animals to learn new tasks without erasing prior knowledge. CL in artificial neural networks (NNs) is challenging due to catastrophic forgetting, where new learning degrades performance on older tasks. While various techniques exist to mitigate forgetting, theoretical insights into when and why CL fails in NNs are lacking. Here, we present a statistical-mechanics theory of CL in deep, wide NNs, which characterizes the network's input-output mapping as it learns a sequence of tasks. It gives rise to order parameters (OPs) that capture how task relations and network architecture influence forgetting and anterograde interference, as verified by numerical evaluations. For networks with a shared readout for all tasks (single-head CL), the relevant-feature and rule similarity between tasks, respectively measured by two OPs, are sufficient to predict a wide range of CL behaviors. In addition, the theory predicts that increasing the network depth can effectively reduce interference between tasks, thereby lowering forgetting. For networks with task-specific readouts (multi-head CL), the theory identifies a phase transition where CL performance shifts dramatically as tasks become less similar, as measured by another task-similarity OP. While forgetting is relatively mild compared to single-head CL across all tasks, sufficiently low similarity leads to catastrophic anterograde interference, where the network retains old tasks perfectly but completely fails to generalize new learning. Our results delineate important factors affecting CL performance and suggest strategies for mitigating forgetting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10315v2</guid>
      <category>cs.LG</category>
      <category>physics.app-ph</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haozhe Shan, Qianyi Li, Haim Sompolinsky</dc:creator>
    </item>
    <item>
      <title>Domain Adaptation-Enhanced Searchlight: Enabling classification of brain states from visual perception to mental imagery</title>
      <link>https://arxiv.org/abs/2408.01163</link>
      <description>arXiv:2408.01163v3 Announce Type: replace-cross 
Abstract: In cognitive neuroscience and brain-computer interface research, accurately predicting imagined stimuli is crucial. This study investigates the effectiveness of Domain Adaptation (DA) in enhancing imagery prediction using primarily visual data from fMRI scans of 18 subjects. Initially, we train a baseline model on visual stimuli to predict imagined stimuli, utilizing data from 14 brain regions. We then develop several models to improve imagery prediction, comparing different DA methods. Our results demonstrate that DA significantly enhances imagery prediction in binary classification on our dataset, as well as in multiclass classification on a publicly available dataset. We then conduct a DA-enhanced searchlight analysis, followed by permutation-based statistical tests to identify brain regions where imagery decoding is consistently above chance across subjects. Our DA-enhanced searchlight predicts imagery contents in a highly distributed set of brain regions, including the visual cortex and the frontoparietal cortex, thereby outperforming standard cross-domain classification methods. The complete code and data for this paper have been made openly available for the use of the scientific community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01163v3</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alexander Olza, David Soto, Roberto Santana</dc:creator>
    </item>
    <item>
      <title>Plasma-CycleGAN: Plasma Biomarker-Guided MRI to PET Cross-modality Translation Using Conditional CycleGAN</title>
      <link>https://arxiv.org/abs/2501.02146</link>
      <description>arXiv:2501.02146v2 Announce Type: replace-cross 
Abstract: Cross-modality translation between MRI and PET imaging is challenging due to the distinct mechanisms underlying these modalities. Blood-based biomarkers (BBBMs) are revolutionizing Alzheimer's disease (AD) detection by identifying patients and quantifying brain amyloid levels. However, the potential of BBBMs to enhance PET image synthesis remains unexplored. In this paper, we performed a thorough study on the effect of incorporating BBBM into deep generative models. By evaluating three widely used cross-modality translation models, we found that BBBMs integration consistently enhances the generative quality across all models. By visual inspection of the generated results, we observed that PET images generated by CycleGAN exhibit the best visual fidelity. Based on these findings, we propose Plasma-CycleGAN, a novel generative model based on CycleGAN, to synthesize PET images from MRI using BBBMs as conditions. This is the first approach to integrate BBBMs in conditional cross-modality translation between MRI and PET.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02146v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanxi Chen, Yi Su, Celine Dumitrascu, Kewei Chen, David Weidman, Richard J Caselli, Nicholas Ashton, Eric M Reiman, Yalin Wang</dc:creator>
    </item>
  </channel>
</rss>
