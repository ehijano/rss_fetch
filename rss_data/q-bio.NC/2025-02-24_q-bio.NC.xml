<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 24 Feb 2025 05:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Applications of Random Matrix Theory in Machine Learning and Brain Mapping</title>
      <link>https://arxiv.org/abs/2502.14878</link>
      <description>arXiv:2502.14878v1 Announce Type: new 
Abstract: Brain mapping analyzes the wavelengths of brain signals and outputs them in a map, which is then analyzed by a radiologist. Introducing Machine Learning (ML) into the brain mapping process reduces the variable of human error in reading such maps and increases efficiency. A key area of interest is determining the correlation between the functional areas of the brain on a voxel (3-dimensional pixel) wise basis. This leads to determining how a brain is functioning and can be used to detect diseases, disabilities, and sicknesses. As such, random noise presents a challenge in consistently determining the actual signals from the scan. This paper discusses how an algorithm created by Random Matrix Theory (RMT) can be used as a tool for ML, as it detects the correlation of the functional areas of the brain. Random matrices are simulated to represent the voxel signal intensity strength for each time interval where a stimulus is presented in an fMRI scan. Using the Marchenko-Pastur law for Wishart Matrices, a result of RMT, it was found that no matter what type of noise was added to the random matrices, the observed eigenvalue distribution of the Wishart Matrices would converge to the theoretical distribution. This means that RMT is robust and has a high test-re-test reliability. These results further indicate that a strong correlation exists between the eigenvalues, and hence the functional regions of the brain. Any eigenvalue that differs significantly from those predicted from RMT may indicate the discovery of a new discrete brain network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14878v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Katrina Lawrence</dc:creator>
    </item>
    <item>
      <title>Estimating Neural Representation Alignment from Limited Inputs and Features</title>
      <link>https://arxiv.org/abs/2502.15104</link>
      <description>arXiv:2502.15104v1 Announce Type: new 
Abstract: In both artificial and biological systems, the centered kernel alignment (CKA) has become a widely used tool for quantifying neural representation similarity. While current CKA estimators typically correct for the effects of finite stimuli sampling, the effects of sampling a subset of neurons are overlooked, introducing notable bias in standard experimental scenarios. Here, we provide a theoretical analysis showing how this bias is affected by the representation geometry. We then introduce a novel estimator that corrects for both input and feature sampling. We use our method for evaluating both brain-to-brain and model-to-brain alignments and show that it delivers reliable comparisons even with very sparsely sampled neurons. We perform within-animal and across-animal comparisons on electrophysiological data from visual cortical areas V1, V4, and IT data, and use these as benchmarks to evaluate model-to-brain alignment. We also apply our method to reveal how object representations become progressively disentangled across layers in both biological and artificial systems. These findings underscore the importance of correcting feature-sampling biases in CKA and demonstrate that our bias-corrected estimator provides a more faithful measure of representation alignment. The improved estimates increase our understanding of how neural activity is structured across both biological and artificial systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15104v1</guid>
      <category>q-bio.NC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chanwoo Chun, Abdulkadir Canatar, SueYeon Chung, Daniel D. Lee</dc:creator>
    </item>
    <item>
      <title>State-space kinetic Ising model reveals task-dependent entropy flow in sparsely active nonequilibrium neuronal dynamics</title>
      <link>https://arxiv.org/abs/2502.15440</link>
      <description>arXiv:2502.15440v1 Announce Type: new 
Abstract: Neuronal ensemble activity, including coordinated and oscillatory patterns, exhibits hallmarks of nonequilibrium systems with time-asymmetric trajectories to maintain their organization. However, assessing time asymmetry from neuronal spiking activity remains challenging. The kinetic Ising model provides a framework for studying the causal, nonequilibrium dynamics in spiking recurrent neural networks. Recent theoretical advances in this model have enabled time-asymmetry estimation from large-scale steady-state data. Yet, neuronal activity often exhibits time-varying firing rates and coupling strengths, violating steady-state assumption. To overcome these limitations, we developed a state-space kinetic Ising model that accounts for non-stationary and nonequilibrium properties of neural systems. This approach incorporates a mean-field method for estimating time-varying entropy flow, a key measure for maintaining the system's organization by dissipation. Applying this method to mouse visual cortex data revealed greater variability in causal couplings during task engagement despite the reduced neuronal activity with increased sparsity. Moreover, higher-performing mice exhibited increased entropy flow in higher-firing neurons during task engagement, suggesting that stronger directed activity emerged in a fewer neurons within sparsely active populations. These findings underscore the model's utility in uncovering intricate asymmetric causal dynamics in neuronal ensembles and linking them to behavior through the thermodynamic underpinnings of neural computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15440v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ken Ishihara, Hideaki Shimazaki</dc:creator>
    </item>
    <item>
      <title>BAN: Neuroanatomical Aligning in Auditory Recognition between Artificial Neural Network and Human Cortex</title>
      <link>https://arxiv.org/abs/2502.15503</link>
      <description>arXiv:2502.15503v1 Announce Type: new 
Abstract: Drawing inspiration from neurosciences, artificial neural networks (ANNs) have evolved from shallow architectures to highly complex, deep structures, yielding exceptional performance in auditory recognition tasks. However, traditional ANNs often struggle to align with brain regions due to their excessive depth and lack of biologically realistic features, like recurrent connection. To address this, a brain-like auditory network (BAN) is introduced, which incorporates four neuroanatomically mapped areas and recurrent connection, guided by a novel metric called the brain-like auditory score (BAS). BAS serves as a benchmark for evaluating the similarity between BAN and human auditory recognition pathway. We further propose that specific areas in the cerebral cortex, mainly the middle and medial superior temporal (T2/T3) areas, correspond to the designed network structure, drawing parallels with the brain's auditory perception pathway. Our findings suggest that the neuroanatomical similarity in the cortex and auditory classification abilities of the ANN are well-aligned. In addition to delivering excellent performance on a music genre classification task, the BAN demonstrates a high BAS score. In conclusion, this study presents BAN as a recurrent, brain-inspired ANN, representing the first model that mirrors the cortical pathway of auditory recognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15503v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haidong Wang, Pengfei Xiao, Ao Liu, Jianhua Zhang, Qia Shan</dc:creator>
    </item>
    <item>
      <title>Sparks of cognitive flexibility: self-guided context inference for flexible stimulus-response mapping by attentional routing</title>
      <link>https://arxiv.org/abs/2502.15634</link>
      <description>arXiv:2502.15634v1 Announce Type: new 
Abstract: Flexible cognition demands discovering hidden rules to quickly adapt stimulus-response mappings. Standard neural networks struggle in tasks requiring rapid, context-driven remapping. Recently, Hummos (2023) introduced a fast-and-slow learning algorithm to mitigate this shortfall, but its scalability to complex, image-computable tasks was unclear. Here, we propose the Wisconsin Neural Network (WiNN), which expands on fast-and-slow learning for real-world tasks demanding flexible rule-based behavior. WiNN employs a pretrained convolutional neural network for vision, coupled with an adjustable "context state" that guides attention to relevant features. If WiNN produces an incorrect response, it first iteratively updates its context state to refocus attention on task-relevant cues, then performs minimal parameter updates to attention and readout layers. This strategy preserves generalizable representations in the sensory network, reducing catastrophic forgetting. We evaluate WiNN on an image-based extension of the Wisconsin Card Sorting Task, revealing several markers of cognitive flexibility: (i) WiNN autonomously infers underlying rules, (ii) requires fewer examples to do so than control models reliant on large-scale parameter updates, (iii) can perform context-based rule inference solely via context-state adjustments-further enhanced by slow updates of attention and readout parameters, and (iv) generalizes to unseen compositional rules through context-state inference alone. By blending fast context inference with targeted attentional guidance, WiNN achieves "sparks" of flexibility. This approach offers a path toward context-sensitive models that retain knowledge while rapidly adapting to complex, rule-based tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15634v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rowan Sommers, Sushrut Thorat, Daniel Anthes, Tim C. Kietzmann</dc:creator>
    </item>
    <item>
      <title>Graph-Based Deep Learning on Stereo EEG for Predicting Seizure Freedom in Epilepsy Patients</title>
      <link>https://arxiv.org/abs/2502.15198</link>
      <description>arXiv:2502.15198v1 Announce Type: cross 
Abstract: Predicting seizure freedom is essential for tailoring epilepsy treatment. But accurate prediction remains challenging with traditional methods, especially with diverse patient populations. This study developed a deep learning-based graph neural network (GNN) model to predict seizure freedom from stereo electroencephalography (sEEG) data in patients with refractory epilepsy. We utilized high-quality sEEG data from 15 pediatric patients to train a deep learning model that can accurately predict seizure freedom outcomes and advance understanding of brain connectivity at the seizure onset zone. Our model integrates local and global connectivity using graph convolutions with multi-scale attention mechanisms to capture connections between difficult-to-study regions such as the thalamus and motor regions. The model achieved an accuracy of 92.4% in binary class analysis, 86.6% in patient-wise analysis, and 81.4% in multi-class analysis. Node and edge-level feature analysis highlighted the anterior cingulate and frontal pole regions as key contributors to seizure freedom outcomes. The nodes identified by our model were also more likely to coincide with seizure onset zones. Our findings underscore the potential of new connectivity-based deep learning models such as GNNs for enhancing the prediction of seizure freedom, predicting seizure onset zones, connectivity analysis of the brain during seizure, as well as informing AI-assisted personalized epilepsy treatment planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15198v1</guid>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Artur Agaronyan, Syeda Abeera Amir, Nunthasiri Wittayanakorn, John Schreiber, Marius G. Linguraru, William Gaillard, Chima Oluigbo, Syed Muhammad Anwar</dc:creator>
    </item>
    <item>
      <title>A Dynamic Interference Model for Benham's Top</title>
      <link>https://arxiv.org/abs/2303.04624</link>
      <description>arXiv:2303.04624v5 Announce Type: replace 
Abstract: In 2023, I published a paper titled "A Dynamic Interference Model for Benham's Top." Here, I would like to concisely explain the most important aspect of dynamic interference by organizing and integrating the figures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.04624v5</guid>
      <category>q-bio.NC</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yutaka Nishiyama</dc:creator>
    </item>
    <item>
      <title>Spike-timing-dependent plasticity and random inputs shape interspike interval regularity of model STN neurons</title>
      <link>https://arxiv.org/abs/2410.16123</link>
      <description>arXiv:2410.16123v2 Announce Type: replace 
Abstract: Neuronal oscillations are closely related to the symptoms of Parkinson's disease (PD). In this study, we explore how random fluctuations (or "stochastic inputs") affect these oscillations in brain states, which reflect the collective activity of interconnected neurons. These random inputs are modeled in the context of the subthalamic nucleus (STN), a brain region implicated in PD, and their interaction with synaptic dynamics and spike-timing-dependent plasticity (STDP) in both healthy and PD-affected neurons. Specifically, we investigate the effects of random synaptic inputs and their correlations on the membrane potential of STN neurons. Our results show that these random inputs significantly influence the firing patterns of STN neurons, both in healthy cells and in those affected by PD under deep brain stimulation (DBS) treatment. We also find that STDP increases the regularity of the interspike intervals (ISI) in spike trains of output neurons. However, the introduction of random refractory periods and fluctuating input currents can induce greater irregularity in the spike trains. Furthermore, when random inputs and STDP are combined, the correlation between the activity of different neurons increases. These findings suggest that the stochastic dynamics of STN neurons, in conjunction with STDP, could offer insights into the mechanisms underlying PD symptoms and their potential management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16123v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thoa Thieu, Roderick Melnik</dc:creator>
    </item>
    <item>
      <title>Time-Domain Classification of the Brain Reward System: Analysis of Natural and Drug Reward Driven Local Field Potential Signals in Hippocampus and Nucleus Accumbens</title>
      <link>https://arxiv.org/abs/2211.08288</link>
      <description>arXiv:2211.08288v3 Announce Type: replace-cross 
Abstract: Addiction is a major public health concern characterized by compulsive reward-seeking behavior. The excitatory glutamatergic signals from the hippocampus (HIP) to the Nucleus accumbens (NAc) mediate learned behavior in addiction. Limited comparative studies have investigated the neural pathways activated by natural and unnatural reward sources. This study has evaluated neural activities in HIP and NAc associated with food (natural) and morphine (drug) reward sources using local field potential (LFP). We developed novel approaches to classify LFP signals into the source of reward and recorded regions by considering the time-domain feature of these signals. Proposed methods included a validation step of the LFP signals using autocorrelation, Lyapunov exponent and Hurst exponent to assess the meaningful stability of these signals (lack of chaos). By utilizing the probability density function (PDF) of LFP signals and applying Kullback-Leibler divergence (KLD), data were classified to the source of the reward. Also, HIP and NAc regions were visually separated and classified using the symmetrized dot pattern technique, which can be applied in real-time to ensure the deep brain region of interest is being targeted accurately during LFP recording. We believe our method provides a computationally light and fast, real-time signal analysis approach with real-world implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.08288v3</guid>
      <category>eess.SP</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>AmirAli Kalbasi, Shole Jamali, Mahdi Aliyari Shoorehdeli, Alireza Behzadnia, Abbas Haghparast</dc:creator>
    </item>
    <item>
      <title>Explanations of Deep Language Models Explain Language Representations in the Brain</title>
      <link>https://arxiv.org/abs/2502.14671</link>
      <description>arXiv:2502.14671v2 Announce Type: replace-cross 
Abstract: Recent advances in artificial intelligence have given rise to large language models (LLMs) that not only achieve human-like performance but also share computational principles with the brain's language processing mechanisms. While previous research has primarily focused on aligning LLMs' internal representations with neural activity, we introduce a novel approach that leverages explainable AI (XAI) methods to forge deeper connections between the two domains. Using attribution methods, we quantified how preceding words contribute to an LLM's next-word predictions and employed these explanations to predict fMRI recordings from participants listening to the same narratives. Our findings demonstrate that attribution methods robustly predict brain activity across the language network, surpassing traditional internal representations in early language areas. This alignment is hierarchical: early-layer explanations correspond to the initial stages of language processing in the brain, while later layers align with more advanced stages. Moreover, the layers more influential on LLM next-word prediction$\unicode{x2014}$those with higher attribution scores$\unicode{x2014}$exhibited stronger alignment with neural activity. This work establishes a bidirectional bridge between AI and neuroscience. First, we demonstrate that attribution methods offer a powerful lens for investigating the neural mechanisms of language comprehension, revealing how meaning emerges from preceding context. Second, we propose using brain alignment as a metric to evaluate the validity of attribution methods, providing a framework for assessing their biological plausibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14671v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Maryam Rahimi, Yadollah Yaghoobzadeh, Mohammad Reza Daliri</dc:creator>
    </item>
  </channel>
</rss>
