<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 15 Oct 2024 04:02:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Evaluating Cooling Center Coverage Using Persistent Homology of a Filtered Witness Complex</title>
      <link>https://arxiv.org/abs/2410.09067</link>
      <description>arXiv:2410.09067v1 Announce Type: new 
Abstract: In light of the increase in frequency of extreme heat events, there is a critical need to develop tools to identify geographic locations that are at risk of heat-related mortality. This paper aims to identify locations by assessing holes in cooling-center coverage using persistent homology, a method from topological data analysis. Methods involving persistent homology have shown promising results in identifying holes in coverage of specific resources. We adapt these methods using a witness complex construction to study the coverage of cooling centers. One standard technique for studying the risk of heat-related mortality for a geographic area is a heat vulnerability index (HVIs) based on demographic information. We test our topological approach and an HVI on four locations (central Boston, MA; central Austin, TX; Portland, OR; and Miami, FL) and use death times of connected components and cycles to identify most at risk regions. PH and HVI identify different locations as vulnerable, thus showing measures of coverage need to extend beyond a simple statistic. Using persistent homology along side the HVI score identifies a complementary set of regions at risk, thus the combination of the two provide a more holistic understanding of coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09067v1</guid>
      <category>stat.AP</category>
      <category>cs.CG</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erin O'Neil, Sarah Tymochko</dc:creator>
    </item>
    <item>
      <title>Incorporating Asymmetric Loss for Real Estate Prediction with Area-level Spatial Data</title>
      <link>https://arxiv.org/abs/2410.09673</link>
      <description>arXiv:2410.09673v1 Announce Type: new 
Abstract: We investigate two asymmetric loss functions, namely LINEX loss and power divergence loss for optimal spatial prediction with area-level data. With our motivation arising from the real estate industry, namely in real estate valuation, we use the Zillow Home Value Index (ZHVI) for county-level values to show the change in prediction when the loss is different (asymmetric) from a traditional squared error loss (symmetric) function. Additionally, we discuss the importance of choosing the asymmetry parameter, and propose a solution to this choice for a general asymmetric loss function. Since the focus is on area-level data predictions, we propose the methodology in the context of conditionally autoregressive (CAR) models. We conclude that choice of the loss functions for spatial area-level predictions can play a crucial role, and is heavily driven by the choice of parameters in the respective loss.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09673v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vaidehi Dixit, Scott H. Holan, Christopher K. Wikle</dc:creator>
    </item>
    <item>
      <title>Modeling and Prediction of the UEFA EURO 2024 via Combined Statistical Learning Approaches</title>
      <link>https://arxiv.org/abs/2410.09068</link>
      <description>arXiv:2410.09068v1 Announce Type: cross 
Abstract: In this work, three fundamentally different machine learning models are combined to create a new, joint model for forecasting the UEFA EURO 2024. Therefore, a generalized linear model, a random forest model, and a extreme gradient boosting model are used to predict the number of goals a team scores in a match. The three models are trained on the match results of the UEFA EUROs 2004-2020, with additional covariates characterizing the teams for each tournament as well as three enhanced variables derived from different ranking methods for football teams. The first enhanced variable is based on historic match data from national teams, the second is based on the bookmakers' tournament winning odds of all participating teams, and the third is based on historic match data of individual players both for club and international matches, resulting in player ratings. Then, based on current covariate information of the participating teams, the final trained model is used to predict the UEFA EURO 2024. For this purpose, the tournament is simulated 100.000 times, based on the estimated expected number of goals for all possible matches, from which probabilities across the different tournament stages are derived. Our combined model identifies France as the clear favourite with a winning probability of 19.2%, followed by England (16.7%) and host Germany (13.7%).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09068v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andreas Groll, Lars M. Hvattum, Christophe Ley, Jonas Sternemann, Gunther Schauberger, Achim Zeileis</dc:creator>
    </item>
    <item>
      <title>Hierarchical Latent Class Models for Mortality Surveillance Using Partially Verified Verbal Autopsies</title>
      <link>https://arxiv.org/abs/2410.09274</link>
      <description>arXiv:2410.09274v1 Announce Type: cross 
Abstract: Monitoring data on causes of death is an important part of understanding the burden of diseases and effects of public health interventions. Verbal autopsy (VA) is a well-established method for gathering information about deaths outside of hospitals by conducting an interview to family members or caregivers of a deceased person. Existing cause-of-death assignment algorithms using VA data require either domain knowledge about the symptom-cause relationship, or large training datasets. When a new disease emerges, however, only limited information on symptom-cause relationship exists and training data are usually lacking, making it challenging to evaluate the impact of the disease. In this paper, we propose a novel Bayesian framework to estimate the fraction of deaths due to an emerging disease using VAs collected with partially verified cause of death. We use a latent class model to capture the distribution of symptoms and their dependence in a parsimonious way. We discuss potential sources of bias that may occur due to the cause-of-death verification process and adapt our framework to account for the verification mechanism. We also develop structured priors to improve prevalence estimation for sub-populations. We demonstrate the performance of our model using a mortality surveillance dataset that includes suspected COVID-19 related deaths in Brazil in 2021.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09274v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Zhu, Zehang Richard Li</dc:creator>
    </item>
    <item>
      <title>The 2020 United States Decennial Census Is More Private Than You (Might) Think</title>
      <link>https://arxiv.org/abs/2410.09296</link>
      <description>arXiv:2410.09296v1 Announce Type: cross 
Abstract: The U.S. Decennial Census serves as the foundation for many high-profile policy decision-making processes, including federal funding allocation and redistricting. In 2020, the Census Bureau adopted differential privacy to protect the confidentiality of individual responses through a disclosure avoidance system that injects noise into census data tabulations. The Bureau subsequently posed an open question: Could sharper privacy guarantees be obtained for the 2020 U.S. Census compared to their published guarantees, or equivalently, had the nominal privacy budgets been fully utilized?
  In this paper, we affirmatively address this open problem by demonstrating that between 8.50% and 13.76% of the privacy budget for the 2020 U.S. Census remains unused for each of the eight geographical levels, from the national level down to the block level. This finding is made possible through our precise tracking of privacy losses using $f$-differential privacy, applied to the composition of private queries across various geographical levels. Our analysis indicates that the Census Bureau introduced unnecessarily high levels of injected noise to achieve the claimed privacy guarantee for the 2020 U.S. Census. Consequently, our results enable the Bureau to reduce noise variances by 15.08% to 24.82% while maintaining the same privacy budget for each geographical level, thereby enhancing the accuracy of privatized census statistics. We empirically demonstrate that reducing noise injection into census statistics mitigates distortion caused by privacy constraints in downstream applications of private census data, illustrated through a study examining the relationship between earnings and education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09296v1</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Buxin Su, Weijie J. Su, Chendi Wang</dc:creator>
    </item>
    <item>
      <title>Dynamic Estimation of Learning Rates Using a Non-Linear Autoregressive Model</title>
      <link>https://arxiv.org/abs/2410.09943</link>
      <description>arXiv:2410.09943v1 Announce Type: cross 
Abstract: We introduce a new class of adaptive non-linear autoregressive (Nlar) models incorporating the concept of momentum, which dynamically estimate both the learning rates and momentum as the number of iterations increases. In our method, the growth of the gradients is controlled using a scaling (clipping) function, leading to stable convergence. Within this framework, we propose three distinct estimators for learning rates and provide theoretical proof of their convergence. We further demonstrate how these estimators underpin the development of effective Nlar optimizers. The performance of the proposed estimators and optimizers is rigorously evaluated through extensive experiments across several datasets and a reinforcement learning environment. The results highlight two key features of the Nlar optimizers: robust convergence despite variations in underlying parameters, including large initial learning rates, and strong adaptability with rapid convergence during the initial epochs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09943v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ramin Okhrati</dc:creator>
    </item>
    <item>
      <title>fastHDMI: Fast Mutual Information Estimation for High-Dimensional Data</title>
      <link>https://arxiv.org/abs/2410.10082</link>
      <description>arXiv:2410.10082v1 Announce Type: cross 
Abstract: In this paper, we introduce fastHDMI, a Python package designed for efficient variable screening in high-dimensional datasets, particularly neuroimaging data. This work pioneers the application of three mutual information estimation methods for neuroimaging variable selection, a novel approach implemented via fastHDMI. These advancements enhance our ability to analyze the complex structures of neuroimaging datasets, providing improved tools for variable selection in high-dimensional spaces.
  Using the preprocessed ABIDE dataset, we evaluate the performance of these methods through extensive simulations. The tests cover a range of conditions, including linear and nonlinear associations, as well as continuous and binary outcomes. Our results highlight the superiority of the FFTKDE-based mutual information estimation for feature screening in continuous nonlinear outcomes, while binning-based methods outperform others for binary outcomes with nonlinear probability preimages. For linear simulations, both Pearson correlation and FFTKDE-based methods show comparable performance for continuous outcomes, while Pearson excels in binary outcomes with linear probability preimages.
  A comprehensive case study using the ABIDE dataset further demonstrates fastHDMI's practical utility, showcasing the predictive power of models built from variables selected using our screening techniques. This research affirms the computational efficiency and methodological strength of fastHDMI, significantly enriching the toolkit available for neuroimaging analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10082v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kai Yang, Masoud Asgharian, Nikhil Bhagwat, Jean-Baptiste Poline, Celia M. T. Greenwood</dc:creator>
    </item>
    <item>
      <title>Unified Representation of Genomic and Biomedical Concepts through Multi-Task, Multi-Source Contrastive Learning</title>
      <link>https://arxiv.org/abs/2410.10144</link>
      <description>arXiv:2410.10144v1 Announce Type: cross 
Abstract: We introduce GENomic Encoding REpresentation with Language Model (GENEREL), a framework designed to bridge genetic and biomedical knowledge bases. What sets GENEREL apart is its ability to fine-tune language models to infuse biological knowledge behind clinical concepts such as diseases and medications. This fine-tuning enables the model to capture complex biomedical relationships more effectively, enriching the understanding of how genomic data connects to clinical outcomes. By constructing a unified embedding space for biomedical concepts and a wide range of common SNPs from sources such as patient-level data, biomedical knowledge graphs, and GWAS summaries, GENEREL aligns the embeddings of SNPs and clinical concepts through multi-task contrastive learning. This allows the model to adapt to diverse natural language representations of biomedical concepts while bypassing the limitations of traditional code mapping systems across different data sources. Our experiments demonstrate GENEREL's ability to effectively capture the nuanced relationships between SNPs and clinical concepts. GENEREL also emerges to discern the degree of relatedness, potentially allowing for a more refined identification of concepts. This pioneering approach in constructing a unified embedding system for both SNPs and biomedical concepts enhances the potential for data integration and discovery in biomedical research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10144v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongyi Yuan, Suqi Liu, Kelly Cho, Katherine Liao, Alexandre Pereira, Tianxi Cai</dc:creator>
    </item>
    <item>
      <title>A Geometric Model with Stochastic Error for Abnormal Motion Detection of Portal Crane Bucket Grab</title>
      <link>https://arxiv.org/abs/2410.10246</link>
      <description>arXiv:2410.10246v1 Announce Type: cross 
Abstract: Abnormal swing angle detection of bucket grabs is crucial for efficient harbor operations. In this study, we develop a practically convenient swing angle detection method for crane operation, requiring only a single standard surveillance camera at the fly-jib head, without the need for sophisticated sensors or markers on the payload. Specifically, our algorithm takes the video images from the camera as input. Next, a fine-tuned 'the fifth version of the You Only Look Once algorithm' (YOLOv5) model is used to automatically detect the position of the bucket grab on the image plane. Subsequently, a novel geometric model is constructed, which takes the pixel position of the bucket grab, the steel rope length provided by the Programmable Logic Controller system, and the optical lens information of the camera into consideration. The key parameters of this geometric model are statistically estimated by a novel iterative algorithm. Once the key parameters are estimated, the algorithm can automatically detect swing angles from video streams. Being analytically simple, the computation of our algorithm is fast, as it takes about 0.01 seconds to process one single image generated by the surveillance camera. Therefore, we are able to obtain an accurate and fast estimation of the swing angle of an operating crane in real-time applications. Simulation studies are conducted to validate the model and algorithm. Real video examples from Qingdao Seaport under various weather conditions are analyzed to demonstrate its practical performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10246v1</guid>
      <category>cs.CE</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Baichen Yu, Xiao Wang, Hansheng Wang</dc:creator>
    </item>
    <item>
      <title>Regression Model for Speckled Data with Extremely Variability</title>
      <link>https://arxiv.org/abs/2410.10482</link>
      <description>arXiv:2410.10482v1 Announce Type: cross 
Abstract: Synthetic aperture radar (SAR) is an efficient and widely used remote sensing tool. However, data extracted from SAR images are contaminated with speckle, which precludes the application of techniques based on the assumption of additive and normally distributed noise. One of the most successful approaches to describing such data is the multiplicative model, where intensities can follow a variety of distributions with positive support. The $\mathcal{G}^0_I$ model is among the most successful ones. Although several estimation methods for the $\mathcal{G}^0_I$ parameters have been proposed, there is no work exploring a regression structure for this model. Such a structure could allow us to infer unobserved values from available ones. In this work, we propose a $\mathcal{G}^0_I$ regression model and use it to describe the influence of intensities from other polarimetric channels. We derive some theoretical properties for the new model: Fisher information matrix, residual measures, and influential tools. Maximum likelihood point and interval estimation methods are proposed and evaluated by Monte Carlo experiments. Results from simulated and actual data show that the new model can be helpful for SAR image analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10482v1</guid>
      <category>stat.ME</category>
      <category>eess.IV</category>
      <category>physics.data-an</category>
      <category>physics.ins-det</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.isprsjprs.2024.05.009</arxiv:DOI>
      <arxiv:journal_reference>Elsevier ISPRS Journal of Photogrammetry and Remote Sensing, Volume 213, July 2024, Pages 1-13</arxiv:journal_reference>
      <dc:creator>A. D. C. Nascimento, J. M. Vasconcelos, R. J. Cintra, A. C. Frery</dc:creator>
    </item>
    <item>
      <title>Causal Modeling of Climate Activism on Reddit</title>
      <link>https://arxiv.org/abs/2410.10562</link>
      <description>arXiv:2410.10562v1 Announce Type: cross 
Abstract: Climate activism is crucial in stimulating collective societal and behavioral change towards sustainable practices through political pressure. Although multiple factors contribute to the participation in activism, their complex relationships and the scarcity of data on their interactions have restricted most prior research to studying them in isolation, thus preventing the development of a quantitative, causal understanding of why people approach activism. In this work, we develop a comprehensive causal model of how and why Reddit users engage with activist communities driving mass climate protests (mainly the 2019 Earth Strike, Fridays for Future, and Extinction Rebellion). Our framework, based on Stochastic Variational Inference applied to Bayesian Networks, learns the causal pathways over multiple time periods. Distinct from previous studies, our approach uses large-scale and fine-grained longitudinal data (2016 to 2022) to jointly model the roles of sociodemographic makeup, experience of extreme weather events, exposure to climate-related news, and social influence through online interactions. We find that among users interested in climate change, participation in online activist communities is indeed influenced by direct interactions with activists and largely by recent exposure to media coverage of climate protests. Among people aware of climate change, left-leaning people from lower socioeconomic backgrounds are particularly represented in online activist groups. Our findings offer empirical validation for theories of media influence and critical mass, and lay the foundations to inform interventions and future studies to foster public participation in collective action.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10562v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jacopo Lenti, Luca Maria Aiello, Corrado Monti, Gianmarco De Francisci Morales</dc:creator>
    </item>
    <item>
      <title>Variational Bayesian surrogate modelling with application to robust design optimisation</title>
      <link>https://arxiv.org/abs/2404.14857</link>
      <description>arXiv:2404.14857v2 Announce Type: replace 
Abstract: Surrogate models provide a quick-to-evaluate approximation to complex computational models and are essential for multi-query problems like design optimisation. The inputs of current deterministic computational models are usually high-dimensional and uncertain. We consider Bayesian inference for constructing statistical surrogates with input uncertainties and intrinsic dimensionality reduction. The surrogate is trained by fitting to data obtained from a deterministic computational model. The assumed prior probability density of the surrogate is a Gaussian process. We determine the respective posterior probability density and parameters of the posited statistical model using variational Bayes. The non-Gaussian posterior is approximated by a Gaussian trial density with free variational parameters and the discrepancy between them is measured using the Kullback-Leibler (KL) divergence. We employ the stochastic gradient method to compute the variational parameters and other statistical model parameters by minimising the KL divergence. We demonstrate the accuracy and versatility of the proposed reduced dimension variational Gaussian process (RDVGP) surrogate on illustrative and robust structural optimisation problems where cost functions depend on a weighted sum of the mean and standard deviation of model outputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14857v2</guid>
      <category>stat.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cma.2024.117423</arxiv:DOI>
      <arxiv:journal_reference>Comput. Methods Appl. Mech. Engrg. 432 (2024) 117423</arxiv:journal_reference>
      <dc:creator>Thomas A. Archbold, Ieva Kazlauskaite, Fehmi Cirak</dc:creator>
    </item>
    <item>
      <title>The impact of complexity in the built environment on vehicular routing behavior: Insights from an empirical study of taxi mobility in Beijing, China</title>
      <link>https://arxiv.org/abs/2404.15589</link>
      <description>arXiv:2404.15589v2 Announce Type: replace 
Abstract: The modeling of disaggregated vehicular mobility and its associations with the ambient urban built environment is essential for developing operative transport intervention and urban optimization plans. However, established vehicular route choice models failed to fully consider the bounded behavioral rationality and the complex characteristics of the urban built environment affecting drivers' route choice preference. Therefore, the spatio-temporal characteristics of vehicular mobility patterns were not fully explained, which limited the granular implementation of relevant transport interventions. To address this limitation, we proposed a vehicular route choice model that mimics the anchoring effect and the exposure preference while driving. The proposed model enables us to quantitatively examine the impact of the built environment on vehicular routing behavior, which has been largely neglected in previous studies. Results show that the proposed model performs 12% better than the conventional vehicular route choice model based on the shortest path principle. Our empirical analysis of taxi drivers' routing behavior patterns in Beijing, China uncovers that drivers are inclined to choose routes with shorter time duration and with less loss at traversal intersections. Counterintuitively, we also found that drivers heavily rely on circuitous ring roads and expressways to deliver passengers, which are unexpectedly longer than the shortest paths. Moreover, characteristics of the urban built environment including road eccentricity, centrality, average road length, land use diversity, sky visibility, and building coverage can affect drivers' route choice behaviors, accounting for about 5% of the increase in the proposed model's performance. We also refine the above explorations according to the modeling results of trips that differ in departure time, travel distance, and occupation status.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15589v2</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chaogui Kang, Zheren Liu</dc:creator>
    </item>
    <item>
      <title>Simplifying Random Forests' Probabilistic Forecasts</title>
      <link>https://arxiv.org/abs/2408.12332</link>
      <description>arXiv:2408.12332v3 Announce Type: replace 
Abstract: Since their introduction by Breiman, Random Forests (RFs) have proven to be useful for both classification and regression tasks. The RF prediction of a previously unseen observation can be represented as a weighted sum of all training sample observations. This nearest-neighbor-type representation is useful, among other things, for constructing forecast distributions (Meinshausen, 2006). In this paper, we consider simplifying RF-based forecast distributions by sparsifying them. That is, we focus on a small subset of nearest neighbors while setting the remaining weights to zero. This sparsification step greatly improves the interpretability of RF predictions. It can be applied to any forecasting task without re-training existing RF models. In empirical experiments, we document that the simplified predictions can be similar to or exceed the original ones in terms of forecasting performance. We explore the statistical sources of this finding via a stylized analytical model of RFs. The model suggests that simplification is particularly promising if the unknown true forecast distribution contains many small weights that are estimated imprecisely.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12332v3</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nils Koster, Fabian Kr\"uger</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification in Synthetic Controls with Staggered Treatment Adoption</title>
      <link>https://arxiv.org/abs/2210.05026</link>
      <description>arXiv:2210.05026v4 Announce Type: replace-cross 
Abstract: We propose principled prediction intervals to quantify the uncertainty of a large class of synthetic control predictions (or estimators) in settings with staggered treatment adoption, offering precise non-asymptotic coverage probability guarantees. From a methodological perspective, we provide a detailed discussion of different causal quantities to be predicted, which we call causal predictands, allowing for multiple treated units with treatment adoption at possibly different points in time. From a theoretical perspective, our uncertainty quantification methods improve on prior literature by (i) covering a large class of causal predictands in staggered adoption settings, (ii) allowing for synthetic control methods with possibly nonlinear constraints, (iii) proposing scalable robust conic optimization methods and principled data-driven tuning parameter selection, and (iv) offering valid uniform inference across post-treatment periods. We illustrate our methodology with an empirical application studying the effects of economic liberalization on real GDP per capita for Sub-Saharan African countries. Companion general-purpose software packages are provided in Python, R, and Stata.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.05026v4</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Yingjie Feng, Filippo Palomba, Rocio Titiunik</dc:creator>
    </item>
    <item>
      <title>Bayesian inference for partial orders from random linear extensions: power relations from 12th Century Royal Acta</title>
      <link>https://arxiv.org/abs/2212.05524</link>
      <description>arXiv:2212.05524v3 Announce Type: replace-cross 
Abstract: In the eleventh and twelfth centuries in England, Wales and Normandy, Royal Acta were legal documents in which witnesses were listed in order of social status. Any bishops present were listed as a group. For our purposes, each witness-list is an ordered permutation of bishop names with a known date or date-range. Changes over time in the order bishops are listed may reflect changes in their authority. Historians would like to detect and quantify these changes. There is no reason to assume that the underlying social order which constrains bishop-order within lists is a complete order. We therefore model the evolving social order as an evolving partial ordered set or {\it poset}.
  We construct a Hidden Markov Model for these data. The hidden state is an evolving poset (the evolving social hierarchy) and the emitted data are random total orders (dated lists) respecting the poset present at the time the order was observed. This generalises existing models for rank-order data such as Mallows and Plackett-Luce. We account for noise via a random ``queue-jumping'' process. Our latent-variable prior for the random process of posets is marginally consistent. A parameter controls poset depth and actor-covariates inform the position of actors in the hierarchy. We fit the model, estimate posets and find evidence for changes in status over time. We interpret our results in terms of court politics. Simpler models, based on Bucket Orders and vertex-series-parallel orders, are rejected. We compare our results with a time-series extension of the Plackett-Luce model. Our software is publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.05524v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geoff K. Nicholls, Jeong Eun Lee, Nicholas Karn, David Johnson, Rukuang Huang, Alexis Muir-Watt</dc:creator>
    </item>
    <item>
      <title>On the Distribution of Probe Traffic Volume Estimated without Trajectory Reconstruction</title>
      <link>https://arxiv.org/abs/2307.15274</link>
      <description>arXiv:2307.15274v2 Announce Type: replace-cross 
Abstract: In recent years, passively recorded probe traffic volumes have increasingly been used to estimate traffic volumes. However, it is not always possible to count probe traffic volume in a spatial dataset when probe trajectories cannot be fully reconstructed from raw probe point location data due to sparse recording intervals, lack of pseudonyms or timestamps. As a result, the application of such probe point location data has been limited in traffic volume estimation. To relax these constraints, we present the exact distribution of the estimated probe traffic volume in a road segment based on probe point location data without trajectory reconstruction. The distribution of the estimated probe traffic volume can exhibit multimodality, without necessarily being line-symmetric with respect to the true probe traffic volume. As more probes are present, the distribution approaches a normal distribution. The conformity of the distribution was visualised through numerical simulations. Sometimes, there exists a local optimal cordon length that maximises estimation precision. The theoretical variance of estimated probe traffic volume can address heteroscedasticity in the modelling of traffic volume estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.15274v2</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kentaro Iio, Gulshan Noorsumar, Dominique Lord, Yunlong Zhang</dc:creator>
    </item>
    <item>
      <title>BGF-YOLO: Enhanced YOLOv8 with Multiscale Attentional Feature Fusion for Brain Tumor Detection</title>
      <link>https://arxiv.org/abs/2309.12585</link>
      <description>arXiv:2309.12585v3 Announce Type: replace-cross 
Abstract: You Only Look Once (YOLO)-based object detectors have shown remarkable accuracy for automated brain tumor detection. In this paper, we develop a novel BGF-YOLO architecture by incorporating Bi-level routing attention, Generalized feature pyramid networks, and Fourth detecting head into YOLOv8. BGF-YOLO contains an attention mechanism to focus more on important features, and feature pyramid networks to enrich feature representation by merging high-level semantic features with spatial details. Furthermore, we investigate the effect of different attention mechanisms and feature fusions, detection head architectures on brain tumor detection accuracy. Experimental results show that BGF-YOLO gives a 4.7% absolute increase of mAP$_{50}$ compared to YOLOv8x, and achieves state-of-the-art on the brain tumor detection dataset Br35H. The code is available at https://github.com/mkang315/BGF-YOLO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.12585v3</guid>
      <category>cs.CV</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-72111-3_4</arxiv:DOI>
      <arxiv:journal_reference>In MICCAI (2024) 15008 35-45</arxiv:journal_reference>
      <dc:creator>Ming Kang, Chee-Ming Ting, Fung Fung Ting, Rapha\"el C. -W. Phan</dc:creator>
    </item>
    <item>
      <title>A Unified Bayesian Framework for Modeling Measurement Error in Multinomial Data</title>
      <link>https://arxiv.org/abs/2310.09345</link>
      <description>arXiv:2310.09345v2 Announce Type: replace-cross 
Abstract: Measurement error in multinomial data is a well-known and well-studied inferential problem that is encountered in many fields, including engineering, biomedical and omics research, ecology, finance, official statistics, and social sciences. Methods developed to accommodate measurement error in multinomial data are typically equipped to handle false negatives or false positives, but not both. We provide a unified framework for accommodating both forms of measurement error using a Bayesian hierarchical approach. We demonstrate the proposed method's performance on simulated data and apply it to acoustic bat monitoring and official crime data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.09345v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew D. Koslovsky, Andee Kaplan, Victoria A. Terranova, Mevin B. Hooten</dc:creator>
    </item>
    <item>
      <title>Balance Correlations, Agentic Zeros, and Networks: The Structure of 192 Years of War and Peace</title>
      <link>https://arxiv.org/abs/2312.04358</link>
      <description>arXiv:2312.04358v2 Announce Type: replace-cross 
Abstract: Social network extensions of Heider's balance theory have not always been consistent. Structural balance theory primarily focuses on graph partitioning, thereby assuming, homogeneity in balance-driven behavior of nodes. We present a general model and formal notation that permit testing such behavioral assumptions. Specifically, we formulate statements as a comparison of two conditional probabilities of a tie, $Ego\stackrel{q}{\text{-}}Alter$, first conditional on 2-paths $Ego\, \stackrel{r}{\text{-}}\,X\,\stackrel{s}{\text{-}}\,Alter$, and second conditional on all others, $\neg (Ego\,\stackrel{r}{\text{-}}\,X\,\stackrel{s}{\text{-}}\,Alter)$. The key here is that $q$, $r$ and $s$ represent indices of relations in a set of mutually exclusive and exhaustive relations (their sum produces a complete graph). This relaxes the assumption of a signed graph dichotomy. Here we identify neutral as distinct from negative and positive ties. Descriptive statistics measuring the difference in conditional probabilities, or the prevalence for any stipulated balance configuration, are given by the point bi-serial correlations of relation $q$ with the count of $2$-paths (through relations $r$ and $s$). Two major advantages are: direct comparison, even if network sizes and densities differ, and evaluation of specific (un)balance behaviors. We apply this approach on a data set with friendly vs hostile relations between countries from 1816 to 2007. We find strong evidence for one of the four classic Heiderian balance theory predictions, and virtually no evidence in support of the unbalanced predictions. However, we do find stable and surprising evidence that the neutral ties are important in balancing the relations among nations. Results further suggest that prevalence of balance driven behavior varies over time, and that other triadic motivated behaviors prevail among countries in certain eras.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04358v2</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>David Dekker, David Krackhardt, Patrick Doreian, Pavel N. Krivitsky</dc:creator>
    </item>
    <item>
      <title>Does AI help humans make better decisions? A statistical evaluation framework for experimental and observational studies</title>
      <link>https://arxiv.org/abs/2403.12108</link>
      <description>arXiv:2403.12108v3 Announce Type: replace-cross 
Abstract: The use of Artificial Intelligence (AI), or more generally data-driven algorithms, has become ubiquitous in today's society. Yet, in many cases and especially when stakes are high, humans still make final decisions. The critical question, therefore, is whether AI helps humans make better decisions compared to a human-alone or AI-alone system. We introduce a new methodological framework to empirically answer this question with a minimal set of assumptions. We measure a decision maker's ability to make correct decisions using standard classification metrics based on the baseline potential outcome. We consider a single-blinded and unconfounded treatment assignment, where the provision of AI-generated recommendations is assumed to be randomized across cases with humans making final decisions. Under this study design, we show how to compare the performance of three alternative decision-making systems--human-alone, human-with-AI, and AI-alone. Importantly, the AI-alone system includes any individualized treatment assignment, including those that are not used in the original study. We also show when AI recommendations should be provided to a human-decision maker, and when one should follow such recommendations. We apply the proposed methodology to our own randomized controlled trial evaluating a pretrial risk assessment instrument. We find that the risk assessment recommendations do not improve the classification accuracy of a judge's decision to impose cash bail. Furthermore, we find that replacing a human judge with algorithms--the risk assessment score and a large language model in particular--leads to a worse classification performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12108v3</guid>
      <category>cs.AI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eli Ben-Michael, D. James Greiner, Melody Huang, Kosuke Imai, Zhichao Jiang, Sooahn Shin</dc:creator>
    </item>
    <item>
      <title>Tree balance in phylogenetic models</title>
      <link>https://arxiv.org/abs/2406.05185</link>
      <description>arXiv:2406.05185v4 Announce Type: replace-cross 
Abstract: Tree shape statistics, particularly measures of tree (im)balance, play an important role in the analysis of the shape of phylogenetic trees. With applications ranging from testing evolutionary models to studying the impact of fertility inheritance and selection, or tumor development and language evolution, the assessment of measures of tree balance is important. Currently, a multitude of at least 30 (im)balance indices can be found in the literature, alongside numerous other tree shape statistics.
  This diversity prompts essential questions: How can we assist researchers in choosing only a small number of indices to mitigate the challenges of multiple testing? Is there a preeminent balance index tailored to specific tasks?
  This research expands previous studies on the examination of index power, encompassing almost all established indices and a broader array of alternative models, such as a variety of trait-based models. Our investigation reveals distinct groups of balance indices better suited for different tree models, suggesting that decisions on balance index selection can be enhanced with prior knowledge. Furthermore, we present the \textsf{R} software package \textsf{poweRbal} which allows the inclusion of new indices and models, thus facilitating future research on the power of tree shape statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05185v4</guid>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sophie J. Kersting, Kristina Wicke, Mareike Fischer</dc:creator>
    </item>
    <item>
      <title>A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios</title>
      <link>https://arxiv.org/abs/2408.01963</link>
      <description>arXiv:2408.01963v3 Announce Type: replace-cross 
Abstract: We evaluate the robustness of several large language models on multiple datasets. Robustness here refers to the relative insensitivity of the model's answers to meaning-preserving variants of their input. Benchmark datasets are constructed by introducing naturally-occurring, non-malicious perturbations, or by generating semantically equivalent paraphrases of input questions or statements. We further propose a novel metric for assessing a model robustness, and demonstrate its benefits in the non-adversarial scenario by empirical evaluation of several models on the created datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01963v3</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Ackerman, Ella Rabinovich, Eitan Farchi, Ateret Anaby-Tavor</dc:creator>
    </item>
    <item>
      <title>Locally Adaptive Random Walk Stochastic Volatility</title>
      <link>https://arxiv.org/abs/2408.11315</link>
      <description>arXiv:2408.11315v2 Announce Type: replace-cross 
Abstract: We introduce a novel Bayesian framework for estimating time-varying volatility by extending the Random Walk Stochastic Volatility (RWSV) model with a new Dynamic Shrinkage Process (DSP) in (log) variances. Unlike classical Stochastic Volatility or GARCH-type models with restrictive parametric stationarity assumptions, our proposed Adaptive Stochastic Volatility (ASV) model provides smooth yet dynamically adaptive estimates of evolving volatility and its uncertainty (vol of vol). We derive the theoretical properties of the proposed global-local shrinkage prior. Through simulation studies, we demonstrate that ASV exhibits remarkable misspecification resilience with low prediction error across various data generating scenarios in simulation. Furthermore, ASV's capacity to yield locally smooth and interpretable estimates facilitates a clearer understanding of underlying patterns and trends in volatility. Additionally, we propose and illustrate an extension for Bayesian Trend Filtering simultaneously in both mean and variance. Finally, we show that this attribute makes ASV a robust tool applicable across a wide range of disciplines, including in finance, environmental science, epidemiology, and medicine, among others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11315v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason B. Cho, David S. Matteson</dc:creator>
    </item>
    <item>
      <title>Neural Networks with LSTM and GRU in Modeling Active Fires in the Amazon</title>
      <link>https://arxiv.org/abs/2409.02681</link>
      <description>arXiv:2409.02681v4 Announce Type: replace-cross 
Abstract: This study presents a comprehensive methodology for modeling and forecasting the historical time series of active fire spots detected by the AQUA\_M-T satellite in the Amazon, Brazil. The approach employs a mixed Recurrent Neural Network (RNN) model, combining Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) architectures to predict the monthly accumulations of daily detected active fire spots. Data analysis revealed a consistent seasonality over time, with annual maximum and minimum values tending to repeat at the same periods each year. The primary objective is to verify whether the forecasts capture this inherent seasonality through machine learning techniques. The methodology involved careful data preparation, model configuration, and training using cross-validation with two seeds, ensuring that the data generalizes well to both the test and validation sets for both seeds. The results indicate that the combined LSTM and GRU model delivers excellent forecasting performance, demonstrating its effectiveness in capturing complex temporal patterns and modeling the observed time series. This research significantly contributes to the application of deep learning techniques in environmental monitoring, specifically in forecasting active fire spots. The proposed approach highlights the potential for adaptation to other time series forecasting challenges, opening new opportunities for research and development in machine learning and prediction of natural phenomena.
  Keywords: Time Series Forecasting; Recurrent Neural Networks; Deep Learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02681v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ramon Tavares, Ricardo Olinda</dc:creator>
    </item>
    <item>
      <title>A Functional Extension of Semi-Structured Networks</title>
      <link>https://arxiv.org/abs/2410.05430</link>
      <description>arXiv:2410.05430v2 Announce Type: replace-cross 
Abstract: Semi-structured networks (SSNs) merge the structures familiar from additive models with deep neural networks, allowing the modeling of interpretable partial feature effects while capturing higher-order non-linearities at the same time. A significant challenge in this integration is maintaining the interpretability of the additive model component. Inspired by large-scale biomechanics datasets, this paper explores extending SSNs to functional data. Existing methods in functional data analysis are promising but often not expressive enough to account for all interactions and non-linearities and do not scale well to large datasets. Although the SSN approach presents a compelling potential solution, its adaptation to functional data remains complex. In this work, we propose a functional SSN method that retains the advantageous properties of classical functional regression approaches while also improving scalability. Our numerical experiments demonstrate that this approach accurately recovers underlying signals, enhances predictive performance, and performs favorably compared to competing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05430v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David R\"ugamer, Bernard X. W. Liew, Zainab Altai, Almond St\"ocker</dc:creator>
    </item>
  </channel>
</rss>
