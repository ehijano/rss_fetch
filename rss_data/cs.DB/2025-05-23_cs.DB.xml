<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 May 2025 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Common Data Format (CDF): A Standardized Format for Match-Data in Football (Soccer)</title>
      <link>https://arxiv.org/abs/2505.15820</link>
      <description>arXiv:2505.15820v1 Announce Type: new 
Abstract: During football matches, a variety of different parties (e.g., companies) each collect (possibly overlapping) data about the match ranging from basic information (e.g., starting players) to detailed positional data. This data is provided to clubs, federations, and other organizations who are increasingly interested in leveraging this data to inform their decision making. Unfortunately, analyzing such data pose significant barriers because each provider may (1) collect different data, (2) use different specifications even within the same category of data, (3) represent the data differently, and (4) delivers the data in a different manner (e.g., file format, protocol). Consequently, working with these data requires a significant investment of time and money. The goal of this work is to propose a uniform and standardized format for football data called the Common Data Format (CDF). The CDF specifies a minimal schema for five types of match data: match sheet data, video footage, event data, tracking data, and match meta data. It aims to ensure that the provided data is clear, sufficiently contextualized (e.g., its provenance is clear), and complete such that it enables common downstream analysis tasks. Concretely, this paper will detail the technical specifications of the CDF, the representational choices that were made to help ensure the clarity of the provided data, and a concrete approach for delivering data in the CDF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15820v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel Anzer, Kilian Arnsmeyer, Pascal Bauer, Joris Bekkers, Ulf Brefeld, Jesse Davis, Nicolas Evans, Matthias Kempe, Samuel J Robertson, Joshua Wyatt Smith, Jan Van Haaren</dc:creator>
    </item>
    <item>
      <title>MAPS: A Multilingual Benchmark for Global Agent Performance and Security</title>
      <link>https://arxiv.org/abs/2505.15935</link>
      <description>arXiv:2505.15935v1 Announce Type: new 
Abstract: Agentic AI systems, which build on Large Language Models (LLMs) and interact with tools and memory, have rapidly advanced in capability and scope. Yet, since LLMs have been shown to struggle in multilingual settings, typically resulting in lower performance and reduced safety, agentic systems risk inheriting these limitations. This raises concerns about the global accessibility of such systems, as users interacting in languages other than English may encounter unreliable or security-critical agent behavior. Despite growing interest in evaluating agentic AI, existing benchmarks focus exclusively on English, leaving multilingual settings unexplored. To address this gap, we propose MAPS, a multilingual benchmark suite designed to evaluate agentic AI systems across diverse languages and tasks. MAPS builds on four widely used agentic benchmarks - GAIA (real-world tasks), SWE-bench (code generation), MATH (mathematical reasoning), and the Agent Security Benchmark (security). We translate each dataset into ten diverse languages, resulting in 805 unique tasks and 8,855 total language-specific instances. Our benchmark suite enables a systematic analysis of how multilingual contexts affect agent performance and robustness. Empirically, we observe consistent degradation in both performance and security when transitioning from English to other languages, with severity varying by task and correlating with the amount of translated input. Building on these findings, we provide actionable recommendations to guide agentic AI systems development and assessment under multilingual settings. This work establishes a standardized evaluation framework, encouraging future research towards equitable, reliable, and globally accessible agentic AI. MAPS benchmark suite is publicly available at https://huggingface.co/datasets/Fujitsu-FRE/MAPS</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15935v1</guid>
      <category>cs.DB</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omer Hofman, Oren Rachmil, Shamik Bose, Vikas Pahuja, Jonathan Brokman, Toshiya Shimizu, Trisha Starostina, Kelly Marchisio, Seraphina Goldfarb-Tarrant, Roman Vainshtein</dc:creator>
    </item>
    <item>
      <title>Modular Fault-Tolerant DBMS</title>
      <link>https://arxiv.org/abs/2505.15953</link>
      <description>arXiv:2505.15953v1 Announce Type: new 
Abstract: The article addresses the problem of storing data in extreme environmental conditions with limited computing resources and memory. There is a requirement to create portable, fault-tolerant, modular database management systems (DBMS) that are optimized for use in embedded systems. Existing databases, such as LittleDB, LMDB, and Berkeley DB, are reviewed, and their limitations are identified. A variant of a portable DBMS is introduced to efficiently manage data in environments where computational resource usage must be minimized, while meeting specific requirements for fault tolerance and noise immunity. Common solutions for optimizing of insertion, storage and management of data are reviewed. Algorithms for fault-tolerant data encoding in RAM are implemented. An architectural solution to data storage and minimizing the impact of bit errors is proposed. Software that manages relational data in extreme conditions is developed, that allows testing and comparing results with existing solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15953v1</guid>
      <category>cs.DB</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nikolay Fot, Alexander Vinarsky</dc:creator>
    </item>
    <item>
      <title>WikiDBGraph: Large-Scale Database Graph of Wikidata for Collaborative Learning</title>
      <link>https://arxiv.org/abs/2505.16635</link>
      <description>arXiv:2505.16635v1 Announce Type: new 
Abstract: Tabular data, ubiquitous and rich in informational value, is an increasing focus for deep representation learning, yet progress is hindered by studies centered on single tables or isolated databases, which limits model capabilities due to data scale. While collaborative learning approaches such as federated learning, transfer learning, split learning, and tabular foundation models aim to learn from multiple correlated databases, they are challenged by a scarcity of real-world interconnected tabular resources. Current data lakes and corpora largely consist of isolated databases lacking defined inter-database correlations. To overcome this, we introduce WikiDBGraph, a large-scale graph of 100,000 real-world tabular databases from WikiData, interconnected by 17 million edges and characterized by 13 node and 12 edge properties derived from its database schema and data distribution. WikiDBGraph's weighted edges identify both instance- and feature-overlapped databases. Experiments on these newly identified databases confirm that collaborative learning yields superior performance, thereby offering considerable promise for structured foundation model training while also exposing key challenges and future directions for learning from interconnected tabular data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16635v1</guid>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhaomin Wu, Ziyang Wang, Bingsheng He</dc:creator>
    </item>
    <item>
      <title>A Chase-based Approach to Consistent Answers of Analytic Queries in Star Schemas</title>
      <link>https://arxiv.org/abs/2505.16802</link>
      <description>arXiv:2505.16802v1 Announce Type: new 
Abstract: We present an approach to computing consistent answers to analytic queries in data warehouses operating under a star schema and possibly containing missing values and inconsistent data. Our approach is based on earlier work concerning consistent query answering for standard, non-analytic queries in multi-table databases. In that work we presented polynomial algorithms for computing either the exact consistent answer to a standard, non analytic query or bounds of the exact answer, depending on whether the query involves a selection condition or not.
  We extend this approach to computing exact consistent answers of analytic queries over star schemas, provided that the selection condition in the query involves no keys and satisfies the property of independency (i.e., the condition can be expressed as a conjunction of conditions each involving a single attribute). The main contributions of this paper are: (a) a polynomial algorithm for computing the exact consistent answer to a usual projection-selection-join query over a star schema under the above restrictions on the selection condition, and (b) showing that, under the same restrictions the exact consistent answer to an analytic query over a star schema can be computed in time polynomial in the size of the data warehouse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16802v1</guid>
      <category>cs.DB</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dominique Laurent, Nicolas Spyratos</dc:creator>
    </item>
    <item>
      <title>Towards Machine-actionable FAIR Digital Objects with a Typing Model that Enables Operations</title>
      <link>https://arxiv.org/abs/2505.16550</link>
      <description>arXiv:2505.16550v1 Announce Type: cross 
Abstract: FAIR Digital Objects support research data management aligned with the FAIR principles. To be machine-actionable, they must support operations that interact with their contents. This can be achieved by associating operations with FAIR-DO data types. However, current typing models and Data Type Registries lack support for type-associated operations. In this work, we introduce a typing model that describes type-associated and technology-agnostic FAIR Digital Object Operations in a machine-actionable way, building and improving on the existing concepts. In addition, we introduce the Integrated Data Type and Operations Registry with Inheritance System, a prototypical implementation of this model that integrates inheritance mechanisms for data types, a rule-based validation system, and the computation of type-operation associations. Our approach significantly improves the machine-actionability of FAIR Digital Objects, paving the way towards dynamic, interoperable, and reproducible research workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16550v1</guid>
      <category>cs.DL</category>
      <category>cs.DB</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maximilian Inckmann, Nicolas Blumenr\"ohr, Rossella Aversa</dc:creator>
    </item>
    <item>
      <title>Restricted Chase Termination: You Want More than Fairness</title>
      <link>https://arxiv.org/abs/2505.16551</link>
      <description>arXiv:2505.16551v1 Announce Type: cross 
Abstract: The chase is a fundamental algorithm with ubiquitous uses in database theory. Given a database and a set of existential rules (aka tuple-generating dependencies), it iteratively extends the database to ensure that the rules are satisfied in a most general way. This process may not terminate, and a major problem is to decide whether it does. This problem has been studied for a large number of chase variants, which differ by the conditions under which a rule is applied to extend the database. Surprisingly, the complexity of the universal termination of the restricted (aka standard) chase is not fully understood. We close this gap by placing universal restricted chase termination in the analytical hierarchy. This higher hardness is due to the fairness condition, and we propose an alternative condition to reduce the hardness of universal termination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16551v1</guid>
      <category>cs.LO</category>
      <category>cs.DB</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3725246</arxiv:DOI>
      <dc:creator>David Carral, Lukas Gerlach, Lucas Larroque, Micha\"el Thomazo</dc:creator>
    </item>
    <item>
      <title>DePLOI: Applying NL2SQL to Synthesize and Audit Database Access Control</title>
      <link>https://arxiv.org/abs/2402.07332</link>
      <description>arXiv:2402.07332v4 Announce Type: replace 
Abstract: In every enterprise database, administrators must define an access control policy that specifies which users have access to which tables. Access control straddles two worlds: policy (organization-level principles that define who should have access) and process (database-level primitives that actually implement the policy). Assessing and enforcing process compliance with a policy is a manual and ad-hoc task. This paper introduces a new access control model called Intent-Based Access Control for Databases (IBAC-DB). In IBAC-DB, access control policies are expressed using abstractions that scale to high numbers of database objects, and are traceable with respect to implementations. This paper proposes DePLOI (Deployment Policy Linter for Organization Intents), a LLM-backed system leveraging access control-specific task decompositions to accurately synthesize and audit access control implementation from IBAC-DB abstractions. As DePLOI is the first system of its kind to our knowledge, this paper further proposes IBACBench, the first benchmark for evaluating the synthesis and auditing capabilities of DePLOI. IBACBench leverages a combination of current NL2SQL benchmarks, real-world role hierarchies and access control policies, and LLM-generated data. We find that DePLOI achieves high synthesis accuracies and auditing F1 scores overall, and greatly outperforms other LLM prompting strategies (e.g., by 10 F1 points).</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07332v4</guid>
      <category>cs.DB</category>
      <category>cs.CR</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pranav Subramaniam, Sanjay Krishnan</dc:creator>
    </item>
    <item>
      <title>Data-Driven Prescriptive Analytics Applications: A Comprehensive Survey</title>
      <link>https://arxiv.org/abs/2412.00034</link>
      <description>arXiv:2412.00034v3 Announce Type: replace 
Abstract: Prescriptive Analytics (PSA), an emerging business analytics field suggesting concrete options for solving business problems, has seen an increasing amount of interest after more than a decade of multidisciplinary research. This paper is a comprehensive survey of existing applications within PSA in terms of their use cases, methodologies, and possible future research directions. To ensure a manageable scope, we focus on PSA applications that develop data-driven, automatic workflows, i.e., Data-Driven PSA (DPSA). Following a systematic methodology, we identify and include 104 papers in our survey. As our key contributions, we derive a number of novel taxonomies of the field and use them to analyse the field's temporal development. In terms of use cases, we derive 10 application domains for DPSA, from Healthcare to Manufacturing, and subsumed problem types within each. In terms of individual method usage, we derive 5 method types and map them to a comprehensive taxonomy of method usage within DPSA applications, covering mathematical optimization, data mining and machine learning, probabilistic modelling, domain expertise, as well as simulations. As for combined method usage, we provide a statistical overview of how different method usage combinations are distributed and derive 2 generic workflow patterns along with subsumed workflow patterns, combining methods by either sequential or simultaneous relationships. Finally, we derive 5 possible research directions based on frequently recurring issues among surveyed papers, suggesting new frontiers in terms of methods, tools, and use cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00034v3</guid>
      <category>cs.DB</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Moesmann, Torben Bach Pedersen</dc:creator>
    </item>
    <item>
      <title>SQL-o1: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL</title>
      <link>https://arxiv.org/abs/2502.11741</link>
      <description>arXiv:2502.11741v3 Announce Type: replace 
Abstract: Text-to-SQL (Text2SQL) aims to map natural language questions to executable SQL queries. Although large language models (LLMs) have driven significant progress, current approaches struggle with poor transferability to open-source LLMs, limited robustness against logic and function errors in complex queries, and inefficiencies in structured search. We introduce SQL-o1, a self-reward-driven heuristic search framework built on an agent-based architecture to enhance model reasoning capabilities. SQL-o1 leverages Monte Carlo Tree Search (MCTS) for structured, multi-step exploration, and incorporates a dynamic pruning strategy to accelerate inference without sacrificing accuracy. On the Spider and Bird benchmarks, SQL-o1 achieves a +10.8 execution accuracy improvement on the complex Bird dataset, surpassing even GPT-4-based models. Notably, it exhibits strong few-shot generalization and robust cross-model transferability across open-source LLMs. Our code is available at:https://github.com/ShuaiLyu0110/SQL-o1.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11741v3</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuai Lyu, Haoran Luo, Ripeng Li, Zhonghong Ou, Jiangfeng Sun, Yang Qin, Xiaoran Shang, Meina Song, Yifan Zhu</dc:creator>
    </item>
    <item>
      <title>In-Context Adaptation to Concept Drift for Learned Database Operations</title>
      <link>https://arxiv.org/abs/2505.04404</link>
      <description>arXiv:2505.04404v2 Announce Type: replace 
Abstract: Machine learning has demonstrated transformative potential for database operations, such as query optimization and in-database data analytics. However, dynamic database environments, characterized by frequent updates and evolving data distributions, introduce concept drift, which leads to performance degradation for learned models and limits their practical applicability. Addressing this challenge requires efficient frameworks capable of adapting to shifting concepts while minimizing the overhead of retraining or fine-tuning.
  In this paper, we propose FLAIR, an online adaptation framework that introduces a new paradigm called \textit{in-context adaptation} for learned database operations. FLAIR leverages the inherent property of data systems, i.e., immediate availability of execution results for predictions, to enable dynamic context construction. By formalizing adaptation as $f:(\mathbf{x} \,| \,C_t) \to \mathbf{y}$, with $C_t$ representing a dynamic context memory, FLAIR delivers predictions aligned with the current concept, eliminating the need for runtime parameter optimization. To achieve this, FLAIR integrates two key modules: a Task Featurization Module for encoding task-specific features into standardized representations, and a Dynamic Decision Engine, pre-trained via Bayesian meta-training, to adapt seamlessly using contextual information at runtime. Extensive experiments across key database tasks demonstrate that FLAIR outperforms state-of-the-art baselines, achieving up to 5.2x faster adaptation and reducing error by 22.5% for cardinality estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04404v2</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaqi Zhu, Shaofeng Cai, Yanyan Shen, Gang Chen, Fang Deng, Beng Chin Ooi</dc:creator>
    </item>
  </channel>
</rss>
