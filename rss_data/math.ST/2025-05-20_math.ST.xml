<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 21 May 2025 04:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Minimax Rates of Estimation for Optimal Transport Map between Infinite-Dimensional Spaces</title>
      <link>https://arxiv.org/abs/2505.13570</link>
      <description>arXiv:2505.13570v1 Announce Type: new 
Abstract: We investigate the estimation of an optimal transport map between probability measures on an infinite-dimensional space and reveal its minimax optimal rate. Optimal transport theory defines distances within a space of probability measures, utilizing an optimal transport map as its key component. Estimating the optimal transport map from samples finds several applications, such as simulating dynamics between probability measures and functional data analysis. However, some transport maps on infinite-dimensional spaces require exponential-order data for estimation, which undermines their applicability. In this paper, we investigate the estimation of an optimal transport map between infinite-dimensional spaces, focusing on optimal transport maps characterized by the notion of $\gamma$-smoothness. Consequently, we show that the order of the minimax risk is polynomial rate in the sample size even in the infinite-dimensional setup. We also develop an estimator whose estimation error matches the minimax optimal rate. With these results, we obtain a class of reasonably estimable optimal transport maps on infinite-dimensional spaces and a method for their estimation. Our experiments validate the theory and practical utility of our approach with application to functional data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13570v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Donlapark Ponnoprat, Masaaki Imaizumi</dc:creator>
    </item>
    <item>
      <title>Characterization of Efficient Influence Function for Off-Policy Evaluation Under Optimal Policies</title>
      <link>https://arxiv.org/abs/2505.13809</link>
      <description>arXiv:2505.13809v1 Announce Type: new 
Abstract: Off-policy evaluation (OPE) provides a powerful framework for estimating the value of a counterfactual policy using observational data, without the need for additional experimentation. Despite recent progress in robust and efficient OPE across various settings, rigorous efficiency analysis of OPE under an estimated optimal policy remains limited. In this paper, we establish a concise characterization of the efficient influence function for the value function under optimal policy within canonical Markov decision process models. Specifically, we provide the sufficient conditions for the existence of the efficient influence function and characterize its expression. We also give the conditions under which the EIF does not exist.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13809v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyu Wei</dc:creator>
    </item>
    <item>
      <title>Information bounds for inference in stochastic evolution equations observed under noise</title>
      <link>https://arxiv.org/abs/2505.14051</link>
      <description>arXiv:2505.14051v1 Announce Type: new 
Abstract: We consider statistics for stochastic evolution equations in Hilbert space with emphasis on stochastic partial differential equations (SPDEs). We observe a solution process under additional measurement errors and want to estimate a real or functional parameter in the drift. Main targets of estimation are the diffusivity, transport or source coefficient in a parabolic SPDE. By bounding the Hellinger distance between observation laws under different parameters we derive lower bounds on the estimation error, which reveal the underlying information structure. The estimation rates depend on the measurement noise level, the observation time, the covariance of the dynamic noise, the dimension and the order, at which the parametrised coefficient appears in the differential operator. A general estimation procedure attains these rates in many parametric cases and proves their minimax optimality. For nonparametric estimation problems, where the parameter is an unknown function, the lower bounds exhibit an even more complex information structure. The proofs are to a large extent based on functional calculus, perturbation theory and monotonicity of the semigroup generators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14051v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gregor Pasemann, Markus Rei{\ss}</dc:creator>
    </item>
    <item>
      <title>A Characterization of a Subclass of Separate Ratio-Type Copulas</title>
      <link>https://arxiv.org/abs/2505.14058</link>
      <description>arXiv:2505.14058v1 Announce Type: new 
Abstract: Copulas are essential tools in statistics and probability theory, enabling the study of the dependence structure between random variables independently of their marginal distributions. Among the various types of copulas, Ratio-Type Copulas have gained significant attention due to their flexibility in modeling joint distributions. This paper focuses on Separate Ratio-Type Copulas, where the dependence function is a separate product of univariate functions. We revisit a theorem characterizing the validity of these copulas under certain assumptions, generalize it to broader settings, and examine the conditions for reversing the theorem in the case of concave generating functions. To address its limitations, we propose new assumptions that ensure the validity of separate copulas under specific conditions. These results refine the theoretical framework for separate copulas, extending their applicability to pure mathematics and applied fields such as finance, risk management, and machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14058v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziad Adwan, Nicola Sottocornola</dc:creator>
    </item>
    <item>
      <title>Sample Complexity of Correlation Detection in the Gaussian Wigner Model</title>
      <link>https://arxiv.org/abs/2505.14138</link>
      <description>arXiv:2505.14138v1 Announce Type: new 
Abstract: Correlation analysis is a fundamental step in uncovering meaningful insights from complex datasets. In this paper, we study the problem of detecting correlations between two random graphs following the Gaussian Wigner model with unlabeled vertices. Specifically, the task is formulated as a hypothesis testing problem: under the null hypothesis, the two graphs are independent, while under the alternative hypothesis, they are edge-correlated through a latent vertex permutation, yet maintain the same marginal distributions as under the null. We focus on the scenario where two induced subgraphs, each with a fixed number of vertices, are sampled. We determine the optimal rate for the sample size required for correlation detection, derived through an analysis of the conditional second moment. Additionally, we propose an efficient approximate algorithm that significantly reduces running time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14138v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dong Huang, Pengkun Yang</dc:creator>
    </item>
    <item>
      <title>Adaptive Estimation of the Transition Density of Controlled Markov Chains</title>
      <link>https://arxiv.org/abs/2505.14458</link>
      <description>arXiv:2505.14458v1 Announce Type: new 
Abstract: Estimating the transition dynamics of controlled Markov chains is crucial in fields such as time series analysis, reinforcement learning, and system exploration. Traditional non-parametric density estimation methods often assume independent samples and require oracle knowledge of smoothness parameters like the H\"older continuity coefficient. These assumptions are unrealistic in controlled Markovian settings, especially when the controls are non-Markovian, since such parameters need to hold uniformly over all control values. To address this gap, we propose an adaptive estimator for the transition densities of controlled Markov chains that does not rely on prior knowledge of smoothness parameters or assumptions about the control sequence distribution. Our method builds upon recent advances in adaptive density estimation by selecting an estimator that minimizes a loss function {and} fitting the observed data well, using a constrained minimax criterion over a dense class of estimators. We validate the performance of our estimator through oracle risk bounds, employing both randomized and deterministic versions of the Hellinger distance as loss functions. This approach provides a robust and flexible framework for estimating transition densities in controlled Markovian systems without imposing strong assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14458v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Imon Banerjee, Vinayak Rao, Harsha Honnappa</dc:creator>
    </item>
    <item>
      <title>An Asymptotic Equation Linking WAIC and WBIC in Singular Models</title>
      <link>https://arxiv.org/abs/2505.13902</link>
      <description>arXiv:2505.13902v1 Announce Type: cross 
Abstract: In statistical learning, models are classified as regular or singular depending on whether the mapping from parameters to probability distributions is injective. Most models with hierarchical structures or latent variables are singular, for which conventional criteria such as the Akaike Information Criterion and the Bayesian Information Criterion are inapplicable due to the breakdown of normal approximations for the likelihood and posterior. To address this, the Widely Applicable Information Criterion (WAIC) and the Widely Applicable Bayesian Information Criterion (WBIC) have been proposed. Since WAIC and WBIC are computed using posterior distributions at different temperature settings, separate posterior sampling is generally required. In this paper, we theoretically derive an asymptotic equation that links WAIC and WBIC, despite their dependence on different posteriors. This equation yields an asymptotically unbiased expression of WAIC in terms of the posterior distribution used for WBIC. The result clarifies the structural relationship between these criteria within the framework of singular learning theory, and deepens understanding of their asymptotic behavior. This theoretical contribution provides a foundation for future developments in the computational efficiency of model selection in singular models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13902v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naoki Hayashi, Takuro Kutsuna, Sawa Takamuku</dc:creator>
    </item>
    <item>
      <title>Regularized least squares learning with heavy-tailed noise is minimax optimal</title>
      <link>https://arxiv.org/abs/2505.14214</link>
      <description>arXiv:2505.14214v1 Announce Type: cross 
Abstract: This paper examines the performance of ridge regression in reproducing kernel Hilbert spaces in the presence of noise that exhibits a finite number of higher moments. We establish excess risk bounds consisting of subgaussian and polynomial terms based on the well known integral operator framework. The dominant subgaussian component allows to achieve convergence rates that have previously only been derived under subexponential noise - a prevalent assumption in related work from the last two decades. These rates are optimal under standard eigenvalue decay conditions, demonstrating the asymptotic robustness of regularized least squares against heavy-tailed noise. Our derivations are based on a Fuk-Nagaev inequality for Hilbert-space valued random variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14214v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mattes Mollenhauer, Nicole M\"ucke, Dimitri Meunier, Arthur Gretton</dc:creator>
    </item>
    <item>
      <title>Fisher-Rao distances between finite energy signals in noise</title>
      <link>https://arxiv.org/abs/2505.14611</link>
      <description>arXiv:2505.14611v1 Announce Type: cross 
Abstract: This paper proposes to represent finite-energy signals observed in agiven bandwidth as parameters of a probability distribution, and use the information geometrical framework to compute the Fisher-Rao distance between these signals, seen as distributions. The observations are represented by their discrete Fourier transform, which are modeled as complex Gaussian vectors with fixed diagonal covariance matrix and parametrized means. The parameters define the coordinate system of a statistical manifold. This work investigates the possibility of obtaining closed-form expressions for the Fisher-Rao distance. We study two cases: the general case representing any finite energy signal observed in a given bandwidth and a parametrized example of observing an attenuated signal with a known magnitude spectrum and unknown phase spectrum, and we calculate the Fisher-Rao distances for both cases. The finite energy signal manifold corresponds to the manifold of the Gaussian distribution with a known covariance matrix, and the manifold of known magnitude spectrum signals is a submanifold. We derive the expressions for the Christoffel symbols and the tensorial equations of the geodesics. This leads to geodesic equations expressed as second order differential equations. We show that the tensor differential equations can be transformed into matrix equations. These equations depend on the parametric model but simplify to only two vectorial equations, which combine the magnitude and phase of the signal and their gradients with respect to the parameters. We compute closed-form expressions of the Fisher-Rao distances for both studied cases and show that the submanifold is non-geodesic, indicating that the Fisher-Rao distance measured within the submanifold is greater than in the full manifold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14611v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Franck Florin</dc:creator>
    </item>
    <item>
      <title>Determining classes for generalized $\psi$-estimators</title>
      <link>https://arxiv.org/abs/2402.10817</link>
      <description>arXiv:2402.10817v2 Announce Type: replace 
Abstract: We prove that the values of a generalized $\psi$-estimator (introduced by Barczy and P\'ales in 2025) on samples of arbitrary length but having only two different observations uniquely determine the values of the estimator on any sample of arbitrary length without any restriction on the number of different observations. In other words, samples of arbitrary length but having only two different observations form a determining class for generalized $\psi$-estimators. We also obtain a similar statement for the comparison of generalized $\psi$-estimators using comparative functions, and, as a corollary of this result, we derive the Schweitzer's inequality (also called Kantorovich's inequality).</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10817v2</guid>
      <category>math.ST</category>
      <category>math.CA</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matyas Barczy, Zsolt P\'ales</dc:creator>
    </item>
    <item>
      <title>Fractional interacting particle system: drift parameter estimation via Malliavin calculus</title>
      <link>https://arxiv.org/abs/2502.06514</link>
      <description>arXiv:2502.06514v2 Announce Type: replace 
Abstract: We address the problem of estimating the drift parameter in a system of $N$ interacting particles driven by additive fractional Brownian motion of Hurst index \( H \geq 1/2 \). Considering continuous observation of the interacting particles over a fixed interval \([0, T]\), we examine the asymptotic regime as \( N \to \infty \). Our main tool is a random variable reminiscent of the least squares estimator but unobservable due to its reliance on the Skorohod integral. We demonstrate that this object is consistent and asymptotically normal by establishing a quantitative propagation of chaos for Malliavin derivatives, which holds for any \( H \in (0,1) \). Leveraging a connection between the divergence integral and the Young integral, we construct computable estimators of the drift parameter. These estimators are shown to be consistent and asymptotically Gaussian. Finally, a numerical study highlights the strong performance of the proposed estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06514v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Amorino, Ivan Nourdin, Radomyra Shevchenko</dc:creator>
    </item>
    <item>
      <title>Global Activity Scores</title>
      <link>https://arxiv.org/abs/2505.00711</link>
      <description>arXiv:2505.00711v2 Announce Type: replace 
Abstract: We introduce a new global sensitivity measure called global activity score. The new measure is obtained from the global active subspace method, similar to the way the activity score measure is obtained from the active subspace method. We present theoretical results on the relationship between Sobol' sensitivity indices and global activity scores. We present numerical examples where we compare the results of the global sensitivity analysis of some models using Sobol' sensitivity indices, derivative-based sensitivity measures, activity scores, and global activity scores. The numerical results reveal the scenarios when the global activity score has advantages over derivative-based sensitivity measures and activity scores, and when the three measures give similar results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00711v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruilong Yue, Giray \"Okten</dc:creator>
    </item>
    <item>
      <title>Sequential Kernelized Independence Testing</title>
      <link>https://arxiv.org/abs/2212.07383</link>
      <description>arXiv:2212.07383v4 Announce Type: replace-cross 
Abstract: Independence testing is a classical statistical problem that has been extensively studied in the batch setting when one fixes the sample size before collecting data. However, practitioners often prefer procedures that adapt to the complexity of a problem at hand instead of setting sample size in advance. Ideally, such procedures should (a) stop earlier on easy tasks (and later on harder tasks), hence making better use of available resources, and (b) continuously monitor the data and efficiently incorporate statistical evidence after collecting new data, while controlling the false alarm rate. Classical batch tests are not tailored for streaming data: valid inference after data peeking requires correcting for multiple testing which results in low power. Following the principle of testing by betting, we design sequential kernelized independence tests that overcome such shortcomings. We exemplify our broad framework using bets inspired by kernelized dependence measures, e.g., the Hilbert-Schmidt independence criterion. Our test is also valid under non-i.i.d., time-varying settings. We demonstrate the power of our approaches on both simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.07383v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aleksandr Podkopaev, Patrick Bl\"obaum, Shiva Prasad Kasiviswanathan, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Nonlinear Meta-Learning Can Guarantee Faster Rates</title>
      <link>https://arxiv.org/abs/2307.10870</link>
      <description>arXiv:2307.10870v5 Announce Type: replace-cross 
Abstract: Many recent theoretical works on \emph{meta-learning} aim to achieve guarantees in leveraging similar representational structures from related tasks towards simplifying a target task. The main aim of theoretical guarantees on the subject is to establish the extent to which convergence rates -- in learning a common representation -- \emph{may scale with the number $N$ of tasks} (as well as the number of samples per task). First steps in this setting demonstrate this property when both the shared representation amongst tasks, and task-specific regression functions, are linear. This linear setting readily reveals the benefits of aggregating tasks, e.g., via averaging arguments. In practice, however, the representation is often highly nonlinear, introducing nontrivial biases in each task that cannot easily be averaged out as in the linear case. In the present work, we derive theoretical guarantees for meta-learning with nonlinear representations. In particular, assuming the shared nonlinearity maps to an infinite dimensional reproducing kernel Hilbert space, we show that additional biases can be mitigated with careful regularization that leverages the smoothness of task-specific regression functions, yielding improved rates that scale with the number of tasks as desired.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10870v5</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dimitri Meunier, Zhu Li, Arthur Gretton, Samory Kpotufe</dc:creator>
    </item>
    <item>
      <title>Fine-Grained Uncertainty Quantification via Collisions</title>
      <link>https://arxiv.org/abs/2411.12127</link>
      <description>arXiv:2411.12127v3 Announce Type: replace-cross 
Abstract: We propose a new and intuitive metric for aleatoric uncertainty quantification (UQ), the prevalence of class collisions defined as the same input being observed in different classes. We use the rate of class collisions to define the collision matrix, a novel and uniquely fine-grained measure of uncertainty. For a classification problem involving $K$ classes, the $K\times K$ collision matrix $S$ measures the inherent difficulty in distinguishing between each pair of classes. We discuss several applications of the collision matrix, establish its fundamental mathematical properties, as well as show its relationship with existing UQ methods, including the Bayes error rate (BER). We also address the new problem of estimating the collision matrix using one-hot labeled data by proposing a series of innovative techniques to estimate $S$. First, we learn a pair-wise contrastive model which accepts two inputs and determines if they belong to the same class. We then show that this contrastive model (which is PAC learnable) can be used to estimate the Gramian matrix of $S$, defined as $G=S^TS$. Finally, we show that under reasonable assumptions, $G$ can be used to uniquely recover $S$, a new result on non-negative matrices which could be of independent interest. With a method to estimate $S$ established, we demonstrate how this estimate of $S$, in conjunction with the contrastive model, can be used to estimate the posterior class portability distribution of any point. Experimental results are also presented to validate our methods of estimating the collision matrix and class posterior distributions on several datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12127v3</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesse Friedbaum, Sudarshan Adiga, Ravi Tandon</dc:creator>
    </item>
    <item>
      <title>A new approach to locally adaptive polynomial regression</title>
      <link>https://arxiv.org/abs/2412.19802</link>
      <description>arXiv:2412.19802v2 Announce Type: replace-cross 
Abstract: Adaptive bandwidth selection is a fundamental challenge in nonparametric regression. This paper introduces a new bandwidth selection procedure inspired by the optimality criteria for $\ell_0$-penalized regression. Although similar in spirit to Lepski's method and its variants in selecting the largest interval satisfying an admissibility criterion, our approach stems from a distinct philosophy, utilizing criteria based on $\ell_2$-norms of interval projections rather than explicit point and variance estimates. We obtain non-asymptotic risk bounds for the local polynomial regression methods based on our bandwidth selection procedure which adapt (near-)optimally to the local H\"{o}lder exponent of the underlying regression function simultaneously at all points in its domain. Furthermore, we show that there is a single ideal choice of a global tuning parameter in each case under which the above-mentioned local adaptivity holds. The optimal risks of our methods derive from the properties of solutions to a new ``bandwidth selection equation'' which is of independent interest. We believe that the principles underlying our approach provide a new perspective to the classical yet ever relevant problem of locally adaptive nonparametric regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19802v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sabyasachi Chatterjee, Subhajit Goswami, Soumendu Sundar Mukherjee</dc:creator>
    </item>
    <item>
      <title>CARROT: A Cost Aware Rate Optimal Router</title>
      <link>https://arxiv.org/abs/2502.03261</link>
      <description>arXiv:2502.03261v2 Announce Type: replace-cross 
Abstract: With the rapid growth in the number of Large Language Models (LLMs), there has been a recent interest in LLM routing, or directing queries to the cheapest LLM that can deliver a suitable response. We conduct a minimax analysis of the routing problem, providing a lower bound and finding that a simple router that predicts both cost and accuracy for each question can be minimax optimal. Inspired by this, we introduce CARROT, a Cost AwaRe Rate Optimal rouTer that selects a model based on estimates of the models' cost and performance. Alongside CARROT, we also introduce the Smart Price-aware ROUTing (SPROUT) dataset to facilitate routing on a wide spectrum of queries with the latest state-of-the-art LLMs. Using SPROUT and prior benchmarks such as Routerbench and open-LLM-leaderboard-v2 we empirically validate CARROT's performance against several alternative routers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03261v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seamus Somerstep, Felipe Maia Polo, Allysson Flavio Melo de Oliveira, Prattyush Mangal, M\'irian Silva, Onkar Bhardwaj, Mikhail Yurochkin, Subha Maity</dc:creator>
    </item>
    <item>
      <title>Beyond Smoothness and Convexity: Optimization via sampling</title>
      <link>https://arxiv.org/abs/2504.02831</link>
      <description>arXiv:2504.02831v2 Announce Type: replace-cross 
Abstract: This work explores a novel perspective on solving nonconvex and nonsmooth optimization problems by leveraging sampling based methods. Instead of treating the objective function purely through traditional (often deterministic) optimization approaches, we view it as inducing a target distribution.We then draw samples from this distribution using Markov Chain Monte Carlo (MCMC) techniques, particularly Langevin Dynamics (LD), to locate regions of low function values. By analyzing the convergence properties of LD in both KL divergence and total variation distance, we establish explicit bounds on how many iterations are required for the induced distribution to approximate the target. We also provide probabilistic guarantees that an appropriately drawn sample will lie within a desired neighborhood of the global minimizer, even when the objective is nonconvex or nonsmooth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02831v2</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nahom Seyoum, Haoxiang You</dc:creator>
    </item>
    <item>
      <title>New affine invariant ensemble samplers and their dimensional scaling</title>
      <link>https://arxiv.org/abs/2505.02987</link>
      <description>arXiv:2505.02987v2 Announce Type: replace-cross 
Abstract: We introduce new affine invariant ensemble samplers that are easy to construct and improve upon existing algorithms, especially for high-dimensional problems. Specifically, we propose a derivative-free ensemble side move sampler that performs favorably compared to popular samplers in the $\texttt{emcee}$ package. Additionally, we develop a class of derivative-based ensemble Hamiltonian Monte Carlo (HMC) samplers with affine invariance, which outperform standard HMC without affine invariance when sampling highly skewed distributions. We provide asymptotic scaling analysis for high-dimensional Gaussian targets to further elucidate the properties of these affine invariant ensemble samplers. In particular, with derivative information, the affine invariant ensemble HMC can scale much better with dimension compared to derivative-free ensemble samplers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02987v2</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Chen</dc:creator>
    </item>
    <item>
      <title>Statistically Significant Linear Regression Coefficients Solely Driven By Outliers In Finite-sample Inference</title>
      <link>https://arxiv.org/abs/2505.10738</link>
      <description>arXiv:2505.10738v2 Announce Type: replace-cross 
Abstract: In this paper, we investigate the impact of outliers on the statistical significance of coefficients in linear regression. We demonstrate, through numerical simulation using R, that a single outlier can cause an otherwise insignificant coefficient to appear statistically significant. We compare this with robust Huber regression, which reduces the effects of outliers. Afterwards, we approximate the influence of a single outlier on estimated regression coefficients and discuss common diagnostic statistics to detect influential observations in regression (e.g., studentized residuals). Furthermore, we relate this issue to the optional normality assumption in simple linear regression [14], required for exact finite-sample inference but asymptotically justified for large n by the Central Limit Theorem (CLT). We also address the general dangers of relying solely on p-values without performing adequate regression diagnostics. Finally, we provide a brief overview of regression methods and discuss how they relate to the assumptions of the Gauss-Markov theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10738v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Felix Reichel</dc:creator>
    </item>
  </channel>
</rss>
