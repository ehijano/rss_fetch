<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Dec 2025 05:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Has ACL Lost Its Crown? A Decade-Long Quantitative Analysis of Scale and Impact Across Leading AI Conferences</title>
      <link>https://arxiv.org/abs/2512.04448</link>
      <description>arXiv:2512.04448v1 Announce Type: new 
Abstract: The recent surge of language models has rapidly expanded NLP research, driving an exponential rise in submissions and acceptances at major conferences. Yet this growth has been shadowed by escalating concerns over conference quality, e.g., plagiarism, reviewer inexperience and collusive bidding. However, existing studies rely largely on qualitative accounts (e.g., expert interviews, social media discussions, etc.), lacking longitudinal empirical evidence. To fill this gap, we conduct a ten year empirical study spanning seven leading conferences. We build a four dimensional bibliometric framework covering conference scale, core citation statistics,impact dispersion, cross venue and journal influence, etc. Notably, we further propose a metric Quality Quantity Elasticity, which measures the elasticity of citation growth relative to acceptance growth. Our findings show that ML venues sustain dominant and stable impact, NLP venues undergo widening stratification with mixed expansion efficiency, and AI venues exhibit structural decline. This study provides the first decade-long, cross-venue empirical evidence on the evolution of major conferences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04448v1</guid>
      <category>cs.DL</category>
      <category>cs.CY</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jianglin Ma, Ben Yao, Xiang Li, Yazhou Zhang</dc:creator>
    </item>
    <item>
      <title>Aging and the Narrowing of Scientific Innovation</title>
      <link>https://arxiv.org/abs/2202.04044</link>
      <description>arXiv:2202.04044v4 Announce Type: replace 
Abstract: With rising life expectancies around the world and an older scientific workforce than ever before, what does aging mean for individual scientists, and what do aging scientists mean for scientific progress as a whole? Here we examine how scientists and scholars age in terms of how their ideas and contributions relate to the evolving frontier of knowledge and how demographically aging fields relate to field-level advance. At the individual level, we examine how research experiences and choices can moderate the effects of intellectual aging. At the collective level, we explore mechanisms that link individual and collective aging. Prior research focuses on star scientists, their changing dates and rates of breakthrough success throughout history. We explore this for scientists in all fields over time, drawing upon novel deep learning measurements that allow us not only to trace positive attention through citation but also negative attention through explicit criticism with a novel, comprehensive database of over 20,000 human-validated critical citations. We find that younger scientists tend toward disruptive contributions that push the frontier, while older scientists engage in combinatorial innovation with an aging collection of components. This includes analyzing the impact of the 1994 U.S. Supreme Court ruling on mandatory retirement and examining how unexpected collaborations affect citation patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.04044v4</guid>
      <category>cs.DL</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Haochuan Cui, Yiling Lin, Lingfei Wu, James A. Evans</dc:creator>
    </item>
    <item>
      <title>Introducing multiverse analysis to bibliometrics: The case of team size effects on disruptive research</title>
      <link>https://arxiv.org/abs/2506.03726</link>
      <description>arXiv:2506.03726v2 Announce Type: replace 
Abstract: Although bibliometrics has become an essential tool in the evaluation of research performance, bibliometric analyses are sensitive to a range of methodological choices. Subtle choices in data selection, indicator construction, and modeling decisions can substantially alter results. Ensuring robustness (meaning that findings hold up under different reasonable scenarios) is therefore critical for credible research and research evaluation. To address this issue, this study introduces multiverse analysis to bibliometrics. Multiverse analysis is a statistical tool that enables analysts to transparently discuss modeling assumptions and thoroughly assess model robustness. Whereas standard robustness checks usually cover only a small subset of all plausible models, multiverse analysis includes all plausible models. The benefits of multiverse analysis are illustrated by assessing the robustness of the findings reported by Wu et al. (2019), who observed that small teams tend to produce more disruptive research than large teams. While we found robust evidence of a negative effect of team size on disruption scores, the effect size depends substantially on the model specification. Our findings underscore the importance of assessing the multiverse robustness of bibliometric results to clarify their practical implications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03726v2</guid>
      <category>cs.DL</category>
      <category>stat.AP</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Leibel, Lutz Bornmann</dc:creator>
    </item>
  </channel>
</rss>
