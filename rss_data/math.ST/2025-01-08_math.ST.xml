<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 08 Jan 2025 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Modeling directional monotonicity with copulas</title>
      <link>https://arxiv.org/abs/2501.03350</link>
      <description>arXiv:2501.03350v1 Announce Type: new 
Abstract: The purpose of this paper is to characterize the concept of monotonicity according to a direction related to a set of n random variables in terms of its associated n-copula C. We start establishing relationships in the bivariate and trivariate cases, which help to understand the extension to the multivariate case. Examples of copulas in all the studied cases are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03350v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enrique de Amo, David Garc\'ia-Fern\'andez, Jos\'e Juan Quesada-Molina, Manuel \'Ubeda-Flores</dc:creator>
    </item>
    <item>
      <title>On the Adversarial Robustness of Benjamini Hochberg</title>
      <link>https://arxiv.org/abs/2501.03402</link>
      <description>arXiv:2501.03402v1 Announce Type: new 
Abstract: The Benjamini-Hochberg (BH) procedure is widely used to control the false detection rate (FDR) in multiple testing. Applications of this control abound in drug discovery, forensics, anomaly detection, and, in particular, machine learning, ranging from nonparametric outlier detection to out-of-distribution detection and one-class classification methods. Considering this control could be relied upon in critical safety/security contexts, we investigate its adversarial robustness. More precisely, we study under what conditions BH does and does not exhibit adversarial robustness, we present a class of simple and easily implementable adversarial test-perturbation algorithms, and we perform computational experiments. With our algorithms, we demonstrate that there are conditions under which BH's control can be significantly broken with relatively few (even just one) test score perturbation(s), and provide non-asymptotic guarantees on the expected adversarial-adjustment to FDR. Our technical analysis involves a combinatorial reframing of the BH procedure as a ``balls into bins'' process, and drawing a connection to generalized ballot problems to facilitate an information-theoretic approach for deriving non-asymptotic lower bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03402v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Louis L Chen, Roberto Szechtman, Matan Seri</dc:creator>
    </item>
    <item>
      <title>Auxiliary Learning and its Statistical Understanding</title>
      <link>https://arxiv.org/abs/2501.03463</link>
      <description>arXiv:2501.03463v1 Announce Type: new 
Abstract: Modern statistical analysis often encounters high-dimensional problems but with a limited sample size. It poses great challenges to traditional statistical estimation methods. In this work, we adopt auxiliary learning to solve the estimation problem in high-dimensional settings. We start with the linear regression setup. To improve the statistical efficiency of the parameter estimator for the primary task, we consider several auxiliary tasks, which share the same covariates with the primary task. Then a weighted estimator for the primary task is developed, which is a linear combination of the ordinary least squares estimators of both the primary task and auxiliary tasks. The optimal weight is analytically derived and the statistical properties of the corresponding weighted estimator are studied. We then extend the weighted estimator to generalized linear regression models. Extensive numerical experiments are conducted to verify our theoretical results. Last, a deep learning-related real-data example of smart vending machines is presented for illustration purposes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03463v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanchao Yan, Feifei Wang, Chuanxin Xia, Hansheng Wang</dc:creator>
    </item>
    <item>
      <title>Finite-sample properties of the trimmed mean</title>
      <link>https://arxiv.org/abs/2501.03694</link>
      <description>arXiv:2501.03694v1 Announce Type: new 
Abstract: The trimmed mean of $n$ scalar random variables from a distribution $P$ is the variant of the standard sample mean where the $k$ smallest and $k$ largest values in the sample are discarded for some parameter $k$. In this paper, we look at the finite-sample properties of the trimmed mean as an estimator for the mean of $P$. Assuming finite variance, we prove that the trimmed mean is ``sub-Gaussian'' in the sense of achieving Gaussian-type concentration around the mean. Under slightly stronger assumptions, we show the left and right tails of the trimmed mean satisfy a strong ratio-type approximation by the corresponding Gaussian tail, even for very small probabilities of the order $e^{-n^c}$ for some $c&gt;0$. In the more challenging setting of weaker moment assumptions and adversarial sample contamination, we prove that the trimmed mean is minimax-optimal up to constants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03694v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roberto I. Oliveira, Paulo Orenstein, Zoraida F. Rico</dc:creator>
    </item>
    <item>
      <title>Fixed and Random Covariance Regression Analyses</title>
      <link>https://arxiv.org/abs/2501.03753</link>
      <description>arXiv:2501.03753v1 Announce Type: new 
Abstract: Covariance regression analysis is an approach to linking the covariance of responses to a set of explanatory variables $X$, where $X$ can be a vector, matrix, or tensor. Most of the literature on this topic focuses on the "Fixed-$X$" setting and treats $X$ as nonrandom. By contrast, treating explanatory variables $X$ as random, namely the "Random-$X$" setting, is often more realistic in practice. This article aims to fill this gap in the literature on the estimation and model assessment theory for Random-$X$ covariance regression models. Specifically, we construct a new theoretical framework for studying the covariance estimators under the Random-$X$ setting, and we demonstrate that the quasi-maximum likelihood estimator and the weighted least squares estimator are both consistent and asymptotically normal. In addition, we develop pioneering work on the model assessment theory of covariance regression. In particular, we obtain the bias-variance decompositions for the expected test errors under both the Fixed-$X$ and Random-$X$ settings. We show that moving from a Fixed-$X$ to a Random-$X$ setting can increase both the bias and the variance in expected test errors. Subsequently, we propose estimators of the expected test errors under the Fixed-$X$ and Random-$X$ settings, which can be used to assess the performance of the competing covariance regression models. The proposed estimation and model assessment approaches are illustrated via extensive simulation experiments and an empirical study of stock returns in the US market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03753v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tao Zou, Wei Lan, Runze Li, Chih-Ling Tsai</dc:creator>
    </item>
    <item>
      <title>Sequentializing a Test: Anytime Validity is Free</title>
      <link>https://arxiv.org/abs/2501.03982</link>
      <description>arXiv:2501.03982v1 Announce Type: new 
Abstract: An anytime valid sequential test permits us to peek at observations as they arrive. This means we can stop, continue or adapt the testing process based on the current data, without invalidating the inference. Given a maximum number of observations $N$, one may believe that this benefit must be paid for in terms of power when compared to a conventional test that waits until all $N$ observations have arrived. Our key contribution is to show that this is false: for any valid test based on $N$ observations, we derive an anytime valid sequential test that matches it after $N$ observations. In addition, we show that the value of the sequential test before a rejection is attained can be directly used as a significance level for a subsequent test. We illustrate this for the $z$-test. There, we find that the current state-of-the-art based on log-optimal $e$-values can be obtained as a special limiting case that replicates a $z$-test with level $\alpha \to 0$ as $N \to \infty$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03982v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick W. Koning, Sam van Meer</dc:creator>
    </item>
    <item>
      <title>An explicit link between graphical models and Gaussian Markov random fields on metric graphs</title>
      <link>https://arxiv.org/abs/2501.03701</link>
      <description>arXiv:2501.03701v1 Announce Type: cross 
Abstract: We derive an explicit link between Gaussian Markov random fields on metric graphs and graphical models, and in particular show that a Markov random field restricted to the vertices of the graph is, under mild regularity conditions, a Gaussian graphical model with a distribution which is faithful to its pairwise independence graph, which coincides with the neighbor structure of the metric graph. This is used to show that there are no Gaussian random fields on general metric graphs which are both Markov and isotropic in some suitably regular metric on the graph, such as the geodesic or resistance metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03701v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Bolin, Alexandre B. Simas, Jonas Wallin</dc:creator>
    </item>
    <item>
      <title>Concentration of Empirical First-Passage Times</title>
      <link>https://arxiv.org/abs/2501.03810</link>
      <description>arXiv:2501.03810v1 Announce Type: cross 
Abstract: First-passage properties are central to the kinetics of target-search processes. Theoretical approaches so far primarily focused on predicting first-passage statistics for a given process or model. In practice, however, one faces the reverse problem of inferring first-passage statistics from, typically sub-sampled, experimental or simulation data. Obtaining trustworthy estimates from under-sampled data and unknown underlying dynamics remains a daunting task, and the assessment of the uncertainty is imperative. In this chapter, we highlight recent progress in understanding and controlling finite-sample effects in empirical first-passage times of reversible Markov processes. Precisely, we present concentration inequalities bounding from above the deviations of the sample mean for any sample size from the true mean first-passage time and construct non-asymptotic confidence intervals. Moreover, we present two-sided bounds on the range of fluctuations, i.e, deviations of the expected maximum and minimum from the mean in any given sample, which control uncertainty even in situations where the mean is a priori not a sufficient statistic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03810v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.soft</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-67802-8_2</arxiv:DOI>
      <dc:creator>Rick Bebon, Aljaz Godec</dc:creator>
    </item>
    <item>
      <title>Data Models With Two Manifestations of Imprecision</title>
      <link>https://arxiv.org/abs/2404.09741</link>
      <description>arXiv:2404.09741v2 Announce Type: replace 
Abstract: Motivated by recently emerging problems in machine learning and statistics, we propose data models which relax the familiar i.i.d. assumption. In essence, we seek to understand what it means for data to come from a set of probability measures. We show that our frequentist data models, parameterized by such sets, manifest two aspects of imprecision. We characterize the intricate interplay of these manifestations, aggregate (ir)regularity and local (ir)regularity, where a much richer set of behaviours compared to an i.i.d. model is possible. In doing so we shed new light on the relationship between non-stationary, locally precise and stationary, locally imprecise data models. We discuss possible applications of these data models in machine learning and how the set of probabilities can be estimated. For the estimation of aggregate irregularity, we provide a negative result but argue that it does not warrant pessimism. Understanding these frequentist aspects of imprecise probabilities paves the way for deriving generalization of proper scoring rules and calibration to the imprecise case, which can then contribute to tackling practical problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09741v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Christian Fr\"ohlich, Robert C. Williamson</dc:creator>
    </item>
    <item>
      <title>Gradient descent inference in empirical risk minimization</title>
      <link>https://arxiv.org/abs/2412.09498</link>
      <description>arXiv:2412.09498v2 Announce Type: replace 
Abstract: Gradient descent is one of the most widely used iterative algorithms in modern statistical learning. However, its precise algorithmic dynamics in high-dimensional settings remain only partially understood, which has therefore limited its broader potential for statistical inference applications.
  This paper provides a precise, non-asymptotic distributional characterization of gradient descent iterates in a broad class of empirical risk minimization problems, in the so-called mean-field regime where the sample size is proportional to the signal dimension. Our non-asymptotic state evolution theory holds for both general non-convex loss functions and non-Gaussian data, and reveals the central role of two Onsager correction matrices that precisely characterize the non-trivial dependence among all gradient descent iterates in the mean-field regime.
  Although the Onsager correction matrices are typically analytically intractable, our state evolution theory facilitates a generic gradient descent inference algorithm that consistently estimates these matrices across a broad class of models. Leveraging this algorithm, we show that the state evolution can be inverted to construct (i) data-driven estimators for the generalization error of gradient descent iterates and (ii) debiased gradient descent iterates for inference of the unknown signal. Detailed applications to two canonical models--linear regression and (generalized) logistic regression--are worked out to illustrate model-specific features of our general theory and inference methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09498v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiyang Han, Xiaocong Xu</dc:creator>
    </item>
    <item>
      <title>Using negative controls to identify causal effects with invalid instrumental variables</title>
      <link>https://arxiv.org/abs/2204.04119</link>
      <description>arXiv:2204.04119v5 Announce Type: replace-cross 
Abstract: Many proposals for the identification of causal effects require an instrumental variable that satisfies strong, untestable unconfoundedness and exclusion restriction assumptions. In this paper, we show how one can potentially identify causal effects under violations of these assumptions by harnessing a negative control population or outcome. This strategy allows one to leverage sup-populations for whom the exposure is degenerate, and requires that the instrument-outcome association satisfies a certain parallel trend condition. We develop the semiparametric efficiency theory for a general instrumental variable model, and obtain a multiply robust, locally efficient estimator of the average treatment effect in the treated. The utility of the estimators is demonstrated in simulation studies and an analysis of the Life Span Study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.04119v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oliver Dukes, David B. Richardson, Zachary Shahn, James M. Robins, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Fast data inversion for high-dimensional dynamical systems from noisy measurements</title>
      <link>https://arxiv.org/abs/2501.01324</link>
      <description>arXiv:2501.01324v2 Announce Type: replace-cross 
Abstract: In this work, we develop a scalable approach for a flexible latent factor model for high-dimensional dynamical systems. Each latent factor process has its own correlation and variance parameters, and the orthogonal factor loading matrix can be either fixed or estimated. We utilize an orthogonal factor loading matrix that avoids computing the inversion of the posterior covariance matrix at each time of the Kalman filter, and derive closed-form expressions in an expectation-maximization algorithm for parameter estimation, which substantially reduces the computational complexity without approximation. Our study is motivated by inversely estimating slow slip events from geodetic data, such as continuous GPS measurements. Extensive simulated studies illustrate higher accuracy and scalability of our approach compared to alternatives. By applying our method to geodetic measurements in the Cascadia region, our estimated slip better agrees with independently measured seismic data of tremor events. The substantial acceleration from our method enables the use of massive noisy data for geological hazard quantification and other applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01324v2</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yizi Lin, Xubo Liu, Paul Segall, Mengyang Gu</dc:creator>
    </item>
  </channel>
</rss>
