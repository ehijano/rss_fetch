<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Feb 2025 02:53:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Matsuoka-Based GARMA Model for Hydrological Forecasting: Theory, Estimation, and Applications</title>
      <link>https://arxiv.org/abs/2502.18645</link>
      <description>arXiv:2502.18645v1 Announce Type: new 
Abstract: Time series in natural sciences, such as hydrology and climatology, and other environmental applications, often consist of continuous observations constrained to the unit interval (0,1). Traditional Gaussian-based models fail to capture these bounds, requiring more flexible approaches. This paper introduces the Matsuoka Autoregressive Moving Average (MARMA) model, extending the GARMA framework by assuming a Matsuoka-distributed random component taking values in (0,1) and an ARMA-like systematic structure allowing for random time-dependent covariates. Parameter estimation is performed via partial maximum likelihood (PMLE), for which we present the asymptotic theory. It enables statistical inference, including confidence intervals and model selection. To construct prediction intervals, we propose a novel bootstrap-based method that accounts for dependence structure uncertainty. A comprehensive Monte Carlo simulation study assesses the finite sample performance of the proposed methodologies, while an application to forecasting the useful water volume of the Guarapiranga Reservoir in Brazil showcases their practical usefulness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18645v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guilherme Pumi, Danilo Hiroshi Matsuoka, Taiane Schaedler Prass, Bruna Gregory Palm</dc:creator>
    </item>
    <item>
      <title>Rejoinder to Reader Reaction "On exact randomization-based covariate-adjusted confidence intervals" by Jacob Fiksel</title>
      <link>https://arxiv.org/abs/2502.18666</link>
      <description>arXiv:2502.18666v1 Announce Type: new 
Abstract: We applaud Fiksel (2024) for their valuable contributions to randomization-based inference, particularly their work on inverting the Fisher randomization test (FRT) to construct confidence intervals using the covariate-adjusted test statistic. FRT is advocated by many scholars because it produces finite-sample exact p-values for any test statistic and can be easily adopted for any experimental design (Rosenberger et al., 2019; Proschan and Dodd, 2019; Young, 2019; Bind and Rubin, 2020). By inverting FRTs, we can construct the randomization-based confidence interval (RBCI). To the best of our knowledge, Zhu and Liu (2023) are the first to analytically invert the FRT for the difference-in-means statistic. Fiksel (2024) extended this analytical approach to the covariate-adjusted statistic, producing a monotonic p-value function under certain conditions. In this rejoinder, we propose an analytical approach to invert the FRT for test statistics that yield a non-monotonic p-value function, with the studentized t-statistic as an important special case. Exploiting our analytical approach, we can recover the non-monotonic p-value function and construct RBCI based on the studentized t-statistic. The RBCI generated by the proposed analytical approach is guaranteed to achieve the desired coverage probability and resolve the contradiction between Luo et al. (2021) and Wu and Ding (2021). Simulation results validate our findings and demonstrate that our method is also computationally efficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18666v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/biomtc/ujae052</arxiv:DOI>
      <dc:creator>Ke Zhu, Hanzhong Liu</dc:creator>
    </item>
    <item>
      <title>bayesNMF: Fast Bayesian Poisson NMF with Automatically Learned Rank Applied to Mutational Signatures</title>
      <link>https://arxiv.org/abs/2502.18674</link>
      <description>arXiv:2502.18674v1 Announce Type: new 
Abstract: Bayesian Non-Negative Matrix Factorization (NMF) is a method of interest across fields including genomics, neuroscience, and audio and image processing. Bayesian Poisson NMF is of particular importance for counts data, for example in cancer mutational signatures analysis. However, MCMC methods for Bayesian Poisson NMF require a computationally intensive augmentation. Further, identifying latent rank is necessary, but commonly used heuristic approaches are slow and potentially subjective, and methods that learn rank automatically are unable to provide posterior uncertainties. In this paper, we introduce bayesNMF, a computationally efficient Gibbs sampler for Bayesian Poisson NMF. The desired Poisson-likelihood NMF is paired with a Normal-likelihood NMF used for high overlap proposal distributions in approximate Metropolis steps, avoiding augmentation. We additionally define Bayesian factor inclusion (BFI) and sparse Bayesian factor inclusion (SBFI) as methods to identify rank automatically while preserving posterior uncertainty quantification on the learned matrices. We provide an open-source R software package with all models and plotting capabilities demonstrated in this paper on GitHub at jennalandy/bayesNMF. While our applications focus on mutational signatures, our software and results can be extended to any use of Bayesian Poisson NMF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18674v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jenna M. Landy, Nishanth Basava, Giovanni Parmigiani</dc:creator>
    </item>
    <item>
      <title>An Accurate Computational Approach for Partial Likelihood Using Poisson-Binomial Distributions</title>
      <link>https://arxiv.org/abs/2502.18715</link>
      <description>arXiv:2502.18715v1 Announce Type: new 
Abstract: In a Cox model, the partial likelihood, as the product of a series of conditional probabilities, is used to estimate the regression coefficients. In practice, those conditional probabilities are approximated by risk score ratios based on a continuous time model, and thus result in parameter estimates from only an approximate partial likelihood. Through a revisit to the original partial likelihood idea, an accurate partial likelihood computing method for the Cox model is proposed, which calculates the exact conditional probability using the Poisson-binomial distribution. New estimating and inference procedures are developed, and theoretical results are established for the proposed computational procedure. Although ties are common in real studies, current theories for the Cox model mostly do not consider cases for tied data. In contrast, the new approach includes the theory for grouped data, which allows ties, and also includes the theory for continuous data without ties, providing a unified framework for computing partial likelihood for data with or without ties. Numerical results show that the proposed method outperforms current methods in reducing bias and mean squared error, while achieving improved confidence interval coverage rates, especially when there are many ties or when the variability in risk scores is large. Comparisons between methods in real applications have been made.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18715v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Youngjin Cho, Yili Hong, Pang Du</dc:creator>
    </item>
    <item>
      <title>On Robust Aggregation for Distributed Data</title>
      <link>https://arxiv.org/abs/2502.18740</link>
      <description>arXiv:2502.18740v1 Announce Type: new 
Abstract: When data are stored across multiple locations, directly pooling all the data together for statistical analysis may be impossible due to communication costs and privacy concerns. Distributed computing systems allow the analysis of such data, by getting local servers to separately process their own statistical analyses and using a central processor to aggregate the local statistical results. Naive aggregation of local statistics using simple or weighted averages, is vulnerable to contamination within a distributed computing system. This paper develops and investigates a Huber-type aggregation method for locally computed M-estimators to handle contamination in the local estimates. Our implementation of this aggregation method requires estimating the asymptotic variance-covariance matrix of the M-estimator, which we accomplish using a robust spatial median approach. Theoretically, the Huber-type aggregation achieves the same convergence rate as if all the data were pooled. We establish its asymptotic normality for making inferences, including justifying a two-step approach for detecting contamination in the distributed computing system. Extensive simulation studies are conducted to validate the theoretical results and the usefulness of our proposed approach is demonstrated on U.S. airline data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18740v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xian Li, Xuan Liang, A. H. Welsh, Tao Zou</dc:creator>
    </item>
    <item>
      <title>One-at-a-time knockoffs: controlled false discovery rate with higher power</title>
      <link>https://arxiv.org/abs/2502.18750</link>
      <description>arXiv:2502.18750v1 Announce Type: new 
Abstract: We propose one-at-a-time knockoffs (OATK), a new methodology for detecting important explanatory variables in linear regression models while controlling the false discovery rate (FDR). For each explanatory variable, OATK generates a knockoff design matrix that preserves the Gram matrix by replacing one-at-a-time only the single corresponding column of the original design matrix. OATK is a substantial relaxation and simplification of the knockoff filter by Barber and Cand\`es (BC), which simultaneously generates all columns of the knockoff design matrix to satisfy a much larger set of constraints. To test each variable's importance, statistics are then constructed by comparing the original vs. knockoff coefficients. Under a mild correlation assumption on the original design matrix, OATK asymptotically controls the FDR at any desired level. Moreover, OATK consistently achieves (often substantially) higher power than BC and other approaches across a variety of simulation examples and a real genetics dataset. Generating knockoffs one-at-a-time also has substantial computational advantages and facilitates additional enhancements, such as conditional calibration or derandomization, to further improve power and consistency of FDR control. OATK can be viewed as the conditional randomization test (CRT) generalized to fixed-design linear regression problems, and can generate fine-grained p-values for each hypothesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18750v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charlie K. Guan, Zhimei Ren, Daniel W. Apley</dc:creator>
    </item>
    <item>
      <title>Validating uncertainty propagation approaches for two-stage Bayesian spatial models using simulation-based calibration</title>
      <link>https://arxiv.org/abs/2502.18962</link>
      <description>arXiv:2502.18962v1 Announce Type: new 
Abstract: This work tackles the problem of uncertainty propagation in two-stage Bayesian models, with a focus on spatial applications. A two-stage modeling framework has the advantage of being more computationally efficient than a fully Bayesian approach when the first-stage model is already complex in itself, and avoids the potential problem of unwanted feedback effects. Two ways of doing two-stage modeling are the crude plug-in method and the posterior sampling method. The former ignores the uncertainty in the first-stage model, while the latter can be computationally expensive. This paper validates the two aforementioned approaches and proposes a new approach to do uncertainty propagation, which we call the $\mathbf{Q}$ uncertainty method, implemented using the Integrated Nested Laplace Approximation (INLA). We validate the different approaches using the simulation-based calibration method, which tests the self-consistency property of Bayesian models. Results show that the crude plug-in method underestimates the true posterior uncertainty in the second-stage model parameters, while the resampling approach and the proposed method are correct. We illustrate the approaches in a real life data application which aims to link relative humidity and Dengue cases in the Philippines for August 2018.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18962v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephen Jun Villejo, Sara Martino, Janine Illian, William Ryan, Finn Lindgren</dc:creator>
    </item>
    <item>
      <title>Sparkle: A Statistical Learning Toolkit for High-Dimensional Hawkes Processes in Python</title>
      <link>https://arxiv.org/abs/2502.18979</link>
      <description>arXiv:2502.18979v1 Announce Type: new 
Abstract: This paper introduces Sparkle, a statistical learning toolkit for Hawkes processes in Python, designed to bring together efficiency and ease of use. The purpose of this package is to provide the Python community with a complete suite of cutting-edge tools specifically tailored for the study of exponential Hawkes processes, with a particular focus on highdimensional framework. It includes state-of-the-art estimation tools with built-in support for incorporating regularization techniques, and novel classification methods. To enhance computational performance, Sparkle leverages a high-performance C++ core for intensive tasks. This dual-language approach makes Sparkle a powerful solution for computationally demanding real-world applications. Here, we present its implementation framework and provide illustrative examples, demonstrating its capabilities and practical usage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18979v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Romain Edmond Lacoste (LAMA)</dc:creator>
    </item>
    <item>
      <title>Scalable skewed Bayesian inference for latent Gaussian models</title>
      <link>https://arxiv.org/abs/2502.19083</link>
      <description>arXiv:2502.19083v1 Announce Type: new 
Abstract: Approximate Bayesian inference for the class of latent Gaussian models can be achieved efficiently with integrated nested Laplace approximations (INLA). Based on recent reformulations in the INLA methodology, we propose a further extension that is necessary in some cases like heavy-tailed likelihoods or binary regression with imbalanced data. This extension formulates a skewed version of the Laplace method such that some marginals are skewed and some are kept Gaussian while the dependence is maintained with the Gaussian copula from the Laplace method. Our approach is formulated to be scalable in model and data size, using a variational inferential framework enveloped in INLA. We illustrate the necessity and performance using simulated cases, as well as a case study of a rare disease where class imbalance is naturally present.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19083v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shourya Dutta, Janet van Niekerk, Haavard Rue</dc:creator>
    </item>
    <item>
      <title>Utility-Based Dose Optimization Approaches for Multiple-Dose Randomized Trial Designs Accounting for Multiple Endpoints</title>
      <link>https://arxiv.org/abs/2502.19216</link>
      <description>arXiv:2502.19216v1 Announce Type: new 
Abstract: The initiation of dose optimization has driven a paradigm shift in oncology clinical trials to determine the optimal biological dose (OBD). Early-phase trials with randomized doses can facilitate additional investigation of the identified OBD in targeted populations by incorporating safety, efficacy, and biomarker data. To support dose comparison in such settings, we propose to extend the utility score-based approach (U-MET) and introduce the clinical utility index-based approach (CUI-MET) to account for multiple endpoints and doses. The utility-based dose optimization approach for multiple-dose randomized trial designs accounting for multiple endpoints and doses (U-MET-m) extends the U-MET, using a utility score to account for multiple endpoints jointly (e.g., toxicity-efficacy trade-off), while the CUI-MET uses a utility index to do this marginally. U-MET-m and CUI-MET use Bayesian inference within a hypothesis framework to compare utility metrics across doses to identify the OBD. Here we describe simulation studies and present an example to compare the U-MET-m design, CUI-MET, and empirical design. The U-MET-m design and CUI-MET were shown to have satisfactory operating characteristics for selecting the OBD. Based on these findings, we recommend using the U-MET-m and CUI-MET designs as the primary dose comparison approach or as supportive evidence to select the OBD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19216v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gina DAngelo, Guannan Chen, Di Ran</dc:creator>
    </item>
    <item>
      <title>AI-Powered Bayesian Inference</title>
      <link>https://arxiv.org/abs/2502.19231</link>
      <description>arXiv:2502.19231v1 Announce Type: new 
Abstract: The advent of Generative Artificial Intelligence (GAI) has heralded an inflection point that changed how society thinks about knowledge acquisition. While GAI cannot be fully trusted for decision-making, it may still provide valuable information that can be integrated into a decision pipeline. Rather than seeing the lack of certitude and inherent randomness of GAI as a problem, we view it as an opportunity. Indeed, variable answers to given prompts can be leveraged to construct a prior distribution which reflects assuredness of AI predictions. This prior distribution may be combined with tailored datasets for a fully Bayesian analysis with an AI-driven prior. In this paper, we explore such a possibility within a non-parametric Bayesian framework. The basic idea consists of assigning a Dirichlet process prior distribution on the data-generating distribution with AI generative model as its baseline. Hyper-parameters of the prior can be tuned out-of-sample to assess the informativeness of the AI prior. Posterior simulation is achieved by computing a suitably randomized functional on an augmented data that consists of observed (labeled) data as well as fake data whose labels have been imputed using AI. This strategy can be parallelized and rapidly produces iid samples from the posterior by optimization as opposed to sampling from conditionals. Our method enables (predictive) inference and uncertainty quantification leveraging AI predictions in a coherent probabilistic manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19231v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>stat.ML</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Veronika Ro\v{c}kov\'a, Sean O'Hagan</dc:creator>
    </item>
    <item>
      <title>Deep Computerized Adaptive Testing</title>
      <link>https://arxiv.org/abs/2502.19275</link>
      <description>arXiv:2502.19275v1 Announce Type: new 
Abstract: Computerized adaptive tests (CATs) play a crucial role in educational assessment and diagnostic screening in behavioral health. Unlike traditional linear tests that administer a fixed set of pre-assembled items, CATs adaptively tailor the test to an examinee's latent trait level by selecting a smaller subset of items based on their previous responses. Existing CAT frameworks predominantly rely on item response theory (IRT) models with a single latent variable, a choice driven by both conceptual simplicity and computational feasibility. However, many real-world item response datasets exhibit complex, multi-factor structures, limiting the applicability of CATs in broader settings. In this work, we develop a novel CAT system that incorporates multivariate latent traits, building on recent advances in Bayesian sparse multivariate IRT. Our approach leverages direct sampling from the latent factor posterior distributions, significantly accelerating existing information-theoretic item selection criteria by eliminating the need for computationally intensive Markov Chain Monte Carlo (MCMC) simulations. Recognizing the potential sub-optimality of existing item selection rules, which are often based on myopic one-step-lookahead optimization of some information-theoretic criterion, we propose a double deep Q-learning algorithm to learn an optimal item selection policy. Through simulation and real-data studies, we demonstrate that our approach not only accelerates existing item selection methods but also highlights the potential of reinforcement learning in CATs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19275v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiguang Li, Robert Gibbons, Veronika Rockova</dc:creator>
    </item>
    <item>
      <title>Tukey Depth Mechanisms for Practical Private Mean Estimation</title>
      <link>https://arxiv.org/abs/2502.18698</link>
      <description>arXiv:2502.18698v1 Announce Type: cross 
Abstract: Mean estimation is a fundamental task in statistics and a focus within differentially private statistical estimation. While univariate methods based on the Gaussian mechanism are widely used in practice, more advanced techniques such as the exponential mechanism over quantiles offer robustness and improved performance, especially for small sample sizes. Tukey depth mechanisms carry these advantages to multivariate data, providing similar strong theoretical guarantees. However, practical implementations fall behind these theoretical developments.
  In this work, we take the first step to bridge this gap by implementing the (Restricted) Tukey Depth Mechanism, a theoretically optimal mean estimator for multivariate Gaussian distributions, yielding improved practical methods for private mean estimation. Our implementations enable the use of these mechanisms for small sample sizes or low-dimensional data. Additionally, we implement variants of these mechanisms that use approximate versions of Tukey depth, trading off accuracy for faster computation. We demonstrate their efficiency in practice, showing that they are viable options for modest dimensions. Given their strong accuracy and robustness guarantees, we contend that they are competitive approaches for mean estimation in this regime. We explore future directions for improving the computational efficiency of these algorithms by leveraging fast polytope volume approximation techniques, paving the way for more accurate private mean estimation in higher dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18698v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gavin Brown, Lydia Zakynthinou</dc:creator>
    </item>
    <item>
      <title>MPO: An Efficient Post-Processing Framework for Mixing Diverse Preference Alignment</title>
      <link>https://arxiv.org/abs/2502.18699</link>
      <description>arXiv:2502.18699v1 Announce Type: cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) has shown promise in aligning large language models (LLMs). Yet its reliance on a singular reward model often overlooks the diversity of human preferences. Recent approaches address this limitation by leveraging multi-dimensional feedback to fine-tune corresponding reward models and train LLMs using reinforcement learning. However, the process is costly and unstable, especially given the competing and heterogeneous nature of human preferences. In this paper, we propose Mixing Preference Optimization (MPO), a post-processing framework for aggregating single-objective policies as an alternative to both multi-objective RLHF (MORLHF) and MaxMin-RLHF. MPO avoids alignment from scratch. Instead, it log-linearly combines existing policies into a unified one with the weight of each policy computed via a batch stochastic mirror descent. Empirical results demonstrate that MPO achieves balanced performance across diverse preferences, outperforming or matching existing models with significantly reduced computational costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18699v1</guid>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianze Wang, Dongnan Gui, Yifan Hu, Shuhang Lin, Linjun Zhang</dc:creator>
    </item>
    <item>
      <title>A Causal Lens for Evaluating Faithfulness Metrics</title>
      <link>https://arxiv.org/abs/2502.18848</link>
      <description>arXiv:2502.18848v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) offer natural language explanations as an alternative to feature attribution methods for model interpretability. However, despite their plausibility, they may not reflect the model's internal reasoning faithfully, which is crucial for understanding the model's true decision-making processes. Although several faithfulness metrics have been proposed, a unified evaluation framework remains absent. To address this gap, we present Causal Diagnosticity, a framework to evaluate faithfulness metrics for natural language explanations. Our framework employs the concept of causal diagnosticity, and uses model-editing methods to generate faithful-unfaithful explanation pairs. Our benchmark includes four tasks: fact-checking, analogy, object counting, and multi-hop reasoning. We evaluate a variety of faithfulness metrics, including post-hoc explanation and chain-of-thought-based methods. We find that all tested faithfulness metrics often fail to surpass a random baseline. Our work underscores the need for improved metrics and more reliable interpretability methods in LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18848v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kerem Zaman, Shashank Srivastava</dc:creator>
    </item>
    <item>
      <title>Association of normalization, non-differentially expressed genes and data source with machine learning performance in intra-dataset or cross-dataset modelling of transcriptomic and clinical data</title>
      <link>https://arxiv.org/abs/2502.18888</link>
      <description>arXiv:2502.18888v2 Announce Type: cross 
Abstract: Cross-dataset testing is critical for examining machine learning (ML) model's performance. However, most studies on modelling transcriptomic and clinical data only conducted intra-dataset testing. It is also unclear whether normalization and non-differentially expressed genes (NDEG) can improve cross-dataset modeling performance of ML. We thus aim to understand whether normalization, NDEG and data source are associated with performance of ML in cross-dataset testing. The transcriptomic and clinical data shared by the lung adenocarcinoma cases in TCGA and ONCOSG were used. The best cross-dataset ML performance was reached using transcriptomic data alone and statistically better than those using transcriptomic and clinical data. The best balance accuracy, area under curve and accuracy were significantly better in ML algorithms training on TCGA and tested on ONCOSG than those trained on ONCOSG and tested on TCGA (p&lt;0.05 for all). Normalization and NDEG greatly improved intra-dataset ML performances in both datasets, but not in cross-dataset testing. Strikingly, modelling transcriptomic data of ONCOSG alone outperformed modelling transcriptomic and clinical data whereas including clinical data in TCGA did not significantly impact ML performance, suggesting limited clinical data value or an overwhelming influence of transcriptomic data in TCGA. Performance gains in intra-dataset testing were more pronounced for ML models trained on ONCOSG than TCGA. Among the six ML models compared, Support vector machine was the most frequent best-performer in both intra-dataset and cross-dataset testing. Therefore, our data show data source, normalization and NDEG are associated with intra-dataset and cross-dataset ML performance in modelling transcriptomic and clinical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18888v2</guid>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fei Deng, Lanjing Zhang</dc:creator>
    </item>
    <item>
      <title>A Multifacet Hierarchical Sentiment-Topic Model with Application to Multi-Brand Online Review Analysis</title>
      <link>https://arxiv.org/abs/2502.18927</link>
      <description>arXiv:2502.18927v1 Announce Type: cross 
Abstract: Multi-brand analysis based on review comments and ratings is a commonly used strategy to compare different brands in marketing. It can help consumers make more informed decisions and help marketers understand their brand's position in the market. In this work, we propose a multifacet hierarchical sentiment-topic model (MH-STM) to detect brand-associated sentiment polarities towards multiple comparative aspects from online customer reviews. The proposed method is built on a unified generative framework that explains review words with a hierarchical brand-associated topic model and the overall polarity score with a regression model on the empirical topic distribution. Moreover, a novel hierarchical Polya urn (HPU) scheme is proposed to enhance the topic-word association among topic hierarchy, such that the general topics shared by all brands are separated effectively from the unique topics specific to individual brands. The performance of the proposed method is evaluated on both synthetic data and two real-world review corpora. Experimental studies demonstrate that the proposed method can be effective in detecting reasonable topic hierarchy and deriving accurate brand-associated rankings on multi-aspects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18927v1</guid>
      <category>cs.IR</category>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiao Liang, Xinwei Deng</dc:creator>
    </item>
    <item>
      <title>(Mis)Fitting: A Survey of Scaling Laws</title>
      <link>https://arxiv.org/abs/2502.18969</link>
      <description>arXiv:2502.18969v1 Announce Type: cross 
Abstract: Modern foundation models rely heavily on using scaling laws to guide crucial training decisions. Researchers often extrapolate the optimal architecture and hyper parameters settings from smaller training runs by describing the relationship between, loss, or task performance, and scale. All components of this process vary, from the specific equation being fit, to the training setup, to the optimization method. Each of these factors may affect the fitted law, and therefore, the conclusions of a given study. We discuss discrepancies in the conclusions that several prior works reach, on questions such as the optimal token to parameter ratio. We augment this discussion with our own analysis of the critical impact that changes in specific details may effect in a scaling study, and the resulting altered conclusions. Additionally, we survey over 50 papers that study scaling trends: while 45 of these papers quantify these trends using a power law, most under-report crucial details needed to reproduce their findings. To mitigate this, we we propose a checklist for authors to consider while contributing to scaling law research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18969v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Margaret Li, Sneha Kudugunta, Luke Zettlemoyer</dc:creator>
    </item>
    <item>
      <title>Age Group Sensitivity Analysis of Epidemic Models: Investigating the Impact of Contact Matrix Structure</title>
      <link>https://arxiv.org/abs/2502.19206</link>
      <description>arXiv:2502.19206v1 Announce Type: cross 
Abstract: Understanding the role of different age groups in disease transmission is crucial for designing effective intervention strategies. A key parameter in age-structured epidemic models is the contact matrix, which defines the interaction structure between age groups. However, accurately estimating contact matrices is challenging, as different age groups respond differently to surveys and are accessible through different channels. This variability introduces significant epistemic uncertainty in epidemic models.
  In this study, we introduce the Age Group Sensitivity Analysis (AGSA) method, a novel framework for assessing the impact of age-structured contact patterns on epidemic outcomes. Our approach integrates age-stratified epidemic models with Latin Hypercube Sampling (LHS) and the Partial Rank Correlation Coefficient (PRCC) method, enabling a systematic sensitivity analysis of age-specific interactions. Additionally, we propose a new sensitivity aggregation technique that quantifies the contribution of each age group to key epidemic parameters.
  By identifying the age groups to which the model is most sensitive, AGSA helps pinpoint those that introduce the greatest epistemic uncertainty. This allows for targeted data collection efforts, focusing surveys and empirical studies on the most influential age groups to improve model accuracy. As a result, AGSA can enhance epidemic forecasting and inform the design of more effective and efficient public health interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19206v1</guid>
      <category>q-bio.QM</category>
      <category>math.DS</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zsolt Vizi, Evans Kiptoo Korir, Norbert Bogya, Csaba Roszt\'oczy, G\'eza Makay, P\'eter Boldog</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference of Reproduction Number from Epidemiological and Genetic Data Using Particle MCMC</title>
      <link>https://arxiv.org/abs/2311.09838</link>
      <description>arXiv:2311.09838v2 Announce Type: replace 
Abstract: Inference of the reproduction number through time is of vital importance during an epidemic outbreak. Typically, epidemiologists tackle this using observed prevalence or incidence data. However, prevalence and incidence data alone is often noisy or partial. Models can also have identifiability issues with determining whether a large amount of a small epidemic or a small amount of a large epidemic has been observed. Sequencing data however is becoming more abundant, so approaches which can incorporate genetic data are an active area of research. We propose using particle MCMC methods to infer the time-varying reproduction number from a combination of prevalence data reported at a set of discrete times and a dated phylogeny reconstructed from sequences. We validate our approach on simulated epidemics with a variety of scenarios. We then apply the method to real data sets of HIV-1 in North Carolina, USA and tuberculosis in Buenos Aires, Argentina. The models and algorithms are implemented in an open source R package called EpiSky which is available at https://github.com/alicia-gill/EpiSky.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.09838v2</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alicia Gill, Jere Koskela, Xavier Didelot, Richard G. Everitt</dc:creator>
    </item>
    <item>
      <title>Estimation of Spectral Risk Measure for Left Truncated and Right Censored Data</title>
      <link>https://arxiv.org/abs/2402.14322</link>
      <description>arXiv:2402.14322v2 Announce Type: replace 
Abstract: Left truncated and right censored data are encountered frequently in insurance loss data due to deductibles and policy limits. Risk estimation is an important task in insurance as it is a necessary step for determining premiums under various policy terms. Spectral risk measures are inherently coherent and have the benefit of connecting the risk measure to the user's risk aversion. In this paper we study the estimation of spectral risk measure based on left truncated and right censored data. We propose a non parametric estimator of spectral risk measure using the product limit estimator and establish the asymptotic normality for our proposed estimator. We also develop an Edgeworth expansion of our proposed estimator. The bootstrap is employed to approximate the distribution of our proposed estimator and shown to be second order accurate. Monte Carlo studies are conducted to compare the proposed spectral risk measure estimator with the existing parametric and nonparametric estimators for left truncated and right censored data. Our observation reveal that the proposed estimator outperforms all the estimators for small values of $k$ (coefficient of absolute risk aversion) and for small sample sizes for i.i.d. case. In the dependent case, it demonstrates superior performance for small $k$ across all sample sizes. Finally, we estimate the exponential spectral risk measure for two data sets viz; the Norwegian fire claims and the French marine losses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14322v2</guid>
      <category>stat.ME</category>
      <category>q-fin.RM</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suparna Biswas, Rituparna Sen</dc:creator>
    </item>
    <item>
      <title>Seemingly unrelated Bayesian additive regression trees for cost-effectiveness analyses in healthcare</title>
      <link>https://arxiv.org/abs/2404.02228</link>
      <description>arXiv:2404.02228v3 Announce Type: replace 
Abstract: In recent years, theoretical results and simulation evidence have shown Bayesian additive regression trees to be a highly-effective method for nonparametric regression. Motivated by cost-effectiveness analyses in health economics, where interest lies in jointly modelling the costs of healthcare treatments and the associated health-related quality of life experienced by a patient, we propose a multivariate extension of BART which is applicable in regression analyses with several dependent outcome variables. Our framework allows for continuous or binary outcomes and overcomes some key limitations of existing multivariate BART models by allowing each individual response to be associated with different ensembles of trees, while still handling dependencies between the outcomes. In the case of continuous outcomes, our model is essentially a nonparametric version of seemingly unrelated regression. Likewise, our proposal for binary outcomes is a nonparametric generalisation of the multivariate probit model. We give suggestions for easily interpretable prior distributions, which allow specification of both informative and uninformative priors. We provide detailed discussions of MCMC sampling methods to conduct posterior inference. Our methods are implemented in the R package "subart". We showcase their performance through extensive simulation experiments and an application to an empirical case study from health economics. By also accommodating propensity scores in a manner befitting a causal analysis, we find substantial evidence for a novel trauma care intervention's cost-effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02228v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Esser, Mateus Maia, Andrew C. Parnell, Judith Bosmans, Hanneke van Dongen, Thomas Klausch, Keefe Murphy</dc:creator>
    </item>
    <item>
      <title>Tail-robust factor modelling of vector and tensor time series in high dimensions</title>
      <link>https://arxiv.org/abs/2407.09390</link>
      <description>arXiv:2407.09390v3 Announce Type: replace 
Abstract: We study the problem of factor modelling vector- and tensor-valued time series in the presence of heavy tails in the data, which produce anomalous observations with non-negligible probability. For this, we propose to combine a two-step procedure for tensor data decomposition with data truncation, which is easy to implement and does not require an iterative search for a numerical solution. Departing away from the light-tail assumptions often adopted in the time series factor modelling literature, we derive the consistency and asymptotic normality of the proposed estimators while assuming the existence of the $(2 + 2\epsilon)$-th moment only for some $\epsilon \in (0, 1)$. Our rates explicitly depend on $\epsilon$ characterising the effect of heavy tails, and on the chosen level of truncation. We also propose a consistent criterion for determining the number of factors. Simulation studies and applications to two macroeconomic datasets demonstrate the good performance of the proposed estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09390v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Barigozzi, Haeran Cho, Hyeyoung Maeng</dc:creator>
    </item>
    <item>
      <title>Simplifying Causal Mediation Analysis for Time-to-Event Outcomes using Pseudo-Values</title>
      <link>https://arxiv.org/abs/2411.17533</link>
      <description>arXiv:2411.17533v2 Announce Type: replace 
Abstract: Mediation analysis for survival outcomes is challenging. Most existing methods quantify the treatment effect using the hazard ratio (HR) and attempt to decompose the HR into the direct effect of treatment plus an indirect, or mediated, effect. However, the HR is not expressible as an expectation, which complicates this decomposition, both in terms of estimation and interpretation. Here, we present an alternative approach which leverages pseudo-values to simplify estimation and inference. Pseudo-values take censoring into account during their construction, and once derived, can be modeled in the same way as any continuous outcome. Thus, pseudo-values enable mediation analysis for a survival outcome to fit seamlessly into standard mediation software (e.g. CMAverse in R). Pseudo-values are easy to calculate via a leave-one-observation-out procedure (i.e. jackknifing) and the calculation can be accelerated when the influence function of the estimator is known. Mediation analysis for causal effects defined by survival probabilities, restricted mean survival time, and cumulative incidence functions - in the presence of competing risks - can all be performed within this framework. Extensive simulation studies demonstrate that the method is unbiased across 324 scenarios/estimands and controls the type-I error at the nominal level under the null of no mediation. We illustrate the approach using data from the PARADIGMS clinical trial for the treatment of pediatric multiple sclerosis using fingolimod. In particular, we evaluate whether an imaging biomarker lies on the causal path between treatment and time-to-relapse, which aids in justifying this biomarker as a surrogate outcome. Our approach greatly simplifies mediation analysis for survival data and provides a decomposition of the total effect that is both intuitive and interpretable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17533v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Ocampo, Enrico Giudice, Dieter A. H\"aring, Baldur Magnusson, Theis Lange, Zachary R. McCaw</dc:creator>
    </item>
    <item>
      <title>COADVISE: Covariate Adjustment with Variable Selection in Randomized Controlled Trials</title>
      <link>https://arxiv.org/abs/2501.08945</link>
      <description>arXiv:2501.08945v3 Announce Type: replace 
Abstract: Adjusting for covariates in randomized controlled trials can enhance the credibility and efficiency of treatment effect estimation. However, handling numerous covariates and their complex (non-linear) transformations poses a challenge. Motivated by the case study of the Best Apnea Interventions for Research (BestAIR) trial data from the National Sleep Research Resource (NSRR), where the number of covariates (p=114) is comparable to the sample size (N=196), we propose a principled Covariate Adjustment with Variable Selection (COADVISE) framework. COADVISE enables variable selection for covariates most relevant to the outcome while accommodating both linear and nonlinear adjustments. This framework ensures consistent estimates with improved efficiency over unadjusted estimators and provides robust variance estimation, even under outcome model misspecification. We demonstrate efficiency gains through theoretical analysis, extensive simulations, and a re-analysis of the BestAIR trial data to compare alternative variable selection strategies, offering cautionary recommendations. A user-friendly R package, Coadvise, is available to facilitate practical implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08945v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Liu, Ke Zhu, Larry Han, Shu Yang</dc:creator>
    </item>
    <item>
      <title>Forecasting macroeconomic data with Bayesian VARs: Sparse or dense? It depends!</title>
      <link>https://arxiv.org/abs/2206.04902</link>
      <description>arXiv:2206.04902v5 Announce Type: replace-cross 
Abstract: Vector autogressions (VARs) are widely applied when it comes to modeling and forecasting macroeconomic variables. In high dimensions, however, they are prone to overfitting. Bayesian methods, more concretely shrinkage priors, have shown to be successful in improving prediction performance. In the present paper, we introduce the semi-global framework, in which we replace the traditional global shrinkage parameter with group-specific shrinkage parameters. We show how this framework can be applied to various shrinkage priors, such as global-local priors and stochastic search variable selection priors. We demonstrate the virtues of the proposed framework in an extensive simulation study and in an empirical application forecasting data of the US economy. Further, we shed more light on the ongoing ``Illusion of Sparsity'' debate, finding that forecasting performances under sparse/dense priors vary across evaluated economic variables and across time frames. Dynamic model averaging, however, can combine the merits of both worlds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.04902v5</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ijforecast.2025.02.001</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Forecasting (2025)</arxiv:journal_reference>
      <dc:creator>Luis Gruber, Gregor Kastner</dc:creator>
    </item>
    <item>
      <title>Extremal correlation coefficient for functional data</title>
      <link>https://arxiv.org/abs/2405.17318</link>
      <description>arXiv:2405.17318v2 Announce Type: replace-cross 
Abstract: We propose a coefficient that measures dependence in paired samples of functions. It has properties similar to the Pearson correlation, but differs in significant ways: 1) it is designed to measure dependence between curves, 2) it focuses only on extreme curves. The new coefficient is derived within the framework of regular variation in Banach spaces. A consistent estimator is proposed and justified by an asymptotic analysis and a simulation study. The usefulness of the new coefficient is illustrated on financial and and climate functional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17318v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mihyun Kim, Piotr Kokoszka</dc:creator>
    </item>
    <item>
      <title>Low dimensional representation of multi-patient flow cytometry datasets using optimal transport for minimal residual disease detection in leukemia</title>
      <link>https://arxiv.org/abs/2407.17329</link>
      <description>arXiv:2407.17329v3 Announce Type: replace-cross 
Abstract: Representing and quantifying Minimal Residual Disease (MRD) in Acute Myeloid Leukemia (AML), a type of cancer that affects the blood and bone marrow, is essential in the prognosis and follow-up of AML patients. As traditional cytological analysis cannot detect leukemia cells below 5\%, the analysis of flow cytometry dataset is expected to provide more reliable results. In this paper, we explore statistical learning methods based on optimal transport (OT) to achieve a relevant low-dimensional representation of multi-patient flow cytometry measurements (FCM) datasets considered as high-dimensional probability distributions. Using the framework of OT, we justify the use of the K-means algorithm for dimensionality reduction of multiple large-scale point clouds through mean measure quantization by merging all the data into a single point cloud. After this quantization step, the visualization of the intra and inter-patients FCM variability is carried out by embedding low-dimensional quantized probability measures into a linear space using either Wasserstein Principal Component Analysis (PCA) through linearized OT or log-ratio PCA of compositional data. Using a publicly available FCM dataset and a FCM dataset from Bordeaux University Hospital, we demonstrate the benefits of our approach over the popular kernel mean embedding technique for statistical learning from multiple high-dimensional probability distributions. We also highlight the usefulness of our methodology for low-dimensional projection and clustering patient measurements according to their level of MRD in AML from FCM. In particular, our OT-based approach allows a relevant and informative two-dimensional representation of the results of the FlowSom algorithm, a state-of-the-art method for the detection of MRD in AML using multi-patient FCM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17329v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erell Gachon, J\'er\'emie Bigot, Elsa Cazelles, Audrey Bidet, Jean-Philippe Vial, Pierre-Yves Dumas, Aguirre Mimoun</dc:creator>
    </item>
    <item>
      <title>Enriched Functional Tree-Based Classifiers: A Novel Approach Leveraging Derivatives and Geometric Features</title>
      <link>https://arxiv.org/abs/2409.17804</link>
      <description>arXiv:2409.17804v2 Announce Type: replace-cross 
Abstract: The positioning of this research falls within the scalar-on-function classification literature, a field of significant interest across various domains, particularly in statistics, mathematics, and computer science. This study introduces an advanced methodology for supervised classification by integrating Functional Data Analysis (FDA) with tree-based ensemble techniques for classifying high-dimensional time series. The proposed framework, Enriched Functional Tree-Based Classifiers (EFTCs), leverages derivative and geometric features, benefiting from the diversity inherent in ensemble methods to further enhance predictive performance and reduce variance. While our approach has been tested on the enrichment of Functional Classification Trees (FCTs), Functional K-NN (FKNN), Functional Random Forest (FRF), Functional XGBoost (FXGB), and Functional LightGBM (FLGBM), it could be extended to other tree-based and non-tree-based classifiers, with appropriate considerations emerging from this investigation. Through extensive experimental evaluations on seven real-world datasets and six simulated scenarios, this proposal demonstrates fascinating improvements over traditional approaches, providing new insights into the application of FDA in complex, high-dimensional learning problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17804v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabrizio Maturo, Annamaria Porreca</dc:creator>
    </item>
    <item>
      <title>The No-Underrun Sampler: A Locally-Adaptive, Gradient-Free MCMC Method</title>
      <link>https://arxiv.org/abs/2501.18548</link>
      <description>arXiv:2501.18548v2 Announce Type: replace-cross 
Abstract: In this work, we introduce the No-Underrun Sampler (NURS), a locally-adaptive, gradient-free Markov chain Monte Carlo method that blends ideas from Hit-and-Run and the No-U-Turn Sampler. NURS dynamically adapts to the local scale of the target distribution without requiring gradient evaluations, making it especially suitable for applications where gradients are unavailable or costly. We establish key theoretical properties, including reversibility, formal connections to Hit-and-Run and Random Walk Metropolis, Wasserstein contraction comparable to Hit-and-Run in Gaussian targets, and bounds on the total variation distance between the transition kernels of Hit-and-Run and NURS. Empirical experiments, supported by theoretical insights, illustrate the ability of NURS to sample from Neal's funnel, a challenging multi-scale distribution from Bayesian hierarchical inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18548v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nawaf Bou-Rabee, Bob Carpenter, Sifan Liu, Stefan Oberd\"orster</dc:creator>
    </item>
  </channel>
</rss>
