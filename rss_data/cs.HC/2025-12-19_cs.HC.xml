<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Dec 2025 05:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Enhanced Web User Interface Design Via Cross-Device Responsiveness Assessment Using An Improved HCI-INTEGRATED DL Schemes</title>
      <link>https://arxiv.org/abs/2512.15775</link>
      <description>arXiv:2512.15775v1 Announce Type: new 
Abstract: User Interface (UI) optimization is essential in the digital era to enhance user satisfaction in web environments. Nevertheless, the existing UI optimization models had overlooked the Cross-Responsiveness (CR) assessment, affecting the user interaction efficiency. Consequently, this article proposes a dynamic web UI optimization through CR assessment using Finite Exponential Continuous State Machine (FECSM) and Quokka Nonlinear Difference Swarm Optimization Algorithm (QNDSOA). Initially, the design and user interaction related information is collected as well as pre-processed for min-max normalization. Next, the Human-Computer Interaction (HCI)-based features are extracted, followed by user behaviour pattern grouping. Meanwhile, the CR assessment is done using FECSM. Then, the proposed Bidirectional Gated Luong and Mish Recurrent Unit (BiGLMRU) is used to classify the User eXperience (UX) change type, which is labelled based on the User Interface Change Prediction Index (UICPI). Lastly, a novel QNDSOA is utilized to optimize the UI design with an average fitness of 98.5632%. Feedback monitoring is done after optimal deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15775v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.MM</category>
      <category>cs.SE</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shrinivass Arunachalam Balasubramanian</dc:creator>
    </item>
    <item>
      <title>Management von Sensordaten im Smarthome: Besonderheiten und Ans\"atze</title>
      <link>https://arxiv.org/abs/2512.15918</link>
      <description>arXiv:2512.15918v1 Announce Type: new 
Abstract: A wide variety of simple sensors, e.g. for temperature, light, or humidity, is finding its way into smart homes. There are special features to consider with regard to the data collected by these sensors: a) the nature of the measured data as "thin but big data" that needs to be contextualized and interpreted, b) which both algorithms and humans are capable of doing (resulting in comprehensive information in the context of the home, including the recognition of activities, behavior, and health of the residents), and c) uses that lead to interesting positive applications, but also to misuse and implications for privacy. When managing such data, it is necessary to take these special features into account, for which the principles of user experience, human-data interaction, and data protection should be considered together. We present our research tool "Sensorkit" and the participatory research approach used with it to collect sensor data in real homes. In our findings, we present identified challenges and explain how we address them through a) meaningful default settings, b) opportunities for users to interact and intervene, and c) life-cycle management of the data. Important aspects include phases before, during, and after the collection, processing, and use of the sensor data, as well as the provision of user-friendly tools and user involvement. Our findings inform beyond the scope of a research project also the development and use of commercial smart home devices and services.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15918v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.14925630</arxiv:DOI>
      <dc:creator>Albrecht Kurze, Karola K\"oferl, Andy B\"orner</dc:creator>
    </item>
    <item>
      <title>Non-Stationarity in Brain-Computer Interfaces: An Analytical Perspective</title>
      <link>https://arxiv.org/abs/2512.15941</link>
      <description>arXiv:2512.15941v1 Announce Type: new 
Abstract: Non-invasive Brain-Computer Interface (BCI) systems based on electroencephalography (EEG) signals suffer from multiple obstacles to reach a wide adoption in clinical settings for communication or rehabilitation. Among these challenges, the non-stationarity of the EEG signal is a key problem as it leads to various changes in the signal. There are changes within a session, across sessions, and across individuals. Variations over time for a given individual must be carefully managed to improve the BCI performance, including its accuracy, reliability, and robustness over time. This review paper presents and discusses the causes of non-stationarity in the EEG signal, along with its consequences for BCI applications, including covariate shift. The paper reviews recent studies on covariate shift, focusing on methods for detecting and correcting this phenomenon. Signal processing and machine learning techniques can be employed to normalize the EEG signal and address the covariate shift.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15941v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hubert Cecotti, Rashmi Mrugank Shah, Raksha Jagadish, Toshihisa Tanaka</dc:creator>
    </item>
    <item>
      <title>The Emerging Use of GenAI for UX Research in Software Development: Challenges and Opportunities</title>
      <link>https://arxiv.org/abs/2512.15944</link>
      <description>arXiv:2512.15944v1 Announce Type: new 
Abstract: The growing adoption of generative AI (GenAI) is reshaping how user experience (UX) research teams conduct qualitative research in software development, creating opportunities to streamline the production of qualitative insights. This paper presents findings from two user studies examining how current practices are challenged by GenAI and offering design implications for future AI assistance. Semi-structured interviews with 21 UX researchers, product managers, and designers reveal challenges of aligning AI capabilities with the interpretive, collaborative nature of qualitative research and tensions between roles. UX researchers expressed limited trust in AI-generated results, while product managers often overestimated AI capabilities, amplifying organizational pressures to accelerate research within agile workflows. In a second study, we validated an AI analysis approach more closely aligned with human analysis processes to address trust issues bottoms-up. We outline interaction patterns and design guidelines for responsibly integrating AI into software development cycles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15944v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Heloisa Candello, Werner Geyer, Siya Kunde, Michael Muller, Daita Sarkar, Jessica He, Mariela Claudia Lanza, Carlos Rosemberg, Gord Davison, Lisa Pelletier</dc:creator>
    </item>
    <item>
      <title>Augmented Reality-Based Smart Structural Health Monitoring System With Accurate 3D Model Alignment</title>
      <link>https://arxiv.org/abs/2512.16008</link>
      <description>arXiv:2512.16008v1 Announce Type: new 
Abstract: Structural Health Monitoring (SHM) has become increasingly critical due to the rapid deterioration of civil infrastructure. Traditional methods involving heavy equipment are costly and time-consuming. Recent SHM approaches use advanced non-contact sensors, IoT, and Augmented Reality (AR) glasses for faster inspections and immersive experiences during inspections. However, current methods lack quantitative damage data, remote collaboration support, and accurate 3D model alignment with the real structure. Recognizing these current challenges, this paper proposes an AR-based system that integrates Building Information Modelling (BIM) visualization and follows a flexible manipulation approach of 3D holograms to improve structural condition assessments. The proposed framework utilizes the Vuforia software development toolkit to enable the automatic alignment of 3D models to the real structure, ensuring successful model alignment to assist users in accurately visualizing damage locations. The framework also enables flexible manipulation of damage locations, making it easier for users to identify multiple damage points in the 3D models. The system is validated through lab-scale and full-scale bridge use cases, with data transfer performance analyzed under 4G and 5G conditions for remote collaboration. This study demonstrates that the proposed AR-based SHM framework successfully aligns 3D models with real structures, allowing users to manually adjust models and damage locations. The experimental results confirm its feasibility for remote collaborative inspections, highlighting significant improvements with 5G networks. Nevertheless, performance under 4G remains acceptable, ensuring reliability even without 5G coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16008v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.iintel.2025.100186</arxiv:DOI>
      <arxiv:journal_reference>Journal of Infrastructure Intelligence and Resilience, 5(1), 100186 (2026)</arxiv:journal_reference>
      <dc:creator>Omar Awadallah, Katarina Grolinger, Ayan Sadhu</dc:creator>
    </item>
    <item>
      <title>A Multi-Agent Large Language Model Framework for Automated Qualitative Analysis</title>
      <link>https://arxiv.org/abs/2512.16063</link>
      <description>arXiv:2512.16063v1 Announce Type: new 
Abstract: Understanding patients experiences is essential for advancing patient centered care, especially in chronic diseases that require ongoing communication. However, qualitative thematic analysis, the primary approach for exploring these experiences, remains labor intensive, subjective, and difficult to scale. In this study, we developed a multi agent large language model framework that automates qualitative thematic analysis through three agents (Instructor, Thematizer, CodebookGenerator), named Collaborative Theme Identification Agent (CoTI). We applied CoTI to 12 heart failure patient interviews to analyze their perceptions of medication intensity. CoTI identified key phrases, themes, and codebook that were more similar to those of the senior investigator than both junior investigators and baseline NLP models. We also implemented CoTI into a user-facing application to enable AI human interaction in qualitative analysis. However, collaboration between CoTI and junior investigators provided only marginal gains, suggesting they may overrely on CoTI and limit their independent critical thinking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16063v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Qidi Xu, Nuzha Amjad, Grace Giles, Alexa Cumming, De'angelo Hermesky, Alexander Wen, Min Ji Kwak, Yejin Kim</dc:creator>
    </item>
    <item>
      <title>WING: An Adaptive and Gamified Mobile Learning Platform for Neurodivergent Literacy</title>
      <link>https://arxiv.org/abs/2512.16067</link>
      <description>arXiv:2512.16067v1 Announce Type: new 
Abstract: This paper presents WING, an adaptive and gamified mobile learning platform designed to support literacy development for neurodivergent children. Motivated by the limitations of traditional literacy approaches in addressing diverse cognitive profiles, the platform integrates inclusive Human-Computer Interaction principles, multisensory design, and adaptive learning paths. WING digitally transposes the Alfabetiza\c{c}\~ao Adaptada (AFA) method into an interactive mobile environment, combining usability guidelines for neurodivergent users with gamification strategies to enhance engagement and autonomy. The study follows an applied research methodology, encompassing requirements elicitation, inclusive interface design, high-fidelity prototyping, and qualitative and quantitative evaluation planning. Preliminary results include a functional minimum viable product validated through expert feedback and public exhibitions, indicating the feasibility and potential pedagogical impact of the proposed approach. The platform aims to act as a complementary educational tool, promoting accessibility, personalization, and inclusive digital literacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16067v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mirella Emily Bezerra Santana, Cau\~a Otaviano Jord\~ao, Victor Barbosa dos Santos, Leonardo Jos\'e Oliveira Ibiapina, Gabriel Moraes da Silva, Marina Robalinho Cavalcanti, Lucas Rodolfo Celestino Farias</dc:creator>
    </item>
    <item>
      <title>Evaluation of Generative Models for Emotional 3D Animation Generation in VR</title>
      <link>https://arxiv.org/abs/2512.16081</link>
      <description>arXiv:2512.16081v1 Announce Type: new 
Abstract: Social interactions incorporate nonverbal signals to convey emotions alongside speech, including facial expressions and body gestures. Generative models have demonstrated promising results in creating full-body nonverbal animations synchronized with speech; however, evaluations using statistical metrics in 2D settings fail to fully capture user-perceived emotions, limiting our understanding of model effectiveness. To address this, we evaluate emotional 3D animation generative models within a Virtual Reality (VR) environment, emphasizing user-centric metrics emotional arousal realism, naturalness, enjoyment, diversity, and interaction quality in a real-time human-agent interaction scenario. Through a user study (N=48), we examine perceived emotional quality for three state of the art speech-driven 3D animation methods across two emotions happiness (high arousal) and neutral (mid arousal). Additionally, we compare these generative models against real human expressions obtained via a reconstruction-based method to assess both their strengths and limitations and how closely they replicate real human facial and body expressions. Our results demonstrate that methods explicitly modeling emotions lead to higher recognition accuracy compared to those focusing solely on speech-driven synchrony. Users rated the realism and naturalness of happy animations significantly higher than those of neutral animations, highlighting the limitations of current generative models in handling subtle emotional states. Generative models underperformed compared to reconstruction-based methods in facial expression quality, and all methods received relatively low ratings for animation enjoyment and interaction quality, emphasizing the importance of incorporating user-centric evaluations into generative model development. Finally, participants positively recognized animation diversity across all generative models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16081v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.3389/fcomp.2025.1598099</arxiv:DOI>
      <dc:creator>Kiran Chhatre, Renan Guarese, Andrii Matviienko, Christopher Peters</dc:creator>
    </item>
    <item>
      <title>The Agony of Opacity: Foundations for Reflective Interpretability in AI-Mediated Mental Health Support</title>
      <link>https://arxiv.org/abs/2512.16206</link>
      <description>arXiv:2512.16206v1 Announce Type: new 
Abstract: Throughout history, a prevailing paradigm in mental healthcare has been one in which distressed people may receive treatment with little understanding around how their experience is perceived by their care provider, and in turn, the decisions made by their provider around how treatment will progress. Paralleling this offline model of care, people who seek mental health support from AI chatbots are similarly provided little context for how their expressions of distress are processed by the model, and subsequently, the logic that may underlie model responses. People in severe distress who turn to AI chatbots for support thus find themselves caught between black boxes, with unique forms of agony that arise from these intersecting opacities, including misinterpreting model outputs or attributing greater capabilities to a model than are yet possible, which has led to documented real-world harms. Building on empirical research from clinical psychology and AI safety, alongside rights-oriented frameworks from medical ethics, we describe how the distinct psychological state induced by severe distress can influence chatbot interaction patterns, and argue that this state of mind (combined with differences in how a user might perceive a chatbot compared to a care provider) uniquely necessitates a higher standard of interpretability in comparison to general AI chatbot use. Drawing inspiration from newer interpretable treatment paradigms, we then describe specific technical and interface design approaches that could be used to adapt interpretability strategies from four specific mental health fields (psychotherapy, community-based crisis intervention, psychiatry, and care authorization) to AI models, including consideration of the role of interpretability in the treatment process and tensions that may arise with greater interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16206v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sachin R. Pendse, Darren Gergle, Rachel Kornfield, Kaylee Kruzan, David Mohr, Jessica Schleider, Jina Suh, Annie Wescott, Jonah Meyerhoff</dc:creator>
    </item>
    <item>
      <title>Machines, AI and the past//future of things</title>
      <link>https://arxiv.org/abs/2512.16285</link>
      <description>arXiv:2512.16285v1 Announce Type: new 
Abstract: This essay explores a techno-artistic experiment that reanimates a 1980s East German typewriter using a contemporary AI language model. Situated at the intersection of media archaeology and speculative design, the project questions dominant narratives of progress by embedding generative AI in an obsolete, tactile interface. Through public exhibitions and aesthetic intervention, we demonstrate how slowness, friction, and material render artificial intelligence not only visible but open to critical inquiry. Drawing on concepts such as zombie media, technostalgia, and speculative design, we argue that reappropriating outdated technologies enables new forms of critical engagement. Erika - the AI-enabled typewriter - functions as both interface and interruption, making space for reflection, irony, and cultural memory. In a moment of accelerated digital abstraction, projects like this foreground the value of deliberate slowness, experiential materiality, and historical depth. We conclude by advocating for a historicist design sensibility that challenges presentism and reorients human-machine interaction toward alternative, perceived futures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16285v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.17084154</arxiv:DOI>
      <dc:creator>Karola K\"opferl, Albrecht Kurze</dc:creator>
    </item>
    <item>
      <title>Ein Typenrad auf der \"Uberholspur: Die Kult-Schreibmaschine "Erika" trifft KI</title>
      <link>https://arxiv.org/abs/2512.16293</link>
      <description>arXiv:2512.16293v1 Announce Type: new 
Abstract: In the 15th century, printing revolutionized the dissemination of information. Innovations such as typewriters and computers have increased the speed and volume of information flows over time. More recent developments in large language models such as ChatGPT enable text to be generated in a matter of seconds. However, many people do not understand how this works and what the long-term implications are. That is why we have "hacked" an old typewriter so that users can interact with an LLM chatbot, which over 1,200 participants have now been able to experience. It helps to understand the possibilities and limitations of AI. It gives us researchers insights into participants' concepts of AI as well as their expectations and concerns. It raises questions about these technological developments and stimulates discussions about the social impact of the intensification and acceleration of information and communication flows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16293v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.14925622</arxiv:DOI>
      <dc:creator>Karola K\"opferl, Albrecht Kurze</dc:creator>
    </item>
    <item>
      <title>Mind the Gaze: Improving the Usability of Dwell Input by Adapting Gaze Targets Based on Viewing Distance</title>
      <link>https://arxiv.org/abs/2512.16366</link>
      <description>arXiv:2512.16366v1 Announce Type: new 
Abstract: Dwell input shows promise for handheld mobile contexts, but its performance is impacted by target size and viewing distance. While fixed target sizes suffice in static setups, in mobile settings, frequent posture changes alter viewing distances, which in turn distort perceived size and hinder dwell performance. We address this through GAUI, a Gaze-based Adaptive User Interface that dynamically resizes targets to maximise performance at the given viewing distance. In a two-phased study (N=24), GAUI leveraged the strengths of its distance-responsive design, outperforming the large UI static baseline in task time, and being less error-prone than the small UI static baseline. It was rated the most preferred interface overall. Participants reflected on using GAUI in six different postures. We discuss how their experience is impacted by posture, and propose guidelines for designing context-aware adaptive UIs for dwell interfaces on handheld mobile devices that maximise performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16366v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omar Namnakani, Yasmeen Abdrabou, Cristina Fiani, John H. Williamson, Mohamed Khamis</dc:creator>
    </item>
    <item>
      <title>Preparing Future-Ready Learners: K12 Skills Shift and GenAI EdTech Innovation Direction</title>
      <link>https://arxiv.org/abs/2512.16428</link>
      <description>arXiv:2512.16428v1 Announce Type: new 
Abstract: Since Generative AI came out it has quickly embedded itself in our social fabric, triggering lots of discussions, predictions, and efforts from research, industry, government and capital market to experiment and embrace the technology. The question for the global K12 education is, what and how should our children learn in this fast changing world to be prepared for the changing labor market and live a happy and balanced life? Three key aspects will be discussed: 1) Skills; 2) Evaluation of Learning; 3) Strategic GenAI-powered EdTech innovation for long term educational impact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16428v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Miao, Pawan Kumar Mishra</dc:creator>
    </item>
    <item>
      <title>Investigating the Effect of Encumbrance on Gaze- and Touch-based Target Acquisition on Handheld Mobile Devices</title>
      <link>https://arxiv.org/abs/2512.16472</link>
      <description>arXiv:2512.16472v1 Announce Type: new 
Abstract: The potential of using gaze as an input modality in the mobile context is growing. While users often encumber themselves by carrying objects and using mobile devices while walking, the impact of encumbrance on gaze input performance remains unexplored. To investigate this, we conducted a user study (N=24) to evaluate the effect of encumbrance on the performance of 1) Gaze using Dwell time (with/without visual feedback), 2) GazeTouch (with/without visual feedback), and 3) One- or two-hand touch input. While Touch generally performed better, Gaze, especially with feedback, showed a consistent performance regardless of whether participants were encumbered or unencumbered. Participants' preferences for input modalities varied with encumbrance: they preferred Gaze when encumbered, and touch when unencumbered. Our findings enhance understanding of the effect of encumbrance on gaze input and contribute towards selecting appropriate input modalities in future mobile user interfaces to account for situational impairments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16472v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omar Namnakani, Yasmeen Abdrabou, John H. Williamson, Mohamed Khamis</dc:creator>
    </item>
    <item>
      <title>Poster: Recognizing Hidden-in-the-Ear Private Key for Reliable Silent Speech Interface Using Multi-Task Learning</title>
      <link>https://arxiv.org/abs/2512.16518</link>
      <description>arXiv:2512.16518v1 Announce Type: new 
Abstract: Silent speech interface (SSI) enables hands-free input without audible vocalization, but most SSI systems do not verify speaker identity. We present HEar-ID, which uses consumer active noise-canceling earbuds to capture low-frequency "whisper" audio and high-frequency ultrasonic reflections. Features from both streams pass through a shared encoder, producing embeddings that feed a contrastive branch for user authentication and an SSI head for silent spelling recognition. This design supports decoding of 50 words while reliably rejecting impostors, all on commodity earbuds with a single model. Experiments demonstrate that HEar-ID achieves strong spelling accuracy and robust authentication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16518v1</guid>
      <category>cs.HC</category>
      <category>eess.AS</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3714394.3754429</arxiv:DOI>
      <dc:creator>Xuefu Dong, Liqiang Xu, Lixing He, Zengyi Han, Ken Christofferson, Yifei Chen, Akihito Taya, Yuuki Nishiyama, Kaoru Sezaki</dc:creator>
    </item>
    <item>
      <title>Plausibility as Failure: How LLMs and Humans Co-Construct Epistemic Error</title>
      <link>https://arxiv.org/abs/2512.16750</link>
      <description>arXiv:2512.16750v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used as epistemic partners in everyday reasoning, yet their errors remain predominantly analyzed through predictive metrics rather than through their interpretive effects on human judgment. This study examines how different forms of epistemic failure emerge, are masked, and are tolerated in human AI interaction, where failure is understood as a relational breakdown shaped by model-generated plausibility and human interpretive judgment. We conducted a three round, multi LLM evaluation using interdisciplinary tasks and progressively differentiated assessment frameworks to observe how evaluators interpret model responses across linguistic, epistemic, and credibility dimensions. Our findings show that LLM errors shift from predictive to hermeneutic forms, where linguistic fluency, structural coherence, and superficially plausible citations conceal deeper distortions of meaning. Evaluators frequently conflated criteria such as correctness, relevance, bias, groundedness, and consistency, indicating that human judgment collapses analytical distinctions into intuitive heuristics shaped by form and fluency. Across rounds, we observed a systematic verification burden and cognitive drift. As tasks became denser, evaluators increasingly relied on surface cues, allowing erroneous yet well formed answers to pass as credible. These results suggest that error is not solely a property of model behavior but a co-constructed outcome of generative plausibility and human interpretive shortcuts. Understanding AI epistemic failure therefore requires reframing evaluation as a relational interpretive process, where the boundary between system failure and human miscalibration becomes porous. The study provides implications for LLM assessment, digital literacy, and the design of trustworthy human AI communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16750v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Claudia Vale Oliveira (DigiMedia, University of Aveiro, Aveiro, Portugal), Nelson Zagalo (DigiMedia, University of Aveiro, Aveiro, Portugal), Filipe Silva (DigiMedia, University of Aveiro, Aveiro, Portugal), Anabela Brandao (DigiMedia, University of Aveiro, Aveiro, Portugal), Syeda Faryal Hussain Khurrum (DigiMedia, University of Aveiro, Aveiro, Portugal), Joaquim Santos (DigiMedia, University of Aveiro, Aveiro, Portugal)</dc:creator>
    </item>
    <item>
      <title>TinyMyo: a Tiny Foundation Model for Flexible EMG Signal Processing at the Edge</title>
      <link>https://arxiv.org/abs/2512.15729</link>
      <description>arXiv:2512.15729v1 Announce Type: cross 
Abstract: Surface electromyography (EMG) is a non-invasive sensing modality used in several domains, including biomechanics, rehabilitation, prosthetic control, and emerging human-machine interaction paradigms. Despite decades of use, significant challenges remain in achieving robust generalization across subjects, recording systems, and acquisition protocols. To tackle these challenges, foundation models (FMs) are gaining traction when targeting end-to-end applications based on EMG signals. Yet, existing EMG FMs remain limited to single downstream tasks and lack deployability on embedded platforms. In this work, we present TinyMyo, a lightweight FM based on a Transformer encoder architecture. The model is pre-trained in a self-supervised manner on publicly available datasets and achieves high reconstruction fidelity with only 3.6M parameters. With minimal task-specific head adaptations, the same backbone is used to tackle multiple downstream tasks, leveraging datasets acquired from diverse sensing locations and hardware platforms. We demonstrate generalization across hand gesture classification, hand kinematic regression, speech production and recognition, with performance comparable to or surpassing the state of the art (SoA), and model size below 5M parameters. We achieve SoA results compared to previous FM-based works on the NinaPro DB5 ($89.4\pm0.16\%$), UCI-EMG ($97.56\pm0.32\%$), and EPN-612 ($96.74\pm0.09\%$) datasets. We report, to the best of our knowledge, the first deployment of an EMG FM on an ultra-low-power microcontroller (GAP9), achieving an average power envelope of 36.45mW. By open-sourcing the pre-trained and the downstream task architectures (https://github.com/pulp-bio/BioFoundation), we aim to provide a flexible resource that can accelerate future research and serve as a common foundation for the EMG community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15729v1</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Fasulo, Giusy Spacone, Thorir Mar Ingolfsson, Yawei Li, Luca Benini, Andrea Cossettini</dc:creator>
    </item>
    <item>
      <title>Scaling Text2SQL via LLM-efficient Schema Filtering with Functional Dependency Graph Rerankers</title>
      <link>https://arxiv.org/abs/2512.16083</link>
      <description>arXiv:2512.16083v1 Announce Type: cross 
Abstract: Most modern Text2SQL systems prompt large language models (LLMs) with entire schemas -- mostly column information -- alongside the user's question. While effective on small databases, this approach fails on real-world schemas that exceed LLM context limits, even for commercial models. The recent Spider 2.0 benchmark exemplifies this with hundreds of tables and tens of thousands of columns, where existing systems often break. Current mitigations either rely on costly multi-step prompting pipelines or filter columns by ranking them against user's question independently, ignoring inter-column structure. To scale existing systems, we introduce \toolname, an open-source, LLM-efficient schema filtering framework that compacts Text2SQL prompts by (i) ranking columns with a query-aware LLM encoder enriched with values and metadata, (ii) reranking inter-connected columns via a lightweight graph transformer over functional dependencies, and (iii) selecting a connectivity-preserving sub-schema with a Steiner-tree heuristic. Experiments on real datasets show that \toolname achieves near-perfect recall and higher precision than CodeS, SchemaExP, Qwen rerankers, and embedding retrievers, while maintaining sub-second median latency and scaling to schemas with 23,000+ columns. Our source code is available at https://github.com/thanhdath/grast-sql.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16083v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Thanh Dat Hoang, Thanh Tam Nguyen, Thanh Trung Huynh, Hongzhi Yin, Quoc Viet Hung Nguyen</dc:creator>
    </item>
    <item>
      <title>ParamExplorer: A framework for exploring parameters in generative art</title>
      <link>https://arxiv.org/abs/2512.16529</link>
      <description>arXiv:2512.16529v1 Announce Type: cross 
Abstract: Generative art systems often involve high-dimensional and complex parameter spaces in which aesthetically compelling outputs occupy only small, fragmented regions. Because of this combinatorial explosion, artists typically rely on extensive manual trial-and-error, leaving many potentially interesting configurations undiscovered. In this work we make two contributions. First, we introduce ParamExplorer, an interactive and modular framework inspired by reinforcement learning that helps the exploration of parameter spaces in generative art algorithms, guided by human-in-the-loop or even automated feedback. The framework also integrates seamlessly with existing p5.js projects. Second, within this framework we implement and evaluate several exploration strategies, referred to as agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16529v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julien Gachadoat, Guillaume Lagarde</dc:creator>
    </item>
    <item>
      <title>OMG-Bench: A New Challenging Benchmark for Skeleton-based Online Micro Hand Gesture Recognition</title>
      <link>https://arxiv.org/abs/2512.16727</link>
      <description>arXiv:2512.16727v1 Announce Type: cross 
Abstract: Online micro gesture recognition from hand skeletons is critical for VR/AR interaction but faces challenges due to limited public datasets and task-specific algorithms. Micro gestures involve subtle motion patterns, which make constructing datasets with precise skeletons and frame-level annotations difficult. To this end, we develop a multi-view self-supervised pipeline to automatically generate skeleton data, complemented by heuristic rules and expert refinement for semi-automatic annotation. Based on this pipeline, we introduce OMG-Bench, the first large-scale public benchmark for skeleton-based online micro gesture recognition. It features 40 fine-grained gesture classes with 13,948 instances across 1,272 sequences, characterized by subtle motions, rapid dynamics, and continuous execution. To tackle these challenges, we propose Hierarchical Memory-Augmented Transformer (HMATr), an end-to-end framework that unifies gesture detection and classification by leveraging hierarchical memory banks which store frame-level details and window-level semantics to preserve historical context. In addition, it employs learnable position-aware queries initialized from the memory to implicitly encode gesture positions and semantics. Experiments show that HMATr outperforms state-of-the-art methods by 7.6\% in detection rate, establishing a strong baseline for online micro gesture recognition. Project page: https://omg-bench.github.io/</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16727v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haochen Chang, Pengfei Ren, Buyuan Zhang, Da Li, Tianhao Han, Haoyang Zhang, Liang Xie, Hongbo Chen, Erwei Yin</dc:creator>
    </item>
    <item>
      <title>PrivateXR: Defending Privacy Attacks in Extended Reality Through Explainable AI-Guided Differential Privacy</title>
      <link>https://arxiv.org/abs/2512.16851</link>
      <description>arXiv:2512.16851v1 Announce Type: cross 
Abstract: The convergence of artificial AI and XR technologies (AI XR) promises innovative applications across many domains. However, the sensitive nature of data (e.g., eye-tracking) used in these systems raises significant privacy concerns, as adversaries can exploit these data and models to infer and leak personal information through membership inference attacks (MIA) and re-identification (RDA) with a high success rate. Researchers have proposed various techniques to mitigate such privacy attacks, including differential privacy (DP). However, AI XR datasets often contain numerous features, and applying DP uniformly can introduce unnecessary noise to less relevant features, degrade model accuracy, and increase inference time, limiting real-time XR deployment. Motivated by this, we propose a novel framework combining explainable AI (XAI) and DP-enabled privacy-preserving mechanisms to defend against privacy attacks. Specifically, we leverage post-hoc explanations to identify the most influential features in AI XR models and selectively apply DP to those features during inference. We evaluate our XAI-guided DP approach on three state-of-the-art AI XR models and three datasets: cybersickness, emotion, and activity classification. Our results show that the proposed method reduces MIA and RDA success rates by up to 43% and 39%, respectively, for cybersickness tasks while preserving model utility with up to 97% accuracy using Transformer models. Furthermore, it improves inference time by up to ~2x compared to traditional DP approaches. To demonstrate practicality, we deploy the XAI-guided DP AI XR models on an HTC VIVE Pro headset and develop a user interface (UI), namely PrivateXR, allowing users to adjust privacy levels (e.g., low, medium, high) while receiving real-time task predictions, protecting user privacy during XR gameplay.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16851v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ISMAR67309.2025.00061</arxiv:DOI>
      <dc:creator>Ripan Kumar Kundu, Istiak Ahmed, Khaza Anuarul Hoque</dc:creator>
    </item>
    <item>
      <title>Easy Come, Easy Go? Examining the Perceptions and Learning Effects of LLM-based Chatbot in the Context of Search-as-Learning</title>
      <link>https://arxiv.org/abs/2410.01396</link>
      <description>arXiv:2410.01396v2 Announce Type: replace 
Abstract: The cognitive process of Search-as-Learning (SAL) is most effective when searching promotes active encoding of information. The rise of LLMs-based chatbots, which provide instant answers, introduces a trade-off between efficiency and depth of processing. Such answer-centric approaches accelerate information access, but they also raise concerns about shallower learning. To examine these issues in the context of SAL, we conducted a large-scale survey of educators and students to capture perceived risks and benefits of LLM-based chatbots. In addition, we adopted the encoding-storage paradigm to design a within-subjects experiment, where participants (N=92) engaged in SAL tasks using three different modalities: books, search engines, and chatbots. Our findings provide a counterintuitive insight into stakeholder concerns: while LLM-based chatbots and search engines validated perceived benefits on learning efficiency by outperforming book-based search in immediate conceptual understanding, they did not result in a long-term inferiority as feared. Our study provides insights for designing human-AI collaborative learning systems that promote cognitive engagement by balancing learning efficiency and long-term knowledge retention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01396v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yeonsun Yang, Ahyeon Shin, Mincheol Kang, Jiheon Kang, Xu Wang, Jean Y. Song</dc:creator>
    </item>
    <item>
      <title>How Warm-Glow Alters the Usability of Technology</title>
      <link>https://arxiv.org/abs/2506.14720</link>
      <description>arXiv:2506.14720v4 Announce Type: replace 
Abstract: As technology increasingly aligns with users' personal values, traditional models of usability, focused on functionality and specifically effectiveness, efficiency, and satisfaction, may not fully capture how people perceive and evaluate it. This study investigates how the warm-glow phenomenon, the positive feeling associated with doing good, shapes perceived usability. An experimental approach was taken in which participants evaluated a hypothetical technology under conditions designed to evoke either the intrinsic (i.e., personal fulfillment) or extrinsic (i.e., social recognition) dimensions of warm-glow. A Multivariate Analysis of Variance as well as subsequent follow-up analyses revealed that intrinsic warm-glow significantly enhances all dimensions of perceived usability, while extrinsic warm-glow selectively influences perceived effectiveness and satisfaction. These findings suggest that perceptions of usability extend beyond functionality and are shaped by how technology resonates with users' broader sense of purpose. We conclude by proposing that designers consider incorporating warm-glow into technology as a strategic design decision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14720v4</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3783862.3783873</arxiv:DOI>
      <dc:creator>Antonios Saravanos (New York University)</dc:creator>
    </item>
    <item>
      <title>Virtual Reality User Interface Design: Best Practices and Implementation</title>
      <link>https://arxiv.org/abs/2508.09358</link>
      <description>arXiv:2508.09358v2 Announce Type: replace 
Abstract: Designing effective user interfaces (UIs) for virtual reality (VR) is essential to enhance user immersion, usability, comfort, and accessibility in virtual environments. Despite the growing adoption of VR across domains, there is a noticeable lack of unified and comprehensive design guidelines for VR UI design. To address this gap, we conducted a systematic literature review to identify existing best practices and propose 28 unified guidelines for UI development in VR.
  Building on these insights, this research proposes a framework to guide the creation of more effective VR interfaces. To demonstrate and validate these practices, we developed a VR application called FlUId and an interactive Web Tool that serves as a guideline explorer and project planning resource for developers. A user study was conducted to evaluate the impact of the proposed guidelines. The findings aim to bridge the gap between theory and practice, offering concrete recommendations and digital tools for VR designers and developers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09358v2</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Esin Mehmedova, Santiago Berrezueta-Guzman, Stefan Wagner</dc:creator>
    </item>
    <item>
      <title>AI Autonomy Coefficient ($\alpha$): Defining Boundaries for Responsible AI Systems</title>
      <link>https://arxiv.org/abs/2512.11295</link>
      <description>arXiv:2512.11295v3 Announce Type: replace 
Abstract: The integrity of many contemporary AI systems is compromised by the misuse of Human-in-the-Loop (HITL) models to obscure systems that remain heavily dependent on human labor. We define this structural dependency as Human-Instead-of-AI (HISOAI), an ethically problematic and economically unsustainable design in which human workers function as concealed operational substitutes rather than intentional, high-value collaborators. To address this issue, we introduce the AI-First, Human-Empowered (AFHE) paradigm, which requires AI systems to demonstrate a quantifiable level of functional independence prior to deployment. This requirement is formalized through the AI Autonomy Coefficient, measuring the proportion of tasks completed without mandatory human intervention. We further propose the AFHE Deployment Algorithm, an algorithmic gate that enforces a minimum autonomy threshold during offline evaluation and shadow deployment. Our results show that the AI Autonomy Coefficient effectively identifies HISOAI systems with an autonomy level of 0.38, while systems governed by the AFHE framework achieve an autonomy level of 0.85. We conclude that AFHE provides a metric-driven approach for ensuring verifiable autonomy, transparency, and sustainable operational integrity in modern AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11295v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nattaya Mairittha, Gabriel Phorncharoenmusikul, Sorawit Worapradidth</dc:creator>
    </item>
    <item>
      <title>Analyzing Social Media Claims regarding Youth Online Safety Features to Identify Problem Areas and Communication Gaps</title>
      <link>https://arxiv.org/abs/2512.14965</link>
      <description>arXiv:2512.14965v2 Announce Type: replace 
Abstract: Social media platforms have faced increasing scrutiny over whether and how they protect youth online. While online risks to children have been well-documented by prior research, how social media platforms communicate about these risks and their efforts to improve youth safety have not been holistically examined. To fill this gap, we analyzed N=352 press releases and safety-related blogs published between 2019 and 2024 by four platforms popular among youth: YouTube, TikTok, Meta (Facebook and Instagram), and Snapchat. Leveraging both inductive and deductive qualitative approaches, we developed a comprehensive framework of seven problem areas where risks arise, and a taxonomy of safety features that social media platforms claim address these risks. Our analysis revealed uneven emphasis across problem areas, with most communications focused on Content Exposure and Interpersonal Communication, whereas less emphasis was placed on Content Creation, Data Access, and Platform Access. Additionally, we identified three problematic communication practices related to their described safety features, including discrepancies between feature implementation and availability, unclear or inconsistent explanations of safety feature operation, and a lack of evidence regarding the effectiveness of safety features in mitigating risks once implemented. Based on these findings, we discuss the communication gaps between risks and the described safety features, as well as the tensions in achieving transparency in platform communication. Our analysis of platform communication informs guidelines for responsibly communicating about youth safety features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14965v2</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Renkai Ma, Dominique Geissler, Stefan Feuerriegel, Tobias Lauinger, Damon McCoy, Pamela Wisniewski</dc:creator>
    </item>
    <item>
      <title>"I am here for you": How relational conversational AI appeals to adolescents, especially those who are socially and emotionally vulnerable</title>
      <link>https://arxiv.org/abs/2512.15117</link>
      <description>arXiv:2512.15117v2 Announce Type: replace 
Abstract: General-purpose conversational AI chatbots and AI companions increasingly provide young adolescents with emotionally supportive conversations, raising questions about how conversational style shapes anthropomorphism and emotional reliance. In a preregistered online experiment with 284 adolescent-parent dyads, youth aged 11-15 and their parents read two matched transcripts in which a chatbot responded to an everyday social problem using either a relational style (first-person, affiliative, commitment language) or a transparent style (explicit nonhumanness, informational tone). Adolescents more often preferred the relational than the transparent style, whereas parents were more likely to prefer transparent style than adolescents. Adolescents rated the relational chatbot as more human-like, likable, trustworthy and emotionally close, while perceiving both styles as similarly helpful. Adolescents who preferred relational style had lower family and peer relationship quality and higher stress and anxiety than those preferring transparent style or both chatbots. These findings identify conversational style as a key design lever for youth AI safety, showing that relational framing heightens anthropomorphism, trust and emotional closeness and can be especially appealing to socially and emotionally vulnerable adolescents, who may be at increased risk for emotional reliance on conversational AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15117v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pilyoung Kim, Yun Xie, Sujin Yang</dc:creator>
    </item>
    <item>
      <title>A Constructive Scientific Methodology to Improve Climate Figures from IPCC</title>
      <link>https://arxiv.org/abs/2512.15514</link>
      <description>arXiv:2512.15514v2 Announce Type: replace 
Abstract: We propose a methodology to improve figures from the Intergovernmental Panel on Climate Change (IPCC), ensuring that all modifications remain scientifically rigorous. IPCC figures are notoriously difficult to understand, and although designers have proposed alternatives, these lack formal IPCC validation and can be dismissed by skeptics. To address this gap, our approach starts from official IPCC figures. We gather their associated learning objectives and devise tests to score a pool of figure readers to assess how well they learn the objectives.We define improvement as higher scores obtained by a comparable reader pool after viewing a revised figure, where all modifications undergo review to ensure scientific validity. This assessment gives freedom to designers, who can deviate from the original design while making sure the objectives are still met and improved. We demonstrate the methodology through a case study and describe unexpected challenges encountered during the process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15514v2</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lu Ying, Junxiu Tang, Tingying He, Jean-Daniel Fekete</dc:creator>
    </item>
    <item>
      <title>Vibe Coding as a Reconfiguration of Intent Mediation in Software Development: Definition, Implications, and Research Agenda</title>
      <link>https://arxiv.org/abs/2507.21928</link>
      <description>arXiv:2507.21928v3 Announce Type: replace-cross 
Abstract: Software development is undergoing a fundamental transformation as vibe coding becomes widespread, with large portions of contemporary codebases now being generated by Artificial Intelligence (AI). The disconnect between rapid adoption and limited conceptual understanding highlights the need for an inquiry into this emerging paradigm. Drawing on an intent perspective and historical analysis, we define vibe coding as a software development paradigm where humans and Generative AI (GenAI) engage in collaborative flow to co-create software artifacts through natural language dialogue, shifting the mediation of developer intent from deterministic instruction to probabilistic inference. By intent mediation, we refer to the fundamental process through which developers translate their conceptual goals into representations that computational systems can execute. Our results show that vibe coding redistributes epistemic labor between humans and machines, shifting expertise from technical implementation toward collaborative orchestration. We identify key opportunities, including democratization, acceleration, and systemic leverage, alongside risks such as black-box codebases, responsibility gaps, and ecosystem bias. We conclude with a research agenda spanning human-, technology-, and organization-centered directions to guide future investigations of this paradigm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21928v3</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Meske, Tobias Hermanns, Esther von der Weiden, Kai-Uwe Loser, Thorsten Berger</dc:creator>
    </item>
    <item>
      <title>Advancing mathematics research with generative AI</title>
      <link>https://arxiv.org/abs/2511.07420</link>
      <description>arXiv:2511.07420v3 Announce Type: replace-cross 
Abstract: The main drawback of using generative AI models for advanced mathematics is that these models are not primarily logical reasoning engines. However, Large Language Models, and their refinements, can pick up on patterns in higher mathematics that are difficult for humans to see. By putting the design of generative AI models to their advantage, mathematicians may use them as powerful interactive assistants that can carry out laborious tasks, generate and debug code, check examples, formulate conjectures and more. We discuss how generative AI models can be used to advance mathematics research. We also discuss their integration with neuro-symbolic solvers, Computer Algebra Systems and formal proof assistants such as Lean.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07420v3</guid>
      <category>math.HO</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>math.GR</category>
      <category>math.LO</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lisa Carbone</dc:creator>
    </item>
    <item>
      <title>KarmaTS: A Universal Simulation Platform for Multivariate Time Series with Functional Causal Dynamics</title>
      <link>https://arxiv.org/abs/2511.11357</link>
      <description>arXiv:2511.11357v2 Announce Type: replace-cross 
Abstract: We introduce KarmaTS, an interactive framework for constructing lag-indexed, executable spatiotemporal causal graphical models for multivariate time series (MTS) simulation. Motivated by the challenge of access-restricted physiological data, KarmaTS generates synthetic MTS with known causal dynamics and augments real-world datasets with expert knowledge. The system constructs a discrete-time structural causal process (DSCP) by combining expert knowledge and algorithmic proposals in a mixed-initiative, human-in-the-loop workflow. The resulting DSCP supports simulation and causal interventions, including those under user-specified distribution shifts. KarmaTS handles mixed variable types, contemporaneous and lagged edges, and modular edge functionals ranging from parameterizable templates to neural network models. Together, these features enable flexible validation and benchmarking of causal discovery algorithms through expert-informed simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11357v2</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of Machine Learning Research 297, 2025</arxiv:journal_reference>
      <dc:creator>Haixin Li, Yanke Li, Diego Paez-Granados</dc:creator>
    </item>
  </channel>
</rss>
