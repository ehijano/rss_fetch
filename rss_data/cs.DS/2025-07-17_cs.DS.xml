<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 18 Jul 2025 04:00:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>DNA Probe Computing System for Solving NP-Complete Problems</title>
      <link>https://arxiv.org/abs/2507.12470</link>
      <description>arXiv:2507.12470v1 Announce Type: new 
Abstract: Efficiently solving NP-complete problems-such as protein structure prediction, cryptographic decryption, and vulnerability detection-remains a central challenge in computer science. Traditional electronic computers, constrained by the Turing machine's one-dimensional data processing and sequential operations, struggle to address these issues effectively. To overcome this bottleneck, computational models must adopt multidimensional data structures and parallel information processing mechanisms. Building on our team's proposed probe machine model (a non-Turing computational framework), this study develops a blocking probe technique that leverages DNA computing's inherent parallelism to identify all valid solutions for NP-complete problems in a single probe operation. Using the 27-vertex 3-coloring problem as a case study, we successfully retrieved all solutions through DNA molecular probe experiments. This breakthrough demonstrates the first implementation of a fully parallel computing system at the molecular level, offering a novel paradigm for tackling computational complexity. Our results indicate that the probe machine, with its parallel architecture and molecular implementation, transcends the limitations of classical models and holds promise for solving intricate real-world problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12470v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin Xu, XiaoLong Shi, Xin Chen, Fang Wang, Sirui Li, Pali Ye, Boliang Zhang, Di Deng, Zheng Kou, Xiaoli Qiang</dc:creator>
    </item>
    <item>
      <title>Max-Cut with Multiple Cardinality Constraints</title>
      <link>https://arxiv.org/abs/2507.12607</link>
      <description>arXiv:2507.12607v1 Announce Type: new 
Abstract: We study the classic Max-Cut problem under multiple cardinality constraints, which we refer to as the Constrained Max-Cut problem. Given a graph $G=(V, E)$, a partition of the vertices into $c$ disjoint parts $V_1, \ldots, V_c$, and cardinality parameters $k_1, \ldots, k_c$, the goal is to select a set $S \subseteq V$ such that $|S \cap V_i| = k_i$ for each $i \in [c]$, maximizing the total weight of edges crossing $S$ (i.e., edges with exactly one endpoint in $S$).
  By designing an approximate kernel for Constrained Max-Cut and building on the correlation rounding technique of Raghavendra and Tan (2012), we present a $(0.858 - \varepsilon)$-approximation algorithm for the problem when $c = O(1)$. The algorithm runs in time $O\left(\min\{k/\varepsilon, n\}^{\poly(c/\varepsilon)} + \poly(n)\right)$, where $k = \sum_{i \in [c]} k_i$ and $n=|V|$. This improves upon the $(\frac{1}{2} + \varepsilon_0)$-approximation of Feige and Langberg (2001) for $\maxcut_k$ (the special case when $c=1, k_1 = k$), and generalizes the $(0.858 - \varepsilon)$-approximation of Raghavendra and Tan (2012), which only applies when $\min\{k,n-k\}=\Omega(n)$ and does not handle multiple constraints.
  We also establish that, for general values of $c$, it is NP-hard to determine whether a feasible solution exists that cuts all edges. Finally, we present a $1/2$-approximation algorithm for Max-Cut under an arbitrary matroid constraint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12607v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yury Makarychev, Madhusudhan Reddy Pittu, Ali Vakilian</dc:creator>
    </item>
    <item>
      <title>Fast Approximate Rank Determination and Selection with Group Testing</title>
      <link>https://arxiv.org/abs/2507.12634</link>
      <description>arXiv:2507.12634v1 Announce Type: new 
Abstract: Suppose that a group test operation is available for checking order relations in a set, can this speed up problems like finding the minimum/maximum element, rank determination and selection? We consider a one-sided group test to be available, where queries are of the form $u \le_Q V$ or $V \le_Q u$, and the answer is `yes' if and only if there is some $v \in V$ such that $u \le v$ or $v \le u$, respectively. We restrict attention to total orders and focus on query-complexity; for min or max finding, we give a Las Vegas algorithm that makes $\mathcal{O}(\log^2 n)$ expected queries. We also give randomized approximate algorithms for rank determination and selection; we allow a relative error of $1 \pm \delta$ for $\delta &gt; 0$ in the estimated rank or selected element. In this case, we give a Monte Carlo algorithm for approximate rank determination with expected query complexity $\tilde{\mathcal{O}}(1/\delta^2 - \log \epsilon)$, where $1-\epsilon$ is the probability that the algorithm succeeds. We also give a Monte Carlo algorithm for approximate selection that has expected query complexity $\tilde{\mathcal{O}}(-\log( \epsilon \delta^2) / \delta^4)$; it has probability at least $\frac{1}{2}$ to output an element $x$, and if so, $x$ has the desired approximate rank with probability $1-\epsilon$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12634v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Adiesha Liyanage, Braeden Sopp, Brendan Mumey</dc:creator>
    </item>
    <item>
      <title>An EPTAS for multiprocessor scheduling with rejection under a machine cost constraint</title>
      <link>https://arxiv.org/abs/2507.12635</link>
      <description>arXiv:2507.12635v1 Announce Type: new 
Abstract: We study the multiprocessor scheduling with rejection problem under a machine cost constraint. In this problem, each job is either rejected with a rejection penalty or; accepted and scheduled on one of the machines for processing. The machine cost is proportional to the total processing time of the jobs scheduled on it. The problem aims to minimize the makespan of accepted jobs plus the total rejection penalty of rejected jobs while the total machine cost does not exceed a given upper bound. We present a simple $2$-approximation algorithm for the problem and we achieve an EPTAS when the number $m$ of machines is a fixed constant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12635v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mingyang Gong, Brendan Mumey</dc:creator>
    </item>
    <item>
      <title>Computing and Bounding Equilibrium Concentrations in Athermic Chemical Systems</title>
      <link>https://arxiv.org/abs/2507.12699</link>
      <description>arXiv:2507.12699v1 Announce Type: new 
Abstract: Computing equilibrium concentrations of molecular complexes is generally analytically intractable and requires numerical approaches. In this work we focus on the polymer-monomer level, where indivisible molecules (monomers) combine to form complexes (polymers). Rather than employing free-energy parameters for each polymer, we focus on the athermic setting where all interactions preserve enthalpy. This setting aligns with the strongly bonded (domain-based) regime in DNA nanotechnology when strands can bind in different ways, but always with maximum overall bonding -- and is consistent with the saturated configurations in the Thermodynamic Binding Networks (TBNs) model. Within this context, we develop an iterative algorithm for assigning polymer concentrations to satisfy detailed-balance, where on-target (desired) polymers are in high concentrations and off-target (undesired) polymers are in low. Even if not directly executed, our algorithm provides effective insights into upper bounds on concentration of off-target polymers, connecting combinatorial arguments about discrete configurations such as those in the TBN model to real-valued concentrations. We conclude with an application of our method to decreasing leak in DNA logic and signal propagation. Our results offer a new framework for design and verification of equilibrium concentrations when configurations are distinguished by entropic forces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12699v1</guid>
      <category>cs.DS</category>
      <category>q-bio.MN</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hamidreza Akef, Minki Hhan, David Soloveichik</dc:creator>
    </item>
    <item>
      <title>Splittable Spanning Trees and Balanced Forests in Dense Random Graphs</title>
      <link>https://arxiv.org/abs/2507.12707</link>
      <description>arXiv:2507.12707v1 Announce Type: new 
Abstract: Weighted equitable partitioning of a graph has been of interest lately due to several applications, including redistricting, network algorithms, and image decomposition. Weighting a partition according to the spanning-tree metric has been of mathematical and practical interest because it typically favors partitions with more compact pieces. An appealing algorithm suggested by Charikar et al. is to sample a random spanning tree and remove k-1 edges, producing a random forest. If the components of the forest form a balanced partition, the partition is equitable under an easily computed acceptance probability. Cannon et al. recently showed that spanning trees on grid graphs and grid-like graphs on $n$ vertices are splittable into $k$ equal sized pieces with probability at least $n^{-2k}$, leading to the first rigorous sampling algorithm for a class of graphs. We present complementary results showing that spanning trees on dense random graphs also have inverse polynomial probability of being splittable, giving another class of graphs where equitable partitions can be efficiently sampled exactly. These proofs also guarantee fast almost-uniform sampling for the up-down walk on forests, giving another provably efficient randomized method for generating equitable partitions.
  Further, we show that problems with the well-studied ReCom algorithm for equitable partitioning are more extensive than previously known, even in special cases that were believed to be more promising. We present a family of graphs where the Markov chain fails to be irreducible when it must keep the components perfectly equitable; yet when the chain is allowed an imbalance of just one vertex between components, the rejection sampling step may take exponential time. This is true even when the graph satisfies desirable properties that have been conjectured to be sufficient for fast sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12707v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Gillman, Jacob Platnick, Dana Randall</dc:creator>
    </item>
    <item>
      <title>Waiting is worth it and can be improved with predictions</title>
      <link>https://arxiv.org/abs/2507.12822</link>
      <description>arXiv:2507.12822v1 Announce Type: new 
Abstract: We revisit the well-known online traveling salesman problem (OLTSP) and its extension, the online dial-a-ride problem (OLDARP). A server starting at a designated origin in a metric space, is required to serve online requests, and return to the origin such that the completion time is minimized. The SmartStart algorithm, introduced by Ascheuer et al., incorporates a waiting approach into an online schedule-based algorithm and attains the optimal upper bound of 2 for the OLTSP and the OLDARP if each schedule is optimal. Using the Christofides' heuristic to approximate each schedule leads to the currently best upper bound of (7 + sqrt(13)) / 4 approximately 2.6514 in polynomial time.
  In this study, we investigate how an online algorithm with predictions, a recent popular framework (i.e. the so-called learning-augmented algorithms), can be used to improve the best competitive ratio in polynomial time. In particular, we develop a waiting strategy with online predictions, each of which is only a binary decision-making for every schedule in a whole route, rather than forecasting an entire set of requests in the beginning (i.e. offline predictions). That is, it does not require knowing the number of requests in advance. The proposed online schedule-based algorithm can achieve 1.1514 * lambda + 1.5-consistency and 1.5 + 1.5 / (2.3028 * lambda - 1)-robustness in polynomial time, where lambda lies in the interval (1/theta, 1] and theta is set to (1 + sqrt(13)) / 2 approximately 2.3028. The best consistency tends to approach to 2 when lambda is close to 1/theta. Meanwhile, we show any online schedule-based algorithms cannot derive a competitive ratio of less than 2 even with perfect online predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12822v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ya-Chun Liang, Meng-Hsi Li, Chung-Shou Liao, Clifford Stein</dc:creator>
    </item>
    <item>
      <title>Cut-Matching Games for Bipartiteness Ratio of Undirected Graphs</title>
      <link>https://arxiv.org/abs/2507.12847</link>
      <description>arXiv:2507.12847v1 Announce Type: new 
Abstract: We propose an $O(\log n)$-approximation algorithm for the bipartiteness ratio for undirected graphs introduced by Trevisan (SIAM Journal on Computing, vol. 41, no. 6, 2012), where $n$ is the number of vertices. Our approach extends the cut-matching game framework for sparsest cut to the bipartiteness ratio. Our algorithm requires only $\mathrm{poly}\log n$ many single-commodity undirected maximum flow computations. Therefore, with the current fastest undirected max-flow algorithms, it runs in nearly linear time. Along the way, we introduce the concept of well-linkedness for skew-symmetric graphs and prove a novel characterization of bipartitness ratio in terms of well-linkedness in an auxiliary skew-symmetric graph, which may be of independent interest.
  As an application, we devise an $\tilde{O}(mn)$-time algorithm that given a graph whose maximum cut deletes a $1-\eta$ fraction of edges, finds a cut that deletes a $1 - O(\log n \log(1/\eta)) \cdot \eta$ fraction of edges, where $m$ is the number of edges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12847v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tasuku Soma, Mingquan Ye, Yuichi Yoshida</dc:creator>
    </item>
    <item>
      <title>A 1/2-Approximation for Budgeted $k$-Submodular Maximization</title>
      <link>https://arxiv.org/abs/2507.12875</link>
      <description>arXiv:2507.12875v1 Announce Type: new 
Abstract: A $k$-submodular function naturally generalizes submodular functions by taking as input $k$ disjoint subsets, rather than a single subset. Unlike standard submodular maximization, which only requires selecting elements for the solution, $k$-submodular maximization adds the challenge of determining the subset to which each selected element belongs. Prior research has shown that the greedy algorithm is a 1/2-approximation for the monotone $k$-submodular maximization problem under cardinality or matroid constraints. However, whether a firm 1/2-approximation exists for the budgeted version (i.e., with a knapsack constraint) has remained open for several years. We resolve this question affirmatively by proving that the 1-Guess Greedy algorithm, which first guesses an appropriate element from an optimal solution before proceeding with the greedy algorithm, achieves a 1/2-approximation. This result is asymptotically tight as $((k+1)/(2k)+\epsilon)$-approximation requires exponentially many value oracle queries even without constraints (Iwata et al., SODA 2016). We further show that 1-Guess Greedy is 1/3-approximation for the non-monotone problem. This algorithm is both simple and parallelizable, making it well-suited for practical applications. Using the thresholding technique from (Badanidiyuru and Vondrak, SODA 2014), it runs in nearly $\tilde O(n^2k^2)$ time.
  The proof idea is simple: we introduce a novel continuous transformation from an optimal solution to a greedy solution, using the multilinear extension to evaluate every fractional solution during the transformation. This continuous analysis approach yields two key extensions. First, it enables improved approximation ratios of various existing algorithms. Second, our method naturally extends to $k$-submodular maximization problems under broader constraints, offering a more flexible and unified analysis framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12875v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.OC</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenhao Wang</dc:creator>
    </item>
    <item>
      <title>Efficient Semi-External Breadth-First Search</title>
      <link>https://arxiv.org/abs/2507.12925</link>
      <description>arXiv:2507.12925v1 Announce Type: new 
Abstract: Breadth-first search (BFS) is known as a basic search strategy for learning graph properties. As the scales of graph databases have increased tremendously in recent years, large-scale graphs G are often disk-resident. Obtaining the BFS results of G in semi-external memory model is inevitable, because the in-memory BFS algorithm has to maintain the entire G in the main memory, and external BFS algorithms consume high computational costs. As a good trade-off between the internal and external memory models, semi-external memory model assumes that the main memory can at least reside a spanning tree of G. Nevertheless, the semi-external BFS problem is still an open issue due to its difficulty. Therefore, this paper presents a comprehensive study for processing BFS in semi-external memory model. After discussing the naive solutions based on the basic framework of semi-external graph algorithms, this paper presents an efficient algorithm, named EP-BFS, with a small minimum memory space requirement, which is an important factor for evaluating semi-external algorithms. Extensive experiments are conducted on both real and synthetic large-scale graphs, where graph WDC-2014 contains over 1.7 billion nodes, and graph eu-2015 has over 91 billion edges. Experimental results confirm that EP-BFS can achieve up to 10 times faster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12925v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaolong Wan, Xixian Han</dc:creator>
    </item>
    <item>
      <title>The Price of Diversity of the Traveling Salesman Problem</title>
      <link>https://arxiv.org/abs/2507.13026</link>
      <description>arXiv:2507.13026v1 Announce Type: new 
Abstract: This paper introduces the concept of the "Price of Diversity" (PoD) in discrete optimization problems, quantifying the trade-off between solution diversity and cost. For a minimization problem, the PoD is defined as the worst-case ratio, over all instances, of the minimum achievable cost of a diverse set of $k$ solutions to the cost of a single optimal solution for the same instance. Here, the cost of a $k$-solution set is determined by the most expensive solution within the set. Focusing on the Traveling Salesman Problem (TSP) as a key example, we study the PoD in the setting where $k$ edge-disjoint tours are required. We establish that, asymptotically, the PoD of finding two edge-disjoint tours is $\frac{8}{5}$ in a special one-dimensional case and 2 in a general metric space. We obtain these results from analyzing a related fundamental problem: the Shortest Hamiltonian Path problem (SHP), for which we establish similar results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13026v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark de Berg, Andr\'es L\'opez Mart\'inez, Frits Spieksma</dc:creator>
    </item>
    <item>
      <title>Maintaining Routing Structures under Deletions via Self-Pruning</title>
      <link>https://arxiv.org/abs/2507.13044</link>
      <description>arXiv:2507.13044v1 Announce Type: new 
Abstract: Expanders are powerful algorithmic structures with two key properties: they are
  a) routable: for any multi-commodity flow unit demand, there exists a routing with low congestion over short paths, where a demand is unit if the amount of demand sent / received by any vertex is at most the number of edges adjacent to it.
  b) stable / prunable: for any (sequence of) edge failures, there exists a proportionally small subset of vertices that can be disabled, such that the graph induced on the remaining vertices is an expander.
  Two natural algorithmic problems correspond to these two existential guarantees: expander routing, i.e. computing a low-congestion routing for a unit multi-commodity demand on an expander, and expander pruning, i.e., maintaining the subset of disabled vertices under a sequence of edge failures.
  This paper considers the combination of the two problems: maintaining a routing for a unit multi-commodity demand under pruning steps. This is done through the introduction of a family of expander graphs that, like hypercubes, are easy to route in, and are self-pruning: for an online sequence of edge deletions, a simple self-contained algorithm can find a few vertices to prune with each edge deletion, such that the remaining graph always remains an easy-to-route-in expander in the family.
  Notably, and with considerable technical work, this self-pruning can be made worst-case, i.e., such that every single adversarial deletion only causes a small number of additional deletions. Our results also allow tight constant-factor control over the length of routing paths (with the usual trade-offs in congestion and pruning ratio) and therefore extend to constant-hop and length-constrained expanders in which routing over constant length paths is crucial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13044v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bernhard Haeupler, Antti Roeyskoe</dc:creator>
    </item>
    <item>
      <title>Kernelization for $H$-Coloring</title>
      <link>https://arxiv.org/abs/2507.13129</link>
      <description>arXiv:2507.13129v1 Announce Type: new 
Abstract: For a fixed graph $H$, the $H$-Coloring problem asks whether a given graph admits an edge-preserving function from its vertex set to that of $H$. A seminal theorem of Hell and Ne\v{s}et\v{r}il asserts that the $H$-Coloring problem is NP-hard whenever $H$ is loopless and non-bipartite. A result of Jansen and Pieterse implies that for every graph $H$, the $H$-Coloring problem parameterized by the vertex cover number $k$ admits a kernel with $O(k^{\Delta(H)})$ vertices and bit-size bounded by $O(k^{\Delta(H)} \cdot \log k)$, where $\Delta(H)$ denotes the maximum degree in $H$. For the case where $H$ is a complete graph on at least three vertices, this kernel size nearly matches conditional lower bounds established by Jansen and Kratsch and by Jansen and Pieterse.
  This paper presents new upper and lower bounds on the kernel size of $H$-Coloring problems parameterized by the vertex cover number. The upper bounds arise from two kernelization algorithms. The first is purely combinatorial, and its size is governed by a structural quantity of the graph $H$, called the non-adjacency witness number. As applications, we obtain kernels whose size is bounded by a fixed polynomial for natural classes of graphs $H$ with unbounded maximum degree. More strikingly, we show that for almost every graph $H$, the degree of the polynomial that bounds the size of our combinatorial kernel grows only logarithmically in $\Delta(H)$. Our second kernel leverages linear-algebraic tools and involves the notion of faithful independent representations of graphs. It strengthens the general bound from prior work and, among other applications, yields near-optimal kernels for problems concerning the dimension of orthogonal graph representations over finite fields. We complement these results with conditional lower bounds, thereby nearly settling the kernel complexity of the problem for various target graphs $H$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13129v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yael Berkman, Ishay Haviv</dc:creator>
    </item>
    <item>
      <title>Online Rounding for Set Cover under Subset Arrivals</title>
      <link>https://arxiv.org/abs/2507.13159</link>
      <description>arXiv:2507.13159v1 Announce Type: new 
Abstract: A rounding scheme for set cover has served as an important component in design of approximation algorithms for the problem, and there exists an H_s-approximate rounding scheme, where s denotes the maximum subset size, directly implying an approximation algorithm with the same approximation guarantee. A rounding scheme has also been considered under some online models, and in particular, under the element arrival model used as a crucial subroutine in algorithms for online set cover, an O(log s)-competitive rounding scheme is known [Buchbinder, Chen, and Naor, SODA 2014]. On the other hand, under a more general model, called the subset arrival model, only a simple O(log n)-competitive rounding scheme is known, where n denotes the number of elements in the ground set.
  In this paper, we present an O(log^2 s)-competitive rounding scheme under the subset arrival model, with one mild assumption that s is known upfront. Using our rounding scheme, we immediately obtain an O(log^2 s)-approximation algorithm for multi-stage stochastic set cover, improving upon the existing algorithms [Swamy and Shmoys, SICOMP 2012; Byrka and Srinivasan, SIDMA 2018] when s is small enough compared to the number of stages and the number of elements. Lastly, for set cover with s = 2, also known as edge cover, we present a 1.8-competitive rounding scheme under the edge arrival model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13159v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaros{\l}aw Byrka, Yongho Shin</dc:creator>
    </item>
    <item>
      <title>Efficiently Constructing Sparse Navigable Graphs</title>
      <link>https://arxiv.org/abs/2507.13296</link>
      <description>arXiv:2507.13296v1 Announce Type: new 
Abstract: Graph-based nearest neighbor search methods have seen a surge of popularity in recent years, offering state-of-the-art performance across a wide variety of applications. Central to these methods is the task of constructing a sparse navigable search graph for a given dataset endowed with a distance function. Unfortunately, doing so is computationally expensive, so heuristics are universally used in practice.
  In this work, we initiate the study of fast algorithms with provable guarantees for search graph construction. For a dataset with $n$ data points, the problem of constructing an optimally sparse navigable graph can be framed as $n$ separate but highly correlated minimum set cover instances. This yields a naive $O(n^3)$ time greedy algorithm that returns a navigable graph whose sparsity is at most $O(\log n)$ higher than optimal. We improve significantly on this baseline, taking advantage of correlation between the set cover instances to leverage techniques from streaming and sublinear-time set cover algorithms. Combined with problem-specific pre-processing techniques, we present an $\tilde{O}(n^2)$ time algorithm for constructing an $O(\log n)$-approximate sparsest navigable graph under any distance function.
  The runtime of our method is optimal up to logarithmic factors under the Strong Exponential Time Hypothesis via a reduction from Monochromatic Closest Pair. Moreover, we prove that, as with general set cover, obtaining better than an $O(\log n)$-approximation is NP-hard, despite the significant additional structure present in the navigable graph problem. Finally, we show that our techniques can also beat cubic time for the closely related and practically important problems of constructing $\alpha$-shortcut reachable and $\tau$-monotonic graphs, which are also used for nearest neighbor search. For such graphs, we obtain $\tilde{O}(n^{2.5})$ time or better algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13296v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <category>cs.IR</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Conway, Laxman Dhulipala, Martin Farach-Colton, Rob Johnson, Ben Landrum, Christopher Musco, Yarin Shechter, Torsten Suel, Richard Wen</dc:creator>
    </item>
    <item>
      <title>Analysis of Langevin midpoint methods using an anticipative Girsanov theorem</title>
      <link>https://arxiv.org/abs/2507.12791</link>
      <description>arXiv:2507.12791v1 Announce Type: cross 
Abstract: We introduce a new method for analyzing midpoint discretizations of stochastic differential equations (SDEs), which are frequently used in Markov chain Monte Carlo (MCMC) methods for sampling from a target measure $\pi \propto \exp(-V)$. Borrowing techniques from Malliavin calculus, we compute estimates for the Radon-Nikodym derivative for processes on $L^2([0, T); \mathbb{R}^d)$ which may anticipate the Brownian motion, in the sense that they may not be adapted to the filtration at the same time. Applying these to various popular midpoint discretizations, we are able to improve the regularity and cross-regularity results in the literature on sampling methods. We also obtain a query complexity bound of $\widetilde{O}(\frac{\kappa^{5/4} d^{1/4}}{\varepsilon^{1/2}})$ for obtaining a $\varepsilon^2$-accurate sample in $\mathsf{KL}$ divergence, under log-concavity and strong smoothness assumptions for $\nabla^2 V$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12791v1</guid>
      <category>math.NA</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew S. Zhang</dc:creator>
    </item>
    <item>
      <title>Computational-Statistical Tradeoffs from NP-hardness</title>
      <link>https://arxiv.org/abs/2507.13222</link>
      <description>arXiv:2507.13222v1 Announce Type: cross 
Abstract: A central question in computer science and statistics is whether efficient algorithms can achieve the information-theoretic limits of statistical problems. Many computational-statistical tradeoffs have been shown under average-case assumptions, but since statistical problems are average-case in nature, it has been a challenge to base them on standard worst-case assumptions.
  In PAC learning where such tradeoffs were first studied, the question is whether computational efficiency can come at the cost of using more samples than information-theoretically necessary. We base such tradeoffs on $\mathsf{NP}$-hardness and obtain:
  $\circ$ Sharp computational-statistical tradeoffs assuming $\mathsf{NP}$ requires exponential time: For every polynomial $p(n)$, there is an $n$-variate class $C$ with VC dimension $1$ such that the sample complexity of time-efficiently learning $C$ is $\Theta(p(n))$.
  $\circ$ A characterization of $\mathsf{RP}$ vs. $\mathsf{NP}$ in terms of learning: $\mathsf{RP} = \mathsf{NP}$ iff every $\mathsf{NP}$-enumerable class is learnable with $O(\mathrm{VCdim}(C))$ samples in polynomial time. The forward implication has been known since (Pitt and Valiant, 1988); we prove the reverse implication.
  Notably, all our lower bounds hold against improper learners. These are the first $\mathsf{NP}$-hardness results for improperly learning a subclass of polynomial-size circuits, circumventing formal barriers of Applebaum, Barak, and Xiao (2008).</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13222v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guy Blanc, Caleb Koch, Carmen Strassle, Li-Yang Tan</dc:creator>
    </item>
    <item>
      <title>Parameterized algorithms for block-structured integer programs with large entries</title>
      <link>https://arxiv.org/abs/2311.01890</link>
      <description>arXiv:2311.01890v2 Announce Type: replace 
Abstract: We study two classic variants of block-structured integer programming. Two-stage stochastic programs are integer programs of the form $\{A_i \mathbf{x} + D_i \mathbf{y}_i = \mathbf{b}_i\textrm{ for all }i=1,\ldots,n\}$, where $A_i$ and $D_i$ are bounded-size matrices. On the other hand, $n$-fold programs are integer programs of the form $\{{\sum_{i=1}^n C_i\mathbf{y}_i=\mathbf{a}} \textrm{ and } D_i\mathbf{y}_i=\mathbf{b}_i\textrm{ for all }i=1,\ldots,n\}$, where again $C_i$ and $D_i$ are bounded-size matrices. It is known that solving these kind of programs is fixed-parameter tractable when parameterized by the maximum dimension among the relevant matrices $A_i,C_i,D_i$ and the maximum absolute value of any entry appearing in the constraint matrix.
  We show that the parameterized tractability results for two-stage stochastic and $n$-fold programs persist even when one allows large entries in the global part of the program. More precisely, we prove that:
  - The feasibility problem for two-stage stochastic programs is fixed-parameter tractable when parameterized by the dimensions of matrices $A_i,D_i$ and by the maximum absolute value of the entries of matrices $D_i$. That is, we allow matrices $A_i$ to have arbitrarily large entries.
  - The linear optimization problem for $n$-fold integer programs that are uniform -- all matrices $C_i$ are equal -- is fixed-parameter tractable when parameterized by the dimensions of matrices $C_i$ and $D_i$ and by the maximum absolute value of the entries of matrices $D_i$. That is, we require that $C_i=C$ for all $i=1,\ldots,n$, but we allow $C$ to have arbitrarily large entries.
  In the second result, the uniformity assumption is necessary; otherwise the problem is $\mathsf{NP}$-hard already when the parameters take constant values. Both our algorithms are weakly polynomial: the running time is measured in the total bitsize of the input.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01890v2</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.46298/theoretics.25.15</arxiv:DOI>
      <arxiv:journal_reference>TheoretiCS, Volume 4 (2025), Article 15, 1-49</arxiv:journal_reference>
      <dc:creator>Jana Cslovjecsek, Martin Kouteck\'y, Alexandra Lassota, Micha{\l} Pilipczuk, Adam Polak</dc:creator>
    </item>
    <item>
      <title>Approximate counting of permutation patterns</title>
      <link>https://arxiv.org/abs/2411.04718</link>
      <description>arXiv:2411.04718v2 Announce Type: replace 
Abstract: We consider the problem of counting the copies of a length-$k$ pattern $\sigma$ in a sequence $f \colon [n] \to \mathbb{R}$, where a copy is a subset of indices $i_1 &lt; \ldots &lt; i_k \in [n]$ such that $f(i_j) &lt; f(i_\ell)$ if and only if $\sigma(j) &lt; \sigma(\ell)$. This problem is motivated by a range of connections and applications in ranking, nonparametric statistics, combinatorics, and fine-grained complexity, especially when $k$ is a small fixed constant.
  Recent advances have significantly improved our understanding of counting and detecting patterns. Guillemot and Marx [2014] obtained an $O(n)$ time algorithm for the detection variant for any fixed $k$. Their proof has laid the foundations for the discovery of the twin-width, a concept that has notably advanced parameterized complexity in recent years. Counting, in contrast, is harder: it has a conditional lower bound of $n^{\Omega(k / \log k)}$ [Berendsohn, Kozma, and Marx, 2019] and is expected to be polynomially harder than detection as early as $k = 4$, given its equivalence to counting $4$-cycles in graphs [Dudek and Gawrychowski, 2020].
  In this work, we design a deterministic near-linear time $(1+\varepsilon)$-approximation algorithm for counting $\sigma$-copies in $f$ for all $k \leq 5$. Combined with the conditional lower bound for $k=4$, this establishes the first known separation between approximate and exact pattern counting. Interestingly, our algorithm leverages the Birg\'e decomposition -- a sublinear tool for monotone distributions widely used in distribution testing -- which, to our knowledge, has not been used in a pattern counting context before. Along the way, we develop a near-optimal data structure for $(1+\varepsilon)$-approximate increasing pair range queries in the plane, which exhibits a conditional separation from the exact case and may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04718v2</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Omri Ben-Eliezer, Slobodan Mitrovi\'c, Pranjal Srivastava</dc:creator>
    </item>
    <item>
      <title>Faster and Space Efficient Indexing for Locality Sensitive Hashing</title>
      <link>https://arxiv.org/abs/2503.06737</link>
      <description>arXiv:2503.06737v2 Announce Type: replace 
Abstract: This work suggests faster and space-efficient index construction algorithms for LSH for Euclidean distance (\textit{a.k.a.}~\ELSH) and cosine similarity (\textit{a.k.a.}~\SRP). The index construction step of these LSHs relies on grouping data points into several bins of hash tables based on their hashcode. To generate an $m$-dimensional hashcode of the $d$-dimensional data point, these LSHs first project the data point onto a $d$-dimensional random Gaussian vector and then discretise the resulting inner product. The time and space complexity of both \ELSH~and \SRP~for computing an $m$-sized hashcode of a $d$-dimensional vector is $O(md)$, which becomes impractical for large values of $m$ and $d$. To overcome this problem, we propose two alternative LSH hashcode generation algorithms, both for Euclidean distance and cosine similarity, namely, \CSELSH, \HCSELSH~and \CSSRP, \HCSSRP, respectively. \CSELSH~and \CSSRP~are based on count sketch \cite{count_sketch} and \HCSELSH~and \HCSSRP~utilize higher-order count sketch \cite{shi2019higher}. These proposals significantly reduce the hashcode computation time from $O(md)$ to $O(d)$. Additionally, both \CSELSH~and \CSSRP~reduce the space complexity from $O(md)$ to $O(d)$; ~and \HCSELSH, \HCSSRP~ reduce the space complexity from $O(md)$ to $O(N \sqrt[N]{d})$ respectively, where $N\geq 1$ denotes the size of the input/reshaped tensor. Our proposals are backed by strong mathematical guarantees, and we validate their performance through simulations on various real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06737v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bhisham Dev Verma, Rameshwar Pratap</dc:creator>
    </item>
    <item>
      <title>Beyond Worst-Case Subset Sum: An Adaptive, Structure-Aware Solver with Sub-$2^{n/2}$ Enumeration</title>
      <link>https://arxiv.org/abs/2503.20162</link>
      <description>arXiv:2503.20162v2 Announce Type: replace 
Abstract: The Subset Sum problem, which asks whether a set of $n$ integers has a subset summing to a target $t$, is a fundamental NP-complete problem in cryptography and combinatorial optimization. The classical meet-in-the-middle (MIM) algorithm of Horowitz--Sahni runs in $\mathcal{O}^*(2^{n/2})$, which remains the best-known deterministic bound. Yet in practice, many instances exhibit abundant collisions in partial sums, so the true difficulty is often governed by $U = |\Sigma(S)|$, the number of unique subset sums.
  We present a structure-aware, adaptive solver that enumerates only the distinct subset sums, pruning duplicates on the fly and achieving deterministic runtime $\mathcal{O}(U \cdot n^2)$ and expected randomized runtime $\mathcal{O}(U \cdot n)$. Its core is a canonical unique-subset-sums enumerator combined with a double meet-in-the-middle strategy, supporting anytime and online modes.
  To ensure worst-case gains even on unstructured inputs, we introduce a Controlled Aliasing technique that provably reduces the enumeration space by a fixed constant factor. This yields a guaranteed global runtime of $\mathcal{O}^*(2^{n/2 - \varepsilon})$ for some $\varepsilon &gt; 0$, strictly improving upon classical bounds.
  Empirical results show that the solver adapts efficiently to structured inputs with low entropy (e.g., instances with small doubling constants, duplicates, or additive progressions) often approaching near-dynamic programming performance. We conclude by outlining how this adaptive framework can be extended to other NP-complete problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20162v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesus Salas</dc:creator>
    </item>
    <item>
      <title>List Decoding Expander-Based Codes up to Capacity in Near-Linear Time</title>
      <link>https://arxiv.org/abs/2504.20333</link>
      <description>arXiv:2504.20333v2 Announce Type: replace 
Abstract: We give a new framework based on graph regularity lemmas, for list decoding and list recovery of codes based on spectral expanders. Using existing algorithms for computing regularity decompositions of sparse graphs in (randomized) near-linear time, and appropriate choices for the constant-sized inner/base codes, we prove the following:
  - Expander-based codes constructed using the distance amplification technique of Alon, Edmonds and Luby [FOCS 1995] with rate $\rho$, can be list decoded to a radius $1 - \rho - \epsilon$ in near-linear time. By known results, the output list has size $O(1/\epsilon)$.
  - The above codes of Alon, Edmonds and Luby, with rate $\rho$, can also be list recovered to radius $1 - \rho - \epsilon$ in near-linear time, with constant-sized output lists.
  - The Tanner code construction of Sipser and Spielman [IEEE Trans. Inf. Theory 1996] with distance $\delta$, can be list decoded to radius $\delta - \epsilon$ in near-linear time, with constant-sized output lists.
  Our results imply novel combinatorial as well as algorithmic bounds for each of the above explicit constructions. All of these bounds are obtained via combinatorial rigidity phenomena, proved using (weak) graph regularity. The regularity framework allows us to lift the list decoding and list recovery properties for the local base codes, to the global codes obtained via the above constructions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20333v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shashank Srivastava, Madhur Tulsiani</dc:creator>
    </item>
    <item>
      <title>Compressing Suffix Trees by Path Decompositions</title>
      <link>https://arxiv.org/abs/2506.14734</link>
      <description>arXiv:2506.14734v3 Announce Type: replace 
Abstract: In this paper, we solve the long-standing problem of designing I/O-efficient compressed indexes. Our solution broadly consists of generalizing suffix sorting and revisiting suffix tree path compression. In classic suffix trees, path compression works by replacing unary suffix trie paths with pairs of pointers to $T$, which must be available in the form of some random access oracle at query time. In our approach, instead, we (i) sort the suffix tree's leaves according to a more general priority function $\pi$ (generalizing suffix sorting), (ii) we build a suffix tree path decomposition prioritizing the leftmost paths in such an order, and (iii) we path-compress the decomposition's paths as pointers to a small subset of the string's suffixes. At this point, we show that the colexicographically-sorted array of those pointers represents a new elegant, simple, and remarkably I/O-efficient compressed suffix tree. For instance, by taking $\pi$ to be the lexicographic rank of $T$'s suffixes, we can compress the suffix tree topology in $O(r)$ space on top of a $n\log\sigma + O(\log n)$-bits text representation while essentially matching the pattern matching I/O complexity of Weiner and McCreight's suffix tree. Another (more practical) solution is obtained by taking $\pi$ to be the colexicographic rank of $T$'s prefixes and using a fully-compressed random access oracle. The resulting self-index allows us to locate all occurrences of a given query pattern in less space and orders of magnitude faster than the $r$-index.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14734v3</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruben Becker, Davide Cenzato, Travis Gagie, Sung-Hwan Kim, Ragnar Groot Koerkamp, Giovanni Manzini, Nicola Prezza</dc:creator>
    </item>
    <item>
      <title>Do you know what q-means?</title>
      <link>https://arxiv.org/abs/2308.09701</link>
      <description>arXiv:2308.09701v3 Announce Type: replace-cross 
Abstract: Clustering is one of the most important tools for analysis of large datasets, and perhaps the most popular clustering algorithm is Lloyd's algorithm for $k$-means. This algorithm takes $n$ vectors $V=[v_1,\dots,v_n]\in\mathbb{R}^{d\times n}$ and outputs $k$ centroids $c_1,\dots,c_k\in\mathbb{R}^d$; these partition the vectors into clusters based on which centroid is closest to a particular vector. We present a classical $\varepsilon$-$k$-means algorithm that performs an approximate version of one iteration of Lloyd's algorithm with time complexity $\tilde{O}\big(\frac{\|V\|_F^2}{n}\frac{k^{2}d}{\varepsilon^2}(k + \log{n})\big)$, exponentially improving the dependence on the data size $n$ and matching that of the "$q$-means" quantum algorithm originally proposed by Kerenidis, Landman, Luongo, and Prakash (NeurIPS'19). Moreover, we propose an improved $q$-means quantum algorithm with time complexity $\tilde{O}\big(\frac{\|V\|_F}{\sqrt{n}}\frac{k^{3/2}d}{\varepsilon}(\sqrt{k}+\sqrt{d})(\sqrt{k} + \log{n})\big)$ that quadratically improves the runtime of our classical $\varepsilon$-$k$-means algorithm in several parameters. Our quantum algorithm does not rely on quantum linear algebra primitives of prior work, but instead only uses QRAM to prepare simple states based on the current iteration's clusters and multivariate quantum amplitude estimation. Finally, we provide classical and quantum query lower bounds, showing that our algorithms are optimal in most parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.09701v3</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arjan Cornelissen, Joao F. Doriguello, Alessandro Luongo, Ewin Tang</dc:creator>
    </item>
    <item>
      <title>Untangling Graphs on Surfaces</title>
      <link>https://arxiv.org/abs/2311.00437</link>
      <description>arXiv:2311.00437v2 Announce Type: replace-cross 
Abstract: Consider a graph drawn on a surface (for example, the plane minus a finite set of obstacle points), possibly with crossings. We provide an algorithm to decide whether such a drawing can be untangled, namely, if one can slide the vertices and edges of the graph on the surface (avoiding the obstacles) to remove all crossings; in other words, whether the drawing is homotopic to an embedding. While the problem boils down to planarity testing when the surface is the sphere or the disk (or equivalently the plane without any obstacle), the other cases have never been studied before, except when the input graph is a cycle, in an abundant literature in topology and more recently by Despr\'e and Lazarus [SoCG 2017, J. ACM 2019].
  Our algorithm runs in O(m + poly(g+b) n log n) time, where g &gt;= 0 and b &gt;= 0 are the genus and the number of boundary components of the input orientable surface S, and n is the size of the input graph drawing, lying on some fixed graph of size m cellularly embedded on S.
  We use various techniques from two-dimensional computational topology and from the theory of hyperbolic surfaces. Most notably, we introduce reducing triangulations, a novel discrete analog of hyperbolic surfaces in the spirit of systems of quads by Lazarus and Rivaud [FOCS 2012] and Erickson and Whittlesey [SODA 2013], which have the additional benefit that reduced paths are unique and stable upon reversal; they are likely of independent interest. Tailored data structures are needed to achieve certain homotopy tests efficiently on these triangulations. As a key subroutine, we rely on an algorithm to test the weak simplicity of a graph drawn on a surface by Akitaya, Fulek, and T\'oth [SODA 2018, TALG 2019].</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00437v2</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>\'Eric Colin de Verdi\`ere, Vincent Despr\'e, Lo\"ic Dubois</dc:creator>
    </item>
    <item>
      <title>Weighted Pseudorandom Generators for Read-Once Branching Programs via Weighted Pseudorandom Reductions</title>
      <link>https://arxiv.org/abs/2502.08272</link>
      <description>arXiv:2502.08272v3 Announce Type: replace-cross 
Abstract: We study weighted pseudorandom generators (WPRGs) and derandomizations for read-once branching programs (ROBPs). Denote $n$ and $w$ as the length and the width of a ROBP. We have the following results.
  For standard ROBPs, we give an explicit $\varepsilon$-WPRG with seed length
  $$O\left(\frac{\log n\log (nw)}{\max\left\{1,\log\log w-\log\log n\right\}}+\log w \left(\log\log\log w-\log\log\max\left\{2,\frac{\log w}{\log \frac{n}{\varepsilon}}\right\}\right)+\log\frac{1}{\varepsilon}\right).$$
  For permutation ROBPs with unbounded widths and single accept nodes, we give an explicit $\varepsilon$-WPRG with seed length
  $$O\left( \log n\left( \log\log n + \sqrt{\log(1/\varepsilon)} \right)+\log(1/\varepsilon)\right). $$
  We also give a new Nisan-Zuckerman style derandomization for regular ROBPs with width $w$, length $n = 2^{O(\sqrt{\log w})}$, and multiple accept nodes. We attain optimal space complexity $O(\log w)$ for arbitrary approximation error $\varepsilon = 1/\text{poly} (w)$.
  All our results are based on iterative weighted pseudorandom reductions, which can iteratively reduce fooling long ROBPs to fooling short ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08272v3</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kuan Cheng, Ruiyang Wu</dc:creator>
    </item>
  </channel>
</rss>
