<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Feb 2025 04:16:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On Space Folds of ReLU Neural Networks</title>
      <link>https://arxiv.org/abs/2502.09954</link>
      <description>arXiv:2502.09954v1 Announce Type: cross 
Abstract: Recent findings suggest that the consecutive layers of ReLU neural networks can be understood geometrically as space folding transformations of the input space, revealing patterns of self-similarity. In this paper, we present the first quantitative analysis of this space folding phenomenon in ReLU neural networks. Our approach focuses on examining how straight paths in the Euclidean input space are mapped to their counterparts in the Hamming activation space. In this process, the convexity of straight lines is generally lost, giving rise to non-convex folding behavior. To quantify this effect, we introduce a novel measure based on range metrics, similar to those used in the study of random walks, and provide the proof for the equivalence of convexity notions between the input and activation spaces. Furthermore, we provide empirical analysis on a geometrical analysis benchmark (CantorNet) as well as an image classification benchmark (MNIST). Our work advances the understanding of the activation space in ReLU neural networks by leveraging the phenomena of geometric folding, providing valuable insights on how these models process input information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09954v1</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michal Lewandowski, Hamid Eghbalzadeh, Bernhard Heinzl, Raphael Pisoni, Bernhard A. Moser</dc:creator>
    </item>
    <item>
      <title>Dynamic Reinforcement Learning for Actors</title>
      <link>https://arxiv.org/abs/2502.10200</link>
      <description>arXiv:2502.10200v1 Announce Type: cross 
Abstract: Dynamic Reinforcement Learning (Dynamic RL), proposed in this paper, directly controls system dynamics, instead of the actor (action-generating neural network) outputs at each moment, bringing about a major qualitative shift in reinforcement learning (RL) from static to dynamic. The actor is initially designed to generate chaotic dynamics through the loop with its environment, enabling the agent to perform flexible and deterministic exploration. Dynamic RL controls global system dynamics using a local index called "sensitivity," which indicates how much the input neighborhood contracts or expands into the corresponding output neighborhood through each neuron's processing. While sensitivity adjustment learning (SAL) prevents excessive convergence of the dynamics, sensitivity-controlled reinforcement learning (SRL) adjusts them -- to converge more to improve reproducibility around better state transitions with positive TD error and to diverge more to enhance exploration around worse transitions with negative TD error. Dynamic RL was applied only to the actor in an Actor-Critic RL architecture while applying it to the critic remains a challenge. It was tested on two dynamic tasks and functioned effectively without external exploration noise or backward computation through time. Moreover, it exhibited excellent adaptability to new environments, although some problems remain. Drawing parallels between 'exploration' and 'thinking,' the author hypothesizes that "exploration grows into thinking through learning" and believes this RL could be a key technique for the emergence of thinking, including inspiration that cannot be reconstructed from massive existing text data. Finally, despite being presumptuous, the author presents the argument that this research should not proceed due to its potentially fatal risks, aiming to encourage discussion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10200v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Katsunari Shibata</dc:creator>
    </item>
    <item>
      <title>Coevolution of Camouflage</title>
      <link>https://arxiv.org/abs/2304.11793</link>
      <description>arXiv:2304.11793v3 Announce Type: replace-cross 
Abstract: Camouflage in nature seems to arise from competition between predator and prey. To survive, predators must find prey, and prey must avoid being found. This work simulates an abstract model of that adversarial relationship. It looks at crypsis through evolving prey camouflage patterns (as color textures) in competition with evolving predator vision. During their "lifetime" predators learn to better locate camouflaged prey. The environment for this 2D simulation is provided by a set of photographs, typically of natural scenes. This model is based on two evolving populations, one of prey and another of predators. Mutual conflict between these populations can produce both effective prey camouflage and predators skilled at "breaking" camouflage. The result is an open source artificial life model to help study camouflage in nature, and the perceptual phenomenon of camouflage more generally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.11793v3</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <category>cs.NE</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1162/isal_a_00583</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the ALIFE 2023: Ghost in the Machine: Proceedings of the 2023 Artificial Life Conference. (pp. 11). ASME</arxiv:journal_reference>
      <dc:creator>Craig Reynolds</dc:creator>
    </item>
    <item>
      <title>Evidence of Scaling Regimes in the Hopfield Dynamics of Whole Brain Model</title>
      <link>https://arxiv.org/abs/2401.07538</link>
      <description>arXiv:2401.07538v3 Announce Type: replace-cross 
Abstract: It is shown that a Hopfield recurrent neural network exhibits a scaling regime, whose specific exponents depend on the number of parcels used and the decay length of the coupling strength. This scaling regime recovers the picture introduced by Deco et al., according to which the process of information transfer within the human brain shows spatially correlated patterns qualitatively similar to those displayed by turbulent flows, although with a more singular exponent, 1/2 instead of 2/3. Both models employ a coupling strength which decays exponentially with the Euclidean distance between the nodes, informed by experimentally derived brain topology. Nevertheless, their mathematical nature is very different, Hopf oscillators versus a Hopfield neural network, respectively. Hence, their convergence for the same data parameters, suggests an intriguing robustness of the scaling picture.Furthermore, the present analysis shows that the Hopfield model brain remains functional by removing links above about five decay lengths, corresponding to about one sixth of the size of the global brain. This suggests that, in terms of connectivity decay length, the Hopfield brain functions in a sort of intermediate ``turbulent liquid''-like state, whose essential connections are the intermediate ones between the connectivity decay length and the global brain size. The evident sensitivity of the scaling exponent to the value of the decay length, as well as to the number of brain parcels employed, leads us to take with great caution any quantitative assessment regarding the specific nature of the scaling regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07538v3</guid>
      <category>cond-mat.dis-nn</category>
      <category>cs.NE</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giorgio Gosti, Sauro Succi, Giancarlo Ruocco</dc:creator>
    </item>
    <item>
      <title>A Consolidated Volatility Prediction with Back Propagation Neural Network and Genetic Algorithm</title>
      <link>https://arxiv.org/abs/2412.07223</link>
      <description>arXiv:2412.07223v4 Announce Type: replace-cross 
Abstract: This paper provides a unique approach with AI algorithms to predict emerging stock markets volatility. Traditionally, stock volatility is derived from historical volatility,Monte Carlo simulation and implied volatility as well. In this paper, the writer designs a consolidated model with back-propagation neural network and genetic algorithm to predict future volatility of emerging stock markets and found that the results are quite accurate with low errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07223v4</guid>
      <category>q-fin.CP</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zong Ke, Jingyu Xu, Zizhou Zhang, Yu Cheng, Wenjun Wu</dc:creator>
    </item>
  </channel>
</rss>
