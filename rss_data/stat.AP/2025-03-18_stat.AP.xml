<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Mar 2025 04:00:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Optimizing Sensor Data Interpretation via Hybrid Parametric Bootstrapping</title>
      <link>https://arxiv.org/abs/2503.11661</link>
      <description>arXiv:2503.11661v1 Announce Type: new 
Abstract: The Chalk River Laboratories (CRL) site in Ontario, Canada, has long been a hub for nuclear research, which has resulted in the accumulation of legacy nuclear waste, including radioactive materials such as uranium, plutonium, and other radionuclides. Effective management of this legacy requires precise contamination and risk assessments, with a particular focus on the concentration levels of fissile materials such as U-235. These assessments are essential for maintaining nuclear criticality safety. This study estimates the upper bounds of U-235 concentrations. We investigated the use of a hybrid parametric bootstrapping method and robust statistical techniques to analyze datasets with outliers, then compared these outcomes with those derived from nonparametric bootstrapping. This study underscores the significance of measuring U-235 for ensuring safety, conducting environmental monitoring, and adhering to regulatory compliance requirements at nuclear legacy sites. We used publicly accessible U-235 data from the Eastern Desert of Egypt to demonstrate the application of these statistical methods to small datasets, providing reliable upper limit estimates that are vital for remediation and decommissioning efforts. This method seeks to enhance the interpretation of sensor data, ultimately supporting safer nuclear waste management practices at legacy sites such as CRL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11661v1</guid>
      <category>stat.AP</category>
      <category>nucl-ex</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Victor V. Golovko</dc:creator>
    </item>
    <item>
      <title>The New Generalized Odd Median Based Unit Rayleigh with a New Shape Oscillating Hazard Rate Function</title>
      <link>https://arxiv.org/abs/2503.11668</link>
      <description>arXiv:2503.11668v1 Announce Type: new 
Abstract: In this paper, the author presents the generalized form of the Median-Based Unit Rayleigh (MBUR) distribution, a novel statistical distribution that is specifically defined within the interval (0, 1) expressing oscillating hazard rate function. This generalization adds a new parameter to the MBUR distribution that significantly addresses the unique characteristics of data represented as ratios and proportions, which are commonly encountered in various fields of research. The establishment of this generalization aims to deepen our understanding of these phenomena by providing a robust framework for analysis. The paper offers a thorough and meticulous derivation of the probability density function (PDF) for the MBUR distribution, illuminating each phase of the process with clarity and precision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11668v1</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iman Attia</dc:creator>
    </item>
    <item>
      <title>MealMeter: Using Multimodal Sensing and Machine Learning for Automatically Estimating Nutrition Intake</title>
      <link>https://arxiv.org/abs/2503.11683</link>
      <description>arXiv:2503.11683v1 Announce Type: new 
Abstract: Accurate estimation of meal macronutrient composition is a pre-perquisite for precision nutrition, metabolic health monitoring, and glycemic management. Traditional dietary assessment methods, such as self-reported food logs or diet recalls are time-intensive and prone to inaccuracies and biases. Several existing AI-driven frameworks are data intensive. In this study, we propose MealMeter, a machine learning driven method that leverages multimodal sensor data of wearable and mobile devices. Data are collected from 12 participants to estimate macronutrient intake. Our approach integrates physiological signals (e.g., continuous glucose, heart rate variability), inertial motion data, and environmental cues to model the relationship between meal intake and metabolic responses. Using lightweight machine learning models trained on a diverse dataset of labeled meal events, MealMeter predicts the composition of carbohydrates, proteins, and fats with high accuracy. Our results demonstrate that multimodal sensing combined with machine learning significantly improves meal macronutrient estimation compared to the baselines including foundation model and achieves average mean absolute errors (MAE) and average root mean squared relative errors (RMSRE) as low as 13.2 grams and 0.37, respectively, for carbohydrates. Therefore, our developed system has the potential to automate meal tracking, enhance dietary interventions, and support personalized nutrition strategies for individuals managing metabolic disorders such as diabetes and obesity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11683v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asiful Arefeen, Samantha Fessler, Sayyed Mostafa Mostafavi, Carol S Johnston, Hassan Ghasemzadeh</dc:creator>
    </item>
    <item>
      <title>Investigating dimensionally-reduced highly-damped systems with multivariate variational mode decomposition: An experimental approach</title>
      <link>https://arxiv.org/abs/2503.11689</link>
      <description>arXiv:2503.11689v1 Announce Type: new 
Abstract: Structural health monitoring (SHM) is an essential engineering field aimed at ensuring the safety and reliability of civil infrastructures. This study proposes a methodology using multivariate variational mode decomposition (MVMD) for damage detection and modal identification. MVMD decomposes multi-sensor vibration responses into intrinsic modal components, facilitating the extraction of natural frequencies and damping ratios by analyzing amplitude decay in the identified modes. Mode shapes are determined through peak-normalization of Fourier spectra corresponding to each mode. The methodology is further applied to detect damage by identifying changes in the extracted modal parameters and spatial features of the structure. The proposed approach enables damage detection by tracking variations in modal parameters and spatial structural characteristics. To validate its efficacy, the methodology is applied to a benchmark eight-degree-of-freedom (8-DOF) system from Los Alamos National Laboratory (LANL), demonstrating its robustness in identifying structural damage under non-stationary excitation and narrowband frequency content. The results confirm that MVMD provides a reliable and adaptable framework for modal analysis and damage assessment in complex infrastructure systems, addressing key challenges such as environmental variability and practical scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11689v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lakhadive Mehulkumar R, Anshu Sharma, Basuraj Bhowmik</dc:creator>
    </item>
    <item>
      <title>The Impact of Meteorological Factors on Crop Price Volatility in India: Case studies of Soybean and Brinjal</title>
      <link>https://arxiv.org/abs/2503.11690</link>
      <description>arXiv:2503.11690v1 Announce Type: new 
Abstract: The price volatility of agricultural crops (commodities) is influenced by meteorological variables such as temperature and precipitation (and many other environmental, social and governance factors), which is a critical challenge in sustainable finance, agricultural planning, and policy-making. This paper studies the impact of meteorological variables on price volatility of agricultural crops. As case studies, we choose the two Indian states of Madhya Pradesh (for Soybean) and Odisha (for Brinjal). We employ an Exponential Generalized Autoregressive Conditional Heteroskedasticity (EGARCH) model to estimate the conditional volatility of the log returns of crop prices from 2012 to 2024. This study further explores the cross-correlations between volatility and the meteorological variables. Further, a Granger-causality test is carried out to analyze the causal effect of meteorological variables on the price volatility. Finally, the Seasonal Auto-Regressive Integrated Moving Average with Exogenous Regressors (SARIMAX) and Long Short-Term Memory (LSTM) models are implemented as simple machine learning models of price volatility with meteorological factors as exogenous variables. We believe that this will illustrate the usefulness of simple machine learning models in agricultural finance, and help the farmers to make informed decisions by considering climate patterns and making beneficial decisions with regard to crop rotation or allocations. In general, implementing meteorological factors to assess agricultural performance could help to understand and reduce price volatility and possibly lead to economic stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11690v1</guid>
      <category>stat.AP</category>
      <category>econ.GN</category>
      <category>physics.soc-ph</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ashok Kumar, Abbinav Sankar Kailasam, Anish Rai, Sudeep Shukla, Sourish Das, Anirban Chakraborti</dc:creator>
    </item>
    <item>
      <title>Comparative Study of the Median Based Unit Rayleigh and its Generalized Form the Generalized Odd Median Based Unit Rayleigh</title>
      <link>https://arxiv.org/abs/2503.11700</link>
      <description>arXiv:2503.11700v1 Announce Type: new 
Abstract: In the present paper, the author discusses the Generalized Odd Median Base Unit Rayleigh (GOMBUR) in relation to the Median Based Unit Rayleigh (MBUR) to evaluate the additive value of the new shape parameter on the estimation process as regards validity indices, goodness of fit statistics, estimated variances of the estimated parameters and their standard errors. This evaluation is conducted on real datasets. Each dataset is analyzed by fitting different competitor distributions in addition to MBUR and GOMBUR distributions. The parameter estimation is achieved by applying Maximum likelihood estimator (MLE) using Nelder Mead optimizer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11700v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iman Mohammed Attia</dc:creator>
    </item>
    <item>
      <title>Analysis of Information Loss on Composition Measurement in Stiff Chemically Reacting Systems</title>
      <link>https://arxiv.org/abs/2503.11729</link>
      <description>arXiv:2503.11729v1 Announce Type: new 
Abstract: Gas sampling methods have been crucial for the advancement of combustion science, enabling analysis of reaction kinetics and pollutant formation. However, the measured composition can deviate from the true one because of the potential residual reactions in the sampling probes. This study formulates the initial composition estimation in stiff chemically reacting systems as a Bayesian inference problem, solved using the No-U-Turn Sampler (NUTS). Information loss arises from the restriction of system dynamics by low dimensional attracting manifold, where constrained evolution causes initial perturbations to decay or vanish in fast eigen-directions in composition space. This study systematically investigates the initial value inference in combustion systems and successfully validates the methodological framework in the Robertson toy system and hydrogen autoignition. Furthermore, a gas sample collected from a one-dimensional hydrogen diffusion flame is analyzed to investigate the effect of frozen temperature on information loss. The research highlights the importance of species covariance information from observations in improving estimation accuracy and identifies how the rank reduction in the sensitivity matrix leads to inference failures. Critical failure times for species inference in the Robertson and hydrogen autoignition systems are analyzed, providing insights into the limits of inference reliability and its physical significance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11729v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiming Lu, Xu Zhu, Long Zhang, Hua Zhou</dc:creator>
    </item>
    <item>
      <title>Spatiotemporal clustering of GHGs emissions in Europe: exploring the role of spatial component</title>
      <link>https://arxiv.org/abs/2503.11909</link>
      <description>arXiv:2503.11909v1 Announce Type: new 
Abstract: In this study, we propose a novel application of spatiotemporal clustering in the environmental sciences, with a particular focus on regionalised time series of greenhouse gases (GHGs) emissions from a range of economic sectors. Utilising a hierarchical spatiotemporal clustering methodology, we analyse yearly time series of emissions by gases and sectors from 1990 to 2022 for European regions at the NUTS-2 level. While the clustering algorithm inherently incorporates spatial information based on geographical distance, the extent to which space contributes to the definition of groups still requires further exploration. To address this gap in the literature, we propose a novel indicator, namely the Joint Inertia, which quantifies the contribution of spatial distances when integrated with other features. Through a simulation experiment, we explore the relationship between the Joint Inertia and the relevance of geography in exploiting the groups structure under several configurations of spatial and features patterns, providing insights into the behaviour and potential of the proposed indicator. The empirical findings demonstrate the relevance of the spatial component in identifying emission patterns and dynamics, and the results reveal significant heterogeneity across clusters in trends and dynamics by gases and sectors. This reflects the heterogeneous economic and industrial characteristics of European regions. The study highlights the importance of the spatial and temporal dimensions in understanding GHGs emissions, offering baseline insights for future spatiotemporal modelling and supporting more targeted and regionally informed environmental policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11909v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Caterina Morelli, Paolo Maranzano, Philipp Otto</dc:creator>
    </item>
    <item>
      <title>A Simple and Explainable Model for Park-and-Ride Car Park Occupancy Prediction</title>
      <link>https://arxiv.org/abs/2503.12044</link>
      <description>arXiv:2503.12044v1 Announce Type: new 
Abstract: In a scenario of growing usage of park-and-ride facilities, understanding and predicting car park occupancy is becoming increasingly important. This study presents a model that effectively captures the occupancy patterns of park-and-ride car parks for commuters using truncated normal distributions for vehicle arrival and departure times. The objective is to develop a predictive model with minimal parameters corresponding to commuter behaviour, enabling the estimation of parking saturation and unfulfilled demand. The proposed model successfully identifies the regular, periodic nature of commuter parking behaviour, where vehicles arrive in the morning and depart in the afternoon. It operates using aggregate data, eliminating the need for individual tracking of arrivals and departures. The model's predictive and now-casting capabilities are demonstrated through real-world data from car parks in the Barcelona Metropolitan Area. A simple model extension furthermore enables the prediction of when a car park will reach its occupancy limit and estimates the additional spaces required to accommodate such excess demand. Thus, beyond forecasting, the model serves as a valuable tool for evaluating interventions, such as expanding parking capacity, to optimize park-and-ride facilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12044v1</guid>
      <category>stat.AP</category>
      <category>cs.ET</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Andreas Kaltenbrunner, Josep Ferrer, David Moreno, Vicen\c{c} G\'omez</dc:creator>
    </item>
    <item>
      <title>Community Detection Analysis of Spatial Transcriptomics Data</title>
      <link>https://arxiv.org/abs/2503.12351</link>
      <description>arXiv:2503.12351v1 Announce Type: new 
Abstract: The spatial transcriptomics (ST) data produced by recent biotechnologies, such as CosMx and Xenium, contain huge amount of information about cancer tissue samples, which has great potential for cancer research via detection of community: a collection of cells with distinct cell-type composition and similar neighboring patterns. But existing clustering methods do not work well for community detection of CosMx ST data, and the commonly used kNN compositional data method shows lack of informative neighboring cell patterns for huge CosMx data. In this article, we propose a novel and more informative disk compositional data (DCD) method, which identifies neighboring patterns of each cell via taking into account of ST data features from recent new technologies. After initial processing ST data into DCD matrix, a new innovative and interpretable DCD-TMHC community detection method is proposed here. Extensive simulation studies and CosMx breast cancer data analysis clearly show that our proposed DCD-TMHC method is superior to other methods. Based on the communities detected by DCD-TMHC method for CosMx breast cancer data, the logistic regression analysis results demonstrate that DCD-TMHC method is clearly interpretable and superior, especially in terms of assessment for different stages of cancer. These suggest that our proposed novel, innovative, informative and interpretable DCD-TMHC method here will be helpful and have impact to future cancer research based on ST data, which can improve cancer diagnosis and monitor cancer treatment progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12351v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charles Zhao, Susana Garcia-Recio, Brooke M. Felsheim, Charles M. Perou, J. S. Marron</dc:creator>
    </item>
    <item>
      <title>Multivariate disaggregation modeling of air pollutants: a case-study of PM2.5, PM10 and ozone prediction in Portugal and Italy</title>
      <link>https://arxiv.org/abs/2503.12394</link>
      <description>arXiv:2503.12394v1 Announce Type: new 
Abstract: Air pollution remains a critical environmental and public health challenge, demanding high-resolution spatial data to better understand its spatial distribution and impacts. This study addresses the challenges of conducting multivariate spatial analysis of air pollutants observed at aggregated levels, particularly when the goal is to model the underlying continuous processes and perform spatial predictions at varying resolutions. To address these issues, we propose a continuous multivariate spatial model based on Gaussian processes (GPs), naturally accommodating the support of aggregated sampling units. Computationally efficient inference is achieved using R-INLA, leveraging the connection between GPs and Gaussian Markov random fields (GMRFs). A custom projection matrix maps the GMRFs defined on the triangulation of the study region and the aggregated GPs at sampling units, ensuring accurate handling of changes in spatial support. This approach integrates shared information among pollutants and incorporates covariates, enhancing interpretability and explanatory power. This approach is used to downscale PM2.5, PM10 and ozone levels in Portugal and Italy, improving spatial resolution from 0.1{\deg} (~10 km) to 0.02{\deg} (~2 km), and revealing dependencies among pollutants. Our framework provides a robust foundation for analyzing complex pollutant interactions, offering valuable insights for decision-makers seeking to address air pollution and its impacts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12394v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fernando Rodriguez Avellaneda, Erick A. Chac\'on-Montalv\'an, Paula Moraga</dc:creator>
    </item>
    <item>
      <title>A Method for the Time-Frequency Analysis of High-Order Interactions in Non-Stationary Physiological Networks</title>
      <link>https://arxiv.org/abs/2503.12421</link>
      <description>arXiv:2503.12421v1 Announce Type: new 
Abstract: Several data-driven approaches based on information theory have been proposed for analyzing high-order interactions involving three or more components of a network system. Most of these methods are defined only in the time domain and rely on the assumption of stationarity in the underlying dynamics, making them inherently unable to detect frequency-specific behaviors and track transient functional links in physiological networks. This study introduces a new framework which enables the time-varying and time-frequency analysis of high-order interactions in network of random processes through the spectral representation of vector autoregressive models. The time- and frequency-resolved analysis of synergistic and redundant interactions among groups of processes is ensured by a robust identification procedure based on a recursive least squares estimator with a forgetting factor. Validation on simulated networks illustrates how the time-frequency analysis is able to highlight transient synergistic behaviors emerging in specific frequency bands which cannot be detected by time-domain stationary analyses. The application on brain evoked potentials in rats elicits the presence of redundant information timed with whisker stimulation and mostly occurring in the contralateral hemisphere. The proposed framework enables a comprehensive time-varying and time-frequency analysis of the hierarchical organization of dynamic networks. As our approach goes beyond pairwise interactions, it is well suited for the study of transient high-order behaviors arising during state transitions in many network systems commonly studied in physiology, neuroscience and other fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12421v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yuri Antonacci, Chiara Bara', Laura Sparacino, Gorana Mijatovic, Ludovico Minati, Luca Faes</dc:creator>
    </item>
    <item>
      <title>Discussing Diminishing Returns: A New Scoring System for Powerlifting</title>
      <link>https://arxiv.org/abs/2503.13040</link>
      <description>arXiv:2503.13040v1 Announce Type: new 
Abstract: Maximal strength increases with body weight, this is why scoring methods have been developed in order to fairly scale powerlifting performances based on athletes' body weight. The International Powerlifting Federation (IPF) Good Lift (GL) system, introduced in 2020, is a scaling method based on a Von Bertalanffy function. It is specifically tailored to elite powerlifters; defined as those achieving at least 84% of the current world record in their weight class, according to the IPF's "Golden Standard". A key assumption of the GL system is a principle of diminishing returns, stating that performance gains decrease as body weight increases. However, data from the Open Powerlifting database reveals the presence of a phase of increasing returns below a certain body weight threshold. To capture this initial phase, we propose a new scoring system based on a logistic function. This approach is designed to establish strength standards for the general population and for athletes not specialised in powerlifting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13040v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jean C. Peyen, Ruheyan Nuermaimaiti, Joshua Cunningham</dc:creator>
    </item>
    <item>
      <title>Multivariate Sparse Functional Linear Discriminant Analysis: An Application to Inflammatory Bowel Disease Classification</title>
      <link>https://arxiv.org/abs/2503.13372</link>
      <description>arXiv:2503.13372v1 Announce Type: new 
Abstract: Inflammatory Bowel Disease (IBD), including Crohn's Disease (CD) and Ulcerative Colitis (UC), presents significant public health challenges due to its complex etiology. Motivated by the IBD study of the Integrative Human Microbiome Project, our objective is to identify microbial pathways that distinguish between CD, UC and non-IBD over time. Most current research relies on simplistic analyses that examine one variable or time point at a time, or address binary classification problems, limiting our understanding of the dynamic interactions within the microbiome over time. To address these limitations, we develop a novel functional data analysis approach for discriminant analysis of multivariate functional data that can effectively handle multiple high-dimensional predictors, sparse time points, and categorical outcomes. Our method seeks linear combinations of functions (i.e., discriminant functions) that maximize separation between two or more groups over time. We impose a sparsity-inducing penalty when estimating the discriminant functions, allowing us to identify relevant discriminating variables over time. Applications of our method to the motivating data identified microbial features related to mucin degradation, amino acid metabolism, and peptidoglycan recognition, which are implicated in the progression and development of IBD. Furthermore, our method highlighted the role of multiple vitamin B deficiencies in the context of IBD. By moving beyond traditional analytical frameworks, our innovative approach holds the potential for uncovering clinically meaningful discoveries in IBD research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13372v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Limeng Liu, Guannan Wang, Sandra E. Safo</dc:creator>
    </item>
    <item>
      <title>Fast Algorithm for Calculating Probability of Chess Winning Streaks</title>
      <link>https://arxiv.org/abs/2503.11669</link>
      <description>arXiv:2503.11669v1 Announce Type: cross 
Abstract: Motivated by the controversy in the chess community, where Hikaru Nakamura, a renowned grandmaster, has posted multiple impressive winning streaks over the years on the online platform chess.com, we derive the probabilities of various types of streaks in online chess and/or other sports. Specifically, given the winning/drawing/losing probabilities of individual games, we derive the probabilities of "pure" winning streaks, non-losing streaks, and "in-between" streaks involving at most one draw over the course of games played in a period. The performance of the developed algorithms is examined through numerical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11669v1</guid>
      <category>math.HO</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guoqing Diao</dc:creator>
    </item>
    <item>
      <title>The Kolmogorov-Smirnov Statistic Revisited</title>
      <link>https://arxiv.org/abs/2503.11673</link>
      <description>arXiv:2503.11673v1 Announce Type: cross 
Abstract: The Kolmogorov-Smirnov (KS) statistic is a classical nonparametric test widely used for comparing an empirical distribution function with a reference distribution or for comparing two empirical distributions. Despite its broad applicability in statistical hypothesis testing and model validation, certain aspects of the KS statistic remain under-explored among the young generation, particularly under finite sample conditions. This paper revisits the KS statistic in both one-sample and two-sample scenarios, considering one-sided and two-sided variants. We derive exact probabilities for the supremum of the empirical process and present a unified treatment of the KS statistic under diverse settings. Additionally, we explore the discrete nature of the hitting times of the normalized empirical process, providing practical insights into the computation of KS test p-values. The study also discusses the Dvoretzky-Kiefer-Wolfowitz-Massart (DKWM) inequality, highlighting its role in constructing confidence bands for distribution functions. Using empirical process theory, we establish the limit distribution of the KS statistic when the true distribution includes unknown parameters. Our findings extend existing results, offering improved methodologies for statistical analysis and hypothesis testing using the KS statistic, particularly in finite sample scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11673v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elvis Han Cui, Yihao Li, Zhuang Liu</dc:creator>
    </item>
    <item>
      <title>A Bayesian Proportional Mean Model Using Panel Binary Data-An Application to Health and Retirement Study</title>
      <link>https://arxiv.org/abs/2503.11994</link>
      <description>arXiv:2503.11994v1 Announce Type: cross 
Abstract: In recurrent event studies, panel binary data arise when subjects are observed at discrete time points and only the recurrent event status within each observation window is recorded. Such data frequently occur in longitudinal studies due to recall difficulties or participants' privacy concerns during follow-ups, necessitating rigorous statistical analysis. While frequentist methods exist for handling such data, Bayesian approaches remain largely unexplored. This article proposes an efficient Bayesian proportional mean model for analysing recurrent events using panel binary data. In addition to the estimation procedure, the article introduces techniques for model validation, selection, and Bayesian influence diagnostics. Simulation studies demonstrate the method's effectiveness and robustness in different practical scenarios. The proposed approach is then applied to analyse the latest version of the Health and Retirement Study dataset, identifying key risk factors influencing doctor visits among the elderly. The analysis is therefore capable of providing valuable insights into healthcare utilisation patterns in ageing populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11994v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pavithra Hariharan, P. G. Sankaran</dc:creator>
    </item>
    <item>
      <title>On self-training of summary data with genetic applications</title>
      <link>https://arxiv.org/abs/2503.12155</link>
      <description>arXiv:2503.12155v1 Announce Type: cross 
Abstract: Prediction model training is often hindered by limited access to individual-level data due to privacy concerns and logistical challenges, particularly in biomedical research. Resampling-based self-training presents a promising approach for building prediction models using only summary-level data. These methods leverage summary statistics to sample pseudo datasets for model training and parameter optimization, allowing for model development without individual-level data. Although increasingly used in precision medicine, the general behaviors of self-training remain unexplored. In this paper, we leverage a random matrix theory framework to establish the statistical properties of self-training algorithms for high-dimensional sparsity-free summary data. We demonstrate that, within a class of linear estimators, resampling-based self-training achieves the same asymptotic predictive accuracy as conventional training methods that require individual-level datasets. These results suggest that self-training with only summary data incurs no additional cost in prediction accuracy, while offering significant practical convenience. Our analysis provides several valuable insights and counterintuitive findings. For example, while pseudo-training and validation datasets are inherently dependent, their interdependence unexpectedly cancels out when calculating prediction accuracy measures, preventing overfitting in self-training algorithms. Furthermore, we extend our analysis to show that the self-training framework maintains this no-cost advantage when combining multiple methods or when jointly training on data from different distributions. We numerically validate our findings through simulations and real data analyses using the UK Biobank. Our study highlights the potential of resampling-based self-training to advance genetic risk prediction and other fields that make summary data publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12155v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Buxin Su, Jiaoyang Huang, Jin Jin, Bingxin Zhao</dc:creator>
    </item>
    <item>
      <title>Prior distributions for Gaussian processes in computer model emulation and calibration</title>
      <link>https://arxiv.org/abs/2503.12257</link>
      <description>arXiv:2503.12257v1 Announce Type: cross 
Abstract: This article discusses prior distributions for the parameters of Gaussian processes (GPs) that are widely used as surrogate models to emulate expensive computer simulations. The parameters typically involve mean parameters, a variance parameter, and correlation parameters. These parameters are often estimated by maximum likelihood (MLE). In some scenarios, however, the MLE can be unstable, particularly when the number of simulation runs is small, and some Bayesian estimators display better properties. We introduce default Bayesian priors for the parameters of GPs with isotropic and separable correlation functions for emulating computer simulations with both scalar-valued and vector-valued outputs. We also summarize recent developments of Bayesian priors for calibrating computer models by field or experimental observations. Finally, we review software packages for computer model emulation and calibration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12257v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengyang Gu, Victor De Oliveira</dc:creator>
    </item>
    <item>
      <title>Identification and estimation of structural vector autoregressive models via LU decomposition</title>
      <link>https://arxiv.org/abs/2503.12378</link>
      <description>arXiv:2503.12378v1 Announce Type: cross 
Abstract: Structural vector autoregressive (SVAR) models are widely used to analyze the simultaneous relationships between multiple time-dependent data. Various statistical inference methods have been studied to overcome the identification problems of SVAR models. However, most of these methods impose strong assumptions for innovation processes such as the uncorrelation of components. In this study, we relax the assumptions for innovation processes and propose an identification method for SVAR models under the zero-restrictions on the coefficient matrices, which correspond to sufficient conditions for LU decomposition of the coefficient matrices of the reduced form of the SVAR models. Moreover, we establish asymptotically normal estimators for the coefficient matrices and impulse responses, which enable us to construct test statistics for the simultaneous relationships of time-dependent data. The finite-sample performance of the proposed method is elucidated by numerical simulations. We also present an example of an empirical study that analyzes the impact of policy rates on unemployment and prices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12378v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masato Shimokawa, Kou Fujimori</dc:creator>
    </item>
    <item>
      <title>Nonlinear Principal Component Analysis with Random Bernoulli Features for Process Monitoring</title>
      <link>https://arxiv.org/abs/2503.12456</link>
      <description>arXiv:2503.12456v1 Announce Type: cross 
Abstract: The process generates substantial amounts of data with highly complex structures, leading to the development of numerous nonlinear statistical methods. However, most of these methods rely on computations involving large-scale dense kernel matrices. This dependence poses significant challenges in meeting the high computational demands and real-time responsiveness required by online monitoring systems. To alleviate the computational burden of dense large-scale matrix multiplication, we incorporate the bootstrap sampling concept into random feature mapping and propose a novel random Bernoulli principal component analysis method to efficiently capture nonlinear patterns in the process. We derive a convergence bound for the kernel matrix approximation constructed using random Bernoulli features, ensuring theoretical robustness. Subsequently, we design four fast process monitoring methods based on random Bernoulli principal component analysis to extend its nonlinear capabilities for handling diverse fault scenarios. Finally, numerical experiments and real-world data analyses are conducted to evaluate the performance of the proposed methods. Results demonstrate that the proposed methods offer excellent scalability and reduced computational complexity, achieving substantial cost savings with minimal performance loss compared to traditional kernel-based approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12456v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ke Chen, Dandan Jiang</dc:creator>
    </item>
    <item>
      <title>Learning response functions of analog quantum computers: analysis of neutral-atom and superconducting platforms</title>
      <link>https://arxiv.org/abs/2503.12520</link>
      <description>arXiv:2503.12520v1 Announce Type: cross 
Abstract: Analog quantum computation is an attractive paradigm for the simulation of time-dependent quantum systems. Programmable analog quantum computers have been realized in hardware using a variety of physical principles, including neutral-atom and superconducting technologies. The input parameters of the physical Hamiltonians that are used to program the quantum simulator generally differ from the parameters that characterize the output distribution of data produced under a specified quantum dynamics. The relationship between the input and output parameters is known as the response function of the analog device. Here, we introduce a streaming algorithm for learning the response function of analog quantum computers from arbitrary user inputs, thus not requiring special calibration runs. We use the method to learn and compare the response functions of several generations of analog quantum simulators based on superconducting and neutral-atom programmable arrays.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12520v1</guid>
      <category>quant-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cenk T\"uys\"uz, Abhijith Jayakumar, Carleton Coffrin, Marc Vuffray, Andrey Y. Lokhov</dc:creator>
    </item>
    <item>
      <title>Causal Feature Learning in the Social Sciences</title>
      <link>https://arxiv.org/abs/2503.12784</link>
      <description>arXiv:2503.12784v1 Announce Type: cross 
Abstract: Variable selection poses a significant challenge in causal modeling, particularly within the social sciences, where constructs often rely on inter-related factors such as age, socioeconomic status, gender, and race. Indeed, it has been argued that such attributes must be modeled as macro-level abstractions of lower-level manipulable features, in order to preserve the modularity assumption essential to causal inference. This paper accordingly extends the theoretical framework of Causal Feature Learning (CFL). Empirically, we apply the CFL algorithm to diverse social science datasets, evaluating how CFL-derived macrostates compare with traditional microstates in downstream modeling tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12784v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingzhou Huang, Jiuyao Lu, Alexander Williams Tolbert</dc:creator>
    </item>
    <item>
      <title>A robust score test in g-computation for covariate adjustment in randomized clinical trials leveraging different variance estimators via influence functions</title>
      <link>https://arxiv.org/abs/2503.13066</link>
      <description>arXiv:2503.13066v1 Announce Type: cross 
Abstract: G-computation has become a widely used robust method for estimating unconditional (marginal) treatment effects with covariate adjustment in the analysis of randomized clinical trials. Statistical inference in this context typically relies on the Wald test or Wald interval, which can be easily implemented using a consistent variance estimator. However, existing literature suggests that when sample sizes are small or when parameters of interest are near boundary values, Wald-based methods may be less reliable due to type I error rate inflation and insufficient interval coverage. In this article, we propose a robust score test for g-computation estimators in the context of two-sample treatment comparisons. The proposed test is asymptotically valid under simple and stratified (biased-coin) randomization schemes, even when regression models are misspecified. These test statistics can be conveniently computed using existing variance estimators, and the corresponding confidence intervals have closed-form expressions, making them convenient to implement. Through extensive simulations, we demonstrate the superior finite-sample performance of the proposed method. Finally, we apply the proposed method to reanalyze a completed randomized clinical trial. The new analysis using our proposed score test achieves statistical significance, whilst reducing the issue of type I error inflation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13066v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Zhang, Haitao Chu, Lin Liu, Satrajit Roychoudhury</dc:creator>
    </item>
    <item>
      <title>Tracking the Hidden Forces Behind Laos' 2022 Exchange Rate Crisis and Balance of Payments Instability</title>
      <link>https://arxiv.org/abs/2503.13308</link>
      <description>arXiv:2503.13308v1 Announce Type: cross 
Abstract: This working paper uses a Dynamic Factor Model ('the model') to identify underlying factors contributing to the debt-induced economic crisis in the People's Democratic Republic of Laos ('Laos'). The analysis aims to use the latent macroeconomic insights to propose ways forward for forecasting. We focus on Laos's historic structural weaknesses to identify when a balance of payments crisis with either a persistent current account imbalance or rapid capital outflows would occur. By extracting latent economic factors from macroeconomic indicators, the model provides a starting point for analyzing the structural vulnerabilities leading to the value of the kip in USD terms dropping and contributing to inflation in the country. This findings of this working paper contribute to the broader literature on exchange rate instability and external sector vulnerabilities in emerging economies, offering insights on what constitutes as 'signals' as opposed to plain 'noise' from a macroeconomic forecasting standpoint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13308v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mariza Cooray, Rolando Gonzales Martinez</dc:creator>
    </item>
    <item>
      <title>Reliable and Efficient Amortized Model-based Evaluation</title>
      <link>https://arxiv.org/abs/2503.13335</link>
      <description>arXiv:2503.13335v1 Announce Type: cross 
Abstract: Comprehensive evaluations of language models (LM) during both development and deployment phases are necessary because these models possess numerous capabilities (e.g., mathematical reasoning, legal support, or medical diagnostic) as well as safety risks (e.g., racial bias, toxicity, or misinformation). The average score across a wide range of benchmarks provides a signal that helps guide the use of these LMs in practice. Currently, holistic evaluations are costly due to the large volume of benchmark questions, making frequent evaluations impractical. A popular attempt to lower the cost is to compute the average score on a subset of the benchmark. This approach, unfortunately, often renders an unreliable measure of LM performance because the average score is often confounded with the difficulty of the questions in the benchmark subset. Item response theory (IRT) was designed to address this challenge, providing a reliable measurement by careful controlling for question difficulty. Unfortunately, question difficulty is expensive to estimate. Facing this challenge, we train a model that predicts question difficulty from its content, enabling a reliable measurement at a fraction of the cost. In addition, we leverage this difficulty predictor to further improve the evaluation efficiency through training a question generator given a difficulty level. This question generator is essential in adaptive testing, where, instead of using a random subset of the benchmark questions, informative questions are adaptively chosen based on the current estimation of LLM performance. Experiments on 22 common natural language benchmarks and 172 LMs show that this approach is more reliable and efficient compared to current common practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13335v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sang Truong, Yuheng Tu, Percy Liang, Bo Li, Sanmi Koyejo</dc:creator>
    </item>
    <item>
      <title>Investigating symptom duration using current status data: a case study of post-acute COVID-19 syndrome</title>
      <link>https://arxiv.org/abs/2407.04214</link>
      <description>arXiv:2407.04214v3 Announce Type: replace 
Abstract: For infectious diseases, characterizing symptom duration is of clinical and public health importance. Symptom duration may be assessed by surveying infected individuals and querying symptom status at the time of survey response. For example, in a SARS-CoV-2 testing program at the University of Washington, participants were surveyed at least $28$ days after testing positive and asked to report current symptom status. This study design yielded current status data: outcome measurements for each respondent consisted only of the time of survey response and a binary indicator of whether symptoms had resolved by that time. Such study design benefits from limited risk of recall bias, but analyzing the resulting data necessitates tailored statistical tools. Here, we review methods for current status data and describe a novel application of modern nonparametric techniques to this setting. The proposed approach is valid under weaker assumptions compared to existing methods, allows use of flexible machine learning tools, and handles potential survey nonresponse. From the university study, under an assumption that the survey response time is conditionally independent of symptom resolution time within strata of measured covariates, we estimate that 19% of participants experienced ongoing symptoms 30 days after testing positive, decreasing to 7% at 90 days. We assess the sensitivity of these results to deviations from conditional independence, finding the estimates to be more sensitive to assumption violations at 30 days compared to 90 days. Female sex, fatigue during acute infection, and higher viral load were associated with slower symptom resolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04214v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charles J. Wolock, Susan Jacob, Julia C. Bennett, Anna Elias-Warren, Jessica O'Hanlon, Avi Kenny, Nicholas P. Jewell, Andrea Rotnitzky, Stephen R. Cole, Ana A. Weil, Helen Y. Chu, Marco Carone</dc:creator>
    </item>
    <item>
      <title>A Versatility Measure for Parametric Risk Models</title>
      <link>https://arxiv.org/abs/2407.19218</link>
      <description>arXiv:2407.19218v2 Announce Type: replace 
Abstract: Parametric statistical methods play a central role in analyzing risk through its underlying frequency and severity components. Given the wide availability of numerical algorithms and high-speed computers, researchers and practitioners often model these separate (although possibly statistically dependent) random variables by fitting a large number of parametric probability distributions to historical data and then comparing goodness-of-fit statistics. However, this approach is highly susceptible to problems of overfitting because it gives insufficient weight to fundamental considerations of functional simplicity and adaptability. To address this shortcoming, we propose a formal mathematical measure for assessing the versatility of frequency and severity distributions prior to their application. We then illustrate this approach by computing and comparing values of the versatility measure for a variety of probability distributions commonly used in risk analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19218v2</guid>
      <category>stat.AP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael R. Powers, Jiaxin Xu</dc:creator>
    </item>
    <item>
      <title>Scalable Causal Structure Learning via Amortized Conditional Independence Testing</title>
      <link>https://arxiv.org/abs/2310.16626</link>
      <description>arXiv:2310.16626v2 Announce Type: replace-cross 
Abstract: Controlling false positives (Type I errors) through statistical hypothesis testing is a foundation of modern scientific data analysis. Existing causal structure discovery algorithms either do not provide Type I error control or cannot scale to the size of modern scientific datasets. We consider a variant of the causal discovery problem with two sets of nodes, where the only edges of interest form a bipartite causal subgraph between the sets. We develop Scalable Causal Structure Learning (SCSL), a method for causal structure discovery on bipartite subgraphs that provides Type I error control. SCSL recasts the discovery problem as a simultaneous hypothesis testing problem and uses discrete optimization over the set of possible confounders to obtain an upper bound on the test statistic for each edge. Semi-synthetic simulations demonstrate that SCSL scales to handle graphs with hundreds of nodes while maintaining error control and good power. We demonstrate the practical applicability of the method by applying it to a cancer dataset to reveal connections between somatic gene mutations and metastases to different tissues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.16626v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Leiner, Brian Manzo, Aaditya Ramdas, Wesley Tansey</dc:creator>
    </item>
    <item>
      <title>Nested Instrumental Variables Design: Switcher Average Treatment Effect, Identification, Efficient Estimation and Generalizability</title>
      <link>https://arxiv.org/abs/2405.07102</link>
      <description>arXiv:2405.07102v4 Announce Type: replace-cross 
Abstract: Instrumental variables (IV) are a commonly used tool to estimate causal effects from non-randomized data. An archetype of an IV is a randomized trial with non-compliance where the randomized treatment assignment serves as an IV for the non-ignorable treatment received. Under a monotonicity assumption, a valid IV non-parametrically identifies the average treatment effect among a non-identified, latent complier subgroup, whose generalizability is often under debate. In many studies, there could exist multiple versions of an IV, for instance, different nudges to take the same treatment in different study sites in a multicentre clinical trial. These different versions of an IV may result in different compliance rates and offer a unique opportunity to study IV estimates' generalizability. In this article, we introduce a novel nested IV assumption and study identification of the average treatment effect among two latent subgroups: always-compliers and switchers, who are defined based on the joint potential treatment received under two versions of a binary IV. We derive the efficient influence function for the SWitcher Average Treatment Effect (SWATE) under a non-parametric model and propose efficient estimators. We then propose formal statistical tests of the generalizability of IV estimates under the nested IV framework. We apply the proposed method to the Prostate, Lung, Colorectal and Ovarian (PLCO) Cancer Screening Trial and study the causal effect of colorectal cancer screening and its generalizability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07102v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Wang, Ying-Qi Zhao, Oliver Dukes, Bo Zhang</dc:creator>
    </item>
    <item>
      <title>Item response parameter estimation performance using Gaussian quadrature and Laplace</title>
      <link>https://arxiv.org/abs/2405.20164</link>
      <description>arXiv:2405.20164v2 Announce Type: replace-cross 
Abstract: Item parameter estimation in pharmacometric item response theory (IRT) models is predominantly performed using the Laplace estimation algorithm as implemented in NONMEM. In psychometrics a wide range of different software tools, including several packages for the open-source software R for implementation of IRT are also available. Each have their own set of benefits and limitations and to date a systematic comparison of the primary estimation algorithms has not been evaluated. A simulation study evaluating varying number of hypothetical sample sizes and item scenarios at baseline was performed using both Laplace and Gauss-hermite quadrature (GHQ-EM). In scenarios with at least 20 items and more than 100 subjects, item parameters were estimated with good precision and were similar between estimation algorithms as demonstrated by several measures of bias and precision. The minimal differences observed for certain parameters or sample size scenarios were reduced when translating to the total score scale. The ease of use, speed of estimation and relative accuracy of the GHQ-EM method employed in mirt make it an appropriate alternative or supportive analytical approach to NONMEM for potential pharmacometrics IRT applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20164v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leticia Arrington (Uppsala University), Sebastian Ueckert (Uppsala University)</dc:creator>
    </item>
    <item>
      <title>Longitudinal Causal Inference with Selective Eligibility</title>
      <link>https://arxiv.org/abs/2410.17864</link>
      <description>arXiv:2410.17864v2 Announce Type: replace-cross 
Abstract: Dropout poses a significant challenge to causal inference in longitudinal studies with time-varying treatments. However, existing research does not simultaneously address dropout and time-varying treatments. We examine selective eligibility, an important yet overlooked source of non-ignorable dropout in such settings. This problem arises when a unit's prior treatment history influences its eligibility for subsequent treatments, a common scenario in medical and other settings. We propose a general methodological framework for longitudinal causal inference with selective eligibility. By focusing on a subgroup of units who would become eligible for treatment given a specific past treatment sequence, we define the time-specific eligible treatment effect and expected number of outcome events under a treatment sequence of interest. Under a generalized version of sequential ignorability, we derive two nonparametric identification formulae, each leveraging different parts of the observed data distribution. We then derive the efficient influence function of each causal estimand, yielding the corresponding doubly robust estimator. Finally, we apply the proposed methodology to an impact evaluation of a pre-trial risk assessment instrument in the criminal justice system, in which selective eligibility arises due to recidivism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17864v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhichao Jiang, Eli Ben-Michael, D. James Greiner, Ryan Halen, Kosuke Imai</dc:creator>
    </item>
    <item>
      <title>Rethinking the Win Ratio: A Causal Framework for Hierarchical Outcome Analysis</title>
      <link>https://arxiv.org/abs/2501.16933</link>
      <description>arXiv:2501.16933v2 Announce Type: replace-cross 
Abstract: Quantifying causal effects in the presence of complex and multivariate outcomes is a key challenge to evaluate treatment effects. For hierarchical multivarariates outcomes, the FDA recommends the Win Ratio and Generalized Pairwise Comparisons approaches. However, as far as we know, these empirical methods lack causal or statistical foundations to justify their broader use in recent studies. To address this gap, we establish causal foundations for hierarchical comparison methods. We define related causal effect measures, and highlight that depending on the methodology used to compute Win Ratios or Net Benefits of treatments, the causal estimand targeted can be different, as proved by our consistency results. Quite dramatically, it appears that the causal estimand related to the historical estimation approach can yield reversed and incorrect treatment recommendations in heterogeneous populations, as we illustrate through striking examples. In order to compensate for this fallacy, we introduce a novel, individual-level yet identifiable causal effect measure that better approximates the ideal, non-identifiable individual-level estimand. We prove that computing Win Ratio or Net Benefits using a Nearest Neighbor pairing approach between treated and controlled patients, an approach that can be seen as an extreme form of stratification, leads to estimating this new causal estimand measure. We extend our methods to observational settings via propensity weighting, distributional regression to address the curse of dimensionality, and a doubly robust framework. We prove the consistency of our methods, and the double robustness of our augmented estimator. Finally, we validate our approach using synthetic data and on CRASH-3, a major clinical trial focused on assessing the effects of tranexamic acid in patients with traumatic brain injury.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16933v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathieu Even, Julie Josse</dc:creator>
    </item>
    <item>
      <title>Bayes factor functions for testing partial correlation coefficients</title>
      <link>https://arxiv.org/abs/2503.10787</link>
      <description>arXiv:2503.10787v2 Announce Type: replace-cross 
Abstract: Partial correlation coefficients are widely applied in the social sciences to evaluate the relationship between two variables after accounting for the influence of others. In this article, we present Bayes Factor Functions (BFFs) for assessing the presence of partial correlation. BFFs represent Bayes factors derived from test statistics and are expressed as functions of a standardized effect size. While traditional frequentist methods based on $p$-values have been criticized for their inability to provide cumulative evidence in favor of the true hypothesis, Bayesian approaches are often challenged due to their computational demands and sensitivity to prior distributions. BFFs overcome these limitations and offer summaries of hypothesis tests as alternative hypotheses are varied over a range of prior distributions on standardized effects. They also enable the integration of evidence across multiple studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10787v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saptati Datta, Valen E. Johnson</dc:creator>
    </item>
  </channel>
</rss>
