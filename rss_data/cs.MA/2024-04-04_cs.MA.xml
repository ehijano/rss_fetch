<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.MA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.MA</link>
    <description>cs.MA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.MA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Apr 2024 04:00:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 04 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>EnergAIze: Multi Agent Deep Deterministic Policy Gradient for Vehicle to Grid Energy Management</title>
      <link>https://arxiv.org/abs/2404.02361</link>
      <description>arXiv:2404.02361v1 Announce Type: new 
Abstract: This paper investigates the increasing roles of Renewable Energy Sources (RES) and Electric Vehicles (EVs). While indicating a new era of sustainable energy, these also introduce complex challenges, including the need to balance supply and demand and smooth peak consumptions amidst rising EV adoption rates. Addressing these challenges requires innovative solutions such as Demand Response (DR), energy flexibility management, Renewable Energy Communities (RECs), and more specifically for EVs, Vehicle-to-Grid (V2G). However, existing V2G approaches often fall short in real-world adaptability, global REC optimization with other flexible assets, scalability, and user engagement. To bridge this gap, this paper introduces EnergAIze, a Multi-Agent Reinforcement Learning (MARL) energy management framework, leveraging the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm. EnergAIze enables user-centric and multi-objective energy management by allowing each prosumer to select from a range of personal management objectives, thus encouraging engagement. Additionally, it architects' data protection and ownership through decentralized computing, where each prosumer can situate an energy management optimization node directly at their own dwelling. The local node not only manages local energy assets but also fosters REC wide optimization. The efficacy of EnergAIze was evaluated through case studies employing the CityLearn simulation framework. These simulations were instrumental in demonstrating EnergAIze's adeptness at implementing V2G technology within a REC and other energy assets. The results show reduction in peak loads, ramping, carbon emissions, and electricity costs at the REC level while optimizing for individual prosumers objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02361v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tiago Fonseca, Luis Ferreira, Bernardo Cabral, Ricardo Severino, Isabel Praca</dc:creator>
    </item>
    <item>
      <title>Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization</title>
      <link>https://arxiv.org/abs/2404.02183</link>
      <description>arXiv:2404.02183v1 Announce Type: cross 
Abstract: Recent advancements in automatic code generation using large language model (LLM) agent have brought us closer to the future of automated software development. However, existing single-agent approaches face limitations in generating and improving large-scale, complex codebases due to constraints in context length. To tackle this challenge, we propose Self-Organized multi-Agent framework (SoA), a novel multi-agent framework that enables the scalable and efficient generation and optimization of large-scale code. In SoA, self-organized agents operate independently to generate and modify code components while seamlessly collaborating to construct the overall codebase. A key feature of our framework is the automatic multiplication of agents based on problem complexity, allowing for dynamic scalability. This enables the overall code volume to be increased indefinitely according to the number of agents, while the amount of code managed by each agent remains constant. We evaluate SoA on the HumanEval benchmark and demonstrate that, compared to a single-agent system, each agent in SoA handles significantly less code, yet the overall generated code is substantially greater. Moreover, SoA surpasses the powerful single-agent baseline by 5% in terms of Pass@1 accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02183v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yoichi Ishibashi, Yoshimasa Nishimura</dc:creator>
    </item>
    <item>
      <title>Federated Multi-Agent Mapping for Planetary Exploration</title>
      <link>https://arxiv.org/abs/2404.02289</link>
      <description>arXiv:2404.02289v1 Announce Type: cross 
Abstract: In multi-agent robotic exploration, managing and effectively utilizing the vast, heterogeneous data generated from dynamic environments poses a significant challenge. Federated learning (FL) is a promising approach for distributed mapping, addressing the challenges of decentralized data in collaborative learning. FL enables joint model training across multiple agents without requiring the centralization or sharing of raw data, overcoming bandwidth and storage constraints. Our approach leverages implicit neural mapping, representing maps as continuous functions learned by neural networks, for compact and adaptable representations. We further enhance this approach with meta-initialization on Earth datasets, pre-training the network to quickly learn new map structures. This combination demonstrates strong generalization to diverse domains like Martian terrain and glaciers. We rigorously evaluate this approach, demonstrating its effectiveness for real-world deployment in multi-agent exploration scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02289v1</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tiberiu-Ioan Szatmari, Abhishek Cauligi</dc:creator>
    </item>
    <item>
      <title>Electric Vehicle Routing Problem for Emergency Power Supply: Towards Telecom Base Station Relief</title>
      <link>https://arxiv.org/abs/2404.02448</link>
      <description>arXiv:2404.02448v1 Announce Type: cross 
Abstract: As a telecom provider, our company has a critical mission to maintain telecom services even during power outages. To accomplish the mission, it is essential to maintain the power of the telecom base stations. Here we consider a solution where electric vehicles (EVs) directly supply power to base stations by traveling to their locations. The goal is to find EV routes that minimize both the total travel distance of all EVs and the number of downed base stations. In this paper, we formulate this routing problem as a new variant of the Electric Vehicle Routing Problem (EVRP) and propose a solver that combines a rule-based vehicle selector and a reinforcement learning (RL)-based node selector. The rule of the vehicle selector ensures the exact environmental states when the selected EV starts to move. In addition, the node selection by the RL model enables fast route generation, which is critical in emergencies. We evaluate our solver on both synthetic datasets and real datasets. The results show that our solver outperforms baselines in terms of the objective value and computation time. Moreover, we analyze the generalization and scalability of our solver, demonstrating the capability toward unseen settings and large-scale problems. Check also our project page: https://ntt-dkiku.github.io/rl-evrpeps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02448v1</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daisuke Kikuta, Hiroki Ikeuchi, Kengo Tajiri, Yuta Toyama, Yuusuke Nakano</dc:creator>
    </item>
    <item>
      <title>Off-Policy Correction For Multi-Agent Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2111.11229</link>
      <description>arXiv:2111.11229v3 Announce Type: replace-cross 
Abstract: Multi-agent reinforcement learning (MARL) provides a framework for problems involving multiple interacting agents. Despite apparent similarity to the single-agent case, multi-agent problems are often harder to train and analyze theoretically. In this work, we propose MA-Trace, a new on-policy actor-critic algorithm, which extends V-Trace to the MARL setting. The key advantage of our algorithm is its high scalability in a multi-worker setting. To this end, MA-Trace utilizes importance sampling as an off-policy correction method, which allows distributing the computations with no impact on the quality of training. Furthermore, our algorithm is theoretically grounded - we prove a fixed-point theorem that guarantees convergence. We evaluate the algorithm extensively on the StarCraft Multi-Agent Challenge, a standard benchmark for multi-agent algorithms. MA-Trace achieves high performance on all its tasks and exceeds state-of-the-art results on some of them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.11229v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Micha{\l} Zawalski, B{\l}a\.zej Osi\'nski, Henryk Michalewski, Piotr Mi{\l}o\'s</dc:creator>
    </item>
    <item>
      <title>An active learning method for solving competitive multi-agent decision-making and control problems</title>
      <link>https://arxiv.org/abs/2212.12561</link>
      <description>arXiv:2212.12561v3 Announce Type: replace-cross 
Abstract: To identify a stationary action profile for a population of competitive agents, each executing private strategies, we introduce a novel active-learning scheme where a centralized external observer (or entity) can probe the agents' reactions and recursively update simple local parametric estimates of the action-reaction mappings. Under very general working assumptions (not even assuming that a stationary profile exists), sufficient conditions are established to assess the asymptotic properties of the proposed active learning methodology so that, if the parameters characterizing the action-reaction mappings converge, a stationary action profile is achieved. Such conditions hence act also as certificates for the existence of such a profile. Extensive numerical simulations involving typical competitive multi-agent control and decision-making problems illustrate the practical effectiveness of the proposed learning-based approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.12561v3</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Filippo Fabiani, Alberto Bemporad</dc:creator>
    </item>
    <item>
      <title>LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models</title>
      <link>https://arxiv.org/abs/2310.03903</link>
      <description>arXiv:2310.03903v2 Announce Type: replace-cross 
Abstract: The emergent reasoning and Theory of Mind (ToM) abilities demonstrated by Large Language Models (LLMs) make them promising candidates for developing coordination agents. In this study, we introduce a new LLM-Coordination Benchmark aimed at a detailed analysis of LLMs within the context of Pure Coordination Games, where participating agents need to cooperate for the most gain. This benchmark evaluates LLMs through two distinct tasks: (1) \emph{Agentic Coordination}, where LLMs act as proactive participants for cooperation in 4 pure coordination games; (2) \emph{Coordination Question Answering (QA)}, where LLMs are prompted to answer 198 multiple-choice questions from the 4 games for evaluation of three key reasoning abilities: Environment Comprehension, ToM Reasoning, and Joint Planning. Furthermore, to enable LLMs for multi-agent coordination, we introduce a Cognitive Architecture for Coordination (CAC) framework that can easily integrate different LLMs as plug-and-play modules for pure coordination games. Our findings indicate that LLM agents equipped with GPT-4-turbo achieve comparable performance to state-of-the-art reinforcement learning methods in games that require commonsense actions based on the environment. Besides, zero-shot coordination experiments reveal that, unlike RL methods, LLM agents are robust to new unseen partners. However, results on Coordination QA show a large room for improvement in the Theory of Mind reasoning and joint planning abilities of LLMs. The analysis also sheds light on how the ability of LLMs to understand their environment and their partner's beliefs and intentions plays a part in their ability to plan for coordination. Our code is available at \url{https://github.com/eric-ai-lab/llm_coordination}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.03903v2</guid>
      <category>cs.CL</category>
      <category>cs.MA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saaket Agashe, Yue Fan, Anthony Reyna, Xin Eric Wang</dc:creator>
    </item>
    <item>
      <title>Adversary-Augmented Simulation to evaluate fairness on HyperLedger Fabric</title>
      <link>https://arxiv.org/abs/2403.14342</link>
      <description>arXiv:2403.14342v2 Announce Type: replace-cross 
Abstract: This paper presents a novel adversary model specifically tailored to distributed systems, aiming to assess the security of blockchain networks. Building upon concepts such as adversarial assumptions, goals, and capabilities, our proposed adversary model classifies and constrains the use of adversarial actions based on classical distributed system models, defined by both failure and communication models. The objective is to study the effects of these allowed actions on the properties of distributed protocols under various system models. A significant aspect of our research involves integrating this adversary model into the Multi-Agent eXperimenter (MAX) framework. This integration enables fine-grained simulations of adversarial attacks on blockchain networks. In this paper, we particularly study four distinct fairness properties on Hyperledger Fabric with the Byzantine Fault Tolerant Tendermint consensus algorithm being selected for its ordering service. We define novel attacks that combine adversarial actions on both protocols, with the aim of violating a specific client-fairness property. Simulations confirm our ability to violate this property and allow us to evaluate the impact of these attacks on several order-fairness properties that relate orders of transaction reception and delivery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14342v2</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.MA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erwan Mahe, Rouwaida Abdallah, Sara Tucci-Piergiovanni, Pierre-Yves Piriou</dc:creator>
    </item>
  </channel>
</rss>
