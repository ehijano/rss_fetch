<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 May 2025 04:00:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Constant-Sum High-Order Barrier Functions for Safety Between Parallel Boundaries</title>
      <link>https://arxiv.org/abs/2505.15932</link>
      <description>arXiv:2505.15932v1 Announce Type: new 
Abstract: This paper takes a step towards addressing the difficulty of constructing Control Barrier Functions (CBFs) for parallel safety boundaries. A single CBF for both boundaries has been reported to be difficult to validate for safety, and we identify why this challenge is inherent. To overcome this, the proposed method constructs separate CBFs for each boundary. We begin by presenting results for the relative degree one case and then extend these to higher relative degrees using the CBF backstepping technique, establishing conditions that guarantee safety. Finally, we showcase our method by applying it to a unicycle system, deriving a simple, verifiable condition to validate the target CBFs for direct implementation of our results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15932v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kwang Hak Kim, Mamadou Diagne, Miroslav Krsti\'c</dc:creator>
    </item>
    <item>
      <title>Grassmann and Flag Varieties in Linear Algebra, Optimization, and Statistics: An Algebraic Perspective</title>
      <link>https://arxiv.org/abs/2505.15969</link>
      <description>arXiv:2505.15969v1 Announce Type: new 
Abstract: Grassmann and flag varieties lead many lives in pure and applied mathematics. Here we focus on the algebraic complexity of solving various problems in linear algebra and statistics as optimization problems over these varieties. The measure of the algebraic complexity is the amount of complex critical points of the corresponding optimization problem. After an exposition of different realizations of these manifolds as algebraic varieties we present a sample of optimization problems over them and we compute their algebraic complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15969v1</guid>
      <category>math.OC</category>
      <category>math.AG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hannah Friedman, Serkan Ho\c{s}ten</dc:creator>
    </item>
    <item>
      <title>Extremum Seeking for PDE Systems using Physics-Informed Neural Networks</title>
      <link>https://arxiv.org/abs/2505.15972</link>
      <description>arXiv:2505.15972v1 Announce Type: new 
Abstract: Extremum Seeking (ES) is an effective real-time optimization method for PDE systems in cascade with nonlinear quadratic maps. To address PDEs in the feedback loop, a boundary control law and a re-design of the additive probing signal are mandatory. The latter, commonly called "trajectory generation" or "motion planning," involves designing perturbation signals that anticipate their propagation through PDEs. Specifically, this requires solving motion planning problems for systems governed by parabolic and hyperbolic PDEs. Physics-Informed Neural Networks (PINN) is a powerful tool for solving PDEs by embedding physical laws as constraints in the neural network's loss function, enabling efficient solutions for high-dimensional, nonlinear, and complex problems. This paper proposes a novel construction integrating PINN and ES, automating the motion planning process for specific PDE systems and eliminating the need for case-by-case analytical derivations. The proposed strategy efficiently extracts perturbation signals, optimizing the PDE system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15972v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haojin Guo, Zongyi Guo, Jianguo Guo, Tiago Roux Oliveira</dc:creator>
    </item>
    <item>
      <title>Asset liability management under sequential stochastic dominance constraints</title>
      <link>https://arxiv.org/abs/2505.16486</link>
      <description>arXiv:2505.16486v1 Announce Type: new 
Abstract: We consider a financial intermediary managing assets and liabilities exposed to several risk sources and seeking an optimal portfolio strategy to minimise the initial capital invested and the total risk associated with investment losses and financial debt. We formulate the problem as a multistage stochastic programming model, with a time-consistent dynamic risk measure in the objective function to control the investment risk. To ensure that the intermediary's financial equilibrium is preserved, we introduce a funding constraint in the model by enforcing in a time-consistent manner a sequential second-order stochastic dominance (SSD) of the portfolio return distribution over the liability distribution. We demonstrate that imposing the SSD constraint at the last-but-one stage is sufficient to enforce the SSD ordering at each stage. To deal with the computational burden of associated MSP, we develop a novel decomposition scheme integrating, for the first time in the literature, time-consistent dynamic risk measures and sequential stochastic dominance constraints. The proposed methodology is computationally validated on a case study developed on a property and casualty ALM problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16486v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giorgio Consigli, Darinka Dentcheva, Francesca Maggioni, Giovanni Micheli</dc:creator>
    </item>
    <item>
      <title>Graph splitting methods: Fixed points and strong convergence for linear subspaces</title>
      <link>https://arxiv.org/abs/2505.16564</link>
      <description>arXiv:2505.16564v1 Announce Type: new 
Abstract: In this paper, we develop a general analysis for the fixed points of the operators defining the graph splitting methods from [SIAM J. Optim., 34 (2024), pp. 1569-1594] by Bredies, Chenchene and Naldi. We particularize it to the case where the maximally monotone operators are normal cones of closed linear subspaces and provide an explicit formula for the limit points of the graph splitting schemes. We exemplify these results on some particular algorithms, unifying in this way some results previously derived as well as obtaining new ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16564v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francisco J. Arag\'on-Artacho, Heinz H. Bauschke, Rub\'en Campoy, C\'esar L\'opez-Pastor</dc:creator>
    </item>
    <item>
      <title>Risk-averse formulations of Stochastic Optimal Control and Markov Decision Processes</title>
      <link>https://arxiv.org/abs/2505.16651</link>
      <description>arXiv:2505.16651v1 Announce Type: new 
Abstract: The aim of this paper is to investigate risk-averse and distributionally robust modeling of Stochastic Optimal Control (SOC) and Markov Decision Process (MDP). We discuss construction of conditional nested risk functionals, a particular attention is given to the Value-at-Risk measure. Necessary and sufficient conditions for existence of non-randomized optimal policies in the framework of robust SOC and MDP are derived. We also investigate sample complexity of optimization problems involving the Value-at-Risk measure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16651v1</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alexander Shapiro, Yan Li</dc:creator>
    </item>
    <item>
      <title>Optimal control of mean-field limit of multiagent systems with and without common noise</title>
      <link>https://arxiv.org/abs/2505.16721</link>
      <description>arXiv:2505.16721v1 Announce Type: new 
Abstract: We consider a generic, suitable class of optimal control problems under a constraint given by a finite-dimensional SDE-ODE system, describing a system of two interacting species of particles: the herd, described by SDEs, and the herders, described by ODEs with the addition of a control function.
  In particular, we firstly show that for a low number of herders and for the limit of large number of herd individuals, the SDE-ODE system can be approximated by an infinite-dimensional system given by a McKean-Vlasov single SDE coupled with ODEs. Then, thanks to this we show the $\Gamma-$convergence of the optimal control problem for the finite-dimensional system to a certain optimal control problem for the mean-field system.
  Differently from Ascione-Castorina-Solombrino [9] (SIAM J. Math. Anal., Vol. 55, No. 6, pp. 6965-6990 (2023)), we do not consider an additive noise for the herd, but a more general class, given by idiosyncratic noises (due to a single herd individual) together with common noise (due to how the environment affects the whole herd), and they are independent one from another. As well as this, we consider a more general class of control functions in the ODEs for herders, where the control is applied not only on the herd dynamics, but also on the herd one.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16721v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giuseppe La Scala</dc:creator>
    </item>
    <item>
      <title>First is the worst, second is the best? A Markov chain analysis of the basketball game knockout</title>
      <link>https://arxiv.org/abs/2505.16842</link>
      <description>arXiv:2505.16842v1 Announce Type: new 
Abstract: The game of Knockout is a classic playground game played with two basketballs. This paper uses a Markov process to analyze each player's probability of winning the game given their starting position in line and shooting percentages, assuming all players are equally skilled. The two-player case is solved in general for any probability of a long shot and short shot shooting percentage and the n-player case with n &gt; 2 is solved numerically. In doing so, this paper answers the question of whether or not the playground wisdom of ``first is the worst, second is best'' is true. We also examine the average number of rounds it takes before the game ends, analyze trends in the data to recommend tips to win at Knockout, and provide questions in the case of players not being equally skilled.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16842v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Flatz, Michael C. Loper, Lezlie Weyer</dc:creator>
    </item>
    <item>
      <title>Horospherically Convex Optimization on Hadamard Manifolds Part I: Analysis and Algorithms</title>
      <link>https://arxiv.org/abs/2505.16970</link>
      <description>arXiv:2505.16970v1 Announce Type: new 
Abstract: Geodesic convexity (g-convexity) is a natural generalization of convexity to Riemannian manifolds. However, g-convexity lacks many desirable properties satisfied by Euclidean convexity. For instance, the natural notions of half-spaces and affine functions are themselves not g-convex. Moreover, recent studies have shown that the oracle complexity of geodesically convex optimization necessarily depends on the curvature of the manifold (Criscitiello and Boumal, 2022; Criscitiello and Boumal, 2023; Hamilton and Moitra, 2021), a computational bottleneck for several problems, e.g., tensor scaling. Recently, Lewis et al. (2024) addressed this challenge by proving curvature-independent convergence of subgradient descent, assuming horospherical convexity of the objective's sublevel sets. Using a similar idea, we introduce a generalization of convex functions to Hadamard manifolds, utilizing horoballs and Busemann functions as building blocks (as proxies for half-spaces and affine functions). We refer to this new notion as horospherical convexity (h-convexity). We provide algorithms for both nonsmooth and smooth h-convex optimization, which have curvature-independent guarantees exactly matching those from Euclidean space; this includes generalizations of subgradient descent and Nesterov's accelerated method. Motivated by applications, we extend these algorithms and their convergence rates to minimizing a sum of horospherically convex functions, assuming access to a weighted-Fr\'echet-mean oracle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16970v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.DG</category>
      <category>math.NA</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher Criscitiello, Jungbin Kim</dc:creator>
    </item>
    <item>
      <title>Generalized algebraic connectivity of graphs in Euclidean spaces: extremal properties and bounds</title>
      <link>https://arxiv.org/abs/2505.16015</link>
      <description>arXiv:2505.16015v1 Announce Type: cross 
Abstract: Graph rigidity, the study of vertex realizations in $\mathbb{R}^d$ and the motions that preserve the induced edge lengths, has been the focus of extensive research for decades. Its equivalency to graph connectivity for $d=1$ is well known; thus it can be viewed as a generalization that incorporates geometric constraints. Graph connectivity is commonly quantified by the algebraic connectivity, the second-smallest eigenvalue of the Laplacian matrix. Recently, a graph invariant for quantifying graph rigidity in $\mathbb{R}^d$, termed the generalized algebraic connectivity, was introduced. Recognizing the intrinsic relationship between rigidity and connectivity, this article presents new contributions. In particular, we introduce the d-rigidity ratio as a metric for expressing the level of rigidity of a graph in $\mathbb{R}^d$ relative to its connectivity. We show that this ratio is bounded and provide extremal examples. Additionally, we offer a new upper bound for the generalized algebraic connectivity that depends inversely on the diameter and on the vertex connectivity, thereby improving previous bounds. Moreover, we investigate the relationship between graph rigidity and the diameter, a measure of the graph's overall extent. We provide the maximal diameter achievable by rigid graphs and show that generalized path graphs serve as extremal examples. Finally, we derive an upper bound for the generalized algebraic connectivity of generalized path graphs that (asymptotically) improves upon existing ones by a factor of four.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16015v1</guid>
      <category>math.CO</category>
      <category>math.OC</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juan F. Presenza, Ignacio Mas, Juan I. Giribet, J. Ignacio Alvarez-Hamelin</dc:creator>
    </item>
    <item>
      <title>Dimension-adapted Momentum Outscales SGD</title>
      <link>https://arxiv.org/abs/2505.16098</link>
      <description>arXiv:2505.16098v1 Announce Type: cross 
Abstract: We investigate scaling laws for stochastic momentum algorithms with small batch on the power law random features model, parameterized by data complexity, target complexity, and model size. When trained with a stochastic momentum algorithm, our analysis reveals four distinct loss curve shapes determined by varying data-target complexities. While traditional stochastic gradient descent with momentum (SGD-M) yields identical scaling law exponents to SGD, dimension-adapted Nesterov acceleration (DANA) improves these exponents by scaling momentum hyperparameters based on model size and data complexity. This outscaling phenomenon, which also improves compute-optimal scaling behavior, is achieved by DANA across a broad range of data and target complexities, while traditional methods fall short. Extensive experiments on high-dimensional synthetic quadratics validate our theoretical predictions and large-scale text experiments with LSTMs show DANA's improved loss exponents over SGD hold in a practical setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16098v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Damien Ferbach, Katie Everett, Gauthier Gidel, Elliot Paquette, Courtney Paquette</dc:creator>
    </item>
    <item>
      <title>Arrival Control in Quasi-Reversible Queueing Systems: Optimization and Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2505.16353</link>
      <description>arXiv:2505.16353v1 Announce Type: cross 
Abstract: In this paper, we introduce a versatile scheme for optimizing the arrival rates of quasi-reversible queueing systems. We first propose an alternative definition of quasi-reversibility that encompasses reversibility and highlights the importance of the definition of customer classes. In a second time, we introduce balanced arrival control policies, which generalize the notion of balanced arrival rates introduced in the context of Whittle networks, to the much broader class of quasi-reversible queueing systems. We prove that supplementing a quasi-reversible queueing system with a balanced arrival-control policy preserves the quasi-reversibility, and we specify the form of the stationary measures. We revisit two canonical examples of quasi-reversible queueing systems, Whittle networks and order-independent queues. Lastly, we focus on the problem of admission control and leverage our results in the frameworks of optimization and reinforcement learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16353v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>C\'eline Comte (CNRS, LAAS-SARA, LAAS-RISC), Pascal Moyal (IECL)</dc:creator>
    </item>
    <item>
      <title>Neural network based control of unknown nonlinear systems via contraction analysis</title>
      <link>https://arxiv.org/abs/2505.16511</link>
      <description>arXiv:2505.16511v1 Announce Type: cross 
Abstract: This paper studies the design of neural network (NN)-based controllers for unknown nonlinear systems, using contraction analysis. A Neural Ordinary Differential Equation (NODE) system is constructed by approximating the unknown draft dynamics with a feedforward NN. Incremental sector bounds and contraction theory are applied to the activation functions and the weights of the NN, respectively. It is demonstrated that if the incremental sector bounds and the weights satisfy some non-convex conditions, the NODE system is contractive. To improve computational efficiency, these non-convex conditions are reformulated as convex LMI conditions. Additionally, it is proven that when the NODE system is contractive, the trajectories of the original autonomous system converge to a neighborhood of the unknown equilibrium, with the size of this neighborhood determined by the approximation error. For a single-layer NN, the NODE system is simplified to a continuous-time Hopfield NN. If the NODE system does not satisfy the contraction conditions, an NN-based controller is designed to enforce contractivity. This controller integrates a linear component, which ensures contraction through suitable control gains, and an NN component, which compensates for the NODE system's nonlinearities. This integrated controller guarantees that the trajectories of the original affine system converge to a neighborhood of the unknown equilibrium. The effectiveness of the proposed approach is demonstrated through two illustrative examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16511v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Yin, Claudio De Persis, Bayu Jayawardhana, Santiago Sanchez Escalonilla Plaza</dc:creator>
    </item>
    <item>
      <title>Learning non-equilibrium diffusions with Schr\"odinger bridges: from exactly solvable to simulation-free</title>
      <link>https://arxiv.org/abs/2505.16644</link>
      <description>arXiv:2505.16644v1 Announce Type: cross 
Abstract: We consider the Schr\"odinger bridge problem which, given ensemble measurements of the initial and final configurations of a stochastic dynamical system and some prior knowledge on the dynamics, aims to reconstruct the "most likely" evolution of the system compatible with the data. Most existing literature assume Brownian reference dynamics and are implicitly limited to potential-driven dynamics. We depart from this regime and consider reference processes described by a multivariate Ornstein-Uhlenbeck process with generic drift matrix $\mathbf{A} \in \mathbb{R}^{d \times d}$. When $\mathbf{A}$ is asymmetric, this corresponds to a non-equilibrium system with non-conservative forces at play: this is important for applications to biological systems, which are naturally exist out-of-equilibrium. In the case of Gaussian marginals, we derive explicit expressions that characterise the solution of both the static and dynamic Schr\"odinger bridge. For general marginals, we propose mvOU-OTFM, a simulation-free algorithm based on flow and score matching for learning the Schr\"odinger bridge. In application to a range of problems based on synthetic and real single cell data, we demonstrate that mvOU-OTFM achieves higher accuracy compared to competing methods, whilst being significantly faster to train.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16644v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephen Y. Zhang, Michael P H Stumpf</dc:creator>
    </item>
    <item>
      <title>Optimising the decision threshold in a weighted voting system: The case of the IMF's Board of Governors</title>
      <link>https://arxiv.org/abs/2505.16654</link>
      <description>arXiv:2505.16654v1 Announce Type: cross 
Abstract: In a weighted majority voting game, the players' weights are determined based on the decision-maker's intentions. The weights are challenging to change in numerous cases, as they represent some desired disparity. However, the voting weights and the actual voting power do not necessarily coincide. Changing a decision threshold would offer some remedy. The International Monetary Fund (IMF) is one of the most important international organisations that uses a weighted voting system to make decisions. The voting weights in its Board of Governors depend on the quotas of the 191 member countries, which reflect their economic strengths to some extent. We analyse the connection between the decision threshold and the a priori voting power of the countries by calculating the Banzhaf indices for each threshold between 50% and 87\%. The difference between the quotas and voting powers is minimised if the decision threshold is 58% or 60%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16654v1</guid>
      <category>econ.GN</category>
      <category>math.OC</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>D\'ora Gr\'eta Petr\'oczy</dc:creator>
    </item>
    <item>
      <title>Sampling and equidistribution theorems for elliptic second order operators, lifting of eigenvalues, and applications</title>
      <link>https://arxiv.org/abs/2505.16655</link>
      <description>arXiv:2505.16655v1 Announce Type: cross 
Abstract: We consider elliptic second order partial differential operators with Lipschitz continuous leading order coefficients on finite cubes and the whole Euclidean space. We prove quantitative sampling and equidistribution theorems for eigenfunctions. The estimates are scale-free, in the sense that for a sequence of growing cubes we obtain uniform estimates. These results are applied to prove lifting of eigenvalues as well as the infimum of the essential spectrum, and an uncertainty relation (aka spectral inequality) for short energy interval spectral projectors. Several application including random operators are discussed. In the proof we have to overcome several challenges posed by the variable coefficients of the leading term.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16655v1</guid>
      <category>math.AP</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.OC</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Tautenhahn, Ivan Veselic</dc:creator>
    </item>
    <item>
      <title>Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization</title>
      <link>https://arxiv.org/abs/2505.16737</link>
      <description>arXiv:2505.16737v1 Announce Type: cross 
Abstract: The significant progress of large language models (LLMs) has led to remarkable achievements across numerous applications. However, their ability to generate harmful content has sparked substantial safety concerns. Despite the implementation of safety alignment techniques during the pre-training phase, recent research indicates that fine-tuning LLMs on adversarial or even benign data can inadvertently compromise their safety. In this paper, we re-examine the fundamental issue of why fine-tuning on non-harmful data still results in safety degradation. We introduce a safety-aware probing (SAP) optimization framework designed to mitigate the safety risks of fine-tuning LLMs. Specifically, SAP incorporates a safety-aware probe into the gradient propagation process, mitigating the model's risk of safety degradation by identifying potential pitfalls in gradient directions, thereby enhancing task-specific performance while successfully preserving model safety. Our extensive experimental results demonstrate that SAP effectively reduces harmfulness below the original fine-tuned model and achieves comparable test loss to standard fine-tuning methods. Our code is available at https://github.com/ChengcanWu/SAP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16737v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>math.OC</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengcan Wu, Zhixin Zhang, Zeming Wei, Yihao Zhang, Meng Sun</dc:creator>
    </item>
    <item>
      <title>Meta-reinforcement learning with minimum attention</title>
      <link>https://arxiv.org/abs/2505.16741</link>
      <description>arXiv:2505.16741v1 Announce Type: cross 
Abstract: Minimum attention applies the least action principle in the changes of control concerning state and time, first proposed by Brockett. The involved regularization is highly relevant in emulating biological control, such as motor learning. We apply minimum attention in reinforcement learning (RL) as part of the rewards and investigate its connection to meta-learning and stabilization. Specifically, model-based meta-learning with minimum attention is explored in high-dimensional nonlinear dynamics. Ensemble-based model learning and gradient-based meta-policy learning are alternately performed. Empirically, we show that the minimum attention does show outperforming competence in comparison to the state-of-the-art algorithms in model-free and model-based RL, i.e., fast adaptation in few shots and variance reduction from the perturbations of the model and environment. Furthermore, the minimum attention demonstrates the improvement in energy efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16741v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pilhwa Lee, Shashank Gupta</dc:creator>
    </item>
    <item>
      <title>Revenue Optimization with Price-Sensitive and Interdependent Demand</title>
      <link>https://arxiv.org/abs/2505.16748</link>
      <description>arXiv:2505.16748v1 Announce Type: cross 
Abstract: As Kalyan T. Talluri and Garrett J. Van Ryzin describe in their work [3], Revenue Management aims to maximize an organization's revenue by considering three types of decision categories: structural, pricing, and quantity. In this document, our primary focus will be on decisions related to pricing and quantity for the sale of airline tickets on a direct flight over a certain number of time periods. More specifically, we will only focus on the optimization aspect of this problem. We will assume the demand data to be given, since Air France estimates it beforehand using real data. Similarly, we assume all price options to be predetermined by Air France's algorithms and verified by their analysts. Our objective will be to maximize the revenue of a direct flight by choosing the prices for each product from the predefined set of options.
  --
  Comme d\'ecrit par Kalyan T. Talluri et Garrett J. Van Ryzin dans leur ouvrage [3], le Revenue Management consiste en la maximisation du revenu d'un organisme \`a partir de trois types de cat\'egories de d\'ecision : structurelles, prix et quantit\'e. Dans ce document, nous nous int\'eresserons principalement aux d\'ecisions de type prix et quantit\'e pour la vente de billets d'avion sur un vol direct au cours d'un certain nombre de pas de temps. Plus pr\'ecis\'ement, nous nous situerons dans la partie optimisation du probl\`eme. Nous prendrons ainsi les donn\'ees de demande comme acquises, car elles sont estim\'ees au pr\'ealable par Air France \`a partir des donn\'ees r\'eelles. De m\^eme, pour chaque produit que l'on cherchera \`a vendre, on nous impose en amont les prix possibles que l'on a droit d'utiliser et qui se basent sur des algorithmes d'Air France dont les r\'esultats sont v\'erifi\'es par des analystes. Notre but sera alors de maximiser le revenu d'un vol direct en choisissant les prix de chaque produit parmi ceux impos\'es.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16748v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julien Laasri, Marc Revol</dc:creator>
    </item>
    <item>
      <title>The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm</title>
      <link>https://arxiv.org/abs/2505.16932</link>
      <description>arXiv:2505.16932v1 Announce Type: cross 
Abstract: Computing the polar decomposition and the related matrix sign function, has been a well-studied problem in numerical analysis for decades. More recently, it has emerged as an important subroutine in deep learning, particularly within the Muon optimization framework. However, the requirements in this setting differ significantly from those of traditional numerical analysis. In deep learning, methods must be highly efficient and GPU-compatible, but high accuracy is often unnecessary. As a result, classical algorithms like Newton-Schulz (which suffers from slow initial convergence) and methods based on rational functions (which rely on QR decompositions or matrix inverses) are poorly suited to this context. In this work, we introduce Polar Express, a GPU-friendly algorithm for computing the polar decomposition. Like classical polynomial methods such as Newton-Schulz, our approach uses only matrix-matrix multiplications, making it GPU-compatible. Motivated by earlier work of Chen &amp; Chow and Nakatsukasa &amp; Freund, Polar Express adapts the polynomial update rule at each iteration by solving a minimax optimization problem, and we prove that it enjoys a strong worst-case optimality guarantee. This property ensures both rapid early convergence and fast asymptotic convergence. We also address finite-precision issues, making it stable in bfloat16 in practice. We apply Polar Express within the Muon optimization framework and show consistent improvements in validation loss on large-scale models such as GPT-2, outperforming recent alternatives across a range of learning rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16932v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noah Amsel, David Persson, Christopher Musco, Robert Gower</dc:creator>
    </item>
    <item>
      <title>Delayed dynamic-feedback controller design for multi-frequency vibration suppression</title>
      <link>https://arxiv.org/abs/2505.16939</link>
      <description>arXiv:2505.16939v1 Announce Type: cross 
Abstract: We present a methodology for designing a dynamic controller with delayed output feedback for achieving non-collocated vibration suppression with a focus on the multi-frequency case. To synthesize the delay-based controller, we first remodel the system of equations as a delay-differential algebraic equation (DDAE) in such a way that existing tools for design of a static output feedback controller can be easily adapted. The problem of achieving non-collocated vibration suppression with sufficient damping is formulated as a constrained optimization problem of minimizing the spectral abscissa in the presence of zero-location constraints, with the constraints exhibiting polynomial dependence on its parameters. We transform the problem into an unconstrained one using elimination, following which we solve the resulting non-convex, non-smooth optimization problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16939v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrian Saldanha, Adam Peichl, Wim Michiels, Tom\'a\v{s} Vyhl\'idal</dc:creator>
    </item>
    <item>
      <title>Relative-Interior Solution for the (Incomplete) Linear Assignment Problem with Applications to the Quadratic Assignment Problem</title>
      <link>https://arxiv.org/abs/2301.11201</link>
      <description>arXiv:2301.11201v4 Announce Type: replace 
Abstract: We study the set of optimal solutions of the dual linear programming formulation of the linear assignment problem (LAP) to propose a method for computing a solution from the relative interior of this set. Assuming that an arbitrary dual-optimal solution and an optimal assignment are available (for which many efficient algorithms already exist), our method computes a relative-interior solution in linear time. Since the LAP occurs as a subproblem in the linear programming (LP) relaxation of the quadratic assignment problem (QAP), we employ our method as a new component in the family of dual-ascent algorithms that provide bounds on the optimal value of the QAP. To make our results applicable to the incomplete QAP, which is of interest in practical use-cases, we also provide a linear-time reduction from the incomplete LAP to the complete LAP along with a mapping that preserves optimality and membership in the relative interior. Our experiments on publicly available benchmarks indicate that our approach with relative-interior solution can frequently provide bounds near the optimum of the LP relaxation and its runtime is much lower when compared to a commercial LP solver.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.11201v4</guid>
      <category>math.OC</category>
      <category>cs.CV</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tom\'a\v{s} Dlask, Bogdan Savchynskyy</dc:creator>
    </item>
    <item>
      <title>Partial State-Feedback Reduced-Order Switching Predictive Models for Next-Generation Optical Lithography Systems</title>
      <link>https://arxiv.org/abs/2402.13928</link>
      <description>arXiv:2402.13928v3 Announce Type: replace 
Abstract: This paper presents a partial state-feedback reduced-order switching predictive model designed to support the next-generation lithography roadmap. The proposed approach addresses the trade-off between increasing the number of measurements to improve overlay accuracy and the resulting challenges, including higher measurement noise, reduced throughput and overlay/placement errors under uncertain operating conditions.. By minimizing (die-) placement errors and reducing unnecessary measurements, the method enhances system performance and throughput. Our solution employs a streamlined model with adaptive switching logic to manage time-varying uncertainties induced by fluctuating operating conditions. The methodology is implemented on a state-of-the-art lithographic scanner to mitigate the spatial-temporal dynamics of reticle heating, serving as a representative industrial application. Reticle heating, which worsens with increased throughput, introduces spatial-temporal distortions that directly degrade die placement accuracy. Experimental results demonstrate significant improvements: placement errors are reduced by a factor of $2-3$x, and throughput is improved by $0.3$seconds per wafer. Importantly, the method accounts for the fact that increased throughput can exacerbate reticle heating, which directly impacts overlay performance. By actively compensating for these thermomechanical effects, the proposed approach ensures that overlay accuracy is maintained or improved -- even under increased throughput conditions -- highlighting its potential for broader application in advanced lithographic systems, particularly in thermal and vibration control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13928v3</guid>
      <category>math.OC</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Raaja Ganapathy Subramanian, Barry Moest, Bart Paarhuis</dc:creator>
    </item>
    <item>
      <title>Majorization-minimization Bregman proximal gradient algorithms for NMF with the Kullback--Leibler divergence</title>
      <link>https://arxiv.org/abs/2405.11185</link>
      <description>arXiv:2405.11185v3 Announce Type: replace 
Abstract: Nonnegative matrix factorization (NMF) is a popular method in machine learning and signal processing to decompose a given nonnegative matrix into two nonnegative matrices. In this paper, we propose new algorithms, called majorization-minimization Bregman proximal gradient algorithm (MMBPG) and MMBPG with extrapolation (MMBPGe) to solve NMF. These iterative algorithms minimize the objective function and its potential function monotonically. Assuming the Kurdyka--\L{}ojasiewicz property, we establish that a sequence generated by MMBPG(e) globally converges to a stationary point. We apply MMBPG and MMBPGe to the Kullback--Leibler (KL) divergence-based NMF. While most existing KL-based NMF methods update two blocks or each variable alternately, our algorithms update all variables simultaneously. MMBPG and MMBPGe for KL-based NMF are equipped with a separable Bregman distance that satisfies the smooth adaptable property and that makes its subproblem solvable in closed form. Using this fact, we guarantee that a sequence generated by MMBPG(e) globally converges to a Karush--Kuhn--Tucker (KKT) point of KL-based NMF. In numerical experiments, we compare proposed algorithms with existing algorithms on synthetic data and real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11185v3</guid>
      <category>math.OC</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shota Takahashi, Mirai Tanaka, Shiro Ikeda</dc:creator>
    </item>
    <item>
      <title>Distributed alternating gradient descent for convex semi-infinite programs over a network</title>
      <link>https://arxiv.org/abs/2408.11937</link>
      <description>arXiv:2408.11937v2 Announce Type: replace 
Abstract: This paper presents a first-order distributed algorithm for solving a convex semi-infinite program (SIP) over a time-varying network. In this setting, the objective function associated with the optimization problem is a summation of a set of functions, each held by one node in a network. The semi-infinite constraint, on the other hand, is known to all agents. The nodes collectively aim to solve the problem using local data about the objective and limited communication capabilities depending on the network topology. Our algorithm is built on three key ingredients: consensus step, gradient descent in the local objective, and local gradient descent iterations in the constraint at a node when the estimate violates the semi-infinite constraint. The algorithm is constructed, and its parameters are prescribed in such a way that the iterates held by each agent provably converge to an optimizer. That is, as the algorithm progresses, the estimates achieve consensus, and the constraint violation and the error in the optimal value are bounded above by vanishing terms. Simulation examples illustrate our results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11937v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashwin Aravind, Debasish Chatterjee, Ashish Cherukuri</dc:creator>
    </item>
    <item>
      <title>Enforcing Mesh Quality Constraints in Shape Optimization with a Gradient Projection Method</title>
      <link>https://arxiv.org/abs/2412.00006</link>
      <description>arXiv:2412.00006v3 Announce Type: replace 
Abstract: For the numerical solution of shape optimization problems, particularly those constrained by partial differential equations (PDEs), the quality of the underlying mesh is of utmost importance. Particularly when investigating complex geometries, the mesh quality tends to deteriorate over the course of a shape optimization so that either the optimization comes to a halt or an expensive remeshing operation must be performed before the optimization can be continued. In this paper, we present a novel, semi-discrete approach for enforcing a minimum mesh quality in shape optimization. Our approach is based on Rosen's gradient projection method, which incorporates mesh quality constraints into the shape optimization problem. The proposed constraints bound the angles of triangular and solid angles of tetrahedral mesh cells and, thus, also bound the quality of these mesh cells. The method treats these constraints by projecting the search direction to the linear subspace of the currently active constraints. Additionally, only slight modifications to the usual line search procedure are required to ensure the feasibility of the method. We present our method for two- and three-dimensional simplicial meshes. We investigate the proposed approach numerically for the drag minimization of an obstacle in a two-dimensional Stokes flow, the optimization of the flow in a pipe governed by the Navier-Stokes equations, and for the large-scale, three-dimensional optimization of a structured packing used in a distillation column. Our results show that the proposed method is indeed capable of guaranteeing a minimum mesh quality for both academic examples and challenging industrial applications. Particularly, our approach allows the shape optimization of complex structures while ensuring that the mesh quality does not deteriorate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00006v3</guid>
      <category>math.OC</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Blauth, Christian Leith\"auser</dc:creator>
    </item>
    <item>
      <title>Fast computation of the TGOSPA metric for multiple target tracking via unbalanced optimal transport</title>
      <link>https://arxiv.org/abs/2503.09449</link>
      <description>arXiv:2503.09449v2 Announce Type: replace 
Abstract: In multiple target tracking, it is important to be able to evaluate the performance of different tracking algorithms. The trajectory generalized optimal sub-pattern assignment metric (TGOSPA) is a recently proposed metric for such evaluations. The TGOSPA metric is computed as the solution to an optimization problem, but for large tracking scenarios, solving this problem becomes computationally demanding. In this paper, we present an approximation algorithm for evaluating the TGOSPA metric, based on casting the TGOSPA problem as an unbalanced multimarginal optimal transport problem. Following recent advances in computational optimal transport, we introduce an entropy regularization and derive an iterative scheme for solving the Lagrangian dual of the regularized problem. Numerical results suggest that our proposed algorithm is more computationally efficient than the alternative of computing the exact metric using a linear programming solver, while still providing an adequate approximation of the metric.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09449v2</guid>
      <category>math.OC</category>
      <category>cs.CV</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Viktor Nevelius Wernholm, Alfred W\"arns\"ater, Axel Ringh</dc:creator>
    </item>
    <item>
      <title>Distributionally Robust Optimization over Wasserstein Balls with i.i.d. Structure</title>
      <link>https://arxiv.org/abs/2503.23543</link>
      <description>arXiv:2503.23543v2 Announce Type: replace 
Abstract: We consider distributionally robust optimization problems where the uncertainty is modeled via a structured Wasserstein ambiguity set. Specifically, the ambiguity is restricted to product measures $P^{\otimes N}$, where $P$ lies within a Wasserstein ball centered at an empirical distribution $\widehat{P}$. This structure reflects the assumption of independent and identically distributed (i.i.d.) uncertainty components and yields a non-convex ambiguity set that is strictly contained in its unstructured counterpart, thereby reducing conservatism. The resulting optimization problem is generally intractable due to the loss of convexity. We address this by introducing a sequence of tractable convex relaxations, each admitting strong duality, and prove that this sequence converges to the original problem value under suitable conditions. Numerical examples are provided to illustrate the effectiveness of the proposed approach. As a byproduct of our proofs, we establish a novel formula, of independent interest, relating the Wasserstein distance of a mixture of product distributions to the Wasserstein distance between its constituent measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23543v2</guid>
      <category>math.OC</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrey Kharitenko, Marta Fochesato, Anastasios Tsiamis, Niklas Schmid, John Lygeros</dc:creator>
    </item>
    <item>
      <title>Computing Equilibria in Stochastic Nonconvex and Non-monotone Games via Gradient-Response Schemes</title>
      <link>https://arxiv.org/abs/2504.14056</link>
      <description>arXiv:2504.14056v2 Announce Type: replace 
Abstract: We consider a class of smooth $N$-player noncooperative games, where player objectives are expectation-valued and potentially nonconvex. In such a setting, we consider the largely open question of efficiently computing a suitably defined {\em quasi}-Nash equilibrium (QNE) via a single-step gradient-response framework. First, under a suitably defined quadratic growth property, we prove that the stochastic synchronous gradient-response ({\bf SSGR}) scheme and its asynchronous counterpart ({\bf SAGR}) are characterized by almost sure convergence to a QNE and a sublinear rate guarantee. Notably, when a potentiality requirement is overlaid under a somewhat stronger pseudomonotonicity condition, this claim can be made for NE, rather than QNE. Second, under a weak sharpness property, we show that the deterministic synchronous variant displays a {\em linear} rate of convergence sufficiently close to a QNE by leveraging a geometric decay in steplengths. This suggests the development of a two-stage scheme with global non-asymptotic sublinear rates and a local linear rate. Third, when player problems are convex but the associated concatenated gradient map is potentially non-monotone, we prove that a zeroth-order asynchronous modified gradient-response ({\bf ZAMGR}) scheme can efficiently compute NE under a suitable copositivity requirement. Collectively, our findings represent amongst the first inroads into efficient computation of QNE/NE in nonconvex settings, leading to a set of single-step schemes that are characterized by broader reach while often providing last-iterate rate guarantees. We present applications satisfying the prescribed requirements where preliminary numerics appear promising.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14056v2</guid>
      <category>math.OC</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuoyu Xiao, Uday V. Shanbhag</dc:creator>
    </item>
    <item>
      <title>CGD: Modifying the Loss Landscape by Gradient Regularization</title>
      <link>https://arxiv.org/abs/2504.16182</link>
      <description>arXiv:2504.16182v2 Announce Type: replace 
Abstract: Line-search methods are commonly used to solve optimization problems. The simplest line search method is steepest descent where one always moves in the direction of the negative gradient. Newton's method on the other hand is a second-order method that uses the curvature information in the Hessian to pick the descent direction. In this work, we propose a new line-search method called Constrained Gradient Descent (CGD) that implicitly changes the landscape of the objective function for efficient optimization. CGD is formulated as a solution to the constrained version of the original problem where the constraint is on a function of the gradient. We optimize the corresponding Lagrangian function thereby favourably changing the landscape of the objective function. This results in a line search procedure where the Lagrangian penalty acts as a control over the descent direction and can therefore be used to iterate over points that have smaller gradient values, compared to iterates of vanilla steepest descent. We establish global linear convergence rates for CGD and provide numerical experiments on synthetic test functions to illustrate the performance of CGD. We also provide two practical variants of CGD, CGD-FD which is a Hessian free variant and CGD-QN, a quasi-Newton variant and demonstrate their effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16182v2</guid>
      <category>math.OC</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shikhar Saxena, Tejas Bodas, Arti Yardi</dc:creator>
    </item>
    <item>
      <title>Temporal Robustness in Discrete Time Linear Dynamical Systems</title>
      <link>https://arxiv.org/abs/2505.02347</link>
      <description>arXiv:2505.02347v2 Announce Type: replace 
Abstract: Discrete time linear dynamical systems, including Markov chains, have found many applications. However, in some problems, there is uncertainty about the time horizon for which the system runs. This creates uncertainty about the cost (or reward) incurred based on the state distribution when the system stops. Given past data samples of how long a system ran, we propose to theoretically analyze a distributional robust cost estimation task in a Wasserstein ambiguity set, instead of learning a probability distribution from a few samples. Towards this, we show an equivalence between a discrete time Markov Chain on a probability simplex and a global asymptotic stable (GAS) discrete time linear dynamical system, allowing us to base our study on a GAS system only. Then, we provide various polynomial time algorithms and hardness results for different cases in our theoretical study, including a fundamental result about Wasserstein distance based polytope.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02347v2</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nilava Metya, Arunesh Sinha</dc:creator>
    </item>
    <item>
      <title>A deep solver for BSDEs with jumps</title>
      <link>https://arxiv.org/abs/2211.04349</link>
      <description>arXiv:2211.04349v3 Announce Type: replace-cross 
Abstract: The aim of this work is to propose an extension of the deep solver by Han, Jentzen, E (2018) to the case of forward backward stochastic differential equations (FBSDEs) with jumps. As in the aforementioned solver, starting from a discretized version of the FBSDE and parametrizing the (high dimensional) control processes by means of a family of artificial neural networks (ANNs), the FBSDE is viewed as a model-based reinforcement learning problem and the ANN parameters are fitted so as to minimize a prescribed loss function. We take into account both finite and infinite jump activity by introducing, in the latter case, an approximation with finitely many jumps of the forward process. We successfully apply our algorithm to option pricing problems in low and high dimension and discuss the applicability in the context of counterparty credit risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.04349v3</guid>
      <category>math.PR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>q-fin.CP</category>
      <category>q-fin.PR</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kristoffer Andersson, Alessandro Gnoatto, Marco Patacca, Athena Picarelli</dc:creator>
    </item>
    <item>
      <title>Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination</title>
      <link>https://arxiv.org/abs/2311.02960</link>
      <description>arXiv:2311.02960v3 Announce Type: replace-cross 
Abstract: Over the past decade, deep learning has proven to be a highly effective tool for learning meaningful features from raw data. However, it remains an open question how deep networks perform hierarchical feature learning across layers. In this work, we attempt to unveil this mystery by investigating the structures of intermediate features. Motivated by our empirical findings that linear layers mimic the roles of deep layers in nonlinear networks for feature learning, we explore how deep linear networks transform input data into output by investigating the output (i.e., features) of each layer after training in the context of multi-class classification problems. Toward this goal, we first define metrics to measure within-class compression and between-class discrimination of intermediate features, respectively. Through theoretical analysis of these two metrics, we show that the evolution of features follows a simple and quantitative pattern from shallow to deep layers when the input data is nearly orthogonal and the network weights are minimum-norm, balanced, and approximate low-rank: Each layer of the linear network progressively compresses within-class features at a geometric rate and discriminates between-class features at a linear rate with respect to the number of layers that data have passed through. To the best of our knowledge, this is the first quantitative characterization of feature evolution in hierarchical representations of deep linear networks. Empirically, our extensive experiments not only validate our theoretical results numerically but also reveal a similar pattern in deep nonlinear networks which aligns well with recent empirical studies. Moreover, we demonstrate the practical implications of our results in transfer learning. Our code is available at https://github.com/Heimine/PNC_DLN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02960v3</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>math.OC</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peng Wang, Xiao Li, Can Yaras, Zhihui Zhu, Laura Balzano, Wei Hu, Qing Qu</dc:creator>
    </item>
    <item>
      <title>A fast algorithm to minimize prediction loss of the optimal solution in inverse optimization problem of MILP</title>
      <link>https://arxiv.org/abs/2405.14273</link>
      <description>arXiv:2405.14273v2 Announce Type: replace-cross 
Abstract: We consider the inverse optimization problem of estimating the weights of the objective function such that the given solution is an optimal solution for a mixed integer linear program (MILP). In this inverse optimization problem, the known methods exhibit inefficient convergence. Specifically, if $d$ denotes the dimension of the weights and $k$ the number of iterations, then the error of the weights is bounded by $O(k^{-1/(d-1)})$, leading to slow convergence as $d$ increases.We propose a projected subgradient method with a step size of $k^{-1/2}$ based on suboptimality loss. We theoretically show and demonstrate that the proposed method efficiently learns the weights. In particular, we show that there exists a constant $\gamma &gt; 0$ such that the distance between the learned and true weights is bounded by $ O\left(k^{-1/(1+\gamma)} \exp\left(-\frac{\gamma k^{1/2}}{2+\gamma}\right)\right), $ or the optimal solution is exactly recovered. Furthermore, experiments demonstrate that the proposed method solves the inverse optimization problems of MILP using fewer than $1/7$ the number of MILP calls required by known methods, and converges within a finite number of iterations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14273v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akira Kitaoka</dc:creator>
    </item>
    <item>
      <title>Gradient Flows and Riemannian Structure in the Gromov-Wasserstein Geometry</title>
      <link>https://arxiv.org/abs/2407.11800</link>
      <description>arXiv:2407.11800v2 Announce Type: replace-cross 
Abstract: The Wasserstein space of probability measures is known for its intricate Riemannian structure, which underpins the Wasserstein geometry and enables gradient flow algorithms. However, the Wasserstein geometry may not be suitable for certain tasks or data modalities. Motivated by scenarios where the global structure of the data needs to be preserved, this work initiates the study of gradient flows and Riemannian structure in the Gromov-Wasserstein (GW) geometry, which is particularly suited for such purposes. We focus on the inner product GW (IGW) distance between distributions on $\mathbb{R}^d$. Given a functional $\mathsf{F}:\mathcal{P}_2(\mathbb{R}^d)\to\mathbb{R}$ to optimize, we present an implicit IGW minimizing movement scheme that generates a sequence of distributions $\{\rho_i\}_{i=0}^n$, which are close in IGW and aligned in the 2-Wasserstein sense. Taking the time step to zero, we prove that the discrete solution converges to an IGW generalized minimizing movement (GMM) $(\rho_t)_t$ that follows the continuity equation with a velocity field $v_t\in L^2(\rho_t;\mathbb{R}^d)$, specified by a global transformation of the Wasserstein gradient of $\mathsf{F}$. The transformation is given by a mobility operator that modifies the Wasserstein gradient to encode not only local information, but also global structure. Our gradient flow analysis leads us to identify the Riemannian structure that gives rise to the intrinsic IGW geometry, using which we establish a Benamou-Brenier-like formula for IGW. We conclude with a formal derivation, akin to the Otto calculus, of the IGW gradient as the inverse mobility acting on the Wasserstein gradient. Numerical experiments validating our theory and demonstrating the global nature of IGW interpolations are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11800v2</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhengxin Zhang, Ziv Goldfeld, Kristjan Greenewald, Youssef Mroueh, Bharath K. Sriperumbudur</dc:creator>
    </item>
    <item>
      <title>Ringmaster ASGD: The First Asynchronous SGD with Optimal Time Complexity</title>
      <link>https://arxiv.org/abs/2501.16168</link>
      <description>arXiv:2501.16168v2 Announce Type: replace-cross 
Abstract: Asynchronous Stochastic Gradient Descent (Asynchronous SGD) is a cornerstone method for parallelizing learning in distributed machine learning. However, its performance suffers under arbitrarily heterogeneous computation times across workers, leading to suboptimal time complexity and inefficiency as the number of workers scales. While several Asynchronous SGD variants have been proposed, recent findings by Tyurin &amp; Richt\'arik (NeurIPS 2023) reveal that none achieve optimal time complexity, leaving a significant gap in the literature. In this paper, we propose Ringmaster ASGD, a novel Asynchronous SGD method designed to address these limitations and tame the inherent challenges of Asynchronous SGD. We establish, through rigorous theoretical analysis, that Ringmaster ASGD achieves optimal time complexity under arbitrarily heterogeneous and dynamically fluctuating worker computation times. This makes it the first Asynchronous SGD method to meet the theoretical lower bounds for time complexity in such scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16168v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Artavazd Maranjyan, Alexander Tyurin, Peter Richt\'arik</dc:creator>
    </item>
    <item>
      <title>Joint Pricing and Resource Allocation: An Optimal Online-Learning Approach</title>
      <link>https://arxiv.org/abs/2501.18049</link>
      <description>arXiv:2501.18049v2 Announce Type: replace-cross 
Abstract: We study an online learning problem on dynamic pricing and resource allocation, where we make joint pricing and inventory decisions to maximize the overall net profit. We consider the stochastic dependence of demands on the price, which complicates the resource allocation process and introduces significant non-convexity and non-smoothness to the problem. To solve this problem, we develop an efficient algorithm that utilizes a "Lower-Confidence Bound (LCB)" meta-strategy over multiple OCO agents. Our algorithm achieves $\tilde{O}(\sqrt{Tmn})$ regret (for $m$ suppliers and $n$ consumers), which is optimal with respect to the time horizon $T$. Our results illustrate an effective integration of statistical learning methodologies with complex operations research problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18049v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianyu Xu, Xuan Wang, Yu-Xiang Wang, Jiashuo Jiang</dc:creator>
    </item>
    <item>
      <title>ATA: Adaptive Task Allocation for Efficient Resource Management in Distributed Machine Learning</title>
      <link>https://arxiv.org/abs/2502.00775</link>
      <description>arXiv:2502.00775v2 Announce Type: replace-cross 
Abstract: Asynchronous methods are fundamental for parallelizing computations in distributed machine learning. They aim to accelerate training by fully utilizing all available resources. However, their greedy approach can lead to inefficiencies using more computation than required, especially when computation times vary across devices. If the computation times were known in advance, training could be fast and resource-efficient by assigning more tasks to faster workers. The challenge lies in achieving this optimal allocation without prior knowledge of the computation time distributions. In this paper, we propose ATA (Adaptive Task Allocation), a method that adapts to heterogeneous and random distributions of worker computation times. Through rigorous theoretical analysis, we show that ATA identifies the optimal task allocation and performs comparably to methods with prior knowledge of computation times. Experimental results further demonstrate that ATA is resource-efficient, significantly reducing costs compared to the greedy approach, which can be arbitrarily expensive depending on the number of workers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00775v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Artavazd Maranjyan, El Mehdi Saad, Peter Richt\'arik, Francesco Orabona</dc:creator>
    </item>
    <item>
      <title>Optimizing Asynchronous Federated Learning: A~Delicate Trade-Off Between Model-Parameter Staleness and Update Frequency</title>
      <link>https://arxiv.org/abs/2502.08206</link>
      <description>arXiv:2502.08206v3 Announce Type: replace-cross 
Abstract: Synchronous federated learning (FL) scales poorly with the number of clients due to the straggler effect. Algorithms like FedAsync and GeneralizedFedAsync address this limitation by enabling asynchronous communication between clients and the central server. In this work, we rely on stochastic modeling and analysis to better understand the impact of design choices in asynchronous FL algorithms, such as the concurrency level and routing probabilities, and we leverage this knowledge to optimize loss. Compared to most existing studies, we account for the joint impact of heterogeneous and variable service speeds and heterogeneous datasets at the clients. We characterize in particular a fundamental trade-off for optimizing asynchronous FL: minimizing gradient estimation errors by avoiding model parameter staleness, while also speeding up the system by increasing the throughput of model updates. Our two main contributions can be summarized as follows. First, we prove a discrete variant of Little's law to derive a closed-form expression for relative delay, a metric that quantifies staleness. This allows us to efficiently minimize the average loss per model update, which has been the gold standard in literature to date. Second, we observe that naively optimizing this metric leads us to slow down the system drastically by overemphazing staleness at the detriment of throughput. This motivates us to introduce an alternative metric that also takes system speed into account, for which we derive a tractable upper-bound that can be minimized numerically. Extensive numerical results show that these optimizations enhance accuracy by 10% to 30%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08206v3</guid>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abdelkrim Alahyane (LAAS-SARA, LAAS), C\'eline Comte (CNRS, LAAS-SARA, LAAS), Matthieu Jonckheere (CNRS, LAAS-SARA, LAAS), \'Eric Moulines (X)</dc:creator>
    </item>
    <item>
      <title>Dion: Distributed Orthonormalized Updates</title>
      <link>https://arxiv.org/abs/2504.05295</link>
      <description>arXiv:2504.05295v2 Announce Type: replace-cross 
Abstract: Recent work has shown that orthonormal matrix updates speed up neural network optimization, improve training stability, and offer better hyperparameter transfer across model sizes. Applying these updates efficiently when model weights and optimizer states are sharded across a large-scale distributed LLM training system remains a major challenge. We introduce Dion (DIstributed OrthoNormalization), a scalable and communication-efficient orthonormalizing optimizer. Dion leverages low-rank approximation and decoupled momentum buffers, eliminating the need for full gradient synchronization while producing numerically equivalent results. It is compatible with simultaneous DDP, FSDP, and TP parallelism, and it computes an orthonormalized update without unsharding a full parameter matrix on any single device. We evaluate Dion on language models from 120M to 3B parameters and find that its benefits improve with increasing model size and batch size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05295v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kwangjun Ahn, Byron Xu, Natalie Abreu, John Langford</dc:creator>
    </item>
    <item>
      <title>Architecture independent generalization bounds for overparametrized deep ReLU networks</title>
      <link>https://arxiv.org/abs/2504.05695</link>
      <description>arXiv:2504.05695v3 Announce Type: replace-cross 
Abstract: We prove that overparametrized neural networks are able to generalize with a test error that is independent of the level of overparametrization, and independent of the Vapnik-Chervonenkis (VC) dimension. We prove explicit bounds that only depend on the metric geometry of the test and training sets, on the regularity properties of the activation function, and on the operator norms of the weights and norms of biases. For overparametrized deep ReLU networks with a training sample size bounded by the input space dimension, we explicitly construct zero loss minimizers without use of gradient descent, and prove that the generalization error is independent of the network architecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05695v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.AP</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Chen, Chun-Kai Kevin Chien, Patricia Mu\~noz Ewald, Andrew G. Moore</dc:creator>
    </item>
  </channel>
</rss>
