<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Sep 2024 04:00:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Comprehensive Analysis of Process Energy Consumption on Multi-Socket Systems with GPUs</title>
      <link>https://arxiv.org/abs/2409.04941</link>
      <description>arXiv:2409.04941v1 Announce Type: cross 
Abstract: Robustly estimating energy consumption in High-Performance Computing (HPC) is essential for assessing the energy footprint of modern workloads, particularly in fields such as Artificial Intelligence (AI) research, development, and deployment. The extensive use of supercomputers for AI training has heightened concerns about energy consumption and carbon emissions. Existing energy estimation tools often assume exclusive use of computing nodes, a premise that becomes problematic with the advent of supercomputers integrating microservices, as seen in initiatives like Acceleration as a Service (XaaS) and cloud computing.
  This work investigates the impact of executed instructions on overall power consumption, providing insights into the comprehensive behaviour of HPC systems. We introduce two novel mathematical models to estimate a process's energy consumption based on the total node energy, process usage, and a normalised vector of the probability distribution of instruction types for CPU and GPU processes. Our approach enables energy accounting for specific processes without the need for isolation.
  Our models demonstrate high accuracy, predicting CPU power consumption with a mere 1.9% error. For GPU predictions, the models achieve a central relative error of 9.7%, showing a clear tendency to fit the test data accurately. These results pave the way for new tools to measure and account for energy consumption in shared supercomputing environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04941v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luis G. Le\'on-Vega, Niccol\`o Tosato, Stefano Cozzini</dc:creator>
    </item>
    <item>
      <title>From Concept to Reality: 5G Positioning with Open-Source Implementation of UL-TDoA in OpenAirInterface</title>
      <link>https://arxiv.org/abs/2409.05217</link>
      <description>arXiv:2409.05217v1 Announce Type: cross 
Abstract: This paper presents, for the first time, an open-source implementation of the 3GPP Uplink Time Difference of Arrival (UL-TDoA) positioning method using the OpenAirInterface (OAI) framework. UL-TDoA is a critical positioning technique in 5G networks, leveraging the time differences of signal arrival at multiple base stations to determine the precise location of User Equipment (UE). This implementation aims to democratize access to advanced positioning technology by integrating UL-TDoA capabilities into both the Radio Access Network (RAN) and Core Network (CN) components of OAI, providing a comprehensive and 3GPP-compliant solution.
  The development includes the incorporation of essential protocol procedures, message flows, and interfaces as defined by 3GPP standards. Validation is conducted using two distinct methods: an OAI-RF simulator-based setup for controlled testing and an O-RAN-based Localization Testbed at EURECOM in real-world conditions. The results demonstrate the viability of this open-source UL-TDoA implementation, enabling precise positioning in various environments. By making this implementation publicly available, the study paves the way for widespread research, development, and innovation in the field of 5G positioning technologies, fostering collaboration and accelerating the advancement of cellular network positioning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05217v1</guid>
      <category>cs.IT</category>
      <category>cs.ET</category>
      <category>cs.PF</category>
      <category>math.IT</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Adeel Malik, Mohsen Ahadi, Florian Kaltenberger, Klaus Warnke, Nguyen Tien Thinh, Nada Bouknana, Cedric Thienot, Godswill Onche, Sagar Arora</dc:creator>
    </item>
    <item>
      <title>Optimizing VarLiNGAM for Scalable and Efficient Time Series Causal Discovery</title>
      <link>https://arxiv.org/abs/2409.05500</link>
      <description>arXiv:2409.05500v1 Announce Type: cross 
Abstract: Causal discovery is designed to identify causal relationships in data, a task that has become increasingly complex due to the computational demands of traditional methods such as VarLiNGAM, which combines Vector Autoregressive Model with Linear Non-Gaussian Acyclic Model for time series data.
  This study is dedicated to optimising causal discovery specifically for time series data, which is common in practical applications. Time series causal discovery is particularly challenging due to the need to account for temporal dependencies and potential time lag effects. By designing a specialised dataset generator and reducing the computational complexity of the VarLiNGAM model from \( O(m^3 \cdot n) \) to \( O(m^3 + m^2 \cdot n) \), this study significantly improves the feasibility of processing large datasets. The proposed methods have been validated on advanced computational platforms and tested across simulated, real-world, and large-scale datasets, showcasing enhanced efficiency and performance. The optimised algorithm achieved 7 to 13 times speedup compared with the original algorithm and around 4.5 times speedup compared with the GPU-accelerated version on large-scale datasets with feature sizes between 200 and 400.
  Our methods aim to push the boundaries of current causal discovery capabilities, making them more robust, scalable, and applicable to real-world scenarios, thus facilitating breakthroughs in various fields such as healthcare and finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05500v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyang Jiao, Ce Guo, Wayne Luk</dc:creator>
    </item>
    <item>
      <title>Mixture of Experts with Mixture of Precisions for Tuning Quality of Service</title>
      <link>https://arxiv.org/abs/2407.14417</link>
      <description>arXiv:2407.14417v2 Announce Type: replace-cross 
Abstract: The increasing demand for deploying large Mixture-of-Experts (MoE) models in resource-constrained environments necessitates efficient approaches to address their high memory and computational requirements challenges. Moreover, given that tasks come in different user-defined constraints and the available resources change over time in multi-tenant environments, it is necessary to design an approach which provides a flexible configuration space. This paper presents an adaptive serving approach for the efficient deployment of MoE models, capitalizing on partial quantization of the experts. By dynamically determining the number of quantized experts and their distribution across CPU and GPU, our approach explores the Pareto frontier and offers a fine-grained range of configurations for tuning throughput and model quality. Our evaluation on an NVIDIA A100 GPU using a Mixtral 8x7B MoE model for three language modelling benchmarks demonstrates that the throughput of token generation can be adjusted from 0.63 to 13.00 token per second. This enhancement comes with a marginal perplexity increase of 3.81 to 4.00, 13.59 to 14.17, and 7.24 to 7.40 for WikiText2, PTB, and C4 datasets respectively under maximum quantization. These results highlight the practical applicability of our approach in dynamic and accuracy-sensitive applications where both memory usage and output quality are important.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14417v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>HamidReza Imani, Abdolah Amirany, Tarek El-Ghazawi</dc:creator>
    </item>
  </channel>
</rss>
