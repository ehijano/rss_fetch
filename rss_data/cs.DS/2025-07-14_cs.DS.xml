<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 15 Jul 2025 02:17:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On the Parallel Complexity of Finding a Matroid Basis</title>
      <link>https://arxiv.org/abs/2507.08194</link>
      <description>arXiv:2507.08194v1 Announce Type: new 
Abstract: A fundamental question in parallel computation, posed by Karp, Upfal, and Wigderson (FOCS 1985, JCSS 1988), asks: \emph{given only independence-oracle access to a matroid on $n$ elements, how many rounds are required to find a basis using only polynomially many queries?} This question generalizes, among others, the complexity of finding bases of linear spaces, partition matroids, and spanning forests in graphs. In their work, they established an upper bound of $O(\sqrt{n})$ rounds and a lower bound of $\widetilde{\Omega}(n^{1/3})$ rounds for this problem, and these bounds have remained unimproved since then.
  In this work, we make the first progress in narrowing this gap by designing a parallel algorithm that finds a basis of an arbitrary matroid in $\tilde{O}(n^{7/15})$ rounds (using polynomially many independence queries per round) with high probability, surpassing the long-standing $O(\sqrt{n})$ barrier. Our approach introduces a novel matroid decomposition technique and other structural insights that not only yield this general result but also lead to a much improved new algorithm for the class of \emph{partition matroids} (which underlies the $\widetilde\Omega(n^{1/3})$ lower bound of Karp, Upfal, and Wigderson). Specifically, we develop an $\tilde{O}(n^{1/3})$-round algorithm, thereby settling the round complexity of finding a basis in partition matroids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08194v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sanjeev Khanna, Aaron Putterman, Junkai Song</dc:creator>
    </item>
    <item>
      <title>Approximation Algorithms for the Cumulative Vehicle Routing Problem with Stochastic Demands</title>
      <link>https://arxiv.org/abs/2507.08316</link>
      <description>arXiv:2507.08316v1 Announce Type: new 
Abstract: In the Cumulative Vehicle Routing Problem (Cu-VRP), we need to find a feasible itinerary for a capacitated vehicle located at the depot to satisfy customers' demand, as in the well-known Vehicle Routing Problem (VRP), but the goal is to minimize the cumulative cost of the vehicle, which is based on the vehicle's load throughout the itinerary. If the demand of each customer is unknown until the vehicle visits it, the problem is called Cu-VRP with Stochastic Demands (Cu-VRPSD). Assume that the approximation ratio of metric TSP is $1.5$. In this paper, we propose a randomized $3.456$-approximation algorithm for Cu-VRPSD, improving the best-known approximation ratio of $6$ (Discret. Appl. Math. 2020). Since VRP with Stochastic Demands (VRPSD) is a special case of Cu-VRPSD, as a corollary, we also obtain a randomized $3.25$-approximation algorithm for VRPSD, improving the best-known approximation ratio of $3.5$ (Oper. Res. 2012). For Cu-VRP, we give a randomized $3.194$-approximation algorithm, improving the best-known approximation ratio of $4$ (Oper. Res. Lett. 2013). Moreover, if each customer is allowed to be satisfied by using multiple tours, we obtain further improvements for Cu-VRPSD and Cu-VRP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08316v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyang Zhao, Mingyu Xiao</dc:creator>
    </item>
    <item>
      <title>H-Planarity and Parametric Extensions: when Modulators Act Globally</title>
      <link>https://arxiv.org/abs/2507.08541</link>
      <description>arXiv:2507.08541v1 Announce Type: new 
Abstract: We introduce a series of graph decompositions based on the modulator/target scheme of modification problems that enable several algorithmic applications that parametrically extend the algorithmic potential of planarity. In the core of our approach is a polynomial time algorithm for computing planar H-modulators. Given a graph class H, a planar H-modulator of a graph G is a set X \subseteq V(G) such that the ``torso'' of X is planar and all connected components of G - X belong to H. Here, the torso of X is obtained from G[X] if, for every connected component of G-X, we form a clique out of its neighborhood on G[X]. We introduce H-Planarity as the problem of deciding whether a graph G has a planar H-modulator. We prove that, if H is hereditary, CMSO-definable, and decidable in polynomial time, then H-Planarity is solvable in polynomial time. Further, we introduce two parametric extensions of H-Planarity by defining the notions of H-planar treedepth and H-planar treewidth, which generalize the concepts of elimination distance and tree decompositions to the class H. Combining this result with existing FPT algorithms for various H-modulator problems, we thereby obtain FPT algorithms parameterized by H-planar treedepth and H-planar treewidth for numerous graph classes H. By combining the well-known algorithmic properties of planar graphs and graphs of bounded treewidth, our methods for computing H-planar treedepth and H-planar treewidth lead to a variety of algorithmic applications. For instance, once we know that a given graph has bounded H-planar treedepth or bounded H-planar treewidth, we can derive additive approximation algorithms for graph coloring and polynomial-time algorithms for counting (weighted) perfect matchings. Furthermore, we design Efficient Polynomial-Time Approximation Schemes (EPTAS-es) for several problems, including Maximum Independent Set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08541v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fedor V. Fomin, Petr A. Golovach, Laure Morelle, Dimitrios M. Thilikos</dc:creator>
    </item>
    <item>
      <title>Beer Path Problems in Temporal Graphs</title>
      <link>https://arxiv.org/abs/2507.08685</link>
      <description>arXiv:2507.08685v1 Announce Type: new 
Abstract: Computing paths in graph structures is a fundamental operation in a wide range of applications, from transportation networks to data analysis. The beer path problem, which captures the option of visiting points of interest, such as gas stations or convenience stops, prior to reaching the final destination, has been recently introduced and extensively studied in static graphs. However, existing approaches do not account for temporal information, which is often crucial in real-world scenarios. For instance, transit services may follow fixed schedules, and shops may only be accessible during certain hours.
  In this work, we introduce the notion of beer paths in temporal graphs, where edges are time-dependent and certain vertices (beer vertices) are active only at specific time instances. We formally define the problems of computing earliest-arrival, latest-departure, fastest, and shortest temporal beer paths and propose efficient algorithms for these problems under both edge stream and adjacency list representations. The time complexity of each of our algorithms is aligned with that of corresponding temporal pathfinding algorithms, thus preserving efficiency.
  Additionally, we present preprocessing techniques that enable efficient query answering under dynamic conditions, for example new openings or closings of shops. We achieve this through appropriate precomputation of selected paths or by transforming a temporal graph into an equivalent static graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08685v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea D'Ascenzo, Giuseppe F. Italiano, Sotiris Kanellopoulos, Anna Mpanti, Aris Pagourtzis, Christos Pergaminelis</dc:creator>
    </item>
    <item>
      <title>On the Constant-Factor Approximability of Minimum Cost Constraint Satisfaction Problems</title>
      <link>https://arxiv.org/abs/2507.08693</link>
      <description>arXiv:2507.08693v1 Announce Type: new 
Abstract: We study minimum cost constraint satisfaction problems (MinCostCSP) through the algebraic lens. We show that for any constraint language $\Gamma$ which has the dual discriminator operation as a polymorphism, there exists a $|D|$-approximation algorithm for MinCostCSP$(\Gamma)$ where $D$ is the domain. Complementing our algorithmic result, we show that any constraint language $\Gamma$ where MinCostCSP$(\Gamma)$ admits a constant-factor approximation must have a \emph{near-unanimity} (NU) polymorphism unless P = NP, extending a similar result by Dalmau et al. on MinCSPs. These results imply a dichotomy of constant-factor approximability for constraint languages that contain all permutation relations (a natural generalization for Boolean CSPs that allow variable negation): either MinCostCSP$(\Gamma)$ has an NU polymorphism and is $|D|$-approximable, or it does not have any NU polymorphism and is NP-hard to approximate within any constant factor. Finally, we present a constraint language which has a majority polymorphism, but is nonetheless NP-hard to approximate within any constant factor assuming the Unique Games Conjecture, showing that the condition of having an NU polymorphism is in general not sufficient unless UGC fails.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08693v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ian DeHaan, Neng Huang, Euiwoong Lee</dc:creator>
    </item>
    <item>
      <title>To buy or not to buy: deterministic rent-or-buy problems on node-weighted graphs</title>
      <link>https://arxiv.org/abs/2507.08698</link>
      <description>arXiv:2507.08698v1 Announce Type: new 
Abstract: We study the rent-or-buy variant of the online Steiner forest problem on node- and edge-weighted graphs. For $n$-node graphs with at most $\bar{n}$ non-zero node-weights, and at most $\tilde{k}$ different arriving terminal pairs, we obtain a deterministic, $O(\log n \log \bar{n})$-competitive algorithm. This improves on the previous best, $O(\log^4 n)$-competitive algorithm obtained by the black-box reduction from (Bartal et al. 2021) combined with the previously best deterministic algorithms for the simpler 'buy-only' setting. We also obtain a deterministic, $O(\bar{n}\log \tilde{k})$-competitive algorithm. This generalizes the $O(\log \tilde{k})$-competitive algorithm for the purely edge-weighted setting from (Umboh 2015). We also obtain a randomized, $O(\log \tilde{k} \log \bar{n})$-competitive algorithm. All previous approaches were based on the randomized, black-box reduction from~\cite{AwerbuchAzarBartal96} that achieves a $O(\log \tilde{k} \log n)$-competitive ratio when combined with an algorithm for the 'buy-only' setting. Our key technical ingredient is a novel charging scheme to an instance of \emph{online prize-collecting set cover}. This allows us to extend the witness-technique of (Umboh 2015) to the node-weighted setting and obtain refined guarantees with respect to $\bar{n}$, already in the much simpler 'buy-only' setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08698v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sander Borst, Moritz Venzin</dc:creator>
    </item>
    <item>
      <title>On Fair Epsilon Net and Geometric Hitting Set</title>
      <link>https://arxiv.org/abs/2507.08758</link>
      <description>arXiv:2507.08758v1 Announce Type: new 
Abstract: Fairness has emerged as a formidable challenge in data-driven decisions. Many of the data problems, such as creating compact data summaries for approximate query processing, can be effectively tackled using concepts from computational geometry, such as $\varepsilon$-nets. However, these powerful tools have yet to be examined from the perspective of fairness. To fill this research gap, we add fairness to classical geometric approximation problems of $\varepsilon$-net, $\varepsilon$-sample, and geometric hitting set. We introduce and address two notions of group fairness: demographic parity, which requires preserving group proportions from the input distribution, and custom-ratios fairness, which demands satisfying arbitrary target ratios. We develop two algorithms to enforce fairness: one based on sampling and another on discrepancy theory. The sampling-based algorithm is faster and computes a fair $\varepsilon$-net of size which is only larger by a $\log(k)$ factor compared to the standard (unfair) $\varepsilon$-net, where $k$ is the number of demographic groups. The discrepancy-based algorithm is slightly slower (for bounded VC dimension), but it computes a smaller fair $\varepsilon$-net. Notably, we reduce the fair geometric hitting set problem to finding fair $\varepsilon$-nets. This results in a $O(\log \mathsf{OPT} \times \log k)$ approximation of a fair geometric hitting set. Additionally, we show that under certain input distributions, constructing fair $\varepsilon$-samples can be infeasible, highlighting limitations in fair sampling. Beyond the theoretical guarantees, our experimental results validate the practical effectiveness of the proposed algorithms. In particular, we achieve zero unfairness with only a modest increase in output size compared to the unfair setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08758v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Dehghankar, Stavros Sintos, Abolfazl Asudeh</dc:creator>
    </item>
    <item>
      <title>Mallows Model with Learned Distance Metrics: Sampling and Maximum Likelihood Estimation</title>
      <link>https://arxiv.org/abs/2507.08108</link>
      <description>arXiv:2507.08108v1 Announce Type: cross 
Abstract: \textit{Mallows model} is a widely-used probabilistic framework for learning from ranking data, with applications ranging from recommendation systems and voting to aligning language models with human preferences~\cite{chen2024mallows, kleinberg2021algorithmic, rafailov2024direct}. Under this model, observed rankings are noisy perturbations of a central ranking $\sigma$, with likelihood decaying exponentially in distance from $\sigma$, i.e, $P (\pi) \propto \exp\big(-\beta \cdot d(\pi, \sigma)\big),$ where $\beta &gt; 0$ controls dispersion and $d$ is a distance function.
  Existing methods mainly focus on fixed distances (such as Kendall's $\tau$ distance), with no principled approach to learning the distance metric directly from data. In practice, however, rankings naturally vary by context; for instance, in some sports we regularly see long-range swaps (a low-rank team beating a high-rank one), while in others such events are rare. Motivated by this, we propose a generalization of Mallows model that learns the distance metric directly from data. Specifically, we focus on $L_\alpha$ distances: $d_\alpha(\pi,\sigma):=\sum_{i=1} |\pi(i)-\sigma(i)|^\alpha$.
  For any $\alpha\geq 1$ and $\beta&gt;0$, we develop a Fully Polynomial-Time Approximation Scheme (FPTAS) to efficiently generate samples that are $\epsilon$- close (in total variation distance) to the true distribution. Even in the special cases of $L_1$ and $L_2$, this generalizes prior results that required vanishing dispersion ($\beta\to0$). Using this sampling algorithm, we propose an efficient Maximum Likelihood Estimation (MLE) algorithm that jointly estimates the central ranking, the dispersion parameter, and the optimal distance metric. We prove strong consistency results for our estimators (for any values of $\alpha$ and $\beta$), and we validate our approach empirically using datasets from sports rankings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08108v1</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yeganeh Alimohammadi, Kiana Asgari</dc:creator>
    </item>
    <item>
      <title>Finding a solution to the Erd\H{o}s-Ginzburg-Ziv theorem in $O(n\log\log\log n)$ time</title>
      <link>https://arxiv.org/abs/2507.08139</link>
      <description>arXiv:2507.08139v1 Announce Type: cross 
Abstract: The Erd\H{o}s-Ginzburg-Ziv theorem states that for any sequence of $2n-1$ integers, there exists a subsequence of $n$ elements whose sum is divisible by $n$. In this article, we provide a simple, practical $O(n\log\log n)$ algorithm and a theoretical $O(n\log\log\log n)$ algorithm, both of which improve upon the best previously known $O(n\log n)$ approach. This shows that a specific variant of boolean convolution can be implemented in time faster than the usual $O(n\log n)$ expected from FFT-based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08139v1</guid>
      <category>math.CO</category>
      <category>cs.DS</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yui Hin Arvin Leung</dc:creator>
    </item>
    <item>
      <title>Fast and Efficient Merge of Sorted Input Lists in Hardware Using List Offset Merge Sorters</title>
      <link>https://arxiv.org/abs/2507.08658</link>
      <description>arXiv:2507.08658v1 Announce Type: cross 
Abstract: A new set of hardware merge sort devices are introduced here, which merge multiple sorted input lists into a single sorted output list in a fast and efficient manner. In each merge sorter, the values from the sorted input lists are arranged in an input 2-D setup array, but with the order of each sorted input list offset from the order of each of the other sorted input lists. In these new devices, called List Offset Merge Sorters (LOMS), a minimal set of column sort stages alternating with row sort stages process the input setup array into a final output array, now in the defined sorted order. LOMS 2-way sorters, which merge 2 sorted input lists, require only 2 merge stages and are significantly faster than Kenneth Batcher's previous state-of-the-art 2-way merge devices, Bitonic Merge Sorters and Odd-Even Merge Sorters. LOMS 2-way sorters utilize the recently-introduced Single-Stage 2-way Merge Sorters (S2MS) in their first stage. Both LOMS and S2MS devices can merge any mixture of input list sizes, while Batcher's merge sorters are difficult to design unless the 2 input lists are equal, and a power-of-2. By themselves, S2MS devices are the fastest 2-way merge sorters when implemented in this study's target FPGA devices, but they tend to use a large number of LUT resources. LOMS 2-way devices use fewer resources than comparable S2MS devices, enabling some large LOMS devices to be implemented in a given FPGA when comparable S2MS devices cannot fit in that FPGA. A List Offset 2-way sorter merges 2 lists, each with 32 values, into a sorted output list of those 64 values in 2.24 nS, a speedup of 2.63 versus a comparable Batcher device. A LOMS 3-way merge sorter, merging 3 sorted input lists with 7 values, fully merges the 21 values in 3.4 nS, a speedup of 1.36 versus the comparable state-of-the-art 3-way merge device.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08658v1</guid>
      <category>cs.AR</category>
      <category>cs.DS</category>
      <category>eess.IV</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Robert B. Kent, Marios S. Pattichis</dc:creator>
    </item>
    <item>
      <title>Sequence graphs realizations and ambiguity in language models</title>
      <link>https://arxiv.org/abs/2402.08830</link>
      <description>arXiv:2402.08830v2 Announce Type: replace 
Abstract: Several popular language models represent local contexts in an input text $x$ as bags of words. Such representations are naturally encoded by a sequence graph whose vertices are the distinct words occurring in $x$, with edges representing the (ordered) co-occurrence of two words within a sliding window of size $w$. However, this compressed representation is not generally bijective: some may be ambiguous, admitting several realizations as a sequence, while others may not admit any realization. In this paper, we study the realizability and ambiguity of sequence graphs from a combinatorial and algorithmic point of view. We consider the existence and enumeration of realizations of a sequence graph under multiple settings: window size $w$, presence/absence of graph orientation, and presence/absence of weights (multiplicities). When $w=2$, we provide polynomial time algorithms for realizability and enumeration in all cases except the undirected/weighted setting, where we show the $\#$P-hardness of enumeration. For $w \ge 3$, we prove the hardness of all variants, even when $w$ is considered as a constant, with the notable exception of the undirected unweighted case for which we propose XP algorithms for both problems, tight due to a corresponding $W[1]-$hardness result. We conclude with an integer program formulation to solve the realizability problem, and a dynamic programming algorithm to solve the enumeration problem in instances of moderate sizes. This work leaves open the membership to NP of both problems, a non-trivial question due to the existence of minimum realizations having size exponential on the instance encoding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08830v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.CL</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sammy Khalife, Yann Ponty, Laurent Bulteau</dc:creator>
    </item>
    <item>
      <title>Optimizing Probabilistic Propagation in Graphs by Adding Edges</title>
      <link>https://arxiv.org/abs/2407.02624</link>
      <description>arXiv:2407.02624v3 Announce Type: replace 
Abstract: Probabilistic graphs are an abstraction that allow us to study randomized propagation in graphs. In a probabilistic graph, each edge is "active" with a certain probability, independent of the other edges. For two vertices $u,v$, a classic quantity of interest, that we refer to as the proximity $\mathcal{P}_{G}(u, v)$, is the probability that there exists a path between $u$ and $v$ all of whose edges are active. For a given subset of vertices $V_s$, the reach of $V_s$ is defined as the minimum over pairs $u \in V_s$ and $v \in V$ of the proximity $\mathcal{P}_{G}(u,v)$. This quantity has been studied in the context of multicast in unreliable communication networks and in social network analysis.
  We study the problem of improving the reach in a probabilistic graph via edge augmentation. Formally, given a budget $k$ of edge additions and a set of source vertices $V_s$, the goal of Reach Improvement is to maximize the reach of $V_s$ by adding at most $k$ new edges to the graph. The problem was introduced in earlier empirical work in the algorithmic fairness community. We provide the first approximation guarantees and hardness results for Reach Improvement.
  We prove that the existence of a good augmentation implies a cluster structure for the graph. We use this structural result to analyze a novel algorithm that outputs a $k$-edge augmentation with an objective value that is poly($\beta^*$), where $\beta^*$ is the objective value for the optimal augmentation. We also give an algorithm that adds $O(k \log n)$ edges and yields a multiplicative approximation to $\beta^*$. Our arguments rely on new probabilistic tools for analyzing proximity, inspired by techniques in percolation theory; these tools may be of broader interest. Finally, we show that significantly better approximations are unlikely, under known hardness assumptions related to gap variants of the classic Set Cover problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02624v3</guid>
      <category>cs.DS</category>
      <category>cs.SI</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Bhaskara, Alex Crane, Shweta Jain, Md Mumtahin Habib Ullah Mazumder, Blair D. Sullivan, Prasanth Yalamanchili</dc:creator>
    </item>
    <item>
      <title>On Deterministically Finding an Element of High Order Modulo a Composite</title>
      <link>https://arxiv.org/abs/2506.07668</link>
      <description>arXiv:2506.07668v2 Announce Type: replace 
Abstract: We give a deterministic algorithm that, given a composite number $N$ and a target order $D \ge N^{1/6}$, runs in time $D^{1/2+o(1)}$ and finds either an element $a \in \mathbb{Z}_N^*$ of multiplicative order at least $D$, or a nontrivial factor of $N$. Our algorithm improves upon an algorithm of Hittmeir (arXiv:1608.08766), who designed a similar algorithm under the stronger assumption $D \ge N^{2/5}$. Hittmeir's algorithm played a crucial role in the recent breakthrough deterministic integer factorization algorithms of Hittmeir and Harvey (arXiv:2006.16729, arXiv:2010.05450, arXiv:2105.11105). When $N$ is assumed to have an $r$-power divisor with $r\ge 2$, our algorithm provides the same guarantees assuming $D \ge N^{1/6r}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07668v2</guid>
      <category>cs.DS</category>
      <category>math.NT</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziv Oznovich, Ben Lee Volk</dc:creator>
    </item>
    <item>
      <title>Compressing Suffix Trees by Path Decompositions</title>
      <link>https://arxiv.org/abs/2506.14734</link>
      <description>arXiv:2506.14734v2 Announce Type: replace 
Abstract: In this paper, we solve the long-standing problem of designing I/O-efficient compressed indexes. Our solution broadly consists of generalizing suffix sorting and revisiting suffix tree path compression. In classic suffix trees, path compression works by replacing unary suffix trie paths with pairs of pointers to $T$, which must be available in the form of some random access oracle at query time. In our approach, instead, we (i) sort the suffix tree's leaves according to a more general priority function $\pi$ (generalizing suffix sorting), (ii) we build a suffix tree path decomposition prioritizing the leftmost paths in such an order, and (iii) we path-compress the decomposition's paths as pointers to a small subset of the string's suffixes. At this point, we show that the colexicographically-sorted array of those pointers represents a new elegant, simple, and remarkably I/O-efficient compressed suffix tree. For instance, by taking $\pi$ to be the lexicographic rank of $T$'s suffixes, we can compress the suffix tree topology in $O(r)$ space on top of a $n\log\sigma + O(\log n)$-bits text representation while essentially matching the pattern matching I/O complexity of Weiner and McCreight's suffix tree. Another (more practical) solution is obtained by taking $\pi$ to be the colexicographic rank of $T$'s prefixes and using a fully-compressed random access oracle. The resulting self-index allows us to locate all occurrences of a given query pattern in less space and orders of magnitude faster than the $r$-index.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14734v2</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruben Becker, Davide Cenzato, Travis Gagie, Sung-Hwan Kim, Ragnar Groot Koerkamp, Giovanni Manzini, Nicola Prezza</dc:creator>
    </item>
    <item>
      <title>Review of Three Variants of the k-d Tree</title>
      <link>https://arxiv.org/abs/2506.20687</link>
      <description>arXiv:2506.20687v4 Announce Type: replace 
Abstract: The original description of the k-d tree recognized that rebalancing techniques, such as used to build an AVL tree or a red-black tree, are not applicable to a k-d tree. Hence, in order to build a balanced k-d tree, it is necessary to find the median of a set of data for each recursive subdivision of that set. The sort or selection used to find the median, and the technique used to partition the set about that median, strongly influence the computational complexity of building a k-d tree. This article describes and contrasts three variants of the k-d tree that differ in their technique used to partition the set, and compares the performance of those variants. In addition, dual-threaded execution is proposed and analyzed for one of the three variants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20687v4</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Russell A. Brown</dc:creator>
    </item>
    <item>
      <title>On the (In)Approximability of the Monitoring Edge Geodetic Set Problem</title>
      <link>https://arxiv.org/abs/2507.00708</link>
      <description>arXiv:2507.00708v2 Announce Type: replace 
Abstract: We study the minimum \emph{Monitoring Edge Geodetic Set} (\megset) problem introduced in [Foucaud et al., CALDAM'23]: given a graph $G$, we say that an edge is monitored by a pair $u,v$ of vertices if \emph{all} shortest paths between $u$ and $v$ traverse $e$; the goal of the problem consists in finding a subset $M$ of vertices of $G$ such that each edge of $G$ is monitored by at least one pair of vertices in $M$, and $|M|$ is minimized.
  In this paper, we prove that all polynomial-time approximation algorithms for the minimum \megset problem must have an approximation ratio of $\Omega(\log n)$, unless \p = \np. To the best of our knowledge, this is the first non-constant inapproximability result known for this problem. We also strengthen the known \np-hardness of the problem on $2$-apex graphs by showing that the same result holds for $1$-apex graphs. This leaves open the problem of determining whether the problem remains \np-hard on planar (i.e., $0$-apex) graphs.
  On the positive side, we design an algorithm that computes good approximate solutions for hereditary graph classes that admit efficiently computable balanced separators of truly sublinear size. This immediately results in polynomial-time approximation algorithms achieving an approximation ratio of $O(n^{\frac{1}{4}} \sqrt{\log n})$ on planar graphs, graphs with bounded genus, and $k$-apex graphs with $k=O(n^{\frac{1}{4}})$. On graphs with bounded treewidth, we obtain an approximation ratio of $O(\log^{3/2} n)$ for any constant $\varepsilon &gt; 0$. This compares favorably with the best-known approximation algorithm for general graphs, which achieves an approximation ratio of $O(\sqrt{n \log n})$ via a simple reduction to the \textsc{Set Cover} problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00708v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Davide Bil\`o, Giordano Colli, Luca Forlizzi, Stefano Leucci</dc:creator>
    </item>
    <item>
      <title>Finding One Local Optimum Is Easy -- But What about Two?</title>
      <link>https://arxiv.org/abs/2507.07524</link>
      <description>arXiv:2507.07524v2 Announce Type: replace 
Abstract: The class PLS (Polynomial Local Search) captures the complexity of finding a solution that is locally optimal and has proven to be an important concept in the theory of local search. It has been shown that local search versions of various combinatorial optimization problems, such as Maximum Independent Set and Max Cut, are complete for this class. Such computational intractability typically arises in local search problems allowing arbitrary weights; in contrast, for unweighted problems, locally optimal solutions can be found in polynomial time under standard settings. In this paper, we pursue the complexity of local search problems from a different angle: We show that computing two locally optimal solutions is NP-hard for various natural unweighted local search problems, including Maximum Independent Set, Minimum Dominating Set, Max SAT, and Max Cut. We also discuss several tractable cases for finding two (or more) local optimal solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07524v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yasuaki Kobayashi, Kazuhiro Kurita, Yutaro Yamaguchi</dc:creator>
    </item>
    <item>
      <title>Black-Box Identity Testing of Noncommutative Rational Formulas in Deterministic Quasipolynomial Time</title>
      <link>https://arxiv.org/abs/2309.15647</link>
      <description>arXiv:2309.15647v4 Announce Type: replace-cross 
Abstract: Rational Identity Testing (RIT) is the decision problem of determining whether or not a noncommutative rational formula computes zero in the free skew field. It admits a deterministic polynomial-time white-box algorithm [Garg, Gurvits, Oliveira, and Wigderson (2016); Ivanyos, Qiao, Subrahmanyam (2018); Hamada and Hirai (2021)], and a randomized polynomial-time algorithm [Derksen and Makam (2017)] in the black-box setting, via singularity testing of linear matrices over the free skew field. Indeed, a randomized NC algorithm for RIT in the white-box setting follows from the result of Derksen and Makam (2017).
  Designing an efficient deterministic black-box algorithm for RIT and understanding the parallel complexity of RIT are major open problems in this area. Despite being open since the work of Garg, Gurvits, Oliveira, and Wigderson (2016), these questions have seen limited progress. In fact, the only known result in this direction is the construction of a quasipolynomial-size hitting set for rational formulas of only inversion height two [Arvind, Chatterjee, Mukhopadhyay (2022)].
  In this paper, we significantly improve the black-box complexity of this problem and obtain the first quasipolynomial-size hitting set for all rational formulas of polynomial size. Our construction also yields the first deterministic quasi-NC upper bound for RIT in the white-box setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.15647v4</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>V. Arvind, Abhranil Chatterjee, Partha Mukhopadhyay</dc:creator>
    </item>
    <item>
      <title>Algorithmic contiguity from low-degree conjecture and applications in correlated random graphs</title>
      <link>https://arxiv.org/abs/2502.09832</link>
      <description>arXiv:2502.09832v3 Announce Type: replace-cross 
Abstract: In this paper, assuming a natural strengthening of the low-degree conjecture, we provide evidence of computational hardness for two problems: (1) the (partial) matching recovery problem in the sparse correlated Erd\H{o}s-R\'enyi graphs $\mathcal G(n,q;\rho)$ when the edge-density $q=n^{-1+o(1)}$ and the correlation $\rho&lt;\sqrt{\alpha}$ lies below the Otter's threshold, solving a remaining problem in \cite{DDL23+}; (2) the detection problem between the correlated sparse stochastic block model $\mathcal S(n,\tfrac{\lambda}{n};k,\epsilon;s)$ and a pair of independent stochastic block models $\mathcal S(n,\tfrac{\lambda s}{n};k,\epsilon)$ when $\epsilon^2 \lambda s&lt;1$ lies below the Kesten-Stigum (KS) threshold and $s&lt;\sqrt{\alpha}$ lies below the Otter's threshold, solving a remaining problem in \cite{CDGL24+}.
  One of the main ingredient in our proof is to derive certain forms of \emph{algorithmic contiguity} between two probability measures based on bounds on their low-degree advantage. To be more precise, consider the high-dimensional hypothesis testing problem between two probability measures $\mathbb{P}$ and $\mathbb{Q}$ based on the sample $\mathsf Y$. We show that if the low-degree advantage $\mathsf{Adv}_{\leq D} \big( \frac{\mathrm{d}\mathbb{P}}{\mathrm{d}\mathbb{Q}} \big)=O(1)$, then (assuming the low-degree conjecture) there is no efficient algorithm $\mathcal A$ such that $\mathbb{Q}(\mathcal A(\mathsf Y)=0)=1-o(1)$ and $\mathbb{P}(\mathcal A(\mathsf Y)=1)=\Omega(1)$. This framework provides a useful tool for performing reductions between different inference tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09832v3</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhangsong Li</dc:creator>
    </item>
    <item>
      <title>Sharp Phase Transitions in Estimation with Low-Degree Polynomials</title>
      <link>https://arxiv.org/abs/2502.14407</link>
      <description>arXiv:2502.14407v2 Announce Type: replace-cross 
Abstract: High-dimensional planted problems, such as finding a hidden dense subgraph within a random graph, often exhibit a gap between statistical and computational feasibility. While recovering the hidden structure may be statistically possible, it is conjectured to be computationally intractable in certain parameter regimes. A powerful approach to understanding this hardness involves proving lower bounds on the efficacy of low-degree polynomial algorithms. We introduce new techniques for establishing such lower bounds, leading to novel results across diverse settings: planted submatrix, planted dense subgraph, the spiked Wigner model, and the stochastic block model. Notably, our results address the estimation task -- whereas most prior work is limited to hypothesis testing -- and capture sharp phase transitions such as the "BBP" transition in the spiked Wigner model (named for Baik, Ben Arous, and P\'{e}ch\'{e}) and the Kesten-Stigum threshold in the stochastic block model. Existing work on estimation either falls short of achieving these sharp thresholds or is limited to polynomials of very low (constant or logarithmic) degree. In contrast, our results rule out estimation with polynomials of degree $n^{\delta}$ where $n$ is the dimension and $\delta &gt; 0$ is a constant, and in some cases we pin down the optimal constant $\delta$. Our work resolves open problems posed by Hopkins &amp; Steurer (2017) and Schramm &amp; Wein (2022), and provides rigorous support within the low-degree framework for conjectures by Abbe &amp; Sandon (2018) and Lelarge &amp; Miolane (2019).</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14407v2</guid>
      <category>math.ST</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Youngtak Sohn, Alexander S. Wein</dc:creator>
    </item>
    <item>
      <title>Discovering Algorithms with Computational Language Processing</title>
      <link>https://arxiv.org/abs/2507.03190</link>
      <description>arXiv:2507.03190v2 Announce Type: replace-cross 
Abstract: Algorithms are the engine for reproducible problem-solving. We present a framework automating algorithm discovery by conceptualizing them as sequences of operations, represented as tokens. These computational tokens are chained using a grammar, enabling the formation of increasingly sophisticated procedures. Our ensemble Monte Carlo tree search (MCTS) guided by reinforcement learning (RL) explores token chaining and drives the creation of new tokens. This methodology rediscovers, improves, and generates new algorithms that substantially outperform existing methods for strongly NP-hard combinatorial optimization problems and foundational quantum computing approaches such as Grover's and Quantum Approximate Optimization Algorithm. Operating at the computational rather than code-generation level, our framework produces algorithms that can be tailored specifically to problem instances, not merely classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03190v2</guid>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Theo Bourdais, Abeynaya Gnanasekaran, Houman Owhadi, Tuhin Sahai</dc:creator>
    </item>
  </channel>
</rss>
