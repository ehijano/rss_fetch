<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 30 Jun 2025 04:00:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A new algorithm for sampling parameters in a structured correlation matrix with application to estimating optimal combinations of muscles to quantify progression in Duchenne muscular dystrophy</title>
      <link>https://arxiv.org/abs/2506.21719</link>
      <description>arXiv:2506.21719v1 Announce Type: new 
Abstract: The goal of this paper is to estimate an optimal combination of biomarkers for individuals with Duchenne muscular dystrophy (DMD), which provides the most sensitive combinations of biomarkers to assess disease progression (in this case, optimal with respect to standardized response mean (SRM) for 4 muscle biomarkers). The biomarker data is an incomplete (missing and irregular) multivariate longitudinal data. We propose a normal model with structured covariance designed for our setting. To sample from the posterior distribution of parameters, we develop a Markov Chain Monte Carlo (MCMC) algorithm to address the positive definiteness constraint on the structured correlation matrix. In particular, we propose a novel approach to compute the support of the parameters in the structured correlation matrix; we modify the approach from \cite{Barnard} on the set of largest possible submatrices of the correlation matrix, where the correlation parameter is a unique element. For each posterior sample, we compute the optimal weights of our construct. We conduct data analysis and simulation studies to evaluate the algorithm and the frequentist properties of the posteriors of correlations and weights. We found that the lower extremities are the most responsive muscles at the early and late ambulatory disease stages and the biceps brachii is the most responsive at the non-ambulatory disease stage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21719v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael K. Kim, Michael J. Daniels, William D. Rooney, Rebecca J. Willcocks, Glenn A. Walter, Krista H. Vandenborne</dc:creator>
    </item>
    <item>
      <title>Efficient Estimation of Causal Effects Under Two-Phase Sampling with Error-Prone Outcome and Treatment Measurements</title>
      <link>https://arxiv.org/abs/2506.21777</link>
      <description>arXiv:2506.21777v1 Announce Type: new 
Abstract: Measurement error is a common challenge for causal inference studies using electronic health record (EHR) data, where clinical outcomes and treatments are frequently mismeasured. Researchers often address measurement error by conducting manual chart reviews to validate measurements in a subset of the full EHR data -- a form of two-phase sampling. To improve efficiency, phase-two samples are often collected in a biased manner dependent on the patients' initial, error-prone measurements. In this work, motivated by our aim of performing causal inference with error-prone outcome and treatment measurements under two-phase sampling, we develop solutions applicable to both this specific problem and the broader problem of causal inference with two-phase samples. For our specific measurement error problem, we construct two asymptotically equivalent doubly-robust estimators of the average treatment effect and demonstrate how these estimators arise from two previously disconnected approaches to constructing efficient estimators in general two-phase sampling settings. We document various sources of instability affecting estimators from each approach and propose modifications that can considerably improve finite sample performance in any two-phase sampling context. We demonstrate the utility of our proposed methods through simulation studies and an illustrative example assessing effects of antiretroviral therapy on occurrence of AIDS-defining events in patients with HIV from the Vanderbilt Comprehensive Care Clinic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21777v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keith Barnatchez, Kevin P. Josey, Nima S. Hejazi, Bryan E. Shepherd, Giovanni Parmigiani, Rachel Nethery</dc:creator>
    </item>
    <item>
      <title>Estimating Average Causal Effects with Incomplete Exposure and Confounders</title>
      <link>https://arxiv.org/abs/2506.21786</link>
      <description>arXiv:2506.21786v1 Announce Type: new 
Abstract: Standard methods for estimating average causal effects require complete observations of the exposure and confounders. In observational studies, however, missing data are ubiquitous. Motivated by a study on the effect of prescription opioids on mortality, we propose methods for estimating average causal effects when exposures and potential confounders may be missing. We consider missingness at random and additionally propose several specific missing not at random (MNAR) assumptions. Under our proposed MNAR assumptions, we show that the average causal effects are identified from the observed data and derive corresponding influence functions in a nonparametric model, which form the basis of our proposed estimators. Our simulations show that standard multiple imputation techniques paired with a complete data estimator is unbiased when data are missing at random (MAR) but can be biased otherwise. For each of the MNAR assumptions, we instead propose doubly robust targeted maximum likelihood estimators (TMLE), allowing misspecification of either (i) the outcome models or (ii) the exposure and missingness models. The proposed methods are suitable for any outcome types, and we apply them to a motivating study that examines the effect of prescription opioid usage on all-cause mortality using data from the National Health and Nutrition Examination Survey (NHANES).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21786v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lan Wen, Glen McGee</dc:creator>
    </item>
    <item>
      <title>Change Point Localization and Inference in Dynamic Multilayer Networks</title>
      <link>https://arxiv.org/abs/2506.21878</link>
      <description>arXiv:2506.21878v1 Announce Type: new 
Abstract: We study offline change point localization and inference in dynamic multilayer random dot product graphs (D-MRDPGs), where at each time point, a multilayer network is observed with shared node latent positions and time-varying, layer-specific connectivity patterns. We propose a novel two-stage algorithm that combines seeded binary segmentation with low-rank tensor estimation, and establish its consistency in estimating both the number and locations of change points. Furthermore, we derive the limiting distributions of the refined estimators under both vanishing and non-vanishing jump regimes. To the best of our knowledge, this is the first result of its kind in the context of dynamic network data. We also develop a fully data-driven procedure for constructing confidence intervals. Extensive numerical experiments demonstrate the superior performance and practical utility of our methods compared to existing alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21878v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fan Wang, Kyle Ritscher, Yik Lun Kei, Xin Ma, Oscar Hernan Madrid Padilla</dc:creator>
    </item>
    <item>
      <title>Universal Modelling of Autocovariance Functions via Spline Kernels</title>
      <link>https://arxiv.org/abs/2506.21953</link>
      <description>arXiv:2506.21953v1 Announce Type: new 
Abstract: Flexible modelling of the autocovariance function (ACF) is central to time-series, spatial, and spatio-temporal analysis. Modern applications often demand flexibility beyond classical parametric models, motivating non-parametric descriptions of the ACF. Bochner's Theorem guarantees that any positive spectral measure yields a valid ACF via the inverse Fourier transform; however, existing non-parametric approaches in the spectral domain rarely return closed-form expressions for the ACF itself. We develop a flexible, closed-form class of non-parametric ACFs by deriving the inverse Fourier transform of B-spline spectral bases with arbitrary degree and knot placement. This yields a general class of ACF with three key features: (i) it is provably dense, under an $L^1$ metric, in the space of weakly stationary, mean-square continuous ACFs with mild regularity conditions; (ii) it accommodates univariate, multivariate, and multidimensional processes; and (iii) it naturally supports non-separable structure without requiring explicit imposition. Jackson-type approximation bounds establish convergence rates, and empirical results on simulated and real-world data demonstrate accurate process recovery. The method provides a practical and theoretically grounded approach for constructing a non-parametric class of ACF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21953v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lachlan Astfalck</dc:creator>
    </item>
    <item>
      <title>Using Large Language Models to Suggest Informative Prior Distributions in Bayesian Statistics</title>
      <link>https://arxiv.org/abs/2506.21964</link>
      <description>arXiv:2506.21964v1 Announce Type: new 
Abstract: Selecting prior distributions in Bayesian statistics is challenging, resource-intensive, and subjective. We analyze using large-language models (LLMs) to suggest suitable, knowledge-based informative priors. We developed an extensive prompt asking LLMs not only to suggest priors but also to verify and reflect on their choices.
  We evaluated Claude Opus, Gemini 2.5 Pro, and ChatGPT-4o-mini on two real datasets: heart disease risk and concrete strength. All LLMs correctly identified the direction for all associations (e.g., that heart disease risk is higher for males). The quality of suggested priors was measured by their Kullback-Leibler divergence from the maximum likelihood estimator's distribution.
  The LLMs suggested both moderately and weakly informative priors. The moderate priors were often overconfident, resulting in distributions misaligned with the data. In our experiments, Claude and Gemini provided better priors than ChatGPT. For weakly informative priors, a key performance difference emerged: ChatGPT and Gemini defaulted to an "unnecessarily vague" mean of 0, while Claude did not, demonstrating a significant advantage.
  The ability of LLMs to identify correct associations shows their great potential as an efficient, objective method for developing informative priors. However, the primary challenge remains in calibrating the width of these priors to avoid over- and under-confidence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21964v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael A. Riegler, Kristoffer Herland Hellton, Vajira Thambawita, Hugo L. Hammer</dc:creator>
    </item>
    <item>
      <title>Simulated Intervention on Cross-Sectional Nested Data: Development of a Multilevel NIRA Approach</title>
      <link>https://arxiv.org/abs/2506.21991</link>
      <description>arXiv:2506.21991v1 Announce Type: new 
Abstract: With the rise of the network perspective, researchers have made numerous important discoveries over the past decade by constructing psychological networks. Unfortunately, most of these networks are based on cross-sectional data, which can only reveal associations between variables but not their directional or causal relationships. Recently, the development of the nodeIdentifyR algorithm (NIRA) technique has provided a promising method for simulating causal processes based on cross-sectional network structures. However, this algorithm is not capable of handling cross-sectional nested data, which greatly limits its applicability. In response to this limitation, the present study proposes a multilevel extension of the NIRA algorithm, referred to as multilevel NIRA. We provide a detailed explanation of the algorithm's core principles and modeling procedures. Finally, we discuss the potential applications and practical implications of this approach, as well as its limitations and directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21991v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiming Wu, Fei Wang</dc:creator>
    </item>
    <item>
      <title>Stop Lying to Me: New Visual Tools to Choose the Most Honest Nonlinear Dimension Reduction</title>
      <link>https://arxiv.org/abs/2506.22051</link>
      <description>arXiv:2506.22051v1 Announce Type: new 
Abstract: Nonlinear dimension reduction (NLDR) techniques such as tSNE, and UMAP provide a low-dimensional representation of high-dimensional data (\pD{}) by applying a nonlinear transformation. NLDR often exaggerates random patterns. But NLDR views have an important role in data analysis because, if done well, they provide a concise visual (and conceptual) summary of \pD{} distributions. The NLDR methods and hyper-parameter choices can create wildly different representations, making it difficult to decide which is best, or whether any or all are accurate or misleading. To help assess the NLDR and decide on which, if any, is the most reasonable representation of the structure(s) present in the \pD{} data, we have developed an algorithm to show the \gD{} NLDR model in the \pD{} space, viewed with a tour, a movie of linear projections. From this, one can see if the model fits everywhere, or better in some subspaces, or completely mismatches the data. Also, we can see how different methods may have similar summaries or quirks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22051v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jayani P. Gamage, Dianne Cook, Paul Harrison, Michael Lydeamore</dc:creator>
    </item>
    <item>
      <title>Brownian motion, bridges and Bayesian inference in phylogenetic tree space</title>
      <link>https://arxiv.org/abs/2506.22135</link>
      <description>arXiv:2506.22135v1 Announce Type: new 
Abstract: Billera-Holmes-Vogtmann (BHV) tree space is a geodesic metric space of edge-weighted phylogenetic trees with a fixed leaf set. Constructing parametric distributions on this space is challenging due to its non-Euclidean geometry and the intractability of normalizing constants. We address this by fitting Brownian motion transition kernels to tree-valued data via a non-Euclidean bridge construction. Each kernel is determined by a source tree $x_0$ (the Brownian motion's starting point) and a dispersion parameter $t_0$ (its duration). Observed trees are modelled as independent draws from the transition kernel defined by $(x_0, t_0)$, analogous to a Gaussian model in Euclidean space. Brownian motion is approximated by an $m$-step random walk, with the parameter space augmented to include full sample paths. We develop a bridge algorithm to sample paths conditional on their endpoints, and introduce methods for sampling a Bayesian posterior for $(x_0, t_0)$ and for marginal likelihood evaluation. This enables hypothesis testing for alternative source trees. The approach is validated on simulated data and applied to an experimental data set of yeast gene trees. These methods provide a foundation for future development of a wider class of probabilistic models of tree-valued data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22135v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William M. Woodman, Tom M. W. Nye</dc:creator>
    </item>
    <item>
      <title>Bayesian non-parametric lumping and splitting of nodes in Network Meta-Analysis under heterogeneity</title>
      <link>https://arxiv.org/abs/2506.22154</link>
      <description>arXiv:2506.22154v1 Announce Type: new 
Abstract: Network meta-analysis (NMA) synthesizes evidence for multiple treatments, but decisions on node formation can have important statistical implications including bias or inflated uncertainty. Existing data-driven methods often lack flexibility or fail to fully account for node uncertainty and adjust for between-trial heterogeneity simultaneously. We introduce a Bayesian non-parametric framework using a Dirichlet process prior with a regularized horseshoe base measure. This data-driven approach allows treatments to cluster based on their effects while formally propagating uncertainty about the clustering structure itself. We extend this method to incorporate baseline risk meta-regression, enabling clustering even under heterogeneity, and demonstrate implementation using standard MCMC software. We apply the method to case studies in rheumatology and pain and find adjusting for baseline risk heterogeneity can substantially change which treatments are clustered together, highlighting the importance of methods to allow for meta-regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22154v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timothy Disher, Chris Cameron, Brian Hutton</dc:creator>
    </item>
    <item>
      <title>Bias in estimating Theil, Atkinson, and dispersion indices for gamma mixture populations</title>
      <link>https://arxiv.org/abs/2506.22168</link>
      <description>arXiv:2506.22168v1 Announce Type: new 
Abstract: In this paper, we derive closed-form expressions for the bias of estimators of the Theil, Atkinson, and dispersion indices when the underlying population follows a finite mixture of gamma distributions. Our methodology builds on probabilistic techniques grounded in Mosimann's proportion-sum independence theorem and the gamma-Dirichlet connection, enabling analytical tractability in the presence of population heterogeneity. These results extend existing findings for single gamma models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22168v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jackson Assis, Roberto Vila, Helton Saulo</dc:creator>
    </item>
    <item>
      <title>An Efficient Class of Bayesian Generalized Quadratic Nonlinear Dynamic Models with Application to Birth Rate Monitoring</title>
      <link>https://arxiv.org/abs/2506.22188</link>
      <description>arXiv:2506.22188v1 Announce Type: new 
Abstract: Many real-world spatio-temporal processes exhibit nonlinear dynamics that can often be described through stochastic partial differential equations. These models are flexible and scientifically motivated, however, implementing them in a fully Bayesian framework can be computationally challenging. We are motivated by birth rate data, which has important implications for public health and are known to follow nonlinear dynamics. We propose a covariance calibration strategy that specifies the covariance matrix of a linear mixed effects model to be close in Frobenius norm to that of a Generalized Quadratic Nonlinearity (GQN) model. We refer to this as Frobenius norm matching. This allows us to model nonlinear dynamics using an easier to implement linear framework. The calibrated linear model is efficiently implemented using Exact Posterior Regression (EPR), a recently proposed Bayesian model that enables sampling of fixed and random effects directly from the posterior distribution. We provide simulation studies that compare to implementations using MCMC. Finally, we use this approach to analyze Florida county-level birth rate data from 1990-2023. Our results indicate that our non-linear spatio-temporal model outperforms linear dynamic spatio-temporal models for this data, and identifies covariate effects consistent with existing literature, all while avoiding the computational difficulties of MCMC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22188v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Madelyn Clinch, Jonathan R. Bradley</dc:creator>
    </item>
    <item>
      <title>Manifold-Constrained Gaussian Processes for Inference of Mixed-effects Ordinary Differential Equations with Application to Pharmacokinetics</title>
      <link>https://arxiv.org/abs/2506.22313</link>
      <description>arXiv:2506.22313v1 Announce Type: new 
Abstract: Pharmacokinetic modeling using ordinary differential equations (ODEs) has an important role in dose optimization studies, where dosing must balance sustained therapeutic efficacy with the risk of adverse side effects. Such ODE models characterize drug plasma concentration over time and allow pharmacokinetic parameters to be inferred, such as drug absorption and elimination rates. For time-course studies involving treatment groups with multiple subjects, mixed-effects ODE models are commonly used. However, existing methods tend to lack uncertainty quantification on a subject-level, for key measures such as peak or trough concentration and for making predictions of drug concentration. To address such limitations, we propose an extension of manifold-constrained Gaussian processes for inference of general mixed-effects ODE models within a Bayesian statistical framework. We evaluate our method on simulated examples, demonstrating its ability to provide fast and accurate inference for parameters and trajectories using nested optimization. To illustrate the practical efficacy of the proposed method, we provide a real data analysis of a pharmacokinetic model used for an HIV combination therapy study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22313v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxuan Zhao, Samuel W. K. Wong</dc:creator>
    </item>
    <item>
      <title>General measures of effect size to calculate power and sample size for Wald tests with generalized linear models</title>
      <link>https://arxiv.org/abs/2506.22324</link>
      <description>arXiv:2506.22324v1 Announce Type: new 
Abstract: Power and sample size calculations for Wald tests in generalized linear models (GLMs) are often limited to specific cases like logistic regression. More general methods typically require detailed study parameters that are difficult to obtain during planning. We introduce two new effect size measures for estimating power, sample size, or the minimally detectable effect size in studies using Wald tests across any GLM. These measures accommodate any number of predictors or adjusters and require only basic study information. We provide practical guidance for interpreting and applying these measures to approximate a key parameter in power calculations. We also derive asymptotic bounds on the relative error of these approximations, showing that accuracy depends on features of the GLM such as the nonlinearity of the link function. To complement this analysis, we conduct simulation studies across common model specifications, identifying best use cases and opportunities for improvement. Finally, we test the methods in finite samples to confirm their practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22324v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amy L Cochran, Shijie Yuan, Paul J Rathouz</dc:creator>
    </item>
    <item>
      <title>Measuring frailty in the elderly: an indicator based on a super-classifier</title>
      <link>https://arxiv.org/abs/2506.22349</link>
      <description>arXiv:2506.22349v1 Announce Type: new 
Abstract: Identifying frail older adults in an ageing population has become crucial to improve the services offered by a healthcare system. This work aims to develop a composite indicator to assess the level of frailty of individuals using administrative health data. Since frailty has a complex, multidimensional nature, a multi-outcome approach was used. After an extensive literature research, some health adverse events were identified to represent frailty. These adverse events were modelled by logistic classifiers, with frailty determinants (previously selected with a gradient tree boosting) used as covariates. The sensitivity and specificity of individual classifiers are exploited to rewrite their combined likelihood. From this combined likelihood, we are able to obtain an indicator capable of measuring the frailty of the population. The indicator demonstrates strong performance across all outcomes for various years. The main innovation brought by this indicator lies in the possibility to use diverse subgroups of frailty determinants specific to each outcome, without imposing constraints on their structural form. In conclusion, this indicator has proven to be an efficient tool for quantifying frailty of elderly individuals, providing potential help to health authorities in preventing frailty-related adverse events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22349v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sara Rebottini, Pietro Belloni</dc:creator>
    </item>
    <item>
      <title>A General Test for Independent and Identically Distributed Hypothesis</title>
      <link>https://arxiv.org/abs/2506.22361</link>
      <description>arXiv:2506.22361v1 Announce Type: new 
Abstract: We propose a simple and intuitive test for arguably the most prevailing hypothesis in statistics that data are independent and identically distributed (IID), based on a newly introduced off-diagonal sequential U-process. This IID test is fully nonparametric and applicable to random objects in general spaces, while requiring no specific alternatives such as structural breaks or serial dependence, which allows for detecting general types of violations of the IID assumption. An easy-to-implement jackknife multiplier bootstrap is tailored to produce critical values of the test. Under mild conditions, we establish Gaussian approximation for the proposed U-processes, and derive non-asymptotic coupling and Kolmogorov distance bounds for its maximum and the bootstrapped version, providing rigorous theoretical guarantees. Simulations and real data applications are conducted to demonstrate the usefulness and versatility compared with existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22361v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tongyu Li, Jonas Mueller, Fang Yao</dc:creator>
    </item>
    <item>
      <title>Over the Stability Space of a Multivariate Time Series</title>
      <link>https://arxiv.org/abs/2506.22407</link>
      <description>arXiv:2506.22407v1 Announce Type: new 
Abstract: This paper jointly addresses the challenges of non-stationarity and high dimensionality in analysing multivariate time series. Building on the classical concept of cointegration, we introduce a more flexible notion, called stability space, aimed at capturing stationary components in settings where traditional assumptions may not hold. We examine the parametric Johansen procedure alongside two non-parametric alternatives based on dimensionality reduction techniques: Partial Least Squares and Principal Component Analysis. Additionally, we propose a targeted selection of components that prioritises stationarity. Through simulations and real-data applications, we evaluated the performance of these methodologies across various scenarios, including high-dimensional configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22407v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto V\'asquez-Mart\'inez, Graciela Gonz\'alez-Far\'ias, Jos\'e Ulises M\'arquez Urbina, Francisco Corona</dc:creator>
    </item>
    <item>
      <title>SERP Interference Network and Its Applications in Search Advertising</title>
      <link>https://arxiv.org/abs/2506.21598</link>
      <description>arXiv:2506.21598v1 Announce Type: cross 
Abstract: Search Engine marketing teams in the e-commerce industry manage global search engine traffic to their websites with the aim to optimize long-term profitability by delivering the best possible customer experience on Search Engine Results Pages (SERPs). In order to do so, they need to run continuous and rapid Search Marketing A/B tests to continuously evolve and improve their products. However, unlike typical e-commerce A/B tests that can randomize based on customer identification, their tests face the challenge of anonymized users on search engines. On the other hand, simply randomizing on products violates Stable Unit Treatment Value Assumption for most treatments of interest. In this work, we propose leveraging censored observational data to construct bipartite (Search Query to Product Ad or Text Ad) SERP interference networks. Using a novel weighting function, we create weighted projections to form unipartite graphs which can then be use to create clusters to randomized on. We demonstrate this experimental design's application in evaluating a new bidding algorithm for Paid Search. Additionally, we provide a blueprint of a novel system architecture utilizing SageMaker which enables polyglot programming to implement each component of the experimental framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21598v1</guid>
      <category>cs.IR</category>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Purak Jain, Sandeep Appala</dc:creator>
    </item>
    <item>
      <title>Dynamic Bayesian Item Response Model with Decomposition (D-BIRD): Modeling Cohort and Individual Learning Over Time</title>
      <link>https://arxiv.org/abs/2506.21723</link>
      <description>arXiv:2506.21723v1 Announce Type: cross 
Abstract: We present D-BIRD, a Bayesian dynamic item response model for estimating student ability from sparse, longitudinal assessments. By decomposing ability into a cohort trend and individual trajectory, D-BIRD supports interpretable modeling of learning over time. We evaluate parameter recovery in simulation and demonstrate the model using real-world personalized learning data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21723v1</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hansol Lee, Jason B. Cho, David S. Matteson, Benjamin W. Domingue</dc:creator>
    </item>
    <item>
      <title>Monte Carlo and quasi-Monte Carlo integration for likelihood functions</title>
      <link>https://arxiv.org/abs/2506.21733</link>
      <description>arXiv:2506.21733v1 Announce Type: cross 
Abstract: We compare the integration error of Monte Carlo (MC) and quasi-Monte Carlo (QMC) methods for approximating the normalizing constant of posterior distributions and certain marginal likelihoods. In doing so, we characterize the dependency of the relative and absolute integration errors on the number of data points ($n$), the number of grid points ($m$) and the dimension of the integral ($p$). We find that if the dimension of the integral remains fixed as $n$ and $m$ tend to infinity, the scaling rate of the relative error of MC integration includes an additional $n^{1/2}\log(n)^{p/2}$ data-dependent factor, while for QMC this factor is $\log(n)^{p/2}$. In this scenario, QMC will outperform MC if $\log(m)^{p - 1/2}/\sqrt{mn\log(n)} &lt; 1$, which differs from the usual result that QMC will outperform MC if $\log(m)^p/m^{1/2} &lt; 1$.The accuracies of MC and QMC methods are also examined in the high-dimensional setting as $p \rightarrow \infty$, where MC gives more optimistic results as the scaling in dimension is slower than that of QMC when the Halton sequence is used to construct the low discrepancy grid; however both methods display poor dimensional scaling as expected. An additional contribution of this work is a bound on the high-dimensional scaling of the star discrepancy for the Halton sequence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21733v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanbo Tang</dc:creator>
    </item>
    <item>
      <title>Less Greedy Equivalence Search</title>
      <link>https://arxiv.org/abs/2506.22331</link>
      <description>arXiv:2506.22331v1 Announce Type: cross 
Abstract: Greedy Equivalence Search (GES) is a classic score-based algorithm for causal discovery from observational data. In the sample limit, it recovers the Markov equivalence class of graphs that describe the data. Still, it faces two challenges in practice: computational cost and finite-sample accuracy. In this paper, we develop Less Greedy Equivalence Search (LGES), a variant of GES that retains its theoretical guarantees while partially addressing these limitations. LGES modifies the greedy step: rather than always applying the highest-scoring insertion, it avoids edge insertions between variables for which the score implies some conditional independence. This more targeted search yields up to a \(10\)-fold speed-up and a substantial reduction in structural error relative to GES. Moreover, LGES can guide the search using prior assumptions, while correcting these assumptions when contradicted by the data. Finally, LGES can exploit interventional data to refine the learned observational equivalence class. We prove that LGES recovers the true equivalence class in the sample limit from observational and interventional data, even with misspecified prior assumptions. Experiments demonstrate that LGES outperforms GES and other baselines in speed, accuracy, and robustness to misspecified assumptions. Our code is available at https://github.com/CausalAILab/lges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22331v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adiba Ejaz, Elias Bareinboim</dc:creator>
    </item>
    <item>
      <title>Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts</title>
      <link>https://arxiv.org/abs/2506.22343</link>
      <description>arXiv:2506.22343v1 Announce Type: cross 
Abstract: Text watermarks in large language models (LLMs) are an increasingly important tool for detecting synthetic text and distinguishing human-written content from LLM-generated text. While most existing studies focus on determining whether entire texts are watermarked, many real-world scenarios involve mixed-source texts, which blend human-written and watermarked content. In this paper, we address the problem of optimally estimating the watermark proportion in mixed-source texts. We cast this problem as estimating the proportion parameter in a mixture model based on \emph{pivotal statistics}. First, we show that this parameter is not even identifiable in certain watermarking schemes, let alone consistently estimable. In stark contrast, for watermarking methods that employ continuous pivotal statistics for detection, we demonstrate that the proportion parameter is identifiable under mild conditions. We propose efficient estimators for this class of methods, which include several popular unbiased watermarks as examples, and derive minimax lower bounds for any measurable estimator based on pivotal statistics, showing that our estimators achieve these lower bounds. Through evaluations on both synthetic data and mixed-source text generated by open-source models, we demonstrate that our proposed estimators consistently achieve high estimation accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22343v1</guid>
      <category>stat.ML</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiang Li, Garrett Wen, Weiqing He, Jiayuan Wu, Qi Long, Weijie J. Su</dc:creator>
    </item>
    <item>
      <title>Experimentation for Homogenous Policy Change</title>
      <link>https://arxiv.org/abs/2101.12318</link>
      <description>arXiv:2101.12318v2 Announce Type: replace 
Abstract: When the Stable Unit Treatment Value Assumption is violated and there is interference among units, there is not a uniquely defined Average Treatment Effect, and alternative estimands may be of interest. Among these are average unit-level differences in outcomes under different homogeneous treatment policies. We refer to such targets as Global Average Treatment Effects. We consider approaches to experimental design with multiple treatment conditions under partial interference and, given the estimand of interest, we show that difference-in-means estimators may perform better than correctly specified regression models in finite samples on root mean squared error for such targets. With errors correlated at the cluster level, we demonstrate that two-stage randomization procedures with intra-cluster correlation of treatment strictly between zero and one may dominate one-stage randomization designs on the same metric. Simulations illustrate performance of this approach; we consider an application to online experiments at Facebook.</description>
      <guid isPermaLink="false">oai:arXiv.org:2101.12318v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Molly Offer-Westort, Drew Dimmery</dc:creator>
    </item>
    <item>
      <title>A geometric approach in non-parametric Changepoint detection in circular data</title>
      <link>https://arxiv.org/abs/2403.00508</link>
      <description>arXiv:2403.00508v2 Announce Type: replace 
Abstract: In many temporally ordered data sets, it is observed that the parameters of the underlying distribution change abruptly at unknown times. The detection of such changepoints is important for many applications. While this problem has been studied substantially in the linear data setup, not much work has been done for angular data. In this article, we utilize the intrinsic geometry of a torus to propose new non-parametric tests. First, we propose new tests for the existence of changepoint(s) in the concentration, and second, a test to detect mean direction and/or concentration. The limiting distributions of the test statistics are derived, and their powers are obtained using extensive simulation. It is seen that the tests have better power than the corresponding existing tests. The proposed methods have been implemented on three real-life data sets, revealing interesting insights. In particular, our method, when used to detect simultaneous changes in mean direction and concentration for hourly wind direction measurements of the cyclonic storm "Amphan," identified changepoints that could be associated with important meteorological events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00508v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Surojit Biswas, Buddhananda Banerjee, Arnab Kumar Laha</dc:creator>
    </item>
    <item>
      <title>Direct-Assisted Bayesian Unit-level Modeling for Small Area Estimation of Rare Event Prevalence</title>
      <link>https://arxiv.org/abs/2408.16129</link>
      <description>arXiv:2408.16129v2 Announce Type: replace 
Abstract: Small area estimation using survey data can be achieved by using either a design-based or a model-based inferential approach. Design-based direct estimators are generally preferable because of their consistency, asymptotic normality, and reliance on fewer assumptions. However, when data are sparse at the desired area level, as is often the case when measuring rare events, these direct estimators can have extremely large uncertainty, making a model-based approach preferable. A model-based approach with a random spatial effect borrows information from surrounding areas at the cost of inducing shrinkage towards the local average. As a result, estimates may be over-smoothed and inconsistent with design-based estimates at higher area levels when aggregated. We propose two unit-level Bayesian models for small area estimation of rare event prevalence which use design-based direct estimates at a higher area level to increase consistency in aggregation. This model framework is designed to accommodate sparse data obtained from two-stage stratified cluster sampling, which is particularly relevant to applications in low and middle income countries. After introducing the model framework and its implementation, we conduct a simulation study to evaluate its properties and apply it to the estimation of the neonatal mortality rate in Zambia, using 2014 Demographic Health Surveys data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16129v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alana McGovern, Katherine Wilson, Jon Wakefield</dc:creator>
    </item>
    <item>
      <title>Risk Estimate under a Time-Varying Autoregressive Model for Data-Driven Reproduction Number Estimation</title>
      <link>https://arxiv.org/abs/2409.14937</link>
      <description>arXiv:2409.14937v3 Announce Type: replace 
Abstract: COVID-19 pandemic has brought to the fore epidemiological models which, though describing a wealth of behaviors, have previously received little attention in the signal processing literature. In this work, a generalized time-varying autoregressive model is considered, encompassing, but not reducing to, a state-of-the-art model of viral epidemics propagation. The time-varying parameter of this model is estimated via the minimization of a penalized likelihood estimator. A major challenge is that the estimation accuracy strongly depends on hyperparameters fine-tuning. Without available ground truth, hyperparameters are selected by minimizing specifically designed data-driven oracles, used as proxy for the estimation error. Focusing on the time-varying autoregressive Poisson model, the Stein's Unbiased Risk Estimate formalism is generalized to construct asymptotically unbiased risk estimators based on the derivation of an original autoregressive counterpart of Stein's lemma. The accuracy of these oracles and of the resulting estimates are assessed through intensive Monte Carlo simulations on synthetic data. Then, elaborating on recent epidemiological models, a novel weekly scaled Poisson model is proposed, better accounting for intrinsic variability of the contamination while being robust to reporting errors. Finally, the overall data-driven procedure is particularized to the estimation of COVID-19 reproduction number demonstrating its ability to yield very consistent estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14937v3</guid>
      <category>stat.ME</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Barbara Pascal, Samuel Vaiter</dc:creator>
    </item>
    <item>
      <title>Invariant Coordinate Selection and Fisher discriminant subspace beyond the case of two groups</title>
      <link>https://arxiv.org/abs/2409.17631</link>
      <description>arXiv:2409.17631v2 Announce Type: replace 
Abstract: Invariant Coordinate Selection (ICS) is a multivariate technique that relies on the simultaneous diagonalization of two scatter matrices. It serves various purposes, including its use as a dimension reduction tool prior to clustering or outlier detection. ICS's theoretical foundation establishes why and when the identified subspace should contain relevant information by demonstrating its connection with the Fisher discriminant subspace (FDS). These general results have been examined in detail primarily for specific scatter combinations within a two-cluster framework. In this study, we expand these investigations to include more clusters and scatter combinations. Our analysis reveals the importance of distinguishing whether the group centers matrix has full rank. In the full-rank case, we establish deeper connections between ICS and FDS. We provide a detailed study of these relationships for three clusters when the group centers matrix has full rank and when it does not. Based on these expanded theoretical insights and supported by numerical studies, we conclude that ICS is indeed suitable for recovering the FDS under very general settings and cases of failure seem rare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17631v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Colombe Becquart, Aurore Archimbaud, Anne Ruiz-Gazen, Luka Pril\'c, Klaus Nordhausen</dc:creator>
    </item>
    <item>
      <title>Asymptotic and compound e-values: multiple testing and empirical Bayes</title>
      <link>https://arxiv.org/abs/2409.19812</link>
      <description>arXiv:2409.19812v3 Announce Type: replace 
Abstract: We explicitly define the notions of (bona fide, approximate or asymptotic) compound p-values and e-values, which have been implicitly presented and extensively used in the recent multiple testing literature. While it is known that the e-BH procedure with compound e-values controls the FDR, we show the converse: every FDR controlling procedure can be recovered by instantiating the e-BH procedure with certain compound e-values. Since compound e-values are closed under averaging, this allows for combination and derandomization of FDR procedures. We then connect compound e-values to empirical Bayes. In particular, we use the fundamental theorem of compound decision theory to derive the log-optimal simple separable compound e-value for testing a set of point nulls against point alternatives: it is a ratio of mixture likelihoods. As one example, we construct asymptotic compound e-values for multiple t-tests, where the (nuisance) variances may be different across hypotheses. Our construction may be interpreted as a data-driven instantiation of the optimal discovery procedure (ODP), and our results provide the first type-I error guarantees for data-driven ODP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19812v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikolaos Ignatiadis, Ruodu Wang, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Robust Detection of Watermarks for Large Language Models Under Human Edits</title>
      <link>https://arxiv.org/abs/2411.13868</link>
      <description>arXiv:2411.13868v2 Announce Type: replace 
Abstract: Watermarking has offered an effective approach to distinguishing text generated by large language models (LLMs) from human-written text. However, the pervasive presence of human edits on LLM-generated text dilutes watermark signals, thereby significantly degrading detection performance of existing methods. In this paper, by modeling human edits through mixture model detection, we introduce a new method in the form of a truncated goodness-of-fit test for detecting watermarked text under human edits, which we refer to as Tr-GoF. We prove that the Tr-GoF test achieves optimality in robust detection of the Gumbel-max watermark in a certain asymptotic regime of substantial text modifications and vanishing watermark signals. Importantly, Tr-GoF achieves this optimality \textit{adaptively} as it does not require precise knowledge of human edit levels or probabilistic specifications of the LLMs, in contrast to the optimal but impractical (Neyman--Pearson) likelihood ratio test. Moreover, we establish that the Tr-GoF test attains the highest detection efficiency rate in a certain regime of moderate text modifications. In stark contrast, we show that sum-based detection rules, as employed by existing methods, fail to achieve optimal robustness in both regimes because the additive nature of their statistics is less resilient to edit-induced noise. Finally, we demonstrate the competitive and sometimes superior empirical performance of the Tr-GoF test on both synthetic data and open-source LLMs in the OPT and LLaMA families.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13868v2</guid>
      <category>stat.ME</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, Weijie J. Su</dc:creator>
    </item>
    <item>
      <title>Robust Inference for the Direct Average Treatment Effect with Treatment Assignment Interference</title>
      <link>https://arxiv.org/abs/2502.13238</link>
      <description>arXiv:2502.13238v2 Announce Type: replace 
Abstract: This paper develops methods for uncertainty quantification in causal inference settings with random network interference. We study the large-sample distributional properties of the classical difference-in-means Hajek treatment effect estimator, and propose a robust inference procedure for the (conditional) direct average treatment effect. Our framework allows for cross-unit interference in both the outcome equation and the treatment assignment mechanism. Drawing from statistical physics, we introduce a novel Ising model to capture complex dependencies in treatment assignment, and derive three results. First, we establish a Berry-Esseen-type distributional approximation that holds pointwise in the degree of interference induced by the Ising model. This approximation recovers existing results in the absence of treatment interference, and highlights the fragility of inference procedures that do not account for the presence of interference in treatment assignment. Second, we establish a uniform distributional approximation for the Hajek estimator and use it to develop robust inference procedures that remain valid uniformly over all interference regimes allowed by the model. Third, we propose a novel resampling method to implement the robust inference procedure and validate its performance through Monte Carlo simulations. A key technical innovation is the introduction of a conditional i.i.d. Gaussianization that may have broader applications. We also discuss extensions and generalizations of our results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13238v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo (Rae), Yihan He (Rae),  Ruiqi (Rae),  Yu</dc:creator>
    </item>
    <item>
      <title>Model-Based Clustering with Sequential Outlier Identification using the Distribution of Mahalanobis Distances</title>
      <link>https://arxiv.org/abs/2505.11668</link>
      <description>arXiv:2505.11668v2 Announce Type: replace 
Abstract: The presence of outliers can prevent clustering algorithms from accurately determining an appropriate group structure within a data set. We present outlierMBC, a model-based approach for sequentially removing outliers and clustering the remaining observations. Our method identifies outliers one at a time while fitting a multivariate Gaussian mixture model to data. Since it can be difficult to classify observations as outliers without knowing what the correct cluster structure is a priori, and the presence of outliers interferes with the process of modelling clusters correctly, we use an iterative method to identify outliers one by one. At each iteration, outlierMBC removes the observation with the lowest density and fits a Gaussian mixture model to the remaining data. The method continues to remove potential outliers until a pre-set maximum number of outliers is reached, then retrospectively identifies the optimal number of outliers. To decide how many outliers to remove, it uses the fact that the squared sample Mahalanobis distances of Gaussian distributed observations are Beta distributed when scaled appropriately. outlierMBC chooses the number of outliers which minimises a dissimilarity between this theoretical Beta distribution and the observed distribution of the scaled squared sample Mahalanobis distances. This means that our method both clusters the data using a Gaussian mixture model and implements a model-based procedure to identify the optimal outliers to remove without requiring the number of outliers to be pre-specified. Unlike leading methods in the literature, outlierMBC does not assume that the outliers follow a known distribution or that the number of outliers can be pre-specified. Moreover, outlierMBC performs strongly compared to these algorithms when applied to a range of simulated and real data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11668v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ult\'an P. Doherty, Paul D. McNicholas, Arthur White</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Latent Outcomes Learned with Factor Models</title>
      <link>https://arxiv.org/abs/2506.20549</link>
      <description>arXiv:2506.20549v2 Announce Type: replace 
Abstract: In many fields$\unicode{x2013}$including genomics, epidemiology, natural language processing, social and behavioral sciences, and economics$\unicode{x2013}$it is increasingly important to address causal questions in the context of factor models or representation learning. In this work, we investigate causal effects on $\textit{latent outcomes}$ derived from high-dimensional observed data using nonnegative matrix factorization. To the best of our knowledge, this is the first study to formally address causal inference in this setting. A central challenge is that estimating a latent factor model can cause an individual's learned latent outcome to depend on other individuals' treatments, thereby violating the standard causal inference assumption of no interference. We formalize this issue as $\textit{learning-induced interference}$ and distinguish it from interference present in a data-generating process. To address this, we propose a novel, intuitive, and theoretically grounded algorithm to estimate causal effects on latent outcomes while mitigating learning-induced interference and improving estimation efficiency. We establish theoretical guarantees for the consistency of our estimator and demonstrate its practical utility through simulation studies and an application to cancer mutational signature analysis. All baseline and proposed methods are available in our open-source R package, ${\tt causalLFO}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20549v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jenna M. Landy, Dafne Zorzetto, Roberta De Vito, Giovanni Parmigiani</dc:creator>
    </item>
    <item>
      <title>Learning treatment effects while treating those in need</title>
      <link>https://arxiv.org/abs/2407.07596</link>
      <description>arXiv:2407.07596v2 Announce Type: replace-cross 
Abstract: Many social programs attempt to allocate scarce resources to people with the greatest need. Indeed, public services increasingly use algorithmic risk assessments motivated by this goal. However, targeting the highest-need recipients often conflicts with attempting to evaluate the causal effect of the program as a whole, as the best evaluations would be obtained by randomizing the allocation. We propose a framework to design randomized allocation rules which optimally balance targeting high-need individuals with learning treatment effects, presenting policymakers with a Pareto frontier between the two goals. We give sample complexity guarantees for the policy learning problem and provide a computationally efficient strategy to implement it. We then collaborate with the human services department of Allegheny County, Pennsylvania to evaluate our methods on data from real service delivery settings. Optimized policies can substantially mitigate the tradeoff between learning and targeting. For example, it is often possible to obtain 90% of the optimal utility in targeting high-need individuals while ensuring that the average treatment effect can be estimated with less than 2 times the samples that a randomized controlled trial would require. Mechanisms for targeting public services often focus on measuring need as accurately as possible. However, our results suggest that algorithmic systems in public services can be most impactful if they incorporate program evaluation as an explicit goal alongside targeting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07596v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>ACM EC 2025</arxiv:journal_reference>
      <dc:creator>Bryan Wilder, Pim Welle</dc:creator>
    </item>
    <item>
      <title>Testing Causal Models with Hidden Variables in Polynomial Delay via Conditional Independencies</title>
      <link>https://arxiv.org/abs/2409.14593</link>
      <description>arXiv:2409.14593v2 Announce Type: replace-cross 
Abstract: Testing a hypothesized causal model against observational data is a key prerequisite for many causal inference tasks. A natural approach is to test whether the conditional independence relations (CIs) assumed in the model hold in the data. While a model can assume exponentially many CIs (with respect to the number of variables), testing all of them is both impractical and unnecessary. Causal graphs, which encode these CIs in polynomial space, give rise to local Markov properties that enable model testing with a significantly smaller subset of CIs. Model testing based on local properties requires an algorithm to list the relevant CIs. However, existing algorithms for realistic settings with hidden variables and non-parametric distributions can take exponential time to produce even a single CI constraint. In this paper, we introduce the c-component local Markov property (C-LMP) for causal graphs with hidden variables. Since C-LMP can still invoke an exponential number of CIs, we develop a polynomial delay algorithm to list these CIs in poly-time intervals. To our knowledge, this is the first algorithm that enables poly-delay testing of CIs in causal graphs with hidden variables against arbitrary data distributions. Experiments on real-world and synthetic data demonstrate the practicality of our algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14593v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1609/aaai.v39i25.34885</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the AAAI Conference on Artificial Intelligence, 39(25), 26813-26822 (2025)</arxiv:journal_reference>
      <dc:creator>Hyunchai Jeong, Adiba Ejaz, Jin Tian, Elias Bareinboim</dc:creator>
    </item>
    <item>
      <title>To Study Properties of a Known Procedure in Adaptive Sequential Sampling Design</title>
      <link>https://arxiv.org/abs/2412.17791</link>
      <description>arXiv:2412.17791v3 Announce Type: replace-cross 
Abstract: We consider the procedure proposed by Bhandari et al. (2009) in the context of two-treatment clinical trials, with the objective of minimizing the applications of the less effective drug to the least number of patients. Our focus is on an adaptive sequential procedure that is both simple and intuitive. Through a refined theoretical analysis, we establish that the number of applications of the less effective drug is a finite random variable whose all moments are also finite. In contrast, Bhandari et al. (2009) observed that this number increases logarithmically with the total sample size. We attribute this discrepancy to differences in their choice of the initial sample size and the method of analysis employed. We further extend the allocation rule to multi-treatment setup and derive analogous finiteness results, reinforcing the generalizability of our findings. Extensive simulation studies and real-data analyses support theoretical developments, showing stabilization in allocation and reduced patient exposure to inferior treatments as the total sample size grows. These results enhance the long-term ethical strength of the proposed adaptive allocation strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17791v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sampurna Kundu, Jayant Jha, Subir Kumar Bhandari</dc:creator>
    </item>
    <item>
      <title>Green LIME: Improving AI Explainability through Design of Experiments</title>
      <link>https://arxiv.org/abs/2502.12753</link>
      <description>arXiv:2502.12753v2 Announce Type: replace-cross 
Abstract: In artificial intelligence (AI), the complexity of many models and processes surpasses human understanding, making it challenging to determine why a specific prediction is made. This lack of transparency is particularly problematic in critical fields like healthcare, where trust in a model's predictions is paramount. As a result, the explainability of machine learning (ML) and other complex models has become a key area of focus. Efforts to improve model explainability often involve experimenting with AI systems and approximating their behavior through interpretable surrogate mechanisms. However, these procedures can be resource-intensive. Optimal design of experiments, which seeks to maximize the information obtained from a limited number of observations, offers promising methods for improving the efficiency of these explainability techniques. To demonstrate this potential, we explore Local Interpretable Model-agnostic Explanations (LIME), a widely used method introduced by Ribeiro et al. (2016). LIME provides explanations by generating new data points near the instance of interest and passing them through the model. While effective, this process can be computationally expensive, especially when predictions are costly or require many samples. LIME is highly versatile and can be applied to a wide range of models and datasets. In this work, we focus on models involving tabular data, regression tasks, and linear models as interpretable local approximations. By utilizing optimal design of experiments' techniques, we reduce the number of function evaluations of the complex model, thereby reducing the computational effort of LIME by a significant amount. We consider this modified version of LIME to be energy-efficient or "green".</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12753v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandra Stadler, Werner G. M\"uller, Radoslav Harman</dc:creator>
    </item>
    <item>
      <title>Causal Inference Isn't Special: Why It's Just Another Prediction Problem</title>
      <link>https://arxiv.org/abs/2504.04320</link>
      <description>arXiv:2504.04320v2 Announce Type: replace-cross 
Abstract: Causal inference is often portrayed as fundamentally distinct from predictive modeling, with its own terminology, goals, and intellectual challenges. But at its core, causal inference is simply a structured instance of prediction under distribution shift. In both cases, we begin with labeled data from a source domain and seek to generalize to a target domain where outcomes are not observed. The key difference is that in causal inference, the labels -- potential outcomes -- are selectively observed based on treatment assignment, introducing bias that must be addressed through assumptions. This perspective reframes causal estimation as a familiar generalization problem and highlights how techniques from predictive modeling, such as reweighting and domain adaptation, apply directly to causal tasks. It also clarifies that causal assumptions are not uniquely strong -- they are simply more explicit. By viewing causal inference through the lens of prediction, we demystify its logic, connect it to familiar tools, and make it more accessible to practitioners and educators alike.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04320v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Fern\'andez-Lor\'ia</dc:creator>
    </item>
  </channel>
</rss>
