<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 18 Aug 2025 04:00:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Zeroth-Order Non-smooth Non-convex Optimization via Gaussian Smoothing</title>
      <link>https://arxiv.org/abs/2508.11073</link>
      <description>arXiv:2508.11073v1 Announce Type: new 
Abstract: This paper addresses stochastic optimization of Lipschitz-continuous, nonsmooth and nonconvex objectives over compact convex sets, where only noisy function evaluations are available. While gradient-free methods have been developed for smooth nonconvex problems, extending these techniques to the nonsmooth setting remains challenging. The primary difficulty arises from the absence of a Taylor series expansion for Clarke subdifferentials, which limits the ability to approximate and analyze the behavior of the objective function in a neighborhood of a point. We propose a two time-scale zeroth-order projected stochastic subgradient method leveraging Gaussian smoothing to approximate Clarke subdifferentials. First, we establish that the expectation of the Gaussian-smoothed subgradient lies within an explicitly bounded error of the Clarke subdifferential, a result that extends prior analyses beyond convex/smooth settings. Second, we design a novel algorithm with coupled updates: a fast timescale tracks the subgradient approximation, while a slow timescale drives convergence. Using continuous-time dynamical systems theory and robust perturbation analysis, we prove that iterates converge almost surely to a neighborhood of the set of Clarke stationary points, with neighborhood size controlled by the smoothing parameter. To our knowledge, this is the first zeroth-order method achieving almost sure convergence for constrained nonsmooth nonconvex optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11073v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anik Kumar Paul, Shalabh Bhatnagar</dc:creator>
    </item>
    <item>
      <title>A Heuristic ADMM-based Approach for Tree-Constrained Optimization</title>
      <link>https://arxiv.org/abs/2508.11078</link>
      <description>arXiv:2508.11078v1 Announce Type: new 
Abstract: This paper presents centralized and distributed Alternating Direction Method of Multipliers (ADMM) frameworks for solving large-scale nonconvex optimization problems with binary decision variables subject to spanning tree or rooted arborescence constraints. We address the combinatorial complexity by introducing a continuous relaxation of the binary variables and enforcing agreement through an augmented Lagrangian formulation. The algorithms alternate between solving a convex continuous subproblem and projecting onto the tree-feasible set, reducing to a Minimum Spanning Tree or Minimum Weight Rooted Arborescence problem, both solvable in polynomial time. The distributed algorithm enables agents to cooperate via local communication, enhancing scalability and robustness. We apply the framework to multicommodity flow design with hop-constrained spanning trees. Numerical experiments demonstrate that our methods yield high-quality feasible solutions, achieving near-optimal performance with significant computational savings compared to the commercial solver Gurobi.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11078v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yacine Mokhtari</dc:creator>
    </item>
    <item>
      <title>Inexact Zeroth-Order Nonsmooth and Nonconvex Stochastic Composite Optimization and Applications</title>
      <link>https://arxiv.org/abs/2508.11519</link>
      <description>arXiv:2508.11519v1 Announce Type: new 
Abstract: In this paper we present an inexact zeroth-order method suitable for the solution nonsmooth and nonconvex stochastic composite optimization problems, in which the objective is split into a real-valued Lipschitz continuous stochastic function and an extended-valued (deterministic) proper, closed, and convex one. The algorithm operates under inexact oracles providing noisy (and biased) stochastic evaluations of the underlying finite-valued part of the objective function. We show that the proposed method converges (non-asymptotically), under very mild assumptions, close to a stationary point of an appropriate surrogate problem which is related (in a precise mathematical sense) to the original one. This, in turn, provides a new notion of approximate stationarity suitable nonsmooth and nonconvex stochastic composite optimization, generalizing conditions used in the available literature.
  In light of the generic oracle properties under which the algorithm operates, we showcase the applicability of the approach in a wide range of problems including large classes of two-stage nonconvex stochastic optimization and nonconvex-nonconcave minimax stochastic optimization instances, without requiring convexity of the lower level problems, or even uniqueness of the associated lower level solution maps. We showcase how the developed theory can be applied in each of these cases under general assumptions, providing algorithmic methodologies that go beyond the current state-of-the-art appearing in each respective literature, enabling the solution of problems that are out of reach of currently available methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11519v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Spyridon Pougkakiotis, Dionysis Kalogerias</dc:creator>
    </item>
    <item>
      <title>A Dynamically Weighted ADMM Framework for Byzantine Resilience</title>
      <link>https://arxiv.org/abs/2508.11572</link>
      <description>arXiv:2508.11572v1 Announce Type: new 
Abstract: The alternating direction of multipliers method (ADMM) is a popular method to solve distributed consensus optimization utilizing efficient communication among various nodes in the network. However, in the presence of faulty or attacked nodes, even a small perturbation (or sharing false data) during the communication can lead to divergence of the solution. To address this issue, in this work we consider ADMM under the effect of Byzantine threat, where an unknown subset of nodes is subject to Byzantine attacks or faults. We propose Dynamically Weighted ADMM (DW-ADMM), a novel variant of ADMM that uses dynamic weights on the edges of the network, thus promoting resilient distributed optimization. We establish that the proposed method (i) produces a nearly identical solution to conventional ADMM in the error-free case, and (ii) guarantees a bounded solution with respect to the global minimizer, even under Byzantine threat. Finally, we demonstrate the effectiveness of our proposed algorithm using an illustrative numerical simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11572v1</guid>
      <category>math.OC</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vishnu Vijay, Kartik A. Pant, Minhyun Cho, Inseok Hwang</dc:creator>
    </item>
    <item>
      <title>MDNS: Masked Diffusion Neural Sampler via Stochastic Optimal Control</title>
      <link>https://arxiv.org/abs/2508.10684</link>
      <description>arXiv:2508.10684v1 Announce Type: cross 
Abstract: We study the problem of learning a neural sampler to generate samples from discrete state spaces where the target probability mass function $\pi\propto\mathrm{e}^{-U}$ is known up to a normalizing constant, which is an important task in fields such as statistical physics, machine learning, combinatorial optimization, etc. To better address this challenging task when the state space has a large cardinality and the distribution is multi-modal, we propose $\textbf{M}$asked $\textbf{D}$iffusion $\textbf{N}$eural $\textbf{S}$ampler ($\textbf{MDNS}$), a novel framework for training discrete neural samplers by aligning two path measures through a family of learning objectives, theoretically grounded in the stochastic optimal control of the continuous-time Markov chains. We validate the efficiency and scalability of MDNS through extensive experiments on various distributions with distinct statistical properties, where MDNS learns to accurately sample from the target distributions despite the extremely high problem dimensions and outperforms other learning-based baselines by a large margin. A comprehensive study of ablations and extensions is also provided to demonstrate the efficacy and potential of the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10684v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuchen Zhu, Wei Guo, Jaemoo Choi, Guan-Horng Liu, Yongxin Chen, Molei Tao</dc:creator>
    </item>
    <item>
      <title>When higher-order interactions enhance synchronization: the case of the Kuramoto model</title>
      <link>https://arxiv.org/abs/2508.10992</link>
      <description>arXiv:2508.10992v1 Announce Type: cross 
Abstract: Synchronization is a fundamental phenomenon in complex systems, observed across a wide range of natural and engineered contexts. The Kuramoto model provides a foundational framework for understanding synchronization among coupled oscillators, traditionally assuming pairwise interactions. However, many real-world systems exhibit group and many-body interactions, which can be effectively modeled through hypergraphs. Previous studies suggest that higher-order interactions shrink the attraction basin of the synchronous state, making it harder to reach and potentially impairing synchronization, despite enriching the dynamics. In this work, we show that this is not always the case. Through extensive numerical analysis of the higher-order Kuramoto model, we find that while strong higher-order interactions do generally work against synchronization, weak higher-order interactions can actually enhance it. This result is further corroborated by a cost-benefit analysis: under a constrained budget of both pairwise and higher-order interactions, a mixed allocation involving both consistently achieves higher synchronization than relying on either interaction type alone. These findings provide new insights into the role of higher-order interactions in shaping collective dynamics and point to design principles for optimizing synchronization in complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10992v1</guid>
      <category>nlin.AO</category>
      <category>cond-mat.stat-mech</category>
      <category>math.OC</category>
      <category>nlin.PS</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Riccardo Muolo, Hiroya Nakao, Marco Coraggio</dc:creator>
    </item>
    <item>
      <title>Quantization through Piecewise-Affine Regularization: Optimization and Statistical Guarantees</title>
      <link>https://arxiv.org/abs/2508.11112</link>
      <description>arXiv:2508.11112v1 Announce Type: cross 
Abstract: Optimization problems over discrete or quantized variables are very challenging in general due to the combinatorial nature of their search space. Piecewise-affine regularization (PAR) provides a flexible modeling and computational framework for quantization based on continuous optimization. In this work, we focus on the setting of supervised learning and investigate the theoretical foundations of PAR from optimization and statistical perspectives. First, we show that in the overparameterized regime, where the number of parameters exceeds the number of samples, every critical point of the PAR-regularized loss function exhibits a high degree of quantization. Second, we derive closed-form proximal mappings for various (convex, quasi-convex, and non-convex) PARs and show how to solve PAR-regularized problems using the proximal gradient method, its accelerated variant, and the Alternating Direction Method of Multipliers. Third, we study statistical guarantees of PAR-regularized linear regression problems; specifically, we can approximate classical formulations of $\ell_1$-, squared $\ell_2$-, and nonconvex regularizations using PAR and obtain similar statistical guarantees with quantized solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11112v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianhao Ma, Lin Xiao</dc:creator>
    </item>
    <item>
      <title>A Convergent Generalized Krylov Subspace Method for Compressed Sensing MRI Reconstruction with Gradient-Driven Denoisers</title>
      <link>https://arxiv.org/abs/2508.11219</link>
      <description>arXiv:2508.11219v1 Announce Type: cross 
Abstract: Model-based reconstruction plays a key role in compressed sensing (CS) MRI, as it incorporates effective image regularizers to improve the quality of reconstruction. The Plug-and-Play and Regularization-by-Denoising frameworks leverage advanced denoisers (e.g., convolutional neural network (CNN)-based denoisers) and have demonstrated strong empirical performance. However, their theoretical guarantees remain limited, as practical CNNs often violate key assumptions. In contrast, gradient-driven denoisers achieve competitive performance, and the required assumptions for theoretical analysis are easily satisfied. However, solving the associated optimization problem remains computationally demanding. To address this challenge, we propose a generalized Krylov subspace method (GKSM) to solve the optimization problem efficiently. Moreover, we also establish rigorous convergence guarantees for GKSM in nonconvex settings. Numerical experiments on CS MRI reconstruction with spiral and radial acquisitions validate both the computational efficiency of GKSM and the accuracy of the theoretical predictions. The proposed optimization method is applicable to any linear inverse problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11219v1</guid>
      <category>eess.IV</category>
      <category>math.OC</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tao Hong, Umberto Villa, Jeffrey A. Fessler</dc:creator>
    </item>
    <item>
      <title>Direct data-driven interpolation and approximation of linear parameter-varying system trajectories</title>
      <link>https://arxiv.org/abs/2508.11332</link>
      <description>arXiv:2508.11332v1 Announce Type: cross 
Abstract: We consider the problem of estimating missing values in trajectories of linear parameter-varying (LPV) systems. We solve this interpolation problem for the class of shifted-affine LPV systems. Conditions for the existence and uniqueness of solutions are given and a direct data-driven algorithm for its computation is presented, i.e., the data-generating system is not given by a parametric model but is implicitly specified by data. We illustrate the applicability of the proposed solution on illustrative examples of a mass-spring-damper system with exogenous and endogenous parameter variation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11332v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>math.OC</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chris Verhoek, Ivan Markovsky, Roland T\'oth</dc:creator>
    </item>
    <item>
      <title>A Comparative Study of Floating-Base Space Parameterizations for Agile Whole-Body Motion Planning</title>
      <link>https://arxiv.org/abs/2508.11520</link>
      <description>arXiv:2508.11520v1 Announce Type: cross 
Abstract: Automatically generating agile whole-body motions for legged and humanoid robots remains a fundamental challenge in robotics. While numerous trajectory optimization approaches have been proposed, there is no clear guideline on how the choice of floating-base space parameterization affects performance, especially for agile behaviors involving complex contact dynamics. In this paper, we present a comparative study of different parameterizations for direct transcription-based trajectory optimization of agile motions in legged systems. We systematically evaluate several common choices under identical optimization settings to ensure a fair comparison. Furthermore, we introduce a novel formulation based on the tangent space of SE(3) for representing the robot's floating-base pose, which, to our knowledge, has not received attention from the literature. This approach enables the use of mature off-the-shelf numerical solvers without requiring specialized manifold optimization techniques. We hope that our experiments and analysis will provide meaningful insights for selecting the appropriate floating-based representation for agile whole-body motion generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11520v1</guid>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Evangelos Tsiatsianas, Chairi Kiourt, Konstantinos Chatzilygeroudis</dc:creator>
    </item>
    <item>
      <title>Integrating Uncertainties for Koopman-Based Stabilization</title>
      <link>https://arxiv.org/abs/2508.11533</link>
      <description>arXiv:2508.11533v1 Announce Type: cross 
Abstract: Over the past decades, the Koopman operator has been widely applied in data-driven control, yet its theoretical foundations remain underexplored. This paper establishes a unified framework to address the robust stabilization problem in data-driven control via the Koopman operator, fully accounting for three uncertainties: projection error, estimation error, and process disturbance. It comprehensively investigates both direct and indirect data-driven control approaches, facilitating flexible methodology selection for analysis and control. For the direct approach, considering process disturbances, the lifted-state feedback controller, designed via a linear matrix inequality (LMI), robustly stabilizes all lifted bilinear systems consistent with noisy data. For the indirect approach requiring system identification, the feedback controller, designed using a nonlinear matrix inequality convertible to an LMI, ensures closed-loop stability under worst-case process disturbances. Numerical simulations via cross-validation validate the effectiveness of both approaches, highlighting their theoretical significance and practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11533v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yicheng Lin, Bingxian Wu, Nan Bai, Zhiyong Sun, Yunxiao Ren, Chuanze Chen, Zhisheng Duan</dc:creator>
    </item>
    <item>
      <title>Bayesian Distributionally Robust Nash Equilibrium and Its Application</title>
      <link>https://arxiv.org/abs/2410.20364</link>
      <description>arXiv:2410.20364v3 Announce Type: replace 
Abstract: Inspired by the recent work by Shapiro et al. [45], we propose a Bayesian distributionally robust Nash equilibrium (BDRNE) model where each player lacks complete information on the true probability distribution of the underlying uncertainty represented by a random variable and subsequently determines the optimal decision by solving a Bayesian distributionally robust optimization (BDRO) problem under the Nash conjecture. Unlike most of the DRO models in the literature, the BDRO model assumes (a) the true unknown distribution of the random variable can be approximated by a randomized parametric family of distributions, (b) the average of the worst-case expected value of the objective function with respect to the posterior distribution of the parameter, instead of the worst-case expected value of the objective function is considered in each player's decision making, and (c) the posterior distribution of the parameter is updated as more and more sampling information of the random variable is gathered. Under some moderate conditions, we demonstrate the existence of a BDRNE and derive asymptotic convergence of the equilibrium as the sample size increases. Moreover, we propose to solve the BDRNE problem by Gauss-Seidel-type iterative method in the case when the ambiguity set of each player is constructed via Kullback-Leibler (KL) divergence. Finally, we apply the BDRNE model to a price competition problem under multinomial logit demand. The preliminary numerical test results show that the proposed model and computational scheme perform well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20364v3</guid>
      <category>math.OC</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jian Liu, Ziheng Su, Huifu Xu</dc:creator>
    </item>
    <item>
      <title>Best Subset Selection: Optimal Pursuit for Feature Selection and Elimination</title>
      <link>https://arxiv.org/abs/2501.16815</link>
      <description>arXiv:2501.16815v3 Announce Type: replace 
Abstract: This paper introduces two novel criteria: one for feature selection and another for feature elimination in the context of best subset selection, which is a benchmark problem in statistics and machine learning. From the perspective of optimization, we revisit the classical selection and elimination criteria in traditional best subset selection algorithms, revealing that these classical criteria capture only partial variations of the objective function after the entry or exit of features. By formulating and solving optimization subproblems for feature entry and exit exactly, new selection and elimination criteria are proposed, proved as the optimal decisions for the current entry-and-exit process compared to classical criteria. Replacing the classical selection and elimination criteria with the proposed ones generates a series of enhanced best subset selection algorithms. These generated algorithms not only preserve the theoretical properties of the original algorithms but also achieve significant meta-gains without increasing computational cost across various scenarios and evaluation metrics on multiple tasks such as compressed sensing and sparse regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16815v3</guid>
      <category>math.OC</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhihan Zhu, Yanhao Zhang, Yong Xia</dc:creator>
    </item>
    <item>
      <title>Symmetric and Non-Symmetric Cone Separation via Bishop-Phelps Cones in Normed Spaces</title>
      <link>https://arxiv.org/abs/2503.10184</link>
      <description>arXiv:2503.10184v2 Announce Type: replace 
Abstract: In this paper, we study relationships between symmetric and non-symmetric separation of (not necessarily convex) cones by using separating cones of Bishop-Phelps type in real normed spaces. Besides extending some known results for the non-symmetric cone separation approach, we propose a new symmetric cone separation approach and establish cone separation results for it by using some cone separation results obtained for the non-symmetric cone separation approach twice (by swapping the roles of the cones). In addition to specifically emphasizing the results for the convex case, we also present some existence results for (bounded) convex bases of convex cones. Finally, we highlight some applications of symmetric and non-symmetric cone separation in optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10184v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fernando Garc\'ia-Casta\~no, Christian G\"unther, M. A. Melguizo-Padial, Christiane Tammer</dc:creator>
    </item>
    <item>
      <title>Time Scaling Makes Accelerated Gradient Flow and Proximal Method Faster in Multiobjective Optimization</title>
      <link>https://arxiv.org/abs/2508.07254</link>
      <description>arXiv:2508.07254v2 Announce Type: replace 
Abstract: This paper extends a class of single-objective gradient flows and accelerated proximal methods to the multiobjective optimization domain within Euclidean spaces. The proposed gradient flow is a second-order differential equation composed of a second-order term, a first-order term with asymptotic vanishing behavior, and a gradient term with time scaling. We prove the existence of trajectory solutions to the equation and, through Lyapunov analysis, demonstrate that with appropriate parameter choices, the trajectory solutions can achieve a sublinear convergence rate faster than $O(1/t^2)$. For the proposed proximal algorithm, we similarly obtain a sublinear convergence rate faster than $O(1/k^2)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07254v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingdong Yin</dc:creator>
    </item>
    <item>
      <title>Byzantine-Resilient Decentralized Online Resource Allocation</title>
      <link>https://arxiv.org/abs/2508.08658</link>
      <description>arXiv:2508.08658v2 Announce Type: replace 
Abstract: In this paper, we investigate the problem of decentralized online resource allocation in the presence of Byzantine attacks. In this problem setting, some agents may be compromised due to external manipulations or internal failures, causing them to behave maliciously and disrupt the resource allocation process by sending incorrect messages to their neighbors. Given the non-consensual nature of the resource allocation problem, we formulate it under a primal-dual optimization framework, where the dual variables are aggregated among the agents, enabling the incorporation of robust aggregation mechanisms to mitigate Byzantine attacks. By leveraging the classical Byzantine attack model, we propose a class of Byzantine-resilient decentralized online resource allocation algorithms that judiciously integrate the adaptive robust clipping technique with the existing robust aggregation rules to filter out adversarial messages. We establish theoretical guarantees, showing that the proposed algorithms achieve tight linear dynamic regret and accumulative constraint violation bounds, where the constants depend on the properties of robust aggregation rules. Numerical experiments on decentralized online economic dispatch validate the effectiveness of our approach and support our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08658v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Runhua Wang, Qing Ling, Hoi-To Wai, Zhi Tian</dc:creator>
    </item>
    <item>
      <title>Fundamental Bias in Inverting Random Sampling Matrices with Application to Sub-sampled Newton</title>
      <link>https://arxiv.org/abs/2502.13583</link>
      <description>arXiv:2502.13583v3 Announce Type: replace-cross 
Abstract: A substantial body of work in machine learning (ML) and randomized numerical linear algebra (RandNLA) has exploited various sorts of random sketching methodologies, including random sampling and random projection, with much of the analysis using Johnson--Lindenstrauss and subspace embedding techniques. Recent studies have identified the issue of inversion bias -- the phenomenon that inverses of random sketches are not unbiased, despite the unbiasedness of the sketches themselves. This bias presents challenges for the use of random sketches in various ML pipelines, such as fast stochastic optimization, scalable statistical estimators, and distributed optimization. In the context of random projection, the inversion bias can be easily corrected for dense Gaussian projections (which are, however, too expensive for many applications). Recent work has shown how the inversion bias can be corrected for sparse sub-gaussian projections. In this paper, we show how the inversion bias can be corrected for random sampling methods, both uniform and non-uniform leverage-based, as well as for structured random projections, including those based on the Hadamard transform. Using these results, we establish problem-independent local convergence rates for sub-sampled Newton methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13583v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chengmei Niu, Zhenyu Liao, Zenan Ling, Michael W. Mahoney</dc:creator>
    </item>
    <item>
      <title>Multi-Class Stackelberg Games for the Co-Design of Networked Systems</title>
      <link>https://arxiv.org/abs/2505.03468</link>
      <description>arXiv:2505.03468v3 Announce Type: replace-cross 
Abstract: We investigate a co-design problem, encompassing simultaneous design of system infrastructure and control, through a game-theoretical framework. To this end, we propose the co-design problem as a two-layer hierarchical strategic interaction. At the upper layer, a leader (or multiple leaders) determines system design parameters, while at the lower layer, a follower (or multiple followers) optimizes the control strategy. To capture this hierarchy, we propose four novel classes of Stackelberg games that integrate diverse strategic behaviors, including combinations of cooperative and non-cooperative interactions across two different layers. Notably, the leaders' interactions are represented using a normal-form game, whereas the followers' interactions are modeled by different games (dynamic games in discrete time). These distinct game structures result in a Stackelberg game that accommodates different game types per layer, and/or supports heterogeneous strategic behaviors involving cooperation and non-cooperation simultaneously. Learning algorithms using the best-response dynamics are used to solve the game problems when considering a discrete strategic space for the leaders. The efficacy of the proposed approach is demonstrated through an application to the co-design of the Barcelona drinking water network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03468v3</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Julian Barreiro-Gomez, Ye Wang</dc:creator>
    </item>
  </channel>
</rss>
