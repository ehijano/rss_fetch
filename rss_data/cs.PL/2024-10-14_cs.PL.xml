<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PL</link>
    <description>cs.PL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 14 Oct 2024 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>LecPrompt: A Prompt-based Approach for Logical Error Correction with CodeBERT</title>
      <link>https://arxiv.org/abs/2410.08241</link>
      <description>arXiv:2410.08241v1 Announce Type: cross 
Abstract: Logical errors in programming don't raise compiler alerts, making them hard to detect. These silent errors can disrupt a program's function or cause run-time issues. Their correction requires deep insight into the program's logic, highlighting the importance of automated detection and repair. In this paper, we introduce LecPrompt to localize and repair logical errors, an prompt-based approach that harnesses the capabilities of CodeBERT, a transformer-based large language model trained on code. First, LecPrompt leverages a large language model to calculate perplexity and log probability metrics, pinpointing logical errors at both token and line levels. Through statistical analysis, it identifies tokens and lines that deviate significantly from the expected patterns recognized by large language models, marking them as potential error sources. Second, by framing the logical error correction challenge as a Masked Language Modeling (MLM) task, LecPrompt employs CodeBERT to autoregressively repair the identified error tokens. Finally, the soft-prompt method provides a novel solution in low-cost scenarios, ensuring that the model can be fine-tuned to the specific nuances of the logical error correction task without incurring high computational costs. To evaluate LecPrompt's performance, we created a method to introduce logical errors into correct code and applying this on QuixBugs to produce the QuixBugs-LE dataset. Our evaluations on the QuixBugs-LE dataset for both Python and Java highlight the impressive capabilities of our method, LecPrompt. For Python, LecPrompt achieves a noteworthy 74.58% top-1 token-level repair accuracy and 27.4% program-level repair accuracy. In Java, LecPrompt delivers a 69.23\% top-1 token-level repair accuracy and 24.7% full program-level repair accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08241v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.PL</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenyu Xu, Victor S. Sheng</dc:creator>
    </item>
    <item>
      <title>From Uncertainty to Innovation: Wearable Prototyping with ProtoBot</title>
      <link>https://arxiv.org/abs/2410.08340</link>
      <description>arXiv:2410.08340v1 Announce Type: cross 
Abstract: Despite AI advancements, individuals without software or hardware expertise still face barriers in designing wearable electronic devices due to the lack of code-free prototyping tools. To eliminate these barriers, we designed ProtoBot, leveraging large language models, and conducted a case study with four professionals from different disciplines through playful interaction. The study resulted in four unique wearable device concepts, with participants using Protobot to prototype selected components. From this experience, we learned that (1) uncertainty can be turned into a positive experience, (2) the ProtoBot should transform to reliably act as a guide, and (3) users need to adjust design parameters when interacting with the prototypes. Our work demonstrates, for the first time, the use of large language models in rapid prototyping of wearable electronics. We believe this approach will pioneer rapid prototyping without fear of uncertainties for people who want to develop both wearable prototypes and other products.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08340v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.PL</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\.Ihsan Ozan Y{\i}ld{\i}r{\i}m, Cansu \c{C}etin Er, Ege Keskin, Murat Ku\c{s}cu, O\u{g}uzhan \"Ozcan</dc:creator>
    </item>
    <item>
      <title>WasmWalker: Path-based Code Representations for Improved WebAssembly Program Analysis</title>
      <link>https://arxiv.org/abs/2410.08517</link>
      <description>arXiv:2410.08517v1 Announce Type: cross 
Abstract: WebAssembly, or Wasm, is a low-level binary language that enables execution of near-native-performance code in web browsers. Wasm has proven to be useful in applications including gaming, audio and video processing, and cloud computing, providing a high-performance, low-overhead alternative to JavaScript in web development. The fast and widespread adoption of WebAssembly by all major browsers has created an opportunity for analysis tools that support this new technology. Deep learning program analysis models can greatly benefit from the program structure information included in Abstract Syntax Tree (AST)-aware code representations. To obtain such code representations, we performed an empirical analysis on the AST paths in the WebAssembly Text format of a large dataset of WebAssembly binary files compiled from source packages in the Ubuntu 18.04 repositories. After refining the collected paths, we discovered that only 3,352 unique paths appeared across all of these binary files. With this insight, we propose two novel code representations for WebAssembly binaries. These novel representations serve not only to generate fixed-size code embeddings but also to supply additional information to sequence-to-sequence models. Ultimately, our approach helps program analysis models uncover new properties from Wasm binaries, expanding our understanding of their potential. We evaluated our new code representation on two applications: (i) method name prediction and (ii) recovering precise return types. Our results demonstrate the superiority of our novel technique over previous methods. More specifically, our new method resulted in 5.36% (11.31%) improvement in Top-1 (Top-5) accuracy in method name prediction and 8.02% (7.92%) improvement in recovering precise return types, compared to the previous state-of-the-art technique, SnowWhite.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08517v1</guid>
      <category>cs.SE</category>
      <category>cs.PL</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Robati Shirzad, Patrick Lam</dc:creator>
    </item>
    <item>
      <title>JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating Large Language Models</title>
      <link>https://arxiv.org/abs/2406.12902</link>
      <description>arXiv:2406.12902v2 Announce Type: replace-cross 
Abstract: Code generation benchmarks such as HumanEval are widely adopted to evaluate LLMs' capabilities. However, after consolidating the latest 24 benchmarks, we noticed three significant imbalances. First, imbalanced programming language. 95.8% of benchmarks involve Python, while only 5 benchmarks involve Java. Second, imbalanced code granularity. Function-/statement-level benchmarks account for over 83.3% of benchmarks. Only a mere handful extends to class-/project-levels, and all are limited to Python. Third, lacking advanced features. Existing benchmarks primarily assess basic coding skills, while overlooking advanced Object-Oriented Programming (OOP) features (i.e., encapsulation, inheritance, and polymorphism).
  To fill these gaps, we propose JavaBench, a project-level Java benchmark that exercises OOP features. It comprises four Java projects with 389 methods in 106 Java classes. The test coverage is up to 92%, and JavaBench is attested by 282 undergraduate students, reaching a 90.93/100 average score (i.e., pass rate against the test suite), ensuring the quality of documentation, code skeleton, and tests. To better evaluate LLM's capability against JavaBench, we introduce a systematic evaluation design covering three context settings and five synthesis strategies at two granularities using three hierarchical metrics. Our extensive experiment yields several interesting findings. First, we noticed that regarding project-level Java programming, LLMs are far behind undergraduate students (no project can be correctly completed by any studied LLMs, and at most 41.17% Pass@5 in a more relaxed evaluation). Second, using method signature as prompt context may strike an ideal balance for project-level code generation. JavaBench is publicly available at https://github.com/java-bench/JavaBench.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12902v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jialun Cao, Zhiyong Chen, Jiarong Wu, Shing-chi Cheung, Chang Xu</dc:creator>
    </item>
  </channel>
</rss>
