<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 May 2025 04:00:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Cumulative Logit Ordinal Regression with Proportional Odds under Nonignorable Missing Response -- Application to Phase III Trial</title>
      <link>https://arxiv.org/abs/2505.04727</link>
      <description>arXiv:2505.04727v1 Announce Type: new 
Abstract: Missing data are inevitable in clinical trials, and trials that produce categorical ordinal responses are not exempted from this. Typically, missing values in the data occur due to different missing mechanisms, such as missing completely at random, missing at random, and missing not at random. Under a specific missing data regime, when the conditional distribution of the missing data is dependent on the ordinal response variable itself along with other predictor variables, then the missing data mechanism is called nonignorable. In this article we propose an expectation maximization based algorithm for fitting a proportional odds regression model when the missing responses are nonignorable. We report results from an extensive simulation study to illustrate the methodology and its finite sample properties. We also apply the proposed method to a recently completed Phase III psoriasis study using an investigational compound. The corresponding SAS program is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04727v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arnab Kumar Maity, Huaming Tan, Vivek Pradhan, Soutir Bandyopadhyay</dc:creator>
    </item>
    <item>
      <title>Assessing Risk Heterogeneity through Heavy-Tailed Frequency and Severity Mixtures</title>
      <link>https://arxiv.org/abs/2505.04795</link>
      <description>arXiv:2505.04795v1 Announce Type: new 
Abstract: In operational risk management and actuarial finance, the analysis of risk often begins by dividing a random damage-generation process into its separate frequency and severity components. In the present article, we construct canonical families of mixture distributions for each of these components, based on a Negative Binomial kernel for frequency and a Gamma kernel for severity. The mixtures are employed to assess the heterogeneity of risk factors underlying an empirical distribution through the shape of the implied mixing distribution. From the duality of the Negative Binomial and Gamma distributions, we first derive necessary and sufficient conditions for heavy-tailed (i.e., inverse power-law) canonical mixtures. We then formulate flexible 4-parameter families of mixing distributions for Geometric and Exponential kernels to generate heavy-tailed 4-parameter mixture models, and extend these mixtures to arbitrary Negative Binomial and Gamma kernels, respectively, yielding 5-parameter mixtures for detecting and measuring risk heterogeneity. To check the robustness of such heterogeneity inferences, we show how a fitted 5-parameter model may be re-expressed in terms of alternative Negative Binomial or Gamma kernels whose associated mixing distributions form a "calibrated" family.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04795v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael R. Powers, Jiaxin Xu</dc:creator>
    </item>
    <item>
      <title>Supervised Integrative Biclustering with applications to Alzheimer's Disease</title>
      <link>https://arxiv.org/abs/2505.04830</link>
      <description>arXiv:2505.04830v1 Announce Type: new 
Abstract: Multiple types or views of data (e.g. genetics, proteomics) measured on the same set of individuals are now popularly generated in many biomedical studies. A particular interest might be the detection of sample subgroups (e.g. subtypes of disease) characterized by specific groups of variables. Biclustering methods are well-suited for this problem since they can group samples and variables simultaneously. However, most existing biclustering methods cannot guarantee that the detected sample clusters are clinically meaningful and related to a clinical outcome because they independently identify biclusters and associate sample clusters with a clinical outcome. Additionally, these methods have been developed for continuous data when integrating data from different views and do not allow for a mixture of data distributions. We propose a new formulation of biclustering and prediction method for multi-view data from different distributions that enhances our ability to identify clinically meaningful biclusters by incorporating a clinical outcome. Sample clusters are defined based on an adaptively chosen subset of variables and their association with a clinical outcome. We use extensive simulations to showcase the effectiveness of our proposed method in comparison to existing methods. Real-world applications using lipidomics, imaging, and cognitive data on Alzheimer's disease(AD) identified biclusters with significant cognitive differences that other methods missed. The distinct lipid categories and brain regions characterizing the biclusters suggest potential new insights into pathology of AD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04830v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaifeng Yang, Thierry Chekouo, Sandra E. Safo</dc:creator>
    </item>
    <item>
      <title>Model Selection for Unit-root Time Series with Many Predictors</title>
      <link>https://arxiv.org/abs/2505.04884</link>
      <description>arXiv:2505.04884v1 Announce Type: new 
Abstract: This paper studies model selection for general unit-root time series, including the case with many exogenous predictors. We propose FHTD, a new model selection algorithm that leverages forward stepwise regression (FSR), a high-dimensional information criterion (HDIC), a backward elimination method based on HDIC, and a data-driven thresholding (DDT) approach. Under some mild assumptions that allow for unknown locations and multiplicities of the characteristic roots on the unit circle of the time series and conditional heteroscedasticity in the predictors and errors, we establish the sure screening property of FSR and the selection consistency of FHTD. Central to our analysis are two key technical contributions, a new functional central limit theorem for multivariate linear processes and a uniform lower bound for the minimum eigenvalue of the sample covariance matrices, both of which are of independent interest. Simulation results corroborate the theoretical properties and show the superior performance of FHTD in model selection. We showcase the application of the proposed FHTD by modeling U.S. monthly housing starts and unemployment data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04884v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuo-Chieh Huang, Ching-Kang Ing, Ruey S. Tsay</dc:creator>
    </item>
    <item>
      <title>Low-Rank Regularization of Global Fr\'{e}chet Regression Models for Distributional Responses</title>
      <link>https://arxiv.org/abs/2505.04926</link>
      <description>arXiv:2505.04926v1 Announce Type: new 
Abstract: Fr\'echet regression has emerged as a useful tool for modeling non-Euclidean response variables associated with Euclidean covariates. In this work, we propose a global Fr\'echet regression estimation method that incorporates low-rank regularization. Focusing on distribution function responses, we demonstrate that leveraging the low-rank structure of the model parameters enhances both the efficiency and accuracy of model fitting. Through theoretical analysis of the large-sample properties, we show that the proposed method enables more robust modeling and estimation than standard dimension reduction techniques. To support our findings, we also present numerical experiments that evaluate the finite-sample performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04926v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyunghee Han, Hsin-Hsiung Huang</dc:creator>
    </item>
    <item>
      <title>Moments of Causal Effects</title>
      <link>https://arxiv.org/abs/2505.04971</link>
      <description>arXiv:2505.04971v1 Announce Type: new 
Abstract: The moments of random variables are fundamental statistical measures for characterizing the shape of a probability distribution, encompassing metrics such as mean, variance, skewness, and kurtosis. Additionally, the product moments, including covariance and correlation, reveal the relationships between multiple random variables. On the other hand, the primary focus of causal inference is the evaluation of causal effects, which are defined as the difference between two potential outcomes. While traditional causal effect assessment focuses on the average causal effect, this work provides definitions, identification theorems, and bounds for moments and product moments of causal effects to analyze their distribution and relationships. We conduct experiments to illustrate the estimation of the moments of causal effects from finite samples and demonstrate their practical application using a real-world medical dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04971v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuta Kawakami, Jin Tian</dc:creator>
    </item>
    <item>
      <title>Decomposition of Probabilities of Causation with Two Mediators</title>
      <link>https://arxiv.org/abs/2505.04983</link>
      <description>arXiv:2505.04983v1 Announce Type: new 
Abstract: Mediation analysis for probabilities of causation (PoC) provides a fundamental framework for evaluating the necessity and sufficiency of treatment in provoking an event through different causal pathways. One of the primary objectives of causal mediation analysis is to decompose the total effect into path-specific components. In this study, we investigate the path-specific probability of necessity and sufficiency (PNS) to decompose the total PNS into path-specific components along distinct causal pathways between treatment and outcome, incorporating two mediators. We define the path-specific PNS for decomposition and provide an identification theorem. Furthermore, we conduct numerical experiments to assess the properties of the proposed estimators from finite samples and demonstrate their practical application using a real-world educational dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04983v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuta Kawakami, Jin Tian</dc:creator>
    </item>
    <item>
      <title>Bayesian Clustering Factor Models</title>
      <link>https://arxiv.org/abs/2505.05280</link>
      <description>arXiv:2505.05280v1 Announce Type: new 
Abstract: We present a novel framework for concomitant dimension reduction and clustering. This framework is based on a novel class of Bayesian clustering factor models. These models assume a factor model structure where the vectors of common factors follow a mixture of Gaussian distributions. We develop a Gibbs sampler to explore the posterior distribution and propose an information criterion to select the number of clusters and the number of factors. Simulation studies show that our inferential approach appropriately quantifies uncertainty. In addition, when compared to a previously published competitor method, our information criterion has favorable performance in terms of correct selection of number of clusters and number of factors. Finally, we illustrate the capabilities of our framework with an application to data on recovery from opioid use disorder where clustering of individuals may facilitate personalized health care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05280v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hwasoo Shin, Marco A. R. Ferreira, Allison N. Tegge</dc:creator>
    </item>
    <item>
      <title>Safe Individualized Treatment Rules with Controllable Harm Rates</title>
      <link>https://arxiv.org/abs/2505.05308</link>
      <description>arXiv:2505.05308v1 Announce Type: new 
Abstract: Estimating individualized treatment rules (ITRs) is crucial for tailoring interventions in precision medicine. Typical ITR estimation methods rely on conditional average treatment effects (CATEs) to guide treatment assignments. However, such methods overlook individual-level harm within covariate-specific subpopulations, potentially leading many individuals to experience worse outcomes under CATE-based ITRs. In this article, we aim to estimate ITRs that maximize the reward while ensuring that the harm rate induced by the ITR remains below a pre-specified threshold. We derive the explicit form of the optimal ITR, propose a plug-in estimator when both the CATE and harm rate are identifiable, and establish the large-sample properties of the proposed estimator. Given that the harm rate is generally unidentifiable due to its dependence on the joint distribution of potential outcomes, we further propose {three} strategies for estimating ITRs with a controllable harm rate under partial identification. By accounting for both reward and harm, our method offers a reliable solution for developing ITRs in high-stakes domains where harm is crucial. Extensive simulations demonstrate the effectiveness of the proposed methods in controlling harm rates. We apply the method to the Right Heart Catheterization dataset, assessing the potential reduction in harm rate compared to historical interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05308v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peng Wu, Qing Jiang, Shanshan Luo</dc:creator>
    </item>
    <item>
      <title>A Unified Approach to Covariate Adjustment for Survival Endpoints in Randomized Clinical Trials</title>
      <link>https://arxiv.org/abs/2505.05338</link>
      <description>arXiv:2505.05338v1 Announce Type: new 
Abstract: Covariate adjustment aims to improve the statistical efficiency of randomized trials by incorporating information from baseline covariates. Popular methods for covariate adjustment include analysis of covariance for continuous endpoints and standardized logistic regression for binary endpoints. For survival endpoints, while some covariate adjustment methods have been developed for specific effect measures, they are not commonly used in practice for various reasons, including high demands for theoretical and methodological sophistication as well as computational skills. This article describes an augmentation approach to covariate adjustment for survival endpoints that is relatively easy to understand and widely applicable to different effect measures. This approach involves augmenting a given treatment effect estimator in a way that preserves interpretation, consistency, and asymptotic normality. The optimal augmentation term, which minimizes asymptotic variance, can be estimated using various statistical and machine learning methods. Simulation results demonstrate that the augmentation approach can bring substantial gains in statistical efficiency. This approach has been implemented in an R package named \texttt{sleete}, which is described in detail and illustrated with real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05338v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiwei Zhang, Ya Wang, Dong Xi</dc:creator>
    </item>
    <item>
      <title>Variable Selection for Fixed and Random Effects in Multilevel Functional Mixed Effects Models</title>
      <link>https://arxiv.org/abs/2505.05416</link>
      <description>arXiv:2505.05416v1 Announce Type: new 
Abstract: We develop a new method for simultaneously selecting fixed and random effects in a multilevel functional regression model. The proposed method is motivated by accelerometer-derived physical activity data from the 2011-12 cohort of the National Health and Nutrition Examination Survey (NHANES), where we are interested in identifying age and race-specific heterogeneity in covariate effects on the diurnal patterns of physical activity across the lifespan. Existing methods for variable selection in function-on-scalar regression have primarily been designed for fixed effect selection and for single-level functional data. In high-dimensional multilevel functional regression, the presence of cluster-specific heterogeneity in covariate effects could be detected through sparsity in fixed and random effects, and for this purpose, we propose a multilevel functional mixed effects selection (MuFuMES) method. The fixed and random functional effects are modelled using splines, with spike-and-slab group lasso (SSGL) priors on the unknown parameters of interest and a computationally efficient MAP estimation approach is employed for mixed effect selection through an Expectation Conditional Maximization (ECM) algorithm. Numerical analysis using simulation study illustrates the satisfactory selection accuracy of the variable selection method in having a negligible false-positive and false-negative rate. The proposed method is applied to the accelerometer data from the NHANES 2011-12 cohort, where it effectively identifies age and race-specific heterogeneity in covariate effects on the diurnal patterns of physical activity, recovering biologically meaningful insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05416v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rahul Ghosal, Marcos Matabuena, Enakshi Saha</dc:creator>
    </item>
    <item>
      <title>The Poisson tensor completion non-parametric differential entropy estimator</title>
      <link>https://arxiv.org/abs/2505.04957</link>
      <description>arXiv:2505.04957v1 Announce Type: cross 
Abstract: We introduce the Poisson tensor completion (PTC) estimator, a non-parametric differential entropy estimator. The PTC estimator leverages inter-sample relationships to compute a low-rank Poisson tensor decomposition of the frequency histogram. Our crucial observation is that the histogram bins are an instance of a space partitioning of counts and thus can be identified with a spatial Poisson process. The Poisson tensor decomposition leads to a completion of the intensity measure over all bins -- including those containing few to no samples -- and leads to our proposed PTC differential entropy estimator. A Poisson tensor decomposition models the underlying distribution of the count data and guarantees non-negative estimated values and so can be safely used directly in entropy estimation. We believe our estimator is the first tensor-based estimator that exploits the underlying spatial Poisson process related to the histogram explicitly when estimating the probability density with low-rank tensor decompositions or tensor completion. Furthermore, we demonstrate that our PTC estimator is a substantial improvement over standard histogram-based estimators for sub-Gaussian probability distributions because of the concentration of norm phenomenon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04957v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel M. Dunlavy, Richard B. Lehoucq, Carolyn D. Mayer, Arvind Prasadan</dc:creator>
    </item>
    <item>
      <title>Scenario Synthesis and Macroeconomic Risk</title>
      <link>https://arxiv.org/abs/2505.05193</link>
      <description>arXiv:2505.05193v1 Announce Type: cross 
Abstract: We introduce methodology to bridge scenario analysis and model-based risk forecasting, leveraging their respective strengths in policy settings. Our Bayesian framework addresses the fundamental challenge of reconciling judgmental narrative approaches with statistical forecasting. Analysis evaluates explicit measures of concordance of scenarios with a reference forecasting model, delivers Bayesian predictive synthesis of the scenarios to best match that reference, and addresses scenario set incompleteness. This underlies systematic evaluation and integration of risks from different scenarios, and quantifies relative support for scenarios modulo the defined reference forecasts. The framework offers advances in forecasting in policy institutions that supports clear and rigorous communication of evolving risks. We also discuss broader questions of integrating judgmental information with statistical model-based forecasts in the face of unexpected circumstances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05193v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tobias Adrian, Domenico Giannone, Matteo Luciani, Mike West</dc:creator>
    </item>
    <item>
      <title>Exact nonlinear state estimation</title>
      <link>https://arxiv.org/abs/2310.10976</link>
      <description>arXiv:2310.10976v2 Announce Type: replace 
Abstract: The majority of data assimilation (DA) methods in the geosciences are based on Gaussian assumptions. While these assumptions facilitate efficient algorithms, they cause analysis biases and subsequent forecast degradations. Non-parametric, particle-based DA algorithms have superior accuracy, but their application to high-dimensional models still poses operational challenges. Drawing inspiration from recent advances in the field of generative artificial intelligence (AI), this article introduces a new nonlinear estimation theory which attempts to bridge the existing gap in DA methodology. Specifically, a Conjugate Transform Filter (CTF) is derived and shown to generalize the celebrated Kalman filter to arbitrarily non-Gaussian distributions. The new filter has several desirable properties, such as its ability to preserve statistical relationships in the prior state and convergence to highly accurate observations. An ensemble approximation of the new theory (ECTF) is also presented and validated using idealized statistical experiments that feature bounded quantities with non-Gaussian distributions, a prevalent challenge in Earth system models. Results from these experiments indicate that the greatest benefits from ECTF occur when observation errors are small relative to the forecast uncertainty and when state variables exhibit strong nonlinear dependencies. Ultimately, the new filtering theory offers exciting avenues for improving conventional DA algorithms through their principled integration with AI techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.10976v2</guid>
      <category>stat.ME</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>math.DS</category>
      <category>physics.ao-ph</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hristo G. Chipilski</dc:creator>
    </item>
    <item>
      <title>Hypergraph adjusted plus-minus</title>
      <link>https://arxiv.org/abs/2403.20214</link>
      <description>arXiv:2403.20214v3 Announce Type: replace 
Abstract: In team sports, traditional ranking statistics do not allow for the simultaneous evaluation of both individuals and combinations of players. Metrics for individual player rankings often fail to include the interaction effects between groups of players, while methods for assessing full lineups cannot be used to identify the value of lower-order combinations of players (pairs, trios, etc.). Given that player and lineup rankings are inherently dependent on each other, these limitations may affect the accuracy of performance evaluations. To address this, we propose a novel adjusted plus-minus (APM) approach that allows for the simultaneous ranking of individual players, lower-order combinations of players, and full lineups within a team. The method adjusts for the complete dependency structure and is motivated by the connection between APM and the hypergraph representation of a team. We discuss the similarities of our approach to other advanced metrics, demonstrate it using NBA data from 2012-2022, and suggest potential directions for future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20214v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathaniel Josephs, Elizabeth Upton</dc:creator>
    </item>
    <item>
      <title>A Preferential Latent Space Model for Text Networks</title>
      <link>https://arxiv.org/abs/2405.15038</link>
      <description>arXiv:2405.15038v3 Announce Type: replace 
Abstract: Network data enriched with textual information, referred to as text networks, arise in a wide range of applications, including email communications, scientific collaborations, and legal contracts. In such settings, both the structure of interactions (i.e., who connects with whom) and their content (i.e., what is communicated) are useful for understanding network relations. Traditional network analyses often focus only on the structure of the network and discard the rich textual information, resulting in an incomplete or inaccurate view of interactions. In this paper, we introduce a new modeling approach that incorporates texts into the analysis of networks using topic-aware text embedding, representing the text network as a generalized multi-layer network where each layer corresponds to a topic extracted from the data. We develop a new and flexible latent space network model that captures how node-topic preferences directly modulate edge formation, and establish identifiability conditions for the proposed model. We tackle model estimation with a projected gradient descent algorithm, and further discuss its theoretical properties. The efficacy of our proposed method is demonstrated through simulations and an analysis of an email network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15038v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Maoyu Zhang, Biao Cai, Dong Li, Xiaoyue Niu, Jingfei Zhang</dc:creator>
    </item>
    <item>
      <title>Statistical inference for partially shape-constrained function-on-scalar linear regression models</title>
      <link>https://arxiv.org/abs/2407.00859</link>
      <description>arXiv:2407.00859v2 Announce Type: replace 
Abstract: We consider functional linear regression models where functional outcomes are associated with scalar predictors by coefficient functions with shape constraints, such as monotonicity and convexity, that apply to sub-domains of interest. To validate the partial shape constraints, we propose testing a composite hypothesis of linear functional constraints on regression coefficients. Our approach employs kernel- and spline-based methods within a unified inferential framework, evaluating the statistical significance of the hypothesis by measuring an $L^2$-distance between constrained and unconstrained model fits. In the theoretical study of large-sample analysis under mild conditions, we show that both methods achieve the standard rate of convergence observed in the nonparametric estimation literature. Through numerical experiments of finite-sample analysis, we demonstrate that the type I error rate keeps the significance level as specified across various scenarios and that the power increases with sample size, confirming the consistency of the test procedure under both estimation methods. Our theoretical and numerical results provide researchers the flexibility to choose a method based on computational preference. The practicality of partial shape-constrained inference is illustrated by two data applications: one involving clinical trials of NeuroBloc in type A-resistant cervical dystonia and the other with the National Institute of Mental Health Schizophrenia Study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00859v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyunghee Han, Yeonjoo Park, Soo-Young Kim</dc:creator>
    </item>
    <item>
      <title>Win Ratio with Multiple Thresholds for Composite Endpoints</title>
      <link>https://arxiv.org/abs/2407.18341</link>
      <description>arXiv:2407.18341v3 Announce Type: replace 
Abstract: Composite endpoints consisting of both terminal and non-terminal events, such as death and hospitalization, are frequently used as primary endpoints in cardiovascular clinical trials. The Win Ratio method (WR) employs a hierarchical structure to combine fatal and non-fatal events by giving death information an absolute priority, which can adversely affect power if the treatment effect is mainly on the non-fatal outcomes. We hereby propose the Win Ratio with Multiple Thresholds (WR-MT) that releases the strict hierarchical structure of the standard WR by adding stages with non-zero thresholds. A weighted adaptive approach is also developed to determine the thresholds in WR-MT. This method preserves the statistical properties of the standard WR but can sometimes increase the chance to detect treatment effects on non-fatal events. We show that WR-MT has a particularly favorable performance than standard WR when the second layer has stronger signals and otherwise comparable performance in our simulations that vary the follow-up time, the correlation between events, and the treatment effect sizes. A case study based on the Digitalis Investigation Group clinical trial data is presented to further illustrate our proposed method. An R package "WRMT" that implements the proposed methodology has been developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18341v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunhan Mou, Tassos Kyriakides, Scott Hummel, Fan Li, Yuan Huang</dc:creator>
    </item>
    <item>
      <title>Extended-support beta regression for $[0, 1]$ responses</title>
      <link>https://arxiv.org/abs/2409.07233</link>
      <description>arXiv:2409.07233v2 Announce Type: replace 
Abstract: We introduce the XBX regression model, a continuous mixture of extended-support beta regressions for modelling bounded responses with boundary observations. The core building block of XBX regression is the extended-support beta distribution, a censored version of a four-parameter beta distribution with the same exceedance on the left and right of $(0, 1)$. Hence, XBX regression is a direct extension of beta regression. We prove that beta regression and heteroscedastic normal regression with censoring at both $0$ and $1$ -- also known as the heteroscedastic two-limit tobit model in the econometrics literature -- are special cases of extended-support beta regression, depending on whether a single extra parameter is zero or infinity, respectively. To overcome identifiability issues due to the similarity of the beta and normal distributions for certain parameter values, we shrink towards beta regression by letting the extra parameter have an exponential distribution with unknown mean. A Gauss-Laguerre quadrature approximation results in efficient likelihood-based estimation and inference procedures, which the betareg R package implements since version 3.2.0. We use XBX regression to analyze investment decisions in a behavioural economics experiment about the occurrence and extent of loss aversion. In contrast to standard approaches, we capture both the probability of rational behaviour and the mean of loss aversion. Extensive comparisons with alternative approaches illustrate the effectiveness of the new model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07233v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ioannis Kosmidis, Achim Zeileis</dc:creator>
    </item>
    <item>
      <title>Design specification of Partial Ordering Continual Reassessment Method based on consistency conditions</title>
      <link>https://arxiv.org/abs/2410.21989</link>
      <description>arXiv:2410.21989v2 Announce Type: replace 
Abstract: The study of combinations of drugs/drug-schedules gained increasing attention in various therapeutic areas recently. In oncology, the aim of phase I combination clinical trial is to find the maximum tolerated combination (MTC). Many innovative designs were proposed, among which the Partial Ordering Continual Reassessment Method (POCRM) is increasingly applied due to its simplicity and versatility. The POCRM requires specification of plausible monotonic orderings of combinations. However, the choice remains a major difficulty, especially in trials with many compounds or/and combinations. Practical recommendations are given to select six orderings based on statistical considerations, while simulation studies found the design performs poorly when the MTC is in the middle of the combination grid. We prove that the POCRM under currently recommended orderings can be inconsistent (i.e., cannot achieve 100\% correct selection even under infinite samples) which translates into poor performance under small sample size. Based on the derived consistency conditions, we provide two practical recommendations on how to select orderings for real studies (i) based on plausible combination-toxicity scenarios, (ii) regardless of the possible scenarios. We also provide guidance on how to choose other design parameters based on the asymptotic properties and demonstrate how it improves small sample behaviours.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21989v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weishi Chen, Pavel Mozgunov</dc:creator>
    </item>
    <item>
      <title>Summarizing Bayesian Nonparametric Mixture Posterior -- Sliced Optimal Transport Metrics for Gaussian Mixtures</title>
      <link>https://arxiv.org/abs/2411.14674</link>
      <description>arXiv:2411.14674v5 Announce Type: replace 
Abstract: Existing methods to summarize posterior inference for mixture models focus on identifying a point estimate of the implied random partition for clustering, with density estimation as a secondary goal (Wade and Ghahramani, 2018; Dahl et al., 2022). We propose a novel approach for summarizing posterior inference in nonparametric Bayesian mixture models, prioritizing estimation of the mixing measure (or mixture) as an inference target. One of the key features is the model-agnostic nature of the approach, which remains valid under arbitrarily complex dependence structures in the underlying sampling model. Using a decision-theoretic framework, our method identifies a point estimate by minimizing posterior expected loss. A loss function is defined as a discrepancy between mixing measures. Estimating the mixing measure implies inference on the mixture density and the random partition. Exploiting the discrete nature of the mixing measure, we use a version of sliced Wasserstein distance. We introduce two specific variants for Gaussian mixtures. The first, mixed sliced Wasserstein, applies generalized geodesic projections on the product of the Euclidean space and the manifold of symmetric positive definite matrices. The second, sliced mixture Wasserstein, leverages the linearity of Gaussian mixture measures for efficient projection</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14674v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khai Nguyen, Peter Mueller</dc:creator>
    </item>
    <item>
      <title>Improved subsample-and-aggregate via the private modified winsorized mean</title>
      <link>https://arxiv.org/abs/2501.14095</link>
      <description>arXiv:2501.14095v3 Announce Type: replace 
Abstract: We develop a univariate, differentially private mean estimator, called the private modified winsorized mean, designed to be used as the aggregator in subsample-and-aggregate. We demonstrate, via real data analysis, that common differentially private multivariate mean estimators may not perform well as the aggregator, even in large datasets, motivating our developments.We show that the modified winsorized mean is minimax optimal for several, large classes of distributions, even under adversarial contamination. We also demonstrate that, empirically, the private modified winsorized mean performs well compared to other private mean estimates. We consider the modified winsorized mean as the aggregator in subsample-and-aggregate, deriving a finite sample deviations bound for a subsample-and-aggregate estimate generated with the new aggregator. This result yields two important insights: (i) the optimal choice of subsamples depends on the bias of the estimator computed on the subsamples, and (ii) the rate of convergence of the subsample-and-aggregate estimator depends on the robustness of the estimator computed on the subsamples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14095v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kelly Ramsay, Dylan Spicker</dc:creator>
    </item>
    <item>
      <title>Design-Based Inference under Random Potential Outcomes via Riesz Representation</title>
      <link>https://arxiv.org/abs/2505.01324</link>
      <description>arXiv:2505.01324v3 Announce Type: replace 
Abstract: We introduce a general framework for design-based causal inference that accommodates stochastic potential outcomes, thereby extending the classical Neyman-Rubin setup in which outcomes are treated as fixed. In our formulation, each unit's potential outcome is modelled as a function $\tilde{y}_i(z, \omega)$, where $\omega$ denotes latent randomness external to the treatment assignment. Building on recent work that connects design-based estimation with the Riesz representation theorem, we construct causal estimators by embedding potential outcomes in a Hilbert space and defining treatment effects as linear functionals. This allows us to derive unbiased and consistent estimators, even when potential outcomes exhibit random variation. The framework retains the key advantage of design-based analysis, namely, the use of a known randomisation scheme for identification, while enabling inference in settings with inherent stochasticity. We establish large-sample properties under local dependence, provide a variance estimator compatible with sparse dependency structures, and illustrate the method through a simulation. Our results unify design-based reasoning with random-outcome modelling, broadening the applicability of causal inference in complex experimental environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01324v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yukai Yang</dc:creator>
    </item>
    <item>
      <title>A unified analysis of likelihood-based estimators in the Plackett--Luce model</title>
      <link>https://arxiv.org/abs/2306.02821</link>
      <description>arXiv:2306.02821v4 Announce Type: replace-cross 
Abstract: The Plackett--Luce model has been extensively used for rank aggregation in social choice theory. A central statistical question in this model concerns estimating the utility vector that governs the model's likelihood. In this paper, we investigate the asymptotic theory of utility vector estimation by maximizing different types of likelihood, such as full, marginal, and quasi-likelihood. Starting from interpreting the estimating equations of these estimators to gain some initial insights, we analyze their asymptotic behavior as the number of compared objects increases. In particular, we establish both uniform consistency and asymptotic normality of these estimators and discuss the trade-off between statistical efficiency and computational complexity. For generality, our results are proven for deterministic graph sequences under appropriate graph topology conditions. These conditions are shown to be informative when applied to common sampling scenarios, such as nonuniform random hypergraph models and hypergraph stochastic block models. Numerical results are provided to support our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.02821v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruijian Han, Yiming Xu</dc:creator>
    </item>
    <item>
      <title>Free Discontinuity Regression: With an Application to the Economic Effects of Internet Shutdowns</title>
      <link>https://arxiv.org/abs/2309.14630</link>
      <description>arXiv:2309.14630v3 Announce Type: replace-cross 
Abstract: Sharp, multidimensional changepoints-abrupt shifts in a regression surface whose locations and magnitudes are unknown-arise in settings as varied as gene-expression profiling, financial covariance breaks, climate-regime detection, and urban socioeconomic mapping. Despite their prevalence, there are no current approaches that jointly estimate the location and size of the discontinuity set in a one-shot approach with statistical guarantees. We therefore introduce Free Discontinuity Regression (FDR), a fully nonparametric estimator that simultaneously (i) smooths a regression surface, (ii) segments it into contiguous regions, and (iii) provably recovers the precise locations and sizes of its jumps. By extending a convex relaxation of the Mumford-Shah functional to random spatial sampling and correlated noise, FDR overcomes the fixed-grid and i.i.d. noise assumptions of classical image-segmentation approaches, thus enabling its application to real-world data of any dimension. This yields the first identification and uniform consistency results for multivariate jump surfaces: under mild SBV regularity, the estimated function, its discontinuity set, and all jump sizes converge to their true population counterparts. Hyperparameters are selected automatically from the data using Stein's Unbiased Risk Estimate, and large-scale simulations up to three dimensions validate the theoretical results and demonstrate good finite-sample performance. Applying FDR to an internet shutdown in India reveals a 25-35% reduction in economic activity around the estimated shutdown boundaries-much larger than previous estimates. By unifying smoothing, segmentation, and effect-size recovery in a general statistical setting, FDR turns free-discontinuity ideas into a practical tool with formal guarantees for modern multivariate data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.14630v3</guid>
      <category>econ.EM</category>
      <category>cs.CV</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Gunsilius, David Van Dijcke</dc:creator>
    </item>
    <item>
      <title>Perils of Label Indeterminacy: A Case Study on Prediction of Neurological Recovery After Cardiac Arrest</title>
      <link>https://arxiv.org/abs/2504.04243</link>
      <description>arXiv:2504.04243v2 Announce Type: replace-cross 
Abstract: The design of AI systems to assist human decision-making typically requires the availability of labels to train and evaluate supervised models. Frequently, however, these labels are unknown, and different ways of estimating them involve unverifiable assumptions or arbitrary choices. In this work, we introduce the concept of label indeterminacy and derive important implications in high-stakes AI-assisted decision-making. We present an empirical study in a healthcare context, focusing specifically on predicting the recovery of comatose patients after resuscitation from cardiac arrest. Our study shows that label indeterminacy can result in models that perform similarly when evaluated on patients with known labels, but vary drastically in their predictions for patients where labels are unknown. After demonstrating crucial ethical implications of label indeterminacy in this high-stakes context, we discuss takeaways for evaluation, reporting, and design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04243v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>stat.ME</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3715275.3732070</arxiv:DOI>
      <dc:creator>Jakob Schoeffer, Maria De-Arteaga, Jonathan Elmer</dc:creator>
    </item>
    <item>
      <title>Particle Gibbs without the Gibbs bit</title>
      <link>https://arxiv.org/abs/2505.04611</link>
      <description>arXiv:2505.04611v2 Announce Type: replace-cross 
Abstract: Exact parameter and trajectory inference in state-space models is typically achieved by one of two methods: particle marginal Metropolis-Hastings (PMMH) or particle Gibbs (PGibbs). PMMH is a pseudo-marginal algorithm which jointly proposes a new trajectory and parameter, and accepts or rejects both at once. PGibbs instead alternates between sampling from the trajectory, using an algorithm known as conditional sequential Monte Carlo (CSMC) and the parameter in a Hastings-within-Gibbs fashion. While particle independent Metropolis Hastings (PIMH), the parameter-free version of PMMH, is known to be statistically worse than CSMC, PGibbs can induce a slow mixing if the parameter and the state trajectory are very correlated. This has made PMMH the method of choice for many practitioners, despite theory and experiments favouring CSMC over PIMH for the parameter-free problem. In this article, we describe a formulation of PGibbs which bypasses the Gibbs step, essentially marginalizing over the trajectory distribution in a fashion similar to PMMH. This is achieved by considering the implementation of a CSMC algortihm for the state-space model integrated over the joint distribution of the current parameter and the parameter proposal. We illustrate the benefits of method on a simple example known to be challenging for PMMH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04611v2</guid>
      <category>stat.CO</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrien Corenflos</dc:creator>
    </item>
  </channel>
</rss>
