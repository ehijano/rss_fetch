<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Apr 2024 04:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 04 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Societal Implications of Blockchain Proliferation</title>
      <link>https://arxiv.org/abs/2404.02451</link>
      <description>arXiv:2404.02451v1 Announce Type: new 
Abstract: Blockchain and its distributed ledger technology have far-reaching implications for consumers across the world. Cryptocurrencies like XRP work to solve key issues in the remittance industry, targeting corridors like Mexico where foreign remittance fuels economies. Blockchain's libertarian principles have the potential to change lives in the third world, replacing corrupt infrastructure with trust-based solutions. While this technology can be used to significantly improve lives, it has a wealth of destructive applications. Bitcoin's blockchain and nefarious websites like the Silk Road have fueled an underground market of drugs, money laundering, and terrorism, complicating digital currency legislation. The negative environmental effects of cryptocurrency may also contribute significantly to global climate change. Negatives aside, cryptocurrency still proves to be a valuable commodity in technological development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02451v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cory Cherven</dc:creator>
    </item>
    <item>
      <title>Responsible Reporting for Frontier AI Development</title>
      <link>https://arxiv.org/abs/2404.02675</link>
      <description>arXiv:2404.02675v1 Announce Type: new 
Abstract: Mitigating the risks from frontier AI systems requires up-to-date and reliable information about those systems. Organizations that develop and deploy frontier systems have significant access to such information. By reporting safety-critical information to actors in government, industry, and civil society, these organizations could improve visibility into new and emerging risks posed by frontier systems. Equipped with this information, developers could make better informed decisions on risk management, while policymakers could design more targeted and robust regulatory infrastructure. We outline the key features of responsible reporting and propose mechanisms for implementing them in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02675v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noam Kolt, Markus Anderljung, Joslyn Barnhart, Asher Brass, Kevin Esvelt, Gillian K. Hadfield, Lennart Heim, Mikel Rodriguez, Jonas B. Sandbrink, Thomas Woodside</dc:creator>
    </item>
    <item>
      <title>Mixing Individual and Collective Behaviours to Predict Out-of-Routine Mobility</title>
      <link>https://arxiv.org/abs/2404.02740</link>
      <description>arXiv:2404.02740v1 Announce Type: new 
Abstract: Predicting human displacements is crucial for addressing various societal challenges, including urban design, traffic congestion, epidemic management, and migration dynamics. While predictive models like deep learning and Markov models offer insights into individual mobility, they often struggle with out-of-routine behaviours. Our study introduces an approach that dynamically integrates individual and collective mobility behaviours, leveraging collective intelligence to enhance prediction accuracy. Evaluating the model on millions of privacy-preserving trajectories across three US cities, we demonstrate its superior performance in predicting out-of-routine mobility, surpassing even advanced deep learning methods. Spatial analysis highlights the model's effectiveness near urban areas with a high density of points of interest, where collective behaviours strongly influence mobility. During disruptive events like the COVID-19 pandemic, our model retains predictive capabilities, unlike individual-based models. By bridging the gap between individual and collective behaviours, our approach offers transparent and accurate predictions, crucial for addressing contemporary mobility challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02740v1</guid>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastiano Bontorin, Simone Centellegher, Riccardo Gallotti, Luca Pappalardo, Bruno Lepri, Massimiliano Luca</dc:creator>
    </item>
    <item>
      <title>AI and personalized learning: bridging the gap with modern educational goals</title>
      <link>https://arxiv.org/abs/2404.02798</link>
      <description>arXiv:2404.02798v1 Announce Type: new 
Abstract: Personalized learning (PL) aspires to provide an alternative to the one-size-fits-all approach in education. Technology-based PL solutions have shown notable effectiveness in enhancing learning performance. However, their alignment with the broader goals of modern education is inconsistent across technologies and research areas. In this paper, we examine the characteristics of AI-driven PL solutions in light of the OECD Learning Compass 2030 goals. Our analysis indicates a gap between the objectives of modern education and the current direction of PL. We identify areas where most present-day PL technologies could better embrace essential elements of contemporary education, such as collaboration, cognitive engagement, and the development of general competencies. While the present PL solutions are instrumental in aiding learning processes, the PL envisioned by educational experts extends beyond simple technological tools and requires a holistic change in the educational system. Finally, we explore the potential of large language models, such as ChatGPT, and propose a hybrid model that blends artificial intelligence with a collaborative, teacher-facilitated approach to personalized learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02798v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kristjan-Julius Laak, Jaan Aru</dc:creator>
    </item>
    <item>
      <title>Identifying Climate Targets in National Laws and Policies using Machine Learning</title>
      <link>https://arxiv.org/abs/2404.02822</link>
      <description>arXiv:2404.02822v1 Announce Type: new 
Abstract: Quantified policy targets are a fundamental element of climate policy, typically characterised by domain-specific and technical language. Current methods for curating comprehensive views of global climate policy targets entail significant manual effort. At present there are few scalable methods for extracting climate targets from national laws or policies, which limits policymakers' and researchers' ability to (1) assess private and public sector alignment with global goals and (2) inform policy decisions. In this paper we present an approach for extracting mentions of climate targets from national laws and policies. We create an expert-annotated dataset identifying three categories of target ('Net Zero', 'Reduction' and 'Other' (e.g. renewable energy targets)) and train a classifier to reliably identify them in text. We investigate bias and equity impacts related to our model and identify specific years and country names as problematic features. Finally, we investigate the characteristics of the dataset produced by running this classifier on the Climate Policy Radar (CPR) dataset of global national climate laws and policies and UNFCCC submissions, highlighting the potential of automated and scalable data collection for existing climate policy databases and supporting further research. Our work represents a significant upgrade in the accessibility of these key climate policy elements for policymakers and researchers. We publish our model at \url{https://huggingface.co/ClimatePolicyRadar/national-climate-targets} and related dataset at \url{https://huggingface.co/datasets/ClimatePolicyRadar/national-climate-targets}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02822v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matyas Juhasz, Tina Marchand, Roshan Melwani, Kalyan Dutia, Sarah Goodenough, Harrison Pim, Henry Franks</dc:creator>
    </item>
    <item>
      <title>Automated Transparency: A Legal and Empirical Analysis of the Digital Services Act Transparency Database</title>
      <link>https://arxiv.org/abs/2404.02894</link>
      <description>arXiv:2404.02894v1 Announce Type: new 
Abstract: The Digital Services Act (DSA) is a much awaited platforms liability reform in the European Union that was adopted on 1 November 2022 with the ambition to set a global example in terms of accountability and transparency. Among other obligations, the DSA emphasizes the need for online platforms to report on their content moderation decisions (`statements of reasons' - SoRs), which is a novel transparency mechanism we refer to as automated transparency in this study. SoRs are currently made available in the DSA Transparency Database, launched by the European Commission in September 2023. The DSA Transparency Database marks a historical achievement in platform governance, and allows investigations about the actual transparency gains, both at structure level as well as at the level of platform compliance. This study aims to understand whether the Transparency Database helps the DSA to live up to its transparency promises. We use legal and empirical arguments to show that while there are some transparency gains, compliance remains problematic, as the current database structure allows for a lot of discretion from platforms in terms of transparency practices. In our empirical study, we analyze a representative sample of the Transparency Database (131m SoRs) submitted in November 2023, to characterise and evaluate platform content moderation practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02894v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rishabh Kaushal, Jacob van de Kerkhof, Catalina Goanta, Gerasimos Spanakis, Adriana Iamnitchi</dc:creator>
    </item>
    <item>
      <title>Exploring How Multiple Levels of GPT-Generated Programming Hints Support or Disappoint Novices</title>
      <link>https://arxiv.org/abs/2404.02213</link>
      <description>arXiv:2404.02213v1 Announce Type: cross 
Abstract: Recent studies have integrated large language models (LLMs) into diverse educational contexts, including providing adaptive programming hints, a type of feedback focuses on helping students move forward during problem-solving. However, most existing LLM-based hint systems are limited to one single hint type. To investigate whether and how different levels of hints can support students' problem-solving and learning, we conducted a think-aloud study with 12 novices using the LLM Hint Factory, a system providing four levels of hints from general natural language guidance to concrete code assistance, varying in format and granularity. We discovered that high-level natural language hints alone can be helpless or even misleading, especially when addressing next-step or syntax-related help requests. Adding lower-level hints, like code examples with in-line comments, can better support students. The findings open up future work on customizing help responses from content, format, and granularity levels to accurately identify and meet students' learning needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02213v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3613905.3650937</arxiv:DOI>
      <dc:creator>Ruiwei Xiao, Xinying Hou, John Stamper</dc:creator>
    </item>
    <item>
      <title>Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds</title>
      <link>https://arxiv.org/abs/2404.02866</link>
      <description>arXiv:2404.02866v1 Announce Type: cross 
Abstract: Protecting privacy during inference with deep neural networks is possible by adding noise to the activations in the last layers prior to the final classifiers or other task-specific layers. The activations in such layers are known as "features" (or, less commonly, as "embeddings" or "feature embeddings"). The added noise helps prevent reconstruction of the inputs from the noisy features. Lower bounding the variance of every possible unbiased estimator of the inputs quantifies the confidentiality arising from such added noise. Convenient, computationally tractable bounds are available from classic inequalities of Hammersley and of Chapman and Robbins -- the HCR bounds. Numerical experiments indicate that the HCR bounds are on the precipice of being effectual for small neural nets with the data sets, "MNIST" and "CIFAR-10," which contain 10 classes each for image classification. The HCR bounds appear to be insufficient on their own to guarantee confidentiality of the inputs to inference with standard deep neural nets, "ResNet-18" and "Swin-T," pre-trained on the data set, "ImageNet-1000," which contains 1000 classes. Supplementing the addition of noise to features with other methods for providing confidentiality may be warranted in the case of ImageNet. In all cases, the results reported here limit consideration to amounts of added noise that incur little degradation in the accuracy of classification from the noisy features. Thus, the added noise enhances confidentiality without much reduction in the accuracy on the task of image classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02866v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kamalika Chaudhuri, Chuan Guo, Laurens van der Maaten, Saeed Mahloujifar, Mark Tygert</dc:creator>
    </item>
    <item>
      <title>A Model for Integrating Generative AI into Course Content Development</title>
      <link>https://arxiv.org/abs/2308.12276</link>
      <description>arXiv:2308.12276v3 Announce Type: replace 
Abstract: This paper introduces "GAIDE: Generative AI for Instructional Development and Education," a novel framework for using Generative AI (GenAI) to enhance educational content creation. GAIDE stands out by offering a practical approach for educators to produce diverse, engaging, and academically rigorous materials. It integrates GenAI into curriculum design, easing the workload of instructors and elevating material quality. With GAIDE, we present a distinct, adaptable model that harnesses technological progress in education, marking a step towards more efficient instructional development. Motivated by the demand for innovative educational content and the rise of GenAI use among students, this research tackles the challenge of adapting and integrating technology into teaching. GAIDE aims to streamline content development, encourage the creation of dynamic materials, and demonstrate GenAI's utility in instructional design. The framework is grounded in constructivist learning theory and TPCK, emphasizing the importance of integrating technology in a manner that complements pedagogical goals and content knowledge. Our approach aids educators in crafting effective GenAI prompts and guides them through interactions with GenAI tools, both of which are critical for generating high-quality, contextually appropriate content. Initial evaluations indicate GAIDE reduces time and effort in content creation, without compromising on the breadth or depth of the content. Moreover, the use of GenAI has shown promise in deterring conventional cheating methods, suggesting a positive impact on academic integrity and student engagement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.12276v3</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ethan Dickey, Andres Bejarano</dc:creator>
    </item>
    <item>
      <title>"Which LLM should I use?": Evaluating LLMs for tasks performed by Undergraduate Computer Science Students</title>
      <link>https://arxiv.org/abs/2402.01687</link>
      <description>arXiv:2402.01687v2 Announce Type: replace 
Abstract: This study evaluates the effectiveness of various large language models (LLMs) in performing tasks common among undergraduate computer science students. Although a number of research studies in the computing education community have explored the possibility of using LLMs for a variety of tasks, there is a lack of comprehensive research comparing different LLMs and evaluating which LLMs are most effective for different tasks. Our research systematically assesses some of the publicly available LLMs such as Google Bard, ChatGPT(3.5), GitHub Copilot Chat, and Microsoft Copilot across diverse tasks commonly encountered by undergraduate computer science students in India. These tasks include code explanation and documentation, solving class assignments, technical interview preparation, learning new concepts and frameworks, and email writing. Evaluation for these tasks was carried out by pre-final year and final year undergraduate computer science students and provides insights into the models' strengths and limitations. This study aims to guide students as well as instructors in selecting suitable LLMs for any specific task and offers valuable insights on how LLMs can be used constructively by students and instructors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01687v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vibhor Agarwal, Madhav Krishan Garg, Sahiti Dharmavaram, Dhruv Kumar</dc:creator>
    </item>
    <item>
      <title>On-Demand Sampling: Learning Optimally from Multiple Distributions</title>
      <link>https://arxiv.org/abs/2210.12529</link>
      <description>arXiv:2210.12529v3 Announce Type: replace-cross 
Abstract: Social and real-world considerations such as robustness, fairness, social welfare and multi-agent tradeoffs have given rise to multi-distribution learning paradigms, such as collaborative learning, group distributionally robust optimization, and fair federated learning. In each of these settings, a learner seeks to uniformly minimize its expected loss over $n$ predefined data distributions, while using as few samples as possible. In this paper, we establish the optimal sample complexity of these learning paradigms and give algorithms that meet this sample complexity. Importantly, our sample complexity bounds for multi-distribution learning exceed that of learning a single distribution by only an additive factor of $n \log(n) / \epsilon^2$. This improves upon the best known sample complexity bounds for fair federated learning by Mohri et al. and collaborative learning by Nguyen and Zakynthinou by multiplicative factors of $n$ and $\log(n)/\epsilon^3$, respectively. We also provide the first sample complexity bounds for the group DRO objective of Sagawa et al. To guarantee these optimal sample complexity bounds, our algorithms learn to sample from data distributions on demand. Our algorithm design and analysis are enabled by our extensions of online learning techniques for solving stochastic zero-sum games. In particular, we contribute stochastic variants of no-regret dynamics that can trade off between players' differing sampling costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.12529v3</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nika Haghtalab, Michael I. Jordan, Eric Zhao</dc:creator>
    </item>
    <item>
      <title>Candidate Incentive Distributions: How voting methods shape electoral incentives</title>
      <link>https://arxiv.org/abs/2306.07147</link>
      <description>arXiv:2306.07147v2 Announce Type: replace-cross 
Abstract: We evaluate the tendency for different voting methods to promote political compromise and reduce tensions in a society by using computer simulations to determine which voters candidates are incentivized to appeal to. We find that Instant Runoff Voting incentivizes candidates to appeal to a wider range of voters than Plurality Voting, but that it leaves candidates far more strongly incentivized to appeal to their base than to voters in opposing factions. In contrast, we find that Condorcet methods and STAR (Score Then Automatic Runoff) Voting provide the most balanced incentives; these differences between voting methods become more pronounced with more candidates are in the race and less pronounced in the presence of strategic voting. We find that the incentives provided by Single Transferable Vote to appeal to opposing voters are negligible, but that a tweak to the tabulation algorithm makes them substantial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.07147v2</guid>
      <category>econ.GN</category>
      <category>cs.CY</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcus Ogren</dc:creator>
    </item>
    <item>
      <title>Ecosystem-level Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes</title>
      <link>https://arxiv.org/abs/2307.05862</link>
      <description>arXiv:2307.05862v2 Announce Type: replace-cross 
Abstract: Machine learning is traditionally studied at the model level: researchers measure and improve the accuracy, robustness, bias, efficiency, and other dimensions of specific models. In practice, the societal impact of machine learning is determined by the surrounding context of machine learning deployments. To capture this, we introduce ecosystem-level analysis: rather than analyzing a single model, we consider the collection of models that are deployed in a given context. For example, ecosystem-level analysis in hiring recognizes that a job candidate's outcomes are not only determined by a single hiring algorithm or firm but instead by the collective decisions of all the firms they applied to. Across three modalities (text, images, speech) and 11 datasets, we establish a clear trend: deployed machine learning is prone to systemic failure, meaning some users are exclusively misclassified by all models available. Even when individual models improve at the population level over time, we find these improvements rarely reduce the prevalence of systemic failure. Instead, the benefits of these improvements predominantly accrue to individuals who are already correctly classified by other models. In light of these trends, we consider medical imaging for dermatology where the costs of systemic failure are especially high. While traditional analyses reveal racial performance disparities for both models and humans, ecosystem-level analysis reveals new forms of racial disparity in model predictions that do not present in human predictions. These examples demonstrate ecosystem-level analysis has unique strengths for characterizing the societal impact of machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.05862v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Connor Toups, Rishi Bommasani, Kathleen A. Creel, Sarah H. Bana, Dan Jurafsky, Percy Liang</dc:creator>
    </item>
    <item>
      <title>Comuniqa : Exploring Large Language Models for improving speaking skills</title>
      <link>https://arxiv.org/abs/2401.15595</link>
      <description>arXiv:2401.15595v2 Announce Type: replace-cross 
Abstract: In this paper, we investigate the potential of Large Language Models (LLMs) to improve English speaking skills. This is particularly relevant in countries like India, where English is crucial for academic, professional, and personal communication but remains a non-native language for many. Traditional methods for enhancing speaking skills often rely on human experts, which can be limited in terms of scalability, accessibility, and affordability. Recent advancements in Artificial Intelligence (AI) offer promising solutions to overcome these limitations.
  We propose Comuniqa, a novel LLM-based system designed to enhance English speaking skills. We adopt a human-centric evaluation approach, comparing Comuniqa with the feedback and instructions provided by human experts. In our evaluation, we divide the participants in three groups: those who use LLM-based system for improving speaking skills, those guided by human experts for the same task and those who utilize both the LLM-based system as well as the human experts. Using surveys, interviews, and actual study sessions, we provide a detailed perspective on the effectiveness of different learning modalities. Our preliminary findings suggest that while LLM-based systems have commendable accuracy, they lack human-level cognitive capabilities, both in terms of accuracy and empathy. Nevertheless, Comuniqa represents a significant step towards achieving Sustainable Development Goal 4: Quality Education by providing a valuable learning tool for individuals who may not have access to human experts for improving their speaking skills.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15595v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manas Mhasakar, Shikhar Sharma, Apurv Mehra, Utkarsh Venaik, Ujjwal Singhal, Dhruv Kumar, Kashish Mittal</dc:creator>
    </item>
  </channel>
</rss>
