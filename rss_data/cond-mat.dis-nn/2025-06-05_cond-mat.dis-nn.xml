<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Jun 2025 04:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Overlap Gap and Computational Thresholds in the Square Wave Perceptron</title>
      <link>https://arxiv.org/abs/2506.05197</link>
      <description>arXiv:2506.05197v1 Announce Type: new 
Abstract: Square Wave Perceptrons (SWPs) form a class of neural network models with oscillating activation function that exhibit intriguing ``hardness'' properties in the high-dimensional limit at a fixed signal-to-noise ratio $\alpha = O(1)$. In this work, we examine two key aspects of these models. The first is related to the so-called overlap-gap property, that is a disconnectivity feature of the geometry of the solution space of combinatorial optimization problems proven to cause the failure of a large family of solvers, and conjectured to be a symptom of algorithmic hardness. We identify, both in the storage and in the teacher-student settings, the emergence of an overlap gap at a threshold $\alpha_{\mathrm{OGP}}(\delta)$, which can be made arbitrarily small by suitably increasing the frequency of oscillations $1/\delta$ of the activation. This suggests that in this small-$\delta$ regime, typical instances of the problem are hard to solve even for small values of $\alpha$. Second, in the teacher-student setup, we show that the recovery threshold of the planted signal for message-passing algorithms can be made arbitrarily large by reducing $\delta$. These properties make SWPs both a challenging benchmark for algorithms and an interesting candidate for cryptographic applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05197v1</guid>
      <category>cond-mat.dis-nn</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Benedetti, Andrej Bogdanov, Enrico M. Malatesta, Marc M\'ezard, Gianmarco Perrupato, Alon Rosen, Nikolaj I. Schwartzbach, Riccardo Zecchina</dc:creator>
    </item>
    <item>
      <title>Transient dynamics of associative memory models</title>
      <link>https://arxiv.org/abs/2506.05303</link>
      <description>arXiv:2506.05303v1 Announce Type: new 
Abstract: Associative memory models such as the Hopfield network and its dense generalizations with higher-order interactions exhibit a "blackout catastrophe"--a discontinuous transition where stable memory states abruptly vanish when the number of stored patterns exceeds a critical capacity. This transition is often interpreted as rendering networks unusable beyond capacity limits. We argue that this interpretation is largely an artifact of the equilibrium perspective. We derive dynamical mean-field equations using a bipartite cavity approach for graded-activity dense associative memory models, with the Hopfield model as a special case, and solve them using a numerical scheme. We show that patterns can be transiently retrieved with high accuracy above capacity despite the absence of stable attractors. This occurs because slow regions persist in the above-capacity energy landscape as shallow, unstable remnants of below-capacity stable basins. The same transient-retrieval effect occurs in below-capacity networks initialized outside basins of attraction. "Transient-recovery curves" provide a concise visual summary of these effects, revealing graceful, non-catastrophic changes in retrieval behavior above capacity and allowing us to compare the behavior across interaction orders. This dynamical perspective reveals rich energy landscape structure obscured by equilibrium analysis and suggests biological neural circuits may exploit transient dynamics for memory retrieval. Furthermore, our approach suggests ways of understanding computational properties of neural circuits without reference to fixed points, advances the technical repertoire of numerical mean-field solution methods for recurrent neural networks, and yields new theoretical results on generalizations of the Hopfield model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05303v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David G. Clark</dc:creator>
    </item>
    <item>
      <title>On the Potential of Microtubules for Scalable Quantum Computation</title>
      <link>https://arxiv.org/abs/2505.20364</link>
      <description>arXiv:2505.20364v1 Announce Type: cross 
Abstract: We examine the quantum coherence properties of tubulin heterodimers in the Microtubule (MT) lattice. In the cavity-MT model proposed by the authors, according to which the MT interiors are modeled as high-Q quantum-electrodynamics cavities, decoherence-resistant entangled states have been argued to emerge under physiological conditions, with decoherence times of order $\mathcal{O}(10^{-6})$ s. The latter is the result of strong electric-dipole interactions of tubulin dimers with ordered-water dipole quanta in the MT interior. We re-interpret the classical nonlinear (pseudospin) $\sigma$-models, describing the emergent dynamics of solitonic excitations in such systems, as representing quantum coherent (or possibly pointer) states, arising from the incomplete collapse of quantum-coherent dipole states. These solitons mediate dissipation-free energy transfer across the MT networks. We underpin logic-gate-like behavior through MT-associated proteins and detail how these structures may support scalable, ambient-temperature quantum computation, with the fundamental unit of information storage being a quDit associated with the basic unit of the MT honeycomb lattice. We describe in detail the decision-making process, after the action of an external stimulus, during which optimal path selection for energy-loss-free signal and information transport across the MT network emerges. Finally, we propose experimental pathways, including Rabi-splitting spectroscopy and entangled surface plasmon probes, to experimentally validate our predictions for MT-based, scalable quantum computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20364v1</guid>
      <category>physics.bio-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>quant-ph</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>KCL-PH-TH/2025-18</arxiv:journal_reference>
      <dc:creator>Nick E. Mavromatos, Andreas Mershin, Dimitri V. Nanopoulos</dc:creator>
    </item>
    <item>
      <title>Experimental Detection of Dissipative Quantum Chaos</title>
      <link>https://arxiv.org/abs/2506.04325</link>
      <description>arXiv:2506.04325v1 Announce Type: cross 
Abstract: More than four decades of research on chaos in isolated quantum systems have led to the identification of universal signatures -- such as level repulsion and eigenstate thermalization -- that serve as cornerstones in our understanding of complex quantum dynamics. The emerging field of dissipative quantum chaos explores how these properties manifest in open quantum systems, where interactions with the environment play an essential role. We report the first experimental detection of dissipative quantum chaos and integrability by measuring the complex spacing ratios (CSRs) of open many-body quantum systems implemented on a high-fidelity superconducting quantum processor. Employing gradient-based tomography, we retrieve a ``donut-shaped'' CSR distribution for chaotic dissipative circuits, a hallmark of level repulsion in open quantum systems. For an integrable circuit, spectral correlations vanish, evidenced by a sharp peak at the origin in the CSR distribution. As we increase the depth of the integrable dissipative circuit, the CSR distribution undergoes an integrability-to-chaos crossover, demonstrating that intrinsic noise in the quantum processor is a dissipative chaotic process. Our results reveal the universal spectral features of dissipative many-body systems and establish present-day quantum computation platforms, which are predominantly used to run unitary simulations, as testbeds to explore dissipative many-body phenomena.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04325v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>nlin.CD</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kristian Wold, Zitian Zhu, Feitong Jin, Xuhao Zhu, Zehang Bao, Jiarun Zhong, Fanhao Shen, Pengfei Zhang, Hekang Li, Zhen Wang, Chao Song, Qiujiang Guo, Sergey Denisov, Lucas S\'a, H. Wang, Pedro Ribeiro</dc:creator>
    </item>
    <item>
      <title>Hybrid between biologically inspired and quantum inspired many-body states</title>
      <link>https://arxiv.org/abs/2506.05050</link>
      <description>arXiv:2506.05050v1 Announce Type: cross 
Abstract: Deep neural networks can represent very different sorts of functions, including complex quantum many-body states. Tensor networks can also represent these states, have more structure and are easier to optimize. However, they can be prohibitively costly computationally in two or higher dimensions. Here, we propose a generalization of the perceptron - the perceptrain - which borrows features from the two different formalisms. We construct variational many-body ansatz from a simple network of perceptrains. The network can be thought of as a neural network with a few distinct features inherited from tensor networks. These include efficient local optimization akin to the density matrix renormalization algorithm, instead of optimizing of all the parameters at once; the possibility to dynamically increase the number of parameters during the optimization; the possibility to compress the state to avoid overfitting; and a structure that remains quantum-inspired. We showcase the ansatz using a combination of Variational Monte-Carlo (VMC) and Green Function Monte-Carlo (GFMC) on a $10\times 10$ transverse field quantum Ising model with a long range $1/r^6$ antiferromagnetic interaction. The model corresponds to the Rydberg (cold) atoms platform proposed for quantum annealing. We consistently find a very high relative accuracy for the ground state energy, around $10^{-5}$ for VMC and $10^{-6}$ for GFMC in all regimes of parameters, including in the vicinity of the quantum phase transition. We used very small ranks ($\sim 2-5$) of perceptrains, as opposed to multiples of thousand used in matrix product states. The optimization of the energy was robust with respect to the choice of initial conditions and hyper-parameters, in contrast to a common experience when using neural network wave functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05050v1</guid>
      <category>cond-mat.str-el</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.quant-gas</category>
      <category>quant-ph</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Miha Srdin\v{s}ek, Xavier Waintal</dc:creator>
    </item>
    <item>
      <title>Associative Memory and Generative Diffusion in the Zero-noise Limit</title>
      <link>https://arxiv.org/abs/2506.05178</link>
      <description>arXiv:2506.05178v1 Announce Type: cross 
Abstract: Connections between generative diffusion and continuous-state associative memory models are studied. Morse-Smale dynamical systems are emphasized as universal approximators of gradient-based associative memory models and diffusion models as white-noise perturbed systems thereof. Universal properties of associative memory that follow from this description are described and used to characterize a generic transition from generation to memory as noise levels diminish. Structural stability inherited by Morse-Smale flows is shown to imply a notion of stability for diffusions at vanishing noise levels. Applied to one- and two-parameter families of gradients, this indicates stability at all but isolated points of associative memory learning landscapes and the learning and generation landscapes of diffusion models with gradient drift in the zero-noise limit, at which small sets of generic bifurcations characterize qualitative transitions between stable systems. Examples illustrating the characterization of these landscapes by sequences of these bifurcations are given, along with structural stability criterion for classic and modern Hopfield networks (equivalently, the attention mechanism).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05178v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>math.DS</category>
      <category>nlin.AO</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Hess, Quaid Morris</dc:creator>
    </item>
    <item>
      <title>How to Train Your Dragon: Quantum Neural Networks</title>
      <link>https://arxiv.org/abs/2506.05244</link>
      <description>arXiv:2506.05244v1 Announce Type: cross 
Abstract: Training of neural networks (NNs) has emerged as a major consumer of both computational and energy resources. We demonstrate that quantum annealing platforms, such as D-Wave, can enable fast and efficient training of classical NNs, which are then deployable on conventional hardware. From a physics perspective, NN training can be viewed as a dynamical phase transition: the system evolves from an initial spin glass state to a highly ordered, trained state. This process involves eliminating numerous undesired minima in its energy landscape--akin to cutting off the ever-regenerating heads of a dragon. The advantage of annealing devices is their ability to rapidly find multiple deep states (dragon heads to be cut). We found that this quantum-assisted training achieves superior performance scaling compared to classical backpropagation methods, with a notably higher scaling exponent (1.01 vs. 0.78). It may be further increased up to a factor of 2 with a fully coherent quantum platform using a variant of the Grover algorithm. Furthermore, we argue that even a modestly sized annealer can be beneficial to train a deep NN by being applied sequentially to a few layers at a time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05244v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Zhang, Alex Kamenev</dc:creator>
    </item>
    <item>
      <title>An analytic theory of creativity in convolutional diffusion models</title>
      <link>https://arxiv.org/abs/2412.20292</link>
      <description>arXiv:2412.20292v2 Announce Type: replace-cross 
Abstract: We obtain an analytic, interpretable and predictive theory of creativity in convolutional diffusion models. Indeed, score-matching diffusion models can generate highly original images that lie far from their training data. However, optimal score-matching theory suggests that these models should only be able to produce memorized training examples. To reconcile this theory-experiment gap, we identify two simple inductive biases, locality and equivariance, that: (1) induce a form of combinatorial creativity by preventing optimal score-matching; (2) result in fully analytic, completely mechanistically interpretable, local score (LS) and equivariant local score (ELS) machines that, (3) after calibrating a single time-dependent hyperparameter can quantitatively predict the outputs of trained convolution only diffusion models (like ResNets and UNets) with high accuracy (median $r^2$ of $0.95, 0.94, 0.94, 0.96$ for our top model on CIFAR10, FashionMNIST, MNIST, and CelebA). Our model reveals a locally consistent patch mosaic mechanism of creativity, in which diffusion models create exponentially many novel images by mixing and matching different local training set patches at different scales and image locations. Our theory also partially predicts the outputs of pre-trained self-attention enabled UNets (median $r^2 \sim 0.77$ on CIFAR10), revealing an intriguing role for attention in carving out semantic coherence from local patch mosaics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20292v2</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mason Kamb, Surya Ganguli</dc:creator>
    </item>
    <item>
      <title>A theoretical framework for overfitting in energy-based modeling</title>
      <link>https://arxiv.org/abs/2501.19158</link>
      <description>arXiv:2501.19158v3 Announce Type: replace-cross 
Abstract: We investigate the impact of limited data on training pairwise energy-based models for inverse problems aimed at identifying interaction networks. Utilizing the Gaussian model as testbed, we dissect training trajectories across the eigenbasis of the coupling matrix, exploiting the independent evolution of eigenmodes and revealing that the learning timescales are tied to the spectral decomposition of the empirical covariance matrix. We see that optimal points for early stopping arise from the interplay between these timescales and the initial conditions of training. Moreover, we show that finite data corrections can be accurately modeled through asymptotic random matrix theory calculations and provide the counterpart of generalized cross-validation in the energy based model context. Our analytical framework extends to binary-variable maximum-entropy pairwise models with minimal variations. These findings offer strategies to control overfitting in discrete-variable models through empirical shrinkage corrections, improving the management of overfitting in energy-based generative models. Finally, we propose a generalization to arbitrary energy-based models by deriving the neural tangent kernel dynamics of the score function under the score-matching algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19158v3</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giovanni Catania, Aur\'elien Decelle, Cyril Furtlehner, Beatriz Seoane</dc:creator>
    </item>
  </channel>
</rss>
