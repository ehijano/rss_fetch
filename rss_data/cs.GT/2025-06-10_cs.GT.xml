<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Jun 2025 06:55:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Delegation with Costly Inspection</title>
      <link>https://arxiv.org/abs/2506.07162</link>
      <description>arXiv:2506.07162v1 Announce Type: new 
Abstract: We study the problem of delegated choice with inspection cost (DCIC), which is a variant of the delegated choice problem by Kleinberg and Kleinberg (EC'18) as well as an extension of the Pandora's box problem with nonobligatory inspection (PNOI) by Doval (JET'18). In our model, an agent may strategically misreport the proposed element's utility, unlike the standard delegated choice problem which assumes that the agent truthfully reports the utility for the proposed alternative. Thus, the principal needs to inspect the proposed element possibly along with other alternatives to maximize its own utility, given an exogenous cost of inspecting each element. Further, the delegation itself incurs a fixed cost, thus the principal can decide whether to delegate or not and inspect by herself.
  We show that DCIC indeed is a generalization of PNOI where the side information from a strategic agent is available at certain cost, implying its NP-hardness by Fu, Li, and Liu (STOC'23). We first consider a costless delegation setting in which the cost of delegation is free. We prove that the maximal mechanism over the pure delegation with a single inspection and an PNOI policy without delegation achieves a $3$-approximation for DCIC with costless delegation, which is further proven to be tight. These results hold even when the cost comes from an arbitrary monotone set function, and can be improved to a $2$-approximation if the cost of inspection is the same for every element. We extend these techniques by presenting a constant factor approximate mechanism for the general setting for rich class of instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07162v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>econ.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad T. Hajiaghayi, Piotr Krysta, Mohammad Mahdavi, Suho Shin</dc:creator>
    </item>
    <item>
      <title>Value-Set Iteration: Computing Optimal Correlated Equilibria in Infinite-Horizon Multi-Player Stochastic Games</title>
      <link>https://arxiv.org/abs/2506.07186</link>
      <description>arXiv:2506.07186v1 Announce Type: new 
Abstract: We study the problem of computing optimal correlated equilibria (CEs) in infinite-horizon multi-player stochastic games, where correlation signals are provided over time. In this setting, optimal CEs require history-dependent policies; this poses new representational and algorithmic challenges as the number of possible histories grows exponentially with the number of time steps. We focus on computing $(\epsilon, \delta)$-optimal CEs -- solutions that achieve a value within $\epsilon$ of an optimal CE, while allowing the agents' incentive constraints to be violated by at most $\delta$. Our main result is an algorithm that computes an $(\epsilon,\delta)$-optimal CE in time polynomial in $1/(\epsilon\delta(1 - \gamma))^{n+1}$, where $\gamma$ is the discount factor, and $n$ is the number of agents. For (a slightly more general variant of) turn-based games, we further reduce the complexity to a polynomial in $n$. We also establish that the bi-criterion approximation is necessary by proving matching inapproximability bounds.
  Our technical core is a novel approach based on inducible value sets, which leverages a compact representation of history-dependent CEs through the values they induce to overcome the representational challenge. We develop the value-set iteration algorithm -- which operates by iteratively updating estimates of inducible value sets -- and characterize CEs as the greatest fixed point of the update map. Our algorithm provides a groundwork for computing optimal CEs in general multi-player stochastic settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07186v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiarui Gan, Rupak Majumdar</dc:creator>
    </item>
    <item>
      <title>Vulnerability and Defence: A Case for Stackelberg Game Dynamics</title>
      <link>https://arxiv.org/abs/2506.07316</link>
      <description>arXiv:2506.07316v1 Announce Type: new 
Abstract: This paper examines the tactical interaction between drones and tanks in modern warfare through game theory, particularly focusing on Stackelberg equilibrium and backward induction. It describes a high-stakes conflict between two teams: one using advanced drones for attack, and the other defending using tanks. The paper conceptualizes this as a sequential game, illustrating the complex strategic dynamics similar to Stackelberg competition, where moves and countermoves are carefully analyzed and predicted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07316v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.3390/g15050032</arxiv:DOI>
      <arxiv:journal_reference>Games, Vol. 15, Issue 5, Art. No. 32 (2024)</arxiv:journal_reference>
      <dc:creator>Azhar Iqbal, Ishan Honhaga, Eyoel Teffera, Anthony Perry, Robin Baker, Glenn Pearce, Claudia Szabo</dc:creator>
    </item>
    <item>
      <title>On the Interplay of Privacy, Persuasion and Quantization</title>
      <link>https://arxiv.org/abs/2506.06321</link>
      <description>arXiv:2506.06321v1 Announce Type: cross 
Abstract: We develop a communication-theoretic framework for privacy-aware and resilient decision making in cyber-physical systems under misaligned objectives between the encoder and the decoder. The encoder observes two correlated signals ($X$,$\theta$) and transmits a finite-rate message $Z$ to aid a legitimate controller (the decoder) in estimating $X+\theta$, while an eavesdropper intercepts $Z$ to infer the private parameter $\theta$. Unlike conventional setups where encoder and decoder share a common MSE objective, here the encoder minimizes a Lagrangian that balances legitimate control fidelity and the privacy leakage about $\theta$. In contrast, the decoder's goal is purely to minimize its own estimation error without regard for privacy. We analyze fully, partially, and non-revealing strategies that arise from this conflict, and characterize optimal linear encoders when the rate constraints are lifted. For finite-rate channels, we employ gradient-based methods to compute the optimal controllers. Numerical experiments illustrate how tuning the privacy parameter shapes the trade-off between control performance and resilience against unauthorized inferences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06321v1</guid>
      <category>eess.SP</category>
      <category>cs.GT</category>
      <category>cs.IT</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.IT</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anju Anand, Emrah Akyol</dc:creator>
    </item>
    <item>
      <title>Evolutionary model for energy trading in community microgrids using Hawk-Dove strategies</title>
      <link>https://arxiv.org/abs/2506.06325</link>
      <description>arXiv:2506.06325v1 Announce Type: cross 
Abstract: This paper proposes a decentralized model of energy cooperation between microgrids, in which decisions are made locally, at the level of the microgrid community. Each microgrid is modeled as an autonomous agent that adopts a Hawk or Dove strategy, depending on the level of energy stored in the battery and its role in the energy trading process. The interactions between selling and buying microgrids are modeled through an evolutionary algorithm. An individual in the algorithm population is represented as an energy trading matrix that encodes the amounts of energy traded between the selling and buying microgrids. The population evolution is achieved by recombination and mutation operators. Recombination uses a specialized operator for matrix structures, and mutation is applied to the matrix elements according to a Gaussian distribution. The evaluation of an individual is made with a multi-criteria fitness function that considers the seller profit, the degree of energy stability at the community level, penalties for energy imbalance at the community level and for the degradation of microgrids batteries. The method was tested on a simulated scenario with 100 microgrids, each with its own selling and buying thresholds, to reflect a realistic environment with variable storage characteristics of microgrids batteries. By applying the algorithm on this scenario, 95 out of the 100 microgrids reached a stable energy state. This result confirms the effectiveness of the proposed model in achieving energy balance both at the individual level, for each microgrid, and at the level of the entire community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06325v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Viorica Rozina Chifu, Tudor Cioara, Cristina Bianca Pop, Ionut Anghel</dc:creator>
    </item>
    <item>
      <title>On the Fundamental Impossibility of Hallucination Control in Large Language Models</title>
      <link>https://arxiv.org/abs/2506.06382</link>
      <description>arXiv:2506.06382v1 Announce Type: cross 
Abstract: This paper explains \textbf{why it is impossible to create large language models that do not hallucinate and what are the trade-offs we should be looking for}. It presents a formal \textbf{impossibility theorem} demonstrating that no inference mechanism can simultaneously satisfy four fundamental properties: \textbf{truthful (non-hallucinatory) generation, semantic information conservation, relevant knowledge revelation, and knowledge-constrained optimality}. By modeling LLM inference as an \textbf{auction of ideas} where neural components compete to contribute to responses, we prove the impossibility using the Green-Laffont theorem. That mathematical framework provides a rigorous foundation for understanding the nature of inference process, with implications for model architecture, training objectives, and evaluation methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06382v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Micha{\l} P. Karpowicz</dc:creator>
    </item>
    <item>
      <title>Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures</title>
      <link>https://arxiv.org/abs/2506.06832</link>
      <description>arXiv:2506.06832v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) define probability measures on text. By considering the implicit knowledge question of what it means for an LLM to know such a measure and what it entails algorithmically, we are naturally led to formulate a series of tasks that go beyond generative sampling, involving forms of summarization, counterfactual thinking, anomaly detection, originality search, reverse prompting, debating, creative solving, etc. These tasks can be formulated as games based on LLM measures, which we call Cross-Entropy (Xent) Games. Xent Games can be single-player or multi-player. They involve cross-entropy scores and cross-entropy constraints, and can be expressed as simple computational graphs and programs. We show the Xent Game space is large enough to contain a wealth of interesting examples, while being constructible from basic game-theoretic consistency axioms. We then discuss how the Xent Game space can be used to measure the abilities of LLMs. This leads to the construction of Xent Game measures: finite families of Xent Games that can be used as capability benchmarks, built from a given scope, by extracting a covering measure. To address the unbounded scope problem associated with the challenge of measuring general abilities, we propose to explore the space of Xent Games in a coherent fashion, using ideas inspired by evolutionary dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06832v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.GT</category>
      <category>cs.IT</category>
      <category>cs.NE</category>
      <category>math.IT</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cl\'ement Hongler, Andrew Emil</dc:creator>
    </item>
    <item>
      <title>Online Job Assignment</title>
      <link>https://arxiv.org/abs/2506.06893</link>
      <description>arXiv:2506.06893v1 Announce Type: cross 
Abstract: Motivated primarily by applications in cloud computing, we study a simple, yet powerful, online allocation problem in which jobs of varying durations arrive over continuous time and must be assigned immediately and irrevocably to one of the available offline servers. Each server has a fixed initial capacity, with assigned jobs occupying one unit for their duration and releasing it upon completion. The algorithm earns a reward for each assignment upon completion. We consider a general heterogeneous setting where both the reward and duration of a job depend on the job-server pair. The objective of the online algorithm is to maximize the total collected reward, and remain competitive against an omniscient benchmark that knows all job arrivals in advance. Our main contribution is the design of a new online algorithm, termed Forward-Looking BALANCE (FLB), and using primal-dual framework to establish that it is (asymptotically) optimal-competitive.
  This meta-algorithm has two main primitives: (i) keeping track of the capacity used for each server at each time and applying a penalty function to this quantity, and (ii) adjusting the reward of assigning a job to a server by subtracting the total penalty of a particularly chosen subset of future times, in contrast to just looking at the current time. The FLB algorithm then assigns the arriving job to the server with the maximum adjusted reward. If R and D are the ratios of maximum over minimum rewards and durations, we show that the FLB algorithm obtains an asymptotic competitive ratio of ln(RD)+3lnln(max(R,D))+O(1). We further show this bound has optimal dependencies on all the parameters. Our main analysis combines a novel dual-fitting technique, which leverages the configuration LP benchmark for this problem, and a novel inductive argument to establish the capacity feasibility of the algorithm, which might be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06893v1</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Farbod Ekbatani, Yiding Feng, Ian Kash, Rad Niazadeh</dc:creator>
    </item>
    <item>
      <title>Evaluating LLM-corrupted Crowdsourcing Data Without Ground Truth</title>
      <link>https://arxiv.org/abs/2506.06991</link>
      <description>arXiv:2506.06991v1 Announce Type: cross 
Abstract: The recent success of generative AI highlights the crucial role of high-quality human feedback in building trustworthy AI systems. However, the increasing use of large language models (LLMs) by crowdsourcing workers poses a significant challenge: datasets intended to reflect human input may be compromised by LLM-generated responses. Existing LLM detection approaches often rely on high-dimension training data such as text, making them unsuitable for annotation tasks like multiple-choice labeling. In this work, we investigate the potential of peer prediction -- a mechanism that evaluates the information within workers' responses without using ground truth -- to mitigate LLM-assisted cheating in crowdsourcing with a focus on annotation tasks. Our approach quantifies the correlations between worker answers while conditioning on (a subset of) LLM-generated labels available to the requester. Building on prior research, we propose a training-free scoring mechanism with theoretical guarantees under a crowdsourcing model that accounts for LLM collusion. We establish conditions under which our method is effective and empirically demonstrate its robustness in detecting low-effort cheating on real-world crowdsourcing datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06991v1</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.HC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yichi Zhang, Jinlong Pang, Zhaowei Zhu, Yang Liu</dc:creator>
    </item>
    <item>
      <title>Diffusion of Responsibility in Collective Decision Making</title>
      <link>https://arxiv.org/abs/2506.07935</link>
      <description>arXiv:2506.07935v1 Announce Type: cross 
Abstract: The term "diffusion of responsibility'' refers to situations in which multiple agents share responsibility for an outcome, obscuring individual accountability. This paper examines this frequently undesirable phenomenon in the context of collective decision-making mechanisms.
  The work shows that if a decision is made by two agents, then the only way to avoid diffusion of responsibility is for one agent to act as a "dictator'', making the decision unilaterally. In scenarios with more than two agents, any diffusion-free mechanism is an "elected dictatorship'' where the agents elect a single agent to make a unilateral decision.
  The technical results are obtained by defining a bisimulation of decision-making mechanisms, proving that bisimulation preserves responsibility-related properties, and establishing the results for a smallest bisimular mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07935v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pavel Naumov, Jia Tao</dc:creator>
    </item>
    <item>
      <title>On the Complexity of Winner Determination and Strategic Control in Conditional Approval Voting</title>
      <link>https://arxiv.org/abs/2202.01660</link>
      <description>arXiv:2202.01660v3 Announce Type: replace 
Abstract: We focus on a generalization of the classic Minisum approval voting rule, introduced by Barrot and Lang (2016), and referred to as Conditional Minisum (CMS), for multi-issue elections with preferential dependencies. Under this rule, voters are allowed to declare dependencies between different issues, but the price we have to pay for this higher level of expressiveness is that we end up with a computationally hard rule. Motivated by this, we first focus on finding special cases that admit efficient algorithms for CMS. Our main result in this direction is that we identify the condition of bounded treewidth (of an appropriate graph, emerging from the provided ballots) as the necessary and sufficient condition for exact polynomial algorithms, under common complexity assumptions. We then move to the design of approximation algorithms. For the (still hard) case of binary issues, we identify natural restrictions on the voters' ballots, under which we provide the first multiplicative approximation algorithms for the problem. The restrictions involve upper bounds on the number of dependencies an issue can have on the others and on the number of alternatives per issue that a voter can approve. Finally, we also investigate the complexity of problems related to the strategic control of conditional approval elections by adding or deleting either voters or alternatives and we show that in most variants of these problems, CMS is computationally resistant against control. Overall, we conclude that CMS can be viewed as a solution that achieves a satisfactory tradeoff between expressiveness and computational efficiency, when we have a limited number of dependencies among issues, while at the same time exhibiting sufficient resistance to control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.01660v3</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Evangelos Markakis, Georgios Papasotiropoulos</dc:creator>
    </item>
    <item>
      <title>Fair Division with Allocator's Preference</title>
      <link>https://arxiv.org/abs/2310.03475</link>
      <description>arXiv:2310.03475v2 Announce Type: replace 
Abstract: We study the fair allocation of indivisible resources among agents. Most prior work focuses on fairness and/or efficiency among agents. However, the allocator, as the resource owner, may also be involved in many scenarios (e.g., government resource allocation, heritage division, company personnel assignment, etc). The allocator inclines to obtain a fair or efficient allocation based on her preference over the items and to whom each item is allocated. We propose a model and study two problems: 1) Find an allocation fair to both agents and allocator; 2) Maximize allocator's efficiency under agents' fairness. We consider the two fundamental fairness criteria: envy-freeness and proportionality.
  For the first problem, we study the existence of an allocation that is envy-free up to $c$ goods (EF-$c$) or proportional up to $c$ goods (PROP-$c$) from both the agents' and the allocator's perspectives, called doubly EF-$c$ or doubly PROP-$c$. When the allocator's utility only depends on the items (not recipients), we prove that a doubly EF-$1$ allocation always exists. For the general setting where the allocator has a preference over the items and to whom each item is allocated, a doubly EF-$1$ allocation always exists for two agents, a doubly PROP-$2$ allocation always exists for personalized bi-valued valuations, and a doubly PROP-$O(\log n)$ allocation always exists.
  For the second problem, we give (in)approximability results with asymptotically tight bounds in most settings. When agents' valuations are binary, maximizing the allocator's social welfare while ensuring agents' fairness criteria of PROP-$c$ (with a general number of agents) and EF-$c$ (with a constant number of agents) are both polynomial-time solvable for any integer $c$. Strong inapproximability holds for most of the other settings (general valuations, EF-$c$, etc).</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.03475v2</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaolin Bu, Zihao Li, Shengxin Liu, Jiaxin Song, Biaoshuai Tao</dc:creator>
    </item>
    <item>
      <title>Stable Menus of Public Goods: A Matching Problem</title>
      <link>https://arxiv.org/abs/2402.11370</link>
      <description>arXiv:2402.11370v2 Announce Type: replace 
Abstract: We study a matching problem between agents and public goods, in settings without monetary transfers. Since goods are public, they have no capacity constraints. There is no exogenously defined budget of goods to be provided. Rather, each provided good must justify its cost by being utilized by sufficiently many agents, leading to strong complementarities in the "preferences" of goods. Furthermore, goods that are in high demand given other already-provided goods must also be provided. The question of the existence of a stable solution (a menu of public goods to be provided) exhibits a rich combinatorial structure. We uncover sufficient conditions and necessary conditions for guaranteeing the existence of a stable solution, and derive both positive and negative results for strategyproof stable matching.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11370v2</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <category>math.CO</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sara Fish, Yannai A. Gonczarowski, Sergiu Hart</dc:creator>
    </item>
    <item>
      <title>Formalizing Feint Actions, and Example Studies in Two-Player Games</title>
      <link>https://arxiv.org/abs/2403.07931</link>
      <description>arXiv:2403.07931v2 Announce Type: replace 
Abstract: Feint actions refer to a set of deceptive actions, which enable players to obtain temporal advantages from their opponents. Such actions are regarded as widely-used tactic in most non-deterministic Two-player Games (e.g. boxing and fencing). However, existing literature does not provide comprehensive and concrete formalization on Feint actions, and their implications on Two-Player Games. We argue that a full exploration on Feint actions is of great importance towards more realistic Two-player Games. In this paper, we provide the first comprehensive and concrete formalization of Feint actions. The key idea of our work is to (1) allow automatic generation of Feint actions, via our proposed Palindrome-directed Generation of Feint actions; and (2) provide concrete principles to properly combine Feint and attack actions. Based on our formalization of Feint actions, we also explore the implications on the game strategy model, and provide optimizations to better incorporate Feint actions. Our experimental results shows that accounting for Feint actions in Non-Deterministic Games (1) brings overall benefits to the game design; and (2) has great benefits on on either game animations or strategy designs, which also introduces a great extent of randomness into randomness-demanded Game models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07931v2</guid>
      <category>cs.GT</category>
      <category>cs.GR</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Junyu Liu, Wangkai Jin, Xiangjun Peng</dc:creator>
    </item>
    <item>
      <title>Feint Behaviors and Strategies: Formalization, Implementation and Evaluation</title>
      <link>https://arxiv.org/abs/2403.07932</link>
      <description>arXiv:2403.07932v2 Announce Type: replace 
Abstract: Feint behaviors refer to a set of deceptive behaviors in a nuanced manner, which enable players to obtain temporal and spatial advantages over opponents in competitive games. Such behaviors are crucial tactics in most competitive multi-player games (e.g., boxing, fencing, basketball, motor racing, etc.). However, existing literature does not provide a comprehensive (and/or concrete) formalization for Feint behaviors, and their implications on game strategies. In this work, we introduce the first comprehensive formalization of Feint behaviors at both action-level and strategy-level, and provide concrete implementation and quantitative evaluation of them in multi-player games. The key idea of our work is to (1) allow automatic generation of Feint behaviors via Palindrome-directed templates, combine them into meaningful behavior sequences via a Dual-Behavior Model; (2) concertize the implications from our formalization of Feint on game strategies, in terms of temporal, spatial and their collective impacts respectively; and (3) provide a unified implementation scheme of Feint behaviors in existing MARL frameworks. The experimental results show that our design of Feint behaviors can (1) greatly improve the game reward gains; (2) significantly improve the diversity of Multi-Player Games; and (3) only incur negligible overheads in terms of time consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07932v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Junyu Liu, Xiangjun Peng</dc:creator>
    </item>
    <item>
      <title>Asymptotically Fair and Truthful Allocation of Public Goods</title>
      <link>https://arxiv.org/abs/2404.15996</link>
      <description>arXiv:2404.15996v2 Announce Type: replace 
Abstract: We study the fair and truthful allocation of m divisible public items among n agents, each with distinct preferences for the items. To aggregate agents' preferences fairly, we focus on finding a core solution. For divisible items, a core solution always exists and can be calculated by maximizing the Nash welfare objective. However, such a solution is easily manipulated; agents might have incentives to misreport their preferences. To mitigate this, the current state-of-the-art finds an approximate core solution with high probability while ensuring approximate truthfulness. However, this approach has two main limitations. First, due to several approximations, the approximation error in the core could grow with n, resulting in a non-asymptotic core solution. This limitation is particularly significant as public-good allocation mechanisms are frequently applied in scenarios involving a large number of agents, such as the allocation of public tax funds for municipal projects. Second, implementing the current approach for practical applications proves to be a highly nontrivial task. To address these limitations, we introduce PPGA, a (differentially) Private Public-Good Allocation algorithm, and show that it attains asymptotic truthfulness and finds an asymptotic core solution with high probability. Additionally, to demonstrate the practical applicability of our algorithm, we implement PPGA and empirically study its properties using municipal participatory budgeting data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15996v2</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Pouya Kananian, Arnesh Sujanani, Seyed Majid Zahedi</dc:creator>
    </item>
    <item>
      <title>Algorithmic Aspects of Strategic Trading</title>
      <link>https://arxiv.org/abs/2502.07606</link>
      <description>arXiv:2502.07606v2 Announce Type: replace 
Abstract: Algorithmic trading in modern financial markets is widely acknowledged to exhibit strategic, game-theoretic behaviors whose complexity can be difficult to model. A recent series of papers (Chriss, 2024b,c,a, 2025) has made progress in the setting of trading for position building. Here parties wish to buy or sell a fixed number of shares in a fixed time period in the presence of both temporary and permanent market impact, resulting in exponentially large strategy spaces. While these papers primarily consider the existence and structural properties of equilibrium strategies, in this work we focus on the algorithmic aspects of the proposed model. We give an efficient algorithm for computing best responses, and show that while the temporary impact only setting yields a potential game, best response dynamics do not generally converge for the general setting, for which no fast algorithm for (Nash) equilibrium computation is known. This leads us to consider the broader notion of Coarse Correlated Equilibria (CCE), which we show can be computed efficiently via an implementation of Follow the Perturbed Leader (FTPL). We illustrate the model and our results with an experimental investigation, where FTPL exhibits interesting behavior in different regimes of the relative weighting between temporary and permanent market impact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07606v2</guid>
      <category>cs.GT</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Kearns, Mirah Shi</dc:creator>
    </item>
    <item>
      <title>An $O(log log n)$-approximate budget feasible mechanism for subadditive valuations</title>
      <link>https://arxiv.org/abs/2506.04665</link>
      <description>arXiv:2506.04665v2 Announce Type: replace 
Abstract: In budget-feasible mechanism design, there is a set of items $U$, each owned by a distinct seller. The seller of item $e$ incurs a private cost $\overline{c}_e$ for supplying her item. A buyer wishes to procure a set of items from the sellers of maximum value, where the value of a set $S\subseteq U$ of items is given by a valuation function $v:2^U\to \mathbb{R}_+$. The buyer has a budget of $B \in \mathbb{R}_+$ for the total payments made to the sellers. We wish to design a mechanism that is truthful, that is, sellers are incentivized to report their true costs, budget-feasible, that is, the sum of the payments made to the sellers is at most the budget $B$, and that outputs a set whose value is large compared to $\text{OPT}:=\max\{v(S):\overline{c}(S)\le B,S\subseteq U\}$.
  Budget-feasible mechanism design has been extensively studied, with the literature focussing on (classes of) subadditive valuation functions, and various polytime, budget-feasible mechanisms, achieving constant-factor approximation, have been devised for the special cases of additive, submodular, and XOS valuations. However, for general subadditive valuations, the best-known approximation factor achievable by a polytime budget-feasible mechanism (given access to demand oracles) was only $O(\log n / \log \log n)$, where $n$ is the number of items.
  We improve this state-of-the-art significantly by designing a randomized budget-feasible mechanism for subadditive valuations that \emph{achieves a substantially-improved approximation factor of $O(\log\log n)$ and runs in polynomial time, given access to demand oracles.}</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04665v2</guid>
      <category>cs.GT</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rian Neogi, Kanstantsin Pashkovich, Chaitanya Swamy</dc:creator>
    </item>
    <item>
      <title>Information Bargaining: Bilateral Commitment in Bayesian Persuasion</title>
      <link>https://arxiv.org/abs/2506.05876</link>
      <description>arXiv:2506.05876v2 Announce Type: replace 
Abstract: Bayesian persuasion, an extension of cheap-talk communication, involves an informed sender committing to a signaling scheme to influence a receiver's actions. Compared to cheap talk, this sender's commitment enables the receiver to verify the incentive compatibility of signals beforehand, facilitating cooperation. While effective in one-shot scenarios, Bayesian persuasion faces computational complexity (NP-hardness) when extended to long-term interactions, where the receiver may adopt dynamic strategies conditional on past outcomes and future expectations. To address this complexity, we introduce the bargaining perspective, which allows: (1) a unified framework and well-structured solution concept for long-term persuasion, with desirable properties such as fairness and Pareto efficiency; (2) a clear distinction between two previously conflated advantages: the sender's informational advantage and first-proposer advantage. With only modest modifications to the standard setting, this perspective makes explicit the common knowledge of the game structure and grants the receiver comparable commitment capabilities, thereby reinterpreting classic one-sided persuasion as a balanced information bargaining framework. The framework is validated through a two-stage validation-and-inference paradigm: We first demonstrate that GPT-o3 and DeepSeek-R1, out of publicly available LLMs, reliably handle standard tasks; We then apply them to persuasion scenarios to test that the outcomes align with what our information-bargaining framework suggests. All code, results, and terminal logs are publicly available at github.com/YueLin301/InformationBargaining.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05876v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Lin, Shuhui Zhu, William A Cunningham, Wenhao Li, Pascal Poupart, Hongyuan Zha, Baoxiang Wang</dc:creator>
    </item>
    <item>
      <title>Adversaries With Incentives: A Strategic Alternative to Adversarial Robustness</title>
      <link>https://arxiv.org/abs/2406.11458</link>
      <description>arXiv:2406.11458v3 Announce Type: replace-cross 
Abstract: Adversarial training aims to defend against adversaries: malicious opponents whose sole aim is to harm predictive performance in any way possible. This presents a rather harsh perspective, which we assert results in unnecessarily conservative training. As an alternative, we propose to model opponents as simply pursuing their own goals--rather than working directly against the classifier. Employing tools from strategic modeling, our approach enables knowledge or beliefs regarding the opponent's possible incentives to be used as inductive bias for learning. Accordingly, our method of strategic training is designed to defend against all opponents within an 'incentive uncertainty set'. This resorts to adversarial learning when the set is maximal, but offers potential gains when the set can be appropriately reduced. We conduct a series of experiments that show how even mild knowledge regarding the opponent's incentives can be useful, and that the degree of potential gains depends on how these incentives relate to the structure of the learning task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11458v3</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>ICLR 2025</arxiv:journal_reference>
      <dc:creator>Maayan Ehrenberg, Roy Ganz, Nir Rosenfeld</dc:creator>
    </item>
    <item>
      <title>Selective Response Strategies for GenAI</title>
      <link>https://arxiv.org/abs/2502.00729</link>
      <description>arXiv:2502.00729v2 Announce Type: replace-cross 
Abstract: The rise of Generative AI (GenAI) has significantly impacted human-based forums like Stack Overflow, which are essential for generating high-quality data. This creates a negative feedback loop, hindering the development of GenAI systems, which rely on such data to provide accurate responses. In this paper, we provide a possible remedy: A novel strategy we call selective response. Selective response implies that GenAI could strategically provide inaccurate (or conservative) responses to queries involving emerging topics and novel technologies, thereby driving users to use human-based forums like Stack Overflow. We show that selective response can potentially have a compounding effect on the data generation process, increasing both GenAI's revenue and user welfare in the long term. From an algorithmic perspective, we propose an approximately optimal approach to maximize GenAI's revenue under social welfare constraints. From a regulatory perspective, we derive sufficient and necessary conditions for selective response to improve welfare improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00729v2</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.SI</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Boaz Taitler, Omer Ben-Porat</dc:creator>
    </item>
    <item>
      <title>EconEvals: Benchmarks and Litmus Tests for LLM Agents in Unknown Environments</title>
      <link>https://arxiv.org/abs/2503.18825</link>
      <description>arXiv:2503.18825v2 Announce Type: replace-cross 
Abstract: We develop benchmarks for LLM agents that act in, learn from, and strategize in unknown environments, the specifications of which the LLM agent must learn over time from deliberate exploration. Our benchmarks consist of decision-making tasks derived from key problems in economics. To forestall saturation, the benchmark tasks are synthetically generated with scalable difficulty levels. Additionally, we propose litmus tests, a new kind of quantitative measure for LLMs and LLM agents. Unlike benchmarks, litmus tests quantify differences in character, values, and tendencies of LLMs and LLM agents, by considering their behavior when faced with tradeoffs (e.g., efficiency versus equality) where there is no objectively right or wrong behavior. Overall, our benchmarks and litmus tests assess the abilities and tendencies of LLM agents in tackling complex economic problems in diverse settings spanning procurement, scheduling, task allocation, and pricing -- applications that should grow in importance as such agents are further integrated into the economy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18825v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.GT</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sara Fish, Julia Shephard, Minkai Li, Ran I. Shorrer, Yannai A. Gonczarowski</dc:creator>
    </item>
  </channel>
</rss>
