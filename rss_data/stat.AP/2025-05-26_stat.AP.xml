<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 27 May 2025 03:13:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Novel Bayesian Extrapolation Design for Assessing Equivalence in Exposure-Response Curves between Pediatric and Adult Populations</title>
      <link>https://arxiv.org/abs/2505.17397</link>
      <description>arXiv:2505.17397v1 Announce Type: new 
Abstract: Development of effective treatments in pediatric population poses unique scientific and ethical challenges in addition to the small population. In this regard, both the U.S. and E.U. regulations suggest a complementary strategy, pediatric extrapolation, based on assessing the relevance of existing information in the adult population to the pediatric population. The pediatric extrapolation approach often relies on data extrapolation from adults, contingent upon evidence of similar disease progression, pharmacology and clinical response to treatment between adult and children. Similarity evaluation in pharmacology is usually characterized through the exposure-response relationship. Current methodologies for comparing exposure-response (E-R) curves between these groups are inadequate, typically focusing on isolated data points rather than the entire curve spectrum (Zhang et al., 2021). To overcome this limitation, we introduce an innovative Bayesian approach for a comprehensive evaluation of E-R curve similarities between adult and pediatric populations. This method encompasses the entire curve, employing logistic regression for binary endpoints. We have developed an algorithm to determine sample size and key design parameters, such as the Bayesian posterior probability threshold, and utilize the maximum curve distance as a measure of similarity. Integrating Bayesian and frequentist principles, our approach involves developing a method to simulate datasets under both null and alternative hypotheses, allowing for type I error and type II error control. Simulation studies and sensitivity analyses demonstrate that our method maintains a stable performance with type I error and type II error control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17397v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongheng Cai, Lian Ma, Jingjing Ye, Haitao Pan</dc:creator>
    </item>
    <item>
      <title>Local improvement of NO2 concentration maps derived from physicochemical models, using low-cost sensors</title>
      <link>https://arxiv.org/abs/2505.17564</link>
      <description>arXiv:2505.17564v1 Announce Type: new 
Abstract: Urban air quality is a major issue today. Pollutant concentrations, such as NO2's, must be monitored to ensure that they do not exceed dangerous thresholds. Two recent techniques help to map pollutant concentrations on a small scale. First, deterministic physicochemical models take into account the street network and calculate concentration estimates on a grid, providing a map. On the other hand, the advent of new low-cost technologies allows monitoring organizations to densify measurement networks. However, these devices are less reliable than reference devices and need to be corrected. We propose a new approach to improve maps generated using deterministic models by combining measurements from multiple sensor networks. More precisely, we model the bias of deterministic models and estimate it using an MCMC method. Our approach also enables to analyze the behavior of the sensors. The method is applied to the city of Rouen, France, with measurements provided by 4 monitoring stations and 10 low-cost sensors during December 2022. Results show that the method indeed allows to correct the map, reducing estimation errors by about 9.7%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17564v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Camille Coron (MIA Paris-Saclay, INRAE), Emma Thulliez (LMI)</dc:creator>
    </item>
    <item>
      <title>Predicting hazards of climate extremes: a statistical perspective</title>
      <link>https://arxiv.org/abs/2505.17622</link>
      <description>arXiv:2505.17622v1 Announce Type: new 
Abstract: Climate extremes such as floods, storms, and heatwaves have caused severe economic and human losses across Europe in recent decades. To support the European Union's climate resilience efforts, we propose a statistical framework for short-to-medium-term prediction of tail risks related to extreme economic losses and fatalities. Our approach builds on Extreme Value Theory and employs the predictive distribution of future tail events to quantify both estimation and aleatoric uncertainty. Using data on EU-wide losses and fatalities from 1980 to 2023, we model extreme events through Peaks Over Threshold methodology and fit Generalised Pareto (GP) and discrete-GP models using an empirical Bayes procedure. Our predictive approach enables a 'What-if' analysis to evaluate hypothetical scenarios beyond observed levels, including potential worst-case outcomes for a precautionary risk assessment of future extreme episodes. To account for a time-varying behavior of extreme losses and fatalities we extend our predictive method using a proportional tail model that allows to handle heteroscedastic extremes over time. Results of our analysis under stationarity and non-stationary settings raise concerns, reinforcing the urgency of integrating predictive tail risk assessment into EU adaptation strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17622v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlotta Pacifici, Simone A. Padoan, Jaroslav Mysiak</dc:creator>
    </item>
    <item>
      <title>Rethinking Climate Econometrics: Data Cleaning, Flexible Trend Controls, and Predictive Validation</title>
      <link>https://arxiv.org/abs/2505.18033</link>
      <description>arXiv:2505.18033v1 Announce Type: new 
Abstract: We assess empirical models in climate econometrics using modern statistical learning techniques. Existing approaches are prone to outliers, ignore sample dependencies, and lack principled model selection. To address these issues, we implement robust preprocessing, nonparametric time-trend controls, and out-of-sample validation across 700+ climate variables. Our analysis reveals that widely used models and predictors-such as mean temperature-have little predictive power. A previously overlooked humidity-related variable emerges as the most consistent predictor, though even its performance remains limited. These findings challenge the empirical foundations of climate econometrics and point toward a more robust, data-driven path forward.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18033v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christof Sch\"otz, Jan Hassel, Christian Otto</dc:creator>
    </item>
    <item>
      <title>Designing Graph Convolutional Neural Networks for Discrete Choice with Network Effects</title>
      <link>https://arxiv.org/abs/2503.09786</link>
      <description>arXiv:2503.09786v1 Announce Type: cross 
Abstract: We introduce a novel model architecture that incorporates network effects into discrete choice problems, achieving higher predictive performance than standard discrete choice models while offering greater interpretability than general-purpose flexible model classes. Econometric discrete choice models aid in studying individual decision-making, where agents select the option with the highest reward from a discrete set of alternatives. Intuitively, the utility an individual derives from a particular choice depends on their personal preferences and characteristics, the attributes of the alternative, and the value their peers assign to that alternative or their previous choices. However, most applications ignore peer influence, and models that do consider peer or network effects often lack the flexibility and predictive performance of recently developed approaches to discrete choice, such as deep learning. We propose a novel graph convolutional neural network architecture to model network effects in discrete choices, achieving higher predictive performance than standard discrete choice models while retaining the interpretability necessary for inference--a quality often lacking in general-purpose deep learning architectures. We evaluate our architecture using revealed commuting choice data, extended with travel times and trip costs for each travel mode for work-related trips in New York City, as well as 2016 U.S. election data aggregated by county, to test its performance on datasets with highly imbalanced classes. Given the interpretability of our models, we can estimate relevant economic metrics, such as the value of travel time savings in New York City. Finally, we compare the predictive performance and behavioral insights from our architecture to those derived from traditional discrete choice and general-purpose deep learning models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09786v1</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel F. Villarraga, Ricardo A. Daziano</dc:creator>
    </item>
    <item>
      <title>Transfer Faster, Price Smarter: Minimax Dynamic Pricing under Cross-Market Preference Shift</title>
      <link>https://arxiv.org/abs/2505.17203</link>
      <description>arXiv:2505.17203v1 Announce Type: cross 
Abstract: We study contextual dynamic pricing when a target market can leverage K auxiliary markets -- offline logs or concurrent streams -- whose mean utilities differ by a structured preference shift. We propose Cross-Market Transfer Dynamic Pricing (CM-TDP), the first algorithm that provably handles such model-shift transfer and delivers minimax-optimal regret for both linear and non-parametric utility models.
  For linear utilities of dimension d, where the difference between source- and target-task coefficients is $s_{0}$-sparse, CM-TDP attains regret $\tilde{O}((d*K^{-1}+s_{0})\log T)$. For nonlinear demand residing in a reproducing kernel Hilbert space with effective dimension $\alpha$, complexity $\beta$ and task-similarity parameter $H$, the regret becomes $\tilde{O}\!(K^{-2\alpha\beta/(2\alpha\beta+1)}T^{1/(2\alpha\beta+1)} + H^{2/(2\alpha+1)}T^{1/(2\alpha+1)})$, matching information-theoretic lower bounds up to logarithmic factors. The RKHS bound is the first of its kind for transfer pricing and is of independent interest.
  Extensive simulations show up to 50% lower cumulative regret and 5 times faster learning relative to single-market pricing baselines. By bridging transfer learning, robust aggregation, and revenue optimization, CM-TDP moves toward pricing systems that transfer faster, price smarter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17203v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Zhang, Elynn Chen, Yujun Yan</dc:creator>
    </item>
    <item>
      <title>Deconfounded Warm-Start Thompson Sampling with Applications to Precision Medicine</title>
      <link>https://arxiv.org/abs/2505.17283</link>
      <description>arXiv:2505.17283v1 Announce Type: cross 
Abstract: Randomized clinical trials often require large patient cohorts before drawing definitive conclusions, yet abundant observational data from parallel studies remains underutilized due to confounding and hidden biases. To bridge this gap, we propose Deconfounded Warm-Start Thompson Sampling (DWTS), a practical approach that leverages a Doubly Debiased LASSO (DDL) procedure to identify a sparse set of reliable measured covariates and combines them with key hidden covariates to form a reduced context. By initializing Thompson Sampling (LinTS) priors with DDL-estimated means and variances on these measured features -- while keeping uninformative priors on hidden features -- DWTS effectively harnesses confounded observational data to kick-start adaptive clinical trials. Evaluated on both a purely synthetic environment and a virtual environment created using real cardiovascular risk dataset, DWTS consistently achieves lower cumulative regret than standard LinTS, showing how offline causal insights from observational data can improve trial efficiency and support more personalized treatment decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17283v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prateek Jaiswal, Esmaeil Keyvanshokooh, Junyu Cao</dc:creator>
    </item>
    <item>
      <title>Robust Distributed Estimation: Extending Gossip Algorithms to Ranking and Trimmed Means</title>
      <link>https://arxiv.org/abs/2505.17836</link>
      <description>arXiv:2505.17836v1 Announce Type: cross 
Abstract: This paper addresses the problem of robust estimation in gossip algorithms over arbitrary communication graphs. Gossip algorithms are fully decentralized, relying only on local neighbor-to-neighbor communication, making them well-suited for situations where communication is constrained. A fundamental challenge in existing mean-based gossip algorithms is their vulnerability to malicious or corrupted nodes. In this paper, we show that an outlier-robust mean can be computed by globally estimating a robust statistic. More specifically, we propose a novel gossip algorithm for rank estimation, referred to as \textsc{GoRank}, and leverage it to design a gossip procedure dedicated to trimmed mean estimation, coined \textsc{GoTrim}. In addition to a detailed description of the proposed methods, a key contribution of our work is a precise convergence analysis: we establish an $\mathcal{O}(1/t)$ rate for rank estimation and an $\mathcal{O}(\log(t)/t)$ rate for trimmed mean estimation, where by $t$ is meant the number of iterations. Moreover, we provide a breakdown point analysis of \textsc{GoTrim}. We empirically validate our theoretical results through experiments on diverse network topologies, data distributions and contamination schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17836v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Van Elst, Igor Colin, Stephan Cl\'emen\c{c}on</dc:creator>
    </item>
    <item>
      <title>Federated Causal Inference from Multi-Site Observational Data via Propensity Score Aggregation</title>
      <link>https://arxiv.org/abs/2505.17961</link>
      <description>arXiv:2505.17961v1 Announce Type: cross 
Abstract: Causal inference typically assumes centralized access to individual-level data. Yet, in practice, data are often decentralized across multiple sites, making centralization infeasible due to privacy, logistical, or legal constraints. We address this by estimating the Average Treatment Effect (ATE) from decentralized observational data using federated learning, which enables inference through the exchange of aggregate statistics rather than individual-level data. We propose a novel method to estimate propensity scores in a (non-)parametric manner by computing a federated weighted average of local scores, using two theoretically grounded weighting schemes -- Membership Weights (MW) and Density Ratio Weights (DW) -- that balance communication efficiency and model flexibility. These federated scores are then used to construct two ATE estimators: the Federated Inverse Propensity Weighting estimator (Fed-IPW) and its augmented variant (Fed-AIPW). Unlike meta-analysis methods, which fail when any site violates positivity, our approach leverages heterogeneity in treatment assignment across sites to improve overlap. We show that Fed-IPW and Fed-AIPW perform well under site-level heterogeneity in sample sizes, treatment mechanisms, and covariate distributions, with theoretical analysis and experiments on simulated and real-world data highlighting their strengths and limitations relative to meta-analysis and related methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17961v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Khellaf R\'emi, Bellet Aur\'elien, Josse Julie</dc:creator>
    </item>
    <item>
      <title>Bayesian Deep Learning for Discrete Choice</title>
      <link>https://arxiv.org/abs/2505.18077</link>
      <description>arXiv:2505.18077v1 Announce Type: cross 
Abstract: Discrete choice models (DCMs) are used to analyze individual decision-making in contexts such as transportation choices, political elections, and consumer preferences. DCMs play a central role in applied econometrics by enabling inference on key economic variables, such as marginal rates of substitution, rather than focusing solely on predicting choices on new unlabeled data. However, while traditional DCMs offer high interpretability and support for point and interval estimation of economic quantities, these models often underperform in predictive tasks compared to deep learning (DL) models. Despite their predictive advantages, DL models remain largely underutilized in discrete choice due to concerns about their lack of interpretability, unstable parameter estimates, and the absence of established methods for uncertainty quantification. Here, we introduce a deep learning model architecture specifically designed to integrate with approximate Bayesian inference methods, such as Stochastic Gradient Langevin Dynamics (SGLD). Our proposed model collapses to behaviorally informed hypotheses when data is limited, mitigating overfitting and instability in underspecified settings while retaining the flexibility to capture complex nonlinear relationships when sufficient data is available. We demonstrate our approach using SGLD through a Monte Carlo simulation study, evaluating both predictive metrics--such as out-of-sample balanced accuracy--and inferential metrics--such as empirical coverage for marginal rates of substitution interval estimates. Additionally, we present results from two empirical case studies: one using revealed mode choice data in NYC, and the other based on the widely used Swiss train choice stated preference data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18077v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel F. Villarraga, Ricardo A. Daziano</dc:creator>
    </item>
    <item>
      <title>Bayesian sample size calculations for external validation studies of risk prediction models</title>
      <link>https://arxiv.org/abs/2504.15923</link>
      <description>arXiv:2504.15923v2 Announce Type: replace 
Abstract: Contemporary sample size calculations for external validation of risk prediction models require users to specify fixed values of assumed model performance metrics alongside target precision levels (e.g., 95% CI widths). However, due to the finite samples of previous studies, our knowledge of true model performance in the target population is uncertain, and so choosing fixed values represents an incomplete picture. As well, for net benefit (NB) as a measure of clinical utility, the relevance of conventional precision-based inference is doubtful. In this work, we propose a general Bayesian framework for multi-criteria sample size considerations for prediction models for binary outcomes. For statistical metrics of performance (e.g., discrimination and calibration), we propose sample size rules that target desired expected precision or desired assurance probability that the precision criteria will be satisfied. For NB, we propose rules based on Optimality Assurance (the probability that the planned study correctly identifies the optimal strategy) and Value of Information (VoI) analysis. We showcase these developments in a case study on the validation of a risk prediction model for deterioration of hospitalized COVID-19 patients. Compared to the conventional sample size calculation methods, a Bayesian approach requires explicit quantification of uncertainty around model performance, and thereby enables flexible sample size rules based on expected precision, assurance probabilities, and VoI. In our case study, calculations based on VoI for NB suggest considerably lower sample sizes are needed than when focusing on precision of calibration metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15923v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Sadatsafavi, Paul Gustafson, Solmaz Setayeshgar, Laure Wynants, Richard D Riley</dc:creator>
    </item>
    <item>
      <title>Data-driven stochastic 3D modeling of the nanoporous binder-conductive additive phase in battery cathodes</title>
      <link>https://arxiv.org/abs/2409.11080</link>
      <description>arXiv:2409.11080v3 Announce Type: replace-cross 
Abstract: A stochastic 3D modeling approach for the nanoporous binder-conductive additive phase in hierarchically structured cathodes of lithium-ion batteries is presented. The binder-conductive additive phase of these electrodes consists of carbon black, polyvinylidene difluoride binder and graphite particles. For its stochastic 3D modeling, a three-step procedure based on methods from stochastic geometry is used. First, the graphite particles are described by a Boolean model with ellipsoidal grains. Second, the mixture of carbon black and binder is modeled by an excursion set of a Gaussian random field in the complement of the graphite particles. Third, large pore regions within the mixture of carbon black and binder are described by a Boolean model with spherical grains. The model parameters are calibrated to 3D image data of cathodes in lithium-ion batteries acquired by focused ion beam scanning electron microscopy. Subsequently, model validation is performed by comparing model realizations with measured image data in terms of various morphological descriptors that are not used for model fitting. Finally, we use the stochastic 3D model for predictive simulations, where we generate virtual, yet realistic, image data of nanoporous binder-conductive additives with varying amounts of graphite particles. Based on these virtual nanostructures, we can investigate structure-property relationships. In particular, we quantitatively study the influence of graphite particles on effective transport properties in the nanoporous binder-conductive additive phase, which have a crucial impact on electrochemical processes in the cathode and thus on the performance of battery cells.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11080v3</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1186/s13362-025-00174-z</arxiv:DOI>
      <arxiv:journal_reference>J.Math.Industry 15, 9 (2025)</arxiv:journal_reference>
      <dc:creator>Phillip Gr\"afensteiner, Markus Osenberg, Andr\'e Hilger, Nicole Bohn, Joachim R. Binder, Ingo Manke, Volker Schmidt, Matthias Neumann</dc:creator>
    </item>
    <item>
      <title>On the optimal stopping of Gauss-Markov bridges with random pinning points</title>
      <link>https://arxiv.org/abs/2505.03636</link>
      <description>arXiv:2505.03636v2 Announce Type: replace-cross 
Abstract: We consider the optimal stopping problem for a Gauss-Markov process conditioned to adopt a prescribed terminal distribution. By applying a time-space transformation, we show it is equivalent to stopping a Brownian bridge pinned at a random endpoint with a time-dependent payoff. We prove that the optimal rule is the first entry into the stopping region, and establish that the value function is Lipschitz continuous on compacts via a coupling of terminal pinning points across different initial conditions. A comparison theorems then order value functions according to likelihood-ratio ordering of terminal densities, and when these densities have bounded support, we bound the optimal boundary by that of a Gauss-Markov bridge. Although the stopping boundary need not be the graph of a function in general, we provide sufficient conditions under which this property holds, and identify strongly log-concave terminal densities that guarantee this structure. Numerical experiments illustrate representative boundary shapes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03636v2</guid>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Abel Azze, Bernardo D'Auria</dc:creator>
    </item>
  </channel>
</rss>
