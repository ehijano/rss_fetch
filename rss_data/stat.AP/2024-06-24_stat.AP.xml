<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 24 Jun 2024 04:01:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 24 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Multiscale modelling of animal movement with persistent dynamics</title>
      <link>https://arxiv.org/abs/2406.15195</link>
      <description>arXiv:2406.15195v1 Announce Type: new 
Abstract: Wild animals are commonly fitted with trackers that record their position through time, to learn about their behaviour. Broadly, statistical models for tracking data often fall into two categories: local models focus on describing small-scale movement decisions, and global models capture large-scale spatial distributions. Due to this dichotomy, it is challenging to describe mathematically how animals' distributions arise from their short-term movement patterns, and to combine data sets collected at different scales. We propose a multiscale model of animal movement and space use based on the underdamped Langevin process, widely used in statistical physics. The model is convenient to describe animal movement for three reasons: it is specified in continuous time (such that its parameters are not dependent on an arbitrary time scale), its speed and direction are autocorrelated (similarly to real animal trajectories), and it has a closed form stationary distribution that we can view as a model of long-term space use. We use the common form of a resource selection function for the stationary distribution, to model the environmental drivers behind the animal's movement decisions. We further increase flexibility by allowing movement parameters to be time-varying, e.g., to account for daily cycles in an animal's activity. We formulate the model as a state-space model and present a method of inference based on the Kalman filter. The approach requires discretising the continuous-time process, and we use simulations to investigate performance for various time resolutions of observation. The approach works well at fine resolutions, though the estimated stationary distribution tends to be too flat when time intervals between observations are very long.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15195v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Th\'eo Michelot</dc:creator>
    </item>
    <item>
      <title>Tackling GenAI Copyright Issues: Originality Estimation and Genericization</title>
      <link>https://arxiv.org/abs/2406.03341</link>
      <description>arXiv:2406.03341v1 Announce Type: cross 
Abstract: The rapid progress of generative AI technology has sparked significant copyright concerns, leading to numerous lawsuits filed against AI developers. While some studies explore methods to mitigate copyright risks by steering the outputs of generative models away from those resembling copyrighted data, little attention has been paid to the question of how much of a resemblance is undesirable; more original or unique data are afforded stronger protection, and the threshold level of resemblance for constituting infringement correspondingly lower. Here, leveraging this principle, we propose a genericization method that modifies the outputs of a generative model to make them more generic and less likely to infringe copyright. To achieve this, we introduce a metric for quantifying the level of originality of data in a manner that is consistent with the legal framework. This metric can be practically estimated by drawing samples from a generative model, which is then used for the genericization process. Experiments demonstrate that our genericization method successfully modifies the output of a text-to-image generative model so that it produces more generic, copyright-compliant images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03341v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiroaki Chiba-Okabe, Weijie J. Su</dc:creator>
    </item>
    <item>
      <title>Analysis of Linked Files: A Missing Data Perspective</title>
      <link>https://arxiv.org/abs/2406.14717</link>
      <description>arXiv:2406.14717v1 Announce Type: cross 
Abstract: In many applications, researchers seek to identify overlapping entities across multiple data files. Record linkage algorithms facilitate this task, in the absence of unique identifiers. As these algorithms rely on semi-identifying information, they may miss records that represent the same entity, or incorrectly link records that do not represent the same entity. Analysis of linked files commonly ignores such linkage errors, resulting in biased, or overly precise estimates of the associations of interest. We view record linkage as a missing data problem, and delineate the linkage mechanisms that underpin analysis methods with linked files. Following the missing data literature, we group these methods under three categories: likelihood and Bayesian methods, imputation methods, and weighting methods. We summarize the assumptions and limitations of the methods, and evaluate their performance in a wide range of simulation scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14717v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gauri Kamat, Roee Gutman</dc:creator>
    </item>
    <item>
      <title>Population Activity Recovery: Milestones Unfolding, Temporal Interdependencies, and Relationship with Physical and Social Vulnerability</title>
      <link>https://arxiv.org/abs/2406.14720</link>
      <description>arXiv:2406.14720v1 Announce Type: cross 
Abstract: Understanding sequential community recovery milestones is crucial for proactive recovery planning and monitoring. This study investigates these milestones related to population activities to examine their temporal interdependencies and evaluate the relationship between recovery milestones and physical (residential property damage) and social vulnerability (household income). This study leverages post-2017 Hurricane Harvey mobility data from Harris County to specify and analyze temporal recovery milestones and their interdependencies. The analysis examined four key milestones: return to evacuated areas, recovery of essential and nonessential services, and the rate of home-switch (moving out of residences). Robust linear regression validates interdependencies between across milestone lags and sequences: achieving earlier milestones accelerates subsequent recovery milestones. The study thus identifies six primary recovery milestone sequences. We found that social vulnerability accounted through the median household income level, rather than physical vulnerability to flooding accounted through the property damage extent, correlates with recovery delays between milestones. We studied variations in recovery sequences across lower and upper quantiles of property damage extent and median household income: lower property damage extent and lower household income show greater representation in the (slowest to recover) sequence, while households with greater damage and higher income are predominant in the group with the (fastest recovery sequences). Milestone sequence variability aligns closely with social vulnerability, independent of physical vulnerability. Understanding the variation in recovery sequences, milestone interdependencies, and social vulnerability disparities provides crucial evidence for targeted interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14720v1</guid>
      <category>cs.CE</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Flavia Ioana Patrascu, Ali Mostafavi</dc:creator>
    </item>
    <item>
      <title>Bayesian neural networks for predicting uncertainty in full-field material response</title>
      <link>https://arxiv.org/abs/2406.14838</link>
      <description>arXiv:2406.14838v1 Announce Type: cross 
Abstract: Stress and material deformation field predictions are among the most important tasks in computational mechanics. These predictions are typically made by solving the governing equations of continuum mechanics using finite element analysis, which can become computationally prohibitive considering complex microstructures and material behaviors. Machine learning (ML) methods offer potentially cost effective surrogates for these applications. However, existing ML surrogates are either limited to low-dimensional problems and/or do not provide uncertainty estimates in the predictions. This work proposes an ML surrogate framework for stress field prediction and uncertainty quantification for diverse materials microstructures. A modified Bayesian U-net architecture is employed to provide a data-driven image-to-image mapping from initial microstructure to stress field with prediction (epistemic) uncertainty estimates. The Bayesian posterior distributions for the U-net parameters are estimated using three state-of-the-art inference algorithms: the posterior sampling-based Hamiltonian Monte Carlo method and two variational approaches, the Monte-Carlo Dropout method and the Bayes by Backprop algorithm. A systematic comparison of the predictive accuracy and uncertainty estimates for these methods is performed for a fiber reinforced composite material and polycrystalline microstructure application. It is shown that the proposed methods yield predictions of high accuracy compared to the FEA solution, while uncertainty estimates depend on the inference approach. Generally, the Hamiltonian Monte Carlo and Bayes by Backprop methods provide consistent uncertainty estimates. Uncertainty estimates from Monte Carlo Dropout, on the other hand, are more difficult to interpret and depend strongly on the method's design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14838v1</guid>
      <category>stat.ML</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George D. Pasparakis, Lori Graham-Brady, Michael D. Shields</dc:creator>
    </item>
    <item>
      <title>A review of feature selection strategies utilizing graph data structures and knowledge graphs</title>
      <link>https://arxiv.org/abs/2406.14864</link>
      <description>arXiv:2406.14864v1 Announce Type: cross 
Abstract: Feature selection in Knowledge Graphs (KGs) are increasingly utilized in diverse domains, including biomedical research, Natural Language Processing (NLP), and personalized recommendation systems. This paper delves into the methodologies for feature selection within KGs, emphasizing their roles in enhancing machine learning (ML) model efficacy, hypothesis generation, and interpretability. Through this comprehensive review, we aim to catalyze further innovation in feature selection for KGs, paving the way for more insightful, efficient, and interpretable analytical models across various domains. Our exploration reveals the critical importance of scalability, accuracy, and interpretability in feature selection techniques, advocating for the integration of domain knowledge to refine the selection process. We highlight the burgeoning potential of multi-objective optimization and interdisciplinary collaboration in advancing KG feature selection, underscoring the transformative impact of such methodologies on precision medicine, among other fields. The paper concludes by charting future directions, including the development of scalable, dynamic feature selection algorithms and the integration of explainable AI principles to foster transparency and trust in KG-driven models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14864v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sisi Shao, Pedro Henrique Ribeiro, Christina Ramirez, Jason H. Moore</dc:creator>
    </item>
    <item>
      <title>The Influence of Nuisance Parameter Uncertainty on Statistical Inference in Practical Data Science Models</title>
      <link>https://arxiv.org/abs/2406.15078</link>
      <description>arXiv:2406.15078v1 Announce Type: cross 
Abstract: For multiple reasons -- such as avoiding overtraining from one data set or because of having received numerical estimates for some parameters in a model from an alternative source -- it is sometimes useful to divide a model's parameters into one group of primary parameters and one group of nuisance parameters. However, uncertainty in the values of nuisance parameters is an inevitable factor that impacts the model's reliability. This paper examines the issue of uncertainty calculation for primary parameters of interest in the presence of nuisance parameters. We illustrate a general procedure on two distinct model forms: 1) the GARCH time series model with univariate nuisance parameter and 2) multiple hidden layer feed-forward neural network models with multivariate nuisance parameters. Leveraging an existing theoretical framework for nuisance parameter uncertainty, we show how to modify the confidence regions for the primary parameters while considering the inherent uncertainty introduced by nuisance parameters. Furthermore, our study validates the practical effectiveness of adjusted confidence regions that properly account for uncertainty in nuisance parameters. Such an adjustment helps data scientists produce results that more honestly reflect the overall uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15078v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunrong Wan, James Spall</dc:creator>
    </item>
    <item>
      <title>Predicting Progression Events in Multiple Myeloma from Routine Blood Work</title>
      <link>https://arxiv.org/abs/2405.18051</link>
      <description>arXiv:2405.18051v3 Announce Type: replace 
Abstract: The ability to accurately predict disease progression is paramount for optimizing multiple myeloma patient care. This study introduces a hybrid neural network architecture, combining Long Short-Term Memory networks with a Conditional Restricted Boltzmann Machine, to predict future blood work of affected patients from a series of historical laboratory results. We demonstrate that our model can replicate the statistical moments of the time series ($0.95~\pm~0.01~\geq~R^2~\geq~0.83~\pm~0.03$) and forecast future blood work features with high correlation to actual patient data ($0.92\pm0.02~\geq~r~\geq~0.52~\pm~0.09$). Subsequently, a second Long Short-Term Memory network is employed to detect and annotate disease progression events within the forecasted blood work time series. We show that these annotations enable the prediction of progression events with significant reliability (AUROC$~=~0.88~\pm~0.01$), up to 12 months in advance (AUROC($t+12~$mos)$~=0.65~\pm~0.01$). Our system is designed in a modular fashion, featuring separate entities for forecasting and progression event annotation. This structure not only enhances interpretability but also facilitates the integration of additional modules to perform subsequent operations on the generated outputs. Our approach utilizes a minimal set of routine blood work measurements, which avoids the need for expensive or resource-intensive tests and ensures accessibility of the system in clinical routine. This capability allows for individualized risk assessment and making informed treatment decisions tailored to a patient's unique disease kinetics. The represented approach contributes to the development of a scalable and cost-effective virtual human twin system for optimized healthcare resource utilization and improved patient outcomes in multiple myeloma care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18051v3</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Maximilian Ferle, Nora Grieb, Markus Kreuz, Uwe Platzbecker, Thomas Neumuth, Kristin Reiche, Alexander Oeser, Maximilian Merz</dc:creator>
    </item>
    <item>
      <title>Estimation of Over-parameterized Models from an Auto-Modeling Perspective</title>
      <link>https://arxiv.org/abs/2206.01824</link>
      <description>arXiv:2206.01824v3 Announce Type: replace-cross 
Abstract: From a model-building perspective, we propose a paradigm shift for fitting over-parameterized models. Philosophically, the mindset is to fit models to future observations rather than to the observed sample. Technically, given an imputation method to generate future observations, we fit over-parameterized models to these future observations by optimizing an approximation of the desired expected loss function based on its sample counterpart and an adaptive $\textit{duality function}$. The required imputation method is also developed using the same estimation technique with an adaptive $m$-out-of-$n$ bootstrap approach. We illustrate its applications with the many-normal-means problem, $n &lt; p$ linear regression, and neural network-based image classification of MNIST digits. The numerical results demonstrate its superior performance across these diverse applications. While primarily expository, the paper conducts an in-depth investigation into the theoretical aspects of the topic. It concludes with remarks on some open problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.01824v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiran Jiang, Chuanhai Liu</dc:creator>
    </item>
    <item>
      <title>Enhancing Actuarial Non-Life Pricing Models via Transformers</title>
      <link>https://arxiv.org/abs/2311.07597</link>
      <description>arXiv:2311.07597v2 Announce Type: replace-cross 
Abstract: Currently, there is a lot of research in the field of neural networks for non-life insurance pricing. The usual goal is to improve the predictive power via neural networks while building upon the generalized linear model, which is the current industry standard. Our paper contributes to this current journey via novel methods to enhance actuarial non-life models with transformer models for tabular data. We build here upon the foundation laid out by the combined actuarial neural network as well as the localGLMnet and enhance those models via the feature tokenizer transformer. The manuscript demonstrates the performance of the proposed methods on a real-world claim frequency dataset and compares them with several benchmark models such as generalized linear models, feed-forward neural networks, combined actuarial neural networks, LocalGLMnet, and pure feature tokenizer transformer. The paper shows that the new methods can achieve better results than the benchmark models while preserving certain generalized linear model advantages. The paper also discusses the practical implications and challenges of applying transformer models in actuarial settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.07597v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s13385-024-00388-2</arxiv:DOI>
      <dc:creator>Alexej Brauer</dc:creator>
    </item>
    <item>
      <title>A clustering approach for pairwise comparison matrices</title>
      <link>https://arxiv.org/abs/2402.06061</link>
      <description>arXiv:2402.06061v3 Announce Type: replace-cross 
Abstract: We consider clustering in group decision making where the opinions are given by pairwise comparison matrices. In particular, the k-medoids model is suggested to classify the matrices since it has a linear programming problem formulation that may contain any condition on the properties of the cluster centres. Its objective function depends on the measure of dissimilarity between the matrices but not on the weights derived from them. Our methodology provides a convenient tool for decision support, for instance, it can be used to quantify the reliability of the aggregation. The proposed theoretical framework is applied to a large-scale experimental dataset, on which it is able to automatically detect some mistakes made by the decision-makers, as well as to identify a common source of inconsistency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06061v3</guid>
      <category>math.OC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kolos Csaba \'Agoston, S\'andor Boz\'oki, L\'aszl\'o Csat\'o</dc:creator>
    </item>
    <item>
      <title>Functional Clustering for Longitudinal Associations between Social Determinants of Health and Stroke Mortality in the US</title>
      <link>https://arxiv.org/abs/2406.10499</link>
      <description>arXiv:2406.10499v2 Announce Type: replace-cross 
Abstract: Understanding longitudinally changing associations between Social determinants of health (SDOH) and stroke mortality is crucial for timely stroke management. Previous studies have revealed a significant regional disparity in the SDOH -- stroke mortality associations. However, they do not develop data-driven methods based on these longitudinal associations for regional division in stroke control. To fill this gap, we propose a novel clustering method for SDOH -- stroke mortality associations in the US counties. To enhance interpretability and statistical efficiency of the clustering outcomes, we introduce a new class of smoothness-sparsity pursued penalties for simultaneous clustering and variable selection in the longitudinal associations. As a result, we can identify important SDOH that contribute to longitudinal changes in the stroke mortality, facilitating clustering of US counties into several regions based on how these SDOH relate to stroke mortality. The effectiveness of our proposed method is demonstrated through extensive numerical studies. By applying our method to a county-level SDOH and stroke mortality longitudinal data, we identify 18 important SDOH for stroke mortality and divide the US counties into two clusters based on these selected SDOH. Our findings unveil complex regional heterogeneity in the longitudinal associations between SDOH and stroke mortality, providing valuable insights in region-specific SDOH adjustments for mitigating stroke mortality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10499v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fangzhi Luo, Jianbin Tan, Donglan Zhang, Hui Huang, Ye Shen</dc:creator>
    </item>
  </channel>
</rss>
