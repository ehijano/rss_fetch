<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 06 Jan 2026 03:18:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Dynamics of Trust: A Stochastic Levy Model Capturing Sudden Behavioral Jumps</title>
      <link>https://arxiv.org/abs/2601.00008</link>
      <description>arXiv:2601.00008v1 Announce Type: new 
Abstract: Trust is the invisible glue that holds together the fabric of societies, economic systems, and political institutions. Yet, its dynamics-especially in real-world settings remain unpredictable and difficult to control. While classical trust game models largely rely on discrete frameworks with limited noise, they fall short in capturing sudden behavioral shifts, extreme volatility, or abrupt breakdowns in cooperation.Here, we propose-for the first time a comprehensive stochastic model of trust based on L\'evy processes that integrates three fundamental components: Brownian motion (representing everyday fluctuations), Poissonian jump intensity (capturing the frequency of shocks), and random distributions for jump magnitudes. This framework surpasses conventional models by enabling simulations of phenomena such as "sudden trust collapse," "chaotic volatility," and "nonlinear recoveries" dynamics often neglected in both theoretical and empirical studies.By implementing four key simulation scenarios and conducting a detailed parameter sensitivity analysis via 3D and contour plots, we demonstrate that the proposed model is not only mathematically more advanced, but also offers a more realistic representation of human dynamics compared to previous approaches. Beyond its technical contributions, this study outlines a conceptual framework for understanding fragile, jump-driven behaviors in social, economic, and geopolitical systems-where trust is not merely a psychological construct, but an inherently unstable and stochastic variable best captured through L\'evy based modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00008v1</guid>
      <category>stat.AP</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohamadali Berahman, Madjid Eshaghi Gordji</dc:creator>
    </item>
    <item>
      <title>Subgroup Identification and Individualized Treatment Policies: A Tutorial on the Hybrid Two-Stage Workflow</title>
      <link>https://arxiv.org/abs/2601.00136</link>
      <description>arXiv:2601.00136v1 Announce Type: new 
Abstract: Patients in clinical studies often exhibit heterogeneous treatment effect (HTE). Classical subgroup analyses provide inferential tools to test for effect modification, while modern machine learning methods estimate the Conditional Average Treatment Effect (CATE) to enable individual level prediction. Each paradigm has limitations: inference focused approaches may sacrifice predictive utility, and prediction focused approaches often lack statistical guarantees. We present a hybrid two-stage workflow that integrates these perspectives. Stage 1 applies statistical inference to test whether credible treatment effect heterogeneity exists with the protection against spurious findings. Stage 2 translates heterogeneity evidence into individualized treatment policies, evaluated by cross fitted doubly robust (DR) metrics with Neyman-Pearson (NP) constraints on harm. We illustrate the workflow with working examples based on simulated data and a real ACTG 175 HIV trial. This tutorial provides practical implementation checklists and discusses links to sponsor oriented HTE workflows, offering a transparent and auditable pathway from heterogeneity assessment to individualized treatment policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00136v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nan Miles Xi, Xin Huang, Lin Wang</dc:creator>
    </item>
    <item>
      <title>Gradient-free ensemble transform methods for generalized Bayesian inference in generative models</title>
      <link>https://arxiv.org/abs/2601.00760</link>
      <description>arXiv:2601.00760v1 Announce Type: new 
Abstract: Bayesian inference in complex generative models is often obstructed by the absence of tractable likelihoods and the infeasibility of computing gradients of high-dimensional simulators. Existing likelihood-free methods for generalized Bayesian inference typically rely on gradient-based optimization or reparameterization, which can be computationally expensive and often inapplicable to black-box simulators. To overcome these limitations, we introduce a gradient-free ensemble transform Langevin dynamics method for generalized Bayesian inference using the maximum mean discrepancy. By relying on ensemble-based covariance structures rather than simulator derivatives, the proposed method enables robust posterior approximation without requiring access to gradients of the forward model, making it applicable to a broader class of likelihood-free models. The method is affine invariant, computationally efficient, and robust to model misspecification. Through numerical experiments on well-specified chaotic dynamical systems, and misspecified generative models with contaminated data, we demonstrate that the proposed method achieves comparable or improved accuracy relative to existing gradient-based methods, while substantially reducing computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00760v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diksha Bhandari, Sebastian Reich</dc:creator>
    </item>
    <item>
      <title>The Weather Paradox: Why Precipitation Fails to Predict Traffic Accident Severity in Large-Scale US Data</title>
      <link>https://arxiv.org/abs/2601.00152</link>
      <description>arXiv:2601.00152v1 Announce Type: cross 
Abstract: This study investigates the predictive capacity of environmental, temporal, and spatial factors on traffic accident severity in the United States. Using a dataset of 500,000 U.S. traffic accidents spanning 2016-2023, we trained an XGBoost classifier optimized through randomized search cross-validation and adjusted for class imbalance via class weighting. The final model achieves an overall accuracy of 78%, with strong performance on the majority class (Severity 2), attaining 87% precision and recall. Feature importance analysis reveals that time of day, geographic location, and weather-related variables, including visibility, temperature, and wind speed, rank among the strongest predictors of accident severity. However, contrary to initial hypotheses, precipitation and visibility demonstrate limited predictive power, potentially reflecting behavioral adaptation by drivers under overtly hazardous conditions. The dataset's predominance of mid-level severity accidents constrains the model's capacity to learn meaningful patterns for extreme cases, highlighting the need for alternative sampling strategies, enhanced feature engineering, and integration of external datasets. These findings contribute to evidence-based traffic management and suggest future directions for severity prediction research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00152v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yann Bellec, Rohan Kaman, Siwen Cui, Aarav Agrawal, Calvin Chen</dc:creator>
    </item>
    <item>
      <title>Continuous monitoring of delayed outcomes in basket trials</title>
      <link>https://arxiv.org/abs/2601.00499</link>
      <description>arXiv:2601.00499v1 Announce Type: cross 
Abstract: Precision medicine has led to a paradigm shift allowing the development of targeted drugs that are agnostic to the tumor location. In this context, basket trials aim to identify which tumor types - or baskets - would benefit from the targeted therapy among patients with the same molecular marker or mutation. We propose the implementation of continuous monitoring for basket trials to increase the likelihood of early identification of non-promising baskets. Although the current Bayesian trial designs available in the literature can incorporate more than one interim analysis, most of them have high computational cost, and none of them handle delayed outcomes that are expected for targeted treatments such as immunotherapies. We leverage the Bayesian empirical approach proposed by Fujiwara et al., which has low computational cost. We also extend ideas of Cai et al to address the practical challenge of performing interim analysis with delayed outcomes using multiple imputation. Operating characteristics of four different strategies to handle delayed outcomes in basket trials are compared in an extensive simulation study with the benchmark strategy where trial accrual is put on hold until complete data is observed to make a decision. The optimal handling of missing data at interim analyses is trial-dependent. With slow accrual, missingness is minimal even with continuous monitoring, favoring simpler approaches over computationally intensive methods. Although individual sample-size savings are small, multiple imputation becomes more appealing when sample size savings scale with the number of baskets and agents tested.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00499v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcio A. Diniz, Hulya Kocyigit, Erin Moshier, Madhu Mazumdar, Deukwoo Kwon</dc:creator>
    </item>
    <item>
      <title>Probability-Aware Parking Selection</title>
      <link>https://arxiv.org/abs/2601.00521</link>
      <description>arXiv:2601.00521v1 Announce Type: cross 
Abstract: Current parking navigation systems often underestimate total travel time by failing to account for the time spent searching for a parking space, which significantly affects user experience, mode choice, congestion, and emissions. To address this issue, this paper introduces the probability-aware parking selection problem, which aims to direct drivers to the best parking location rather than straight to their destination. An adaptable dynamic programming framework is proposed for decision-making based on probabilistic information about parking availability at the parking lot level. Closed-form analysis determines when it is optimal to target a specific parking lot or explore alternatives, as well as the expected time cost. Sensitivity analysis and three illustrative cases are examined, demonstrating the model's ability to account for the dynamic nature of parking availability. Acknowledging the financial costs of permanent sensing infrastructure, the paper provides analytical and empirical assessments of errors incurred when leveraging stochastic observations to estimate parking availability. Experiments with real-world data from the US city of Seattle indicate this approach's viability, with mean absolute error decreasing from 7% to below 2% as observation frequency grows. In data-based simulations, probability-aware strategies demonstrate time savings up to 66% relative to probability-unaware baselines, yet still take up to 123% longer than direct-to-destination estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00521v1</guid>
      <category>eess.SY</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cameron Hickert, Sirui Li, Zhengbing He, Cathy Wu</dc:creator>
    </item>
    <item>
      <title>Bayesian optimization for re-analysis and calibration of extreme sea state events simulated with a spectral third-generation wave model</title>
      <link>https://arxiv.org/abs/2601.00628</link>
      <description>arXiv:2601.00628v1 Announce Type: cross 
Abstract: Accurate hindcasting of extreme sea state events is essential for coastal engineering, risk assessment, and climate studies. However, the reliability of numerical wave models remains limited by uncertainties in physical parameterizations and model inputs. This study presents a novel calibration framework based on Bayesian Optimization (BO), leveraging the Tree structured Parzen Estimator (TPE) to efficiently estimate uncertain sink term parameters, specifically bottom friction dissipation, depth induced breaking, and wave dissipation from strong opposing currents, in the ANEMOC-3 hindcast wave model. The proposed method enables joint optimization of continuous parameters and discrete model structures, significantly reducing discrepancies between model outputs and observations. Applied to a one month period encompassing multiple intense storm events along the French Atlantic coast, the calibrated model demonstrates improved agreement with buoy measurements, achieving lower bias, RMSE, and scatter index relative to the default sea$-$state solver configuration. The results highlight the potential of BO to automate and enhance wave model calibration, offering a scalable and flexible approach applicable to a wide range of geophysical modeling problems. Future extensions include multi-objective optimization, uncertainty quantification, and integration of additional observational datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00628v1</guid>
      <category>physics.ao-ph</category>
      <category>physics.comp-ph</category>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>C\'edric Goeury, Thierry Fouquet, Maria Teles, Michel Benoit</dc:creator>
    </item>
    <item>
      <title>Defining 3-dimensional marine provinces with phytoplankton compositions</title>
      <link>https://arxiv.org/abs/2512.08035</link>
      <description>arXiv:2512.08035v2 Announce Type: replace 
Abstract: Marine provinces rarely include fine-resolution biological data, and are often defined spatially across only latitude and longitude. Therefore, we aimed to determine how phytoplankton distributions define marine provinces across 3-dimensions (i.e., latitude, longitude, and depth). To do this, we developed a new algorithm called \texttt{bioprovince} which can be applied to compositional biological data. The algorithm first clusters compositional samples to identify spatially coherent groups of samples, then makes flexible province predictions in the broader 3d spatial grid based on environmental similarity. We applied \texttt{bioprovince} to phytoplankton Amplicon Sequencing Variants (ASVs) from five, depth-resolved ocean transects spanning north-south in the Pacific Ocean. In the surface layer of the ocean, our method agreed well with traditional Longhurst provinces. In some cases, the method revealed that with more granular taxonomic resolution afforded by ASVs, traditional Longhurst provinces were divided into smaller zones. Also, one of the major advances of this method is its ability to incorporate a third dimension, depth. Indeed, our analysis found significant depth-wise partitions throughout the Pacific with remarkable agreement in the equatorial region with the base of the euphotic zone. Our algorithm's ability to delineate 3-dimensional bioprovinces will enable scientists to discover new ecological interpretations of marine phytoplankton ecology and biogeography. Furthermore, as compositional biological data inherently exists in three spatial dimensions in nature, bioprovince is broadly applicable beyond marine plankton, offering a more holistic perspective on biological provinces across diverse environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08035v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rafael Catoia Pulgrossi, Nathan L R Williams, Yubin Raut, Jed Fuhrman, Sangwon Hyun</dc:creator>
    </item>
    <item>
      <title>General Equilibrium Amplification and Crisis Vulnerability: Cross-Crisis Evidence from Global Banks</title>
      <link>https://arxiv.org/abs/2510.24775</link>
      <description>arXiv:2510.24775v2 Announce Type: replace-cross 
Abstract: This paper develops a continuous framework for analyzing financial contagion that incorporates both geographic proximity and interbank network linkages. The framework characterizes stress propagation through a master equation whose solution admits a Feynman-Kac representation as expected cumulative stress along stochastic paths through spatial-network space. From this representation, I derive the General Equilibrium Amplification Factor -- a structural measure of systemic importance that captures the ratio of total system-wide effects to direct effects following a localized shock. The amplification factor decomposes naturally into spatial, network, and interaction components, revealing which transmission channels contribute most to each institution's systemic importance. The framework nests discrete cascade models as a limiting case when jump intensity becomes infinite above default thresholds, clarifying that continuous and discrete approaches describe different regimes of the same phenomenon. Empirical validation using 38 global banks across the 2008 financial crisis and COVID-19 pandemic demonstrates that the amplification factor correctly identifies systemically important institutions (Pearson correlation $\rho = -0.450$, $p = 0.080$ between amplification factor and crisis drawdowns) and predicts crisis outcomes out-of-sample ($\rho = -0.352$ for COVID-19). Robustness analysis using cumulative abnormal returns -- a measure more directly connected to the Feynman-Kac integral -- strengthens these findings ($\rho = -0.512$, $p = 0.042$). Time-series analysis confirms that average pairwise bank correlations track macroeconomic stress indicators ($\rho = 0.265$ with VIX, $p &lt; 0.001$). Comparing the two crises reveals that COVID-19 produced a sharper correlation spike (+93%) despite smaller equity losses, reflecting different contagion dynamics for exogenous versus endogenous shocks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24775v2</guid>
      <category>econ.EM</category>
      <category>q-fin.GN</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>A Clustering Approach for Basket Trials Based on Treatment Response Trajectories</title>
      <link>https://arxiv.org/abs/2511.09890</link>
      <description>arXiv:2511.09890v2 Announce Type: replace-cross 
Abstract: Heterogeneity in efficacy is sometimes observed across baskets in basket trials. In this study, we propose a model-free clustering framework that groups baskets based on transition probabilities derived from the trajectories of treatment response, rather than relying solely on a single efficacy endpoint such as the objective response rate. The number of clusters is not predetermined but is automatically determined in a data-driven manner based on the similarity structure among baskets. After clustering, baskets within the same cluster are analyzed using a hierarchical Bayesian model. This framework aims to improve the estimation precision of efficacy endpoints and enhance statistical power while maintaining the type~I error rate at the nominal level. The performance of the proposed method was evaluated through simulation studies. The results demonstrated that the proposed method can accurately identify cluster structures in heterogeneous settings and, even under such conditions, maintain the type~I error rate at the nominal level while improving statistical power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09890v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masahiro Kojima, Keisuke Hanada, Atsuya Sato</dc:creator>
    </item>
    <item>
      <title>Sequential Randomization Tests Using e-values: Applications for trial monitoring</title>
      <link>https://arxiv.org/abs/2512.04366</link>
      <description>arXiv:2512.04366v5 Announce Type: replace-cross 
Abstract: Sequential monitoring of randomized trials traditionally relies on parametric assumptions or asymptotic approximations. We discuss a nonparametric sequential test and its application to continuous and time-to-event endpoints that derives validity solely from the randomization mechanism. Using a betting framework, these tests constructs a test martingale by sequentially wagering on treatment assignments given observed outcomes. Under the null hypothesis of no treatment effect, the expected wealth cannot grow, guaranteeing anytime-valid Type I error control regardless of stopping rule. We prove validity and present simulation studies demonstrating calibration and power. These methods provide a conservative, assumption-free complement to model-based sequential analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04366v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fernando G Zampieri</dc:creator>
    </item>
  </channel>
</rss>
