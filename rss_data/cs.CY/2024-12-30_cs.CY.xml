<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 30 Dec 2024 05:00:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Quantifying the Risk of Pastoral Conflict in 4 Central African Countries</title>
      <link>https://arxiv.org/abs/2412.18799</link>
      <description>arXiv:2412.18799v1 Announce Type: new 
Abstract: Climate change is becoming a widely recognized risk factor of farmer-herder conflict in Africa. Using an 8 year dataset (Jan 2015 to Sep 2022) of detailed weather and terrain data across four African nations, we apply statistical and machine learning methods to analyze pastoral conflict. We test hypotheses linking these variables with pastoral conflict within each country using geospatial and statistical analysis. Complementing this analysis are risk maps automatically updated for decision-makers. Our models estimate which cells have a high likelihood of experiencing pastoral conflict with high predictive accuracy and study the variation of this accuracy with the granularity of the cells.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18799v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lirika Solaa, Youdinghuan Chen, Samantha K. Murphy, V. S. Subrahmanian</dc:creator>
    </item>
    <item>
      <title>Reflection on Purpose Changes Students' Academic Interests: A Scalable Intervention in an Online Course Catalog</title>
      <link>https://arxiv.org/abs/2412.19035</link>
      <description>arXiv:2412.19035v1 Announce Type: new 
Abstract: College students routinely use online course catalogs to explore a variety of academic offerings. Course catalogs may therefore be an effective place to encourage reflection on academic choices and interests. To test this, we embedded a psychological intervention in an online course catalog to encourage students to reflect on their purpose during course exploration. Results of a randomized field experiment with over 4,000 students at a large U.S. university show that a purpose intervention increased students' cognitive engagement in describing their interests, but reduced search activities. Students became more interested in courses related to creative arts and social change, but less in computer and data science. The findings demonstrate the malleability of students' interests during course exploration and suggest practical strategies to support purpose reflection and guide students toward deliberate exploration of their interests in higher education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19035v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youjie Chen, Pranathi Iyer, Rene F. Kizilcec</dc:creator>
    </item>
    <item>
      <title>Anvendelse av kunstig intelligens (KI) i Norge i norsk offentlig sektor 2024</title>
      <link>https://arxiv.org/abs/2412.19273</link>
      <description>arXiv:2412.19273v1 Announce Type: new 
Abstract: There are great expectations for the use of AI in Norway. On the other hand, it is reported that the adoption of AI in Norway is slower than expected in both the private and public sectors. Using responses from NOKIOS Technology Radar 2017-2021, IT in Practice surveys conducted by Ramboll in 2021-2024, as well as another national survey as part of a five-year cycle, this article looks at reported and planned use of AI with a focus on local (municipalities) and national government agencies. IT in practice is distributed to a large number of Norwegian public agencies, with a response rate of over 5o percent. The most recent data (2024) presented in this article is based on responses from 335 public organizations, with 237 municipalities, and 98 public organizations at the national or regional level. The survey confirms that the use of AI is still at an early stage, although expectations are high for future use.
  --
  Det er store forventninger til bruk av KI i Norge. P{\aa} den annen side rapporteres det at adopsjonen av KI i Norge g{\aa}r tregere enn forventet b{\aa}de i privat og offentlig sektor. Ved hjelp av svar fra NOKIOS teknologiradar 2017-2021, IT i Praksis unders{\o}kelser utf{\o}rt av Ramb{\o}ll i 2021-2024, samt en annen nasjonal unders{\o}kelse som en del av en fem{\aa}rig syklus, ser vi i denne artikkelen p{\aa} rapportert og planlagt bruk av KI med fokus p{\aa} lokale (kommuner) og nasjonale offentlige etater. IT i praksis distribueres til en lang rekke norske offentlige virksomheter, med en svarprosent p{\aa} over 50 prosent. De nyeste dataene (2024) presentert i denne artikkelen er basert p{\aa} svar fra 335 offentlige organisasjoner, med 237 kommuner, og 98 offentlige organisasjoner p{\aa} nasjonalt eller regionalt niv{\aa}. Unders{\o}kelsen bekrefter at bruken av KI fortsatt er p{\aa} et tidlig stadium, selv om forventningene er h{\o}ye til fremtidig bruk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19273v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>John Krogstie</dc:creator>
    </item>
    <item>
      <title>The Internet of Value: Integrating Blockchain and Lightning Network Micropayments for Knowledge Markets</title>
      <link>https://arxiv.org/abs/2412.19384</link>
      <description>arXiv:2412.19384v1 Announce Type: new 
Abstract: Q&amp;A websites rely on user-generated responses, with incentives such as reputation scores or monetary rewards often offered. While some users may find it intrinsically rewarding to assist others, studies indicate that payment can improve the quality and speed of answers. However, traditional payment processors impose minimum thresholds that many Q&amp;A inquiries fall below. The introduction of Bitcoin enabled direct digital value transfer, yet frequent micropayments remain challenging. Recent advancements like the Lightning Network now allow frictionless micropayments by reducing costs and minimising reliance on intermediaries. This development fosters an "Internet of Value," where transferring even small amounts of money is as simple as sharing data. This study investigates integrating Lightning Network-based micropayment strategies into Q&amp;A platforms, aiming to create a knowledge market free of minimum payment barriers. A survey was conducted to address the gap below the $2 payment level identified in prior research. Responses confirmed that incentives for asking and answering weaken as payments decrease. Findings reveal even minimal payments, such as {\pounds}0.01, significantly encourage higher quality and effort in responses. The study recommends micropayment incentives for service-oriented applications, particularly Q&amp;A platforms. By leveraging the Lightning Network to remove barriers, a more open marketplace can emerge, improving engagement and outcomes. Further research is needed to confirm if users follow through on reported intentions when spending funds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19384v1</guid>
      <category>cs.CY</category>
      <category>cs.CE</category>
      <category>cs.ET</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ellis Solaiman, Jorge Robins</dc:creator>
    </item>
    <item>
      <title>From prediction to explanation: managing influential negative reviews through explainable AI</title>
      <link>https://arxiv.org/abs/2412.19692</link>
      <description>arXiv:2412.19692v1 Announce Type: new 
Abstract: The profound impact of online reviews on consumer decision-making has made it crucial for businesses to manage negative reviews. Recent advancements in artificial intelligence (AI) technology have offered businesses novel and effective ways to manage and analyze substantial consumer feedback. In response to the growing demand for explainablility and transparency in AI applications, this study proposes a novel explainable AI (XAI) algorithm aimed at identifying influential negative reviews. The experiments conducted on 101,338 restaurant reviews validate the algorithm's effectiveness and provides understandable explanations from both the feature-level and word-level perspectives. By leveraging this algorithm, businesses can gain actionable insights for predicting, perceiving, and strategically responding to online negative feedback, fostering improved customer service and mitigating the potential damage caused by negative reviews.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19692v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rongping Shen</dc:creator>
    </item>
    <item>
      <title>Edge-AI for Agriculture: Lightweight Vision Models for Disease Detection in Resource-Limited Settings</title>
      <link>https://arxiv.org/abs/2412.18635</link>
      <description>arXiv:2412.18635v1 Announce Type: cross 
Abstract: This research paper presents the development of a lightweight and efficient computer vision pipeline aimed at assisting farmers in detecting orange diseases using minimal resources. The proposed system integrates advanced object detection, classification, and segmentation models, optimized for deployment on edge devices, ensuring functionality in resource-limited environments. The study evaluates the performance of various state-of-the-art models, focusing on their accuracy, computational efficiency, and generalization capabilities. Notable findings include the Vision Transformer achieving 96 accuracy in orange species classification and the lightweight YOLOv8-S model demonstrating exceptional object detection performance with minimal computational overhead. The research highlights the potential of modern deep learning architectures to address critical agricultural challenges, emphasizing the importance of model complexity versus practical utility. Future work will explore expanding datasets, model compression techniques, and federated learning to enhance the applicability of these systems in diverse agricultural contexts, ultimately contributing to more sustainable farming practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18635v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harsh Joshi</dc:creator>
    </item>
    <item>
      <title>Interplay of ISMS and AIMS in context of the EU AI Act</title>
      <link>https://arxiv.org/abs/2412.18670</link>
      <description>arXiv:2412.18670v1 Announce Type: cross 
Abstract: The EU AI Act (AIA) mandates the implementation of a risk management system (RMS) and a quality management system (QMS) for high-risk AI systems. The ISO/IEC 42001 standard provides a foundation for fulfilling these requirements but does not cover all EU-specific regulatory stipulations. To enhance the implementation of the AIA in Germany, the Federal Office for Information Security (BSI) could introduce the national standard BSI 200-5, which specifies AIA requirements and integrates existing ISMS standards, such as ISO/IEC 27001. This paper examines the interfaces between an information security management system (ISMS) and an AI management system (AIMS), demonstrating that incorporating existing ISMS controls with specific AI extensions presents an effective strategy for complying with Article 15 of the AIA. Four new AI modules are introduced, proposed for inclusion in the BSI IT Grundschutz framework to comprehensively ensure the security of AI systems. Additionally, an approach for adapting BSI's qualification and certification systems is outlined to ensure that expertise in secure AI handling is continuously developed. Finally, the paper discusses how the BSI could bridge international standards and the specific requirements of the AIA through the nationalization of ISO/IEC 42001, creating synergies and bolstering the competitiveness of the German AI landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18670v1</guid>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jordan P\"otsch</dc:creator>
    </item>
    <item>
      <title>Engineering Digital Systems for Humanity: a Research Roadmap</title>
      <link>https://arxiv.org/abs/2412.19668</link>
      <description>arXiv:2412.19668v1 Announce Type: cross 
Abstract: As testified by new regulations like the European AI Act, worries about the human and societal impact of (autonomous) software technologies are becoming of public concern. Human, societal, and environmental values, alongside traditional software quality, are increasingly recognized as essential for sustainability and long-term well-being. Traditionally, systems are engineered taking into account business goals and technology drivers. Considering the growing awareness in the community, in this paper, we argue that engineering of systems should also consider human, societal, and environmental drivers. Then, we identify the macro and technological challenges by focusing on humans and their role while co-existing with digital systems. The first challenge considers humans in a proactive role when interacting with digital systems, i.e., taking initiative in making things happen instead of reacting to events. The second concerns humans having a reactive role in interacting with digital systems, i.e., humans interacting with digital systems as a reaction to events. The third challenge focuses on humans with a passive role, i.e., they experience, enjoy or even suffer the decisions and/or actions of digital systems. The fourth challenge concerns the duality of trust and trustworthiness, with humans playing any role. Building on the new human, societal, and environmental drivers and the macro and technological challenges, we identify a research roadmap of digital systems for humanity. The research roadmap is concretized in a number of research directions organized into four groups: development process, requirements engineering, software architecture and design, and verification and validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19668v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>ACM Transactions on Software Engineering and Methodology (TOSEM), 2025</arxiv:journal_reference>
      <dc:creator>Marco Autili, Martina De Sanctis, Paola Inverardi, Patrizio Pelliccione</dc:creator>
    </item>
    <item>
      <title>Heuristics for Inequality minimization in PageRank values</title>
      <link>https://arxiv.org/abs/2310.18537</link>
      <description>arXiv:2310.18537v2 Announce Type: replace 
Abstract: PageRank is a widely used algorithm for ranking webpages and plays a significant role in determining web traffic. This study employs the Gini coefficient, a measure of income/wealth inequality, to assess the inequality in PageRank distributions and explores six deterministic methods for reducing inequality. Our findings indicate that a combination of two distinct heuristics may present an effective strategy for minimizing inequality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18537v2</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhajit Sahu</dc:creator>
    </item>
    <item>
      <title>Reducing Urban Speed Limits Decreases Work-Related Traffic Injury Severity: Evidence from Santiago, Chile</title>
      <link>https://arxiv.org/abs/2408.00687</link>
      <description>arXiv:2408.00687v2 Announce Type: replace 
Abstract: Work-related transportation incidents significantly impact urban mobility and productivity. These incidents include traffic crashes, collisions between vehicles, and falls that occurred during commuting or work-related transportation (e.g., falling while getting off a bus during the morning commute or while riding a bicycle for work). This study analyzes a decade of work-related transportation incident data (2012--2021) in Santiago, Chile, using records from a major worker's insurance company. Using negative binomial regression, we assess the impact of a 2018 urban speed limit reduction law on incident injury severity. We also explore broader temporal, spatial, and demographic patterns in these incidents in urban and rural areas.
  The urban speed limit reduction is associated with a decrease of 4.26 days in prescribed medical leave for incidents in urban areas, suggesting that lower speed limits contribute to reduced injury severity. Our broader analysis reveals distinct incident patterns across different groups. Workers traveling by motorcycle and bicycle experience more severe injuries when involved in traffic incidents, with marginal effects of 26.94 and 13.06 additional days of medical leave, respectively, compared to motorized vehicles. Women workers tend to have less severe injuries, with an average of 7.57 fewer days of medical leave. Age is also a significant factor, with older workers experiencing more severe injuries -- each additional year of age is associated with 0.57 more days of medical leave. Our results provide insights for urban planning, transportation policy, and workplace safety initiatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00687v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eduardo Graells-Garrido, Mat\'ias Toro, Gabriel Mansilla, Mat\'ias Nicolai, Santiago Mansilla, Jocelyn Dunstan</dc:creator>
    </item>
    <item>
      <title>LearnLM: Improving Gemini for Learning</title>
      <link>https://arxiv.org/abs/2412.16429</link>
      <description>arXiv:2412.16429v2 Announce Type: replace 
Abstract: Today's generative AI systems are tuned to present information by default rather than engage users in service of learning as a human tutor would. To address the wide range of potential education use cases for these systems, we reframe the challenge of injecting pedagogical behavior as one of \textit{pedagogical instruction following}, where training and evaluation examples include system-level instructions describing the specific pedagogy attributes present or desired in subsequent model turns. This framing avoids committing our models to any particular definition of pedagogy, and instead allows teachers or developers to specify desired model behavior. It also clears a path to improving Gemini models for learning -- by enabling the addition of our pedagogical data to post-training mixtures -- alongside their rapidly expanding set of capabilities. Both represent important changes from our initial tech report. We show how training with pedagogical instruction following produces a LearnLM model (available on Google AI Studio) that is preferred substantially by expert raters across a diverse set of learning scenarios, with average preference strengths of 31\% over GPT-4o, 11\% over Claude 3.5, and 13\% over the Gemini 1.5 Pro model LearnLM was based on.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16429v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator> LearnLM Team, Abhinit Modi, Aditya Srikanth Veerubhotla, Aliya Rysbek, Andrea Huber, Brett Wiltshire, Brian Veprek, Daniel Gillick, Daniel Kasenberg, Derek Ahmed, Irina Jurenka, James Cohan, Jennifer She, Julia Wilkowski, Kaiz Alarakyia, Kevin R. McKee, Lisa Wang, Markus Kunesch, Mike Schaekermann, Miruna P\^islar, Nikhil Joshi, Parsa Mahmoudieh, Paul Jhun, Sara Wiltberger, Shakir Mohamed, Shashank Agarwal, Shubham Milind Phal, Sun Jae Lee, Theofilos Strinopoulos, Wei-Jen Ko, Amy Wang, Ankit Anand, Avishkar Bhoopchand, Dan Wild, Divya Pandya, Filip Bar, Garth Graham, Holger Winnemoeller, Mahvish Nagda, Prateek Kolhar, Renee Schneider, Shaojian Zhu, Stephanie Chan, Steve Yadlowsky, Viknesh Sounderajah, Yannis Assael</dc:creator>
    </item>
    <item>
      <title>Explore the Potential of LLMs in Misinformation Detection: An Empirical Study</title>
      <link>https://arxiv.org/abs/2311.12699</link>
      <description>arXiv:2311.12699v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have garnered significant attention for their powerful ability in natural language understanding and reasoning. In this paper, we present a comprehensive empirical study to explore the performance of LLMs on misinformation detection tasks. This study stands as the pioneering investigation into the understanding capabilities of multiple LLMs regarding both content and propagation across social media platforms. Our empirical studies on eight misinformation detection datasets show that LLM-based detectors can achieve comparable performance in text-based misinformation detection but exhibit notably constrained capabilities in comprehending propagation structure compared to existing models in propagation-based misinformation detection. Our experiments further demonstrate that LLMs exhibit great potential to enhance existing misinformation detection models. These findings highlight the potential ability of LLMs to detect misinformation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.12699v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengyang Chen, Lingwei Wei, Han Cao, Wei Zhou, Songlin Hu</dc:creator>
    </item>
    <item>
      <title>A Mathematical Framework for the Problem of Security for Cognition in Neurotechnology</title>
      <link>https://arxiv.org/abs/2403.07945</link>
      <description>arXiv:2403.07945v3 Announce Type: replace-cross 
Abstract: The rapid advancement in neurotechnology in recent years has created an emerging critical intersection between neurotechnology and security. Implantable devices, non-invasive monitoring, and non-invasive therapies all carry with them the prospect of violating the privacy and autonomy of individuals' cognition. A growing number of scientists and physicians have made calls to address this issue, but applied efforts have been relatively limited. A major barrier hampering scientific and engineering efforts to address these security issues is the lack of a clear means of describing and analyzing relevant problems. In this paper we develop Cognitive Neurosecurity, a mathematical framework which enables such description and analysis by drawing on methods and results from multiple fields. We demonstrate certain statistical properties which have significant implications for Cognitive Neurosecurity, and then present descriptions of the algorithmic problems faced by attackers attempting to violate privacy and autonomy, and defenders attempting to obstruct such attempts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07945v3</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bryce Allen Bagley, Claudia K Petritsch</dc:creator>
    </item>
    <item>
      <title>Biomedical Open Source Software: Crucial Packages and Hidden Heroes</title>
      <link>https://arxiv.org/abs/2404.06672</link>
      <description>arXiv:2404.06672v2 Announce Type: replace-cross 
Abstract: Despite the importance of scientific software for research, it is often not formally recognized and rewarded. This is especially true for foundation libraries, which are used by the software packages visible to the users, being ``hidden'' themselves. The funders and other organizations need to understand the complex network of computer programs that the modern research relies upon.
  In this work we used CZ Software Mentions Dataset to map the dependencies of the software used in biomedical papers and find the packages critical to the software ecosystems. We propose the centrality metrics for the network of software dependencies, analyze three ecosystems (PyPi, CRAN, Bioconductor) and determine the packages with the highest centrality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06672v2</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Nesbitt, Boris Veytsman, Daniel Mietchen, Eva Maxfield Brown, James Howison, Jo\~ao Felipe Pimentel, Laurent H\`ebert-Dufresne, Stephan Druskat</dc:creator>
    </item>
    <item>
      <title>FairLay-ML: Intuitive Debugging of Fairness in Data-Driven Social-Critical Software</title>
      <link>https://arxiv.org/abs/2407.01423</link>
      <description>arXiv:2407.01423v2 Announce Type: replace-cross 
Abstract: Data-driven software solutions have significantly been used in critical domains with significant socio-economic, legal, and ethical implications. The rapid adoptions of data-driven solutions, however, pose major threats to the trustworthiness of automated decision-support software. A diminished understanding of the solution by the developer and historical/current biases in the data sets are primary challenges. To aid data-driven software developers and end-users, we present FairLay-ML, a debugging tool to test and explain the fairness implications of data-driven solutions. FairLay-ML visualizes the logic of datasets, trained models, and decisions for a given data point. In addition, it trains various models with varying fairness-accuracy trade-offs. Crucially, FairLay-ML incorporates counterfactual fairness testing that finds bugs beyond the development datasets. We conducted two studies through FairLay-ML that allowed us to measure false positives/negatives in prevalent counterfactual testing and understand the human perception of counterfactual test cases in a class survey. FairLay-ML and its benchmarks are publicly available at https://github.com/Pennswood/FairLay-ML. The live version of the tool is available at https://fairlayml-v2.streamlit.app/. We provide a video demo of the tool at https://youtu.be/wNI9UWkywVU?t=133.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01423v2</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Normen Yu, Luciana Carreon, Gang Tan, Saeid Tizpaz-Niari</dc:creator>
    </item>
    <item>
      <title>Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?</title>
      <link>https://arxiv.org/abs/2407.21792</link>
      <description>arXiv:2407.21792v3 Announce Type: replace-cross 
Abstract: As artificial intelligence systems grow more powerful, there has been increasing interest in "AI safety" research to address emerging and future risks. However, the field of AI safety remains poorly defined and inconsistently measured, leading to confusion about how researchers can contribute. This lack of clarity is compounded by the unclear relationship between AI safety benchmarks and upstream general capabilities (e.g., general knowledge and reasoning). To address these issues, we conduct a comprehensive meta-analysis of AI safety benchmarks, empirically analyzing their correlation with general capabilities across dozens of models and providing a survey of existing directions in AI safety. Our findings reveal that many safety benchmarks highly correlate with both upstream model capabilities and training compute, potentially enabling "safetywashing"--where capability improvements are misrepresented as safety advancements. Based on these findings, we propose an empirical foundation for developing more meaningful safety metrics and define AI safety in a machine learning research context as a set of clearly delineated research goals that are empirically separable from generic capabilities advancements. In doing so, we aim to provide a more rigorous framework for AI safety research, advancing the science of safety evaluations and clarifying the path towards measurable progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21792v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Richard Ren, Steven Basart, Adam Khoja, Alice Gatti, Long Phan, Xuwang Yin, Mantas Mazeika, Alexander Pan, Gabriel Mukobi, Ryan H. Kim, Stephen Fitz, Dan Hendrycks</dc:creator>
    </item>
    <item>
      <title>A Systems Thinking Approach to Algorithmic Fairness</title>
      <link>https://arxiv.org/abs/2412.16641</link>
      <description>arXiv:2412.16641v2 Announce Type: replace-cross 
Abstract: Systems thinking provides us with a way to model the algorithmic fairness problem by allowing us to encode prior knowledge and assumptions about where we believe bias might exist in the data generating process. We can then model this using a series of causal graphs, enabling us to link AI/ML systems to politics and the law. By treating the fairness problem as a complex system, we can combine techniques from machine learning, causal inference, and system dynamics. Each of these analytical techniques is designed to capture different emergent aspects of fairness, allowing us to develop a deeper and more holistic view of the problem. This can help policymakers on both sides of the political aisle to understand the complex trade-offs that exist from different types of fairness policies, providing a blueprint for designing AI policy that is aligned to their political agendas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16641v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chris Lam</dc:creator>
    </item>
    <item>
      <title>SoK: On the Offensive Potential of AI</title>
      <link>https://arxiv.org/abs/2412.18442</link>
      <description>arXiv:2412.18442v2 Announce Type: replace-cross 
Abstract: Our society increasingly benefits from Artificial Intelligence (AI). Unfortunately, more and more evidence shows that AI is also used for offensive purposes. Prior works have revealed various examples of use cases in which the deployment of AI can lead to violation of security and privacy objectives. No extant work, however, has been able to draw a holistic picture of the offensive potential of AI. In this SoK paper we seek to lay the ground for a systematic analysis of the heterogeneous capabilities of offensive AI. In particular we (i) account for AI risks to both humans and systems while (ii) consolidating and distilling knowledge from academic literature, expert opinions, industrial venues, as well as laypeople -- all of which being valuable sources of information on offensive AI.
  To enable alignment of such diverse sources of knowledge, we devise a common set of criteria reflecting essential technological factors related to offensive AI. With the help of such criteria, we systematically analyze: 95 research papers; 38 InfoSec briefings (from, e.g., BlackHat); the responses of a user study (N=549) entailing individuals with diverse backgrounds and expertise; and the opinion of 12 experts. Our contributions not only reveal concerning ways (some of which overlooked by prior work) in which AI can be offensively used today, but also represent a foothold to address this threat in the years to come.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18442v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Saskia Laura Schr\"oer, Giovanni Apruzzese, Soheil Human, Pavel Laskov, Hyrum S. Anderson, Edward W. N. Bernroider, Aurore Fass, Ben Nassi, Vera Rimmer, Fabio Roli, Samer Salam, Ashley Shen, Ali Sunyaev, Tim Wadwha-Brown, Isabel Wagner, Gang Wang</dc:creator>
    </item>
  </channel>
</rss>
