<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 03 Nov 2025 05:01:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Impact of clinical decision support systems (cdss) on clinical outcomes and healthcare delivery in low- and middle-income countries: protocol for a systematic review and meta-analysis</title>
      <link>https://arxiv.org/abs/2510.26812</link>
      <description>arXiv:2510.26812v1 Announce Type: new 
Abstract: Clinical decision support systems (CDSS) are used to improve clinical and service outcomes, yet evidence from low- and middle-income countries (LMICs) is dispersed. This protocol outlines methods to quantify the impact of CDSS on patient and healthcare delivery outcomes in LMICs. We will include comparative quantitative designs (randomized trials, controlled before-after, interrupted time series, comparative cohorts) evaluating CDSS in World Bank-defined LMICs. Standalone qualitative studies are excluded; mixed-methods studies are eligible only if they report comparative quantitative outcomes, for which we will extract the quantitative component. Searches (from inception to 30 September 2024) will cover MEDLINE, Embase, CINAHL, CENTRAL, Web of Science, Global Health, Scopus, IEEE Xplore, LILACS, African Index Medicus, and IndMED, plus grey sources. Screening and extraction will be performed in duplicate. Risk of bias will be assessed with RoB 2 (randomized trials) and ROBINS-I (non-randomized). Random-effects meta-analysis will be performed where outcomes are conceptually or statistically comparable; otherwise, a structured narrative synthesis will be presented. Heterogeneity will be explored using relative and absolute metrics and a priori subgroups or meta-regression (condition area, care level, CDSS type, readiness proxies, study design).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26812v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Garima Jain, Anand Bodade, Sanghamitra Pati</dc:creator>
    </item>
    <item>
      <title>Residual Distribution Predictive Systems</title>
      <link>https://arxiv.org/abs/2510.26914</link>
      <description>arXiv:2510.26914v1 Announce Type: new 
Abstract: Conformal predictive systems are sets of predictive distributions with theoretical out-of-sample calibration guarantees. The calibration guarantees are typically that the set of predictions contains a forecast distribution whose prediction intervals exhibit the correct marginal coverage at all levels. Conformal predictive systems are constructed using conformity measures that quantify how well possible outcomes conform with historical data. However, alternative methods have been proposed to construct predictive systems with more appealing theoretical properties. We study an approach to construct predictive systems that we term Residual Distribution Predictive Systems. In the split conformal setting, this approach nests conformal predictive systems with a popular class of conformity measures, providing an alternative perspective on the classical approach. In the full conformal setting, the two approaches differ, and the new approach has the advantage that it does not rely on a conformity measure satisfying fairly stringent requirements to ensure that the predictive system is well-defined; it can readily be implemented alongside any point-valued regression method to yield predictive systems with out-of-sample calibration guarantees. The empirical performance of this approach is assessed using simulated data, where it is found to perform competitively with conformal predictive systems. However, the new approach offers considerable scope for implementation with alternative regression methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26914v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sam Allen, Enrico Pescara, Johanna Ziegel</dc:creator>
    </item>
    <item>
      <title>A novel generalized additive scalar-on-function regression model for partially observed multidimensional functional data: An application to air quality classification</title>
      <link>https://arxiv.org/abs/2510.26917</link>
      <description>arXiv:2510.26917v1 Announce Type: new 
Abstract: In this work we propose a generalized additive functional regression model for partially observed functional data. Our approach accommodates functional predictors of varying dimensions without requiring imputation of missing observations. Both the functional coefficients and covariates are represented using basis function expansions, with B-splines used in this study, though the method is not restricted to any specific basis choice. Model coefficients are estimated via penalized likelihood, leveraging the mixed model representation of penalized splines for efficient computation and smoothing parameter estimation.The performance of the proposed approach is assessed through two simulation studies: one involving two one-dimensional functional covariates, and another using a two-dimensional functional covariate. Finally, we demonstrate the practical utility of our method in an application to air-pollution classification in Dimapur, India, where images are treated as observations of a two-dimensional functional variable. This case study highlights the models ability to effectively handle incomplete functional data and to accurately discriminate between pollution levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26917v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pavel Hern\'andez-Amaro, Maria Durban, M. Carmen Aguilera-Morillo</dc:creator>
    </item>
    <item>
      <title>Finite Sample MIMO System Identification with Multisine Excitation: Nonparametric, Direct, and Two-step Parametric Estimators</title>
      <link>https://arxiv.org/abs/2510.26929</link>
      <description>arXiv:2510.26929v1 Announce Type: new 
Abstract: Multisine excitations are widely used for identifying multi-input multi-output systems due to their periodicity, data compression properties, and control over the input spectrum. Despite their popularity, the finite sample statistical properties of frequency-domain estimators under multisine excitation, for both nonparametric and parametric settings, remain insufficiently understood. This paper develops a finite-sample statistical framework for least-squares estimation of the frequency response function (FRF) and its implications for parametric modeling. First, we derive exact distributional and covariance properties of the FRF estimator, explicitly accounting for aliasing effects under slow sampling regimes, and establish conditions for unbiasedness, uncorrelatedness, and consistency across multiple experiments. Second, we show that the FRF estimate is a sufficient statistic for any parametric model under Gaussian noise, leading to an exact equivalence between optimal two stage frequency-domain methods and time-domain prediction error and maximum likelihood estimation. This equivalence is shown to yield finite-sample concentration bounds for parametric maximum likelihood estimators, enabling rigorous uncertainty quantification, and closed-form prediction error method estimators without iterative optimization. The theoretical results are demonstrated in a representative case study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26929v1</guid>
      <category>stat.ME</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rodrigo A. Gonz\'alez, Koen Classens, Cristian R. Rojas, Tom Oomen, H{\aa}kan Hjalmarsson</dc:creator>
    </item>
    <item>
      <title>The Interplay between Bayesian Inference and Conformal Prediction</title>
      <link>https://arxiv.org/abs/2510.26930</link>
      <description>arXiv:2510.26930v1 Announce Type: new 
Abstract: Conformal prediction has emerged as a cutting-edge methodology in statistics and machine learning, providing prediction intervals with finite-sample frequentist coverage guarantees. Yet, its interplay with Bayesian statistics, often criticised for lacking frequentist guarantees, remains underexplored. Recent work has suggested that conformal prediction can serve to "calibrate" Bayesian credible sets, thereby imparting frequentist validity and motivating deeper investigation into frequentist-Bayesian hybrids. We further argue that Bayesian procedures have the potential to enhance conformal prediction, not only in terms of more informative intervals, but also for achieving nearly optimal solutions under a decision-theoretic framework. Thus, the two paradigms can be jointly used for a principled balance between validity and efficiency. This work provides a basis for bridging this gap. After surveying existing ideas, we formalise the Bayesian conformal inference framework, covering challenging aspects such as statistical efficiency and computational complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26930v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nina Deliu, Brunero Liseo</dc:creator>
    </item>
    <item>
      <title>LLMs are Overconfident: Evaluating Confidence Interval Calibration with FermiEval</title>
      <link>https://arxiv.org/abs/2510.26995</link>
      <description>arXiv:2510.26995v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at numerical estimation but struggle to correctly quantify uncertainty. We study how well LLMs construct confidence intervals around their own answers and find that they are systematically overconfident. To evaluate this behavior, we introduce FermiEval, a benchmark of Fermi-style estimation questions with a rigorous scoring rule for confidence interval coverage and sharpness. Across several modern models, nominal 99\% intervals cover the true answer only 65\% of the time on average. With a conformal prediction based approach that adjusts the intervals, we obtain accurate 99\% observed coverage, and the Winkler interval score decreases by 54\%. We also propose direct log-probability elicitation and quantile adjustment methods, which further reduce overconfidence at high confidence levels. Finally, we develop a perception-tunnel theory explaining why LLMs exhibit overconfidence: when reasoning under uncertainty, they act as if sampling from a truncated region of their inferred distribution, neglecting its tails.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26995v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elliot L. Epstein, John Winnicki, Thanawat Sornwanee, Rajat Dwaraknath</dc:creator>
    </item>
    <item>
      <title>Generalized Maximum Entropy: When and Why you need it</title>
      <link>https://arxiv.org/abs/2510.27006</link>
      <description>arXiv:2510.27006v1 Announce Type: new 
Abstract: The classical Maximum-Entropy Principle (MEP) based on Shannon entropy is widely used to construct least-biased probability distributions from partial information. However, the Shore-Johnson axioms that single out the Shannon functional hinge on strong system independence, an assumption often violated in real-world, strongly correlated systems. We provide a self-contained guide to when and why practitioners should abandon the Shannon form in favour of the one-parameter Uffink-Jizba-Korbel (UJK) family of generalized entropies. After reviewing the Shore and Johnson axioms from an applied perspective, we recall the most commonly used entropy functionals and locate them within the UJK family. The need for generalized entropies is made clear with two applications, one rooted in economics and the other in ecology. A simple mathematical model worked out in detail shows the power of generalized maximum entropy approaches in dealing with cases where strong system independence does not hold. We conclude with practical guidelines for choosing an entropy measure and reporting results so that analyses remain transparent and reproducible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27006v1</guid>
      <category>stat.ME</category>
      <category>physics.data-an</category>
      <category>q-bio.PE</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giuseppe M. Ferro, Edwin T. Pos, Andrea Somazzi</dc:creator>
    </item>
    <item>
      <title>Inconsistency thresholds revisited: The effect of the graph associated with incomplete pairwise comparisons</title>
      <link>https://arxiv.org/abs/2510.27011</link>
      <description>arXiv:2510.27011v1 Announce Type: new 
Abstract: The inconsistency of pairwise comparisons remains difficult to interpret in the absence of acceptability thresholds. The popular 10% cut-off rule proposed by Saaty has recently been applied to incomplete pairwise comparison matrices, which contain some unknown comparisons. This paper revises these inconsistency thresholds: we uncover that they depend not only on the size of the matrix and the number of missing entries, but also on the undirected graph whose edges represent the known pairwise comparisons. Therefore, using our exact thresholds is especially important if the filling in patterns coincide for a large number of matrices, as has been recommended in the literature. The strong association between the new threshold values and the spectral radius of the representing graph is also demonstrated. Our results can be integrated into software to continuously monitor inconsistency during the collection of pairwise comparisons and immediately detect potential errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27011v1</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kolos Csaba \'Agoston, L\'aszl\'o Csat\'o</dc:creator>
    </item>
    <item>
      <title>Calibrating Bayesian Inference</title>
      <link>https://arxiv.org/abs/2510.27144</link>
      <description>arXiv:2510.27144v1 Announce Type: new 
Abstract: While Bayesian statistics is popular in psychological research for its intuitive uncertainty quantification and flexible decision-making, its performance in finite samples can be unreliable. In this paper, we demonstrate a key vulnerability: When analysts' chosen prior distribution mismatches the true parameter-generating process, Bayesian inference can be misleading in the long run. Given that this true process is rarely known in practice, we propose a safer alternative: calibrating Bayesian credible regions to achieve frequentist validity. This latter criterion is stronger and guarantees validity of Bayesian inference regardless of the underlying parameter-generating mechanism. To solve the calibration problem in practice, we propose a novel stochastic approximation algorithm. A Monte Carlo experiment is conducted and reported, in which we observe that uncalibrated Bayesian inference can be liberal under certain parameter-generating scenarios, whereas our calibrated solution is always able to maintain validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27144v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Liu, Youjin Sung, Jonathan P. Williams, Jan Hannig</dc:creator>
    </item>
    <item>
      <title>Change-in-velocity detection for multidimensional data</title>
      <link>https://arxiv.org/abs/2510.27150</link>
      <description>arXiv:2510.27150v1 Announce Type: new 
Abstract: In this work, we introduce CPLASS (Continuous Piecewise-Linear Approximation via Stochastic Search), an algorithm for detecting changes in velocity within multidimensional data. The one-dimensional version of this problem is known as the change-in-slope problem (see Fearnhead &amp; Grose (2022), Baranowski et al. (2019)). Unlike traditional changepoint detection methods that focus on changes in mean, detecting changes in velocity requires a specialized approach due to continuity constraints and parameter dependencies, which frustrate popular algorithms like binary segmentation and dynamic programming. To overcome these difficulties, we introduce a specialized penalty function to balance improvements in likelihood due to model complexity, and a Markov Chain Monte Carlo (MCMC)-based approach with tailored proposal mechanisms for efficient parameter exploration. Our method is particularly suited for analyzing intracellular transport data, where the multidimensional trajectories of microscale cargo are driven by teams of molecular motors that undergo complex biophysical transitions. To ensure biophysical realism in the results, we introduce a speed penalty that discourages overfitted of short noisy segments while maintaining consistency in the large-sample limit. Additionally, we introduce a summary statistic called the Cumulative Speed Allocation, which is robust with respect to idiosyncracies of changepoint detection while maintaining the ability to discriminate between biophysically distinct populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27150v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linh Do, Dat Do, Keisha J. Cook, Scott A. McKinley</dc:creator>
    </item>
    <item>
      <title>Flexible model for varying levels of zeros and outliers in count data</title>
      <link>https://arxiv.org/abs/2510.27365</link>
      <description>arXiv:2510.27365v1 Announce Type: new 
Abstract: Count regression models are necessary for examining discrete dependent variables alongside covariates. Nonetheless, when data display outliers, overdispersion, and an abundance of zeros, traditional methods like the zero-inflated negative binomial (ZINB) model sometimes do not yield a satisfactory fit, especially in the tail regions. This research presents a versatile, heavy-tailed discrete model as a resilient substitute for the ZINB model. The suggested framework is built by extending the generalized Pareto distribution and its zero-inflated version to the discrete domain. This formulation efficiently addresses both overdispersion and zero inflation, providing increased flexibility for heavy-tailed count data. Through intensive simulation studies and real-world implementations, the proposed models are thoroughly tested to see how well they work. The results show that our models always do better than classic negative binomial and zero-inflated negative binomial regressions when it comes to goodness-of-fit. This is especially true for datasets with a lot of zeros and outliers. These results highlight the proposed framework's potential as a strong and flexible option for modeling complicated count data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27365v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Touqeer Ahmad, Abid Hussain</dc:creator>
    </item>
    <item>
      <title>Mastering an Accurate and Generalizable Simulation-Based Method to Obtain Bias-corrected Point Estimates and Sampling Variance for Any Effect Sizes</title>
      <link>https://arxiv.org/abs/2510.27519</link>
      <description>arXiv:2510.27519v1 Announce Type: new 
Abstract: Meta-analyses require an effect-size estimate and its corresponding sampling variance from primary studies. In some cases, estimators for the sampling variance of a given effect size statistic may not exist, necessitating the derivation of a new formula for sampling variance. Traditionally, sampling variance formulas are obtained via hand-derived Taylor expansions (the delta method), though this procedure can be challenging for non-statisticians. Building on the idea of single-fit parametric resampling, we introduce SAFE bootstrap: a Single-fit, Accurate, Fast, and Easy simulation recipe that replaces potentially complex algebra with four intuitive steps: fit, draw, transform, and summarise. In a unified framework, the SAFE bootstrap yields bias-corrected point estimates and standard errors for any effect size statistic, regardless of whether the outcome is continuous or discrete. SAFE bootstrapping works by drawing once from a simple sampling model (normal, binomial, etc.), converting each replicate into any effect size of interest and then calculating the bias and sampling variance from simulated data. We demonstrate how to implement the SAFE bootstrap for a simple example first, and then for common effect sizes, such as the standardised mean difference and log odds ratio, as well as for less common effect sizes. With some additional coding, SAFE can also handle zero values and small sample sizes. Our tutorial, with R code supplements, should not only enhance understanding of sampling variance for effect sizes, but also serve as an introduction to the power of simulation-based methods for deriving any effect size with bias correction and its associated sampling variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27519v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shinichi Nakagawa, Ayumi Mizuno, Coralie Williams, Santiago Ortega, Szymon M. Drobniak, Malgorzata Lagisz, Yefeng Yang, Alistair M. Senior, Daniel W. A. Noble, Erick Lundgren</dc:creator>
    </item>
    <item>
      <title>Refining capture-recapture methods to estimate case counts in a finite population setting</title>
      <link>https://arxiv.org/abs/2510.27580</link>
      <description>arXiv:2510.27580v1 Announce Type: new 
Abstract: In this paper, we expand upon and refine a monitoring strategy proposed for surveillance of diseases in finite, closed populations. This monitoring strategy consists of augmenting an arbitrarily non-representative data stream (such as a voluntary flu testing program) with a random sample (referred to as an "anchor stream"). This design allows for the use of traditional capture-recapture (CRC) estimators, as well as recently proposed anchor stream estimators that more efficiently utilize the data. Here, we focus on a particularly common situation in which the first data stream only records positive test results, while the anchor stream documents both positives and negatives. Due to the non-representative nature of the first data stream, along with the fact that inference is being performed on a finite, closed population, there are standard and non-standard finite population effects at play. Here, we propose two methods of incorporating finite population corrections (FPCs) for inference, along with an FPC-adjusted Bayesian credible interval. We compare these approaches with existing methods through simulation and demonstrate that the FPC adjustments can lead to considerable gains in precision. Finally, we provide a real data example by applying these methods to estimating the breast cancer recurrence count among Metro Atlanta-area patients in the Georgia Cancer Registry-based Cancer Recurrence Information and Surveillance Program (CRISP) database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27580v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Doerfler, Wenhao Mao, Lin Ge, Yuzi Zhang, Timothy L. Lash, Kevin C. Ward, Lance A. Waller, Robert H. Lyles</dc:creator>
    </item>
    <item>
      <title>Subspace Ordering for Maximum Response Preservation in Sufficient Dimension Reduction</title>
      <link>https://arxiv.org/abs/2510.27593</link>
      <description>arXiv:2510.27593v1 Announce Type: new 
Abstract: Sufficient dimension reduction (SDR) methods aim to identify a dimension reduction subspace (DRS) that preserves all the information about the conditional distribution of a response given its predictor. Traditional SDR methods determine the DRS by solving a method-specific generalized eigenvalue problem and selecting the eigenvectors corresponding to the largest eigenvalues. In this article, we argue against the long-standing convention of using eigenvalues as the measure of subspace importance and propose alternative ordering criteria that directly assess the predictive relevance of each subspace. For a binary response, we introduce a subspace ordering criterion based on the absolute value of the independent Student's t-statistic. Theoretically, our criterion identifies subspaces that achieve the local minimum Bayes' error rate and yields consistent ordering of directions under mild regularity conditions. Additionally, we employ an F-statistic to provide a framework that unifies categorical and continuous responses under a single subspace criterion. We evaluate our proposed criteria within multiple SDR methods through extensive simulation studies and applications to real data. Our empirical results demonstrate the efficacy of reordering subspaces using our proposed criteria, which generally improves classification accuracy and subspace estimation compared to ordering by eigenvalues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27593v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Derik T. Boonstra, Rakheon Kim, Dean M. Young</dc:creator>
    </item>
    <item>
      <title>Testing Inequalities Linear in Nuisance Parameters</title>
      <link>https://arxiv.org/abs/2510.27633</link>
      <description>arXiv:2510.27633v1 Announce Type: new 
Abstract: This paper proposes a new test for inequalities that are linear in possibly partially identified nuisance parameters. This type of hypothesis arises in a broad set of problems, including subvector inference for linear unconditional moment (in)equality models, specification testing of such models, and inference for parameters bounded by linear programs. The new test uses a two-step test statistic and a chi-squared critical value with data-dependent degrees of freedom that can be calculated by an elementary formula. Its simple structure and tuning-parameter-free implementation make it attractive for practical use. We establish uniform asymptotic validity of the test, demonstrate its finite-sample size and power in simulations, and illustrate its use in an empirical application that analyzes women's labor supply in response to a welfare policy reform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27633v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gregory Fletcher Cox, Xiaoxia Shi, Yuya Shimizu</dc:creator>
    </item>
    <item>
      <title>Emergent Dynamical Spatial Boundaries in Emergency Medical Services: A Navier-Stokes Framework from First Principles</title>
      <link>https://arxiv.org/abs/2510.26810</link>
      <description>arXiv:2510.26810v1 Announce Type: cross 
Abstract: Emergency medical services (EMS) response times are critical determinants of patient survival, yet existing approaches to spatial coverage analysis rely on discrete distance buffers or ad-hoc geographic information system (GIS) isochrones without theoretical foundation. This paper derives continuous spatial boundaries for emergency response from first principles using fluid dynamics (Navier-Stokes equations), demonstrating that response effectiveness decays exponentially with time: $\tau(t) = \tau_0 \exp(-\kappa t)$, where $\tau_0$ is baseline effectiveness and $\kappa$ is the temporal decay rate. Using 10,000 simulated emergency incidents from the National Emergency Medical Services Information System (NEMSIS), I estimate decay parameters and calculate critical boundaries $d^*$ where response effectiveness falls below policy-relevant thresholds. The framework reveals substantial demographic heterogeneity: elderly populations (85+) experience 8.40-minute average response times versus 7.83 minutes for younger adults (18-44), with 33.6\% of poor-access incidents affecting elderly populations despite representing 5.2\% of the sample. Non-parametric kernel regression validation confirms exponential decay is appropriate (mean squared error 8-12 times smaller than parametric), while traditional difference-in-differences analysis validates treatment effect existence (DiD coefficient = -1.35 minutes, $p &lt; 0.001$). The analysis identifies vulnerable populations--elderly, rural, and low-income communities--facing systematically longer response times, informing optimal EMS station placement and resource allocation to reduce health disparities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26810v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Proxy Variable in OECD Database: Application of Parametric Quantile Regression and Median Based Unit Rayleigh Distribution</title>
      <link>https://arxiv.org/abs/2510.26811</link>
      <description>arXiv:2510.26811v1 Announce Type: cross 
Abstract: This paper presents an in-depth exploration of the innovative Median-based unit Rayleigh (MBUR) distribution, previously introduced by the author. This new approach is specifically designed for conducting quantile regression analysis, enabling researchers to gain valuable insights into real-world data applications. The author effectively demonstrates the feasible advantage of the MBUR distribution, highlighting its potential to connect advanced statistical theory with meaningful results in data analysis. The author utilized OECD data in employing the parametric MBUR quantile regression using the response variables which are distributed as MBUR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26811v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iman Mohamed Attia</dc:creator>
    </item>
    <item>
      <title>Minimax-Optimal Two-Sample Test with Sliced Wasserstein</title>
      <link>https://arxiv.org/abs/2510.27498</link>
      <description>arXiv:2510.27498v1 Announce Type: cross 
Abstract: We study the problem of nonparametric two-sample testing using the sliced Wasserstein (SW) distance. While prior theoretical and empirical work indicates that the SW distance offers a promising balance between strong statistical guarantees and computational efficiency, its theoretical foundations for hypothesis testing remain limited. We address this gap by proposing a permutation-based SW test and analyzing its performance. The test inherits finite-sample Type I error control from the permutation principle. Moreover, we establish non-asymptotic power bounds and show that the procedure achieves the minimax separation rate $n^{-1/2}$ over multinomial and bounded-support alternatives, matching the optimal guarantees of kernel-based tests while building on the geometric foundations of Wasserstein distances. Our analysis further quantifies the trade-off between the number of projections and statistical power. Finally, numerical experiments demonstrate that the test combines finite-sample validity with competitive power and scalability, and -- unlike kernel-based tests, which require careful kernel tuning -- it performs consistently well across all scenarios we consider.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27498v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Binh Thuan Tran, Nicolas Schreuder</dc:creator>
    </item>
    <item>
      <title>Bayesian Source Apportionment of Spatio-temporal air pollution data</title>
      <link>https://arxiv.org/abs/2510.27551</link>
      <description>arXiv:2510.27551v1 Announce Type: cross 
Abstract: Understanding the sources that contribute to fine particulate matter (PM$_{2.5}$) is of crucial importance for designing and implementing targeted air pollution mitigation strategies. Determining what factors contribute to a pollutant's concentration goes under the name of source apportionment and it is a problem long studied by atmospheric scientists and statisticians alike. In this paper, we propose a Bayesian model for source apportionment, that advances the literature on source apportionment by allowing estimation of the number of sources and accounting for spatial and temporal dependence in the observed pollutants' concentrations. Taking as example observations of six species of fine particulate matter observed over the course of a year, we present a latent functional factor model that expresses the space-time varying observations of log concentrations of the six pollutant as a linear combination of space-time varying emissions produced by an unknown number of sources each multiplied by the corresponding source's relative contribution to the pollutant. Estimation of the number of sources is achieved by introducing source-specific shrinkage parameters. Application of the model to simulated data showcases its ability to retrieve the true number of sources and to reliably estimate the functional latent factors, whereas application to PM$_{2.5}$ speciation data in California identifies 3 major sources for the six PM$_{2.5}$ species.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27551v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michela Frigeri, Veronica Berrocal, Alessandra Guglielmi</dc:creator>
    </item>
    <item>
      <title>Bayesian model selection and misspecification testing in imaging inverse problems only from noisy and partial measurements</title>
      <link>https://arxiv.org/abs/2510.27663</link>
      <description>arXiv:2510.27663v1 Announce Type: cross 
Abstract: Modern imaging techniques heavily rely on Bayesian statistical models to address difficult image reconstruction and restoration tasks. This paper addresses the objective evaluation of such models in settings where ground truth is unavailable, with a focus on model selection and misspecification diagnosis. Existing unsupervised model evaluation methods are often unsuitable for computational imaging due to their high computational cost and incompatibility with modern image priors defined implicitly via machine learning models. We herein propose a general methodology for unsupervised model selection and misspecification detection in Bayesian imaging sciences, based on a novel combination of Bayesian cross-validation and data fission, a randomized measurement splitting technique. The approach is compatible with any Bayesian imaging sampler, including diffusion and plug-and-play samplers. We demonstrate the methodology through experiments involving various scoring rules and types of model misspecification, where we achieve excellent selection and detection accuracy with a low computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27663v1</guid>
      <category>eess.IV</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tom Sprunck, Marcelo Pereyra, Tobias Liaudat</dc:creator>
    </item>
    <item>
      <title>Hybrid Geometry-Adaptive MCMC for Bayesian Inference in Higher-Order Ising Models</title>
      <link>https://arxiv.org/abs/2404.05671</link>
      <description>arXiv:2404.05671v2 Announce Type: replace 
Abstract: We address the inverse problem for the mean-field Ising model with two- and three-body interactions using a Bayesian framework. Parameter recovery in this setting is notoriously difficult, particularly near phase transitions, at criticality, and under non-identifiability, where conventional estimators and standard MCMC samplers fail. To overcome these challenges, we develop a hybrid algorithm that combines Adaptive Metropolis Hastings with geometry-aware Riemannian manifold Hamiltonian dynamics. This approach yields substantially improved mixing and convergence in the three-dimensional parameter space. Through simulated experiments across representative regimes, we demonstrate that the method achieves accurate density reconstruction and reliable uncertainty quantification even in settings where existing approaches are unstable or inapplicable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05671v2</guid>
      <category>stat.ME</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Godwin Osabutey, Robert Richardson, Garritt L. Page</dc:creator>
    </item>
    <item>
      <title>Structural Nested Mean Models Under Parallel Trends with Interference</title>
      <link>https://arxiv.org/abs/2405.11781</link>
      <description>arXiv:2405.11781v2 Announce Type: replace 
Abstract: Despite the common occurrence of interference in Difference-in-Differences (DiD) applications, standard DiD methods rely on an assumption that interference is absent, and comparatively little work has considered how to accommodate and learn about spillover effects within a DiD framework. Here, we extend the `DiD-SNMMs' of Shahn et al (2022) to accommodate interference in a time-varying DiD setting. Doing so enables estimation of a richer set of effects than previous DiD approaches. For example, DiD-SNMMs do not assume the absence of spillover effects after direct exposures and can model how effects of direct or indirect (i.e. spillover) exposures depend on past and concurrent (direct or indirect) exposure and covariate history. We consider both cluster and network interference structures and illustrate the methodology in simulations and an application to effects of Medicaid expansion on uninsurance rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11781v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zach Shahn, Paul Zivich, Audrey Renson</dc:creator>
    </item>
    <item>
      <title>Gibbs sampling for Bayesian P-splines</title>
      <link>https://arxiv.org/abs/2406.03336</link>
      <description>arXiv:2406.03336v2 Announce Type: replace 
Abstract: P-splines provide a flexible setting for modeling nonlinear model components based on a discretized penalty structure with a relatively simple computational backbone. Under a Bayesian inferential framework based on Markov chain Monte Carlo, estimates of model coefficients in P-splines models are typically obtained by means of Metropolis-type algorithms. These algorithms rely on a proposal distribution that has to be carefully chosen to generate Markov chains that efficiently explore the parameter space. To avoid such a sensitive tuning choice, we extend the Gibbs sampler to Bayesian P-splines models. In this model class, conditional posterior distributions of model coefficients are shown to have attractive mathematical properties. Taking advantage of these properties, we propose to sample the conditional posteriors by alternating between the adaptive rejection sampler when targets are log-concave and the Griddy-Gibbs sampler when targets are characterized by more complex shapes. The proposed Gibbs sampler for Bayesian P-splines (GSBPS) algorithm is shown to be an interesting tuning-free tool for inference in Bayesian P-splines models. Moreover, the GSBPS algorithm can be translated in a compact and user-friendly routine. After describing theoretical results, we illustrate the potential of our methodology in density estimation, Binomial regression, and smoothing of epidemic curves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03336v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oswaldo Gressani, Paul H. C. Eilers</dc:creator>
    </item>
    <item>
      <title>Robust Principal Components by Casewise and Cellwise Weighting</title>
      <link>https://arxiv.org/abs/2408.13596</link>
      <description>arXiv:2408.13596v2 Announce Type: replace 
Abstract: Principal component analysis (PCA) is a fundamental tool for analyzing multivariate data. Here the focus is on dimension reduction to the principal subspace, characterized by its projection matrix. The classical principal subspace can be strongly affected by the presence of outliers. Traditional robust approaches consider casewise outliers, that is, cases generated by an unspecified outlier distribution that differs from that of the clean cases. But there may also be cellwise outliers, which are suspicious entries that can occur anywhere in the data matrix. Another common issue is that some cells may be missing. This paper proposes a new robust PCA method, called cellPCA, that can simultaneously deal with casewise outliers, cellwise outliers, and missing cells. Its single objective function combines two robust loss functions, that together mitigate the effect of casewise and cellwise outliers. The objective function is minimized by an iteratively reweighted least squares (IRLS) algorithm. Residual cellmaps and enhanced outlier maps are proposed for outlier detection. The casewise and cellwise influence functions of the principal subspace are derived, and its asymptotic distribution is obtained. Extensive simulations and two real data examples illustrate the performance of cellPCA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13596v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabio Centofanti, Mia Hubert, Peter J. Rousseeuw</dc:creator>
    </item>
    <item>
      <title>Estimating Heterogeneous Treatment Effects for Spatio-Temporal Causal Inference</title>
      <link>https://arxiv.org/abs/2412.15128</link>
      <description>arXiv:2412.15128v2 Announce Type: replace 
Abstract: Scholars from diverse fields increasingly rely on high-frequency spatio-temporal data. Yet, causal inference with these data remains challenging due to spatial spillover and temporal carryover effects. We develop methods to estimate heterogeneous treatment effects by allowing for arbitrary spatial and temporal causal dependencies. We focus on common settings where the treatment and outcomes are time-varying spatial point patterns and where moderators are either spatial or spatio-temporal variables. We define causal estimands based on stochastic interventions where researchers specify counterfactual distributions of treatment events. We propose the Hajek-type estimator of the conditional average treatment effect (CATE) as a function of spatio-temporal moderator variables, and establish its asymptotic normality as the number of time periods increases. We then introduce a statistical test of no heterogeneous treatment effects. Through simulations, we evaluate the finite-sample performance of the proposed CATE estimator and its inferential properties. Our motivating application examines the heterogeneous effects of US airstrikes on insurgent violence in Iraq. Drawing on declassified spatio-temporal data, we examine how prior aid distributions moderate airstrike effects. Contrary to expectations from counterinsurgency theories, we find that prior aid distribution, along with greater amounts of aid per capita, is associated with increased insurgent attacks following airstrikes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15128v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lingxiao Zhou, Kosuke Imai, Jason Lyall, Georgia Papadogeorgou</dc:creator>
    </item>
    <item>
      <title>Qini Curve Estimation under Clustered Network Interference</title>
      <link>https://arxiv.org/abs/2502.20097</link>
      <description>arXiv:2502.20097v2 Announce Type: replace 
Abstract: Qini curves are a widely used tool for assessing treatment policies under allocation constraints as they visualize the incremental gain of a new treatment policy versus the cost of its implementation. Standard Qini curve estimation assumes no interference between units: that is, that treating one unit does not influence the outcome of any other unit. In many real-life applications such as public policy or marketing, however, the presence of interference is common. Ignoring interference in these scenarios can lead to systematically biased Qini curves that over- or under-estimate a treatment policy's cost-effectiveness. In this paper, we address the problem of Qini curve estimation under clustered network interference, where interfering units form independent clusters. We propose a formal description of the problem setting with an experimental study design under which we can account for clustered network interference. Within this framework, we describe three estimation strategies, each suited to different conditions, and provide guidance for selecting the most appropriate approach by highlighting the inherent bias-variance trade-offs. To complement our theoretical analysis, we introduce a marketplace simulator that replicates clustered network interference in a typical e-commerce environment, allowing us to evaluate and compare the proposed strategies in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20097v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rickard Karlsson, Bram van den Akker, Felipe Moraes, Hugo M. Proen\c{c}a, Jesse H. Krijthe</dc:creator>
    </item>
    <item>
      <title>Inference for Heterogeneous Treatment Effects with Efficient Instruments and Machine Learning</title>
      <link>https://arxiv.org/abs/2503.03530</link>
      <description>arXiv:2503.03530v2 Announce Type: replace 
Abstract: We introduce a new instrumental variable (IV) estimator for heterogeneous treatment effects in the presence of endogeneity. Our estimator is based on double/debiased machine learning (DML) and uses efficient machine learning instruments (MLIV) and kernel smoothing. We prove consistency and asymptotic normality of our estimator and also construct confidence sets that are more robust towards weak IV. Along the way, we also provide an accessible discussion of the corresponding estimator for the homogeneous treatment effect with efficient machine learning instruments. The methods are evaluated on synthetic and real datasets and an implementation is made available in the R package IVDML.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03530v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cyrill Scheidegger, Zijian Guo, Peter B\"uhlmann</dc:creator>
    </item>
    <item>
      <title>Hilbert space methods for approximating multi-output latent variable Gaussian processes</title>
      <link>https://arxiv.org/abs/2505.16919</link>
      <description>arXiv:2505.16919v2 Announce Type: replace 
Abstract: Gaussian processes are a powerful class of non-linear models, but have limited applicability for larger datasets due to their high computational complexity. In such cases, approximate methods are required, for example, the recently developed class of Hilbert space Gaussian processes. They have been shown to drastically reduce computation time while retaining most of the favorable properties of exact Gaussian processes. However, Hilbert space approximations have so far only been developed for uni-dimensional outputs and manifest (known) inputs. To this end, we generalize Hilbert space methods to multi-output and latent input settings. Through extensive simulations, we show that the developed approximate Gaussian processes are indeed not only faster, but also provide similar or even better uncertainty calibration and accuracy of latent variable estimates compared to exact Gaussian processes. While not necessarily faster than alternative Gaussian process approximations, our new models provide better calibration and estimation accuracy, thus striking an excellent balance between trustworthiness and speed. We additionally validate our findings in a real world case study from single cell biology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16919v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Soham Mukherjee, Manfred Claassen, Paul-Christian B\"urkner</dc:creator>
    </item>
    <item>
      <title>Valid F-screening in linear regression</title>
      <link>https://arxiv.org/abs/2505.23113</link>
      <description>arXiv:2505.23113v2 Announce Type: replace 
Abstract: Suppose that a data analyst wishes to report the results of a least squares linear regression only if the overall null hypothesis, $H_0^{1:p}: \beta_1= \beta_2 = \ldots = \beta_p=0$, is rejected. This practice, which we refer to as F-screening (since the overall null hypothesis is typically tested using an $F$-statistic), is in fact common practice across a number of applied fields. Unfortunately, it poses a problem: standard guarantees for the inferential outputs of linear regression, such as Type 1 error control of hypothesis tests and nominal coverage of confidence intervals, hold unconditionally, but fail to hold conditional on rejection of the overall null hypothesis. In this paper, we develop an inferential toolbox for the coefficients in a least squares model that are valid conditional on rejection of the overall null hypothesis. We develop selective p-values that lead to tests that are consistent and control the selective Type 1 error, i.e., the Type 1 error conditional on having rejected the overall null hypothesis. Furthermore, they can be computed without access to the raw data, i.e., using only the standard outputs of a least squares linear regression, and therefore are suitable for use in a retrospective analysis of a published study. We also develop confidence intervals that attain nominal selective coverage, and point estimates that account for having rejected the overall null hypothesis. We derive an expression for the Fisher information about the coefficients resulting from the proposed approach, and compare this to the Fisher information that results from an alternative approach that relies on sample splitting. We investigate the proposed approach in simulation and via re-analysis of two datasets from the biomedical literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23113v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Olivia McGough, Daniela Witten, Daniel Kessler</dc:creator>
    </item>
    <item>
      <title>Centered MA Dirichlet ARMA for Financial Compositions: Theory &amp; Empirical Evidence</title>
      <link>https://arxiv.org/abs/2510.18903</link>
      <description>arXiv:2510.18903v2 Announce Type: replace 
Abstract: Observation-driven Dirichlet models for compositional time series commonly use the additive log-ratio (ALR) link and include a moving-average (MA) term based on ALR residuals. In the standard Bayesian Dirichlet Auto-Regressive Moving-Average (B-DARMA) recursion, this MA regressor has a nonzero conditional mean under the Dirichlet likelihood, which biases the mean path and complicates interpretation of the MA coefficients. We propose a minimal change: replace the raw regressor with a centered innovation equal to the ALR residual minus its conditional expectation, computable in closed form using digamma functions. Centering restores mean-zero innovations for the MA block without altering either the likelihood or the ALR link. We provide closed-form identities for the conditional mean and forecast recursion, show first-order equivalence to a digamma-link DARMA while retaining a simple inverse back to the mean composition, and supply ready-to-use code. In a weekly application to the Federal Reserve H.8 bank-asset composition, the centered specification improves log predictive scores with virtually identical point accuracy and markedly cleaner Hamiltonian Monte Carlo diagnostics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18903v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison Katz</dc:creator>
    </item>
    <item>
      <title>Flexible Modeling of Information Diffusion on Networks with Statistical Guarantees</title>
      <link>https://arxiv.org/abs/2411.09100</link>
      <description>arXiv:2411.09100v2 Announce Type: replace-cross 
Abstract: Modeling information spread through a network is one of the key problems of network analysis, with applications in a wide array of areas such as marketing and public health. Most approaches assume that the spread is governed by some probabilistic diffusion model, often parameterized by the strength of connections between network members (edge weights), highlighting the need for methods that can accurately estimate them. Multiple prior works suggest such estimators for particular diffusion models; however, most of them lack a rigorous statistical analysis that would establish the asymptotic properties of the estimator and allow for uncertainty quantification. In this paper, we develop a likelihood-based approach to estimate edge weights from the observed information diffusion paths under the proposed General Linear Threshold (GLT) model, a broad class of discrete-time information diffusion models that includes both the well-known linear threshold (LT) and independent cascade (IC) models. We first derive necessary and sufficient conditions that make the edge weights identifiable under this model. Then, we derive a finite sample error bound for the estimator and demonstrate that it is asymptotically normal under mild conditions. We conclude by studying the GLT model in the context of the Influence Maximization (IM) problem, that is, the task of selecting a subset of $k$ nodes to start the diffusion, so that the average information spread is maximized. Extensive experiments on synthetic and real-world networks demonstrate that the flexibility of the proposed class of GLT models, coupled with the proposed estimation and inference framework for its parameters, can significantly improve estimation of spread from a given subset of nodes, prediction of node activation, and the quality of the IM problem solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09100v2</guid>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Kagan, Elizaveta Levina, Ji Zhu</dc:creator>
    </item>
    <item>
      <title>Generative Adversarial Networks for High-Dimensional Item Factor Analysis: A Deep Adversarial Learning Algorithm</title>
      <link>https://arxiv.org/abs/2502.10650</link>
      <description>arXiv:2502.10650v3 Announce Type: replace-cross 
Abstract: Advances in deep learning and representation learning have transformed item factor analysis (IFA) in the item response theory (IRT) literature by enabling more efficient and accurate parameter estimation. Variational Autoencoders (VAEs) have been one of the most impactful techniques in modeling high-dimensional latent variables in this context. However, the limited expressiveness of the inference model based on traditional VAEs can still hinder the estimation performance. We introduce Adversarial Variational Bayes (AVB) algorithms as an improvement to VAEs for IFA with improved flexibility and accuracy. By bridging the strengths of VAEs and Generative Adversarial Networks (GANs), AVB incorporates an auxiliary discriminator network to reframe the estimation process as a two-player adversarial game and removes the restrictive assumption of standard normal distributions in the inference model. Theoretically, AVB can achieve similar or higher likelihood compared to VAEs. A further enhanced algorithm, Importance-weighted Adversarial Variational Bayes (IWAVB) is proposed and compared with Importance-weighted Autoencoders (IWAE). In an exploratory analysis of empirical data, IWAVB demonstrated superior expressiveness by achieving a higher likelihood compared to IWAE. In confirmatory analysis with simulated data, IWAVB achieved similar mean-square error results to IWAE while consistently achieving higher likelihoods. When latent variables followed a multimodal distribution, IWAVB outperformed IWAE. With its innovative use of GANs, IWAVB is shown to have the potential to extend IFA to handle large-scale data, facilitating the potential integration of psychometrics and multimodal data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10650v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nanyu Luo, Feng Ji</dc:creator>
    </item>
    <item>
      <title>Robust Spectral Fuzzy Clustering of Multivariate Time Series with Applications to Electroencephalogram</title>
      <link>https://arxiv.org/abs/2506.22861</link>
      <description>arXiv:2506.22861v2 Announce Type: replace-cross 
Abstract: Clustering multivariate time series (MTS) is challenging due to non-stationary cross-dependencies, noise contamination, and gradual or overlapping state boundaries. We introduce a robust fuzzy clustering framework in the spectral domain that leverages Kendall's tau-based canonical coherence to extract frequency-specific monotonic relationships across variables. Our method takes advantage of dominant frequency-based cross-regional connectivity patterns to improve clustering accuracy while remaining resilient to outliers, making the approach broadly applicable to noisy, high-dimensional MTS. Each series is projected onto vectors generated from a spectral matrix specifically tailored to capture the underlying fuzzy partitions. Numerical experiments demonstrate the superiority of our framework over existing methods. As a flagship application, we analyze electroencephalogram recordings, where our approach uncovers frequency- and connectivity-specific markers of latent cognitive states such as alertness and drowsiness, revealing discriminative patterns and ambiguous transitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22861v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziling Ma, Mara Sherlin Talento, Ying Sun, Hernando Ombao</dc:creator>
    </item>
  </channel>
</rss>
