<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Dec 2024 02:42:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>SpikeFI: A Fault Injection Framework for Spiking Neural Networks</title>
      <link>https://arxiv.org/abs/2412.06795</link>
      <description>arXiv:2412.06795v1 Announce Type: new 
Abstract: Neuromorphic computing and spiking neural networks (SNNs) are gaining traction across various artificial intelligence (AI) tasks thanks to their potential for efficient energy usage and faster computation speed. This comparative advantage comes from mimicking the structure, function, and efficiency of the biological brain, which arguably is the most brilliant and green computing machine. As SNNs are eventually deployed on a hardware processor, the reliability of the application in light of hardware-level faults becomes a concern, especially for safety- and mission-critical applications. In this work, we propose SpikeFI, a fault injection framework for SNNs that can be used for automating the reliability analysis and test generation. SpikeFI is built upon the SLAYER PyTorch framework with fault injection experiments accelerated on a single or multiple GPUs. It has a comprehensive integrated neuron and synapse fault model library, in accordance to the literature in the domain, which is extendable by the user if needed. It supports: single and multiple faults; permanent and transient faults; specified, random layer-wise, and random network-wise fault locations; and pre-, during, and post-training fault injection. It also offers several optimization speedups and built-in functions for results visualization. SpikeFI is open-source and available for download via GitHub at https://github.com/SpikeFI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06795v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Theofilos Spyrou, Said Hamdioui, Haralampos-G. Stratigopoulos</dc:creator>
    </item>
    <item>
      <title>Reinforcement learning-enhanced genetic algorithm for wind farm layout optimization</title>
      <link>https://arxiv.org/abs/2412.06803</link>
      <description>arXiv:2412.06803v1 Announce Type: new 
Abstract: A reinforcement learning-enhanced genetic algorithm (RLGA) is proposed for wind farm layout optimization (WFLO) problems. While genetic algorithms (GAs) are among the most effective and accessible methods for WFLO, their performance and convergence are highly sensitive to parameter selections. To address the issue, reinforcement learning (RL) is introduced to dynamically select optimal parameters throughout the GA process. To illustrate the accuracy and efficiency of the proposed RLGA, we evaluate the WFLO problem for four layouts (aligned, staggered, sunflower, and unstructured) under unidirectional uniform wind, comparing the results with those from the GA. RLGA achieves similar results to GA for aligned and staggered layouts and outperforms GA for sunflower and unstructured layouts, demonstrating its efficiency. The sunflower and unstructured layouts' complexity highlights RLGA's robustness and efficiency in tackling complex problems. To further validate its capabilities, we investigate larger wind farms with varying turbine placements ($\Delta x = \Delta y = 5D$ and 2$D$, where $D$ is the wind turbine diameter) under three wind conditions: unidirectional, omnidirectional, and non-uniform, presenting greater challenges. The proposed RLGA is about three times more efficient than GA, especially for complex problems. This improvement stems from RL's ability to adjust parameters, avoiding local optima and accelerating convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06803v1</guid>
      <category>cs.NE</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guodan Dong, Jianhua Qin, Chutian Wu, Chang Xu, Xiaolei Yang</dc:creator>
    </item>
    <item>
      <title>Multi-Objective Communication Optimization for Temporal Continuity in Dynamic Vehicular Networks</title>
      <link>https://arxiv.org/abs/2412.07011</link>
      <description>arXiv:2412.07011v1 Announce Type: new 
Abstract: Vehicular Ad-hoc Networks (VANETs) operate in highly dynamic environments characterized by high mobility, time-varying channel conditions, and frequent network disruptions. Addressing these challenges, this paper presents a novel temporal-aware multi-objective robust optimization framework, which for the first time formally incorporates temporal continuity into the optimization of dynamic multi-hop VANETs. The proposed framework simultaneously optimizes communication delay, throughput, and reliability, ensuring stable and consistent communication paths under rapidly changing conditions. A robust optimization model is formulated to mitigate performance degradation caused by uncertainties in vehicular density and channel fluctuations. To solve the optimization problem, an enhanced Non-dominated Sorting Genetic Algorithm II (NSGA-II) is developed, integrating dynamic encoding, elite inheritance, and adaptive constraint handling to efficiently balance trade-offs among conflicting objectives. Simulation results demonstrate that the proposed framework achieves significant improvements in reliability, delay reduction, and throughput enhancement, while temporal continuity effectively stabilizes communication paths over time. This work provides a pioneering and comprehensive solution for optimizing VANET communication, offering critical insights for robust and efficient strategies in intelligent transportation systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07011v1</guid>
      <category>cs.NE</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weian Guo, Wuzhao Li, Li Li, Lun Zhang, Dongyang Li</dc:creator>
    </item>
    <item>
      <title>MO-IOHinspector: Anytime Benchmarking of Multi-Objective Algorithms using IOHprofiler</title>
      <link>https://arxiv.org/abs/2412.07444</link>
      <description>arXiv:2412.07444v1 Announce Type: new 
Abstract: Benchmarking is one of the key ways in which we can gain insight into the strengths and weaknesses of optimization algorithms. In sampling-based optimization, considering the anytime behavior of an algorithm can provide valuable insights for further developments. In the context of multi-objective optimization, this anytime perspective is not as widely adopted as in the single-objective context. In this paper, we propose a new software tool which uses principles from unbounded archiving as a logging structure. This leads to a clearer separation between experimental design and subsequent analysis decisions. We integrate this approach as a new Python module into the IOHprofiler framework and demonstrate the benefits of this approach by showcasing the ability to change indicators, aggregations, and ranking procedures during the analysis pipeline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07444v1</guid>
      <category>cs.NE</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Diederick Vermetten, Jeroen Rook, Oliver L. Preu{\ss}, Jacob de Nobel, Carola Doerr, Manuel L\'opez-Iba\~nez, Heike Trautmann, Thomas B\"ack</dc:creator>
    </item>
    <item>
      <title>Inverting Visual Representations with Detection Transformers</title>
      <link>https://arxiv.org/abs/2412.06534</link>
      <description>arXiv:2412.06534v1 Announce Type: cross 
Abstract: Understanding the mechanisms underlying deep neural networks in computer vision remains a fundamental challenge. While many prior approaches have focused on visualizing intermediate representations within deep neural networks, particularly convolutional neural networks, these techniques have yet to be thoroughly explored in transformer-based vision models. In this study, we apply the approach of training inverse models to reconstruct input images from intermediate layers within a Detection Transformer, showing that this approach is efficient and feasible for transformer-based vision models. Through qualitative and quantitative evaluations of reconstructed images across model stages, we demonstrate critical properties of Detection Transformers, including contextual shape preservation, inter-layer correlation, and robustness to color perturbations, illustrating how these characteristics emerge within the model's architecture. Our findings contribute to a deeper understanding of transformer-based vision models. The code for reproducing our experiments will be made available at github.com/wiskott-lab/inverse-detection-transformer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06534v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Rathjens, Shirin Reyhanian, David Kappel, Laurenz Wiskott</dc:creator>
    </item>
    <item>
      <title>Covered Forest: Fine-grained generalization analysis of graph neural networks</title>
      <link>https://arxiv.org/abs/2412.07106</link>
      <description>arXiv:2412.07106v1 Announce Type: cross 
Abstract: The expressive power of message-passing graph neural networks (MPNNs) is reasonably well understood, primarily through combinatorial techniques from graph isomorphism testing. However, MPNNs' generalization abilities -- making meaningful predictions beyond the training set -- remain less explored. Current generalization analyses often overlook graph structure, limit the focus to specific aggregation functions, and assume the impractical, hard-to-optimize $0$-$1$ loss function. Here, we extend recent advances in graph similarity theory to assess the influence of graph structure, aggregation, and loss functions on MPNNs' generalization abilities. Our empirical study supports our theoretical insights, improving our understanding of MPNNs' generalization properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07106v1</guid>
      <category>cs.LG</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.NE</category>
      <category>stat.ML</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antonis Vasileiou, Ben Finkelshtein, Floris Geerts, Ron Levie, Christopher Morris</dc:creator>
    </item>
    <item>
      <title>ConfigX: Modular Configuration for Evolutionary Algorithms via Multitask Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2412.07507</link>
      <description>arXiv:2412.07507v1 Announce Type: cross 
Abstract: Recent advances in Meta-learning for Black-Box Optimization (MetaBBO) have shown the potential of using neural networks to dynamically configure evolutionary algorithms (EAs), enhancing their performance and adaptability across various BBO instances. However, they are often tailored to a specific EA, which limits their generalizability and necessitates retraining or redesigns for different EAs and optimization problems. To address this limitation, we introduce ConfigX, a new paradigm of the MetaBBO framework that is capable of learning a universal configuration agent (model) for boosting diverse EAs. To achieve so, our ConfigX first leverages a novel modularization system that enables the flexible combination of various optimization sub-modules to generate diverse EAs during training. Additionally, we propose a Transformer-based neural network to meta-learn a universal configuration policy through multitask reinforcement learning across a designed joint optimization task space. Extensive experiments verify that, our ConfigX, after large-scale pre-training, achieves robust zero-shot generalization to unseen tasks and outperforms state-of-the-art baselines. Moreover, ConfigX exhibits strong lifelong learning capabilities, allowing efficient adaptation to new tasks through fine-tuning. Our proposed ConfigX represents a significant step toward an automatic, all-purpose configuration agent for EAs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07507v1</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongshu Guo, Zeyuan Ma, Jiacheng Chen, Yining Ma, Zhiguang Cao, Xinglin Zhang, Yue-Jiao Gong</dc:creator>
    </item>
    <item>
      <title>Competition on Dynamic Optimization Problems Generated by Generalized Moving Peaks Benchmark (GMPB)</title>
      <link>https://arxiv.org/abs/2106.06174</link>
      <description>arXiv:2106.06174v4 Announce Type: replace 
Abstract: The Generalized Moving Peaks Benchmark (GMPB) is a tool for generating continuous dynamic optimization problem instances with controllable dynamic and morphological characteristics. GMPB has been used in recent Competitions on Dynamic Optimization at prestigious conferences, such as the IEEE Congress on Evolutionary Computation (CEC). This dynamic benchmark generator can create a wide variety of landscapes, ranging from simple unimodal to highly complex multimodal configurations and from symmetric to asymmetric forms. It also supports diverse surface textures, from smooth to highly irregular, and can generate varying levels of variable interaction and conditioning. This document provides an overview of GMPB, emphasizing how its parameters can be adjusted to produce landscapes with customizable characteristics. The MATLAB implementation of GMPB is available on the EDOLAB Platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.06174v4</guid>
      <category>cs.NE</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Danial Yazdani (Business Intelligence Team, WINC), Michalis Mavrovouniotis (ERATOSTHENES Centre of Excellence), Changhe Li (School of Artificial Intelligence, Anhui University of Sciences &amp; Technology), Guoyu Chen (School of Artificial Intelligence, Anhui University of Sciences &amp; Technology), Wenjian Luo (Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies, School of Computer Science and Technology, Harbin Institute of Technology and Peng Cheng Laboratory), Mohammad Nabi Omidvar (School of Computing, University of Leeds, and Leeds University Business School), Juergen Branke (Warwick Business school, University of Warwick), Shengxiang Yang (Center for Computational Intelligence), Xin Yao (Department of Computing and Decision Sciences, Lingnan University, and CERCIA, School of Computer Science, University of Birmingham)</dc:creator>
    </item>
    <item>
      <title>Symbolic Regression with a Learned Concept Library</title>
      <link>https://arxiv.org/abs/2409.09359</link>
      <description>arXiv:2409.09359v3 Announce Type: replace-cross 
Abstract: We present a novel method for symbolic regression (SR), the task of searching for compact programmatic hypotheses that best explain a dataset. The problem is commonly solved using genetic algorithms; we show that we can enhance such methods by inducing a library of abstract textual concepts. Our algorithm, called LaSR, uses zero-shot queries to a large language model (LLM) to discover and evolve concepts occurring in known high-performing hypotheses. We discover new hypotheses using a mix of standard evolutionary steps and LLM-guided steps (obtained through zero-shot LLM queries) conditioned on discovered concepts. Once discovered, hypotheses are used in a new round of concept abstraction and evolution. We validate LaSR on the Feynman equations, a popular SR benchmark, as well as a set of synthetic tasks. On these benchmarks, LaSR substantially outperforms a variety of state-of-the-art SR approaches based on deep learning and evolutionary algorithms. Moreover, we show that LaSR can be used to discover a novel and powerful scaling law for LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09359v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <category>cs.SC</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arya Grayeli, Atharva Sehgal, Omar Costilla-Reyes, Miles Cranmer, Swarat Chaudhuri</dc:creator>
    </item>
  </channel>
</rss>
