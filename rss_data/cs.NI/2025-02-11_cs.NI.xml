<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 12 Feb 2025 02:56:56 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Deep Reinforcement Learning for Backhaul Link Selection for Network Slices in IAB Networks</title>
      <link>https://arxiv.org/abs/2502.05707</link>
      <description>arXiv:2502.05707v1 Announce Type: new 
Abstract: Integrated Access and Backhaul (IAB) has been recently proposed by 3GPP to enable network operators to deploy fifth generation (5G) mobile networks with reduced costs. In this paper, we propose to use IAB to build a dynamic wireless backhaul network capable to provide additional capacity to those Base Stations (BS) experiencing congestion momentarily. As the mobile traffic demand varies across time and space, and the number of slice combinations deployed in a BS can be prohibitively high, we propose to use Deep Reinforcement Learning (DRL) to select, from a set of candidate BSs, the one that can provide backhaul capacity for each of the slices deployed in a congested BS. Our results show that a Double Deep Q-Network (DDQN) agent using a fully connected neural network and the Rectified Linear Unit (ReLU) activation function with only one hidden layer is capable to perform the BS selection task successfully, without any failure during the test phase, after being trained for around 20 episodes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05707v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/GLOBECOM54140.2023.10436900</arxiv:DOI>
      <arxiv:journal_reference>GLOBECOM 2023 - 2023 IEEE Global Communications Conference, Kuala Lumpur, Malaysia, 2023, pp. 6267-6272</arxiv:journal_reference>
      <dc:creator>Ant\'onio J. Morgado, Firooz B. Saghezchi, Pablo Fondo-Ferreiro, Felipe Gil-Casti\~neira, Maria Papaioannou, Kostas Ramantas, Jonathan Rodriguez</dc:creator>
    </item>
    <item>
      <title>GWRF: A Generalizable Wireless Radiance Field for Wireless Signal Propagation Modeling</title>
      <link>https://arxiv.org/abs/2502.05708</link>
      <description>arXiv:2502.05708v1 Announce Type: new 
Abstract: We present Generalizable Wireless Radiance Fields (GWRF), a framework for modeling wireless signal propagation at arbitrary 3D transmitter and receiver positions. Unlike previous methods that adapt vanilla Neural Radiance Fields (NeRF) from the optical to the wireless signal domain, requiring extensive per-scene training, GWRF generalizes effectively across scenes. First, a geometry-aware Transformer encoder-based wireless scene representation module incorporates information from geographically proximate transmitters to learn a generalizable wireless radiance field. Second, a neural-driven ray tracing algorithm operates on this field to automatically compute signal reception at the receiver. Experimental results demonstrate that GWRF outperforms existing methods on single scenes and achieves state-of-the-art performance on unseen scenes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05708v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kang Yang, Yuning Chen, Wan Du</dc:creator>
    </item>
    <item>
      <title>Reconfigurable Intelligent Surface-Enabled Physical-Layer Network Coding for Higher Order M-QAM Signals</title>
      <link>https://arxiv.org/abs/2502.05711</link>
      <description>arXiv:2502.05711v1 Announce Type: new 
Abstract: Physical-Layer Network Coding (PNC) is an effective technique to improve the throughput and latency in wireless networks. However, there are two major challenges for PNC, especially when using higher order modulations: 1) phase synchronization and power control at the paired User Equipments (UEs); and 2) the ambiguity removal of the PNC mapping at the relay node. To address these challenges, in this paper, we apply power control at transmitting UEs and exploit Reconfigurable Intelligent Surfaces (RISs) to synchronize the phase of the transmitted signals and ensure that they arrive at the relay with the same power and phase rotation. Then, we employ modular addition for an unambiguous PNC mapping for M-ary Quadrature Amplitude Modulations (M-QAM). We evaluate the performance of the system in the framework of Orthogonal Frequency Division Multiplexing (OFDM)-PNC for different RIS sizes and modulation orders. Furthermore, we study the sensitivity of PNC systems for Channel Estimation Error (CEE). The results reveal that 1) PNC systems show quite higher sensitivity to CEE compared with RIS-assisted one-way relay channel systems; 2) when the CEE is low, RIS can considerably enhance the Signal-to-Noise Ratio (SNR) of the PNC system, e.g., for a Bit Error Rate (BER) of $10^{-3}$ (without channel coding), increasing the RIS size from one to 256 elements in 28 GHz band leads to 200% improvement in SNR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05711v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/GLOBECOM54140.2023.10437585</arxiv:DOI>
      <arxiv:journal_reference>GLOBECOM 2023 - 2023 IEEE Global Communications Conference, Kuala Lumpur, Malaysia, 2023, pp. 86-91</arxiv:journal_reference>
      <dc:creator>Ehsan Atefat Doost, Firooz B. Saghezchi, Pablo Fondo-Ferreiro, Felipe Gil-Casti\~neira, Maria Papaioannou, John Vardakas, Jinwara Surattanagul, Jonathan Rodriguez</dc:creator>
    </item>
    <item>
      <title>Public DNS Resolvers Meet Content Delivery Networks: A Performance Assessment of the Interplay</title>
      <link>https://arxiv.org/abs/2502.05763</link>
      <description>arXiv:2502.05763v1 Announce Type: new 
Abstract: This paper investigates two key performance aspects of the interplay between public DNS resolution services and content delivery networks -- the latency of DNS queries for resolving CDN-accelerated hostnames and the latency between the end-user and the CDN's edge server obtained by the user through a given resolution service. While these important issues have been considered in the past, significant developments, such as the IPv6 finally getting traction, the adoption of the ECS extension to DNS by major DNS resolution services, and the embracing of anycast by some CDNs warrant a reassessment under these new realities. Among the resolution services we consider, We find Google DNS and OpenDNS to lag behind the Cloudflare resolver and, for some CDNs, Quad9 in terms of DNS latency, and trace the cause to drastically lower cache hit rates. At the same time, we find that Google and OpenDNS have largely closed the gap with ISP resolvers in the quality of CDNs'client-to-edge-server mappings as measured by latency, while the Cloudflare resolver still shows some penalty with Akamai, and Quad9 exhibits a noticeable penalty with three of the four CDNs in the study, keeping up only for Cloudflare CDN that does not use DNS to map clients to servers. Finally, in several locations, we observe IPv6 penalty in the latency of client-to-CDN-edge-server mappings produced by the resolvers. Moreover, this penalty does not rise above typical thresholds employed by the Happy Eyeballs algorithm for falling back to IPv4 communication. Thus, dual-stacked clients in these locations may experience suboptimal performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05763v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas Kernan, Joey Li, Rami Al-Dalky, Michael Rabinovich</dc:creator>
    </item>
    <item>
      <title>Performance Analysis of Multi-Hop Networks at Terahertz Frequencies</title>
      <link>https://arxiv.org/abs/2502.06330</link>
      <description>arXiv:2502.06330v1 Announce Type: new 
Abstract: The emergence of THz (Terahertz) frequency wireless networks holds great potential for advancing various high-demand services, including Industrial Internet of Things (IIoT) applications. These use cases benefit significantly from the ultra-high data rates, low latency, and high spatial resolution offered by THz frequencies. However, a primary well-known challenge of THz networks is their limited coverage range due to high path loss and vulnerability to obstructions. This paper addresses this limitation by proposing two novel multi-hop protocols, Table-Less (TL) and Table-Based (TB), respectively, both avoiding centralized control and/or control plane transmissions. Indeed, both solutions are distributed, simple, and rapidly adaptable to network changes. Simulation results demonstrate the effectiveness of our approaches, as well as revealing interesting trade-offs between TL and TB routing protocols, both in a real IIoT THz network and under static and dynamic conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06330v1</guid>
      <category>cs.NI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sara Cavallero, Andrea Pumilia, Giampaolo Cuozzo, Alessia Tarozzi, Chiara Buratti, Roberto Verdone</dc:creator>
    </item>
    <item>
      <title>A Resilient and Energy-Efficient Smart Metering Infrastructure Utilizing a Self-Organizing UAV Swarm</title>
      <link>https://arxiv.org/abs/2502.06508</link>
      <description>arXiv:2502.06508v1 Announce Type: new 
Abstract: The smart metering infrastructure may become one of the key elements in efficiently managing energy in smart cities. At the same time, traditional measurement record collection is performed by manual methods, which raises cost, safety, and accuracy issues. This paper proposes an innovative SMI architecture based on an unmanned aerial vehicle swarm organizing itself for the autonomous data collection in smart metering infrastructure with scalability and cost-effectiveness while minimizing risks. We design an architecture-based comprehensive system with various phases of operation, communication protocols, and robust failure-handling mechanisms to ensure reliable operations. We further perform extensive simulations in maintenance of precise formations during flight, efficient data collection from smart meters, and adaptation to various failure scenarios. Importantly, we analyze the energy consumption of the proposed system in both drone flight operations and network communication. We now propose a battery sizing strategy and provide an estimate of the operational lifetime of the swarm, underlining the feasibility and practicality of our approach. Our results show that UAV swarms have great potential to revolutionize smart metering and to bring a further brick to greener and more resilient smart cities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06508v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mustafa Siham, Qutaiba I. Ali</dc:creator>
    </item>
    <item>
      <title>A Survey on Video Analytics in Cloud-Edge-Terminal Collaborative Systems</title>
      <link>https://arxiv.org/abs/2502.06581</link>
      <description>arXiv:2502.06581v1 Announce Type: new 
Abstract: The explosive growth of video data has driven the development of distributed video analytics in cloud-edge-terminal collaborative (CETC) systems, enabling efficient video processing, real-time inference, and privacy-preserving analysis. Among multiple advantages, CETC systems can distribute video processing tasks and enable adaptive analytics across cloud, edge, and terminal devices, leading to breakthroughs in video surveillance, autonomous driving, and smart cities. In this survey, we first analyze fundamental architectural components, including hierarchical, distributed, and hybrid frameworks, alongside edge computing platforms and resource management mechanisms. Building upon these foundations, edge-centric approaches emphasize on-device processing, edge-assisted offloading, and edge intelligence, while cloud-centric methods leverage powerful computational capabilities for complex video understanding and model training. Our investigation also covers hybrid video analytics incorporating adaptive task offloading and resource-aware scheduling techniques that optimize performance across the entire system. Beyond conventional approaches, recent advances in large language models and multimodal integration reveal both opportunities and challenges in platform scalability, data protection, and system reliability. Future directions also encompass explainable systems, efficient processing mechanisms, and advanced video analytics, offering valuable insights for researchers and practitioners in this dynamic field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06581v1</guid>
      <category>cs.NI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linxiao Gong, Hao Yang, Gaoyun Fang, Bobo Ju, Juncen Guo, Xiaoguang Zhu, Yan Wang, Xiping Hu, Peng Sun, Azzedine Boukerche</dc:creator>
    </item>
    <item>
      <title>RAILS: Risk-Aware Iterated Local Search for Joint SLA Decomposition and Service Provider Management in Multi-Domain Networks</title>
      <link>https://arxiv.org/abs/2502.06674</link>
      <description>arXiv:2502.06674v1 Announce Type: new 
Abstract: The emergence of the fifth generation (5G) technology has transformed mobile networks into multi-service environments, necessitating efficient network slicing to meet diverse Service Level Agreements (SLAs). SLA decomposition across multiple network domains, each potentially managed by different service providers, poses a significant challenge due to limited visibility into real-time underlying domain conditions. This paper introduces Risk-Aware Iterated Local Search (RAILS), a novel risk model-driven meta-heuristic framework designed to jointly address SLA decomposition and service provider selection in multi-domain networks. By integrating online risk modeling with iterated local search principles, RAILS effectively navigates the complex optimization landscape, utilizing historical feedback from domain controllers. We formulate the joint problem as a Mixed-Integer Nonlinear Programming (MINLP) problem and prove its NP-hardness. Extensive simulations demonstrate that RAILS achieves near-optimal performance, offering an efficient, real-time solution for adaptive SLA management in modern multi-domain networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06674v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cyril Shih-Huan Hsu, Chrysa Papagianni, Paola Grosso</dc:creator>
    </item>
    <item>
      <title>Performance Analysis of Pinching-Antenna Systems</title>
      <link>https://arxiv.org/abs/2502.06701</link>
      <description>arXiv:2502.06701v1 Announce Type: new 
Abstract: The sixth generation of wireless networks envisions intelligent and adaptive environments capable of meeting the demands of emerging applications such as immersive extended reality, advanced healthcare, and the metaverse. However, this vision requires overcoming critical challenges, including the limitations of conventional wireless technologies in mitigating path loss and dynamically adapting to diverse user needs. Among the proposed reconfigurable technologies, pinching antenna systems (PASs) offer a novel way to turn path loss into a programmable parameter by using dielectric waveguides to minimize propagation losses at high frequencies. In this paper, we develop a comprehensive analytical framework that derives closed-form expressions for the outage probability and average rate of PASs while incorporating both free-space path loss and waveguide attenuation under realistic conditions. In addition, we characterize the optimal placement of pinching antennas to maximize performance under waveguide losses. Numerical results show the significant impact of waveguide losses on system performance, especially for longer waveguides, emphasizing the importance of accurate loss modeling. Despite these challenges, PASs consistently outperform conventional systems in terms of reliability and data rate, underscoring their potential to enable high-performance programmable wireless environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06701v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dimitrios Tyrovolas, Sotiris A. Tegos, Panagiotis D. Diamantoulakis, Sotiris Ioannidis, Christos K. Liaskos, George K. Karagiannidis</dc:creator>
    </item>
    <item>
      <title>A Fair Federated Learning Framework for Collaborative Network Traffic Prediction and Resource Allocation</title>
      <link>https://arxiv.org/abs/2502.06743</link>
      <description>arXiv:2502.06743v1 Announce Type: new 
Abstract: In the beyond 5G era, AI/ML empowered realworld digital twins (DTs) will enable diverse network operators to collaboratively optimize their networks, ultimately improving end-user experience. Although centralized AI-based learning techniques have been shown to achieve significant network traffic accuracy, resulting in efficient network operations, they require sharing of sensitive data among operators, leading to privacy and security concerns. Distributed learning, and specifically federated learning (FL), that keeps data isolated at local clients, has emerged as an effective and promising solution for mitigating such concerns. Federated learning poses, however, new challenges in ensuring fairness both in terms of collaborative training contributions from heterogeneous data and in mitigating bias in model predictions with respect to sensitive attributes. To address these challenges, a fair FL framework is proposed for collaborative network traffic prediction and resource allocation. To demonstrate the effectiveness of the proposed approach, noniid and imbalanced federated datasets based on real-word traffic traces are utilized for an elastic optical network. The assumption is that different optical nodes may be managed by different operators. Fairness is evaluated according to the coefficient of variations measure in terms of accuracy across the operators and in terms of quality-of-service across the connections (i.e., reflecting end-user experience). It is shown that fair traffic prediction across the operators result in fairer resource allocations across the connections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06743v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saroj Kumar Panda, Tania Panayiotou, Georgios Ellinas, Sadananda Behera</dc:creator>
    </item>
    <item>
      <title>Detecting APT Malware Command and Control over HTTP(S) Using Contextual Summaries</title>
      <link>https://arxiv.org/abs/2502.05367</link>
      <description>arXiv:2502.05367v1 Announce Type: cross 
Abstract: Advanced Persistent Threats (APTs) are among the most sophisticated threats facing critical organizations worldwide. APTs employ specific tactics, techniques, and procedures (TTPs) which make them difficult to detect in comparison to frequent and aggressive attacks. In fact, current network intrusion detection systems struggle to detect APTs communications, allowing such threats to persist unnoticed on victims' machines for months or even years. In this paper, we present EarlyCrow, an approach to detect APT malware command and control over HTTP(S) using contextual summaries.
  The design of EarlyCrow is informed by a novel threat model focused on TTPs present in traffic generated by tools recently used as part of APT campaigns. The threat model highlights the importance of the context around the malicious connections, and suggests traffic attributes which help APT detection. EarlyCrow defines a novel multipurpose network flow format called PairFlow, which is leveraged to build the contextual summary of a PCAP capture, representing key behavioral, statistical and protocol information relevant to APT TTPs. We evaluate the effectiveness of EarlyCrow on unseen APTs obtaining a headline macro average F1-score of 93.02% with FPR of $0.74%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05367v1</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-22390-7_18</arxiv:DOI>
      <dc:creator>Almuthanna Alageel, Sergio Maffeis, Imperial College London</dc:creator>
    </item>
    <item>
      <title>Rate-Matching Framework for RSMA-Enabled Multibeam LEO Satellite Communications</title>
      <link>https://arxiv.org/abs/2502.05535</link>
      <description>arXiv:2502.05535v1 Announce Type: cross 
Abstract: With the goal of ubiquitous global connectivity, multibeam low Earth orbit (LEO) satellite communication (SATCOM) has attracted significant attention in recent years. The traffic demands of users are heterogeneous within the broad coverage of SATCOM due to different geological conditions and user distributions. Motivated by this, this paper proposes a novel rate-matching (RM) framework based on rate-splitting multiple access (RSMA) that minimizes the difference between the traffic demands and offered rates while simultaneously minimizing transmit power for power-hungry satellite payloads. Moreover, channel phase perturbations arising from channel estimation and feedback errors are considered to capture realistic multibeam LEO SATCOM scenarios. To tackle the non-convexity of the RSMA-based RM problem under phase perturbations, we convert it into a tractable convex form via the successive convex approximation method and present an efficient algorithm to solve the RM problem. Through the extensive numerical analysis across various traffic demand distribution and channel state information accuracy at LEO satellites, we demonstrate that RSMA flexibly allocates the power between common and private streams according to different traffic patterns across beams, thereby efficiently satisfying users non-uniform traffic demands. In particular, the use of common messages plays a vital role in overcoming the limited spatial dimension available at LEO satellites, enabling it to manage inter- and intra-beam interference effectively in the presence of phase perturbation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05535v1</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Jaehyup Seong, Juha Park, Juhwan Lee, Jungwoo Lee, Jung-Bin Kim, Wonjae Shin, H. Vincent Poor</dc:creator>
    </item>
    <item>
      <title>Closing the Responsibility Gap in AI-based Network Management: An Intelligent Audit System Approach</title>
      <link>https://arxiv.org/abs/2502.05608</link>
      <description>arXiv:2502.05608v1 Announce Type: cross 
Abstract: Existing network paradigms have achieved lower downtime as well as a higher Quality of Experience (QoE) through the use of Artificial Intelligence (AI)-based network management tools. These AI management systems, allow for automatic responses to changes in network conditions, lowering operation costs for operators, and improving overall performance. While adopting AI-based management tools enhance the overall network performance, it also introduce challenges such as removing human supervision, privacy violations, algorithmic bias, and model inaccuracies. Furthermore, AI-based agents that fail to address these challenges should be culpable themselves rather than the network as a whole. To address this accountability gap, a framework consisting of a Deep Reinforcement Learning (DRL) model and a Machine Learning (ML) model is proposed to identify and assign numerical values of responsibility to the AI-based management agents involved in any decision-making regarding the network conditions, which eventually affects the end-user. A simulation environment was created for the framework to be trained using simulated network operation parameters. The DRL model had a 96% accuracy during testing for identifying the AI-based management agents, while the ML model using gradient descent learned the network conditions at an 83% accuracy during testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05608v1</guid>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emanuel Figetakis, Ahmed Refaey Hussein</dc:creator>
    </item>
    <item>
      <title>DVFS-Aware DNN Inference on GPUs: Latency Modeling and Performance Analysis</title>
      <link>https://arxiv.org/abs/2502.06295</link>
      <description>arXiv:2502.06295v1 Announce Type: cross 
Abstract: The rapid development of deep neural networks (DNNs) is inherently accompanied by the problem of high computational costs. To tackle this challenge, dynamic voltage frequency scaling (DVFS) is emerging as a promising technology for balancing the latency and energy consumption of DNN inference by adjusting the computing frequency of processors. However, most existing models of DNN inference time are based on the CPU-DVFS technique, and directly applying the CPU-DVFS model to DNN inference on GPUs will lead to significant errors in optimizing latency and energy consumption. In this paper, we propose a DVFS-aware latency model to precisely characterize DNN inference time on GPUs. We first formulate the DNN inference time based on extensive experiment results for different devices and analyze the impact of fitting parameters. Then by dividing DNNs into multiple blocks and obtaining the actual inference time, the proposed model is further verified. Finally, we compare our proposed model with the CPU-DVFS model in two specific cases. Evaluation results demonstrate that local inference optimization with our proposed model achieves a reduction of no less than 66% and 69% in inference time and energy consumption respectively. In addition, cooperative inference with our proposed model can improve the partition policy and reduce the energy consumption compared to the CPU-DVFS model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06295v1</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunchu Han, Zhaojun Nan, Sheng Zhou, Zhisheng Niu</dc:creator>
    </item>
    <item>
      <title>An Efficient Security Model for Industrial Internet of Things (IIoT) System Based on Machine Learning Principles</title>
      <link>https://arxiv.org/abs/2502.06502</link>
      <description>arXiv:2502.06502v1 Announce Type: cross 
Abstract: This paper presents a security paradigm for edge devices to defend against various internal and external threats. The first section of the manuscript proposes employing machine learning models to identify MQTT-based (Message Queue Telemetry Transport) attacks using the Intrusion Detection and Prevention System (IDPS) for edge nodes. Because the Machine Learning (ML) model cannot be trained directly on low-performance platforms (such as edge devices),a new methodology for updating ML models is proposed to provide a tradeoff between the model performance and the computational complexity. The proposed methodology involves training the model on a high-performance computing platform and then installing the trained model as a detection engine on low-performance platforms (such as the edge node of the edge layer) to identify new attacks. Multiple security techniques have been employed in the second half of the manuscript to verify that the exchanged trained model and the exchanged data files are valid and undiscoverable (information authenticity and privacy) and that the source (such as a fog node or edge device) is indeed what it it claimed to be (source authentication and message integrity). Finally, the proposed security paradigm is found to be effective against various internal and external threats and can be applied to a low-cost single-board computer (SBC).</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06502v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sahar L. Qaddoori, Qutaiba I. Ali</dc:creator>
    </item>
    <item>
      <title>Network Intrusion Datasets: A Survey, Limitations, and Recommendations</title>
      <link>https://arxiv.org/abs/2502.06688</link>
      <description>arXiv:2502.06688v1 Announce Type: cross 
Abstract: Data-driven cyberthreat detection has become a crucial defense technique in modern cybersecurity. Network defense, supported by Network Intrusion Detection Systems (NIDSs), has also increasingly adopted data-driven approaches, leading to greater reliance on data. Despite its importance, data scarcity has long been recognized as a major obstacle in NIDS research. In response, the community has published many new datasets recently. However, many of them remain largely unknown and unanalyzed, leaving researchers uncertain about their suitability for specific use cases.
  In this paper, we aim to address this knowledge gap by performing a systematic literature review (SLR) of 89 public datasets for NIDS research. Each dataset is comparatively analyzed across 13 key properties, and its potential applications are outlined. Beyond the review, we also discuss domain-specific challenges and common data limitations to facilitate a critical view on data quality. To aid in data selection, we conduct a dataset popularity analysis in contemporary state-of-the-art NIDS research. Furthermore, the paper presents best practices for dataset selection, generation, and usage. By providing a comprehensive overview of the domain and its data, this work aims to guide future research toward improving data quality and the robustness of NIDS solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06688v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrik Goldschmidt, Daniela Chud\'a</dc:creator>
    </item>
    <item>
      <title>An FPTAS for Shortest-Longest Path Problem</title>
      <link>https://arxiv.org/abs/2404.13488</link>
      <description>arXiv:2404.13488v3 Announce Type: replace 
Abstract: Motivated by multi-domain service function chain (SFC) orchestration, we define the shortest-longest path (SLP) problem, prove its hardness, and design an efficient fully polynomial time approximation scheme (FPTAS) using the dynamic programming (DP) and scaling and rounding (SR) techniques to compute an approximation solution with provable performance guarantee. The SLP problem and its solution algorithm have theoretical significance in multicriteria optimization and also have application potential in QoS routing and multi-domain network resource allocation scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13488v3</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianwei Zhang</dc:creator>
    </item>
    <item>
      <title>Performance Prediction of On-NIC Network Functions with Multi-Resource Contention and Traffic Awareness</title>
      <link>https://arxiv.org/abs/2405.05529</link>
      <description>arXiv:2405.05529v5 Announce Type: replace 
Abstract: Network function (NF) offloading on SmartNICs has been widely used in modern data centers, offering benefits in host resource saving and programmability. Co-running NFs on the same SmartNICs can cause performance interference due to contention of onboard resources. To meet performance SLAs while ensuring efficient resource management, operators need mechanisms to predict NF performance under such contention. However, existing solutions lack SmartNIC-specific knowledge and exhibit limited traffic awareness, leading to poor accuracy for on-NIC NFs.
  This paper proposes Yala, a novel performance predictive system for on-NIC NFs. Yala builds upon the key observation that co-located NFs contend for multiple resources, including onboard accelerators and the memory subsystem. It also facilitates traffic awareness according to the behaviors of individual resources to maintain accuracy as the external traffic attributes vary. Evaluation using BlueField-2 SmartNICs shows that Yala improves the prediction accuracy by 78.8% and reduces SLA violations by 92.2% compared to state-of-the-art approaches, and enables new practical usecases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05529v5</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3669940.3707232</arxiv:DOI>
      <dc:creator>Shaofeng Wu, Qiang Su, Zhixiong Niu, Hong Xu</dc:creator>
    </item>
    <item>
      <title>Dynamic Cluster Analysis to Detect and Track Novelty in Network Telescopes</title>
      <link>https://arxiv.org/abs/2405.10545</link>
      <description>arXiv:2405.10545v2 Announce Type: replace 
Abstract: In the context of cybersecurity, tracking the activities of coordinated hosts over time is a daunting task because both participants and their behaviours evolve at a fast pace. We address this scenario by solving a dynamic novelty discovery problem with the aim of both re-identifying patterns seen in the past and highlighting new patterns. We focus on traffic collected by Network Telescopes, a primary and noisy source for cybersecurity analysis. We propose a 3-stage pipeline: (i) we learn compact representations (embeddings) of hosts through their traffic in a self-supervised fashion; (ii) via clustering, we distinguish groups of hosts performing similar activities; (iii) we track the cluster temporal evolution to highlight novel patterns. We apply our methodology to 20 days of telescope traffic during which we observe more than 8 thousand active hosts. Our results show that we efficiently identify 50-70 well-shaped clusters per day, 60-70% of which we associate with already analysed cases, while we pinpoint 10-20 previously unseen clusters per day. These correspond to activity changes and new incidents, of which we document some. In short, our novelty discovery methodology enormously simplifies the manual analysis the security analysts have to conduct to gain insights to interpret novel coordinated activities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10545v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/EuroSPW61312.2024.00037</arxiv:DOI>
      <arxiv:journal_reference>2024 IEEE European Symposium on Security and Privacy Workshops (EuroS&amp;PW)</arxiv:journal_reference>
      <dc:creator>Kai Huang, Luca Gioacchini, Marco Mellia, Luca Vassio</dc:creator>
    </item>
    <item>
      <title>On the Effect of TSN Forwarding Mechanisms on Best-Effort Traffic</title>
      <link>https://arxiv.org/abs/2408.01330</link>
      <description>arXiv:2408.01330v2 Announce Type: replace 
Abstract: Time-Sensitive Networking (TSN) enables the transmission of multiple traffic types within a single network. While the performance of high-priority traffic has been extensively studied in recent years, the performance of low-priority traffic varies significantly between different TSN forwarding algorithms. This paper provides an overview of existing TSN forwarding algorithms and discusses their impact on best-effort traffic. The effects are quantified through simulations of synthetic and realistic networks. The considered forwarding mechanisms are Strict Priority (SP), Asynchronous Traffic Shaper (ATS), Credit-Based Shaper (CBS), Enhanced Transmission Selection (ETS), and Time-Aware Shaper (TAS).
  The findings indicate that ATS, CBS, and ETS can significantly reduce queuing delays and queue lengths for best-effort traffic when compared to SP and TAS. This effect is enhanced when the reserved bandwidth for high priority queues - using CBS, ATS, or ETS - is reduced to the lowest possible value, within the reserved rate and latency requirements. Specifically, the simulations demonstrate that the choice of forwarding algorithm can improve the performance of low-priority traffic by up to twenty times compared to the least effective algorithm. This study not only provides a comprehensive understanding of the various TSN forwarding algorithms but can also serve as guidance at networks' design time to improve the performance for all types of traffic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01330v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3688268.3688283</arxiv:DOI>
      <dc:creator>Lisa Maile, Dominik Voitlein, Anna Arestova, Abdullah S. Alshra'a, Kai-Steffen J. Hielscher, Reinhard German</dc:creator>
    </item>
    <item>
      <title>Comparative Performance Evaluation of 5G-TSN Applications in Indoor Factory Environments</title>
      <link>https://arxiv.org/abs/2501.12792</link>
      <description>arXiv:2501.12792v2 Announce Type: replace 
Abstract: While Time-Sensitive Networking (TSN) enhances the determinism, real-time capabilities, and reliability of Ethernet, future industrial networks will not only use wired but increasingly wireless communications. Wireless networks enable mobility, have lower costs, and are easier to deploy. However, for many industrial applications, wired connections remain the preferred choice, particularly those requiring strict latency bounds and ultra-reliable data flows, such as for controlling machinery or managing power electronics. The emergence of 5G, with its Ultra-Reliable Low-Latency Communication (URLLC) promises to enable high data rates, ultra-low latency, and minimal jitter, presenting a new opportunity for wireless industrial networks. However, as 5G networks include wired links from the base station towards the core network, a combination of 5G with time-sensitive networking is needed to guarantee stringent QoS requirements. In this paper, we evaluate 5G-TSN performance for different indoor factory applications and environments through simulations. Our findings demonstrate that 5G-TSN can address latency-sensitive scenarios in indoor factory environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12792v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kouros Zanbouri, Md. Noor-A-Rahim, Dirk Pesch</dc:creator>
    </item>
    <item>
      <title>Scalability Analysis of 5G-TSN Applications in Indoor Factory Settings</title>
      <link>https://arxiv.org/abs/2501.13138</link>
      <description>arXiv:2501.13138v2 Announce Type: replace 
Abstract: While technologies such as Time-Sensitive Networking (TSN) improve deterministic behaviour, real-time functionality, and robustness of Ethernet, future industrial networks aim to be increasingly wireless. While wireless networks facilitate mobility, reduce cost, and simplify deployment, they do not always provide stringent latency constraints and highly dependable data transmission as required by many manufacturing systems. The advent of 5G, with its Ultra-Reliable Low-Latency Communication (URLLC) capabilities, offers potential for wireless industrial networks. 5G offers elevated data throughput, very low latency, and negligible jitter. As 5G networks typically include wired connections from the base station to the core network, integration of 5G with time-sensitive networking is essential to provide rigorous QoS standards. This paper assesses the scalability of 5G-TSN for various indoor factory applications and conditions using OMNET++ simulation. Our research shows that 5G-TSN has the potential to provide bounded delay for latency-sensitive applications in scalable indoor factory settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13138v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kouros Zanbouri, Md. Noor-A-Rahim, Dirk Pesch</dc:creator>
    </item>
    <item>
      <title>Performing Load Balancing under Constraints</title>
      <link>https://arxiv.org/abs/2502.01843</link>
      <description>arXiv:2502.01843v2 Announce Type: replace 
Abstract: Join-the-shortest queue (JSQ) and its variants have often been used in solving load balancing problems. The aim of such policies is to minimize the average system occupation, e.g., the customer's system time. In this work we extend the traditional load balancing setting to include constraints that may be imposed, e.g., due to the communication network. We cast the problem into the framework of constrained MDPs, enabling the consideration of both action-dependent constraints, such as, e.g, bandwidth limitation, and state-dependent constraints, such as, e.g., minimum queue utilization. Unlike the state-of-the-art approaches, our load-balancing policies, in particular JSED-$k$ and JSSQ, are both provably safe and yet strive to minimize the system occupancy. Their performance is tested with extensive numerical results under various system settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01843v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Fox, Francesco De Pellegrini, Eitan Altman, Arnob Ghosh, Ness Shroff</dc:creator>
    </item>
    <item>
      <title>Naeural AI OS -- Decentralized ubiquitous computing MLOps execution engine</title>
      <link>https://arxiv.org/abs/2306.08708</link>
      <description>arXiv:2306.08708v5 Announce Type: replace-cross 
Abstract: Over the past few years, ubiquitous, or pervasive computing has gained popularity as the primary approach for a wide range of applications, including enterprise-grade systems, consumer applications, and gaming systems. Ubiquitous computing refers to the integration of computing technologies into everyday objects and environments, creating a network of interconnected devices that can communicate with each other and with humans. By using ubiquitous computing technologies, communities can become more connected and efficient, with members able to communicate and collaborate more easily. This enabled interconnectedness and collaboration can lead to a more successful and sustainable community. The spread of ubiquitous computing, however, has emphasized the importance of automated learning and smart applications in general. Even though there have been significant strides in Artificial Intelligence and Deep Learning, large scale adoption has been hesitant due to mounting pressure on expensive and highly complex cloud numerical-compute infrastructures. Adopting, and even developing, practical machine learning systems can come with prohibitive costs, not only in terms of complex infrastructures but also of solid expertise in Data Science and Machine Learning. In this paper we present an innovative approach for low-code development and deployment of end-to-end AI cooperative application pipelines. We address infrastructure allocation, costs, and secure job distribution in a fully decentralized global cooperative community based on tokenized economics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.08708v5</guid>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cristian Bleotiu, Stefan Saraev, Bogdan Hobeanu, Andrei Ionut Damian</dc:creator>
    </item>
  </channel>
</rss>
