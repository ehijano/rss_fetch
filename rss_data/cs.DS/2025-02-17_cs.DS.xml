<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 17 Feb 2025 05:00:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Optimal $k$-Secretary with Logarithmic Memory</title>
      <link>https://arxiv.org/abs/2502.09834</link>
      <description>arXiv:2502.09834v1 Announce Type: new 
Abstract: We study memory-bounded algorithms for the $k$-secretary problem. The algorithm of Kleinberg (2005) achieves an optimal competitive ratio of $1 - O(1/\sqrt{k})$, yet a straightforward implementation requires $\Omega(k)$ memory.
  Our main result is a $k$-secretary algorithm that matches the optimal competitive ratio using $O(\log k)$ words of memory. We prove this result by establishing a general reduction from $k$-secretary to (random-order) quantile estimation, the problem of finding the $k$-th largest element in a stream. We show that a quantile estimation algorithm with an $O(k^{\alpha})$ expected error (in terms of the rank) gives a $(1 - O(1/k^{1-\alpha}))$-competitive $k$-secretary algorithm with $O(1)$ extra words.
  We then introduce a new quantile estimation algorithm that achieves an $O(\sqrt{k})$ expected error bound using $O(\log k)$ memory. Of independent interest, we give a different algorithm that uses $O(\sqrt{k})$ words and finds the $k$-th largest element exactly with high probability, generalizing a result of Munro and Paterson (1980).</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09834v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingda Qiao, Wei Zhang</dc:creator>
    </item>
    <item>
      <title>Emit As You Go: Enumerating Edges of a Spanning Tree</title>
      <link>https://arxiv.org/abs/2502.10279</link>
      <description>arXiv:2502.10279v1 Announce Type: new 
Abstract: Classically, planning tasks are studied as a two-step process: plan creation and plan execution. In situations where plan creation is slow (for example, due to expensive information access or complex constraints), a natural speed-up tactic is interleaving planning and execution. We implement such an approach with an enumeration algorithm that, after little preprocessing time, outputs parts of a plan one by one with little delay in-between consecutive outputs. As concrete planning task, we consider efficient connectivity in a network formalized as the minimum spanning tree problem in all four standard variants: (un)weighted (un)directed graphs. Solution parts to be emitted one by one for this concrete task are the individual edges that form the final tree.
  We show with algorithmic upper bounds and matching unconditional adversary lower bounds that efficient enumeration is possible for three of four problem variants; specifically for undirected unweighted graphs (delay in the order of the average degree), as well as graphs with either weights (delay in the order of the maximum degree and the average runtime per emitted edge of a total-time algorithm) or directions (delay in the order of the maximum degree). For graphs with both weighted and directed edges, we show that no meaningful enumeration is possible.
  Finally, with experiments on random undirected unweighted graphs, we show that the theoretical advantage of little preprocessing and delay carries over to practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10279v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katrin Casel, Stefan Neubert</dc:creator>
    </item>
    <item>
      <title>Interval Selection with Binary Predictions</title>
      <link>https://arxiv.org/abs/2502.10314</link>
      <description>arXiv:2502.10314v1 Announce Type: new 
Abstract: Following a line of work that takes advantage of vast machine-learned data to enhance online algorithms with (possibly erroneous) information about future inputs, we consider predictions in the context of deterministic algorithms for the problem of selecting a maximum weight independent set of intervals arriving on the real line. We look at two weight functions, unit (constant) weights, and weights proportional to the interval's length. In the classical online model of irrevocable decisions, no algorithm can achieve constant competitiveness (Bachmann et al. [BHS13] for unit, Lipton and Tomkins [LT94] for proportional). In this setting, we show that a simple algorithm that is faithful to the predictions is optimal, and achieves an objective value of at least $OPT -\eta$, with $\eta$ being the total error in the predictions, both for unit, and proportional weights.
  When revocable acceptances (a form of preemption) are allowed, the optimal deterministic algorithm for unit weights is $2k$-competitive [BK23], where $k$ is the number of different interval lengths. We give an algorithm with performance $OPT - \eta$ (and therefore $1$-consistent), that is also $(2k +1)$-robust. For proportional weights, Garay et al. [GGKMY97] give an optimal $(2\phi + 1)$-competitive algorithm, where $\phi$ is the golden ratio. We present an algorithm with parameter $\lambda &gt; 1$ that is $\frac{3\lambda}{\lambda -1}$-consistent, and $\frac{4\lambda^2 +2\lambda}{\lambda -1}$-robust. Although these bounds are not tight, we show that for $\lambda &gt; 3.42$ we achieve consistency better than the optimal online guarantee in [GGKMY97], while maintaining bounded robustness.
  We conclude with some experimental results on real-world data that complement our theoretical findings, and show the benefit of prediction algorithms for online interval selection, even in the presence of high error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10314v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christodoulos Karavasilis</dc:creator>
    </item>
    <item>
      <title>On Incremental Approximate Shortest Paths in Directed Graphs</title>
      <link>https://arxiv.org/abs/2502.10348</link>
      <description>arXiv:2502.10348v1 Announce Type: new 
Abstract: In this paper, we show new data structures maintaining approximate shortest paths in sparse directed graphs with polynomially bounded non-negative edge weights under edge insertions.
  We give more efficient incremental $(1+\epsilon)$-approximate APSP data structures that work against an adaptive adversary: a deterministic one with $\tilde{O}(m^{3/2}n^{3/4})$ total update time and a randomized one with $\tilde{O}(m^{4/3}n^{5/6})$ total update time. For sparse graphs, these both improve polynomially upon the best-known bound against an adaptive adversary. To achieve that, building on the ideas of [Chechik-Zhang, SODA'21] and [Kyng-Meierhans-Probst Gutenberg, SODA'22], we show a near-optimal $(1+\epsilon)$-approximate incremental SSSP data structure for a special case when all edge updates are adjacent to the source, that might be of independent interest.
  We also describe a very simple and near-optimal \emph{offline} incremental $(1+\epsilon)$-approximate SSSP data structure. While online near-linear partially dynamic SSSP data structures have been elusive so far (except for dense instances), our result excludes using certain types of impossibility arguments to rule them out. Additionally, our offline solution leads to near-optimal and deterministic all-pairs bounded-leg shortest paths data structure for sparse graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10348v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam G\'orkiewicz, Adam Karczmarz</dc:creator>
    </item>
    <item>
      <title>Algorithmic contiguity from low-degree conjecture and applications in correlated random graphs</title>
      <link>https://arxiv.org/abs/2502.09832</link>
      <description>arXiv:2502.09832v1 Announce Type: cross 
Abstract: In this paper, assuming a natural strengthening of the low-degree conjecture, we provide evidence of computational hardness for two problems: (1) the (partial) matching recovery problem in the sparse correlated Erd\H{o}s-R\'enyi graphs $\mathcal G(n,q;\rho)$ when the edge-density $q=n^{-1+o(1)}$ and the correlation $\rho&lt;\sqrt{\alpha}$ lies below the Otter's threshold, solving a remaining problem in \cite{DDL23+}; (2) the detection problem between the correlated sparse stochastic block model $\mathcal S(n,\tfrac{\lambda}{n};k,\epsilon;s)$ and a pair of independent stochastic block models $\mathcal S(n,\tfrac{\lambda s}{n};k,\epsilon)$ when $\epsilon^2 \lambda s&lt;1$ lies below the Kesten-Stigum (KS) threshold and $s&lt;\sqrt{\alpha}$ lies below the Otter's threshold, solving a remaining problem in \cite{CDGL24+}.
  One of the main ingredient in our proof is to derive certain forms of \emph{algorithmic contiguity} between two probability measures based on bounds on their low-degree advantage. To be more precise, consider the high-dimensional hypothesis testing problem between two probability measures $\mathbb{P}$ and $\mathbb{Q}$ based on the sample $\mathsf Y$. We show that if the low-degree advantage $\mathsf{Adv}_{\leq D} \big( \frac{\mathrm{d}\mathbb{P}}{\mathrm{d}\mathbb{Q}} \big)=O(1)$, then (assuming the low-degree conjecture) there is no efficient algorithm $\mathcal A$ such that $\mathbb{Q}(\mathcal A(\mathsf Y)=0)=1-o(1)$ and $\mathbb{P}(\mathcal A(\mathsf Y)=1)=\Omega(1)$. This framework provides a useful tool for performing reductions between different inference tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09832v1</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhangsong Li</dc:creator>
    </item>
    <item>
      <title>Minimizing the Number of Tardy Jobs and Maximal Tardiness on a Single Machine is NP-hard</title>
      <link>https://arxiv.org/abs/2404.02784</link>
      <description>arXiv:2404.02784v2 Announce Type: replace 
Abstract: This paper resolves a long-standing open question in bicriteria scheduling regarding the complexity of a single machine scheduling problem which combines the number of tardy jobs and the maximal tardiness criteria. We use the lexicographic approach with the maximal tardiness being the primary criterion. Accordingly, the objective is to find, among all solutions minimizing the maximal tardiness, the one which has the minimum number of tardy jobs. The complexity of this problem has been open for over thirty years, and has been known since then to be one of the most challenging open questions in multicriteria scheduling. We resolve this question by proving that the problem is strongly NP-hard. We also prove that the problem is at least weakly NP-hard when we switch roles between the two criteria (i.e., when the number of tardy jobs is the primary criterion). Finally, we provide hardness results for two other approaches (constraint and a priori approaches) to deal with these two criteria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02784v2</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Klaus Heeger, Danny Hermelin, Michael L. Pinedo, Dvir Shabtay</dc:creator>
    </item>
    <item>
      <title>Overcoming Non-Submodularity: Towards Constant Approximation for Network Immunization</title>
      <link>https://arxiv.org/abs/2410.19205</link>
      <description>arXiv:2410.19205v4 Announce Type: replace 
Abstract: Given a network with an ongoing epidemic, the network immunization problem seeks to identify a fixed number of nodes to immunize in order to maximize the number of infections prevented. A fundamental computational challenge in network immunization is that the objective function is generally neither submodular nor supermodular. Consequently, no efficient algorithm is known to consistently achieve a constant-factor approximation. Traditionally, this problem is partially addressed using proxy objectives that offer better approximation properties, but these indirect optimizations often introduce losses in effectiveness due to gaps between the proxy and natural objectives.
  In this paper, we overcome these fundamental barriers by leveraging the underlying stochastic structure of the diffusion process. Similar to the traditional influence objective, the immunization objective is an expectation expressed as a sum over deterministic instances. However, unlike the former, some of these terms are not submodular. Our key step is to prove that this sum has a bounded deviation from submodularity, enabling the classic greedy algorithm to achieve a constant-factor approximation for any sparse cascading network. We demonstrate that this approximation holds across various immunization settings and spread models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19205v4</guid>
      <category>cs.DS</category>
      <category>cs.SI</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ajitesh Srivastava, Shang-Hua Teng</dc:creator>
    </item>
    <item>
      <title>Tight Bounds for Noisy Computation of High-Influence Functions, Connectivity, and Threshold</title>
      <link>https://arxiv.org/abs/2502.04632</link>
      <description>arXiv:2502.04632v2 Announce Type: replace 
Abstract: In the noisy query model, the (binary) return value of every query (possibly repeated) is independently flipped with some fixed probability $p \in (0, 1/2)$. In this paper, we obtain tight bounds on the noisy query complexity of several fundamental problems.
  Our first contribution is to show that any Boolean function with total influence $\Omega(n)$ has noisy query complexity $\Theta(n\log n)$. Previous works often focus on specific problems, and it is of great interest to have a characterization of noisy query complexity for general functions. Our result is the first noisy query complexity lower bound of this generality, beyond what was known for random Boolean functions [Reischuk and Schmeltz, FOCS 1991].
  Our second contribution is to prove that Graph Connectivity has noisy query complexity $\Theta(n^2 \log n)$. In this problem, the goal is to determine whether an undirected graph is connected using noisy edge queries. While the upper bound can be achieved by a simple algorithm, no non-trivial lower bounds were known prior to this work.
  Last but not least, we determine the exact number of noisy queries (up to lower order terms) needed to solve the $k$-Threshold problem and the Counting problem. The $k$-Threshold problem asks to decide whether there are at least $k$ ones among $n$ bits, given noisy query access to the bits. We prove that $(1\pm o(1)) \frac{n\log (\min\{k,n-k+1\}/\delta)}{(1-2p)\log \frac{1-p}p}$ queries are both sufficient and necessary to achieve error probability $\delta = o(1)$. Previously, such a result was only known when $\min\{k,n-k+1\}=o(n)$ [Wang, Ghaddar, Zhu and Wang, arXiv 2024]. We also show a similar $(1\pm o(1)) \frac{n\log (\min\{k+1,n-k+1\}/\delta)}{(1-2p)\log \frac{1-p}p}$ bound for the Counting problem, where one needs to count the number of ones among $n$ bits given noisy query access and $k$ denotes the answer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04632v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuzhou Gu, Xin Li, Yinzhan Xu</dc:creator>
    </item>
    <item>
      <title>Sink-free orientations: a local sampler with applications</title>
      <link>https://arxiv.org/abs/2502.05877</link>
      <description>arXiv:2502.05877v2 Announce Type: replace 
Abstract: For sink-free orientations in graphs of minimum degree at least $3$, we show that there is a deterministic approximate counting algorithm that runs in time $O((n^{73}/\varepsilon^{72})\log(n/\varepsilon))$, a near-linear time sampling algorithm, and a randomised approximate counting algorithm that runs in time $O((n/\varepsilon)^2\log(n/\varepsilon))$, where $n$ denotes the number of vertices of the input graph and $0&lt;\varepsilon&lt;1$ is the desired accuracy. All three algorithms are based on a local implementation of the sink popping method (Cohn, Pemantle, and Propp, 2002) under the partial rejection sampling framework (Guo, Jerrum, and Liu, 2019).</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05877v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.PR</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Konrad Anand, Graham Freifeld, Heng Guo, Chunyang Wang, Jiaheng Wang</dc:creator>
    </item>
    <item>
      <title>Zone Theorem for Arrangements in three dimensions</title>
      <link>https://arxiv.org/abs/2006.01428</link>
      <description>arXiv:2006.01428v2 Announce Type: replace-cross 
Abstract: In this note, a simple description of zone theorem in three dimensions is given.Arrangements in three dimensions are useful for constructing higher-order Voronoi diagrams in plane. An elementary and very intuitive treatment of this result is also given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2006.01428v2</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ipl.2021.106161</arxiv:DOI>
      <arxiv:journal_reference>Information Processing Letters Volume 172, December 2021, 106161</arxiv:journal_reference>
      <dc:creator>Sanjeev Saxena</dc:creator>
    </item>
  </channel>
</rss>
