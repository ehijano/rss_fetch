<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 28 Aug 2024 01:42:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Cross-sectional Dependence in Idiosyncratic Volatility</title>
      <link>https://arxiv.org/abs/2408.13437</link>
      <description>arXiv:2408.13437v1 Announce Type: new 
Abstract: This paper introduces an econometric framework for analyzing cross-sectional dependence in the idiosyncratic volatilities of assets using high frequency data. We first consider the estimation of standard measures of dependence in the idiosyncratic volatilities such as covariances and correlations. Naive estimators of these measures are biased due to the use of the error-laden estimates of idiosyncratic volatilities. We provide bias-corrected estimators and the relevant asymptotic theory. Next, we introduce an idiosyncratic volatility factor model, in which we decompose the variation in idiosyncratic volatilities into two parts: the variation related to the systematic factors such as the market volatility, and the residual variation. Again, naive estimators of the decomposition are biased, and we provide bias-corrected estimators. We also provide the asymptotic theory that allows us to test whether the residual (non-systematic) components of the idiosyncratic volatilities exhibit cross-sectional dependence. We apply our methodology to the S&amp;P 100 index constituents, and document strong cross-sectional dependence in their idiosyncratic volatilities. We consider two different sets of idiosyncratic volatility factors, and find that neither can fully account for the cross-sectional dependence in idiosyncratic volatilities. For each model, we map out the network of dependencies in residual (non-systematic) idiosyncratic volatilities across all stocks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13437v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilze Kalnina, Kokouvi Tewou</dc:creator>
    </item>
    <item>
      <title>Inference on Consensus Ranking of Distributions</title>
      <link>https://arxiv.org/abs/2408.13949</link>
      <description>arXiv:2408.13949v1 Announce Type: new 
Abstract: Instead of testing for unanimous agreement, I propose learning how broad of a consensus favors one distribution over another (of earnings, productivity, asset returns, test scores, etc.). Specifically, given a sample from each of two distributions, I propose statistical inference methods to learn about the set of utility functions for which the first distribution has higher expected utility than the second distribution. With high probability, an "inner" confidence set is contained within this true set, while an "outer" confidence set contains the true set. Such confidence sets can be formed by inverting a proposed multiple testing procedure that controls the familywise error rate. Theoretical justification comes from empirical process results, given that very large classes of utility functions are generally Donsker (subject to finite moments). The theory additionally justifies a uniform (over utility functions) confidence band of expected utility differences, as well as tests with a utility-based "restricted stochastic dominance" as either the null or alternative hypothesis. Simulated and empirical examples illustrate the methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13949v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1080/07350015.2023.2252040</arxiv:DOI>
      <arxiv:journal_reference>Journal of Business &amp; Economic Statistics 42 (2024) 839-850</arxiv:journal_reference>
      <dc:creator>David M. Kaplan</dc:creator>
    </item>
    <item>
      <title>Endogenous Treatment Models with Social Interactions: An Application to the Impact of Exercise on Self-Esteem</title>
      <link>https://arxiv.org/abs/2408.13971</link>
      <description>arXiv:2408.13971v1 Announce Type: new 
Abstract: We address the estimation of endogenous treatment models with social interactions in both the treatment and outcome equations. We model the interactions between individuals in an internally consistent manner via a game theoretic approach based on discrete Bayesian games. This introduces a substantial computational burden in estimation which we address through a sequential version of the nested fixed point algorithm. We also provide some relevant treatment effects, and procedures for their estimation, which capture the impact on both the individual and the total sample. Our empirical application examines the impact of an individual's exercise frequency on her level of self-esteem. We find that an individual's exercise frequency is influenced by her expectation of her friends'. We also find that an individual's level of self-esteem is affected by her level of exercise and, at relatively lower levels of self-esteem, by the expectation of her friends' self-esteem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13971v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongjian Lin, Francis Vella</dc:creator>
    </item>
    <item>
      <title>Modeling the Dynamics of Growth in Master-Planned Communities</title>
      <link>https://arxiv.org/abs/2408.14214</link>
      <description>arXiv:2408.14214v1 Announce Type: new 
Abstract: This paper describes how a time-varying Markov model was used to forecast housing development at a master-planned community during a transition from high to low growth. Our approach draws on detailed historical data to model the dynamics of the market participants, producing results that are entirely data-driven and free of bias. While traditional time series forecasting methods often struggle to account for nonlinear regime changes in growth, our approach successfully captures the onset of buildout as well as external economic shocks, such as the 1990 and 2008-2011 recessions and the 2021 post-pandemic boom.
  This research serves as a valuable tool for urban planners, homeowner associations, and property stakeholders aiming to navigate the complexities of growth at master-planned communities during periods of both system stability and instability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14214v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher K. Allsup, Irene S. Gabashvili</dc:creator>
    </item>
    <item>
      <title>Coarse Personalization</title>
      <link>https://arxiv.org/abs/2204.05793</link>
      <description>arXiv:2204.05793v3 Announce Type: replace 
Abstract: Advances in estimating heterogeneous treatment effects enable firms to personalize marketing mix elements and target individuals at an unmatched level of granularity, but feasibility constraints limit such personalization. In practice, firms choose which unique treatments to offer and which individuals to offer these treatments with the goal of maximizing profits: we call this the coarse personalization problem. We propose a two-step solution that makes segmentation and targeting decisions in concert. First, the firm personalizes by estimating conditional average treatment effects. Second, the firm discretizes by utilizing treatment effects to choose which unique treatments to offer and who to assign to these treatments. We show that a combination of available machine learning tools for estimating heterogeneous treatment effects and a novel application of optimal transport methods provides a viable and efficient solution. With data from a large-scale field experiment for promotions management, we find that our methodology outperforms extant approaches that segment on consumer characteristics or preferences and those that only search over a prespecified grid. Using our procedure, the firm recoups over 99.5% of its expected incremental profits under fully granular personalization while offering only five unique treatments. We conclude by discussing how coarse personalization arises in other domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.05793v3</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Walter W. Zhang, Sanjog Misra</dc:creator>
    </item>
    <item>
      <title>Linear multidimensional regression with interactive fixed-effects</title>
      <link>https://arxiv.org/abs/2209.11691</link>
      <description>arXiv:2209.11691v4 Announce Type: replace 
Abstract: This paper studies a linear and additively separable model for multidimensional panel data of three or more dimensions with unobserved interactive fixed effects. Two approaches are considered to account for these unobserved interactive fixed-effects when estimating coefficients on the observed covariates. First, the model is embedded within the standard two dimensional panel framework and restrictions are formed under which the factor structure methods in Bai (2009) lead to consistent estimation of model parameters, but at slow rates of convergence. The second approach develops a kernel weighted fixed-effects method that is more robust to the multidimensional nature of the problem and can achieve the parametric rate of consistency under certain conditions. Theoretical results and simulations show some benefits to standard two-dimensional panel methods when the structure of the interactive fixed-effect term is known, but also highlight how the kernel weighted method performs well without knowledge of this structure. The methods are implemented to estimate the demand elasticity for beer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.11691v4</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hugo Freeman</dc:creator>
    </item>
    <item>
      <title>On the Efficiency of Finely Stratified Experiments</title>
      <link>https://arxiv.org/abs/2307.15181</link>
      <description>arXiv:2307.15181v4 Announce Type: replace 
Abstract: This paper studies the use of finely stratified designs for the efficient estimation of a large class of treatment effect parameters that arise in the analysis of experiments. By a "finely stratified" design, we mean experiments in which units are divided into groups of a fixed size and a proportion within each group is assigned to a binary treatment uniformly at random. The class of parameters considered are those that can be expressed as the solution to a set of moment conditions constructed using a known function of the observed data. They include, among other things, average treatment effects, quantile treatment effects, and local average treatment effects as well as the counterparts to these quantities in experiments in which the unit is itself a cluster. In this setting, we establish two results. First, we show that under a finely stratified design, the na\"ive method of moments estimator achieves the same asymptotic variance as what could typically be attained under alternative treatment assignment schemes only through ex post covariate adjustment. Second, we argue that in fact the na\"ive method of moments estimator under a finely stratified design is asymptotically efficient by deriving a lower bound on the asymptotic variance of "regular" estimators of the parameter of interest in the form of a convolution theorem. This result accommodates a large class of possible treatment assignment schemes that are used routinely throughout the sciences, such as stratified block randomization and matched pairs. In this sense, "finely stratified" experiments are attractive because they lead to efficient estimators of treatment effect parameters "by design" rather than through ex post covariate adjustment and thereby remain "hands above the table."</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.15181v4</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuehao Bai, Jizhou Liu, Azeem M. Shaikh, Max Tabord-Meehan</dc:creator>
    </item>
    <item>
      <title>Estimating Heterogeneous Treatment Effects with Item-Level Outcome Data: Insights from Item Response Theory</title>
      <link>https://arxiv.org/abs/2405.00161</link>
      <description>arXiv:2405.00161v3 Announce Type: replace 
Abstract: Analyses of heterogeneous treatment effects (HTE) are common in applied causal inference research. However, when outcomes are latent variables assessed via psychometric instruments such as educational tests, standard methods ignore the potential HTE that may exist among the individual items of the outcome measure. Failing to account for ``item-level'' HTE (IL-HTE) can lead to both estimated standard errors that are too small and identification challenges in the estimation of treatment-by-covariate interaction effects. We demonstrate how Item Response Theory (IRT) models that estimate a treatment effect for each assessment item can both address these challenges and provide new insights into HTE generally. This study articulates the theoretical rationale for the IL-HTE model and demonstrates its practical value using 73 data sets from 46 randomized controlled trials containing 5.8 million item responses in economics, education, and health research. Our results show that the IL-HTE model reveals item-level variation masked by single-number scores, provides more meaningful standard errors in many settings, allows for estimates of the generalizability of causal effects to untested items, resolves identification problems in the estimation of interaction effects, and provides estimates of standardized treatment effect sizes corrected for attenuation due to measurement error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00161v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua B. Gilbert, Zachary Himmelsbach, James Soland, Mridul Joshi, Benjamin W. Domingue</dc:creator>
    </item>
    <item>
      <title>Accounting for Nonresponse in Election Polls: Total Margin of Error</title>
      <link>https://arxiv.org/abs/2407.19339</link>
      <description>arXiv:2407.19339v2 Announce Type: replace 
Abstract: The potential impact of nonresponse on election polls is well known and frequently acknowledged. Yet measurement and reporting of polling error has focused solely on sampling error, represented by the margin of error of a poll. Survey statisticians have long recommended measurement of the total survey error of a sample estimate by its mean square error (MSE), which jointly measures sampling and non-sampling errors. Extending the conventional language of polling, we think it reasonable to use the square root of maximum MSE to measure the total margin of error (TME). This paper demonstrates how to measure the potential impact of nonresponse using the concept of TME, which we suggest should be a standard feature in the reporting of election poll results. We first show how to jointly measure statistical imprecision and response bias when a pollster lacks any knowledge of the candidate preferences of non-responders. We then extend the analysis to settings where the pollster has partial knowledge that bounds the preferences of non-responders. In each setting, we derive the poll estimate that minimizes TME, a midpoint estimate, and compare it to a conventional poll estimate. We identify conditions under which the two estimates coincide, noting that the TME exceeds the margin of sampling error whenever the pollster has less than complete knowledge of the nature of nonresponse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19339v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeff Dominitz, Charles F. Manski</dc:creator>
    </item>
    <item>
      <title>Endogeneity Corrections in Binary Outcome Models with Nonlinear Transformations: Identification and Inference</title>
      <link>https://arxiv.org/abs/2408.06977</link>
      <description>arXiv:2408.06977v2 Announce Type: replace 
Abstract: For binary outcome models, an endogeneity correction based on nonlinear rank-based transformations is proposed. Identification without external instruments is achieved under one of two assumptions: either the endogenous regressor is a nonlinear function of one component of the error term, conditional on the exogenous regressors, or the dependence between the endogenous and exogenous regressors is nonlinear. Under these conditions, we prove consistency and asymptotic normality. Monte Carlo simulations and an application on German insolvency data illustrate the usefulness of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06977v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alexander Mayer, Dominik Wied</dc:creator>
    </item>
    <item>
      <title>Extract Mechanisms from Heterogeneous Effects: Identification Strategy for Mediation Analysis</title>
      <link>https://arxiv.org/abs/2403.04131</link>
      <description>arXiv:2403.04131v3 Announce Type: replace-cross 
Abstract: Understanding causal mechanisms is crucial for explaining and generalizing empirical phenomena. Causal mediation analysis offers statistical techniques to quantify mediation effects. However, current methods often require multiple ignorability assumptions or sophisticated research designs. In this paper, we introduce a novel identification strategy that enables the simultaneous identification and estimation of treatment and mediation effects. This strategy is based on a new decomposition of total treatment effects and explores heterogeneous treatment effects. Monte Carlo simulations demonstrate that the method is more accurate and precise across various scenarios. To illustrate the efficiency and efficacy of our method, we apply it to estimate the causal mediation effects in two studies with distinct data structures, focusing on common pool resource governance and voting information. Additionally, we have developed statistical software to facilitate the implementation of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04131v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawei Fu</dc:creator>
    </item>
    <item>
      <title>From day-ahead to mid and long-term horizons with econometric electricity price forecasting models</title>
      <link>https://arxiv.org/abs/2406.00326</link>
      <description>arXiv:2406.00326v2 Announce Type: replace-cross 
Abstract: The recent energy crisis starting in 2021 led to record-high gas, coal, carbon and power prices, with electricity reaching up to 40 times the pre-crisis average. This had dramatic consequences for operational and risk management prompting the need for robust econometric models for mid to long-term electricity price forecasting. After a comprehensive literature analysis, we identify key challenges and address them with novel approaches: 1) Fundamental information is incorporated by constraining coefficients with bounds derived from fundamental models offering interpretability; 2) Short-term regressors such as load and renewables can be used in long-term forecasts by incorporating their seasonal expectations to stabilize the model; 3) Unit root behavior of power prices, induced by fuel prices, can be managed by estimating same-day relationships and projecting them forward. We develop interpretable models for a range of forecasting horizons from one day to one year ahead, providing guidelines on robust modeling frameworks and key explanatory variables for each horizon. Our study, focused on Europe's largest energy market, Germany, analyzes hourly electricity prices using regularized regression methods and generalized additive models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00326v2</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Ghelasi, Florian Ziel</dc:creator>
    </item>
    <item>
      <title>Factorial Difference-in-Differences</title>
      <link>https://arxiv.org/abs/2407.11937</link>
      <description>arXiv:2407.11937v2 Announce Type: replace-cross 
Abstract: In many social science applications, researchers use the difference-in-differences (DID) estimator to establish causal relationships, exploiting cross-sectional variation in a baseline factor and temporal variation in exposure to an event that presumably may affect all units. This approach, which we term factorial DID (FDID), differs from canonical DID in that it lacks a clean control group unexposed to the event after the event occurs. In this paper, we clarify FDID as a research design in terms of its data structure, feasible estimands, and identifying assumptions that allow the DID estimator to recover these estimands. We frame FDID as a factorial design with two factors: the baseline factor, denoted by $G$, and the exposure level to the event, denoted by $Z$, and define the effect modification and causal interaction as the associative and causal effects of $G$ on the effect of $Z$, respectively. We show that under the canonical no anticipation and parallel trends assumptions, the DID estimator identifies only the effect modification of $G$ in FDID, and propose an additional factorial parallel trends assumption to identify the causal interaction. Moreover, we show that the canonical DID research design can be reframed as a special case of the FDID research design with an additional exclusion restriction assumption, thereby reconciling the two approaches. We extend this framework to allow conditionally valid parallel trends assumptions and multiple time periods, and clarify assumptions required to justify regression analysis under FDID. We illustrate these findings with empirical examples from economics and political science, and provide recommendations for improving practice and interpretation under FDID.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11937v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yiqing Xu, Anqi Zhao, Peng Ding</dc:creator>
    </item>
    <item>
      <title>Experimental Design For Causal Inference Through An Optimization Lens</title>
      <link>https://arxiv.org/abs/2408.09607</link>
      <description>arXiv:2408.09607v2 Announce Type: replace-cross 
Abstract: The study of experimental design offers tremendous benefits for answering causal questions across a wide range of applications, including agricultural experiments, clinical trials, industrial experiments, social experiments, and digital experiments. Although valuable in such applications, the costs of experiments often drive experimenters to seek more efficient designs. Recently, experimenters have started to examine such efficiency questions from an optimization perspective, as experimental design problems are fundamentally decision-making problems. This perspective offers a lot of flexibility in leveraging various existing optimization tools to study experimental design problems. This manuscript thus aims to examine the foundations of experimental design problems in the context of causal inference as viewed through an optimization lens.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09607v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinglong Zhao</dc:creator>
    </item>
  </channel>
</rss>
