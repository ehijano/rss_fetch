<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Jan 2025 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Efficient estimation of average treatment effects with unmeasured confounding and proxies</title>
      <link>https://arxiv.org/abs/2501.02214</link>
      <description>arXiv:2501.02214v1 Announce Type: new 
Abstract: One approach to estimating the average treatment effect in binary treatment with unmeasured confounding is the proximal causal inference, which assumes the availability of outcome and treatment confounding proxies. The key identifying result relies on the existence of a so-called bridge function. A parametric specification of the bridge function is usually postulated and estimated using standard techniques. The estimated bridge function is then plugged in to estimate the average treatment effect. This approach may have two efficiency losses. First, the bridge function may not be efficiently estimated since it solves an integral equation. Second, the sequential procedure may fail to account for the correlation between the two steps. This paper proposes to approximate the integral equation with increasing moment restrictions and jointly estimate the bridge function and the average treatment effect. Under sufficient conditions, we show that the proposed estimator is efficient. To assist implementation, we propose a data-driven procedure for selecting the tuning parameter (i.e., number of moment restrictions). Simulation studies reveal that the proposed method performs well in finite samples, and application to the right heart catheterization dataset from the SUPPORT study demonstrates its practical value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02214v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunrong Ai, Jiawei Shan</dc:creator>
    </item>
    <item>
      <title>Sparsity learning via structured functional factor augmentation</title>
      <link>https://arxiv.org/abs/2501.02244</link>
      <description>arXiv:2501.02244v1 Announce Type: new 
Abstract: As one of the most powerful tools for examining the association between functional covariates and a response, the functional regression model has been widely adopted in various interdisciplinary studies. Usually, a limited number of functional covariates are assumed in a functional linear regression model. Nevertheless, correlations may exist between functional covariates in high-dimensional functional linear regression models, which brings significant statistical challenges to statistical inference and functional variable selection. In this article, a novel functional factor augmentation structure (fFAS) is proposed for multivariate functional series, and a multivariate functional factor augmentation selection model (fFASM) is further proposed to deal with issues arising from variable selection of correlated functional covariates. Theoretical justifications for the proposed fFAS are provided, and statistical inference results of the proposed fFASM are established. Numerical investigations support the superb performance of the novel fFASM model in terms of estimation accuracy and selection consistency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02244v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanteng Ma, Ziliang Shen, Xingdong Feng, Xin Liu</dc:creator>
    </item>
    <item>
      <title>A generalized distance covariance framework for genome-wide association studies</title>
      <link>https://arxiv.org/abs/2501.02403</link>
      <description>arXiv:2501.02403v1 Announce Type: new 
Abstract: When testing for the association of a single SNP with a phenotypic response, one usually considers an additive genetic model, assuming that the mean of of the response for the heterozygous state is the average of the means for the two homozygous states. However, this simplification often does not hold. In this paper, we present a novel framework for testing the association of a single SNP and a phenotype. Different from the predominant standard approach, our methodology is guaranteed to detect all dependencies expressed by classical genetic association models. The asymptotic distribution under mild regularity assumptions is derived. Moreover, the finite sample distribution under Gaussianity is provided in which the exact p-value can be efficiently evaluated via the classical Appell hypergeometric series. Both results are extended to a regression-type setting with nuisance covariates, enabling hypotheses testing in a wide range of scenarios. A connection of our approach to score tests is explored, leading to intuitive interpretations as locally most powerful tests. A simulation study demonstrates the computational efficiency and excellent statistical performance of the proposed methodology. A real data example is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02403v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dominic Edelmann, Fernando Castro-Prado, Jelle J. Goeman</dc:creator>
    </item>
    <item>
      <title>Randomization Tests for Monotone Spillover Effects</title>
      <link>https://arxiv.org/abs/2501.02454</link>
      <description>arXiv:2501.02454v1 Announce Type: new 
Abstract: Randomization tests have gained popularity for causal inference under network interference because they are finite-sample valid with minimal assumptions. However, existing procedures are limited as they primarily focus on the existence of spillovers through sharp null hypotheses on potential outcomes. In this paper, we expand the scope of randomization procedures in network settings by developing new tests for the monotonicity of spillover effects. These tests offer insights into whether spillover effects increase, decrease, or exhibit ``diminishing returns" along certain network dimensions of interest. Our approach partitions the network into multiple (possibly overlapping) parts and testing a monotone contrast hypothesis in each sub-network. The test decisions can then be aggregated in various ways depending on how each test is constructed. We demonstrate our method through a re-analysis of a large-scale policing experiment in Colombia, which reveals evidence of monotonicity related to the ``crime displacement hypothesis". In particular, our analysis suggests that crime spillovers on a control street are increasing in the number of nearby streets treated with more intense policing, but the effect is diminishing at higher levels of exposure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02454v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Shunzhuang Huang, Xinran Li, Panos Toulis</dc:creator>
    </item>
    <item>
      <title>High-dimensional inference for single-index model with latent factors</title>
      <link>https://arxiv.org/abs/2501.02489</link>
      <description>arXiv:2501.02489v1 Announce Type: new 
Abstract: Models with latent factors recently attract a lot of attention. However, most investigations focus on linear regression models and thus cannot capture nonlinearity. To address this issue, we propose a novel Factor Augmented Single-Index Model. We first address the concern whether it is necessary to consider the augmented part by introducing a score-type test statistic. Compared with previous test statistics, our proposed test statistic does not need to estimate the high-dimensional regression coefficients, nor high-dimensional precision matrix, making it simpler in implementation. We also propose a Gaussian multiplier bootstrap to determine the critical value. The validity of our procedure is theoretically established under suitable conditions. We further investigate the penalized estimation of the regression model. With estimated latent factors, we establish the error bounds of the estimators. Lastly, we introduce debiased estimator and construct confidence interval for individual coefficient based on the asymptotic normality. No moment condition for the error term is imposed for our proposal. Thus our procedures work well when random error follows heavy-tailed distributions or when outliers are present. We demonstrate the finite sample performance of the proposed method through comprehensive numerical studies and its application to an FRED-MD macroeconomics dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02489v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanmei Shi, Meiling Hao, Yanlin Tang, Heng Lian, Xu Guo</dc:creator>
    </item>
    <item>
      <title>Selection from Hierarchical Data with Conformal e-values</title>
      <link>https://arxiv.org/abs/2501.02514</link>
      <description>arXiv:2501.02514v1 Announce Type: new 
Abstract: Distribution-free predictive inference beyond the construction of prediction sets has gained a lot of interest in recent applications. One such application is the selection task, where the objective is to design a reliable selection rule to pick out individuals with desired unobserved outcomes while controlling the error rate. In this work, we address the selection problem in the context of hierarchical data, where groups of observations may exhibit distinct within-group distributions. This generalizes existing techniques beyond the standard i.i.d./exchangeable data settings. As a correction, For hierarchical data, we introduce methods to construct valid conformal e-values, enabling control of the false discovery rate (FDR) through the e-BH procedure. In particular, we introduce and compare two approaches -- subsampling conformal e-values and hierarchical conformal e-values. Empirical results demonstrate that both approaches achieve valid FDR control while highlighting a tradeoff between stability and power. The subsampling-based method, though random, typically offers higher power, whereas the hierarchical approach, being deterministic, tends to be slightly less powerful. The effectiveness of the proposed methods is illustrated in two real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02514v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yonghoon Lee, Zhimei Ren</dc:creator>
    </item>
    <item>
      <title>Reevaluating Specificity in Neuroimaging: Implications for the Salience Network and Methodological Rigor</title>
      <link>https://arxiv.org/abs/2501.02568</link>
      <description>arXiv:2501.02568v1 Announce Type: new 
Abstract: The accurate assessment of neuroimaging specificity is critical for advancing our understanding of brain disorders. Current methodologies often rely on frequentist approaches and limited cross-pathology comparisons, leading to potential overestimations of specificity. This study critiques these limitations, highlighting the inherent shortcomings of frequentist methods in specificity calculations and the necessity of comprehensive control conditions. Through a review of the Bayesian framework, we demonstrate its superiority in evaluating specificity by incorporating probabilistic modeling and robust reverse inference. The work also emphasizes the pivotal role of well-defined control conditions in mitigating overlap among brain pathologies, particularly within shared networks like the salience network. By applying Bayesian tools such as BACON (Bayes fACtor mOdeliNg), we validate the ability to derive disease-specific patterns, contrasting these with the narrower findings of frequentist analyses. This paper underscores the importance of Bayesian methodologies and extensive meta-analytic datasets in overcoming existing challenges, ultimately paving the way for more precise neuroimaging studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02568v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tommaso Costa, Franco Cauda</dc:creator>
    </item>
    <item>
      <title>Full-conformal novelty detection: A powerful and non-random approach</title>
      <link>https://arxiv.org/abs/2501.02703</link>
      <description>arXiv:2501.02703v1 Announce Type: new 
Abstract: We introduce a powerful and non-random methodology for novelty detection, offering distribution-free false discovery rate (FDR) control guarantees. Building on the full-conformal inference framework and the concept of e-values, we introduce full-conformal e-values to quantify evidence for novelty relative to a given reference dataset. These e-values are then utilized by carefully crafted multiple testing procedures to identify a set of novel units out-of-sample with provable finite-sample FDR control. Furthermore, our method is extended to address distribution shift, accommodating scenarios where novelty detection must be performed on data drawn from a shifted distribution relative to the reference dataset. In all settings, our method is non-random and can perform powerfully with limited amounts of reference data. Empirical evaluations on synthetic and real-world datasets demonstrate that our approach significantly outperforms existing methods for novelty detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02703v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junu Lee, Ilia Popov, Zhimei Ren</dc:creator>
    </item>
    <item>
      <title>Bayesian analysis of nonlinear structured latent factor models using a Gaussian Process Prior</title>
      <link>https://arxiv.org/abs/2501.02846</link>
      <description>arXiv:2501.02846v1 Announce Type: new 
Abstract: Factor analysis models are widely utilized in social and behavioral sciences, such as psychology, education, and marketing, to measure unobservable latent traits. In this article, we introduce a nonlinear structured latent factor analysis model which is more flexible to characterize the relationship between manifest variables and latent factors. The confirmatory identifiability of the latent factor is discussed, ensuring the substantive interpretation of the latent factors. A Bayesian approach with a Gaussian process prior is proposed to estimate the unknown nonlinear function and the unknown parameters. Asymptotic results are established, including structural identifiability of the latent factors, consistency of the estimates of the unknown parameters and the unknown nonlinear function. Simulation studies and a real data analysis are conducted to investigate the performance of the proposed method. Simulation studies show our proposed method performs well in handling nonlinear model and successfully identifies the latent factors. Our analysis incorporates oil flow data, allowing us to uncover the underlying structure of latent nonlinear patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02846v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yimang Zhang, Xiaorui Wang, Jian Qing Shi</dc:creator>
    </item>
    <item>
      <title>Coarsened confounding for causal effects: a large-sample framework</title>
      <link>https://arxiv.org/abs/2501.03129</link>
      <description>arXiv:2501.03129v1 Announce Type: new 
Abstract: There has been widespread use of causal inference methods for the rigorous analysis of observational studies and to identify policy evaluations. In this article, we consider coarsened exact matching, developed in Iacus et al. (2011). While they developed some statistical properties, in this article, we study the approach using asymptotics based on a superpopulation inferential framework. This methodology is generalized to what we termed as coarsened confounding, for which we propose two new algorithms. We develop asymptotic results for the average causal effect estimator as well as providing conditions for consistency. In addition, we provide an asymptotic justification for the variance formulae in Iacus et al. (2011). A bias correction technique is proposed, and we apply the proposed methodology to data from two well-known observational studi</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03129v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Debashis Ghosh, Lei Wang</dc:creator>
    </item>
    <item>
      <title>powerROC: An Interactive Web Tool for Sample Size Calculation in Assessing Models' Discriminative Abilities</title>
      <link>https://arxiv.org/abs/2501.03155</link>
      <description>arXiv:2501.03155v1 Announce Type: new 
Abstract: Rigorous external validation is crucial for assessing the generalizability of prediction models, particularly by evaluating their discrimination (AUROC) on new data. This often involves comparing a new model's AUROC to that of an established reference model. However, many studies rely on arbitrary rules of thumb for sample size calculations, often resulting in underpowered analyses and unreliable conclusions. This paper reviews crucial concepts for accurate sample size determination in AUROC-based external validation studies, making the theory and practice more accessible to researchers and clinicians. We introduce powerROC, an open-source web tool designed to simplify these calculations, enabling both the evaluation of a single model and the comparison of two models. The tool offers guidance on selecting target precision levels and employs flexible approaches, leveraging either pilot data or user-defined probability distributions. We illustrate powerROC's utility through a case study on hospital mortality prediction using the MIMIC database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03155v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Grolleau, Robert Tibshirani, Jonathan H. Chen</dc:creator>
    </item>
    <item>
      <title>Graph Based, Adaptive, Multi Arm, Multiple Endpoint, Two Stage Design</title>
      <link>https://arxiv.org/abs/2501.03197</link>
      <description>arXiv:2501.03197v1 Announce Type: new 
Abstract: The graph based approach to multiple testing is an intuitive method that enables a study team to represent clearly, through a directed graph, its priorities for hierarchical testing of multiple hypotheses, and for propagating the available type-1 error from rejected or dropped hypotheses to hypotheses yet to be tested. Although originally developed for single stage non-adaptive designs, we show how it may be extended to two-stage designs that permit early identification of efficacious treatments, adaptive sample size re-estimation, dropping of hypotheses, and changes in the hierarchical testing strategy at the end of stage one. Two approaches are available for preserving the family wise error rate in the presence of these adaptive changes; the p-value combination method, and the conditional error rate method. In this investigation we will present the statistical methodology underlying each approach and will compare the operating characteristics of the two methods in a large simulation experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03197v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cyrus Mehta, Ajoy Mukhopadhyay, Martin Posch</dc:creator>
    </item>
    <item>
      <title>HMM-LSTM Fusion Model for Economic Forecasting</title>
      <link>https://arxiv.org/abs/2501.02002</link>
      <description>arXiv:2501.02002v1 Announce Type: cross 
Abstract: This paper explores the application of Hidden Markov Models (HMM) and Long Short-Term Memory (LSTM) neural networks for economic forecasting, focusing on predicting CPI inflation rates. The study explores a new approach that integrates HMM-derived hidden states and means as additional features for LSTM modeling, aiming to enhance the interpretability and predictive performance of the models. The research begins with data collection and preprocessing, followed by the implementation of the HMM to identify hidden states representing distinct economic conditions. Subsequently, LSTM models are trained using the original and augmented data sets, allowing for comparative analysis and evaluation. The results demonstrate that incorporating HMM-derived data improves the predictive accuracy of LSTM models, particularly in capturing complex temporal patterns and mitigating the impact of volatile economic conditions. Additionally, the paper discusses the implementation of Integrated Gradients for model interpretability and provides insights into the economic dynamics reflected in the forecasting outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02002v1</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guhan Sivakumar</dc:creator>
    </item>
    <item>
      <title>Evaluation of the HeartSteps Online Sampling Algorithm</title>
      <link>https://arxiv.org/abs/2501.02137</link>
      <description>arXiv:2501.02137v1 Announce Type: cross 
Abstract: Micro-randomized trials (MRTs), which sequentially randomize participants at multiple decision times, have gained prominence in digital intervention development. These sequential randomizations are often subject to certain constraints. In the MRT called HeartSteps V2V3, where an intervention is designed to interrupt sedentary behavior, two core design constraints need to be managed: an average of 1.5 interventions across days and the uniform delivery of interventions across decision times. Meeting both constraints, especially when the times allowed for randomization are not determined beforehand, is challenging. An online algorithm was implemented to meet these constraints in the HeartSteps V2V3 MRT. We present a case study using data from the HeartSteps V2V3 MRT, where we select appropriate metrics, discuss issues in making an accurate evaluation, and assess the algorithm's performance. Our evaluation shows that the algorithm performed well in meeting the two constraints. Furthermore, we identify areas for improvement and provide recommendations for designers of MRTs that need to satisfy these core design constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02137v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiang Meng, Walter Dempsey, Peng Liao, Nick Reid, Pedja Klasnja, Susan Murphy</dc:creator>
    </item>
    <item>
      <title>MCMC Importance Sampling via Moreau-Yosida Envelopes</title>
      <link>https://arxiv.org/abs/2501.02228</link>
      <description>arXiv:2501.02228v1 Announce Type: cross 
Abstract: The use of non-differentiable priors is standard in modern parsimonious Bayesian models. Lack of differentiability, however, precludes gradient-based Markov chain Monte Carlo (MCMC) methods for posterior sampling. Recently proposed proximal MCMC approaches can partially remedy this limitation. These approaches use gradients of a smooth approximation, constructed via Moreau-Yosida (MY) envelopes, to make proposals. In this work, we build an importance sampling paradigm by using the MY envelope as an importance distribution. Leveraging properties of the envelope, we establish asymptotic normality of the importance sampling estimator with an explicit expression for the asymptotic covariance matrix. Since the MY envelope density is smooth, it is amenable to gradient-based samplers. We provide sufficient conditions for geometric ergodicity of Metropolis-adjusted Langevin and Hamiltonian Monte Carlo algorithms, sampling from this importance distribution. A variety of numerical studies show that the proposed scheme can yield lower variance estimators compared to existing proximal MCMC alternatives, and is effective in both low and high dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02228v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Apratim Shukla, Dootika Vats, Eric C. Chi</dc:creator>
    </item>
    <item>
      <title>Interpretable Neural ODEs for Gene Regulatory Network Discovery under Perturbations</title>
      <link>https://arxiv.org/abs/2501.02409</link>
      <description>arXiv:2501.02409v1 Announce Type: cross 
Abstract: Modern high-throughput biological datasets with thousands of perturbations provide the opportunity for large-scale discovery of causal graphs that represent the regulatory interactions between genes. Numerous methods have been proposed to infer a directed acyclic graph (DAG) corresponding to the underlying gene regulatory network (GRN) that captures causal gene relationships. However, existing models have restrictive assumptions (e.g. linearity, acyclicity), limited scalability, and/or fail to address the dynamic nature of biological processes such as cellular differentiation. We propose PerturbODE, a novel framework that incorporates biologically informative neural ordinary differential equations (neural ODEs) to model cell state trajectories under perturbations and derive the causal GRN from the neural ODE's parameters. We demonstrate PerturbODE's efficacy in trajectory prediction and GRN inference across simulated and real over-expression datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02409v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>q-bio.MN</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zaikang Lin, Sei Chang, Aaron Zweig, Elham Azizi, David A. Knowles</dc:creator>
    </item>
    <item>
      <title>Tactics for Improving Least Squares Estimation</title>
      <link>https://arxiv.org/abs/2501.02475</link>
      <description>arXiv:2501.02475v1 Announce Type: cross 
Abstract: This paper deals with tactics for fast computation in least squares regression in high dimensions. These tactics include: (a) the majorization-minimization (MM) principle, (b) smoothing by Moreau envelopes, and (c) the proximal distance principal for constrained estimation. In iteratively reweighted least squares, the MM principle can create a surrogate function that trades case weights for adjusted responses. Reduction to ordinary least squares then permits the reuse of the Gram matrix and its Cholesky decomposition across iterations. This tactic is pertinent to estimation in L2E regression and generalized linear models. For problems such as quantile regression, non-smooth terms of an objective function can be replaced by their Moreau envelope approximations and majorized by spherical quadratics. Finally, penalized regression with distance-to-set penalties also benefits from this perspective. Our numerical experiments validate the speed and utility of deweighting and Moreau envelope approximations. Julia software implementing these experiments is available on our web page.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02475v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiang Heng, Hua Zhou, Kenneth Lange</dc:creator>
    </item>
    <item>
      <title>Re-examining Granger Causality from Causal Bayesian Networks Perspective</title>
      <link>https://arxiv.org/abs/2501.02672</link>
      <description>arXiv:2501.02672v1 Announce Type: cross 
Abstract: Characterizing cause-effect relationships in complex systems could be critical to understanding these systems. For many, Granger causality (GC) remains a computational tool of choice to identify causal relations in time series data. Like other causal discovery tools, GC has limitations and has been criticized as a non-causal framework. Here, we addressed one of the recurring criticisms of GC by endowing it with proper causal interpretation. This was achieved by analyzing GC from Reichenbach's Common Cause Principles (RCCPs) and causal Bayesian networks (CBNs) lenses. We showed theoretically and graphically that this reformulation endowed GC with a proper causal interpretation under certain assumptions and achieved satisfactory results on simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02672v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S. A. Adedayo</dc:creator>
    </item>
    <item>
      <title>MCBench: A Benchmark Suite for Monte Carlo Sampling Algorithms</title>
      <link>https://arxiv.org/abs/2501.03138</link>
      <description>arXiv:2501.03138v1 Announce Type: cross 
Abstract: In this paper, we present MCBench, a benchmark suite designed to assess the quality of Monte Carlo (MC) samples. The benchmark suite enables quantitative comparisons of samples by applying different metrics, including basic statistical metrics as well as more complex measures, in particular the sliced Wasserstein distance and the maximum mean discrepancy. We apply these metrics to point clouds of both independent and identically distributed (IID) samples and correlated samples generated by MC techniques, such as Markov Chain Monte Carlo or Nested Sampling. Through repeated comparisons, we evaluate test statistics of the metrics, allowing to evaluate the quality of the MC sampling algorithms.
  Our benchmark suite offers a variety of target functions with different complexities and dimensionalities, providing a versatile platform for testing the capabilities of sampling algorithms. Implemented as a Julia package, MCBench enables users to easily select test cases and metrics from the provided collections, which can be extended as needed. Users can run external sampling algorithms of their choice on these test functions and input the resulting samples to obtain detailed metrics that quantify the quality of their samples compared to the IID samples generated by our package. This approach yields clear, quantitative measures of sampling quality and allows for informed decisions about the effectiveness of different sampling methods.
  By offering such a standardized method for evaluating MC sampling quality, our benchmark suite provides researchers and practitioners from many scientific fields, such as the natural sciences, engineering, or the social sciences with a valuable tool for developing, validating and refining sampling algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03138v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeyu Ding, Cornelius Grunwald, Katja Ickstadt, Kevin Kr\"oninger, Salvatore La Cagnina</dc:creator>
    </item>
    <item>
      <title>Estimating the optimal time to perform a PET-PSMA exam in prostatectomized patients based on data from clinical practice</title>
      <link>https://arxiv.org/abs/2302.10861</link>
      <description>arXiv:2302.10861v2 Announce Type: replace 
Abstract: Prostatectomized patients are at risk of resurgence, and for this reason, during a follow-up period, they are monitored for Prostate Specific Antigen (PSA) growth, an indicator of tumor progression. The presence of tumors can be evaluated with an expensive exam, called Positron Emission Tomography with Prostate-Specific Membrane Antigen (PET-PSMA). To justify the high cost of the PET-PSMA and, at the same time, to contain the risk for the patient, this exam should be recommended only when the evidence of tumor progression is strong. With the aim of estimating the optimal time to recommend the exam based on the patient's history and collected data, we build a hierarchical Bayesian model that describes, jointly, the PSA growth curve and the probability of a positive PET-PSMA. With our proposal we process all past and present information about the patients PSA measurement and PET-PSMA results, in order to give an informed estimate of the optimal time, improving current practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.10861v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martina Amongero, Gianluca Mastrantonio, Stefano De Luca, Mauro Gasparini</dc:creator>
    </item>
    <item>
      <title>Goodness-of-Fit Tests for High-Dimensional Gaussian Graphical Models via Exchangeable Sampling</title>
      <link>https://arxiv.org/abs/2312.01815</link>
      <description>arXiv:2312.01815v2 Announce Type: replace 
Abstract: We introduce a general framework for testing goodness-of-fit for Gaussian graphical models in both the low- and high-dimensional settings. This framework is based on a novel algorithm for generating exchangeable copies by conditioning on sufficient statistics. This framework provides exact finite-sample error control regardless of the dimension and allows flexible choices of test statistics to improve power. We explore several candidate test statistics and conduct extensive simulation studies to demonstrate their finite-sample performance compared to existing methods. The proposed tests exhibit superior power, particularly in cases where the true precision matrix deviates from the null hypothesis due to many small nonzero entries. To justify theoretically, we consider a high-dimensional setting where the proposed test achieves rate-optimality under two distinct signal patterns in the precision matrix: (1) dense patterns with many small nonzero entries and (2) strong patterns with at least one large entry. Finally, we illustrate the usefulness of the proposed test through real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.01815v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaotong Lin, Weihao Li, Fangqiao Tian, Dongming Huang</dc:creator>
    </item>
    <item>
      <title>Robust inference for linear regression models with possibly skewed error distribution</title>
      <link>https://arxiv.org/abs/2404.03404</link>
      <description>arXiv:2404.03404v2 Announce Type: replace 
Abstract: Traditional methods for linear regression generally assume that the underlying error distribution, equivalently the distribution of the responses, is normal. Yet, sometimes real life response data may exhibit a skewed pattern, and assuming normality would not give reliable results in such cases. This is often observed in cases of some biomedical, behavioral, socio-economic and other variables. In this paper, we propose to use the class of skew normal (SN) distributions, which also includes the ordinary normal distribution as its special case, as the model for the errors in a linear regression setup and perform subsequent statistical inference using the popular and robust minimum density power divergence approach to get stable insights in the presence of possible data contamination (e.g., outliers). We provide the asymptotic distribution of the proposed estimator of the regression parameters and also propose robust Wald-type tests of significance for these parameters. We provide an influence function analysis of these estimators and test statistics, and also provide level and power influence functions. Numerical verification including simulation studies and real data analysis is provided to substantiate the theory developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03404v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amarnath Nandy, Ayanendranath Basu, Abhik Ghosh</dc:creator>
    </item>
    <item>
      <title>Unsupervised Training of Convex Regularizers using Maximum Likelihood Estimation</title>
      <link>https://arxiv.org/abs/2404.05445</link>
      <description>arXiv:2404.05445v3 Announce Type: replace 
Abstract: Imaging is a standard example of an inverse problem, where the task of reconstructing a ground truth from a noisy measurement is ill-posed. Recent state-of-the-art approaches for imaging use deep learning, spearheaded by unrolled and end-to-end models and trained on various image datasets. However, many such methods require the availability of ground truth data, which may be unavailable or expensive, leading to a fundamental barrier that can not be bypassed by choice of architecture. Unsupervised learning presents an alternative paradigm that bypasses this requirement, as they can be learned directly on noisy data and do not require any ground truths. A principled Bayesian approach to unsupervised learning is to maximize the marginal likelihood with respect to the given noisy measurements, which is intrinsically linked to classical variational regularization. We propose an unsupervised approach using maximum marginal likelihood estimation to train a convex neural network-based image regularization term directly on noisy measurements, improving upon previous work in both model expressiveness and dataset size. Experiments demonstrate that the proposed method produces priors that are near competitive when compared to the analogous supervised training method for various image corruption operators, maintaining significantly better generalization properties when compared to end-to-end methods. Moreover, we provide a detailed theoretical analysis of the convergence properties of our proposed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05445v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hong Ye Tan, Ziruo Cai, Marcelo Pereyra, Subhadip Mukherjee, Junqi Tang, Carola-Bibiane Sch\"onlieb</dc:creator>
    </item>
    <item>
      <title>Fast spatio-temporally varying coefficient modeling with reluctant interaction selection</title>
      <link>https://arxiv.org/abs/2410.07229</link>
      <description>arXiv:2410.07229v2 Announce Type: replace 
Abstract: Spatially and temporally varying coefficient (STVC) models are currently attracting attention as a flexible tool to explore the spatio-temporal patterns in regression coefficients. However, these models often struggle with balancing computational efficiency and model flexibility. To address this challenge, this study develops a fast and flexible method for STVC modeling. For enhanced flexibility in modeling, we assume multiple processes in each varying coefficient, including purely spatial, purely temporal, and spatio-temporal interaction processes with or without time cyclicity. While considering multiple processes can be time consuming, we combine a pre-conditioning method with a model selection procedure, inspired by reluctant interaction modeling. This approach allows us to computationally efficiently select and specify the latent space-time structure. Monte Carlo experiments demonstrate that the proposed method outperforms alternatives in terms of coefficient estimation accuracy and computational efficiency. Finally, we apply the proposed method to crime analysis using a sample size of 279,360, confirming that the proposed method provides reasonable estimates of varying coefficients. The STVC model is implemented in an R package spmoran.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07229v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daisuke Murakami, Shinichiro Shirota, Seiji Kajita, Mami Kajita</dc:creator>
    </item>
    <item>
      <title>The ultimate issue error in scientific inference: mistaking parameters for hypotheses</title>
      <link>https://arxiv.org/abs/2411.15398</link>
      <description>arXiv:2411.15398v2 Announce Type: replace 
Abstract: Statistical inference often conflates the probability of a parameter with the probability of a hypothesis, a critical misunderstanding termed the ultimate issue error. This error is pervasive across the social, biological, and medical sciences, where null hypothesis significance testing (NHST) is mistakenly understood to be testing hypotheses rather than evaluating parameter estimates. Here, we advocate for using the Weight of Evidence (WoE) approach, which integrates quantitative data with qualitative background information for more accurate and transparent inference. Through a detailed example involving the relationship between vitamin D (25-hydroxy vitamin D) levels and COVID-19 risk, we demonstrate how WoE quantifies support for hypotheses while accounting for study design biases, power, and confounding factors. These findings emphasise the necessity of combining statistical metrics with contextual evaluation. This offers a structured framework to enhance reproducibility, reduce false interpretations, and foster robust scientific conclusions across disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15398v2</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stanley E. Lazic</dc:creator>
    </item>
    <item>
      <title>moonboot: An R Package Implementing m-out-of-n Bootstrap Methods</title>
      <link>https://arxiv.org/abs/2412.05032</link>
      <description>arXiv:2412.05032v2 Announce Type: replace 
Abstract: The m-out-of-n bootstrap is a possible workaround to compute confidence intervals for bootstrap inconsistent estimators, because it works under weaker conditions than the n-out-of-n bootstrap. It has the disadvantage, however, that it requires knowledge of an appropriate scaling factor {\tau}n and that the coverage probability for finite n depends on the choice of m. This article presents an R package moonboot which implements the computation of m-out-of-n bootstrap confidence intervals and provides functions for estimating the parameters {\tau}n and m. By means of Monte Carlo simulations, we evaluate the different methods and compare them for different estimators</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05032v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christoph Dalitz, Felix L\"ogler</dc:creator>
    </item>
    <item>
      <title>Evaluating Time-Specific Treatment Effects Using Randomization Inference</title>
      <link>https://arxiv.org/abs/2412.09697</link>
      <description>arXiv:2412.09697v2 Announce Type: replace 
Abstract: This study develops a systematic approach for evaluating the effect of a treatment on a time-to-event outcome in a matched-pair study. While most methods for paired right-censored outcomes allow determining an overall treatment effect over the course of follow-up, they generally lack in providing detailed insights into how the effect changes over time. To address this gap, we propose novel tests for paired right-censored outcomes using randomization inference. We further extend our tests to matched observational studies by developing corresponding sensitivity analysis methods to take into account departures from randomization. Simulations demonstrate the robustness of our approach against various non-proportional hazards alternatives, including a crossing survival curves scenario. We demonstrate the application of our methods using a matched observational study from the Korean Longitudinal Study of Aging (KLoSA) data, focusing on the effect of social engagement on survival.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09697v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sangjin Lee, Kwonsang Lee</dc:creator>
    </item>
    <item>
      <title>The R Package WMAP: Tools for Causal Meta-Analysis by Integrating Multiple Observational Studies</title>
      <link>https://arxiv.org/abs/2501.01041</link>
      <description>arXiv:2501.01041v2 Announce Type: replace 
Abstract: Integrating multiple observational studies for meta-analysis has sparked much interest. The presented R package WMAP (Weighted Meta-Analysis with Pseudo-Population) addresses a critical gap in the implementation of integrative weighting approaches for multiple observational studies and causal inferences about various groups of subjects, such as disease subtypes. The package features three weighting approaches, each representing a special case of the unified weighting framework introduced by Guha and Li (2024), which includes an extension of inverse probability weights for data integration settings. It performs meta-analysis on user-inputted datasets as follows: (i) it first estimates the propensity scores for study-group combinations, calculates subject balancing weights, and determines the effective sample size (ESS) for a user-specified weighting method; and (ii) it then estimates various features of multiple counterfactual group outcomes, such as group medians and differences in group means for the mRNA expression of eight genes. Additionally, bootstrap variability estimates are provided. Among the implemented weighting methods, we highlight the FLEXible, Optimized, and Realistic (FLEXOR) method, which is specifically designed to maximize the ESS within the unified framework. The use of the software is illustrated by simulations as well as a multi-site breast cancer study conducted in seven medical centers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01041v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subharup Guha, Mengqi Xu, Kashish Priyam, Yi Li</dc:creator>
    </item>
    <item>
      <title>Inference on many jumps in nonparametric panel regression models</title>
      <link>https://arxiv.org/abs/2312.01162</link>
      <description>arXiv:2312.01162v3 Announce Type: replace-cross 
Abstract: We investigate the significance of change-points within fully nonparametric regression contexts, with a particular focus on panel data where data generation processes vary across units, and error terms may display complex dependency structures. In our setting the threshold effect depends on one specific covariate, and we permit the true nonparametric regression to vary based on additional (latent) variables. We propose two uniform testing procedures: one to assess the existence of change-points and another to evaluate the uniformity of such effects across units. Our approach involves deriving a straightforward analytical expression to approximate the variance-covariance structure of change-point effects under general dependency conditions. Notably, when Gaussian approximations are made to these test statistics, the intricate dependency structures within the data can be safely disregarded owing to the localized nature of the statistics. This finding bears significant implications for obtaining critical values. Through extensive simulations, we demonstrate that our tests exhibit excellent control over size and reasonable power performance in finite samples, irrespective of strong cross-sectional and weak serial dependency within the data. Furthermore, applying our tests to two datasets reveals the existence of significant nonsmooth effects in both cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.01162v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Likai Chen, Georg Keilbar, Liangjun Su, Weining Wang</dc:creator>
    </item>
    <item>
      <title>Combinations of distributional regression algorithms with application in uncertainty estimation of corrected satellite precipitation products</title>
      <link>https://arxiv.org/abs/2407.01623</link>
      <description>arXiv:2407.01623v2 Announce Type: replace-cross 
Abstract: To facilitate effective decision-making, precipitation datasets should include uncertainty estimates. Quantile regression with machine learning has been proposed for issuing such estimates. Distributional regression offers distinct advantages over quantile regression, including the ability to model intermittency as well as a stronger ability to extrapolate beyond the training data, which is critical for predicting extreme precipitation. Therefore, here, we introduce the concept of distributional regression in precipitation dataset creation, specifically for the spatial prediction task of correcting satellite precipitation products. Building upon this concept, we formulated new ensemble learning methods that can be valuable not only for spatial prediction but also for other prediction problems. These methods exploit conditional zero-adjusted probability distributions estimated with generalized additive models for location, scale and shape (GAMLSS), spline-based GAMLSS and distributional regression forests as well as their ensembles (stacking based on quantile regression and equal-weight averaging). To identify the most effective methods for our specific problem, we compared them to benchmarks using a large, multi-source precipitation dataset. Stacking was shown to be superior to individual methods at most quantile levels when evaluated with the quantile loss function. Moreover, while the relative ranking of the methods varied across different quantile levels, stacking methods, and to a lesser extent mean combiners, exhibited lower variance in their performance across different quantiles compared to individual methods that occasionally ranked extremely low. Overall, a task-specific combination of multiple distributional regression algorithms could yield significant benefits in terms of stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01623v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.mlwa.2024.100615</arxiv:DOI>
      <arxiv:journal_reference>Machine Learning with Applications 19 (2025) 100615</arxiv:journal_reference>
      <dc:creator>Georgia Papacharalampous, Hristos Tyralis, Nikolaos Doulamis, Anastasios Doulamis</dc:creator>
    </item>
    <item>
      <title>Estimation of bid-ask spreads in the presence of serial dependence</title>
      <link>https://arxiv.org/abs/2407.17401</link>
      <description>arXiv:2407.17401v3 Announce Type: replace-cross 
Abstract: Starting from a basic model in which the dynamic of the transaction prices is a geometric Brownian motion disrupted by a microstructure white noise, corresponding to the random alternation of bids and asks, we propose moment-based estimators along with their statistical properties. We then make the model more realistic by considering serial dependence: we assume a geometric fractional Brownian motion for the price, then an Ornstein-Uhlenbeck process for the microstructure noise. In these two cases of serial dependence, we propose again consistent and asymptotically normal estimators. All our estimators are compared on simulated data with existing approaches, such as Roll, Corwin-Schultz, Abdi-Ranaldo, or Ardia-Guidotti-Kroencke estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17401v3</guid>
      <category>q-fin.ST</category>
      <category>q-fin.MF</category>
      <category>q-fin.TR</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xavier Brouty, Matthieu Garcin, Hugo Roccaro</dc:creator>
    </item>
    <item>
      <title>Large Language Models for Market Research: A Data-augmentation Approach</title>
      <link>https://arxiv.org/abs/2412.19363</link>
      <description>arXiv:2412.19363v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have transformed artificial intelligence by excelling in complex natural language processing tasks. Their ability to generate human-like text has opened new possibilities for market research, particularly in conjoint analysis, where understanding consumer preferences is essential but often resource-intensive. Traditional survey-based methods face limitations in scalability and cost, making LLM-generated data a promising alternative. However, while LLMs have the potential to simulate real consumer behavior, recent studies highlight a significant gap between LLM-generated and human data, with biases introduced when substituting between the two. In this paper, we address this gap by proposing a novel statistical data augmentation approach that efficiently integrates LLM-generated data with real data in conjoint analysis. Our method leverages transfer learning principles to debias the LLM-generated data using a small amount of human data. This results in statistically robust estimators with consistent and asymptotically normal properties, in contrast to naive approaches that simply substitute human data with LLM-generated data, which can exacerbate bias. We validate our framework through an empirical study on COVID-19 vaccine preferences, demonstrating its superior ability to reduce estimation error and save data and costs by 24.9% to 79.8%. In contrast, naive approaches fail to save data due to the inherent biases in LLM-generated data compared to human data. Another empirical study on sports car choices validates the robustness of our results. Our findings suggest that while LLM-generated data is not a direct substitute for human responses, it can serve as a valuable complement when used within a robust statistical framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19363v2</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengxin Wang (Naveen Jindal School of Management, The University of Texas at Dallas), Dennis J. Zhang (Olin School of Business, Washington University in St. Louis), Heng Zhang (W. P. Carey School of Business, Arizona State University)</dc:creator>
    </item>
  </channel>
</rss>
