<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Jun 2024 04:00:53 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Sequential Monte Carlo for Cut-Bayesian Posterior Computation</title>
      <link>https://arxiv.org/abs/2406.07555</link>
      <description>arXiv:2406.07555v1 Announce Type: new 
Abstract: We propose a sequential Monte Carlo (SMC) method to efficiently and accurately compute cut-Bayesian posterior quantities of interest, variations of standard Bayesian approaches constructed primarily to account for model misspecification. We prove finite sample concentration bounds for estimators derived from the proposed method along with a linear tempering extension and apply these results to a realistic setting where a computer model is misspecified. We then illustrate the SMC method for inference in a modular chemical reactor example that includes submodels for reaction kinetics, turbulence, mass transfer, and diffusion. The samples obtained are commensurate with a direct-sampling approach that consists of running multiple Markov chains, with computational efficiency gains using the SMC method. Overall, the SMC method presented yields a novel, rigorous approach to computing with cut-Bayesian posterior distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07555v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph Mathews, Giri Gopalan, James Gattiker, Sean Smith, Devin Francom</dc:creator>
    </item>
    <item>
      <title>Stochastic Process-based Method for Degree-Degree Correlation of Evolving Networks</title>
      <link>https://arxiv.org/abs/2406.08180</link>
      <description>arXiv:2406.08180v1 Announce Type: new 
Abstract: Existing studies on the degree correlation of evolving networks typically rely on differential equations and statistical analysis, resulting in only approximate solutions due to inherent randomness. To address this limitation, we propose an improved Markov chain method for modeling degree correlation in evolving networks. By redesigning the network evolution rules to reflect actual network dynamics more accurately, we achieve a topological structure that closely matches real-world network evolution. Our method models the degree correlation evolution process for both directed and undirected networks and provides theoretical results that are verified through simulations. This work offers the first theoretical solution for the steady-state degree correlation in evolving network models and is applicable to more complex evolution mechanisms and networks with directional attributes. Additionally, it supports the study of dynamic characteristic control based on network structure at any given time, offering a new tool for researchers in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08180v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Xiao, Xiaojun Zhang</dc:creator>
    </item>
    <item>
      <title>surveygenmod2: A SAS macro for estimating complex survey adjusted generalized linear models and Wald-type tests</title>
      <link>https://arxiv.org/abs/2406.07651</link>
      <description>arXiv:2406.07651v1 Announce Type: cross 
Abstract: surveygenmod2 builds on the macro written by da Silva (2017) for generalized linear models under complex survey designs. The updated macro fixed several minor bugs we encountered while updating the macro for use in SAS\textregistered. We added additional features for conducting basic Wald-type tests on groups of parameters based on the estimated regression coefficients and parameter variance-covariance matrix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07651v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R. Noah Padgett, Ying Chen</dc:creator>
    </item>
    <item>
      <title>Global Tests for Smoothed Functions in Mean Field Variational Additive Models</title>
      <link>https://arxiv.org/abs/2406.08168</link>
      <description>arXiv:2406.08168v1 Announce Type: cross 
Abstract: Variational regression methods are an increasingly popular tool for their efficient estimation of complex. Given the mixed model representation of penalized effects, additive regression models with smoothed effects and scalar-on-function regression models can be fit relatively efficiently in a variational framework. However, inferential procedures for smoothed and functional effects in such a context is limited. We demonstrate that by using the Mean Field Variational Bayesian (MFVB) approximation to the additive model and the subsequent Coordinate Ascent Variational Inference (CAVI) algorithm, we can obtain a form of the estimated effects required of a Frequentist test for semiparametric curves. We establish MFVB approximations and CAVI algorithms for both Gaussian and binary additive models with an arbitrary number of smoothed and functional effects. We then derive a global testing framework for smoothed and functional effects. Our empirical study demonstrates that the test maintains good Frequentist properties in the variational framework and can be used to directly test results from a converged, MFVB approximation and CAVI algorithm. We illustrate the applicability of this approach in a wide range of data illustrations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08168v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark J. Meyer, Junyi Wei</dc:creator>
    </item>
    <item>
      <title>A computationally efficient procedure for combining ecological datasets by means of sequential consensus inference</title>
      <link>https://arxiv.org/abs/2406.08174</link>
      <description>arXiv:2406.08174v1 Announce Type: cross 
Abstract: Combining data has become an indispensable tool for managing the current diversity and abundance of data. But, as data complexity and data volume swell, the computational demands of previously proposed models for combining data escalate proportionally, posing a significant challenge to practical implementation. This study presents a sequential consensus Bayesian inference procedure that allows for a flexible definition of models, aiming to emulate the versatility of integrated models while significantly reducing their computational cost. The method is based on updating the distribution of the fixed effects and hyperparameters from their marginal posterior distribution throughout a sequential inference procedure, and performing a consensus on the random effects after the sequential inference is completed. The applicability, together with its strengths and limitations, is outlined in the methodological description of the procedure. The sequential consensus method is presented in two distinct algorithms. The first algorithm performs a sequential updating and consensus from the stored values of the marginal or joint posterior distribution of the random effects. The second algorithm performs an extra step, addressing the deficiencies that may arise when the model partition does not share the whole latent field. The performance of the procedure is shown by three different examples -- one simulated and two with real data -- intending to expose its strengths and limitations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08174v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mario Figueira, David Conesa, Antonio L\'opez-Qu\'ilez, Iosu Paradinas</dc:creator>
    </item>
    <item>
      <title>A Survey of Pipeline Tools for Data Engineering</title>
      <link>https://arxiv.org/abs/2406.08335</link>
      <description>arXiv:2406.08335v1 Announce Type: cross 
Abstract: Currently, a variety of pipeline tools are available for use in data engineering. Data scientists can use these tools to resolve data wrangling issues associated with data and accomplish some data engineering tasks from data ingestion through data preparation to utilization as input for machine learning (ML). Some of these tools have essential built-in components or can be combined with other tools to perform desired data engineering operations. While some tools are wholly or partly commercial, several open-source tools are available to perform expert-level data engineering tasks. This survey examines the broad categories and examples of pipeline tools based on their design and data engineering intentions. These categories are Extract Transform Load/Extract Load Transform (ETL/ELT), pipelines for Data Integration, Ingestion, and Transformation, Data Pipeline Orchestration and Workflow Management, and Machine Learning Pipelines. The survey also provides a broad outline of the utilization with examples within these broad groups and finally, a discussion is presented with case studies indicating the usage of pipeline tools for data engineering. The studies present some first-user application experiences with sample data, some complexities of the applied pipeline, and a summary note of approaches to using these tools to prepare data for machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08335v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anthony Mbata, Yaji Sripada, Mingjun Zhong</dc:creator>
    </item>
    <item>
      <title>Highest Probability Density Conformal Regions</title>
      <link>https://arxiv.org/abs/2406.08366</link>
      <description>arXiv:2406.08366v1 Announce Type: cross 
Abstract: We propose a new method for finding the highest predictive density set or region using signed conformal inference. The proposed method is computationally efficient, while also carrying conformal coverage guarantees. We prove that under, mild regularity conditions, the conformal prediction set is asymptotically close to its oracle counterpart. The efficacy of the method is illustrated through simulations and real applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08366v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Sampson, Kung-Sik Chan</dc:creator>
    </item>
    <item>
      <title>Non-asymptotic convergence bounds for modified tamed unadjusted Langevin algorithm in non-convex setting</title>
      <link>https://arxiv.org/abs/2207.02600</link>
      <description>arXiv:2207.02600v2 Announce Type: replace-cross 
Abstract: We consider the problem of sampling from a high-dimensional target distribution $\pi_\beta$ on $\mathbb{R}^d$ with density proportional to $\theta\mapsto e^{-\beta U(\theta)}$ using explicit numerical schemes based on discretising the Langevin stochastic differential equation (SDE). In recent literature, taming has been proposed and studied as a method for ensuring stability of Langevin-based numerical schemes in the case of super-linearly growing drift coefficients for the Langevin SDE. In particular, the Tamed Unadjusted Langevin Algorithm (TULA) was proposed in [Bro+19] to sample from such target distributions with the gradient of the potential $U$ being super-linearly growing. However, theoretical guarantees in Wasserstein distances for Langevin-based algorithms have traditionally been derived assuming strong convexity of the potential $U$. In this paper, we propose a novel taming factor and derive, under a setting with possibly non-convex potential $U$ and super-linearly growing gradient of $U$, non-asymptotic theoretical bounds in Wasserstein-1 and Wasserstein-2 distances between the law of our algorithm, which we name the modified Tamed Unadjusted Langevin Algorithm (mTULA), and the target distribution $\pi_\beta$. We obtain respective rates of convergence $\mathcal{O}(\lambda)$ and $\mathcal{O}(\lambda^{1/2})$ in Wasserstein-1 and Wasserstein-2 distances for the discretisation error of mTULA in step size $\lambda$. High-dimensional numerical simulations which support our theoretical findings are presented to showcase the applicability of our algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.02600v2</guid>
      <category>math.PR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ariel Neufeld, Matthew Ng Cheng En, Ying Zhang</dc:creator>
    </item>
    <item>
      <title>Effective experience rating for large insurance portfolios via surrogate modeling</title>
      <link>https://arxiv.org/abs/2211.06568</link>
      <description>arXiv:2211.06568v3 Announce Type: replace-cross 
Abstract: Experience rating in insurance uses a Bayesian credibility model to upgrade the current premiums of a contract by taking into account policyholders' attributes and their claim history. Most data-driven models used for this task are mathematically intractable, and premiums must be obtained through numerical methods such as simulation via MCMC. However, these methods can be computationally expensive and even prohibitive for large portfolios when applied at the policyholder level. Additionally, these computations become ``black-box" procedures as there is no analytical expression showing how the claim history of policyholders is used to upgrade their premiums. To address these challenges, this paper proposes a surrogate modeling approach to inexpensively derive an analytical expression for computing the Bayesian premiums for any given model, approximately. As a part of the methodology, the paper introduces a \emph{likelihood-based summary statistic} of the policyholder's claim history that serves as the main input of the surrogate model and that is sufficient for certain families of distribution, including the exponential dispersion family. As a result, the computational burden of experience rating for large portfolios is reduced through the direct evaluation of such analytical expression, which can provide a transparent and interpretable way of computing Bayesian premiums.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.06568v3</guid>
      <category>stat.ME</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.insmatheco.2024.05.004</arxiv:DOI>
      <arxiv:journal_reference>Insurance: Mathematics and Economics, Volume 118, September 2024, Pages 25-43</arxiv:journal_reference>
      <dc:creator>Sebastian Calcetero-Vanegas, Andrei L. Badescu, X. Sheldon Lin</dc:creator>
    </item>
    <item>
      <title>Deep Bayes Factors</title>
      <link>https://arxiv.org/abs/2312.05411</link>
      <description>arXiv:2312.05411v2 Announce Type: replace-cross 
Abstract: The is no other model or hypothesis verification tool in Bayesian statistics that is as widely used as the Bayes factor. We focus on generative models that are likelihood-free and, therefore, render the computation of Bayes factors (marginal likelihood ratios) far from obvious. We propose a deep learning estimator of the Bayes factor based on simulated data from two competing models using the likelihood ratio trick. This estimator is devoid of summary statistics and obviates some of the difficulties with ABC model choice. We establish sufficient conditions for consistency of our Deep Bayes Factor estimator as well as its consistency as a model selection tool. We investigate the performance of our estimator on various examples using a wide range of quality metrics related to estimation and model decision accuracy. After training, our deep learning approach enables rapid evaluations of the Bayes factor estimator at any fictional data arriving from either hypothesized model, not just the observed data $Y_0$. This allows us to inspect entire Bayes factor distributions under the two models and to quantify the relative location of the Bayes factor evaluated at $Y_0$ in light of these distributions. Such tail area evaluations are not possible for Bayes factor estimators tailored to $Y_0$. We find the performance of our Deep Bayes Factors competitive with existing MCMC techniques that require the knowledge of the likelihood function. We also consider variants for posterior or intrinsic Bayes factors estimation. We demonstrate the usefulness of our approach on a relatively high-dimensional real data example about determining cognitive biases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.05411v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jungeum Kim, Veronika Rockova</dc:creator>
    </item>
    <item>
      <title>Batch and match: black-box variational inference with a score-based divergence</title>
      <link>https://arxiv.org/abs/2402.14758</link>
      <description>arXiv:2402.14758v2 Announce Type: replace-cross 
Abstract: Most leading implementations of black-box variational inference (BBVI) are based on optimizing a stochastic evidence lower bound (ELBO). But such approaches to BBVI often converge slowly due to the high variance of their gradient estimates and their sensitivity to hyperparameters. In this work, we propose batch and match (BaM), an alternative approach to BBVI based on a score-based divergence. Notably, this score-based divergence can be optimized by a closed-form proximal update for Gaussian variational families with full covariance matrices. We analyze the convergence of BaM when the target distribution is Gaussian, and we prove that in the limit of infinite batch size the variational parameter updates converge exponentially quickly to the target mean and covariance. We also evaluate the performance of BaM on Gaussian and non-Gaussian target distributions that arise from posterior inference in hierarchical and deep generative models. In these experiments, we find that BaM typically converges in fewer (and sometimes significantly fewer) gradient evaluations than leading implementations of BBVI based on ELBO maximization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14758v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diana Cai, Chirag Modi, Loucas Pillaud-Vivien, Charles C. Margossian, Robert M. Gower, David M. Blei, Lawrence K. Saul</dc:creator>
    </item>
  </channel>
</rss>
