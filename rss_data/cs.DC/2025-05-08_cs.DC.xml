<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 May 2025 04:00:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Exploring Influence Factors on LLM Suitability for No-Code Development of End User IoT Applications</title>
      <link>https://arxiv.org/abs/2505.04710</link>
      <description>arXiv:2505.04710v1 Announce Type: new 
Abstract: With the increasing popularity of IoT applications, end users demand more personalized and intuitive functionality. A major obstacle for this, however, is that custom IoT functionality today still requires at least some coding skills. To address this, no-code development platforms have been proposed as a solution for empowering non-technical users to create applications. However, such platforms still require a certain level of technical expertise for structuring process steps or defining event-action relations. The advent of LLMs can further enhance no-code platforms by enabling natural language-based interaction, automating of complex tasks, and dynamic code generation. By allowing users to describe their requirements in natural language, LLMs can significantly streamline no-code development. As LLMs vary in performance, architecture, training data used, and the use cases they target, it is still unclear which models are best suited and what are the influence factors determining this fit. In particular, no-code development of IoT applications by non-technical users will have completely different demands on LLMs than, e.g., code generation for more open-ended applications or for supporting professional developers. In this paper, we explore the factors influencing the suitability of LLMs to no-code development of IoT applications. We also examine the role of input prompt language on accuracy and quality of generated applications as well as the influence of LLM training data. By conducting comprehensive experiments with a range of LLMs, we provide valuable insights for optimizing LLM-powered no-code platforms, guiding the selection of the suitable LLMs and their effective application. Our findings contribute to improving the accessibility, efficiency, and user experience of no-code IoT development, ultimately enabling broader adoption of IoT technologies among non-expert users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04710v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minghe Wang, Alexandra Kapp, Trever Schirmer, Tobias Pfandzelter, David Bermbach</dc:creator>
    </item>
    <item>
      <title>Big Data Architecture for Large Organizations</title>
      <link>https://arxiv.org/abs/2505.04717</link>
      <description>arXiv:2505.04717v1 Announce Type: new 
Abstract: The exponential growth of big data has transformed how large organisations leverage information to drive innovation, optimise processes, and maintain competitive advantages. However, managing and extracting insights from vast, heterogeneous data sources requires a scalable, secure, and well-integrated big data architecture. This paper proposes a comprehensive big data framework that aligns with organisational objectives while ensuring flexibility, scalability, and governance. The architecture encompasses multiple layers, including data ingestion, transformation, storage, analytics, machine learning, and security, incorporating emerging technologies such as Generative AI (GenAI) and low-code machine learning. Cloud-based implementations across Google Cloud, AWS, and Microsoft Azure are analysed, highlighting their tools and capabilities. Additionally, this study explores advancements in big data architecture, including AI-driven automation, data mesh, and Data Ocean paradigms. By establishing a structured, adaptable framework, this research provides a foundational blueprint for large organisations to harness big data as a strategic asset effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04717v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fathima Nuzla Ismail, Abira Sengupta, Shanika Amarasoma</dc:creator>
    </item>
    <item>
      <title>DFPL: Decentralized Federated Prototype Learning Across Heterogeneous Data Distributions</title>
      <link>https://arxiv.org/abs/2505.04947</link>
      <description>arXiv:2505.04947v1 Announce Type: new 
Abstract: Federated learning is a distributed machine learning paradigm that enables the collaborative training of multiple clients through centralized model aggregation. However, standard federated learning relies on a centralized server, making it vulnerable to server failures. While existing solutions utilize blockchain technology to implement Decentralized Federated Learning (DFL), the statistical heterogeneity of data distributions among clients severely degrades the DFL's performance. Driven by this issue, this paper proposes a decentralized federated prototype learning framework, named DFPL, which significantly improves the performance of distributed machine learning across heterogeneous data distributions. Specifically, our framework introduces prototype learning into DFL to address statistical heterogeneity, which greatly reduces the number of parameters exchanged between clients. Additionally, blockchain is embedded into our framework, enabling the training and mining processes to be implemented at each client. From a theoretical perspective, we provide convergence guarantee of DFPL by combining resource allocation for training and mining. The experiments highlight the superiority of our DFPL framework in communication efficiency and test performance across three benchmark datasets with heterogeneous data distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04947v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongliang Zhang, Fenghua Xu, Zhongyuan Yu, Chunqiang Hu, Shanchen Pang, Xiaofen Wang, Jiguo Yu</dc:creator>
    </item>
    <item>
      <title>CacheFL: Efficient Federated Cache Model Fine-Tuning for Vision-Language Models</title>
      <link>https://arxiv.org/abs/2505.05130</link>
      <description>arXiv:2505.05130v1 Announce Type: new 
Abstract: Large pre-trained Vision-Language Models (VLMs), such as Contrastive Language-Image Pre-training (CLIP), have exhibited remarkable zero-shot performance across various image classification tasks. Fine-tuning these models on domain-specific datasets further enhances their effectiveness for downstream applications. However, fine-tuning in cloud environments raises significant concerns regarding data security and privacy. Federated Learning (FL) offers a decentralized solution by enabling model training across local clients without centralizing sensitive data, but the high communication and computation costs of transmitting full pre-trained models during training limit its scalability. Additionally, non-Independent and Identically Distributed (non-IID) data across local clients can negatively impact model convergence and performance. To address these challenges, we propose CacheFL, a novel federated learning method that replaces traditional full model fine-tuning with lightweight cache model fine-tuning. The cache model is initialized using a class-balanced dataset generated by a generative pre-trained model, effectively mitigating the impact of non-IID data. This cache model is then distributed to local clients for fine-tuning, and the updated parameters from each client are aggregated on the server and redistributed. With the updated cache model, the classification performance of CLIP is improved after just a few epochs. By limiting the training and communication to the cache model, CacheFL significantly reduces resource demands while ensuring data privacy and security. Extensive experiments conducted on ImageNet and 10 additional datasets demonstrate that CacheFL outperforms traditional approaches in terms of classification accuracy, resource efficiency, and privacy preservation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05130v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengjun Yi, Hanwen Zhang, Hui Dou, Jian Zhao, Furao Shen</dc:creator>
    </item>
    <item>
      <title>Empirical Analysis of Transaction Conflicts in Ethereum and Solana for Parallel Execution</title>
      <link>https://arxiv.org/abs/2505.05358</link>
      <description>arXiv:2505.05358v1 Announce Type: new 
Abstract: This paper presents a comprehensive analysis of historical data across two popular blockchain networks: Ethereum and Solana. Our study focuses on two key aspects: transaction conflicts and the maximum theoretical parallelism within historical blocks. We aim to quantify the degree of transaction parallelism and assess how effectively it can be exploited by systematically examining block-level characteristics, both within individual blocks and across different historical periods. In particular, this study is the first of its kind to leverage historical transactional workloads to evaluate transactional conflict patterns. By offering a structured approach to analyzing these conflicts, our research provides valuable insights and an empirical basis for developing more efficient parallel execution techniques in the Ethereum and Solana Virtual Machines. Our empirical analysis reveals that Ethereum blocks frequently achieve high independence$-$over 50\% in more than 50\% of blocks, while Solana blocks contain longer conflict chains, comprising $\sim$59\% of the block size compared to $\sim$18\% in Ethereum, reflecting fundamentally different parallel execution dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05358v1</guid>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Parwat Singh Anjana, Srivatsan Ravi</dc:creator>
    </item>
    <item>
      <title>Walrus: An Efficient Decentralized Storage Network</title>
      <link>https://arxiv.org/abs/2505.05370</link>
      <description>arXiv:2505.05370v1 Announce Type: new 
Abstract: Decentralized storage systems face a fundamental trade-off between replication overhead, recovery efficiency, and security guarantees. Current approaches either rely on full replication, incurring substantial storage costs, or employ trivial erasure coding schemes that struggle with efficient recovery especially under high storage-node churn. We present Walrus, a novel decentralized blob storage system that addresses these limitations through multiple technical innovations. At the core of Walrus is RedStuff, a two-dimensional erasure coding protocol that achieves high security with only 4.5x replication factor, while enabling self-healing recovery that requires bandwidth proportional to only the lost data $(O(|blob|/n)$ versus $O(|blob|)$ in traditional systems). Crucially, RedStuff is the first protocol to support storage challenges in asynchronous networks, preventing adversaries from exploiting network delays to pass verification without actually storing data. Walrus also introduces a novel multi-stage epoch change protocol that efficiently handles storage node churn while maintaining uninterrupted availability during committee transitions. Our system incorporates authenticated data structures to defend against malicious clients and ensures data consistency throughout storage and retrieval processes. Experimental evaluation demonstrates that Walrus achieves practical performance at scale, making it suitable for a wide range of decentralized applications requiring high-integrity, available blob storage with reasonable overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05370v1</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Danezis, Giacomo Giuliari, Eleftherios Kokoris Kogias, Markus Legner, Jean-Pierre Smith, Alberto Sonnino, Karl W\"ust</dc:creator>
    </item>
    <item>
      <title>ORBIT-2: Scaling Exascale Vision Foundation Models for Weather and Climate Downscaling</title>
      <link>https://arxiv.org/abs/2505.04802</link>
      <description>arXiv:2505.04802v1 Announce Type: cross 
Abstract: Sparse observations and coarse-resolution climate models limit effective regional decision-making, underscoring the need for robust downscaling. However, existing AI methods struggle with generalization across variables and geographies and are constrained by the quadratic complexity of Vision Transformer (ViT) self-attention. We introduce ORBIT-2, a scalable foundation model for global, hyper-resolution climate downscaling. ORBIT-2 incorporates two key innovations: (1) Residual Slim ViT (Reslim), a lightweight architecture with residual learning and Bayesian regularization for efficient, robust prediction; and (2) TILES, a tile-wise sequence scaling algorithm that reduces self-attention complexity from quadratic to linear, enabling long-sequence processing and massive parallelism. ORBIT-2 scales to 10 billion parameters across 32,768 GPUs, achieving up to 1.8 ExaFLOPS sustained throughput and 92-98% strong scaling efficiency. It supports downscaling to 0.9 km global resolution and processes sequences up to 4.2 billion tokens. On 7 km resolution benchmarks, ORBIT-2 achieves high accuracy with R^2 scores in the range of 0.98 to 0.99 against observation data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04802v1</guid>
      <category>cs.LG</category>
      <category>astro-ph.EP</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>physics.ao-ph</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiao Wang, Jong-Youl Choi, Takuya Kurihaya, Isaac Lyngaas, Hong-Jun Yoon, Ming Fan, Nasik Muhammad Nafi, Aristeidis Tsaris, Ashwin M. Aji, Maliha Hossain, Mohamed Wahib, Dali Wang, Peter Thornton, Prasanna Balaprakash, Moetasim Ashfaq, Dan Lu</dc:creator>
    </item>
    <item>
      <title>HiPerRAG: High-Performance Retrieval Augmented Generation for Scientific Insights</title>
      <link>https://arxiv.org/abs/2505.04846</link>
      <description>arXiv:2505.04846v1 Announce Type: cross 
Abstract: The volume of scientific literature is growing exponentially, leading to underutilized discoveries, duplicated efforts, and limited cross-disciplinary collaboration. Retrieval Augmented Generation (RAG) offers a way to assist scientists by improving the factuality of Large Language Models (LLMs) in processing this influx of information. However, scaling RAG to handle millions of articles introduces significant challenges, including the high computational costs associated with parsing documents and embedding scientific knowledge, as well as the algorithmic complexity of aligning these representations with the nuanced semantics of scientific content. To address these issues, we introduce HiPerRAG, a RAG workflow powered by high performance computing (HPC) to index and retrieve knowledge from more than 3.6 million scientific articles. At its core are Oreo, a high-throughput model for multimodal document parsing, and ColTrast, a query-aware encoder fine-tuning algorithm that enhances retrieval accuracy by using contrastive learning and late-interaction techniques. HiPerRAG delivers robust performance on existing scientific question answering benchmarks and two new benchmarks introduced in this work, achieving 90% accuracy on SciQ and 76% on PubMedQA-outperforming both domain-specific models like PubMedGPT and commercial LLMs such as GPT-4. Scaling to thousands of GPUs on the Polaris, Sunspot, and Frontier supercomputers, HiPerRAG delivers million document-scale RAG workflows for unifying scientific knowledge and fostering interdisciplinary innovation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04846v1</guid>
      <category>cs.IR</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3732775.3733586</arxiv:DOI>
      <dc:creator>Ozan Gokdemir, Carlo Siebenschuh, Alexander Brace, Azton Wells, Brian Hsu, Kyle Hippe, Priyanka V. Setty, Aswathy Ajith, J. Gregory Pauloski, Varuni Sastry, Sam Foreman, Huihuo Zheng, Heng Ma, Bharat Kale, Nicholas Chia, Thomas Gibbs, Michael E. Papka, Thomas Brettin, Francis J. Alexander, Anima Anandkumar, Ian Foster, Rick Stevens, Venkatram Vishwanath, Arvind Ramanathan</dc:creator>
    </item>
    <item>
      <title>Federated Learning for Cyber Physical Systems: A Comprehensive Survey</title>
      <link>https://arxiv.org/abs/2505.04873</link>
      <description>arXiv:2505.04873v1 Announce Type: cross 
Abstract: The integration of machine learning (ML) in cyber physical systems (CPS) is a complex task due to the challenges that arise in terms of real-time decision making, safety, reliability, device heterogeneity, and data privacy. There are also open research questions that must be addressed in order to fully realize the potential of ML in CPS. Federated learning (FL), a distributed approach to ML, has become increasingly popular in recent years. It allows models to be trained using data from decentralized sources. This approach has been gaining popularity in the CPS field, as it integrates computer, communication, and physical processes. Therefore, the purpose of this work is to provide a comprehensive analysis of the most recent developments of FL-CPS, including the numerous application areas, system topologies, and algorithms developed in recent years. The paper starts by discussing recent advances in both FL and CPS, followed by their integration. Then, the paper compares the application of FL in CPS with its applications in the internet of things (IoT) in further depth to show their connections and distinctions. Furthermore, the article scrutinizes how FL is utilized in critical CPS applications, e.g., intelligent transportation systems, cybersecurity services, smart cities, and smart healthcare solutions. The study also includes critical insights and lessons learned from various FL-CPS implementations. The paper's concluding section delves into significant concerns and suggests avenues for further research in this fast-paced and dynamic era.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04873v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minh K. Quan, Pubudu N. Pathirana, Mayuri Wijayasundara, Sujeeva Setunge, Dinh C. Nguyen, Christopher G. Brinton, David J. Love, H. Vincent Poor</dc:creator>
    </item>
    <item>
      <title>PUDTune: Multi-Level Charging for High-Precision Calibration in Processing-Using-DRAM</title>
      <link>https://arxiv.org/abs/2505.05266</link>
      <description>arXiv:2505.05266v1 Announce Type: cross 
Abstract: Recently, practical analog in-memory computing has been realized using unmodified commercial DRAM modules. The underlying Processing-Using-DRAM (PUD) techniques enable high-throughput bitwise operations directly within DRAM arrays. However, the presence of inherent error-prone columns hinders PUD's practical adoption. While selectively using only error-free columns would ensure reliability, this approach significantly reduces PUD's computational throughput.
  This paper presents PUDTune, a novel high-precision calibration technique for increasing the number of error-free columns in PUD. PUDTune compensates for errors by applying pre-identified column-specific offsets to PUD operations. By leveraging multi-level charge states of DRAM cells, PUDTune generates fine-grained and wide-range offset variations despite the limited available rows. Our experiments with DDR4 DRAM demonstrate that PUDTune increases the number of error-free columns by 1.81$\times$ compared to conventional implementations, improving addition and multiplication throughput by 1.88$\times$ and 1.89$\times$ respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05266v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tatsuya Kubo, Daichi Tokuda, Lei Qu, Ting Cao, Shinya Takamaeda-Yamazaki</dc:creator>
    </item>
    <item>
      <title>Empowering Scientific Workflows with Federated Agents</title>
      <link>https://arxiv.org/abs/2505.05428</link>
      <description>arXiv:2505.05428v1 Announce Type: cross 
Abstract: Agentic systems, in which diverse agents cooperate to tackle challenging problems, are exploding in popularity in the AI community. However, the agentic frameworks used to build these systems have not previously enabled use with research cyberinfrastructure. Here we introduce Academy, a modular and extensible middleware designed to deploy autonomous agents across the federated research ecosystem, including HPC systems, experimental facilities, and data repositories. To meet the demands of scientific computing, Academy supports asynchronous execution, heterogeneous resources, high-throughput data flows, and dynamic resource availability. It provides abstractions for expressing stateful agents, managing inter-agent coordination, and integrating computation with experimental control. We present microbenchmark results that demonstrate high performance and scalability in HPC environments. To demonstrate the breadth of applications that can be supported by agentic workflow designs, we also present case studies in materials discovery, decentralized learning, and information extraction in which agents are deployed across diverse HPC systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05428v1</guid>
      <category>cs.MA</category>
      <category>cs.DC</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J. Gregory Pauloski, Yadu Babuji, Ryan Chard, Mansi Sakarvadia, Kyle Chard, Ian Foster</dc:creator>
    </item>
    <item>
      <title>Blockchain in a box: A portable blockchain network implementation on Raspberry Pi's</title>
      <link>https://arxiv.org/abs/2404.14282</link>
      <description>arXiv:2404.14282v2 Announce Type: replace 
Abstract: In this paper we describe a prototype of a blockchain-in-a-box system which allows users to easily bootstrap the whole Ethereum Proof-of-Work (PoW) network running on multiple Raspberry Pi nodes - an inexpensive modular computers. Users are able to orchestrate the whole blockchain network using a single web based interface, for example they are able to set the topology of the peer-to-peer (P2P) connections and control the initialization parameters. Each Raspberry Pi has a screen attached which visualizes current state of local blockchain, allowing users to easily visualize the consensus of the network in real time. We show how this platform can be used to perform experiments on consensus quality while using different P2P topologies. Similar experiments can be used for demonstration purposes in a workshop or other educational settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14282v2</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matija Pi\v{s}korec, Anton Ivashkevich, Said Haji Abukar, Lundrim Azemi, Md Rezuanul Haque, Mostafa Chegenizadeh, Claudio J. Tessone</dc:creator>
    </item>
    <item>
      <title>Anonymous Self-Stabilising Localisation via Spatial Population Protocols</title>
      <link>https://arxiv.org/abs/2411.08434</link>
      <description>arXiv:2411.08434v2 Announce Type: replace 
Abstract: In the distributed localization problem (DLP), $n$ anonymous robots (agents) $a_0, a_1, ..., a_{n-1}$ begin at arbitrary positions $p_0, ..., p_{n-1}$ in $S$, where $S$ is an Euclidean space. The primary goal in DLP is for agents to reach a consensus on a unified coordinate system that accurately reflects the relative positions of all points, $p_0, ..., p_{n-1}$. Extensive research on DLP has primarily focused on the feasibility and complexity of achieving consensus when agents have limited access to inter-agent distances, often due to missing or imprecise data. In this paper, however, we examine a minimalist, computationally efficient model of distributed computing in which agents have access to all pairwise distances, if needed. Specifically, we introduce a novel variant of population protocols, referred to as the spatial population protocols model. In this variant each agent can memorise one or a fixed number of coordinates, and when agents $a_i$ and $a_j$ interact, they can not only exchange their current knowledge but also either determine the distance $d(i,j)$ between them in $S$ (distance query model) or obtain the vector $v(i,j)$ spanning points $p_i$ and $p_j$ (vector query model). We propose several localisation protocols, including: (1) Two leader-based protocols with distance queries, stabilizing silently in $o(n)$ time using an efficient multi-contact epidemic, a generalization of the one-way epidemic in population protocols; (2) A distance-based protocol self-stabilizing silently in $O(n(\log n/n)^{1/(k+1)}\log n)$ time in $k$-dimensions, leveraging a leader election mechanism; (3) An optimally fast protocol with vector queries, self-stabilizing silently in $O(\log n)$ time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08434v2</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leszek G\k{a}sieniec, {\L}ukasz Kuszner, Ehsan Latif, Ramviyas Parasuraman, Paul Spirakis, Grzegorz Stachowiak</dc:creator>
    </item>
    <item>
      <title>PPFPL: Cross-silo Privacy-preserving Federated Prototype Learning Against Data Poisoning Attacks on Non-IID Data</title>
      <link>https://arxiv.org/abs/2504.03173</link>
      <description>arXiv:2504.03173v2 Announce Type: replace-cross 
Abstract: Privacy-Preserving Federated Learning (PPFL) allows multiple clients to collaboratively train a deep learning model by submitting hidden model updates. Nonetheless, PPFL is vulnerable to data poisoning attacks due to the distributed training nature of clients. Existing solutions have struggled to improve the performance of cross-silo PPFL in poisoned Non-IID data. To address the issues, this paper proposes a privacy-preserving federated prototype learning framework, named PPFPL, which enhances the cross-silo FL performance in poisoned Non-IID data while effectively resisting data poisoning attacks. Specifically, we adopt prototypes as client-submitted model updates to eliminate the impact of tampered data distribution on federated learning. Moreover, we utilize two servers to achieve Byzantine-robust aggregation by secure aggregation protocol, which greatly reduces the impact of malicious clients. Theoretical analyses confirm the convergence of PPFPL, and experimental results on publicly available datasets show that PPFPL is effective for resisting data poisoning attacks with Non-IID conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03173v2</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongliang Zhang, Jiguo Yu, Fenghua Xu, Chunqiang Hu, Yongzhao Zhang, Xiaofen Wang, Zhongyuan Yu, Xiaosong Zhang</dc:creator>
    </item>
  </channel>
</rss>
