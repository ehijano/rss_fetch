<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Dec 2024 02:54:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Why does time feel the way it does? Towards a principled account of temporal experience</title>
      <link>https://arxiv.org/abs/2412.13198</link>
      <description>arXiv:2412.13198v1 Announce Type: new 
Abstract: Time flows, or at least the time of our experience does. Can we provide an objective account of why experience, confined to the short window of the conscious present, encompasses a succession of moments that slip away from now to then--an account of why time feels flowing? Integrated Information Theory (IIT) aims to account for both the presence and quality of consciousness in objective, physical terms. Given a substrate's architecture and current state, the formalism of IIT allows one to unfold the cause-effect power of the substrate, yielding a cause-effect structure. According to IIT, this accounts in full for the presence and quality of experience, without any additional ingredients. In previous work, we showed how unfolding the cause-effect structure of non-directed grids, like those found in many posterior cortical areas, can account for the way space feels--namely, extended. Here we show that unfolding the cause-effect structure of directed grids can account for how time feels--namely, flowing. First, we argue that the conscious present is experienced as flowing because it is composed of phenomenal distinctions (moments) that are directed, and these distinctions are related in a way that satisfies directed inclusion, connection, and fusion. We then show that directed grids, which we conjecture constitute the substrate of temporal experience, yield a cause-effect structure that accounts for these and other properties of temporal experience. In this account, the experienced present does not correspond to a process unrolling in "clock time," but to a cause-effect structure specified by a system in its current state: time is a structure, not a process. We conclude by outlining similarities and differences between the experience of time and space, and some implications for the neuroscience, psychophysics, and philosophy of time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13198v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renzo Comolatti, Matteo Grasso, Giulio Tononi</dc:creator>
    </item>
    <item>
      <title>Modularity, Hierarchical Flows and Symmetry of the Drosophila Connectome</title>
      <link>https://arxiv.org/abs/2412.13202</link>
      <description>arXiv:2412.13202v1 Announce Type: new 
Abstract: This report investigates the modular organisation of the Central region in the Drosophila connectome. We identify groups of neurones amongst which information circulates rapidly before spreading to the rest of the network using Infomap. We find that information flows along pathways linking distant neurones, forming modules that span across the brain. Remarkably, these modules, derived solely from neuronal connectivity patterns, exhibit a striking left-right symmetry in their spatial distribution as well as in their connections. We also identify a hierarchical structure at the coarse-grained scale of these modules, demonstrating the directional nature of information flow in the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13202v1</guid>
      <category>q-bio.NC</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Grindrod, Renaud Lambiotte, Rohit Sahasrabuddhe</dc:creator>
    </item>
    <item>
      <title>Representational Drift and Learning-Induced Stabilization in the Olfactory Cortex</title>
      <link>https://arxiv.org/abs/2412.13713</link>
      <description>arXiv:2412.13713v1 Announce Type: new 
Abstract: The brain encodes external stimuli through patterns of neural activity, forming internal representations of the world. Recent experiments show that neural representations for a given stimulus change over time. However, the mechanistic origin for the observed "representational drift" (RD) remains unclear. Here, we propose a biologically-realistic computational model of the piriform cortex to study RD in the mammalian olfactory system by combining two mechanisms for the dynamics of synaptic weights at two separate timescales: spontaneous fluctuations on a scale of days and spike-time dependent plasticity (STDP) on a scale of seconds. Our study shows that, while spontaneous fluctuations in synaptic weights induce RD, STDP-based learning during repeated stimulus presentations can reduce it. Our model quantitatively explains recent experiments on RD in the olfactory system and offers a mechanistic explanation for the emergence of drift and its relation to learning, which may be useful to study RD in other brain regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13713v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>nlin.AO</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guillermo B. Morales, Miguel A. Mu\~noz, Yuhai Tu</dc:creator>
    </item>
    <item>
      <title>Who Saves us From Risk? Altruists Promote Cooperation in a Public Investment Game</title>
      <link>https://arxiv.org/abs/2412.13816</link>
      <description>arXiv:2412.13816v1 Announce Type: new 
Abstract: Providing commons in the risky world is crucial for human survival, however, suffers more from the "free-riding" problem. Here, we proposed a solution that limits the access of the resource to an agent and tested its efficiency with a novel public investment game. On each trial, all group members invest in an agent responsible for a gamble and distribution. Resources are distributed evenly between non-agent group members after the agent extracts for him/herself. In 3 laboratory experiments (n = 704), we found that, as an agent, many participants extracted fewer resources than others. This altruistic distribution strategy improved cooperation levels. Furthermore, this cooperation-promoting effect could be spread to selfish agents who are in the same group as the altruistic agent and was replicated in a one-shot setting. We proposed that, when the commons are under risk, this distribution solution serves as a better alternative to the well-documented sanction solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13816v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shen Zhang, Xueyi Shen, Ruida Zhu, Zilu Liang, Chao Liu</dc:creator>
    </item>
    <item>
      <title>Starting a Synthetic Biological Intelligence Lab from Scratch</title>
      <link>https://arxiv.org/abs/2412.14112</link>
      <description>arXiv:2412.14112v1 Announce Type: new 
Abstract: With the recent advancements in artificial intelligence, researchers and industries are deploying gigantic models trained on billions of samples. While training these models consumes a huge amount of energy, human brains produce similar outputs (along with other capabilities) with massively lower data and energy requirements. For this reason, more researchers are increasingly considering alternatives. One of these alternatives is known as synthetic biological intelligence, which involves training \textit{in vitro} neurons for goal-directed tasks. This multidisciplinary field requires knowledge of tissue engineering, bio-materials, digital signal processing, computer programming, neuroscience, and even artificial intelligence. The multidisciplinary requirements make starting synthetic biological intelligence research highly non-trivial and time-consuming. Generally, most labs either specialize in the biological aspects or the computational ones. Here, we propose how a lab focusing on computational aspects, including machine learning and device interfacing, can start working on synthetic biological intelligence, including organoid intelligence. We will also discuss computational aspects, which can be helpful for labs that focus on biological research. To facilitate synthetic biological intelligence research, we will describe such a general process step by step, including risks and precautions that could lead to substantial delay or additional cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14112v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Sayed Tanveer, Dhruvik Patel, Hunter E. Schweiger, Kwaku Dad Abu-Bonsrah, Brad Watmuff, Azin Azadi, Sergey Pryshchep, Karthikeyan Narayanan, Christopher Puleo, Kannathal Natarajan, Mohammed A. Mostajo-Radji, Brett J. Kagan, Ge Wang</dc:creator>
    </item>
    <item>
      <title>Optimized two-stage AI-based Neural Decoding for Enhanced Visual Stimulus Reconstruction from fMRI Data</title>
      <link>https://arxiv.org/abs/2412.13237</link>
      <description>arXiv:2412.13237v1 Announce Type: cross 
Abstract: AI-based neural decoding reconstructs visual perception by leveraging generative models to map brain activity, measured through functional MRI (fMRI), into latent hierarchical representations. Traditionally, ridge linear models transform fMRI into a latent space, which is then decoded using latent diffusion models (LDM) via a pre-trained variational autoencoder (VAE). Due to the complexity and noisiness of fMRI data, newer approaches split the reconstruction into two sequential steps, the first one providing a rough visual approximation, the second on improving the stimulus prediction via LDM endowed by CLIP embeddings. This work proposes a non-linear deep network to improve fMRI latent space representation, optimizing the dimensionality alike. Experiments on the Natural Scenes Dataset showed that the proposed architecture improved the structural similarity of the reconstructed image by about 2\% with respect to the state-of-the-art model, based on ridge linear transform. The reconstructed image's semantics improved by about 4\%, measured by perceptual similarity, with respect to the state-of-the-art. The noise sensitivity analysis of the LDM showed that the role of the first stage was fundamental to predict the stimulus featuring high structural similarity. Conversely, providing a large noise stimulus affected less the semantics of the predicted stimulus, while the structural similarity between the ground truth and predicted stimulus was very poor. The findings underscore the importance of leveraging non-linear relationships between BOLD signal and the latent representation and two-stage generative AI for optimizing the fidelity of reconstructed visual stimuli from noisy fMRI data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13237v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lorenzo Veronese, Andrea Moglia, Luca Mainardi, Pietro Cerveri</dc:creator>
    </item>
    <item>
      <title>An introduction to reinforcement learning for neuroscience</title>
      <link>https://arxiv.org/abs/2311.07315</link>
      <description>arXiv:2311.07315v3 Announce Type: replace 
Abstract: Reinforcement learning (RL) has a rich history in neuroscience, from early work on dopamine as a reward prediction error signal (Schultz et al., 1997) to recent work proposing that the brain could implement a form of 'distributional reinforcement learning' popularized in machine learning (Dabney et al., 2020). There has been a close link between theoretical advances in reinforcement learning and neuroscience experiments throughout this literature, and the theories describing the experimental data have therefore become increasingly complex. Here, we provide an introduction and mathematical background to many of the methods that have been used in systems neroscience. We start with an overview of the RL problem and classical temporal difference algorithms, followed by a discussion of 'model-free', 'model-based', and intermediate RL algorithms. We then introduce deep reinforcement learning and discuss how this framework has led to new insights in neuroscience. This includes a particular focus on meta-reinforcement learning (Wang et al., 2018) and distributional RL (Dabney et al., 2020). Finally, we discuss potential shortcomings of the RL formalism for neuroscience and highlight open questions in the field. Code that implements the methods discussed and generates the figures is also provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.07315v3</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kristopher T. Jensen</dc:creator>
    </item>
    <item>
      <title>Anti-seizure medication tapering correlates with daytime delta band power reduction in the cortex</title>
      <link>https://arxiv.org/abs/2405.01385</link>
      <description>arXiv:2405.01385v3 Announce Type: replace 
Abstract: Anti-seizure medications (ASMs) are the primary treatment for epilepsy, yet medication tapering effects have not been investigated in a dose, region, and time-dependent manner, despite their potential impact on research and clinical practice.
  We examined over 3000 hours of intracranial EEG recordings in 32 subjects during long-term monitoring, of which 22 underwent concurrent ASM tapering. We estimated ASM plasma levels based on known pharmaco-kinetics of all the major ASM types.
  We found an overall decrease in the power of delta band ({\delta}) activity around the period of maximum medication withdrawal in most (80%) subjects, independent of their epilepsy type or medication combination. The degree of withdrawal correlated positively with the magnitude of {\delta} power decrease. This dose-dependent effect was evident across all recorded cortical regions during daytime; but not in sub-cortical regions, or during night time. We found no evidence of a differential effect in seizure onset, spiking, or pathological brain regions.
  The finding of decreased {\delta} band power during ASM tapering agrees with previous literature. Our observed dose-dependent effect indicates that monitoring ASM levels in cortical regions may be feasible for applications such as medication reminder systems, or closed-loop ASM delivery systems. ASMs are also used in other neurological and psychiatric conditions, making our findings relevant to a general neuroscience and neurology audience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01385v3</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guillermo M. Besne, Nathan Evans, Mariella Panagiotopoulou, Billy Smith, Fahmida A Chowdhury, Beate Diehl, John S Duncan, Andrew W McEvoy, Anna Miserocchi, Jane de Tisi, Mathew Walker, Peter N. Taylor, Chris Thornton, Yujiang Wang</dc:creator>
    </item>
  </channel>
</rss>
