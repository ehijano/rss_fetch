<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Jun 2024 04:00:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 11 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>SPARC: Shared Perspective with Avatar Distortion for Remote Collaboration in VR</title>
      <link>https://arxiv.org/abs/2406.05209</link>
      <description>arXiv:2406.05209v1 Announce Type: new 
Abstract: Telepresence VR systems allow for face-to-face communication, promoting the feeling of presence and understanding of nonverbal cues. However, when discussing virtual 3D objects, limitations to presence and communication cause deictic gestures to lose meaning due to disparities in orientation. Current approaches use shared perspective, and avatar overlap to restore these references, which cause occlusions and discomfort that worsen when multiple users participate. We introduce a new approach to shared perspective in multi-user collaboration where the avatars are not co-located. Each person sees the others' avatars at their positions around the workspace while having a first-person view of the workspace. Whenever a user manipulates an object, others will see his/her arms stretching to reach that object in their perspective. SPARC combines a shared orientation and supports nonverbal communication, minimizing occlusions. We conducted a user study (n=18) to understand how the novel approach impacts task performance and workspace awareness. We found evidence that SPARC is more efficient and less mentally demanding than life-like settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05209v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jo\~ao Sim\~oes, Anderson Maciel, Catarina Moreira, Joaquim Jorge</dc:creator>
    </item>
    <item>
      <title>Blended Bots: Infiltration through Identity Deception on Social Media</title>
      <link>https://arxiv.org/abs/2406.05246</link>
      <description>arXiv:2406.05246v1 Announce Type: new 
Abstract: Bots are automated social media users that can be used to amplify (mis)information and sow harmful discourse. In order to effectively influence users, bots can be generated to reproduce human user behavior. Indeed, people tend to trust information coming from users with profiles that fit roles they expect to exist, such as users with gender role stereotypes. In this work, we examine differences in the types of identities in profiles of human and bot accounts with a focus on combinations of identities that represent gender role stereotypes. We find that some types of identities differentiate between human and bot profiles, confirming this approach can be a useful in distinguishing between human and bot accounts on social media. However, contrary to our expectations, we reveal that gender bias is expressed more in human accounts than bots overall. Despite having less gender bias overall, we provide examples of identities with strong associations with gender identities in bot profiles, such as those related to technology, finance, sports, and horoscopes. Finally, we discuss implications for designing constructive social media bot detection training materials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05246v1</guid>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.36190/2024.09</arxiv:DOI>
      <dc:creator>Samantha C. Phillips, Lynnette Hui Xian Ng, Kathleen M. Carley</dc:creator>
    </item>
    <item>
      <title>Artificial social influence via human-embodied AI agent interaction in immersive virtual reality (VR): Effects of similarity-matching during health conversations</title>
      <link>https://arxiv.org/abs/2406.05486</link>
      <description>arXiv:2406.05486v1 Announce Type: new 
Abstract: Interactions with artificial intelligence (AI) based agents can positively influence human behavior and judgment. However, studies to date focus on text-based conversational agents (CA) with limited embodiment, restricting our understanding of how social influence principles, such as similarity, apply to AI agents (i.e., artificial social influence). We address this gap by leveraging the latest advances in AI (language models) and combining them with immersive virtual reality (VR). Specifically, we built VR-ECAs, or embodied conversational agents that can naturally converse with humans about health-related topics in a virtual environment. Then we manipulated human-agent similarity via gender matching and examined its effects on biobehavioral (i.e., gaze), social (e.g., agent likeability), and behavioral outcomes (i.e., healthy snack selection). We found that discussing health with opposite-gender agents enhanced gaze duration and the likelihood of healthy snack selection. In addition, female participants liked the VR-ECAs more than their male counterparts, regardless of the gender of the VR-ECAs. Finally, participants experienced greater presence while conversing with VR-embodied agents than chatting with text-only agents. Overall, our findings highlight embodiment as a crucial factor in how AI influences human behavior, and our paradigm enables new experimental research at the intersection of social influence, human-AI communication, and immersive virtual reality (VR).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05486v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sue Lim, Ralf Schm\"alzle, Gary Bente</dc:creator>
    </item>
    <item>
      <title>Exploring Bridges Between Creative Coding and Visual Generative AI</title>
      <link>https://arxiv.org/abs/2406.05508</link>
      <description>arXiv:2406.05508v1 Announce Type: new 
Abstract: How to bridge generative procedural art and visual generative artificial intelligence (AI) for visual content creation is an under-explored topic. On the one hand, there are many cases where creative programmers can make use of generative AI, including stylizing canvas content and creating new content based on the existing styles of certain procedural art (style learning). On the other hand, existing approaches don't support creative programmers to flexibly leverage visual generative AI methods within the creative coding environment.
  In this work, we explore how to bridge generative procedural art creation and visual generative AI (specifically diffusion models) by programming functionalities integrated into the creative environment. Specifically, we want to explore methodologies to condition/stylize art content and perform style learning upon procedural art via accessible interactions for artists and programmers.
  We proposed two methods: GenP5, a novel p5.js library enabling generative procedural art creation with flexibly stylizing canvas content and conveniently condition art creation with pre-determined patterns; and P52Style, an extended library built upon p5.gui allowing flexible adjustment of art content and leverage of visual generative AI for style learning tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05508v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaqi Wu</dc:creator>
    </item>
    <item>
      <title>GuidelineExplorer -- Navigating through the Forrest of Actionable Guidelines on Node-Link Graph Visualization</title>
      <link>https://arxiv.org/abs/2406.05558</link>
      <description>arXiv:2406.05558v1 Announce Type: new 
Abstract: Creating graph visualizations involves many decisions, such as layout, node and edge appearance, and color choices. These decisions are challenging due to the multitude of options available. For instance, graph layout can be force-directed or orthogonal, and edges can be curved, tapered, partially drawn, or animated. Thus, research offers a multitude of guidelines to optimize graph visualizations for human perception and usability. Guidelines can be actionable, providing direct instructions, or non-actionable, specifying what to avoid. This work focuses on actionable guidelines for node-link diagrams, aiding designers in making better decisions.
  Given the abundance of graph visualization research and the difficulty in navigating it, this work aims to collect and structure actionable guidelines for node-linkvisualizations. To demonstrate the general applicability of our approach to structuring actionable guidelines for node-link diagrams, we also included guidelines for visualizing graphs as matrices. It also proposes a visual interactive system, GuidelineExplorer, to apply guidelines directly to graphs, streamlining the design process and promoting collaboration within the research community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05558v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kathrin Guckes (n\'ee Ballweg), Lisa Eisenhardt, Prof. Margit Pohl, Prof. Tatiana von Landesberger</dc:creator>
    </item>
    <item>
      <title>A Shape Change Enhancing Hierarchical Layout for the Pairwise Comparison of Directed Acyclic Graphs</title>
      <link>https://arxiv.org/abs/2406.05560</link>
      <description>arXiv:2406.05560v1 Announce Type: new 
Abstract: Comparing directed acyclic graphs is essential in various fields such as healthcare, social media, finance, biology, and marketing. DAGs often result from contagion processes over networks, including information spreading, retweet activity, disease transmission, financial crisis propagation, malware spread, and gene mutations. For instance, in disease spreading, an infected patient can transmit the disease to contacts, making it crucial to analyze and predict scenarios. Similarly, in finance, understanding the effects of saving or not saving specific banks during a crisis is vital. Experts often need to identify small differences between DAGs, such as changes in a few nodes or edges. Even the presence or absence of a single edge can be significant. Visualization plays a crucial role in facilitating these comparisons. However, standard hierarchical layout algorithms struggle to visualize subtle changes effectively. The typical hierarchical layout, with the root on top, is preferred due to its performance in comparison to other layouts. Nevertheless, these standard algorithms prioritize single-graph aesthetics over comparison suitability, making it challenging for users to spot changes. To address this issue, we propose a layout that enhances shape changes in DAGs while minimizing the impact on aesthetics. Our approach involves outwardly swapping changes, altering the DAG's shape. We introduce new drawing criteria. Our layout builds upon a Sugiyama-like hierarchical layout and implements these criteria through two extensions. We designed it this way to maintain interchangeability and accommodate future optimizations, such as pseudo-nodes for edge crossing minimization. In our evaluations, our layout achieves excellent results, with edge crossing aesthetics averaging around 0.8 (on a scale of 0 to 1). Additionally, our layout outperforms the base implementation by an average of 60-75\%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05560v1</guid>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kathrin Guckes (n\'ee Ballweg), Marc Sch\"apers, Prof. Margit Pohl, Prof. Andreas Kerren, Prof. Tatiana von Landesberger</dc:creator>
    </item>
    <item>
      <title>Learning Human Detected Differences in Directed Acyclic Graphs</title>
      <link>https://arxiv.org/abs/2406.05561</link>
      <description>arXiv:2406.05561v1 Announce Type: new 
Abstract: Prior research has shown that human perception of similarity differs from mathematical measures in visual comparison tasks, including those involving directed acyclic graphs. This divergence can lead to missed differences and skepticism about algorithmic results. To address this, we aim to learn the structural differences humans detect in graphs visually. We want to visualize these human-detected differences alongside actual changes, enhancing credibility and aiding users in spotting overlooked differences. Our approach aligns with recent research in machine learning capturing human behavior. We provide a data augmentation algorithm, a dataset, and a machine learning model to support this task. This work fills a gap in learning differences in directed acyclic graphs and contributes to better comparative visualizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05561v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kathrin Guckes (n\'ee Ballweg), Alena Beyer, Prof. Margit Pohl, Prof. Tatiana von Landesberger</dc:creator>
    </item>
    <item>
      <title>Wearable Healthcare Devices for Monitoring Stress and Attention Level in Workplace Environments</title>
      <link>https://arxiv.org/abs/2406.05813</link>
      <description>arXiv:2406.05813v1 Announce Type: new 
Abstract: Wearable devices have revolutionized healthcare monitoring, allowing us to track physiological conditions without disrupting daily routines. Whereas monitoring physical health and physical activities have been widely studied, their application and impact on mental health are significantly understudied. This work reviews the state-of-the-art, focusing on stress and concentration levels. These two can play an important role in workplace humanization. For instance, they can guide breaks in high-pressure workplaces, indicating when and how long to take. Those are important to avoid overwork and burn-out, harming employees and employers. To this end, it is necessary to study which sensors can accurately determine stress and attention levels, considering that they should not interfere with their activities and be comfortable to wear. From the software point of view, it is helpful to know the capabilities and performance of various algorithms, especially for uncontrolled workplace environments. This work aims to research, review, and compare commercially available non-intrusive measurement devices, which can be worn during the day and possibly integrated with healthcare systems for stress and concentration assessment. We analyze the performance of various algorithms used for stress and concentration level assessment and discuss future paths for reliable detection of these two parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05813v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Traunmuller, Anice Jahanjoo, Soheil Khooyooz, Amin Aminifar, Nima TaheriNejad</dc:creator>
    </item>
    <item>
      <title>Predictors of the Sense of Presence in an Immersive Audio Storytelling Experience, a Mixed Methods Study. PREPRINT</title>
      <link>https://arxiv.org/abs/2406.05856</link>
      <description>arXiv:2406.05856v1 Announce Type: new 
Abstract: This study examined which variables predicted the sense of presence (being there) in an immersive audio experience, with a focus on the impacts of immersion technology (headphones with spatialised sound versus speaker with 2D stereo sound), the nature of the audio experience, the narrative content, participants emotional and cognitive engagement and personal characteristics such as age and gender. Museum visitors listened to a story on the One Story, Many Voices immersive audio installation. A convergent mixed-methods design was used, including multiple regression analysis of survey data (n 185) and relational analysis of interview data (n 9). This study found mixed methods support from both surveys and interviews to suggest that presence was predicted by the audio quality (especially the audio being perceived as different from other audio stories they had listened to in the past), cognitive engagement (especially being totally absorbed), the narrative (the story and how it was told) and some support was found for emotional engagement (especially feeling a connection with the storyteller). Effects of the environment and immersion technology on presence received only either quantitative or qualitative support. No influences of personal characteristics were found. Findings are relevant for academics, immersive experience (XR) commissioners and developers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05856v1</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Isabelle Verhulst, Rich Hemming, Adam Ganz, James Bennett, Rachel Donnelly, Dawn Watling, Polly Dalton</dc:creator>
    </item>
    <item>
      <title>Adaptive Control in Assistive Application -- A Study Evaluating Shared Control by Users with Limited Upper Limb Mobility</title>
      <link>https://arxiv.org/abs/2406.06103</link>
      <description>arXiv:2406.06103v1 Announce Type: new 
Abstract: Shared control in assistive robotics blends human autonomy with computer assistance, thus simplifying complex tasks for individuals with physical impairments. This study assesses an adaptive Degrees of Freedom control method specifically tailored for individuals with upper limb impairments. It employs a between-subjects analysis with 24 participants, conducting 81 trials across three distinct input devices in a realistic everyday-task setting. Given the diverse capabilities of the vulnerable target demographic and the known challenges in statistical comparisons due to individual differences, the study focuses primarily on subjective qualitative data. The results reveal consistently high success rates in trial completions, irrespective of the input device used. Participants appreciated their involvement in the research process, displayed a positive outlook, and quick adaptability to the control system. Notably, each participant effectively managed the given task within a short time frame.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06103v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Felix Ferdinand Goldau, Max Pascher, Annalies Baumeister, Patrizia Tolle, Jens Gerken, Udo Frese</dc:creator>
    </item>
    <item>
      <title>Revisiting 3D Cartesian Scatterplots with a Novel Plotting Framework and a Survey</title>
      <link>https://arxiv.org/abs/2406.06146</link>
      <description>arXiv:2406.06146v1 Announce Type: new 
Abstract: 3D scatter plots are a powerful visualisation method by being able to represent 3 dimensions spatially. It can also enable the representation of additional dimensions, such as by using a colour map. An important issue with the current state of plotting software is the limited use of physical properties from the real world such as shadows to improve the effectiveness of the plots. A popular example is with the use of isometric axes in combination with same-sized points, which is equivalent to removing one whole dimension (depth perception). In static snapshot images, as found in digital and hard prints, as well with discrete data, additional cues such as movement are not present to mitigate for the loss of spatial information.
  In this paper we present a novel plotting framework that features a wide range of techniques to improve the information transfer from 3D scatterplots for multi-dimensional data. We evaluate the resulting plots by surveying 57 participants from an academic institution to get important insights on what makes 3D scatterplots effective in communicating data of more than two dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06146v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philippos Papaphilippou</dc:creator>
    </item>
    <item>
      <title>Cognitive control and mental workload in multitasking</title>
      <link>https://arxiv.org/abs/2406.06178</link>
      <description>arXiv:2406.06178v1 Announce Type: new 
Abstract: This study examines the relationship between mental workload and the cognitive control implemented in multitasking activity. A MATB-II experiment was conducted to simulate different conditions of multitasking demand, and to collect the behavioral and physiological activities of 17 participants. The results show that implementation of different modes of cognitive control can be detected with physiological indicators, and that cognitive control could be seen as a moderator of the effect of mental stress (task demand) upon mental strain (physiological responses).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06178v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philippe Rauffet (Lab-STICC_FHOOX), Sorin Moga (IMT Atlantique), Alexandre Kostenko (Lab-STICC_FHOOX)</dc:creator>
    </item>
    <item>
      <title>AI Cat Narrator: Designing an AI Tool for Exploring the Shared World and Social Connection with a Cat</title>
      <link>https://arxiv.org/abs/2406.06192</link>
      <description>arXiv:2406.06192v1 Announce Type: new 
Abstract: As technology continues to advance, the interaction between humans and cats is becoming more diverse. Our research introduces a new tool called the AI Cat Narrator, which offers a unique perspective on the shared lives of humans and cats. We combined the method of ethnography with fictional storytelling, using a defamiliarization strategy to merge real-world data seen through the eyes of cats with excerpts from cat literature. This combination serves as the foundation for a database to instruct the AI Cat Narrator in crafting alternative narrative. Our findings indicate that using defamiliarized data for training purposes significantly contributes to the development of characters that are both more empathetic and individualized. The contributions of our study are twofold: 1) proposing an innovative approach to prompting a reevaluation of living alongside cats; 2) establishing a collaborative, exploratory tool developed by humans, cats, and AI together.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06192v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3656156.3663692</arxiv:DOI>
      <dc:creator>Zhenchi Lai, Janet Yi-Ching Huang, Rung-Huei Liang</dc:creator>
    </item>
    <item>
      <title>Understanding Students' Acceptance of ChatGPT as a Translation Tool: A UTAUT Model Analysis</title>
      <link>https://arxiv.org/abs/2406.06254</link>
      <description>arXiv:2406.06254v1 Announce Type: new 
Abstract: The potential of ChatGPT to transform the education landscape is drawing increasing attention. With its translation-related capabilities being tested and examined, ChatGPT presents both opportunities and challenges for translation training. The effective integration of ChatGPT into translation training necessitates an understanding of students' reactions to and acceptance of ChatGPT-assisted translation. Against this backdrop, this study draws on the Unified Theory of Acceptance and Use of Technology (UTAUT) to examine the potential determinants of students' adoption of ChatGPT for translation and investigates the moderating effects of use experience and translation training on those relationships. An online survey targeting university students in Hong Kong collected 308 valid responses, including 148 from translation students and 160 from non-translation students. Respondents were divided into two groups based on their ChatGPT use experience. Data were analyzed using structural equation modeling. A multigroup analysis revealed different structural relationships between the influencing factors of students' intention to use ChatGPT across groups. Notably, less-experienced users' behavioral intention to use ChatGPT for translation was more strongly correlated with social influence compared with experienced users. Non-translation students' use intention was more strongly driven by facilitating conditions compared to translation majors. These results are discussed with the different primary purposes of translation and non-translation students' translation practices. The findings of this study contribute to the growing body of research on AI-powered translation training and provide insights for the ongoing adaptation of translation training programs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06254v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lulu Wang, Simin Xu, Kanglong Liu</dc:creator>
    </item>
    <item>
      <title>Re.Dis.Cover Place with Generative AI: Exploring the Experience and Design of City Wandering with Image-to-Image AI</title>
      <link>https://arxiv.org/abs/2406.06356</link>
      <description>arXiv:2406.06356v1 Announce Type: new 
Abstract: The HCI field has demonstrated a growing interest in leveraging emerging technologies to enrich urban experiences. However, insufficient studies investigate the experience and design space of AI image technology (AIGT) applications for playful urban interaction, despite its widespread adoption. To explore this gap, we conducted an exploratory study involving four participants who wandered and photographed within Eindhoven Centre and interacted with an image-to-image AI. Preliminary findings present their observations, the effect of their familiarity with places, and how AIGT becomes an explorer's tool or co-speculator. We then highlight AIGT's capability of supporting playfulness, reimaginations, and rediscoveries of places through defamiliarizing and familiarizing cityscapes. Additionally, we propose the metaphor AIGT as a 'tourist' to discuss its opportunities for engaging explorations and risks of stereotyping places. Collectively, our research provides initial empirical insights and design considerations, inspiring future HCI endeavors for creating urban play with generative AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06356v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3656156.3663691</arxiv:DOI>
      <dc:creator>Peng-Kai Hung, Janet Yi-Ching Huang, Stephan Wensveen, Rung-Huei Liang</dc:creator>
    </item>
    <item>
      <title>How is the Pilot Doing: VTOL Pilot Workload Estimation by Multimodal Machine Learning on Psycho-physiological Signals</title>
      <link>https://arxiv.org/abs/2406.06448</link>
      <description>arXiv:2406.06448v1 Announce Type: new 
Abstract: Vertical take-off and landing (VTOL) aircraft do not require a prolonged runway, thus allowing them to land almost anywhere. In recent years, their flexibility has made them popular in development, research, and operation. When compared to traditional fixed-wing aircraft and rotorcraft, VTOLs bring unique challenges as they combine many maneuvers from both types of aircraft. Pilot workload is a critical factor for safe and efficient operation of VTOLs. In this work, we conduct a user study to collect multimodal data from 28 pilots while they perform a variety of VTOL flight tasks. We analyze and interpolate behavioral patterns related to their performance and perceived workload. Finally, we build machine learning models to estimate their workload from the collected data. Our results are promising, suggesting that quantitative and accurate VTOL pilot workload monitoring is viable. Such assistive tools would help the research field understand VTOL operations and serve as a stepping stone for the industry to ensure VTOL safe operations and further remote operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06448v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jong Hoon Park, Lawrence Chen, Ian Higgins, Zhaobo Zheng, Shashank Mehrotra, Kevin Salubre, Mohammadreza Mousaei, Steven Willits, Blain Levedahl, Timothy Buker, Eliot Xing, Teruhisa Misu, Sebastian Scherer, Jean Oh</dc:creator>
    </item>
    <item>
      <title>Insights from Social Shaping Theory: The Appropriation of Large Language Models in an Undergraduate Programming Course</title>
      <link>https://arxiv.org/abs/2406.06451</link>
      <description>arXiv:2406.06451v1 Announce Type: new 
Abstract: The capability of large language models (LLMs) to generate, debug, and explain code has sparked the interest of researchers and educators in undergraduate programming, with many anticipating their transformative potential in programming education. However, decisions about why and how to use LLMs in programming education may involve more than just the assessment of an LLM's technical capabilities. Using the social shaping of technology theory as a guiding framework, our study explores how students' social perceptions influence their own LLM usage. We then examine the correlation of self-reported LLM usage with students' self-efficacy and midterm performances in an undergraduate programming course. Triangulating data from an anonymous end-of-course student survey (n = 158), a mid-course self-efficacy survey (n=158), student interviews (n = 10), self-reported LLM usage on homework, and midterm performances, we discovered that students' use of LLMs was associated with their expectations for their future careers and their perceptions of peer usage. Additionally, early self-reported LLM usage in our context correlated with lower self-efficacy and lower midterm scores, while students' perceived over-reliance on LLMs, rather than their usage itself, correlated with decreased self-efficacy later in the course.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06451v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3632620.3671098</arxiv:DOI>
      <dc:creator>Aadarsh Padiyath, Xinying Hou, Amy Pang, Diego Viramontes Vargas, Xingjian Gu, Tamara Nelson-Fromm, Zihan Wu, Mark Guzdial, Barbara Ericson</dc:creator>
    </item>
    <item>
      <title>Socially Responsible Computing in an Introductory Course</title>
      <link>https://arxiv.org/abs/2401.01285</link>
      <description>arXiv:2401.01285v1 Announce Type: cross 
Abstract: Given the potential for technology to inflict harm and injustice on society, it is imperative that we cultivate a sense of social responsibility among our students as they progress through the Computer Science (CS) curriculum. Our students need to be able to examine the social complexities in which technology development and use are situated. Also, aligning students' personal goals and their ability to achieve them in their field of study is important for promoting motivation and a sense of belonging. Promoting communal goals while learning computing can help broaden participation, particularly among groups who have been historically marginalized in computing. Keeping these considerations in mind, we piloted an introductory Java programming course in which activities engaging students in ethical and socially responsible considerations were integrated across modules. Rather than adding social on top of the technical content, our curricular approach seeks to weave them together. The data from the class suggests that the students found the inclusion of the social context in the technical assignments to be more motivating and expressed greater agency in realizing social change. We share our approach to designing this new introductory socially responsible computing course and the students' reflections. We also highlight seven considerations for educators seeking to incorporate socially responsible computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01285v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3626252.3630926</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 55th ACM Technical Symposium on Computer Science Education (SIGCSE 2024)</arxiv:journal_reference>
      <dc:creator>Aakash Gautam, Anagha Kulkarni, Sarah Hug, Jane Lehr, Ilmi Yoon</dc:creator>
    </item>
    <item>
      <title>How to Strategize Human Content Creation in the Era of GenAI?</title>
      <link>https://arxiv.org/abs/2406.05187</link>
      <description>arXiv:2406.05187v1 Announce Type: cross 
Abstract: Generative AI (GenAI) will have significant impact on content creation platforms. In this paper, we study the dynamic competition between a GenAI and a human contributor. Unlike the human, the GenAI's content only improves when more contents are created by human over the time; however, GenAI has the advantage of generating content at a lower cost. We study the algorithmic problem in this dynamic competition model about how the human contributor can maximize her utility when competing against the GenAI for content generation over a set of topics. In time-sensitive content domains (e.g., news or pop music creation) where contents' value diminishes over time, we show that there is no polynomial time algorithm for finding the human's optimal (dynamic) strategy, unless the randomized exponential time hypothesis is false. Fortunately, we are able to design a polynomial time algorithm that naturally cycles between myopically optimizing over a short time window and pausing and provably guarantees an approximation ratio of $\frac{1}{2}$. We then turn to time-insensitive content domains where contents do not lose their value (e.g., contents on history facts). Interestingly, we show that this setting permits a polynomial time algorithm that maximizes the human's utility in the long run.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05187v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seyed A. Esmaeili, Kshipra Bhawalkar, Zhe Feng, Di Wang, Haifeng Xu</dc:creator>
    </item>
    <item>
      <title>Should you use a probabilistic duration model in TTS? Probably! Especially for spontaneous speech</title>
      <link>https://arxiv.org/abs/2406.05401</link>
      <description>arXiv:2406.05401v1 Announce Type: cross 
Abstract: Converting input symbols to output audio in TTS requires modelling the durations of speech sounds. Leading non-autoregressive (NAR) TTS models treat duration modelling as a regression problem. The same utterance is then spoken with identical timings every time, unlike when a human speaks. Probabilistic models of duration have been proposed, but there is mixed evidence of their benefits. However, prior studies generally only consider speech read aloud, and ignore spontaneous speech, despite the latter being both a more common and a more variable mode of speaking. We compare the effect of conventional deterministic duration modelling to durations sampled from a powerful probabilistic model based on conditional flow matching (OT-CFM), in three different NAR TTS approaches: regression-based, deep generative, and end-to-end. Across four different corpora, stochastic duration modelling improves probabilistic NAR TTS approaches, especially for spontaneous speech. Please see https://shivammehta25.github.io/prob_dur/ for audio and resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05401v1</guid>
      <category>eess.AS</category>
      <category>cs.HC</category>
      <category>cs.SD</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shivam Mehta, Harm Lameris, Rajiv Punmiya, Jonas Beskow, \'Eva Sz\'ekely, Gustav Eje Henter</dc:creator>
    </item>
    <item>
      <title>Unsupervised learning of Data-driven Facial Expression Coding System (DFECS) using keypoint tracking</title>
      <link>https://arxiv.org/abs/2406.05434</link>
      <description>arXiv:2406.05434v1 Announce Type: cross 
Abstract: The development of existing facial coding systems, such as the Facial Action Coding System (FACS), relied on manual examination of facial expression videos for defining Action Units (AUs). To overcome the labor-intensive nature of this process, we propose the unsupervised learning of an automated facial coding system by leveraging computer-vision-based facial keypoint tracking. In this novel facial coding system called the Data-driven Facial Expression Coding System (DFECS), the AUs are estimated by applying dimensionality reduction to facial keypoint movements from a neutral frame through a proposed Full Face Model (FFM). FFM employs a two-level decomposition using advanced dimensionality reduction techniques such as dictionary learning (DL) and non-negative matrix factorization (NMF). These techniques enhance the interpretability of AUs by introducing constraints such as sparsity and positivity to the encoding matrix. Results show that DFECS AUs estimated from the DISFA dataset can account for an average variance of up to 91.29 percent in test datasets (CK+ and BP4D-Spontaneous) and also surpass the variance explained by keypoint-based equivalents of FACS AUs in these datasets. Additionally, 87.5 percent of DFECS AUs are interpretable, i.e., align with the direction of facial muscle movements. In summary, advancements in automated facial coding systems can accelerate facial expression analysis across diverse fields such as security, healthcare, and entertainment. These advancements offer numerous benefits, including enhanced detection of abnormal behavior, improved pain analysis in healthcare settings, and enriched emotion-driven interactions. To facilitate further research, the code repository of DFECS has been made publicly accessible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05434v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shivansh Chandra Tripathi, Rahul Garg</dc:creator>
    </item>
    <item>
      <title>Digital Business Model Analysis Using a Large Language Model</title>
      <link>https://arxiv.org/abs/2406.05741</link>
      <description>arXiv:2406.05741v1 Announce Type: cross 
Abstract: Digital transformation (DX) has recently become a pressing issue for many companies as the latest digital technologies, such as artificial intelligence and the Internet of Things, can be easily utilized. However, devising new business models is not easy for compa-nies, though they can improve their operations through digital technologies. Thus, business model design support methods are needed by people who lack digital tech-nology expertise. In contrast, large language models (LLMs) represented by ChatGPT and natural language processing utilizing LLMs have been developed revolutionarily. A business model design support system that utilizes these technologies has great potential. However, research on this area is scant. Accordingly, this study proposes an LLM-based method for comparing and analyzing similar companies from different business do-mains as a first step toward business model design support utilizing LLMs. This method can support idea generation in digital business model design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05741v1</guid>
      <category>cs.OH</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Masahiro Watanabe, Naoshi Uchihira</dc:creator>
    </item>
    <item>
      <title>Methodology and Real-World Applications of Dynamic Uncertain Causality Graph for Clinical Diagnosis with Explainability and Invariance</title>
      <link>https://arxiv.org/abs/2406.05746</link>
      <description>arXiv:2406.05746v1 Announce Type: cross 
Abstract: AI-aided clinical diagnosis is desired in medical care. Existing deep learning models lack explainability and mainly focus on image analysis. The recently developed Dynamic Uncertain Causality Graph (DUCG) approach is causality-driven, explainable, and invariant across different application scenarios, without problems of data collection, labeling, fitting, privacy, bias, generalization, high cost and high energy consumption. Through close collaboration between clinical experts and DUCG technicians, 46 DUCG models covering 54 chief complaints were constructed. Over 1,000 diseases can be diagnosed without triage. Before being applied in real-world, the 46 DUCG models were retrospectively verified by third-party hospitals. The verified diagnostic precisions were no less than 95%, in which the diagnostic precision for every disease including uncommon ones was no less than 80%. After verifications, the 46 DUCG models were applied in the real-world in China. Over one million real diagnosis cases have been performed, with only 17 incorrect diagnoses identified. Due to DUCG's transparency, the mistakes causing the incorrect diagnoses were found and corrected. The diagnostic abilities of the clinicians who applied DUCG frequently were improved significantly. Following the introduction to the earlier presented DUCG methodology, the recommendation algorithm for potential medical checks is presented and the key idea of DUCG is extracted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05746v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s10462-024-10763-w</arxiv:DOI>
      <arxiv:journal_reference>Artificaial Intelligence Review, (2024) 57:151</arxiv:journal_reference>
      <dc:creator>Zhan Zhang, Qin Zhang, Yang Jiao, Lin Lu, Lin Ma, Aihua Liu, Xiao Liu, Juan Zhao, Yajun Xue, Bing Wei, Mingxia Zhang, Ru Gao, Hong Zhao, Jie Lu, Fan Li, Yang Zhang, Yiming Wang, Lei Zhang, Fengwei Tian, Jie Hu, Xin Gou</dc:creator>
    </item>
    <item>
      <title>Decision-Making Behavior Evaluation Framework for LLMs under Uncertain Context</title>
      <link>https://arxiv.org/abs/2406.05972</link>
      <description>arXiv:2406.05972v1 Announce Type: cross 
Abstract: When making decisions under uncertainty, individuals often deviate from rational behavior, which can be evaluated across three dimensions: risk preference, probability weighting, and loss aversion. Given the widespread use of large language models (LLMs) in decision-making processes, it is crucial to assess whether their behavior aligns with human norms and ethical expectations or exhibits potential biases. Several empirical studies have investigated the rationality and social behavior performance of LLMs, yet their internal decision-making tendencies and capabilities remain inadequately understood. This paper proposes a framework, grounded in behavioral economics, to evaluate the decision-making behaviors of LLMs. Through a multiple-choice-list experiment, we estimate the degree of risk preference, probability weighting, and loss aversion in a context-free setting for three commercial LLMs: ChatGPT-4.0-Turbo, Claude-3-Opus, and Gemini-1.0-pro. Our results reveal that LLMs generally exhibit patterns similar to humans, such as risk aversion and loss aversion, with a tendency to overweight small probabilities. However, there are significant variations in the degree to which these behaviors are expressed across different LLMs. We also explore their behavior when embedded with socio-demographic features, uncovering significant disparities. For instance, when modeled with attributes of sexual minority groups or physical disabilities, Claude-3-Opus displays increased risk aversion, leading to more conservative choices. These findings underscore the need for careful consideration of the ethical implications and potential biases in deploying LLMs in decision-making scenarios. Therefore, this study advocates for developing standards and guidelines to ensure that LLMs operate within ethical boundaries while enhancing their utility in complex decision-making environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05972v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>econ.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingru Jia, Zehua Yuan, Junhao Pan, Paul McNamara, Deming Chen</dc:creator>
    </item>
    <item>
      <title>On the Utility of Accounting for Human Beliefs about AI Behavior in Human-AI Collaboration</title>
      <link>https://arxiv.org/abs/2406.06051</link>
      <description>arXiv:2406.06051v1 Announce Type: cross 
Abstract: To enable effective human-AI collaboration, merely optimizing AI performance while ignoring humans is not sufficient. Recent research has demonstrated that designing AI agents to account for human behavior leads to improved performance in human-AI collaboration. However, a limitation of most existing approaches is their assumption that human behavior is static, irrespective of AI behavior. In reality, humans may adjust their action plans based on their observations of AI behavior. In this paper, we address this limitation by enabling a collaborative AI agent to consider the beliefs of its human partner, i.e., what the human partner thinks the AI agent is doing, and design its action plan to facilitate easier collaboration with its human partner. Specifically, we developed a model of human beliefs that accounts for how humans reason about the behavior of their AI partners. Based on this belief model, we then developed an AI agent that considers both human behavior and human beliefs in devising its strategy for working with humans. Through extensive real-world human-subject experiments, we demonstrated that our belief model more accurately predicts humans' beliefs about AI behavior. Moreover, we showed that our design of AI agents that accounts for human beliefs enhances performance in human-AI collaboration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06051v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guanghui Yu, Robert Kasumba, Chien-Ju Ho, William Yeoh</dc:creator>
    </item>
    <item>
      <title>Multimodal Contextualized Semantic Parsing from Speech</title>
      <link>https://arxiv.org/abs/2406.06438</link>
      <description>arXiv:2406.06438v1 Announce Type: cross 
Abstract: We introduce Semantic Parsing in Contextual Environments (SPICE), a task designed to enhance artificial agents' contextual awareness by integrating multimodal inputs with prior contexts. SPICE goes beyond traditional semantic parsing by offering a structured, interpretable framework for dynamically updating an agent's knowledge with new information, mirroring the complexity of human communication. We develop the VG-SPICE dataset, crafted to challenge agents with visual scene graph construction from spoken conversational exchanges, highlighting speech and visual data integration. We also present the Audio-Vision Dialogue Scene Parser (AViD-SP) developed for use on VG-SPICE. These innovations aim to improve multimodal information processing and integration. Both the VG-SPICE dataset and the AViD-SP model are publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06438v1</guid>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jordan Voas, Raymond Mooney, David Harwath</dc:creator>
    </item>
    <item>
      <title>Demonstrating HumanTHOR: A Simulation Platform and Benchmark for Human-Robot Collaboration in a Shared Workspace</title>
      <link>https://arxiv.org/abs/2406.06498</link>
      <description>arXiv:2406.06498v1 Announce Type: cross 
Abstract: Human-robot collaboration (HRC) in a shared workspace has become a common pattern in real-world robot applications and has garnered significant research interest. However, most existing studies for human-in-the-loop (HITL) collaboration with robots in a shared workspace evaluate in either simplified game environments or physical platforms, falling short in limited realistic significance or limited scalability. To support future studies, we build an embodied framework named HumanTHOR, which enables humans to act in the simulation environment through VR devices to support HITL collaborations in a shared workspace. To validate our system, we build a benchmark of everyday tasks and conduct a preliminary user study with two baseline algorithms. The results show that the robot can effectively assist humans in collaboration, demonstrating the significance of HRC. The comparison among different levels of baselines affirms that our system can adequately evaluate robot capabilities and serve as a benchmark for different robot algorithms. The experimental results also indicate that there is still much room in the area and our system can provide a preliminary foundation for future HRC research in a shared workspace. More information about the simulation environment, experiment videos, benchmark descriptions, and additional supplementary materials can be found on the website: https://sites.google.com/view/humanthor/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06498v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenxu Wang, Boyuan Du, Jiaxin Xu, Peiyan Li, Di Guo, Huaping Liu</dc:creator>
    </item>
    <item>
      <title>NarrativeBridge: Enhancing Video Captioning with Causal-Temporal Narrative</title>
      <link>https://arxiv.org/abs/2406.06499</link>
      <description>arXiv:2406.06499v1 Announce Type: cross 
Abstract: Existing video captioning benchmarks and models lack coherent representations of causal-temporal narrative, which is sequences of events linked through cause and effect, unfolding over time and driven by characters or agents. This lack of narrative restricts models' ability to generate text descriptions that capture the causal and temporal dynamics inherent in video content. To address this gap, we propose NarrativeBridge, an approach comprising of: (1) a novel Causal-Temporal Narrative (CTN) captions benchmark generated using a large language model and few-shot prompting, explicitly encoding cause-effect temporal relationships in video descriptions, evaluated automatically to ensure caption quality and relevance; and (2) a dedicated Cause-Effect Network (CEN) architecture with separate encoders for capturing cause and effect dynamics independently, enabling effective learning and generation of captions with causal-temporal narrative. Extensive experiments demonstrate that CEN is more accurate in articulating the causal and temporal aspects of video content than the second best model (GIT): 17.88 and 17.44 CIDEr on the MSVD and MSR-VTT datasets, respectively. The proposed framework understands and generates nuanced text descriptions with intricate causal-temporal narrative structures present in videos, addressing a critical limitation in video captioning. For project details, visit https://narrativebridge.github.io/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06499v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asmar Nadeem, Faegheh Sardari, Robert Dawes, Syed Sameed Husain, Adrian Hilton, Armin Mustafa</dc:creator>
    </item>
    <item>
      <title>Spoken Humanoid Embodied Conversational Agents in Mobile Serious Games: A Usability Assessment</title>
      <link>https://arxiv.org/abs/2309.07773</link>
      <description>arXiv:2309.07773v3 Announce Type: replace 
Abstract: This paper presents an empirical investigation of the extent to which spoken Humanoid Embodied Conversational Agents (HECAs) can foster usability in mobile serious game (MSG) applications. The aim of the research is to assess the impact of multiple agents and illusion of humanness on the quality of the interaction. The experiment investigates two styles of agent presentation: an agent of high human-likeness (HECA) and an agent of low human-likeness (text). The purpose of the experiment is to assess whether and how agents of high humanlikeness can evoke the illusion of humanness and affect usability. Agents of high human-likeness were designed by following the ECA design model that is a proposed guide for ECA development. The results of the experiment with 90 participants show that users prefer to interact with the HECAs. The difference between the two versions is statistically significant with a large effect size (d=1.01), with many of the participants justifying their choice by saying that the human-like characteristics of the HECA made the version more appealing. This research provides key information on the potential effect of HECAs on serious games, which can provide insight into the design of future mobile serious games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.07773v3</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <category>cs.MM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Danai Korre, Judy Robertson</dc:creator>
    </item>
    <item>
      <title>"Living Within Four Walls": Exploring Emotional and Social Dynamics in Mobile Usage During Home Confinement</title>
      <link>https://arxiv.org/abs/2310.13304</link>
      <description>arXiv:2310.13304v2 Announce Type: replace 
Abstract: Home confinement, a situation experienced by individuals for reasons ranging from medical quarantines, rehabilitation needs, disability accommodations, and remote working, is a common yet impactful aspect of modern life. While essential in various scenarios, confinement within the home environment can profoundly influence psychological well-being and digital device usage. In this study, we delve into these effects, utilising the COVID-19 lockdown as a special case study to draw insights extending to various homebound situations. We conducted an in-situ study with 32 participants living in states affected by COVID-19 lockdowns for three weeks and analysed their emotions, well-being, social roles, and mobile usage behaviours. We extracted user activity from app usage records in an unsupervised manner, and experimental results revealed that app usage behaviours are effective indicators of emotional well-being in confined environments. Our research has great potential for developing supportive strategies and remote programs, not only for people facing similar medical isolation situations, but also for individuals in long-term home confinement, such as those with chronic illnesses, recovering from surgery, or adapting to permanent remote work arrangements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13304v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nan Gao, Sam Nolan, Kaixin Ji, Shakila Khan Rumi, Judith Simone Heinisch, Christoph Anderson, Klaus David, Flora D. Salim</dc:creator>
    </item>
    <item>
      <title>What is the focus of XAI in UI design? Prioritizing UI design principles for enhancing XAI user experience</title>
      <link>https://arxiv.org/abs/2402.13939</link>
      <description>arXiv:2402.13939v2 Announce Type: replace 
Abstract: With the widespread application of artificial intelligence(AI), the explainable AI (XAI) field has undergone a notable resurgence. In this background, the importance of user experience in XAI has become increasingly prominent. Simultaneously, the user interface (UI) serves as a crucial link between XAI and users. However, despite the existence of UI design principles for XAI, there is a lack of prioritization based on their significance. This will lead practitioners to have a vague understanding of different design principles, making it difficult to allocate design space reasonably and emphasize design focal points. This paper aims to prioritize four design principles, providing clear guidance for UI design in XAI. Initially, we conducted a lightweight summary to derive five user experience standards for non-expert users in XAI. Subsequently, we developed four corresponding webpage prototypes for the four design principles. Nineteen participants then interacted with these prototypes, providing ratings based on five user experience standards, and We calculated the weights of the design principles. Our findings indicate that, for non-expert users, "sensitivity" is the optimal UI design principle (weight = 0.3296), followed by "flexibility" (weight = 0.3014). Finally, we engage in further discussion and summarization of our research results, and present future works and limitations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13939v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dian Lei, Yao He, Jianyou Zeng</dc:creator>
    </item>
    <item>
      <title>Position: Insights from Survey Methodology can Improve Training Data</title>
      <link>https://arxiv.org/abs/2403.01208</link>
      <description>arXiv:2403.01208v2 Announce Type: replace 
Abstract: Whether future AI models are fair, trustworthy, and aligned with the public's interests rests in part on our ability to collect accurate data about what we want the models to do. However, collecting high-quality data is difficult, and few AI/ML researchers are trained in data collection methods. Recent research in data-centric AI has show that higher quality training data leads to better performing models, making this the right moment to introduce AI/ML researchers to the field of survey methodology, the science of data collection. We summarize insights from the survey methodology literature and discuss how they can improve the quality of training and feedback data. We also suggest collaborative research ideas into how biases in data collection can be mitigated, making models more accurate and human-centric.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01208v2</guid>
      <category>cs.HC</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Stephanie Eckman, Barbara Plank, Frauke Kreuter</dc:creator>
    </item>
    <item>
      <title>Reconfiguring Participatory Design to Resist AI Realism</title>
      <link>https://arxiv.org/abs/2406.03245</link>
      <description>arXiv:2406.03245v2 Announce Type: replace 
Abstract: The growing trend of artificial intelligence (AI) as a solution to social and technical problems reinforces AI Realism -- the belief that AI is an inevitable and natural order. In response, this paper argues that participatory design (PD), with its focus on democratic values and processes, can play a role in questioning and resisting AI Realism. I examine three concerning aspects of AI Realism: the facade of democratization that lacks true empowerment, demands for human adaptability in contrast to AI systems' inflexibility, and the obfuscation of essential human labor enabling the AI system. I propose resisting AI Realism by reconfiguring PD to continue engaging with value-centered visions, increasing its exploration of non-AI alternatives, and making the essential human labor underpinning AI systems visible. I position PD as a means to generate friction against AI Realism and open space for alternative futures centered on human needs and values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03245v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.SI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3661455.3669867</arxiv:DOI>
      <arxiv:journal_reference>Participatory Design Conference 2024</arxiv:journal_reference>
      <dc:creator>Aakash Gautam</dc:creator>
    </item>
    <item>
      <title>Save It for the "Hot" Day: An LLM-Empowered Visual Analytics System for Heat Risk Management</title>
      <link>https://arxiv.org/abs/2406.03317</link>
      <description>arXiv:2406.03317v2 Announce Type: replace 
Abstract: The escalating frequency and intensity of heat-related climate events, particularly heatwaves, emphasize the pressing need for advanced heat risk management strategies. Current approaches, primarily relying on numerical models, face challenges in spatial-temporal resolution and in capturing the dynamic interplay of environmental, social, and behavioral factors affecting heat risks. This has led to difficulties in translating risk assessments into effective mitigation actions. Recognizing these problems, we introduce a novel approach leveraging the burgeoning capabilities of Large Language Models (LLMs) to extract rich and contextual insights from news reports. We hence propose an LLM-empowered visual analytics system, Havior, that integrates the precise, data-driven insights of numerical models with nuanced news report information. This hybrid approach enables a more comprehensive assessment of heat risks and better identification, assessment, and mitigation of heat-related threats. The system incorporates novel visualization designs, such as "thermoglyph" and news glyph, enhancing intuitive understanding and analysis of heat risks. The integration of LLM-based techniques also enables advanced information retrieval and semantic knowledge extraction that can be guided by experts' analytics needs. Our case studies on two cities that faced significant heatwave events and interviews with five experts have demonstrated the usefulness of our system in providing in-depth and actionable insights for heat risk management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03317v2</guid>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haobo Li, Wong Kam-Kwai, Yan Luo, Juntong Chen, Chengzhong Liu, Yaxuan Zhang, Alexis Kai Hon Lau, Huamin Qu, Dongyu Liu</dc:creator>
    </item>
    <item>
      <title>Rough Set improved Therapy-Based Metaverse Assisting System</title>
      <link>https://arxiv.org/abs/2406.04465</link>
      <description>arXiv:2406.04465v2 Announce Type: replace 
Abstract: Chronic neck and shoulder pain (CNSP) is a major global public health issue. Traditional treatments like physiotherapy and rehabilitation have drawbacks, including high costs, low precision, and user discomfort. This paper presents an interactive system based on Cognitive Therapy Theory (CBT) for CNSP treatment. The system includes a pain detection module using EMG and IMU to monitor pain and optimize data with Rough Set theory, and a cognitive therapy module that processes this data further for CBT-based interventions, including massage and heating therapy. An experimental plan is outlined to evaluate the system's effectiveness and performance. The goal is to create an accessible device for CNSP therapy. Additionally, the paper explores the system's application in a metaverse environment, enhancing treatment immersion and personalization. The metaverse platform simulates treatment environments and responds to real-time patient data, allowing for continuous monitoring and adjustment of treatment plans.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04465v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin Cao, Yanhui Jiang, Chang Yu, Feiwei Qin, Zekun Jiang</dc:creator>
    </item>
    <item>
      <title>Dog Heart Rate and Blood Oxygen Metaverse Interaction System</title>
      <link>https://arxiv.org/abs/2406.04466</link>
      <description>arXiv:2406.04466v2 Announce Type: replace 
Abstract: This study developed an improved dog heart rate and blood oxygen sensor system using Arduino. Traditional methods face accuracy and reliability issues. Our system integrates advanced computational techniques with hardware-based sensing to enhance measurement precision. An Arduino microcontroller connected to a heart rate and blood oxygen sensor collects raw data, which is preprocessed and filtered to remove noise. Experimental plans include long-term monitoring of multiple dogs and comparative analysis with traditional methods to validate the system's effectiveness, aiming for widespread use in the home or veterinary clinics. Additionally, the system offers dog owners the possibility to interact with their virtual dogs in the metaverse using AR/VR, allowing them to better understand their real dogs' health conditions by observing their virtual health status.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04466v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanhui Jiang, Jin Cao, Chang Yu</dc:creator>
    </item>
    <item>
      <title>Digital assistant in a point of sales</title>
      <link>https://arxiv.org/abs/2406.04851</link>
      <description>arXiv:2406.04851v2 Announce Type: replace 
Abstract: This article investigates the deployment of a Voice User Interface (VUI)-powered digital assistant in a retail setting and assesses its impact on customer engagement and service efficiency. The study explores how digital assistants can enhance user interactions through advanced conversational capabilities with multilingual support. By integrating a digital assistant into a high-traffic retail environment, we evaluate its effectiveness in improving the quality of customer service and operational efficiency. Data collected during the experiment demonstrate varied impacts on customer interaction, revealing insights into the future optimizations of digital assistant technologies in customer-facing roles. This study contributes to the understanding of digital transformation strategies within the customer relations domain emphasizing the need for service flexibility and user-centric design in modern retail stores.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04851v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emilia Lesiak, Grzegorz Wolny, Bartosz Przyby{\l}, Micha{\l} Szczerbak</dc:creator>
    </item>
    <item>
      <title>ZSC-Eval: An Evaluation Toolkit and Benchmark for Multi-agent Zero-shot Coordination</title>
      <link>https://arxiv.org/abs/2310.05208</link>
      <description>arXiv:2310.05208v2 Announce Type: replace-cross 
Abstract: Zero-shot coordination (ZSC) is a new cooperative multi-agent reinforcement learning (MARL) challenge that aims to train an ego agent to work with diverse, unseen partners during deployment. The significant difference between the deployment-time partners' distribution and the training partners' distribution determined by the training algorithm makes ZSC a unique out-of-distribution (OOD) generalization challenge. The potential distribution gap between evaluation and deployment-time partners leads to inadequate evaluation, which is exacerbated by the lack of appropriate evaluation metrics. In this paper, we present ZSC-Eval, the first evaluation toolkit and benchmark for ZSC algorithms. ZSC-Eval consists of: 1) Generation of evaluation partner candidates through behavior-preferring rewards to approximate deployment-time partners' distribution; 2) Selection of evaluation partners by Best-Response Diversity (BR-Div); 3) Measurement of generalization performance with various evaluation partners via the Best-Response Proximity (BR-Prox) metric. We use ZSC-Eval to benchmark ZSC algorithms in Overcooked and Google Research Football environments and get novel empirical findings. We also conduct a human experiment of current ZSC algorithms to verify the ZSC-Eval's consistency with human evaluation. ZSC-Eval is now available at https://github.com/sjtu-marl/ZSC-Eval.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.05208v2</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xihuai Wang, Shao Zhang, Wenhao Zhang, Wentao Dong, Jingxiao Chen, Ying Wen, Weinan Zhang</dc:creator>
    </item>
    <item>
      <title>Individual misinformation tagging reinforces echo chambers; Collective tagging does not</title>
      <link>https://arxiv.org/abs/2311.11282</link>
      <description>arXiv:2311.11282v2 Announce Type: replace-cross 
Abstract: Fears about the destabilizing impact of misinformation online have motivated individuals and platforms to respond. Individuals have become empowered to challenge others' online claims with fact-checks in pursuit of a healthier information ecosystem and to break down echo chambers of self-reinforcing opinion. Using Twitter data, here we show the consequences of individual misinformation tagging: tagged posters had explored novel political information and expanded topical interests immediately prior, but being tagged caused posters to retreat into information bubbles. These unintended consequences were softened by a collective verification system for misinformation moderation. In Twitter's new platform, Community Notes, misinformation tagging was peer-reviewed by other fact-checkers before exposure to the poster. With collective misinformation tagging, posters were less likely to retreat from diverse information engagement. Detailed comparison suggests differences in toxicity, sentiment, readability, and delay in individual versus collective misinformation tagging messages. These findings provide evidence for differential impacts from individual versus collective moderation strategies on the diversity of information engagement and mobility across the information ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.11282v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junsol Kim, Zhao Wang, Haohan Shi, Hsin-Keng Ling, James Evans</dc:creator>
    </item>
    <item>
      <title>Conformal Prediction Sets Improve Human Decision Making</title>
      <link>https://arxiv.org/abs/2401.13744</link>
      <description>arXiv:2401.13744v3 Announce Type: replace-cross 
Abstract: In response to everyday queries, humans explicitly signal uncertainty and offer alternative answers when they are unsure. Machine learning models that output calibrated prediction sets through conformal prediction mimic this human behaviour; larger sets signal greater uncertainty while providing alternatives. In this work, we study the usefulness of conformal prediction sets as an aid for human decision making by conducting a pre-registered randomized controlled trial with conformal prediction sets provided to human subjects. With statistical significance, we find that when humans are given conformal prediction sets their accuracy on tasks improves compared to fixed-size prediction sets with the same coverage guarantee. The results show that quantifying model uncertainty with conformal prediction is helpful for human-in-the-loop decision making and human-AI teams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13744v3</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jesse C. Cresswell, Yi Sui, Bhargava Kumar, No\"el Vouitsis</dc:creator>
    </item>
    <item>
      <title>BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback</title>
      <link>https://arxiv.org/abs/2402.02479</link>
      <description>arXiv:2402.02479v2 Announce Type: replace-cross 
Abstract: Distribution matching methods for language model alignment such as Generation with Distributional Control (GDC) and Distributional Policy Gradient (DPG) have not received the same level of attention in reinforcement learning from human feedback (RLHF) as contrastive methods such as Sequence Likelihood Calibration (SLiC), Direct Preference Optimization (DPO) and its variants. We identify high variance of the gradient estimate as the primary reason for the lack of success of these methods and propose a self-normalized baseline to reduce the variance. We further generalize the target distribution in DPG, GDC and DPO by using Bayes' rule to define the reward-conditioned posterior. The resulting approach, referred to as BRAIn - Bayesian Reward-conditioned Amortized Inference acts as a bridge between distribution matching methods and DPO and significantly outperforms prior art in summarization and Antropic HH tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02479v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gaurav Pandey, Yatin Nandwani, Tahira Naseem, Mayank Mishra, Guangxuan Xu, Dinesh Raghu, Sachindra Joshi, Asim Munawar, Ram\'on Fernandez Astudillo</dc:creator>
    </item>
    <item>
      <title>ITCMA: A Generative Agent Based on a Computational Consciousness Structure</title>
      <link>https://arxiv.org/abs/2403.20097</link>
      <description>arXiv:2403.20097v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) still face challenges in tasks requiring understanding implicit instructions and applying common-sense knowledge. In such scenarios, LLMs may require multiple attempts to achieve human-level performance, potentially leading to inaccurate responses or inferences in practical environments, affecting their long-term consistency and behavior. This paper introduces the Internal Time-Consciousness Machine (ITCM), a computational consciousness structure to simulate the process of human consciousness. We further propose the ITCM-based Agent (ITCMA), which supports action generation and reasoning in open-world settings, and can independently complete tasks. ITCMA enhances LLMs' ability to understand implicit instructions and apply common-sense knowledge by considering agents' interaction and reasoning with the environment. Evaluations in the Alfworld environment show that trained ITCMA outperforms the state-of-the-art (SOTA) by 9% on the seen set. Even untrained ITCMA achieves a 96% task completion rate on the seen set, 5% higher than SOTA, indicating its superiority over traditional intelligent agents in utility and generalization. In real-world tasks with quadruped robots, the untrained ITCMA achieves an 85% task completion rate, which is close to its performance in the unseen set, demonstrating its comparable utility and universality in real-world settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20097v2</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hanzhong Zhang, Jibin Yin, Haoyang Wang, Ziwei Xiang</dc:creator>
    </item>
    <item>
      <title>Evaluating Privacy Perceptions, Experience, and Behavior of Software Development Teams</title>
      <link>https://arxiv.org/abs/2404.01283</link>
      <description>arXiv:2404.01283v2 Announce Type: replace-cross 
Abstract: With the increase in the number of privacy regulations, small development teams are forced to make privacy decisions on their own. In this paper, we conduct a mixed-method survey study, including statistical and qualitative analysis, to evaluate the privacy perceptions, practices, and knowledge of members involved in various phases of the Software Development Life Cycle (SDLC). Our survey includes 362 participants from 23 countries, encompassing roles such as product managers, developers, and testers. Our results show diverse definitions of privacy across SDLC roles, emphasizing the need for a holistic privacy approach throughout SDLC. We find that software teams, regardless of their region, are less familiar with privacy concepts (such as anonymization), relying on self-teaching and forums. Most participants are more familiar with GDPR and HIPAA than other regulations, with multi-jurisdictional compliance being their primary concern. Our results advocate the need for role-dependent solutions to address the privacy challenges, and we highlight research directions and educational takeaways to help improve privacy-aware SDLC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01283v2</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Maxwell Prybylo, Sara Haghighi, Sai Teja Peddinti, Sepideh Ghanavati</dc:creator>
    </item>
  </channel>
</rss>
