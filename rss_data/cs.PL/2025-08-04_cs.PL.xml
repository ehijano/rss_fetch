<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PL</link>
    <description>cs.PL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 04 Aug 2025 04:00:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Modelling Program Spaces in Program Synthesis with Constraints</title>
      <link>https://arxiv.org/abs/2508.00005</link>
      <description>arXiv:2508.00005v1 Announce Type: new 
Abstract: A core challenge in program synthesis is taming the large space of possible programs. Since program synthesis is essentially a combinatorial search, the community has sought to leverage powerful combinatorial constraint solvers. Here, constraints are used to express the program semantics, but not as a potentially potent tool to remove unwanted programs. Recent inductive logic programming approaches introduce constraints on the program's syntax to be synthesized. These syntactic constraints allow for checking and propagating a constraint without executing the program, and thus for arbitrary operators. In this work, we leverage syntactic constraints to model program spaces, defining not just solutions that are feasible, but also ones that are likely useful. To demonstrate this idea, we introduce BART, a solver that efficiently propagates and solves these constraints. We evaluate BART on program space enumeration tasks, finding that the constraints eliminate up to 99 percent of the program space, and that modeling program spaces significantly reduces enumeration time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00005v1</guid>
      <category>cs.PL</category>
      <category>cs.AI</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tilman Hinnerichs, Bart Swinkels, Jaap de Jong, Reuben Gardos Reid, Tudor Magirescu, Neil Yorke-Smith, Sebastijan Dumancic</dc:creator>
    </item>
    <item>
      <title>From Provable Correctness to Probabilistic Generation: A Comparative Review of Program Synthesis Paradigms</title>
      <link>https://arxiv.org/abs/2508.00013</link>
      <description>arXiv:2508.00013v1 Announce Type: new 
Abstract: Program synthesis--the automated generation of executable code from high-level specifications--has been a central goal of computer science for over fifty years. This thesis provides a comparative literature review of the main paradigms that have shaped the field, tracing its evolution from formal logic based methods to recent advances using large scale neural models. We examine five key approaches: logic based (deductive) synthesis, inductive (example based) synthesis, sketch/schema based synthesis, large language model based synthesis, and neuro-symbolic hybrids. For each, we analyze foundational principles, notable systems, and practical applications, highlighting trade offs between correctness guarantees, specification requirements, search complexity, and expressive power. By reviewing developments from formally verified synthesis tools such as KIDS and Coq to data driven models generating probabilistic code from natural language like Codex, we present a comprehensive narrative of progress and ongoing challenges. This work emphasizes the transition from symbolic to hybrid neuro-symbolic methods and outlines future directions for reliable and scalable program synthesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00013v1</guid>
      <category>cs.PL</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zurabi Kobaladze, Anna Arnania, Tamar Sanikidze</dc:creator>
    </item>
    <item>
      <title>Extended Abstract: Mutable Objects with Several Implementations</title>
      <link>https://arxiv.org/abs/2508.00016</link>
      <description>arXiv:2508.00016v1 Announce Type: new 
Abstract: This extended abstract outlines an ACL2 feature, attach-stobj, that first appeared in ACL2 Version 8.6 (October, 2024). This feature supports different executable operations for a given abstract stobj, without requiring recertification of the book that introduces that stobj or theorems about it. The paper provides background as well as a user-level overview and some implementation notes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00016v1</guid>
      <category>cs.PL</category>
      <category>cs.LO</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.423.7</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 423, 2025, pp. 60-64</arxiv:journal_reference>
      <dc:creator>Matt Kaufmann, Yahya Sohail, Warren A. Hunt Jr</dc:creator>
    </item>
    <item>
      <title>Automated Type Annotation in Python Using Large Language Models</title>
      <link>https://arxiv.org/abs/2508.00422</link>
      <description>arXiv:2508.00422v1 Announce Type: new 
Abstract: Type annotations in Python enhance maintainability and error detection. However, generating these annotations manually is error prone and requires extra effort. Traditional automation approaches like static analysis, machine learning, and deep learning struggle with limited type vocabularies, behavioral over approximation, and reliance on large labeled datasets. In this work, we explore the use of LLMs for generating type annotations in Python. We develop a generate check repair pipeline: the LLM proposes annotations guided by a Concrete Syntax Tree representation, a static type checker (Mypy) verifies them, and any errors are fed back for iterative refinement. We evaluate four LLM variants: GPT 4oMini, GPT 4.1mini (general-purpose), and O3Mini, O4Mini (reasoning optimized), on 6000 code snippets from the ManyTypes4Py benchmark. We first measure the proportion of code snippets annotated by LLMs for which MyPy reported no errors (i.e., consistent results): GPT 4oMini achieved consistency on 65.9% of cases (34.1% inconsistent), while GPT 4.1mini, O3Mini, and O4Mini each reached approximately 88.6% consistency (around 11.4% failures). To measure annotation quality, we then compute exact-match and base-type match accuracies over all 6000 snippets: GPT 4.1mini and O3Mini perform the best, achieving up to 70.5% exact match and 79.1% base type accuracy, requiring under one repair iteration on average. Our results demonstrate that general-purpose and reasoning optimized LLMs, without any task specific fine tuning or additional training can be effective in generating consistent type annotations.They perform competitively with traditional deep learning techniques which require large labeled dataset for training. While our work focuses on Python, the pipeline can be extended to other optionally typed imperative languages like Ruby</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00422v1</guid>
      <category>cs.PL</category>
      <category>cs.LG</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Varun Bharti, Shashwat Jha, Dhruv Kumar, Pankaj Jalote</dc:creator>
    </item>
    <item>
      <title>Semantic Subtyping for Maps in Erlang</title>
      <link>https://arxiv.org/abs/2508.00482</link>
      <description>arXiv:2508.00482v1 Announce Type: new 
Abstract: In this paper we will construct a set-theoretic model of types featuring type variables, base types, set-theoretic types and map types. Syntax of map types spans all the map types available in Erlang. The model of types is used to define a semantic subtyping relation based on set containment. The novelty of this work is the definition of subtyping over parameteric map types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00482v1</guid>
      <category>cs.PL</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erdem Yildirim, Albert Schimpf, Stefan Wehr, Annette Bieniusa</dc:creator>
    </item>
    <item>
      <title>Towards a unified framework for programming paradigms: A systematic review of classification formalisms and methodological foundations</title>
      <link>https://arxiv.org/abs/2508.00534</link>
      <description>arXiv:2508.00534v1 Announce Type: new 
Abstract: The rise of multi-paradigm languages challenges traditional classification methods, leading to practical software engineering issues like interoperability defects. This systematic literature review (SLR) maps the formal foundations of programming paradigms. Our objective is twofold: (1) to assess the state of the art of classification formalisms and their limitations, and (2) to identify the conceptual primitives and mathematical frameworks for a more powerful, reconstructive approach.
  Based on a synthesis of 74 primary studies, we find that existing taxonomies lack conceptual granularity, a unified formal basis, and struggle with hybrid languages. In response, our analysis reveals a strong convergence toward a compositional reconstruction of paradigms. This approach identifies a minimal set of orthogonal, atomic primitives and leverages mathematical frameworks, predominantly Type theory, Category theory and Unifying Theories of Programming (UTP), to formally guarantee their compositional properties.
  We conclude that the literature reflects a significant intellectual shift away from classification towards these promising formal, reconstructive frameworks. This review provides a map of this evolution and proposes a research agenda for their unification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00534v1</guid>
      <category>cs.PL</category>
      <category>cs.CL</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Mikel Vandeloise</dc:creator>
    </item>
    <item>
      <title>Functional vs. Object-Oriented: Comparing How Programming Paradigms Affect the Architectural Characteristics of Systems</title>
      <link>https://arxiv.org/abs/2508.00244</link>
      <description>arXiv:2508.00244v1 Announce Type: cross 
Abstract: After decades of dominance by object-oriented programming (OOP), functional programming (FP) is gaining increasing attention in the software industry. This study compares the impact of OOP and FP on the architectural characteristics of software systems. For that, it examines the design and implementation of a Digital Wallet system, developed in Kotlin (representing OOP) and Scala (representing FP). The comparison is made through both qualitative and quantitative analyses to explore how each paradigm influences the system's architectural characteristics. The self-ethnographic qualitative analysis provides a side-by-side comparison of both implementations, revealing the perspective of those writing such code. The survey-based quantitative analysis gathers feedback from developers with diverse backgrounds, showing their impressions of those reading this code. Hopefully, these results may be useful for developers or organizations seeking to make more informed decisions about which paradigm is best suited for their next project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00244v1</guid>
      <category>cs.SE</category>
      <category>cs.PL</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Briza Mel Dias de Sousa (University of S\~ao Paulo), Renato Cordeiro Ferreira (University of S\~ao Paulo, Jheronimus Academy of Data Science, Technical University of Eindhoven, Tilburg University), Alfredo Goldman (University of S\~ao Paulo)</dc:creator>
    </item>
    <item>
      <title>Loop Invariant Generation: A Hybrid Framework of Reasoning optimised LLMs and SMT Solvers</title>
      <link>https://arxiv.org/abs/2508.00419</link>
      <description>arXiv:2508.00419v1 Announce Type: cross 
Abstract: Loop invariants are essential for proving the correctness of programs with loops. Developing loop invariants is challenging, and fully automatic synthesis cannot be guaranteed for arbitrary programs. Some approaches have been proposed to synthesize loop invariants using symbolic techniques and more recently using neural approaches. These approaches are able to correctly synthesize loop invariants only for subsets of standard benchmarks. In this work, we investigate whether modern, reasoning-optimized large language models can do better. We integrate OpenAI's O1, O1-mini, and O3-mini into a tightly coupled generate-and-check pipeline with the Z3 SMT solver, using solver counterexamples to iteratively guide invariant refinement. We use Code2Inv benchmark, which provides C programs along with their formal preconditions and postconditions. On this benchmark of 133 tasks, our framework achieves 100% coverage (133 out of 133), outperforming the previous best of 107 out of 133, while requiring only 1-2 model proposals per instance and 14-55 seconds of wall-clock time. These results demonstrate that LLMs possess latent logical reasoning capabilities which can help automate loop invariant synthesis. While our experiments target C-specific programs, this approach should be generalizable to other imperative languages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00419v1</guid>
      <category>cs.LO</category>
      <category>cs.LG</category>
      <category>cs.PL</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Varun Bharti, Shashwat Jha, Dhruv Kumar, Pankaj Jalote</dc:creator>
    </item>
    <item>
      <title>Desyan: A Platform for Seamless Value-Flow and Symbolic Analysis</title>
      <link>https://arxiv.org/abs/2508.00508</link>
      <description>arXiv:2508.00508v1 Announce Type: cross 
Abstract: Over the past two decades, two different types of static analyses have emerged as dominant paradigms both in academia and industry: value-flow analysis (e.g., data-flow analysis or points-to analysis) and symbolic analysis (e.g., symbolic execution). Despite their individual successes in numerous application fields, the two approaches have remained largely separate; an artifact of the simple reality that there is no broadly adopted unifying platform for effortless and efficient integration of symbolic techniques with high-performance data-flow reasoning.
  To bridge this gap, we introduce Desyan: a platform for writing program analyses with seamless integration of value-flow and symbolic reasoning. Desyan expands a production-ready Datalog fixpoint engine (Souffl\'e) with full-fledged SMT solving invoking industry-leading SMT engines. Desyan provides constructs for automatically (and efficiently!) handling typical patterns that come up in program analysis. At the same time, the integration is agnostic with respect to the solving technology, and supports Datalog-native symbolic reasoning, via a bottom-up algebraic reasoning module.
  The result is an engine that allows blending different kinds of reasoning, as needed for the underlying analysis. For value-flow analysis, the engine is the best-in-class Datalog evaluator (often by a factor of over 20x in execution time); for applications that require full SMT (e.g., a concolic execution engine or other symbolic evaluator that needs to solve arbitrarily complex conditions), the engine is leveraging the leading SMT solvers; for lightweight symbolic evaluation (e.g., solving simple conditionals in the context of a path-sensitive analysis), the engine can use Datalog-native symbolic reasoning, achieving large speedups (often of over 2x) compared to eagerly appealing to an SMT solver.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00508v1</guid>
      <category>cs.SE</category>
      <category>cs.PL</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Panagiotis Diamantakis, Thanassis Avgerinos, Yannis Smaragdakis</dc:creator>
    </item>
    <item>
      <title>From Code to Career: Assessing Competitive Programmers for Industry Placement</title>
      <link>https://arxiv.org/abs/2508.00772</link>
      <description>arXiv:2508.00772v1 Announce Type: cross 
Abstract: In today's fast-paced tech industry, there is a growing need for tools that evaluate a programmer's job readiness based on their coding performance. This study focuses on predicting the potential of Codeforces users to secure various levels of software engineering jobs. The primary objective is to analyze how a user's competitive programming activity correlates with their chances of obtaining positions, ranging from entry-level roles to jobs at major tech companies. We collect user data using the Codeforces API, process key performance metrics, and build a prediction model using a Random Forest classifier. The model categorizes users into four levels of employability, ranging from those needing further development to those ready for top-tier tech jobs. The system is implemented using Flask and deployed on Render for real-time predictions. Our evaluation demonstrates that the approach effectively distinguishes between different skill levels based on coding proficiency and participation. This work lays a foundation for the use of machine learning in career assessment and could be extended to predict job readiness in broader technical fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00772v1</guid>
      <category>cs.SE</category>
      <category>cs.PL</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Imranur Rahman Akib, Fathima Binthe Muhammed, Umit Saha, Md Fazlul Karim Patwary, Mehrin Anannya, Md Alomgeer Hussein, Md Biplob Hosen</dc:creator>
    </item>
    <item>
      <title>Float Self-Tagging</title>
      <link>https://arxiv.org/abs/2411.16544</link>
      <description>arXiv:2411.16544v3 Announce Type: replace 
Abstract: Dynamic and polymorphic languages attach information, such as types, to run time objects, and therefore adapt the memory layout of values to include space for this information. This makes it difficult to efficiently implement IEEE754 floating-point numbers as this format does not leave an easily accessible space to store type information. The three main floating-point number encodings in use today, tagged pointers, NaN-boxing, and NuN-boxing, have drawbacks. Tagged pointers entail a heap allocation of all float objects, and NaN/NuN-boxing puts additional run time costs on type checks and the handling of other objects.
  This paper introduces self-tagging, a new approach to object tagging that uses an invertible bitwise transformation to map floating-point numbers to tagged values that contain the correct type information at the correct position in their bit pattern, superimposing both their value and type information in a single machine word. Such a transformation can only map a subset of all floats to correctly typed tagged values, hence self-tagging takes advantage of the non-uniform distribution of floating point numbers used in practice to avoid heap allocation of the most frequently encountered floats.
  Variants of self-tagging were implemented in two distinct Scheme compilers and evaluated on four microarchitectures to assess their performance and compare them to tagged pointers, NaN-boxing, and NuN-boxing. Experiments demonstrate that, in practice, the approach eliminates heap allocation of nearly all floating-point numbers and provides good execution speed of float-intensive benchmarks in Scheme with a negligible performance impact on other benchmarks, making it an attractive alternative to tagged pointers, alongside NaN-boxing and NuN-boxing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16544v3</guid>
      <category>cs.PL</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olivier Melan\c{c}on, Manuel Serrano, Marc Feeley</dc:creator>
    </item>
    <item>
      <title>Place Capability Graphs: A General-Purpose Model of Rust's Ownership and Borrowing Guarantees</title>
      <link>https://arxiv.org/abs/2503.21691</link>
      <description>arXiv:2503.21691v4 Announce Type: replace 
Abstract: Rust's novel type system has proved an attractive target for verification and program analysis tools, due to the rich guarantees it provides for controlling aliasing and mutability. However, fully understanding, extracting and exploiting these guarantees is subtle and challenging: existing models for Rust's type checking either support a smaller idealised language disconnected from real-world Rust code, or come with severe limitations in terms of precise modelling of Rust borrows, composite types storing them, function signatures and loops.
  In this paper, we present a novel model of Rust's type-checking called Place Capability Graphs, which lifts these limitations, and which can be directly calculated from the Rust compiler's own programmatic representations and analyses. We demonstrate that our model supports over 97% of Rust functions in the most popular public crates, and show its suitability as a general-purpose basis for verification and program analysis tools by developing promising new prototype versions of the existing Flowistry and Prusti tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21691v4</guid>
      <category>cs.PL</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zachary Grannan, Aurel B\'il\'y, Jon\'a\v{s} Fiala, Jasper Geer, Markus de Medeiros, Peter M\"uller, Alexander J. Summers</dc:creator>
    </item>
    <item>
      <title>Cobblestone: Iterative Automation for Formal Verification</title>
      <link>https://arxiv.org/abs/2410.19940</link>
      <description>arXiv:2410.19940v2 Announce Type: replace-cross 
Abstract: Formal verification using proof assistants, such as Coq, is an effective way of improving software quality, but requires significant effort and expertise. Machine learning can automatically synthesize proofs, but such tools are able to prove only a fraction of desired software properties. We introduce Cobblestone, a divide-and-conquer approach for proof synthesis. Cobblestone uses a large language model (LLM) to generate potential proofs, uses those proofs to break the problem into simpler parts, automatically identifies which of those parts were successfully proven, and iterates on the remaining parts to build a correct proof that is guaranteed to be sound, despite the reliance on unsound LLMs. We evaluate Cobblestone on four benchmarks of open-source Coq projects, controlling for training data leakage. Fully automatically, Cobblestone outperforms state-of-the-art non-LLM tools, and proves many theorems that other LLM-based tools cannot, and on many benchmarks, outperforms them. Each Cobblestone run costs only $1.25 and takes 14.7 minutes, on average. Cobblestone can also be used with external input, from a user or another tool, providing a proof structure or relevant lemmas. Evaluated with such an oracle, Cobblestone proves up to 58% of theorems. Overall, our research shows that tools can make use of partial progress and external input to more effectively automate formal verification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19940v2</guid>
      <category>cs.LO</category>
      <category>cs.AI</category>
      <category>cs.PL</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saketh Ram Kasibatla, Arpan Agarwal, Yuriy Brun, Sorin Lerner, Talia Ringer, Emily First</dc:creator>
    </item>
    <item>
      <title>NaN-Propagation: A Novel Method for Sparsity Detection in Black-Box Computational Functions</title>
      <link>https://arxiv.org/abs/2507.23186</link>
      <description>arXiv:2507.23186v2 Announce Type: replace-cross 
Abstract: When numerically evaluating a function's gradient, sparsity detection can enable substantial computational speedups through Jacobian coloring and compression. However, sparsity detection techniques for black-box functions are limited, and existing finite-difference-based methods suffer from false negatives due to coincidental zero gradients. These false negatives can silently corrupt gradient calculations, leading to difficult-to-diagnose errors. We introduce NaN-propagation, which exploits the universal contamination property of IEEE 754 Not-a-Number values to trace input-output dependencies through floating-point numerical computations. By systematically contaminating inputs with NaN and observing which outputs become NaN, the method reconstructs conservative sparsity patterns that eliminate a major source of false negatives. We demonstrate this approach on an aerospace wing weight model, achieving a 1.52x speedup while uncovering dozens of dependencies missed by conventional methods -- a significant practical improvement since gradient computation is often the bottleneck in optimization workflows. The technique leverages IEEE 754 compliance to work across programming languages and math libraries without requiring modifications to existing black-box codes. Furthermore, advanced strategies such as NaN payload encoding via direct bit manipulation enable faster-than-linear time complexity, yielding speed improvements over existing black-box sparsity detection methods. Practical algorithms are also proposed to mitigate challenges from branching code execution common in engineering applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23186v2</guid>
      <category>cs.LG</category>
      <category>cs.PL</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Sharpe</dc:creator>
    </item>
  </channel>
</rss>
