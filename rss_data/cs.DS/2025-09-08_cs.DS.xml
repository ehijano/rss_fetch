<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Sep 2025 02:23:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Transitivity Preserving Projection in Directed Hypergraphs</title>
      <link>https://arxiv.org/abs/2509.04543</link>
      <description>arXiv:2509.04543v1 Announce Type: new 
Abstract: Directed hypergraphs are vital for modeling complex polyadic relationships in domains such as discrete mathematics, computer science, network security, and systems modeling. However, their inherent complexity often impedes effective visualization and analysis, particularly for large graphs. This paper introduces a novel Transitivity Preserving Projection (TPP) to address the limitations of the computationally intensive Basu and Blanning projection (BBP), which can paradoxically increase complexity by flattening transitive relationships. TPP offers a minimal and complete representation of relationships within a chosen subset of elements, capturing only irreducible dominant metapaths to ensure the smallest set of edges while preserving all essential transitive and direct connections. This approach significantly enhances visualization by reducing edge proliferation and maintains the integrity of the original hypergraph's structure. We develop an efficient algorithm leveraging the set-trie data structure, reducing the computational complexity from an exponential number of metapath searches in BBP to a linear number of metapath searches with polynomial-time filtering, enabling scalability for real-world applications. Experimental results demonstrate TPP's superior performance, completing projections in seconds on graphs where BBP fails to terminate within 24 hours. By providing a minimal yet complete view of relationships, TPP supports applications in network security and supply</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04543v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Parsonage, Matthew Roughan, Hung X Nguyen</dc:creator>
    </item>
    <item>
      <title>Additive, Near-Additive, and Multiplicative Approximations for APSP in Weighted Undirected Graphs: Trade-offs and Algorithms</title>
      <link>https://arxiv.org/abs/2509.04640</link>
      <description>arXiv:2509.04640v1 Announce Type: new 
Abstract: We present a $+2\sum_{i=1}^{k+1}{W_i}$-APASP algorithm for dense weighted graphs with runtime $\tilde O\left(n^{2+\frac{1}{3k+2}}\right)$, where $W_{i}$ is the weight of an $i^\textnormal{th}$ heaviest edge on a shortest path. Dor, Halperin and Zwick [FOCS'96, SICOMP'00] had two algorithms for the commensurate unweighted $+2\cdot\left( k+1\right)$-APASP: $\tilde O\left(n^{2-\frac{1}{k+2}}m^{\frac{1}{k+2}}\right)$ runtime for sparse graphs and $\tilde O\left(n^{2+\frac{1}{3k+2}}\right)$ runtime for dense graphs. Cohen and Zwick [SODA'97, JALG'01] adapted the sparse variant to weighted graphs: $+2\sum_{i=1}^{k+1}{W_i}$-APASP algorithm in the same runtime. We show an algorithm for dense weighted graphs.
  For \emph{nearly additive} APASP, we present a $\left(1+\varepsilon,\min{\left\{2W_1,4W_{2}\right\}}\right)$-APASP algorithm with $\tilde O\left(\left(\frac{1}{\varepsilon}\right)^{O\left(1\right)}\cdot n^{2.15135313}\cdot\log W\right)$ runtime. This improves the $\left(1+\varepsilon,2W_1\right)$-APASP of Saha and Ye [SODA'24].
  For multiplicative APASP, we show a framework of $\left(\frac{3\ell +4}{\ell + 2}+\varepsilon\right)$-APASP algorithms, reducing the runtime of Akav and Roditty [ESA'21] for dense graphs and generalizing the $\left(2+\varepsilon\right)$-APASP algorithm of Dory et al [SODA'24]. Our base case is a $\left(\frac{7}{3}+\varepsilon\right)$-APASP in $\tilde O\left(\left(\frac{1}{\varepsilon}\right)^{O\left(1\right)}\cdot n^{2.15135313}\cdot \log W\right)$ runtime, improving the $\frac{7}{3}$-APASP algorithm of Baswana and Kavitha [FOCS'06, SICOMP'10] for dense graphs.
  Finally, we "bypass" an $\tilde \Omega \left(n^\omega\right)$ conditional lower bound by Dor, Halperin, and Zwick for $\alpha$-APASP with $\alpha &lt; 2$, by allowing an additive term (e.g. $\paren{\frac{6k+3}{3k+2},\sum_{i=1}^{k+1}{W_{i}}}$-APASP in $\tilde O\left(n^{2+\frac{1}{3k+2}}\right)$ runtime.).</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04640v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liam Roditty, Ariel Sapir</dc:creator>
    </item>
    <item>
      <title>A 13/6-Approximation for Strip Packing via the Bottom-Left Algorithm</title>
      <link>https://arxiv.org/abs/2509.04654</link>
      <description>arXiv:2509.04654v1 Announce Type: new 
Abstract: In the Strip Packing problem, we are given a vertical strip of fixed width and unbounded height, along with a set of axis-parallel rectangles. The task is to place all rectangles within the strip, without overlaps, while minimizing the height of the packing. This problem is known to be NP-hard. The Bottom-Left Algorithm is a simple and widely used heuristic for Strip Packing. Given a fixed order of the rectangles, it places them one by one, always choosing the lowest feasible position in the strip and, in case of ties, the leftmost one. Baker, Coffman, and Rivest proved in 1980 that the Bottom-Left Algorithm has approximation ratio 3 if the rectangles are sorted by decreasing width. For the past 45 years, no alternative ordering has been found that improves this bound. We introduce a new rectangle ordering and show that with this ordering the Bottom-Left Algorithm achieves a 13/6 approximation for the Strip Packing problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04654v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Stefan Hougardy, Bart Zondervan</dc:creator>
    </item>
    <item>
      <title>Parameterized Approximability for Modular Linear Equations</title>
      <link>https://arxiv.org/abs/2509.04976</link>
      <description>arXiv:2509.04976v1 Announce Type: new 
Abstract: We consider the Min-$r$-Lin$(Z_m)$ problem: given a system $S$ of length-$r$ linear equations modulo $m$, find $Z \subseteq S$ of minimum cardinality such that $S-Z$ is satisfiable. The problem is NP-hard and UGC-hard to approximate in polynomial time within any constant factor even when $r = m = 2$. We focus on parameterized approximation with solution size as the parameter. Dabrowski et al. showed that Min-$2$-Lin$(Z_m)$ is in FPT if $m$ is prime (i.e. $Z_m$ is a field), and it is W[1]-hard if $m$ is not a prime power. We show that Min-$2$-Lin$(Z_{p^n})$ is FPT-approximable within a factor of $2$ for every prime $p$ and integer $n \geq 2$. This implies that Min-$2$-Lin$(Z_m)$, $m \in Z^+$, is FPT-approximable within a factor of $2\omega(m)$ where $\omega(m)$ counts the number of distinct prime divisors of $m$. The idea behind the algorithm is to solve ever tighter relaxations of the problem, decreasing the set of possible values for the variables at each step. Working over $Z_{p^n}$ and viewing the values in base-$p$, one can roughly think of a relaxation as fixing the number of trailing zeros and the least significant nonzero digits of the values assigned to the variables. To solve the relaxed problem, we construct a certain graph where solutions can be identified with a particular collection of cuts. The relaxation may hide obstructions that will only become visible in the next iteration of the algorithm, which makes it difficult to find optimal solutions. To deal with this, we use a strategy based on shadow removal to compute solutions that (1) cost at most twice as much as the optimum and (2) allow us to reduce the set of values for all variables simultaneously. We complement the algorithmic result with two lower bounds, ruling out constant-factor FPT-approximation for Min-$3$-Lin$(R)$ over any nontrivial ring $R$ and for Min-$2$-Lin$(R)$ over some finite commutative rings $R$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04976v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Konrad K. Dabrowski, Peter Jonsson, Sebastian Ordyniak, George Osipov, Magnus Wahlstr\"om</dc:creator>
    </item>
    <item>
      <title>Graph Reconstruction with a Connected Components Oracle</title>
      <link>https://arxiv.org/abs/2509.05002</link>
      <description>arXiv:2509.05002v1 Announce Type: new 
Abstract: In the Graph Reconstruction (GR) problem, the goal is to recover a hidden graph by utilizing some oracle that provides limited access to the structure of the graph. The interest is in characterizing how strong different oracles are when the complexity of an algorithm is measured in the number of performed queries. We study a novel oracle that returns the set of connected components (CC) on the subgraph induced by the queried subset of vertices. Our main contributions are as follows:
  1. For a hidden graph with $n$ vertices, $m$ edges, maximum degree $\Delta$, and treewidth $k$, GR can be solved in $O(\min\{m, \Delta^2, k^2\} \cdot \log n)$ CC queries by an adaptive randomized algorithm.
  2. For a hidden graph with $n$ vertices, $m$ edges, maximum degree $\Delta$, and treewidth $k$, no algorithm can solve GR in $o(\min\{m, \Delta^2, k^2\})$ CC queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05002v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juha Harviainen, Pekka Parviainen</dc:creator>
    </item>
    <item>
      <title>On approximating the $f$-divergence between two Ising models</title>
      <link>https://arxiv.org/abs/2509.05016</link>
      <description>arXiv:2509.05016v1 Announce Type: new 
Abstract: The $f$-divergence is a fundamental notion that measures the difference between two distributions. In this paper, we study the problem of approximating the $f$-divergence between two Ising models, which is a generalization of recent work on approximating the TV-distance. Given two Ising models $\nu$ and $\mu$, which are specified by their interaction matrices and external fields, the problem is to approximate the $f$-divergence $D_f(\nu\,\|\,\mu)$ within an arbitrary relative error $\mathrm{e}^{\pm \varepsilon}$. For $\chi^\alpha$-divergence with a constant integer $\alpha$, we establish both algorithmic and hardness results. The algorithm works in a parameter regime that matches the hardness result. Our algorithm can be extended to other $f$-divergences such as $\alpha$-divergence, Kullback-Leibler divergence, R\'enyi divergence, Jensen-Shannon divergence, and squared Hellinger distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05016v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weiming Feng, Yucheng Fu</dc:creator>
    </item>
    <item>
      <title>Testing Depth First Search Numbering</title>
      <link>https://arxiv.org/abs/2509.05132</link>
      <description>arXiv:2509.05132v1 Announce Type: new 
Abstract: Property Testing is a formal framework to study the computational power and complexity of sampling from combinatorial objects. A central goal in standard graph property testing is to understand which graph properties are testable with sublinear query complexity. Here, a graph property P is testable with a sublinear query complexity if there is an algorithm that makes a sublinear number of queries to the input graph and accepts with probability at least 2/3, if the graph has property P, and rejects with probability at least 2/3 if it is $\varepsilon$-far from every graph that has property P.
  In this paper, we introduce a new variant of the bounded degree graph model. In this variant, in addition to the standard representation of a bounded degree graph, we assume that every vertex $v$ has a unique label num$(v)$ from $\{1, \dots, |V|\}$, and in addition to the standard queries in the bounded degree graph model, we also allow a property testing algorithm to query for the label of a vertex (but not for a vertex with a given label).
  Our new model is motivated by certain graph processes such as a DFS traversal, which assign consecutive numbers (labels) to the vertices of the graph. We want to study which of these numberings can be tested in sublinear time. As a first step in understanding such a model, we develop a \emph{property testing algorithm for discovery times of a DFS traversal} with query complexity $O(n^{1/3}/\varepsilon)$ and for constant $\varepsilon&gt;0$ we give a matching lower bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05132v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Artur Czumaj, Christian Sohler, Stefan Walzer</dc:creator>
    </item>
    <item>
      <title>Efficient Contractions of Dynamic Graphs -- with Applications</title>
      <link>https://arxiv.org/abs/2509.05157</link>
      <description>arXiv:2509.05157v1 Announce Type: new 
Abstract: A non-trivial minimum cut (NMC) sparsifier is a multigraph $\hat{G}$ that preserves all non-trivial minimum cuts of a given undirected graph $G$. We introduce a flexible data structure for fully dynamic graphs that can efficiently provide an NMC sparsifier upon request at any point during the sequence of updates. We employ simple dynamic forest data structures to achieve a fast from-scratch construction of the sparsifier at query time. Based on the strength of the adversary and desired type of time bounds, the data structure comes with different guarantees. Specifically, let $G$ be a fully dynamic simple graph with $n$ vertices and minimum degree $\delta$. Then our data structure supports an insertion/deletion of an edge to/from $G$ in $n^{o(1)}$ worst-case time. Furthermore, upon request, it can return w.h.p. an NMC sparsifier of $G$ that has $O(n/\delta)$ vertices and $O(n)$ edges, in $\hat{O}(n)$ time. The probabilistic guarantees hold against an adaptive adversary. Alternatively, the update and query times can be improved to $\tilde{O}(1)$ and $\tilde{O}(n)$ respectively, if amortized-time guarantees are sufficient, or if the adversary is oblivious.
  We discuss two applications of our data structure. First, it can be used to efficiently report a cactus representation of all minimum cuts of a fully dynamic simple graph. Using the NMC sparsifier we can w.h.p. build this cactus in worst-case time $\hat{O}(n)$ against an adaptive adversary. Second, our data structure allows us to efficiently compute the maximal $k$-edge-connected subgraphs of undirected simple graphs, by repeatedly applying a minimum cut algorithm on the NMC sparsifier. Specifically, we can compute w.h.p. the maximal $k$-edge-connected subgraphs of a simple graph with $n$ vertices and $m$ edges in $\tilde{O}(m+n^2/k)$ time which is an improvement for $k = \Omega(n^{1/8})$ and works for fully dynamic graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05157v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Monika Henzinger, Evangelos Kosinas, Robin M\"unk, Harald R\"acke</dc:creator>
    </item>
    <item>
      <title>List Decoding Expander-Based Codes via Fast Approximation of Expanding CSPs: I</title>
      <link>https://arxiv.org/abs/2509.05203</link>
      <description>arXiv:2509.05203v1 Announce Type: new 
Abstract: We present near-linear time list decoding algorithms (in the block-length $n$) for expander-based code constructions. More precisely, we show that
  (i) For every $\delta \in (0,1)$ and $\epsilon &gt; 0$, there is an explicit family of good Tanner LDPC codes of (design) distance $\delta$ that is $(\delta - \epsilon, O_\varepsilon(1))$ list decodable in time $\widetilde{\mathcal{O}}_{\varepsilon}(n)$ with alphabet size $O_\delta(1)$,
  (ii) For every $R \in (0,1)$ and $\epsilon &gt; 0$, there is an explicit family of AEL codes of rate $R$, distance $1-R -\varepsilon$ that is $(1-R-\epsilon, O_\varepsilon(1))$ list decodable in time $\widetilde{\mathcal{O}}_{\varepsilon}(n)$ with alphabet size $\text{exp}(\text{poly}(1/\epsilon))$, and
  (iii) For every $R \in (0,1)$ and $\epsilon &gt; 0$, there is an explicit family of AEL codes of rate $R$, distance $1-R-\varepsilon$ that is $(1-R-\epsilon, O(1/\epsilon))$ list decodable in time $\widetilde{\mathcal{O}}_{\varepsilon}(n)$ with alphabet size $\text{exp}(\text{exp}(\text{poly}(1/\epsilon)))$ using recent near-optimal list size bounds from [JMST25].
  Our results are obtained by phrasing the decoding task as an agreement CSP [RWZ20,DHKNT19] on expander graphs and using the fast approximation algorithm for $q$-ary expanding CSPs from [Jer23], which is based on weak regularity decomposition [JST21,FK96]. Similarly to list decoding $q$-ary Ta-Shma's codes in [Jer23], we show that it suffices to enumerate over assignments that are constant in each part (of the constantly many) of the decomposition in order to recover all codewords in the list.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05203v1</guid>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fernando Granha Jeronimo, Aman Singh</dc:creator>
    </item>
    <item>
      <title>Optimal Variance and Covariance Estimation under Differential Privacy in the Add-Remove Model and Beyond</title>
      <link>https://arxiv.org/abs/2509.04919</link>
      <description>arXiv:2509.04919v1 Announce Type: cross 
Abstract: In this paper, we study the problem of estimating the variance and covariance of datasets under differential privacy in the add-remove model. While estimation in the swap model has been extensively studied in the literature, the add-remove model remains less explored and more challenging, as the dataset size must also be kept private. To address this issue, we develop efficient mechanisms for variance and covariance estimation based on the \emph{B\'{e}zier mechanism}, a novel moment-release framework that leverages Bernstein bases. We prove that our proposed mechanisms are minimax optimal in the high-privacy regime by establishing new minimax lower bounds. Moreover, beyond worst-case scenarios, we analyze instance-wise utility and show that the B\'{e}zier-based estimator consistently achieves better utility compared to alternative mechanisms. Finally, we demonstrate the effectiveness of the B\'{e}zier mechanism beyond variance and covariance estimation, showcasing its applicability to other statistical tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04919v1</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shokichi Takakura, Seng Pei Liew, Satoshi Hasegawa</dc:creator>
    </item>
    <item>
      <title>Capturing an Invisible Robber using Separators</title>
      <link>https://arxiv.org/abs/2509.05024</link>
      <description>arXiv:2509.05024v1 Announce Type: cross 
Abstract: We study the zero-visibility cops and robbers game, where the robber is invisible to the cops until they are caught. This differs from the classic game where full information about the robber's location is known at any time. A previously known solution for capturing a robber in the zero-visibility case is based on the pathwidth decomposition. We provide an alternative solution based on a separation hierarchy, improving capture time and space complexity without asymptotically increasing the zero-visibility cop number in most cases. In addition, we provide a better bound on the approximate zero-visibility cop number for various classes of graphs, where approximate refers to the restriction to polynomial time computable strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05024v1</guid>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Igor Potapov, Tymofii Prokopenko, John Sylvester</dc:creator>
    </item>
    <item>
      <title>Improved Bounds for Twin-Width Parameter Variants with Algorithmic Applications to Counting Graph Colorings</title>
      <link>https://arxiv.org/abs/2509.05122</link>
      <description>arXiv:2509.05122v1 Announce Type: cross 
Abstract: The $H$-Coloring problem is a well-known generalization of the classical NP-complete problem $k$-Coloring where the task is to determine whether an input graph admits a homomorphism to the template graph $H$. This problem has been the subject of intense theoretical research and in this article we study the complexity of $H$-Coloring with respect to the parameters clique-width and the more recent component twin-width, which describe desirable computational properties of graphs. We give two surprising linear bounds between these parameters, thus improving the previously known exponential and double exponential bounds. Our constructive proof naturally extends to related parameters and as a showcase we prove that total twin-width and linear clique-width can be related via a tight quadratic bound. These bounds naturally lead to algorithmic applications. The linear bounds between component twin-width and clique-width entail natural approximations of component twin-width, by making use of the results known for clique-width. As for computational aspects of graph coloring, we target the richer problem of counting the number of homomorphisms to $H$ (#$H$-Coloring). The first algorithm that we propose uses a contraction sequence of the input graph $G$ parameterized by the component twin-width of $G$. This leads to a positive FPT result for the counting version. The second uses a contraction sequence of the template graph $H$ and here we instead measure the complexity with respect to the number of vertices in the input graph. Using our linear bounds we show that our algorithms are always at least as fast as the previously best #$H$-Coloring algorithms (based on clique-width) and for several interesting classes of graphs (e.g., cographs, cycles of length $\ge 7$, or distance-hereditary graphs) are in fact strictly faster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05122v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ambroise Baril, Miguel Couceiro, Victor Lagerkvist</dc:creator>
    </item>
    <item>
      <title>Efficient Exact Resistance Distance Computation on Small-Treewidth Graphs: a Labelling Approach</title>
      <link>https://arxiv.org/abs/2509.05129</link>
      <description>arXiv:2509.05129v1 Announce Type: cross 
Abstract: Resistance distance computation is a fundamental problem in graph analysis, yet existing random walk-based methods are limited to approximate solutions and suffer from poor efficiency on small-treewidth graphs (e.g., road networks). In contrast, shortest-path distance computation achieves remarkable efficiency on such graphs by leveraging cut properties and tree decompositions. Motivated by this disparity, we first analyze the cut property of resistance distance. While a direct generalization proves impractical due to costly matrix operations, we overcome this limitation by integrating tree decompositions, revealing that the resistance distance $r(s,t)$ depends only on labels along the paths from $s$ and $t$ to the root of the decomposition. This insight enables compact labelling structures. Based on this, we propose \treeindex, a novel index method that constructs a resistance distance labelling of size $O(n \cdot h_{\mathcal{G}})$ in $O(n \cdot h_{\mathcal{G}}^2 \cdot d_{\max})$ time, where $h_{\mathcal{G}}$ (tree height) and $d_{\max}$ (maximum degree) behave as small constants in many real-world small-treewidth graphs (e.g., road networks). Our labelling supports exact single-pair queries in $O(h_{\mathcal{G}})$ time and single-source queries in $O(n \cdot h_{\mathcal{G}})$ time. Extensive experiments show that TreeIndex substantially outperforms state-of-the-art approaches. For instance, on the full USA road network, it constructs a $405$ GB labelling in $7$ hours (single-threaded) and answers exact single-pair queries in $10^{-3}$ seconds and single-source queries in $190$ seconds--the first exact method scalable to such large graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05129v1</guid>
      <category>cs.DB</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meihao Liao, Yueyang Pan, Rong-Hua Li, Guoren Wang</dc:creator>
    </item>
    <item>
      <title>Vertex-ordering and arc-partitioning problems</title>
      <link>https://arxiv.org/abs/2509.05245</link>
      <description>arXiv:2509.05245v1 Announce Type: cross 
Abstract: We study vertex-ordering problems in loop-free digraphs subject to constraints on the left-going arcs, focusing on existence conditions and computational complexity. As an intriguing special case, we explore vertex-specific lower and upper bounds on the left-outdegrees and right-indegrees. We show, for example, that deciding whether the left-going arcs can form an in-branching is solvable in polynomial time and provide a necessary and sufficient condition, while the analogous problem for an in-arborescence turns out to be NP-complete. We also consider a weighted variant that enforces vertex-specific lower and upper bounds on the weighted left-outdegrees, which is particularly relevant in applications. Furthermore, we investigate the connection between ordering problems and their arc-partitioning counterparts, where one seeks to partition the arcs into a subgraph from a specific digraph family and an acyclic subgraph -- equivalently, one seeks to cover all directed cycles with a subgraph belonging to a specific family. For the family of in-branchings, unions of disjoint dipaths, and matchings, the two formulations coincide, whereas for in-arborescences, dipaths, Hamiltonian dipaths, and perfect matchings the formulations diverge. Our results yield a comprehensive complexity landscape, unify diverse special cases and variants, clarify the algorithmic boundaries of ordered digraphs, and relate them to broader topics including graph degeneracy, acyclic orientations, influence propagation, and rank aggregation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05245v1</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>N\'ora A. Borsik, P\'eter Madarasi</dc:creator>
    </item>
    <item>
      <title>Prophet Inequalities over Time</title>
      <link>https://arxiv.org/abs/2211.10471</link>
      <description>arXiv:2211.10471v2 Announce Type: replace 
Abstract: In this paper, we introduce an over-time variant of the well-known prophet inequality with i.i.d. random variables. Instead of stopping with one realized value at some point in the process, we decide for each step how long we select the value. Then we cannot select another value until this period is over. The goal is to maximize the expectation of the sum of selected values. We describe the structure of the optimal stopping rule and give upper and lower bounds on the prophet inequality. In online algorithms terminology, this corresponds to bounds on the competitive ratio of an online algorithm.
  We give a surprisingly simple algorithm with a single threshold that results in a prophet inequality of $\approx 0.396$ for all input lengths $n$. Additionally, as our main result, we present a more advanced algorithm resulting in a prophet inequality of $\approx 0.598$ when the number of steps tends to infinity. We complement our results by an upper bound that shows that the best possible prophet inequality is at most $1/\varphi \approx 0.618$, where $\varphi$ denotes the golden ratio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.10471v2</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3766547</arxiv:DOI>
      <dc:creator>Andreas Abels, Elias Pitschmann, Daniel Schmand</dc:creator>
    </item>
    <item>
      <title>Multi-Pass Streaming Lower Bounds for Approximating Max-Cut</title>
      <link>https://arxiv.org/abs/2503.23404</link>
      <description>arXiv:2503.23404v2 Announce Type: replace 
Abstract: In the Max-Cut problem in the streaming model, an algorithm is given the edges of an unknown graph $G = (V,E)$ in some fixed order, and its goal is to approximate the size of the largest cut in $G$. Improving upon an earlier result of Kapralov, Khanna and Sudan, it was shown by Kapralov and Krachun that for all $\varepsilon&gt;0$, no $o(n)$ memory streaming algorithm can achieve a $(1/2+\varepsilon)$-approximation for Max-Cut. Their result holds for single-pass streams, i.e.~the setting in which the algorithm only views the stream once, and it was open whether multi-pass access may help. The state-of-the-art result along these lines, due to Assadi and N, rules out arbitrarily good approximation algorithms with constantly many passes and $n^{1-\delta}$ space for any $\delta&gt;0$.
  We improve upon this state-of-the-art result, showing that any non-trivial approximation algorithm for Max-Cut requires either polynomially many passes or polynomially large space. More specifically, we show that for all $\varepsilon&gt;0$, a $k$-pass streaming $(1/2+\varepsilon)$-approximation algorithm for Max-Cut requires $\Omega_{\varepsilon}\left(n^{1/3}/k\right)$ space. This result leads to a similar lower bound for the Maximum Directed Cut problem, showing the near optimality of the algorithm of [Saxena, Singer, Sudan, Velusamy, SODA 2025].
  Our lower bounds proceed by showing a communication complexity lower bound for the Distributional Implicit Hidden Partition (DIHP) Problem, introduced by Kapralov and Krachun. While a naive application of the discrepancy method fails, we identify a property of protocols called ``globalness'', and show that (1) any protocol for DIHP can be turned into a global protocol, (2) the discrepancy of a global protocol must be small. The second step is the more technically involved step in the argument, and therein we use global hypercontractive inequalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23404v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yumou Fei, Dor Minzer, Shuo Wang</dc:creator>
    </item>
    <item>
      <title>Optimizing Districting Plans to Maximize Majority-Minority Districts via IPs and Local Search</title>
      <link>https://arxiv.org/abs/2508.07446</link>
      <description>arXiv:2508.07446v2 Announce Type: replace 
Abstract: In redistricting litigation, effective enforcement of the Voting Rights Act has often involved providing the court with districting plans that display a larger number of majority-minority districts than the current proposal (as was true, for example, in what followed Allen v. Milligan concerning the congressional districting plan for Alabama in 2023). Recent work by Cannon et al. proposed a heuristic algorithm for generating plans to optimize majority-minority districts, which they called short bursts; that algorithm relies on a sophisticated random walk over the space of all plans, transitioning in bursts, where the initial plan for each burst is the most successful plan from the previous burst. We propose a method based on integer programming, where we build upon another previous work, the stochastic hierarchical partitioning algorithm, which heuristically generates a robust set of potential districts (viewed as columns in a standard set partitioning formulation); that approach was designed to optimize a different notion of fairness across a statewide plan. We design a new column generation algorithm to find plans via integer programming that outperforms short bursts on multiple data sets in generating statewide plans with significantly more majority-minority districts. These results also rely on a new local re-optimization algorithm to iteratively improve on any baseline solution, as well as an algorithm to increase the compactness of districts in plans generated (without impacting the number of majority-minority districts).</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07446v2</guid>
      <category>cs.DS</category>
      <category>cs.CY</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1137/1.9781611978759.20</arxiv:DOI>
      <arxiv:journal_reference>in 2025 Proceedings of the Conference on Applied and Computational Discrete Algorithms (ACDA), pp. 264-275</arxiv:journal_reference>
      <dc:creator>Daniel Brous, David Shmoys</dc:creator>
    </item>
    <item>
      <title>Quality control in sublinear time: a case study via random graphs</title>
      <link>https://arxiv.org/abs/2508.16531</link>
      <description>arXiv:2508.16531v2 Announce Type: replace 
Abstract: Many algorithms are designed to work well on average over inputs. When running such an algorithm on an arbitrary input, we must ask: Can we trust the algorithm on this input? We identify a new class of algorithmic problems addressing this, which we call "Quality Control Problems." These problems are specified by a (positive, real-valued) "quality function" $\rho$ and a distribution $D$ such that, with high probability, a sample drawn from $D$ is "high quality," meaning its $\rho$-value is near $1$. The goal is to accept inputs $x \sim D$ and reject potentially adversarially generated inputs $x$ with $\rho(x)$ far from $1$. The objective of quality control is thus weaker than either component problem: testing for "$\rho(x) \approx 1$" or testing if $x \sim D$, and offers the possibility of more efficient algorithms.
  In this work, we consider the sublinear version of the quality control problem, where $D \in \Delta(\{0,1\}^N)$ and the goal is to solve the $(D ,\rho)$-quality problem with $o(N)$ queries and time. As a case study, we consider random graphs, i.e., $D = G_{n,p}$ (and $N = \binom{n}2$), and the $k$-clique count function $\rho_k := C_k(G)/\mathbb{E}_{G' \sim G_{n,p}}[C_k(G')]$, where $C_k(G)$ is the number of $k$-cliques in $G$. Testing if $G \sim G_{n,p}$ with one sample, let alone with sublinear query access to the sample, is of course impossible. Testing if $\rho_k(G)\approx 1$ requires $p^{-\Omega(k^2)}$ samples. In contrast, we show that the quality control problem for $G_{n,p}$ (with $n \geq p^{-ck}$ for some constant $c$) with respect to $\rho_k$ can be tested with $p^{-O(k)}$ queries and time, showing quality control is provably superpolynomially more efficient in this setting. More generally, for a motif $H$ of maximum degree $\Delta(H)$, the respective quality control problem can be solved with $p^{-O(\Delta(H))}$ queries and running time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16531v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.CO</category>
      <category>math.PR</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cassandra Marcussen, Ronitt Rubinfeld, Madhu Sudan</dc:creator>
    </item>
    <item>
      <title>Classical optimization with imaginary time block encoding on quantum computers: The MaxCut problem</title>
      <link>https://arxiv.org/abs/2411.10737</link>
      <description>arXiv:2411.10737v2 Announce Type: replace-cross 
Abstract: Optimization problems in finance, physics and computer science are typically very hard to tackle in classical computing and quantum computing could help speed up computations and provide efficient methods for tackling large problems. Typically, to treat the problem with a quantum computer, the optimal solution is cast as the ground state of a diagonal Hamiltonian. We develop a new method, called ITE-BE, based on a recent imaginary time algorithm, which requires no variational parameter optimization as all parameters can be derived analytically from the target Hamiltonian. We also demonstrate that our method can be successfully combined with other quantum algorithms such as quantum approximate optimization algorithm (QAOA). For illustration, here we study the MaxCut problem. We find that the QAOA ansatz increases the post-selection success of ITE-BE, and shallow QAOA circuits, when boosted with ITE-BE, achieve better performance than deeper QAOA circuits. For the special case of the transverse initial state, we adapt our block encoding scheme to allow for a deterministic application of the first layer of the circuit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10737v2</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>cs.ET</category>
      <category>math.OC</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dawei Zhong, Akhil Francis, Ermal Rrapaj</dc:creator>
    </item>
    <item>
      <title>The Broader Landscape of Robustness in Algorithmic Statistics</title>
      <link>https://arxiv.org/abs/2412.02670</link>
      <description>arXiv:2412.02670v3 Announce Type: replace-cross 
Abstract: The last decade has seen a number of advances in computationally efficient algorithms for statistical methods subject to robustness constraints. An estimator may be robust in a number of different ways: to contamination of the dataset, to heavy-tailed data, or in the sense that it preserves privacy of the dataset. We survey recent results in these areas with a focus on the problem of mean estimation, drawing technical and conceptual connections between the various forms of robustness, showing that the same underlying algorithmic ideas lead to computationally efficient estimators in all these settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02670v3</guid>
      <category>stat.ML</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gautam Kamath</dc:creator>
    </item>
  </channel>
</rss>
