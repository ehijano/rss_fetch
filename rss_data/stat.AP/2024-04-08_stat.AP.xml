<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Apr 2024 04:00:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Evaluating COVID-19 Surveillance Testing Strategies at Colorado School of Mines: A Stochastic Modeling Approach</title>
      <link>https://arxiv.org/abs/2404.04417</link>
      <description>arXiv:2404.04417v1 Announce Type: new 
Abstract: This study introduces a stochastic model of COVID-19 transmission tailored to the Colorado School of Mines campus and evaluates surveillance testing strategies within a university context. Enhancing the conventional SEIR framework with stochastic transitions, our model accounts for the unique characteristics of disease spread in a residential college, including specific states for testing, quarantine, and isolation. Employing an approximate Bayesian computation (ABC) method for parameter estimation, we navigate the complexities inherent in stochastic models, enabling an accurate fit of the model with the campus case data. We then studied our model under the estimated parameters to evaluate the efficacy of different testing policies that could be implemented on a university campus. This framework not only advances understanding of COVID-19 dynamics on the Mines campus but serves as a blueprint for comparable settings, providing insights for informed strategies against infectious diseases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04417v1</guid>
      <category>stat.AP</category>
      <category>math.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Albrecht (Colorado School of Mines), Karin Leiderman (University of North Carolina at Chapel Hill), Suzanne Sindi (University of California, Merced), Douglas Nychka (Colorado School of Mines)</dc:creator>
    </item>
    <item>
      <title>Spatial estimation of virus infection propensity in hosts determined from GPS-based space-time locations</title>
      <link>https://arxiv.org/abs/2404.04455</link>
      <description>arXiv:2404.04455v1 Announce Type: new 
Abstract: Identifying areas in a landscape where individuals have a higher probability of becoming infected with a pathogen is a crucial step towards disease management. Our study data consists of GPS-based tracks of individual white-tailed deer (\textit{Odocoileus virginianus}) and three exotic Cervid species moving freely in a 172-ha high-fenced game preserve over given time periods. A serological test was performed on each individual to measure the antibody concentration of epizootic hemorrhagic disease virus (EHDV) for each of three serotypes (EHDV-1, -2, and -6) at the beginning and at the end of each tracking period. EHDV is a vector-borne viral disease indirectly transmitted between ruminant hosts by biting midges (\textit{Culicoides} spp.). The purpose of this study is to estimate the spatial distribution of infection propensity by performing an epidemiological tomography of a region using tracers. We model the data as a binomial linear inverse problem, where spatial coherence is enforced with a total variation regularization. The smoothness of the reconstructed propensity map is selected by the quantile universal threshold, which can also test the null hypothesis that the propensity map is spatially constant. We apply our method to simulated and real data, showing good statistical properties during simulations and consistent results and interpretations compared to intensive field estimations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04455v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jairo Diaz-Rodriguez, Juan Pablo Gomez, Jeremy P. Orange, Samantha M. Wisely, Jason K. Blackburn, Sylvain Sardy</dc:creator>
    </item>
    <item>
      <title>Application of the Modular Bayesian Approach for Inverse Uncertainty Quantification in Nuclear Thermal-Hydraulics Systems</title>
      <link>https://arxiv.org/abs/2404.04774</link>
      <description>arXiv:2404.04774v1 Announce Type: new 
Abstract: In the framework of BEPU (Best Estimate plus Uncertainty) methodology, the uncertainties involved in the simulations must be quantified to prove that the investigated design is acceptable. The output uncertainties are usually calculated by propagating input uncertainties through the simulation model, which requires knowledge of the model input uncertainties. However, in some best-estimate Thermal-Hydraulics (TH) codes such as TRACE, the physical model parameters used in empirical correlations may have large uncertainties, which are unknown to the code users. Therefore, obtaining uncertainty distributions of those parameters becomes crucial if we want to study the predictive uncertainty or output sensitivity.
  In this study, we present a Modular Bayesian approach that considers the presence model discrepancy during Bayesian calibration. Several TRACE physical model parameters are selected as calibration parameters in this work. Model discrepancy, also referred to as model inadequacy or model bias, accounts for the inaccuracy in computer simulation caused by underlying missing/insufficient physics, numerical approximation errors, and other errors of a computer code, even if all its parameters are fixed at their "true" values. Model discrepancy always exists in computer models because they are reduced representations of the reality. The consideration of model discrepancy is important because it can help avoid the "overfitting" problem in Bayesian calibration. This paper uses a set of steady-state experimental data from PSBT benchmark and it mainly aims at: (1) quantifying the uncertainties of TRACE physical model parameters based on experiment data; (2) quantifying the uncertainties in TRACE outputs based on inversely quantified physical model parameters uncertainties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04774v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chen Wang</dc:creator>
    </item>
    <item>
      <title>The zero degree of freedom non-central chi squared distribution for ensemble postprocessing</title>
      <link>https://arxiv.org/abs/2404.04964</link>
      <description>arXiv:2404.04964v1 Announce Type: new 
Abstract: In this note the use of the zero degree non-central chi squared distribution as predictive distribution for ensemble postprocessing is investigated. It has a point mass at zero by definition, and is thus particularly suited for postprocessing weather variables naturally exhibiting large numbers of zeros, such as precipitation, solar radiation or lightnings. Due to the properties of the distribution no additional truncation or censoring is required to obtain a positive probability at zero. The presented study investigates its performance compared to that of the censored generalized extreme value distribution and the censored and shifted gamma distribution for postprocessing 24h accumulated precipitation using an EMOS (ensemble model output statistics) approach with a rolling training period. The obtained results support the conclusion that it serves well as a predictive distribution in postprocessing precipitation and thus may also be considered in future analyses of other weather variables having substantial zero observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04964v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J\"urgen Gro{\ss}, Annette M\"oller</dc:creator>
    </item>
    <item>
      <title>Data Conditioning for Subsurface Models with Single-Image Generative Adversarial Network (SinGAN)</title>
      <link>https://arxiv.org/abs/2404.05068</link>
      <description>arXiv:2404.05068v1 Announce Type: new 
Abstract: The characterization of subsurface models relies on the accuracy of subsurface models which request integrating a large number of information across different sources through model conditioning, such as data conditioning and geological concepts conditioning. Conventional geostatistical models have a trade-off between honoring geological conditioning (i.e., qualitative geological concepts) and data conditioning (i.e., quantitative static data and dynamic data). To resolve this limit, generative AI methods, such as Generative adversarial network (GAN), have been widely applied for subsurface modeling due to their ability to reproduce complex geological patterns. However, the current practices of data conditioning in GANs conduct quality assessment through ocular inspection to check model plausibility or some preliminary quantitative analysis of the distribution of property of interests. We propose the generative AI realization minimum acceptance criteria for data conditioning, demonstrated with single image GAN. Our conditioning checks include static small-scale local and large-scale exhaustive data conditioning checks, local uncertainty, and spatial nonstationarity reproduction checks. We also check conditioning to geological concepts through multiscale spatial distribution, the number of connected geobodies, the spatial continuity check, and the model facies proportion reproduction check. Our proposed workflow provides guidance on the conditioning of deep learning methods for subsurface modeling and enhanced model conditioning checking essential for applying these models to support uncertainty characterization and decision making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05068v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lei Liu, Eduardo Maldonado-Cruz, Honggeun Jo, Ma\v{s}a Prodanovi\'c, Michael J. Pyrcz</dc:creator>
    </item>
    <item>
      <title>Prediction intervals for overdispersed Poisson data and their application in medical and pre-clinical quality control</title>
      <link>https://arxiv.org/abs/2404.05282</link>
      <description>arXiv:2404.05282v1 Announce Type: new 
Abstract: In pre-clinical and medical quality control, it is of interest to assess the stability of the process under monitoring or to validate a current observation using historical control data. Classically, this is done by the application of historical control limits (HCL) graphically displayed in control charts. In many applications, HCL are applied to count data, e.g. the number of revertant colonies (Ames assay) or the number of relapses per multiple sclerosis patient. Count data may be overdispersed, can be heavily right-skewed and clusters may differ in cluster size or other baseline quantities (e.g. number of petri dishes per control group or different length of monitoring times per patient).
  Based on the quasi-Poisson assumption or the negative-binomial distribution, we propose prediction intervals for overdispersed count data to be used as HCL. Variable baseline quantities are accounted for by offsets. Furthermore, we provide a bootstrap calibration algorithm that accounts for the skewed distribution and achieves equal tail probabilities.
  Comprehensive Monte-Carlo simulations assessing the coverage probabilities of eight different methods for HCL calculation reveal, that the bootstrap calibrated prediction intervals control the type-1-error best. Heuristics traditionally used in control charts (e.g. the limits in Sheward c- or u-charts or the mean plus minus 2 SD) fail to control a pre-specified coverage probability.
  The application of HCL is demonstrated based on data from the Ames assay and for numbers of relapses of multiple sclerosis patients. The proposed prediction intervals and the algorithm for bootstrap calibration are publicly available via the R package predint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05282v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Menssen, Martina Dammann, Firas Fneish, David Ellenberger, Frank Schaarschmid</dc:creator>
    </item>
    <item>
      <title>Continuous-time state-space methods for delta-O-18 and delta-C-13</title>
      <link>https://arxiv.org/abs/2404.05401</link>
      <description>arXiv:2404.05401v1 Announce Type: new 
Abstract: Time series analysis of delta-O-18 and delta-C-13 measurements from benthic foraminifera for purposes of paleoclimatology is challenging. The time series reach back tens of millions of years, they are relatively sparse in the early record and relatively dense in the later, the time stamps of the observations are not evenly spaced, and there are instances of multiple different observations at the same time stamp. The time series appear non-stationary over most of the historical record with clearly visible temporary trends of varying directions. In this paper, we propose a continuous-time state-space framework to analyze the time series. State space models are uniquely suited for this purpose, since they can accommodate all the challenging features mentioned above. We specify univariate models and joint bivariate models for the two time series of delta-O-18 and delta-C-13. The models are estimated using maximum likelihood by way of the Kalman filter recursions. The suite of models we consider has an interpretation as an application of the Butterworth filter. We propose model specifications that take the origin of the data from different studies into account and that allow for a partition of the total period into sub-periods reflecting different climate states. The models can be used, for example, to impute evenly time-stamped values by way of Kalman filtering. They can also be used, in future work, to analyze the relation to proxies for CO2 concentrations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05401v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mikkel Bennedsen, Eric Hillebrand, Siem Jan Koopman, Kathrine By Larsen</dc:creator>
    </item>
    <item>
      <title>Hausdorff Distance-Based Record Linkage for Improved Matching of Households and Individuals in Different Databases</title>
      <link>https://arxiv.org/abs/2404.05566</link>
      <description>arXiv:2404.05566v1 Announce Type: new 
Abstract: Matching households and individuals across different databases poses challenges due to the lack of unique identifiers, typographical errors, and changes in attributes over time. Record linkage tools play a crucial role in overcoming these difficulties. This paper presents a multi-step record linkage procedure that incorporates household information to enhance the entity-matching process across multiple databases. Our approach utilizes the Hausdorff distance to estimate the probability of a match between households in multiple files. Subsequently, probabilities of matching individuals within these households are computed using a logistic regression model based on attribute-level distances. These estimated probabilities are then employed in a linear programming optimization framework to infer one-to-one matches between individuals. To assess the efficacy of our method, we apply it to link data from the Italian Survey of Household Income and Wealth across different years. Through internal and external validation procedures, the proposed method is shown to provide a significant enhancement in the quality of the individual matching process, thanks to the incorporation of household information. A comparison with a standard record linkage approach based on direct matching of individuals, which neglects household information, underscores the advantages of accounting for such information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05566v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thais Pacheco Menezes (School of Mathematics and Statistics, University College Dublin), Thomas Brendan Murphy (School of Mathematics and Statistics, University College Dublin, Insight Centre for Data Analytics, University College Dublin, Institut d'\'Etudes Avanc\'ees, Universit\'e de Lyon, ERIC, Universit\'e de Lyon), Michael Fop (School of Mathematics and Statistics, University College Dublin)</dc:creator>
    </item>
    <item>
      <title>System and Method to Determine ME/CFS and Long COVID Disease Severity Using a Wearable Sensor</title>
      <link>https://arxiv.org/abs/2404.04345</link>
      <description>arXiv:2404.04345v1 Announce Type: cross 
Abstract: Objective: We present a simple parameter, calculated from a single wearable sensor, that can be used to objectively measure disease severity in people with myalgic encephalomyelitis/chronic fatigue syndrome (ME/CFS) or Long COVID. We call this parameter UpTime. Methods: Prior research has shown that the amount of time a person spends upright, defined as lower legs vertical with feet on the floor, correlates strongly with ME/CFS disease severity. We use a single commercial inertial measurement unit (IMU) attached to the ankle to calculate the percentage of time each day that a person spends upright (i.e., UpTime) and number of Steps/Day. As Long COVID shares symptoms with ME/CFS, we also apply this method to determine Long COVID disease severity. We performed a trial with 55 subjects broken into three cohorts, healthy controls, ME/CFS, and Long COVID. Subjects wore the IMU on their ankle for a period of 7 days. UpTime and Steps/Day were calculated each day and results compared between cohorts. Results: UpTime effectively distinguishes between healthy controls and subjects diagnosed with ME/CFS ($\mathbf{p = 0.00004}$) and between healthy controls and subjects diagnosed with Long COVID ($\mathbf{p = 0.01185}$). Steps/Day did distinguish between controls and subjects with ME/CFS ($\mathbf{p = 0.01}$) but did not distinguish between controls and subjects with Long COVID ($\mathbf{p = 0.3}$). Conclusion: UpTime is an objective measure of ME/CFS and Long COVID severity. UpTime can be used as an objective outcome measure in clinical research and treatment trials. Significance: Objective assessment of ME/CFS and Long COVID disease severity using UpTime could spur development of treatments by enabling the effect of those treatments to be easily measured.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04345v1</guid>
      <category>q-bio.QM</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifei Sun, Suzanne D. Vernon, Shad Roundy</dc:creator>
    </item>
    <item>
      <title>Bayesian Methods for Modeling Cumulative Exposure to Extensive Environmental Health Hazards</title>
      <link>https://arxiv.org/abs/2404.04398</link>
      <description>arXiv:2404.04398v1 Announce Type: cross 
Abstract: Measuring the impact of an environmental point source exposure on the risk of disease, like cancer or childhood asthma, is well-developed. Modeling how an environmental health hazard that is extensive in space, like a wastewater canal, is not. We propose a novel Bayesian generative semiparametric model for characterizing the cumulative spatial exposure to an environmental health hazard that is not well-represented by a single point in space. The model couples a dose-response model with a log-Gaussian Cox process integrated against a distance kernel with an unknown length-scale. We show that this model is a well-defined Bayesian inverse model, namely that the posterior exists under a Gaussian process prior for the log-intensity of exposure, and that a simple integral approximation adequately controls the computational error. We quantify the finite-sample properties and the computational tractability of the discretization scheme in a simulation study. Finally, we apply the model to survey data on household risk of childhood diarrheal illness from exposure to a system of wastewater canals in Mezquital Valley, Mexico.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04398v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rob Trangucci, Jesse Contreras, Jon Zelner, Joseph N. S. Eisenberg, Yang Chen</dc:creator>
    </item>
    <item>
      <title>Longitudinal Targeted Minimum Loss-based Estimation with Temporal-Difference Heterogeneous Transformer</title>
      <link>https://arxiv.org/abs/2404.04399</link>
      <description>arXiv:2404.04399v1 Announce Type: cross 
Abstract: We propose Deep Longitudinal Targeted Minimum Loss-based Estimation (Deep LTMLE), a novel approach to estimate the counterfactual mean of outcome under dynamic treatment policies in longitudinal problem settings. Our approach utilizes a transformer architecture with heterogeneous type embedding trained using temporal-difference learning. After obtaining an initial estimate using the transformer, following the targeted minimum loss-based likelihood estimation (TMLE) framework, we statistically corrected for the bias commonly associated with machine learning algorithms. Furthermore, our method also facilitates statistical inference by enabling the provision of 95% confidence intervals grounded in asymptotic statistical theory. Simulation results demonstrate our method's superior performance over existing approaches, particularly in complex, long time-horizon scenarios. It remains effective in small-sample, short-duration contexts, matching the performance of asymptotically efficient estimators. To demonstrate our method in practice, we applied our method to estimate counterfactual mean outcomes for standard versus intensive blood pressure management strategies in a real-world cardiovascular epidemiology cohort study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04399v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Toru Shirakawa, Yi Li, Yulun Wu, Sky Qiu, Yuxuan Li, Mingduo Zhao, Hiroyasu Iso, Mark van der Laan</dc:creator>
    </item>
    <item>
      <title>Gender Bias in Emerging New Research Topics: The Impact of COVID-19 on Women in Science</title>
      <link>https://arxiv.org/abs/2404.04707</link>
      <description>arXiv:2404.04707v1 Announce Type: cross 
Abstract: We investigate the impact of new research opportunities on the long-standing under-representation of women in medical and academic leadership by assessing the impact of the emergence of COVID-19 as a new research topic in the life sciences on women's authorship. After collecting publication data from 2019 and 2020 on biomedical publications, where the position of first and last author is most important for future career development, we use the major Medical Subject Heading (MeSH) terms to identify the main research area of each publication and measure the relation of each paper to COVID-19. Using a Difference-in-Difference approach, we find that although the general female authorship trend is upwards, papers in areas related to COVID-19 are less likely to have a woman as first or last author compared to research areas not related to COVID-19. Conversely, new publication opportunities in the COVID-19 research field increase the proportion of women in middle, less-relevant, author positions. Stay-at-home mandates, journal importance, and access to new funds do not fully explain the drop in women's outcomes. The decline in female first authorship is related to the increase of teams in which both lead authors have no prior experience in the COVID-related research field. In addition, pre-existing publishing teams show reduced bias in female key authorship with respect to new teams specifically formed for COVID-related research. This suggests that opportunistic teams, transitioning into research areas with emerging interests, possess greater flexibility in choosing the primary and final authors, potentially reducing uncertainties associated with engaging in productions divergent from their past scientific experiences by excluding women scientists from key authorship positions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04707v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carolina Biliotti, Massimo Riccaboni, Luca Verginer</dc:creator>
    </item>
    <item>
      <title>Two-Sided Flexibility in Platforms</title>
      <link>https://arxiv.org/abs/2404.04709</link>
      <description>arXiv:2404.04709v1 Announce Type: cross 
Abstract: Flexibility is a cornerstone of operations management, crucial to hedge stochasticity in product demands, service requirements, and resource allocation. In two-sided platforms, flexibility is also two-sided and can be viewed as the compatibility of agents on one side with agents on the other side. Platform actions often influence the flexibility on either the demand or the supply side. But how should flexibility be jointly allocated across different sides? Whereas the literature has traditionally focused on only one side at a time, our work initiates the study of two-sided flexibility in matching platforms. We propose a parsimonious matching model in random graphs and identify the flexibility allocation that optimizes the expected size of a maximum matching. Our findings reveal that flexibility allocation is a first-order issue: for a given flexibility budget, the resulting matching size can vary greatly depending on how the budget is allocated. Moreover, even in the simple and symmetric settings we study, the quest for the optimal allocation is complicated. In particular, easy and costly mistakes can be made if the flexibility decisions on the demand and supply side are optimized independently (e.g., by two different teams in the company), rather than jointly. To guide the search for optimal flexibility allocation, we uncover two effects, flexibility cannibalization, and flexibility abundance, that govern when the optimal design places the flexibility budget only on one side or equally on both sides. In doing so we identify the study of two-sided flexibility as a significant aspect of platform efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04709v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Freund, S\'ebastien Martin, Jiayu Kamessi Zhao</dc:creator>
    </item>
    <item>
      <title>The Spatial Structures in the Austrian COVID-19 Protest Movement: A Virtual and Geospatial Twitter User Network Analysis</title>
      <link>https://arxiv.org/abs/2404.04942</link>
      <description>arXiv:2404.04942v1 Announce Type: cross 
Abstract: The emergence of the COVID-19 pandemic, followed by policy measures to combat the virus, evoked public protest movements world-wide. These movements emerged through virtual social networks as well as local protest gatherings. Prior research has studied such movements solely in the virtual space through social network analysis, thereby disregarding the role of local interaction for protest. This study, however, recognizes the importance of the geo-spatial dimension in protest movements. We therefore introduce a large-scale spatial-social network analysis of a georeferenced Twitter user network to understand the regional connections and transnational influences of the Austrian COVID-19 protest movement through the social network. Our findings reveal that the virtual network is distinctly structured along geographic and linguistic boundaries. We further find that the movement is clearly organized along national protest communities. These results highlight the importance of regional and local influencing factors over the impact of transnational influences for the protest movement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04942v1</guid>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Umut Nefta Kanilmaz, Bernd Resch, Roland Holzinger, Christian Wasner, Thomas Steinmaurer</dc:creator>
    </item>
    <item>
      <title>Efficient Surgical Tool Recognition via HMM-Stabilized Deep Learning</title>
      <link>https://arxiv.org/abs/2404.04992</link>
      <description>arXiv:2404.04992v1 Announce Type: cross 
Abstract: Recognizing various surgical tools, actions and phases from surgery videos is an important problem in computer vision with exciting clinical applications. Existing deep-learning-based methods for this problem either process each surgical video as a series of independent images without considering their dependence, or rely on complicated deep learning models to count for dependence of video frames. In this study, we revealed from exploratory data analysis that surgical videos enjoy relatively simple semantic structure, where the presence of surgical phases and tools can be well modeled by a compact hidden Markov model (HMM). Based on this observation, we propose an HMM-stabilized deep learning method for tool presence detection. A wide range of experiments confirm that the proposed approaches achieve better performance with lower training and running costs, and support more flexible ways to construct and utilize training data in scenarios where not all surgery videos of interest are extensively labelled. These results suggest that popular deep learning approaches with over-complicated model structures may suffer from inefficient utilization of data, and integrating ingredients of deep learning and statistical learning wisely may lead to more powerful algorithms that enjoy competitive performance, transparent interpretation and convenient model training simultaneously.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04992v1</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haifeng Wang, Hao Xu, Jun Wang, Jian Zhou, Ke Deng</dc:creator>
    </item>
    <item>
      <title>BayesPPDSurv: An R Package for Bayesian Sample Size Determination Using the Power and Normalized Power Prior for Time-To-Event Data</title>
      <link>https://arxiv.org/abs/2404.05118</link>
      <description>arXiv:2404.05118v1 Announce Type: cross 
Abstract: The BayesPPDSurv (Bayesian Power Prior Design for Survival Data) R package supports Bayesian power and type I error calculations and model fitting using the power and normalized power priors incorporating historical data with for the analysis of time-to-event outcomes. The package implements the stratified proportional hazards regression model with piecewise constant hazard within each stratum. The package allows the historical data to inform the treatment effect parameter, parameter effects for other covariates in the regression model, as well as the baseline hazard parameters. The use of multiple historical datasets is supported. A novel algorithm is developed for computationally efficient use of the normalized power prior. In addition, the package supports the use of arbitrary sampling priors for computing Bayesian power and type I error rates, and has built-in features that semi-automatically generate sampling priors from the historical data. We demonstrate the use of BayesPPDSurv in a comprehensive case study for a melanoma clinical trial design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05118v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yueqi Shen, Matthew A. Psioda, Joseph G. Ibrahim</dc:creator>
    </item>
    <item>
      <title>Fitting Heterogeneous Lanchester Models on the Kursk Campaign</title>
      <link>https://arxiv.org/abs/1903.06666</link>
      <description>arXiv:1903.06666v2 Announce Type: replace 
Abstract: The battle of Kursk between Soviet and German is known to be the biggest tank battle in the history. The present paper uses the tank and artillery data from the Kursk database for fitting both forms of homogeneous and heterogeneous Lanchester model. Under homogeneous form the Soviet (or German) tank casualty is attributed to only the German(or Soviet) tank engagement. For heterogeneous form the tank casualty is attributed to both tank and artillery engagements. A set of differential equations using both forms have been developed, and the commonly used least square estimation is compared with maximum likelihood estimation for attrition rates and exponent coefficients. For validating the models, different goodness-of-fit measures like R2, sum-of-square-residuals (SSR), root-mean-square error (RMSE), Kolmogorov-Smirnov (KS) and chi-square statistics are used for comparison. Numerical results suggest the model is statistically more accurate when each day of the battle is considered as a mini-battle. The distribution patterns of the SSR and likelihood values with varying parameters are represented using contour plots and 3D surfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:1903.06666v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sumanta Kumar Das</dc:creator>
    </item>
    <item>
      <title>Spatial predictions on physically constrained domains: Applications to Arctic sea salinity data</title>
      <link>https://arxiv.org/abs/2210.03913</link>
      <description>arXiv:2210.03913v4 Announce Type: replace 
Abstract: In this paper we predict sea surface salinity (SSS) in the Arctic Ocean based on satellite measurements. SSS is a crucial indicator for ongoing changes in the Arctic Ocean and can offer important insights about climate change. We particularly focus on areas of water mistakenly flagged as ice by satellite algorithms. To remove bias in the retrieval of salinity near sea ice, the algorithms use conservative ice masks, which result in considerable loss of data. We aim to produce realistic SSS values for such regions to obtain more complete understanding about the SSS surface over the Arctic Ocean and benefit future applications that may require SSS measurements near edges of sea ice or coasts. We propose a class of scalable nonstationary processes that can handle large data from satellite products and complex geometries of the Arctic Ocean. Barrier overlap-removal acyclic directed graph GP (BORA-GP) constructs sparse directed acyclic graphs (DAGs) with neighbors conforming to barriers and boundaries, enabling characterization of dependence in constrained domains. The BORA-GP models produce more sensible SSS values in regions without satellite measurements and show improved performance in various constrained domains in simulation studies compared to state-of-the-art alternatives. An R package is available at https://github.com/jinbora0720/boraGP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.03913v4</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1214/23-AOAS1850</arxiv:DOI>
      <dc:creator>Bora Jin, Amy H. Herring, David Dunson</dc:creator>
    </item>
    <item>
      <title>Scaling-aware rating of count forecasts</title>
      <link>https://arxiv.org/abs/2211.16313</link>
      <description>arXiv:2211.16313v3 Announce Type: replace 
Abstract: Forecast quality should be assessed in the context of what is possible in theory and what is reasonable to expect in practice. Often, one can identify an approximate upper bound to a probabilistic forecast's sharpness, which sets a lower, not necessarily achievable, limit to error metrics. In retail forecasting, a simple, but often unconquerable sharpness limit is given by the Poisson distribution. When evaluating forecasts using traditional metrics such as Mean Absolute Error, it is hard to judge whether a certain achieved value reflects unavoidable Poisson noise or truly indicates an overdispersed prediction model. Moreover, every evaluation metric suffers from precision scaling: Perhaps surprisingly, the metric's value is mostly defined by the selling rate and by the resulting rate-dependent Poisson noise, and only secondarily by the forecast quality. For any metric, comparing two groups of forecasted products often yields "the slow movers are performing worse than the fast movers" or vice versa, the na\"ive scaling trap. To distill the intrinsic quality of a forecast, we stratify predictions into buckets of approximately equal predicted value and evaluate metrics separately per bucket. By comparing the achieved value per bucket to benchmarks, we obtain an intuitive visualization of forecast quality, which can be summarized into a single rating that makes forecast quality comparable among different products or even industries. The thereby developed scaling-aware forecast rating is applied to forecasting models used on the M5 competition dataset as well as to real-life forecasts provided by Blue Yonder's Demand Edge for Retail solution for grocery products in Sainsbury's supermarkets in the United Kingdom. The results permit a clear interpretation and high-level understanding of model quality by non-experts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.16313v3</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Malte C. Tichy, Illia Babounikau, Nikolas Wolke, Stefan Ulbrich, Michael Feindt</dc:creator>
    </item>
    <item>
      <title>Donor's Deferral and Return Behavior: Partial Identification from a Regression Discontinuity Design with Manipulation</title>
      <link>https://arxiv.org/abs/1910.02170</link>
      <description>arXiv:1910.02170v3 Announce Type: replace-cross 
Abstract: Volunteer labor can temporarily yield lower benefits to charities than its costs. In such instances, organizations may wish to defer volunteer donations to a later date. Exploiting a discontinuity in blood donations' eligibility criteria, we show that deferring donors reduces their future volunteerism. In our setting, medical staff manipulates donors' reported hemoglobin levels over a threshold to facilitate donation. Such manipulation invalidates standard regression discontinuity design. To circumvent this issue, we propose a procedure for obtaining partial identification bounds where manipulation is present. Our procedure is applicable in various regression discontinuity settings where the running variable is manipulated and discrete.</description>
      <guid isPermaLink="false">oai:arXiv.org:1910.02170v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Evan Rosenman, Karthik Rajkumar, Romain Gauriot, Robert Slonim</dc:creator>
    </item>
    <item>
      <title>Environmental migration? An overview of the literature</title>
      <link>https://arxiv.org/abs/2112.14097</link>
      <description>arXiv:2112.14097v2 Announce Type: replace-cross 
Abstract: The literature on the relationship between environmental factors such as climatic changes and natural hazards and human mobility (both internal and international) is characterized by heterogeneous results: some contributions highlight the role of climate changes as a driver of migratory flows, while others underline how this impact is mediated by geographical, economic and the features of the environmental shock. This paper attempts to map this literature, focusing on economics and empirical essays.
  The paper improves on the existing literature: (a) providing systematic research of the literature through main bibliographic databases, followed by a review and bibliometric analysis of all resulting papers; (b) building a citation-based network of contributions, that hollows to identify four separate clusters of paper; (c) applying meta-analysis methods on the sample of 96 papers released between 2003 and 2020, published in an academic journal, working papers series or unpublished studies, providing 3,904 point estimates of the effect of slow-onset events and 2,065 point estimates of the effect of fast-onset events.
  Overall, the meta-analytic average effect estimates a small impact of slow- and rapid-onset variables on migration, however positive and significant. When the clustering of the literature is accounted for, however, a significant heterogeneity emerges among the four clusters of papers, giving rise to new evidence on the formation of club-like convergence of literature outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.14097v2</guid>
      <category>cs.DL</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10290-024-00529-5</arxiv:DOI>
      <dc:creator>Maria Cipollina, Luca De Benedictis, Elisa Scib\`e</dc:creator>
    </item>
    <item>
      <title>A tool set for random number generation on GPUs in R</title>
      <link>https://arxiv.org/abs/2201.06604</link>
      <description>arXiv:2201.06604v3 Announce Type: replace-cross 
Abstract: We introduce the R package clrng which leverages the gpuR package and is able to generate random numbers in parallel on a Graphics Processing Unit (GPU) with the clRNG (OpenCL) library. Parallel processing with GPU's can speed up computationally intensive tasks, which when combined with R, it can largely improve R's downsides in terms of slow speed, memory usage and computation mode. clrng enables reproducible research by setting random initial seeds for streams on GPU and CPU, and can thus accelerate several types of statistical simulation and modelling. The random number generator in clrng guarantees independent parallel samples even when R is used interactively in an ad-hoc manner, with sessions being interrupted and restored. This package is portable and flexible, developers can use its random number generation kernel for various other purposes and applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.06604v3</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruoyong Xu, Patrick Brown, Pierre L'Ecuyer</dc:creator>
    </item>
    <item>
      <title>Optimal Priors for the Discounting Parameter of the Normalized Power Prior</title>
      <link>https://arxiv.org/abs/2302.14230</link>
      <description>arXiv:2302.14230v2 Announce Type: replace-cross 
Abstract: The power prior is a popular class of informative priors for incorporating information from historical data. It involves raising the likelihood for the historical data to a power, which acts as discounting parameter. When the discounting parameter is modelled as random, the normalized power prior is recommended. In this work, we prove that the marginal posterior for the discounting parameter for generalized linear models converges to a point mass at zero if there is any discrepancy between the historical and current data, and that it does not converge to a point mass at one when they are fully compatible. In addition, we explore the construction of optimal priors for the discounting parameter in a normalized power prior. In particular, we are interested in achieving the dual objectives of encouraging borrowing when the historical and current data are compatible and limiting borrowing when they are in conflict. We propose intuitive procedures for eliciting the shape parameters of a beta prior for the discounting parameter based on two minimization criteria, the Kullback-Leibler divergence and the mean squared error. Based on the proposed criteria, the optimal priors derived are often quite different from commonly used priors such as the uniform prior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.14230v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yueqi Shen, Luiz M. Carvalho, Matthew A. Psioda, Joseph G. Ibrahim</dc:creator>
    </item>
    <item>
      <title>Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects</title>
      <link>https://arxiv.org/abs/2306.10125</link>
      <description>arXiv:2306.10125v4 Announce Type: replace-cross 
Abstract: Self-supervised learning (SSL) has recently achieved impressive performance on various time series tasks. The most prominent advantage of SSL is that it reduces the dependence on labeled data. Based on the pre-training and fine-tuning strategy, even a small amount of labeled data can achieve high performance. Compared with many published self-supervised surveys on computer vision and natural language processing, a comprehensive survey for time series SSL is still missing. To fill this gap, we review current state-of-the-art SSL methods for time series data in this article. To this end, we first comprehensively review existing surveys related to SSL and time series, and then provide a new taxonomy of existing time series SSL methods by summarizing them from three perspectives: generative-based, contrastive-based, and adversarial-based. These methods are further divided into ten subcategories with detailed reviews and discussions about their key intuitions, main frameworks, advantages and disadvantages. To facilitate the experiments and validation of time series SSL methods, we also summarize datasets commonly used in time series forecasting, classification, anomaly detection, and clustering tasks. Finally, we present the future directions of SSL for time series analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.10125v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kexin Zhang, Qingsong Wen, Chaoli Zhang, Rongyao Cai, Ming Jin, Yong Liu, James Zhang, Yuxuan Liang, Guansong Pang, Dongjin Song, Shirui Pan</dc:creator>
    </item>
    <item>
      <title>Automated threshold selection and associated inference uncertainty for univariate extremes</title>
      <link>https://arxiv.org/abs/2310.17999</link>
      <description>arXiv:2310.17999v3 Announce Type: replace-cross 
Abstract: Threshold selection is a fundamental problem in any threshold-based extreme value analysis. While models are asymptotically motivated, selecting an appropriate threshold for finite samples is difficult and highly subjective through standard methods. Inference for high quantiles can also be highly sensitive to the choice of threshold. Too low a threshold choice leads to bias in the fit of the extreme value model, while too high a choice leads to unnecessary additional uncertainty in the estimation of model parameters. We develop a novel methodology for automated threshold selection that directly tackles this bias-variance trade-off. We also develop a method to account for the uncertainty in the threshold estimation and propagate this uncertainty through to high quantile inference. Through a simulation study, we demonstrate the effectiveness of our method for threshold selection and subsequent extreme quantile estimation, relative to the leading existing methods, and show how the method's effectiveness is not sensitive to the tuning parameters. We apply our method to the well-known, troublesome example of the River Nidd dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17999v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Conor Murphy, Jonathan A. Tawn, Zak Varty</dc:creator>
    </item>
    <item>
      <title>Multinomial belief networks for healthcare data</title>
      <link>https://arxiv.org/abs/2311.16909</link>
      <description>arXiv:2311.16909v3 Announce Type: replace-cross 
Abstract: Healthcare data from patient or population cohorts are often characterized by sparsity, high missingness and relatively small sample sizes. In addition, being able to quantify uncertainty is often important in a medical context. To address these analytical requirements we propose a deep generative Bayesian model for multinomial count data. We develop a collapsed Gibbs sampling procedure that takes advantage of a series of augmentation relations, inspired by the Zhou$\unicode{x2013}$Cong$\unicode{x2013}$Chen model. We visualise the model's ability to identify coherent substructures in the data using a dataset of handwritten digits. We then apply it to a large experimental dataset of DNA mutations in cancer and show that we can identify biologically meaningful clusters of mutational signatures in a fully data-driven way.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.16909v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>H. C. Donker, D. Neijzen, J. de Jong, G. A. Lunter</dc:creator>
    </item>
    <item>
      <title>Electron-Tunnelling-Noise Programmable Random Variate Accelerator for Monte Carlo Sampling</title>
      <link>https://arxiv.org/abs/2403.16421</link>
      <description>arXiv:2403.16421v3 Announce Type: replace-cross 
Abstract: This article presents an electron tunneling noise programmable random variate accelerator for accelerating the sampling stage of Monte Carlo simulations. We used the LiteX framework to generate a FemtoRV imfc RISC-V instruction set soft processor and deploy it on a Digilent Arty-100T FPGA development board. The RISC-V soft processor augmented with our programmable random variate accelerator achieves an average speedup of 8.70 times and a median speedup of 8.68 times for a suite of twelve different benchmark applications when compared to GNU Scientific Library software random number generation. These speedups are achievable because the benchmarks spend an average of 90.0 % of their execution time generating random samples. The results of the Monte Carlo benchmark programs run over the programmable random variate accelerator have an average Wasserstein distance of 1.48 times and a median Wasserstein distance of 1.41 times that of the results produced by the GNU Scientific Library random number generators. The soft processor samples the electron tunneling noise source using the hardened XADC block in the FPGA. The flexibility of the LiteX framework allows for the deployment of any LiteX-supported soft processor with an electron tunneling noise programmable random variate accelerator on any LiteX-supported development board that contains an FPGA with an XADC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16421v3</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <category>physics.comp-ph</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James T. Meech, Vasileios Tsoutsouras, Phillip Stanley-Marbell</dc:creator>
    </item>
  </channel>
</rss>
