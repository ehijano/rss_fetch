<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 02 Apr 2025 02:02:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Association in Facial Phenotype, Gene, Disease: A Dataset for Explainable Rare Genetic Diseases Diagnosis</title>
      <link>https://arxiv.org/abs/2503.22716</link>
      <description>arXiv:2503.22716v1 Announce Type: new 
Abstract: Many rare genetic diseases exhibit recognizable facial phenotypes, which are often used as diagnostic clues. However, current facial phenotype diagnostic models, which are trained on image datasets, have high accuracy but often suffer from an inability to explain their predictions, which reduces physicians' confidence in the model output.In this paper, we constructed a dataset, called FGDD, which was collected from 509 publications and contains 1147 data records, in which each data record represents a patient group and contains patient information, variation information, and facial phenotype information. To verify the availability of the dataset, we evaluated the performance of commonly used classification algorithms on the dataset and analyzed the explainability from global and local perspectives. FGDD aims to support the training of disease diagnostic models, provide explainable results, and increase physicians' confidence with solid evidence. It also allows us to explore the complex relationship between genes, diseases, and facial phenotypes, to gain a deeper understanding of the pathogenesis and clinical manifestations of rare genetic diseases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22716v1</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jie Song, Mengqiao He, Shumin Ren, Bairong Shen</dc:creator>
    </item>
    <item>
      <title>Anti-pathogenic property of thermophile-fermented compost as a feed additive and its in vivo external diagnostic imaging in a fish model</title>
      <link>https://arxiv.org/abs/2503.22916</link>
      <description>arXiv:2503.22916v2 Announce Type: new 
Abstract: Fermentation of organisms for recycling is important for the efficient cycling of nitrogen and phosphorus resources for a sustainable society, but the functionality of fermented products needs to be evaluated. Here, we clarify the anti-pathogenic properties for fish of a compost-type feed additive fermented by thermophilic Bacillaceae using non-edible marine resources as raw materials. After prior administration of the compost extract to seabream as a fish model for 70 days, the mortality rate after 28 days of exposure to the fish pathogen Edwardsiella reached a maximum of 20%, although the rate was 60% without prior administration. Under such conditions, the serum complement activity of seabream increased, and the recovery time after anesthesia treatment was also fasten. Furthermore, texture and HSV analysis using field photos statistically visualized differences in the degree of smoothness and gloss of the fish body surface depending on the administration. These results suggest that thermophile-fermented compost is effective as a functional feed additive against fish disease infection, and that such soundness can be estimated by body surface analysis. This study provides a new perspective for the natural symbiosis industry, as well as for the utilization of field non-invasive diagnosis to efficiently estimate the quality of its production activities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22916v2</guid>
      <category>q-bio.QM</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hirokuni Miyamoto, Shunsuke Ito, Kenta Suzuki, Singo Tamachi, Shion Yamada, Takayuki Nagatsuka, Takashi Satoh, Motoaki Udagawa, Hisashi Miyamoto, Hiroshi Ohno, Jun Kikuchi</dc:creator>
    </item>
    <item>
      <title>Graph Kolmogorov-Arnold Networks for Multi-Cancer Classification and Biomarker Identification, An Interpretable Multi-Omics Approach</title>
      <link>https://arxiv.org/abs/2503.22939</link>
      <description>arXiv:2503.22939v1 Announce Type: cross 
Abstract: The integration of multi-omics data presents a major challenge in precision medicine, requiring advanced computational methods for accurate disease classification and biological interpretation. This study introduces the Multi-Omics Graph Kolmogorov-Arnold Network (MOGKAN), a deep learning model that integrates messenger RNA, micro RNA sequences, and DNA methylation data with Protein-Protein Interaction (PPI) networks for accurate and interpretable cancer classification across 31 cancer types. MOGKAN employs a hybrid approach combining differential expression with DESeq2, Linear Models for Microarray (LIMMA), and Least Absolute Shrinkage and Selection Operator (LASSO) regression to reduce multi-omics data dimensionality while preserving relevant biological features. The model architecture is based on the Kolmogorov-Arnold theorem principle, using trainable univariate functions to enhance interpretability and feature analysis. MOGKAN achieves classification accuracy of 96.28 percent and demonstrates low experimental variability with a standard deviation that is reduced by 1.58 to 7.30 percents compared to Convolutional Neural Networks (CNNs) and Graph Neural Networks (GNNs). The biomarkers identified by MOGKAN have been validated as cancer-related markers through Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) enrichment analysis. The proposed model presents an ability to uncover molecular oncogenesis mechanisms by detecting phosphoinositide-binding substances and regulating sphingolipid cellular processes. By integrating multi-omics data with graph-based deep learning, our proposed approach demonstrates superior predictive performance and interpretability that has the potential to enhance the translation of complex multi-omics data into clinically actionable cancer diagnostics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22939v1</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fadi Alharbi, Nishant Budhiraja, Aleksandar Vakanski, Boyu Zhang, Murtada K. Elbashir, Mohanad Mohammed</dc:creator>
    </item>
    <item>
      <title>Can LLMs Support Medical Knowledge Imputation? An Evaluation-Based Perspective</title>
      <link>https://arxiv.org/abs/2503.22954</link>
      <description>arXiv:2503.22954v1 Announce Type: cross 
Abstract: Medical knowledge graphs (KGs) are essential for clinical decision support and biomedical research, yet they often exhibit incompleteness due to knowledge gaps and structural limitations in medical coding systems. This issue is particularly evident in treatment mapping, where coding systems such as ICD, Mondo, and ATC lack comprehensive coverage, resulting in missing or inconsistent associations between diseases and their potential treatments. To address this issue, we have explored the use of Large Language Models (LLMs) for imputing missing treatment relationships. Although LLMs offer promising capabilities in knowledge augmentation, their application in medical knowledge imputation presents significant risks, including factual inaccuracies, hallucinated associations, and instability between and within LLMs. In this study, we systematically evaluate LLM-driven treatment mapping, assessing its reliability through benchmark comparisons. Our findings highlight critical limitations, including inconsistencies with established clinical guidelines and potential risks to patient safety. This study serves as a cautionary guide for researchers and practitioners, underscoring the importance of critical evaluation and hybrid approaches when leveraging LLMs to enhance treatment mappings on medical knowledge graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22954v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyu Yao, Aditya Sannabhadti, Holly Wiberg, Karmel S. Shehadeh, Rema Padman</dc:creator>
    </item>
    <item>
      <title>In-silico biological discovery with large perturbation models</title>
      <link>https://arxiv.org/abs/2503.23535</link>
      <description>arXiv:2503.23535v1 Announce Type: cross 
Abstract: Data generated in perturbation experiments link perturbations to the changes they elicit and therefore contain information relevant to numerous biological discovery tasks -- from understanding the relationships between biological entities to developing therapeutics. However, these data encompass diverse perturbations and readouts, and the complex dependence of experimental outcomes on their biological context makes it challenging to integrate insights across experiments. Here, we present the Large Perturbation Model (LPM), a deep-learning model that integrates multiple, heterogeneous perturbation experiments by representing perturbation, readout, and context as disentangled dimensions. LPM outperforms existing methods across multiple biological discovery tasks, including in predicting post-perturbation transcriptomes of unseen experiments, identifying shared molecular mechanisms of action between chemical and genetic perturbations, and facilitating the inference of gene-gene interaction networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23535v1</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Djordje Miladinovic, Tobias H\"oppe, Mathieu Chevalley, Andreas Georgiou, Lachlan Stuart, Arash Mehrjou, Marcus Bantscheff, Bernhard Sch\"olkopf, Patrick Schwab</dc:creator>
    </item>
    <item>
      <title>Practical and scalable simulations of non-Markovian stochastic processes and temporal networks with individual node properties</title>
      <link>https://arxiv.org/abs/2212.05059</link>
      <description>arXiv:2212.05059v4 Announce Type: replace 
Abstract: Discrete stochastic processes are prevalent in natural systems, with applications in physics, biochemistry, epidemiology, sociology, and finance. While analytic solutions often cannot be derived, existing simulation frameworks can generate stochastic trajectories compatible with the dynamical laws underlying the random phenomena. Still, most simulation algorithms assume the system dynamics are memoryless (Markovian assumption), under which assumption, future occurrences only depend on the system's present state. This enables efficient and exact simulation via the Gillespie algorithm. However, many real-world systems are inherently non-Markovian and exhibit memory effects. Such systems are difficult to study analytically, and current numerical methods are often computationally expensive or limited by strong simplifying assumptions that conflict with empirical data. To address these limitations, we introduce the Rejection-based Gillespie algorithm for non-Markovian Reactions (REGIR), a general and scalable framework for simulating non-Markovian stochastic systems with arbitrary inter-event time distributions. REGIR provides user-defined accuracy while preserving the same asymptotic computational complexity as the classical Gillespie algorithm. We derive a lower bound on REGIR's approximation accuracy and demonstrate its capabilities across three representative classes of non-Markovian systems: (1) reaction channels with delays, (2) stochastic processes driven by individual reactant properties, and (3) temporal networks governed by node activity. In all cases, REGIR accurately captures memory-dependent dynamics and outperforms existing approaches in terms of flexibility and efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.05059v4</guid>
      <category>q-bio.QM</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aurelien Pelissier, Miroslav Phan, Didier Le Bail, Niko Beerenwinkel, Maria Rodriguez Martinez</dc:creator>
    </item>
    <item>
      <title>Single-Cell Proteomics Using Mass Spectrometry</title>
      <link>https://arxiv.org/abs/2502.11982</link>
      <description>arXiv:2502.11982v2 Announce Type: replace 
Abstract: Single-cell proteomics (SCP) is transforming our understanding of biological complexity by shifting from bulk proteomics, where signals are averaged over thousands of cells, to the proteome analysis of individual cells. This granular perspective reveals distinct cell states, population heterogeneity, and the underpinnings of disease pathogenesis that bulk approaches may obscure. However, SCP demands exceptional sensitivity, precise cell handling, and robust data processing to overcome the inherent challenges of analyzing picogram-level protein samples without amplification. Recent innovations in sample preparation, separations, data acquisition strategies, and specialized mass spectrometry instrumentation have substantially improved proteome coverage and throughput. Approaches that integrate complementary omics, streamline multi-step sample processing, and automate workflows through microfluidics and specialized platforms promise to further push SCP boundaries. Advances in computational methods, especially for data normalization and imputation, address the pervasive issue of missing values, enabling more reliable downstream biological interpretations. Despite these strides, higher throughput, reproducibility, and consensus best practices remain pressing needs in the field. This mini review summarizes the latest progress in SCP technology and software solutions, highlighting how closer integration of analytical, computational, and experimental strategies will facilitate deeper and broader coverage of single-cell proteomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11982v2</guid>
      <category>q-bio.QM</category>
      <category>q-bio.GN</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Amanda Momenzadeh, Jesse G. Meyer</dc:creator>
    </item>
    <item>
      <title>Correlation Improves Group Testing: Modeling Concentration-Dependent Test Errors</title>
      <link>https://arxiv.org/abs/2111.07517</link>
      <description>arXiv:2111.07517v5 Announce Type: replace-cross 
Abstract: Population-wide screening is a powerful tool for controlling infectious diseases. Group testing enables such screening despite limited resources. Viral concentration of pooled samples are often positively correlated, either because prevalence and sample collection are influenced by location, or through intentional enhancement via pooling samples according to risk/household. Such correlation is known to improve efficiency under fixed test sensitivity. However, in reality, a test's sensitivity depends on the concentration of the analyte (e.g., viral RNA), as in the so-called dilution effect, where sensitivity decreases for larger pools. We show that concentration-dependent test error alters correlation's effect under the most widely-used group testing procedure, the two-stage Dorfman procedure. We prove that when test sensitivity increases with concentration, pooling correlated samples together (correlated pooling) achieves asymptotically higher sensitivity than independently pooling the samples (naive pooling). In contrast, in the concentration-independent case, correlation does not affect sensitivity. Moreover, with concentration-dependent errors, correlation can degrade test efficiency compared to naive pooling whereas under concentration-independent errors, correlation always improves efficiency. We propose an alternative measure of test resource usage, the number of positives found per test consumed, which we argue is better aligned with infection control, and show that correlated pooling outperforms naive pooling on this measure. In simulation, we show that the effect of correlation under realistic concentration-dependent test error meaningfully differs from correlation's effect assuming fixed sensitivity. Our findings underscore the importance for policy-makers of using models that incorporate naturally-occurring correlation and of considering ways of strengthening this correlation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.07517v5</guid>
      <category>stat.AP</category>
      <category>physics.soc-ph</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jiayue Wan, Yujia Zhang, Peter I. Frazier</dc:creator>
    </item>
    <item>
      <title>Open reaction-diffusion systems: bridging probabilistic theory and simulations across scales</title>
      <link>https://arxiv.org/abs/2404.07119</link>
      <description>arXiv:2404.07119v3 Announce Type: replace-cross 
Abstract: Reaction-diffusion processes are the foundational model for a diverse range of complex systems, ranging from biochemical reactions to social agent-based phenomena. The underlying dynamics of these systems occur at the individual particle/agent level, and in realistic applications, they often display interaction with their environment through energy or material exchange with a reservoir. This requires intricate mathematical considerations, especially in the case of material exchange since the varying number of particles/agents results in ``on-the-fly'' modification of the system dimension. In this work, we first overview the probabilistic description of reaction-diffusion processes at the particle level, which readily handles varying number of particles. We then extend this model to consistently incorporate interactions with macroscopic material reservoirs. Based on the resulting expressions, we bridge the probabilistic description with macroscopic concentration-based descriptions for linear and nonlinear reaction-diffusion systems, as well as for an archetypal open reaction-diffusion system. Using these mathematical bridges across scales, we finally develop numerical schemes for open reaction-diffusion systems, which we implement in two illustrative examples. This work establishes a methodological workflow to bridge particle-based probabilistic descriptions with macroscopic concentration-based descriptions of reaction-diffusion in open settings, laying the foundations for a multiscale theoretical framework upon which to construct theory and simulation schemes that are consistent across scales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07119v3</guid>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.mes-hall</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>physics.chem-ph</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mauricio J. del Razo, Margarita Kostr\'e</dc:creator>
    </item>
    <item>
      <title>Entropy-Reinforced Planning with Large Language Models for Drug Discovery</title>
      <link>https://arxiv.org/abs/2406.07025</link>
      <description>arXiv:2406.07025v2 Announce Type: replace-cross 
Abstract: The objective of drug discovery is to identify chemical compounds that possess specific pharmaceutical properties toward a binding target. Existing large language models (LLMS) can achieve high token matching scores in terms of likelihood for molecule generation. However, relying solely on LLM decoding often results in the generation of molecules that are either invalid due to a single misused token, or suboptimal due to unbalanced exploration and exploitation as a consequence of the LLMs prior experience. Here we propose ERP, Entropy-Reinforced Planning for Transformer Decoding, which employs an entropy-reinforced planning algorithm to enhance the Transformer decoding process and strike a balance between exploitation and exploration. ERP aims to achieve improvements in multiple properties compared to direct sampling from the Transformer. We evaluated ERP on the SARS-CoV-2 virus (3CLPro) and human cancer cell target protein (RTCB) benchmarks and demonstrated that, in both benchmarks, ERP consistently outperforms the current state-of-the-art algorithm by 1-5 percent, and baselines by 5-10 percent, respectively. Moreover, such improvement is robust across Transformer models trained with different objectives. Finally, to further illustrate the capabilities of ERP, we tested our algorithm on three code generation benchmarks and outperformed the current state-of-the-art approach as well. Our code is publicly available at: https://github.com/xuefeng-cs/ERP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07025v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.QM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuefeng Liu, Chih-chan Tien, Peng Ding, Songhao Jiang, Rick L. Stevens</dc:creator>
    </item>
    <item>
      <title>Ranking nodes in bipartite systems with a non-linear iterative map</title>
      <link>https://arxiv.org/abs/2406.17572</link>
      <description>arXiv:2406.17572v2 Announce Type: replace-cross 
Abstract: Ranking nodes in networks according to a defined measure of importance is an extensively studied task, with applications in ecology, economic trade networks, and social networks. This paper introduces a method based on a non-linear iterative map to evaluate node relevance in bipartite networks. By tuning a single parameter $\gamma$, the method captures different concepts of node importance, including established measures like degree centrality, eigenvector centrality and the fitness-complexity ranking. The algorithm's flexibility allows for efficient ranking optimization tailored to specific tasks, outperforming state-of-the-art algorithms. We apply this method to ecological mutualistic networks, where ranking quality can be assessed by the extinction area - the rate at which the system collapses when species are removed in a certain order. The map with the optimal $\gamma$ value surpasses existing ranking methods on this task. Additionally, our method excels in evaluating nestedness, another crucial structural property of ecological systems, requiring specific node rankings. Finally, we explore theoretical aspects of the map, revealing a phase transition at a critical $\gamma$ dependent on the data structure that can be characterized analytically for random networks. Near the critical point, the map exhibits unique features and a distinctive "triangular" packing pattern of the incidence matrix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17572v2</guid>
      <category>cond-mat.stat-mech</category>
      <category>physics.soc-ph</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Mazzolini, Michele Caselle, Matteo Osella</dc:creator>
    </item>
    <item>
      <title>Persistent Sheaf Laplacian Analysis of Protein Flexibility</title>
      <link>https://arxiv.org/abs/2502.08772</link>
      <description>arXiv:2502.08772v2 Announce Type: replace-cross 
Abstract: Protein flexibility, measured by the B-factor or Debye-Waller factor, is essential for protein functions such as structural support, enzyme activity, cellular communication, and molecular transport. Theoretical analysis and prediction of protein flexibility are crucial for protein design, engineering, and drug discovery. In this work, we introduce the persistent sheaf Laplacian (PSL), an effective tool in topological data analysis, to model and analyze protein flexibility. By representing the local topology and geometry of protein atoms through the multiscale harmonic and non-harmonic spectra of PSLs, the proposed model effectively captures protein flexibility and provides accurate, robust predictions of protein B-factors. Our PSL model demonstrates an increase in accuracy of 32% compared to the classical Gaussian network model (GNM) in predicting B-factors for a dataset of 364 proteins. Additionally, we construct a blind machine learning prediction method utilizing global and local protein features. Extensive computations and comparisons validate the effectiveness of the proposed PSL model for B-factor predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08772v2</guid>
      <category>q-bio.BM</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicole Hayes, Xiaoqi Wei, Hongsong Feng, Ekaterina Merkurjev, Guo-Wei Wei</dc:creator>
    </item>
    <item>
      <title>Leveraging partial coherence in interferometric microscopy to enhance nanoparticle detection sensitivity and throughput</title>
      <link>https://arxiv.org/abs/2503.22565</link>
      <description>arXiv:2503.22565v2 Announce Type: replace-cross 
Abstract: Interferometric-based microscopies stand as powerful label-free approaches for monitoring and characterising chemical reactions and heterogeneous nanoparticle systems in real time with single particle sensitivity. Nevertheless, coherent artifacts, such as speckle and parasitic interferences, together with limited photon fluxes from spatially incoherent sources, pose an ongoing challenge in achieving both high sensitivity and throughput. In this study, we systematically characterise how partial coherence affects both the signal contrast and the background noise level; thus, it offers a route to improve the signal-to-noise ratio from single nanoparticles (NPs), irrespective of their size and composition; or the light source used. We first validate that lasers can be modified into partially coherent sources with performance matching that of spatially incoherent ones; while providing higher photon fluxes. Secondly, we demonstrate that tuning the degree of partial coherence not only enhances the detection sensitivity of both synthetic and biological NPs, but also affects how signal contrasts vary as a function of the focus position. Finally, we apply our findings to single-protein detection, confirming that these principles extend to differential imaging modalities, which deliver the highest sensitivity. Our results address a critical milestone in the detection of weakly scattering NPs in complex matrices, with wide-ranging applications in biotechnology, nanotechnology, chemical synthesis, and biosensing; ushering a new generation of microscopes that push both the sensitivity and throughput boundaries without requiring beam scanning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22565v2</guid>
      <category>physics.optics</category>
      <category>physics.bio-ph</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Lombardo, Andrea Sottini, Sarina Seiter, Gerard Colas des Francs, Jaime Ortega Arroyo, Romain Quidant</dc:creator>
    </item>
  </channel>
</rss>
