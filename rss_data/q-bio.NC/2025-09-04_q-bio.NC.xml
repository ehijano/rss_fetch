<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Sep 2025 01:26:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Data-driven mean-field within whole-brain models</title>
      <link>https://arxiv.org/abs/2509.02799</link>
      <description>arXiv:2509.02799v2 Announce Type: new 
Abstract: Mean-field models provide a link between microscopic neuronal activity and macroscopic brain dynamics. Their derivation depends on simplifying assumptions, such as all-to-all connectivity, limiting their biological realism. To overcome this, we introduce a data-driven framework in which a multi-layer perceptron (MLP) learns the macroscopic dynamics directly from simulations of a network of spiking neurons. The network connection probability serves here as a new parameter, inaccessible to purely analytical treatment, which is validated against ground truth analytical solutions. Through bifurcation analysis on the trained MLP, we demonstrate the existence of new cusp bifurcation that systematically reshapes the system's phase diagram in a degenerate manner with synaptic coupling. By integrating this data-driven mean-field model into a whole-brain computational framework, we show that it extends beyond the macroscopic emergent dynamics generated by the analytical model. For validation, we use simulation-based inference on synthetic functional magnetic resonance imaging (fMRI) data and demonstrate accurate parameter recovery for the novel mean-field model, while the current state-of-the-art models lead to biased estimates. This work presents a flexible and generic framework for building more realistic whole-brain models, bridging the gap between microscale mechanisms and macroscopic brain recordings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02799v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Martin Breyton, Viktor Sip, Marmaduke Woodman, Meysam Hashemi, Spase Petkoski, Viktor Jirsa</dc:creator>
    </item>
    <item>
      <title>Mentality: A Mamba-based Approach towards Foundation Models for EEG</title>
      <link>https://arxiv.org/abs/2509.02746</link>
      <description>arXiv:2509.02746v1 Announce Type: cross 
Abstract: This work explores the potential of foundation models, specifically a Mamba-based selective state space model, for enhancing EEG analysis in neurological disorder diagnosis. EEG, crucial for diagnosing conditions like epilepsy, presents significant challenges due to its noisy, high-dimensional, and nonlinear nature. Traditional machine learning methods have made advances in automating EEG analysis but often fail to capture its complex spatio-temporal dynamics. Recent advances in deep learning, particularly in sequence modeling, offer new avenues for creating more generalized and expressive models capable of handling such complexities. By training a Mamba-based model on a large dataset containing seizure and non-seizure EEG recordings through a self-supervised reconstruction task followed by a seizure detection task, we demonstrate the model's effectiveness, achieving an AUROC of 0.72 on a held-out test set. This approach marks a significant step toward developing large-scale, clinically applicable foundation models for EEG data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02746v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>In Proceedings of the ICLR 2024 Workshop on Learning from Time Series for Health (2024). Retrieved from https://openreview.net/forum?id=O6T38rRiFp</arxiv:journal_reference>
      <dc:creator>Saarang Panchavati, Corey Arnold, William Speier</dc:creator>
    </item>
    <item>
      <title>StableSleep: Source-Free Test-Time Adaptation for Sleep Staging with Lightweight Safety Rails</title>
      <link>https://arxiv.org/abs/2509.02982</link>
      <description>arXiv:2509.02982v1 Announce Type: cross 
Abstract: Sleep staging models often degrade when deployed on patients with unseen physiology or recording conditions. We propose a streaming, source-free test-time adaptation (TTA) recipe that combines entropy minimization (Tent) with Batch-Norm statistic refresh and two safety rails: an entropy gate to pause adaptation on uncertain windows and an EMA-based reset to reel back drift. On Sleep-EDF Expanded, using single-lead EEG (Fpz-Cz, 100 Hz, 30s epochs; R&amp;K to AASM mapping), we show consistent gains over a frozen baseline at seconds-level latency and minimal memory, reporting per-stage metrics and Cohen's k. The method is model-agnostic, requires no source data or patient calibration, and is practical for on-device or bedside use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02982v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hritik Arasu, Faisal R Jahangiri</dc:creator>
    </item>
    <item>
      <title>From Chaos to Coherence: Effects of High-Order Synaptic Correlations on Neural Dynamics</title>
      <link>https://arxiv.org/abs/2504.00300</link>
      <description>arXiv:2504.00300v2 Announce Type: replace 
Abstract: Recurrent Neural Network models have elucidated the interplay between structure and dynamics in biological neural networks, particularly the emergence of irregular and rhythmic activities in cortex. However, most studies have focused on networks with random or simple connectivity structures. Experimental observations find that high-order cortical connectivity patterns affect the temporal patterns of network activity, but a theory that relates such complex structure to network dynamics has yet to be developed. Here, we show that third- and higher-order cyclic correlations in synaptic connectivities greatly impact neuronal dynamics. Specifically, strong cyclic correlations in a network suppress chaotic dynamics and promote oscillatory or fixed activity. This change in dynamics is related to the form of the unstable eigenvalues of the random connectivity matrix. A phase transition from chaotic to fixed or oscillatory activity coincides with the development of a cusp at the leading edge of the eigenvalue support. We also relate the dimensions of activity to the network structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00300v2</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>physics.bio-ph</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nimrod Sherf, Xaq Pitkow, Kre\v{s}imir Josi\'c, Kevin E. Bassler</dc:creator>
    </item>
    <item>
      <title>Meta-learning ecological priors from large language models explains human learning and decision making</title>
      <link>https://arxiv.org/abs/2509.00116</link>
      <description>arXiv:2509.00116v2 Announce Type: replace 
Abstract: Human cognition is profoundly shaped by the environments in which it unfolds. Yet, it remains an open question whether learning and decision making can be explained as a principled adaptation to the statistical structure of real-world tasks. We introduce ecologically rational analysis, a computational framework that unifies the normative foundations of rational analysis with ecological grounding. Leveraging large language models to generate ecologically valid cognitive tasks at scale, and using meta-learning to derive rational models optimized for these environments, we develop a new class of learning algorithms: Ecologically Rational Meta-learned Inference (ERMI). ERMI internalizes the statistical regularities of naturalistic problem spaces and adapts flexibly to novel situations, without requiring hand-crafted heuristics or explicit parameter updates. We show that ERMI captures human behavior across 15 experiments spanning function learning, category learning, and decision making, outperforming several established cognitive models in trial-by-trial prediction. Our results suggest that much of human cognition may reflect adaptive alignment to the ecological structure of the problems we encounter in everyday life.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00116v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akshay K. Jagadish, Mirko Thalmann, Julian Coda-Forno, Marcel Binz, Eric Schulz</dc:creator>
    </item>
    <item>
      <title>On sources to variabilities of simple cells in the primary visual cortex: A principled theory for the interaction between geometric image transformations and receptive field responses</title>
      <link>https://arxiv.org/abs/2509.02139</link>
      <description>arXiv:2509.02139v2 Announce Type: replace 
Abstract: This paper gives an overview of a theory for modelling the interaction between geometric image transformations and receptive field responses for a visual observer that views objects and spatio-temporal events in the environment. This treatment is developed over combinations of (i) uniform spatial scaling transformations, (ii) spatial affine transformations, (iii) Galilean transformations and (iv) temporal scaling transformations.
  By postulating that the family of receptive fields should be covariant under these classes of geometric image transformations, it follows that the receptive field shapes should be expanded over the degrees of freedom of the corresponding image transformations, to enable a formal matching between the receptive field responses computed under different viewing conditions for the same scene or for a structurally similar spatio-temporal event.
  We conclude the treatment by discussing and providing potential support for a working hypothesis that the receptive fields of simple cells in the primary visual cortex ought to be covariant under these classes of geometric image transformations, and thus have the shapes of their receptive fields expanded over the degrees of freedom of the corresponding geometric image transformations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02139v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tony Lindeberg</dc:creator>
    </item>
    <item>
      <title>Frugal inference for control</title>
      <link>https://arxiv.org/abs/2406.14427</link>
      <description>arXiv:2406.14427v3 Announce Type: replace-cross 
Abstract: A key challenge in advancing artificial intelligence is achieving the right balance between utility maximization and resource use by both external movement and internal computation. While this trade-off has been studied in fully observable settings, our understanding of resource efficiency in partially observable environments remains limited. Motivated by this challenge, we develop a version of the POMDP framework where the information gained through inference is treated as a resource that must be optimized alongside task performance and motion effort. By solving this problem in environments described by linear-Gaussian dynamics, we uncover fundamental principles of resource efficiency. Our study reveals a phase transition in the inference, switching from a Bayes-optimal approach to one that strategically leaves some uncertainty unresolved. This frugal behavior gives rise to a structured family of equally effective strategies, facilitating adaptation to later objectives and constraints overlooked during the original optimization. We illustrate the applicability of our framework and the generality of the principles we derived using two nonlinear tasks. Overall, this work provides a foundation for a new type of rational computation that both brains and machines could use for effective but resource-efficient control under uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14427v3</guid>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Itzel Olivos-Castillo, Paul Schrater, Xaq Pitkow</dc:creator>
    </item>
    <item>
      <title>Dynamical stability for dense patterns in discrete attractor neural networks</title>
      <link>https://arxiv.org/abs/2507.10383</link>
      <description>arXiv:2507.10383v2 Announce Type: replace-cross 
Abstract: Neural networks storing multiple discrete attractors are canonical models of biological memory. Previously, the dynamical stability of such networks could only be guaranteed under highly restrictive conditions. Here, we derive a theory of the local stability of discrete fixed points in a broad class of networks with graded neural activities and in the presence of noise. By directly analyzing the bulk and outliers of the Jacobian spectrum, we show that all fixed points are stable below a critical load that is distinct from the classical \textit{critical capacity} and depends on the statistics of neural activities in the fixed points as well as the single-neuron activation function. Our analysis highlights the computational benefits of threshold-linear activation and sparse-like patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10383v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Uri Cohen, M\'at\'e Lengyel</dc:creator>
    </item>
    <item>
      <title>Intermittent localization and fast spatial learning by non-Markov random walks with decaying memory</title>
      <link>https://arxiv.org/abs/2509.01806</link>
      <description>arXiv:2509.01806v2 Announce Type: replace-cross 
Abstract: Random walks on lattices with preferential relocation to previously visited sites provide a simple modeling of the displacements of animals and humans. When the lattice contains a single impurity or resource site where the walker spends more time on average at each visit than on the other sites, the long range memory can suppress diffusion and induce by reinforcement a steady state localized around the resource. This phenomenon can be identified with a spatial learning process by the walker. Here we study theoretically and numerically how the decay of memory impacts learning in these models. If memory decays as $1/\tau$ or slower, where $\tau$ is the time backward into the past, the localized solutions are the same as with perfect, non-decaying memory and they are linearly stable. If forgetting is faster than $1/\tau$, for instance exponential, an unusual regime of intermittent localization is observed, where well localized periods of exponentially distributed duration are interspersed with intervals of diffusive motion. At the transition between the two regimes, for a kernel in $1/\tau$, the approach to the stable localized state is the fastest, opposite to the expected critical slowing down effect. Hence forgetting can allow the walker to save memory without compromising learning and to achieve a faster learning. These findings agree with biological evidence on the benefits of forgetting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01806v2</guid>
      <category>cond-mat.stat-mech</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paulina R. Mart\'in-Cornejo, Denis Boyer</dc:creator>
    </item>
  </channel>
</rss>
