<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Jul 2025 01:29:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>MCP4EDA: LLM-Powered Model Context Protocol RTL-to-GDSII Automation with Backend Aware Synthesis Optimization</title>
      <link>https://arxiv.org/abs/2507.19570</link>
      <description>arXiv:2507.19570v1 Announce Type: new 
Abstract: This paper presents MCP4EDA, the first Model Context Protocol server that enables Large Language Models (LLMs) to control and optimize the complete open-source RTL-to-GDSII design flow through natural language interaction. The system integrates Yosys synthesis, Icarus Verilog simulation, OpenLane place-and-route, GTKWave analysis, and KLayout visualization into a unified LLM-accessible interface, enabling designers to execute complex multi-tool EDA workflows conversationally via AI assistants such as Claude Desktop and Cursor IDE. The principal contribution is a backend-aware synthesis optimization methodology wherein LLMs analyze actual post-layout timing, power, and area metrics from OpenLane results to iteratively refine synthesis TCL scripts, establishing a closed-loop optimization system that bridges the traditional gap between synthesis estimates and physical implementation reality. In contrast to conventional flows that rely on wire-load models, this methodology leverages real backend performance data to guide synthesis parameter tuning, optimization sequence selection, and constraint refinement, with the LLM functioning as an intelligent design space exploration agent. Experimental evaluation on representative digital designs demonstrates 15-30% improvements in timing closure and 10-20% area reduction compared to default synthesis flows, establishing MCP4EDA as the first practical LLM-controlled end-to-end open-source EDA automation system. The code and demo are avaiable at: http://www.agent4eda.com/</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19570v1</guid>
      <category>cs.AR</category>
      <category>cs.MA</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiting Wang, Wanghao Ye, Yexiao He, Yiran Chen, Gang Qu, Ang Li</dc:creator>
    </item>
    <item>
      <title>ChipletPart: Scalable Cost-Aware Partitioning for 2.5D Systems</title>
      <link>https://arxiv.org/abs/2507.19819</link>
      <description>arXiv:2507.19819v1 Announce Type: new 
Abstract: Industry adoption of chiplets has been increasing as a cost-effective option for making larger high-performance systems. Consequently, partitioning large systems into chiplets is increasingly important. In this work, we introduce ChipletPart - a cost-driven 2.5D system partitioner that addresses the unique constraints of chiplet systems, including complex objective functions, limited reach of inter-chiplet I/O transceivers, and the assignment of heterogeneous manufacturing technologies to different chiplets. ChipletPart integrates a sophisticated chiplet cost model with its underlying genetic algorithm-based technology assignment and partitioning methodology, along with a simulated annealing-based chiplet floorplanner. Our results show that: (i) ChipletPart reduces chiplet cost by up to 58% (20% geometric mean) compared to state-of-the-art min-cut partitioners, which often yield floorplan-infeasible solutions; (ii) ChipletPart generates partitions with up to 47% (6% geometric mean) lower cost as compared to the prior work Floorplet; and (iii) for the testcases we study, heterogeneous integration reduces cost by up to 43% (15% geometric mean) compared to homogeneous implementations. We also present case studies that show how changes in packaging or inter-chiplet signaling technologies can affect partitioning solutions. Finally, we make ChipletPart, the underlying chiplet cost model, and a chiplet testcase generator available as open-source tools for the community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19819v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Graening, Puneet Gupta, Andrew B. Kahng, Bodhisatta Pramanik, Zhiang Wang</dc:creator>
    </item>
    <item>
      <title>AxOSyn: An Open-source Framework for Synthesizing Novel Approximate Arithmetic Operators</title>
      <link>https://arxiv.org/abs/2507.20007</link>
      <description>arXiv:2507.20007v1 Announce Type: new 
Abstract: Edge AI deployments are becoming increasingly complex, necessitating energy-efficient solutions for resource-constrained embedded systems. Approximate computing, which allows for controlled inaccuracies in computations, is emerging as a promising approach for improving power and energy efficiency. Among the key techniques in approximate computing are approximate arithmetic operators (AxOs), which enable application-specific optimizations beyond traditional computer arithmetic hardware reduction-based methods, such as quantization and precision scaling. Existing design space exploration (DSE) frameworks for approximate computing limit themselves to selection-based approaches or custom synthesis at fixed abstraction levels, which restricts the flexibility required for finding application-specific optimal solutions. Further, the tools available for the DSE of AxOs are quite limited in terms of exploring different approximation models and extending the analysis to different granularities. To this end, we propose AxOSyn, an open-source framework for the DSE of AxOs that supports both selection and synthesis approaches at various abstraction levels. AxOSyn allows researchers to integrate custom methods for evaluating approximations and facilitates DSE at both the operator-level and application-specific. Our framework provides an effective methodology for achieving energy-efficient, approximate operators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20007v1</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <category>cs.LO</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siva Satyendra Sahoo, Salim Ullah, Akash Kumar</dc:creator>
    </item>
    <item>
      <title>RoCE BALBOA: Service-enhanced Data Center RDMA for SmartNICs</title>
      <link>https://arxiv.org/abs/2507.20412</link>
      <description>arXiv:2507.20412v1 Announce Type: new 
Abstract: Data-intensive applications in data centers, especially machine learning (ML), have made the network a bottleneck, which in turn has motivated the development of more efficient network protocols and infrastructure. For instance, remote direct memory access (RDMA) has become the standard protocol for data transport in the cloud as it minimizes data copies and reduces CPU-utilization via host-bypassing. Similarly, an increasing amount of network functions and infrastructure have moved to accelerators, SmartNICs, and in-network computing to bypass the CPU. In this paper we explore the implementation and deployment of RoCE BALBOA, an open-source, RoCE v2-compatible, scalable up to hundreds of queue-pairs, and 100G-capable RDMA-stack that can be used as the basis for building accelerators and smartNICs. RoCE BALBOA is customizable, opening up a design space and offering a degree of adaptability not available in commercial products. We have deployed BALBOA in a cluster using FPGAs and show that it has latency and performance characteristics comparable to commercial NICs. We demonstrate its potential by exploring two classes of use cases. One involves enhancements to the protocol for infrastructure purposes (encryption, deep packet inspection using ML). The other showcases the ability to perform line-rate compute offloads with deep pipelines by implementing commercial data preprocessing pipelines for recommender systems that process the data as it arrives from the network before transferring it directly to the GPU. These examples demonstrate how BALBOA enables the exploration and development of SmartNICs and accelerators operating on network data streams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20412v1</guid>
      <category>cs.AR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maximilian Jakob Heer, Benjamin Ramhorst, Yu Zhu, Luhao Liu, Zhiyi Hu, Jonas Dann, Gustavo Alonso</dc:creator>
    </item>
    <item>
      <title>Demystifying the 7-D Convolution Loop Nest for Data and Instruction Streaming in Reconfigurable AI Accelerators</title>
      <link>https://arxiv.org/abs/2507.20420</link>
      <description>arXiv:2507.20420v1 Announce Type: new 
Abstract: Convolution remains the most compute-intensive operation in AI acceleration, often constituting over 80-90% of the workload. Existing approaches in spatial architectures such as coarse-grained reconfigurable arrays (CGRAs) and field-programmable gate arrays (FPGAs) frequently rely on loop unrolling or GEMM-based matrix transformations, introducing significant overhead in both data movement and instruction control. This paper presents a new framework designed to systematically demystify the 7-dimensional convolution loop nest by reinterpreting it as a hardware-centric data and instruction streaming problem. Instead of treating the loop nest as a fixed computational construct, our approach exposes its structure as a set of spatial and temporal mappings governed by hardware parameters such as compute element distribution, interconnect topology, and reconfigurability. This abstraction supports lightweight, flexible deployment of convolution without reliance on heavyweight transformations or reordering schemes. We demonstrate the application of our approach on the MAVeC accelerator. We detail the implementation of convolution operations in MAVeC and extend the framework to support full model execution on VGG-16. Our profiling reveals high PE utilization (over 90%), significant fold reuse, and scalable throughput up to 1.56 TFLOPs/sec and 12.7 KIPS for end-to-end VGG-16 inference. These results validate the efficacy of our approach in minimizing control overhead, improving data locality, and enabling efficient large-scale convolution execution without reliance on conventional transformation-based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20420v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md Rownak Hossain Chowdhury, Mostafizur Rahman</dc:creator>
    </item>
    <item>
      <title>Smaller, Faster, Cheaper: Architectural Designs for Efficient Machine Learning</title>
      <link>https://arxiv.org/abs/2507.19795</link>
      <description>arXiv:2507.19795v1 Announce Type: cross 
Abstract: Major advancements in the capabilities of computer vision models have been primarily fueled by rapid expansion of datasets, model parameters, and computational budgets, leading to ever-increasing demands on computational infrastructure. However, as these models are deployed in increasingly diverse and resource-constrained environments, there is a pressing need for architectures that can deliver high performance while requiring fewer computational resources.
  This dissertation focuses on architectural principles through which models can achieve increased performance while reducing their computational demands. We discuss strides towards this goal through three directions. First, we focus on data ingress and egress, investigating how information may be passed into and retrieved from our core neural processing units. This ensures that our models make the most of available data, allowing smaller architectures to become more performant. Second, we investigate modifications to the core neural architecture, applied to restricted attention in vision transformers. This section explores how removing uniform context windows in restricted attention increases the expressivity of the underlying neural architecture. Third, we explore the natural structures of Normalizing Flows and how we can leverage these properties to better distill model knowledge.
  These contributions demonstrate that careful design of neural architectures can increase the efficiency of machine learning algorithms, allowing them to become smaller, faster, and cheaper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19795v1</guid>
      <category>cs.CV</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Steven Walton</dc:creator>
    </item>
    <item>
      <title>A Scalable Resource Management Layer for FPGA SoCs in 6G Radio Units</title>
      <link>https://arxiv.org/abs/2507.19963</link>
      <description>arXiv:2507.19963v1 Announce Type: cross 
Abstract: This work presents a perspective on addressing the underutilization of computing resources in FPGA SoC devices deployed in 5G radio and edge computing infrastructure. The initial step in this approach involves developing a resource management layer capable of dynamically migrating and scaling functions within these devices in response to contextual events. This layer serves as the foundation for designing a hierarchical, data-driven micro-orchestrator responsible for managing the lifecycle of functions in FPGA SoC devices. In this paper, the proposed resource management layer is utilized to reconfigure a function based on events identified by a computer vision edge application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19963v1</guid>
      <category>cs.NI</category>
      <category>cs.AR</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikolaos Bartzoudis, Jos\'e Rubio Fern\'andez, David L\'opez-Bueno, Antonio Rom\'an Villarroel</dc:creator>
    </item>
    <item>
      <title>ACCESS-AV: Adaptive Communication-Computation Codesign for Sustainable Autonomous Vehicle Localization in Smart Factories</title>
      <link>https://arxiv.org/abs/2507.20399</link>
      <description>arXiv:2507.20399v1 Announce Type: cross 
Abstract: Autonomous Delivery Vehicles (ADVs) are increasingly used for transporting goods in 5G network-enabled smart factories, with the compute-intensive localization module presenting a significant opportunity for optimization. We propose ACCESS-AV, an energy-efficient Vehicle-to-Infrastructure (V2I) localization framework that leverages existing 5G infrastructure in smart factory environments. By opportunistically accessing the periodically broadcast 5G Synchronization Signal Blocks (SSBs) for localization, ACCESS-AV obviates the need for dedicated Roadside Units (RSUs) or additional onboard sensors to achieve energy efficiency as well as cost reduction. We implement an Angle-of-Arrival (AoA)-based estimation method using the Multiple Signal Classification (MUSIC) algorithm, optimized for resource-constrained ADV platforms through an adaptive communication-computation strategy that dynamically balances energy consumption with localization accuracy based on environmental conditions such as Signal-to-Noise Ratio (SNR) and vehicle velocity. Experimental results demonstrate that ACCESS-AV achieves an average energy reduction of 43.09% compared to non-adaptive systems employing AoA algorithms such as vanilla MUSIC, ESPRIT, and Root-MUSIC. It maintains sub-30 cm localization accuracy while also delivering substantial reductions in infrastructure and operational costs, establishing its viability for sustainable smart factory environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20399v1</guid>
      <category>eess.SY</category>
      <category>cs.AR</category>
      <category>cs.NI</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajat Bhattacharjya, Arnab Sarkar, Ish Kool, Sabur Baidya, Nikil Dutt</dc:creator>
    </item>
    <item>
      <title>LUT Tensor Core: A Software-Hardware Co-Design for LUT-Based Low-Bit LLM Inference</title>
      <link>https://arxiv.org/abs/2408.06003</link>
      <description>arXiv:2408.06003v3 Announce Type: replace 
Abstract: Large Language Model (LLM) inference becomes resource-intensive, prompting a shift toward low-bit model weights to reduce the memory footprint and improve efficiency. Such low-bit LLMs necessitate the mixed-precision matrix multiplication (mpGEMM), an important yet underexplored operation involving the multiplication of lower-precision weights with higher-precision activations. Off-the-shelf hardware does not support this operation natively, leading to indirect, thus inefficient, dequantization-based implementations.
  In this paper, we study the lookup table (LUT)-based approach for mpGEMM and find that a conventional LUT implementation fails to achieve the promised gains. To unlock the full potential of LUT-based mpGEMM, we propose LUT Tensor Core, a software-hardware co-design for low-bit LLM inference. LUT Tensor Core differentiates itself from conventional LUT designs through: 1) software-based optimizations to minimize table precompute overhead and weight reinterpretation to reduce table storage; 2) a LUT-based Tensor Core hardware design with an elongated tiling shape to maximize table reuse and a bit-serial design to support diverse precision combinations in mpGEMM; 3) a new instruction set and compilation optimizations for LUT-based mpGEMM. LUT Tensor Core significantly outperforms existing pure software LUT implementations and achieves a 1.44$\times$ improvement in compute density and energy efficiency compared to previous state-of-the-art LUT-based accelerators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06003v3</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3695053.3731057</arxiv:DOI>
      <dc:creator>Zhiwen Mo, Lei Wang, Jianyu Wei, Zhichen Zeng, Shijie Cao, Lingxiao Ma, Naifeng Jing, Ting Cao, Jilong Xue, Fan Yang, Mao Yang</dc:creator>
    </item>
    <item>
      <title>FastMamba: A High-Speed and Efficient Mamba Accelerator on FPGA with Accurate Quantization</title>
      <link>https://arxiv.org/abs/2505.18975</link>
      <description>arXiv:2505.18975v4 Announce Type: replace 
Abstract: State Space Models (SSMs), like recent Mamba2, have achieved remarkable performance and received extensive attention. However, deploying Mamba2 on resource-constrained edge devices encounters many problems: severe outliers within the linear layer challenging the quantization, diverse and irregular element-wise tensor operations, and hardware-unfriendly nonlinear functions in the SSM block. To address these issues, this paper presents FastMamba, a dedicated accelerator on FPGA with hardware-algorithm co-design to promote the deployment efficiency of Mamba2. Specifically, we successfully achieve 8-bit quantization for linear layers through Hadamard transformation to eliminate outliers. Moreover, a hardware-friendly and fine-grained power-of-two quantization framework is presented for the SSM block and convolution layer, and a first-order linear approximation is developed to optimize the nonlinear functions. Based on the accurate algorithm quantization, we propose an accelerator that integrates parallel vector processing units, pipelined execution dataflow, and an efficient SSM Nonlinear Approximation Unit, which enhances computational efficiency and reduces hardware complexity. Finally, we evaluate FastMamba on Xilinx VC709 FPGA. For the input prefill task on Mamba2-130M, FastMamba achieves 68.80\times and 8.90\times speedup over Intel Xeon 4210R CPU and NVIDIA RTX 3090 GPU, respectively. In the output decode experiment with Mamba2-2.7B, FastMamba attains 6\times higher energy efficiency than RTX 3090 GPU.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18975v4</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aotao Wang, Haikuo Shao, Shaobo Ma, Zhongfeng Wang</dc:creator>
    </item>
    <item>
      <title>LaMAGIC2: Advanced Circuit Formulations for Language Model-Based Analog Topology Generation</title>
      <link>https://arxiv.org/abs/2506.10235</link>
      <description>arXiv:2506.10235v2 Announce Type: replace-cross 
Abstract: Automation of analog topology design is crucial due to customized requirements of modern applications with heavily manual engineering efforts. The state-of-the-art work applies a sequence-to-sequence approach and supervised finetuning on language models to generate topologies given user specifications. However, its circuit formulation is inefficient due to O(|V |2) token length and suffers from low precision sensitivity to numeric inputs. In this work, we introduce LaMAGIC2, a succinct float-input canonical formulation with identifier (SFCI) for language model-based analog topology generation. SFCI addresses these challenges by improving component-type recognition through identifier-based representations, reducing token length complexity to O(|V |), and enhancing numeric precision sensitivity for better performance under tight tolerances. Our experiments demonstrate that LaMAGIC2 achieves 34% higher success rates under a tight tolerance of 0.01 and 10X lower MSEs compared to a prior method. LaMAGIC2 also exhibits better transferability for circuits with more vertices with up to 58.5% improvement. These advancements establish LaMAGIC2 as a robust framework for analog topology generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10235v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 42st International Conference on Machine Learning, 2025, PMLR</arxiv:journal_reference>
      <dc:creator>Chen-Chia Chang, Wan-Hsuan Lin, Yikang Shen, Yiran Chen, Xin Zhang</dc:creator>
    </item>
  </channel>
</rss>
