<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 14 Aug 2024 01:38:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 13 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>AirChain: A Novel Blockchain Framework and Low-Cost Device for Democratized Air Quality Data Aggregation</title>
      <link>https://arxiv.org/abs/2408.05216</link>
      <description>arXiv:2408.05216v1 Announce Type: new 
Abstract: Air pollutant exposure kills over 6,700,000 people it per annum, yet there remains a systemic lack of accurate ground level data reporting the concentrations of the leading causes of such fatalities. Ambient particulate matter is a primary driver of this effect. Namely, PM1.0, PM2.5, and PM10.0 display a systemic lack of accurate and high-definition reporting. This project suggests and implements a prototype for a distributed and low cost model for reporting such data and designs a novel framework in order to remedy three main shortfalls of previously implemented systems. First, their central operation and distribution, and therefore their requirement of trust in a central governing body. Second, their requirement of the purchase of comparatively high-cost devices for ordinary consumers. Finally, their high degree of error and accordingly low functional certainty. This project explores the creation of AirChain, a prototype system utilizing blockchain technology that will demonstrate the effectiveness of low-cost sensors when paired with simple microcontroller devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05216v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Stankiewicz</dc:creator>
    </item>
    <item>
      <title>SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference Serving</title>
      <link>https://arxiv.org/abs/2408.05235</link>
      <description>arXiv:2408.05235v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) gain traction, their reliance on power-hungry GPUs places ever-increasing energy demands, raising environmental and monetary concerns. Inference dominates LLM workloads, presenting a critical challenge for providers: minimizing energy costs under Service-Level Objectives (SLOs) that ensure optimal user experience. In this paper, we present \textit{throttLL'eM}, a framework that reduces energy consumption while meeting SLOs through the use of instance and GPU frequency scaling. \textit{throttLL'eM} features mechanisms that project future KV cache usage and batch size. Leveraging a Machine-Learning (ML) model that receives these projections as inputs, \textit{throttLL'eM} manages performance at the iteration level to satisfy SLOs with reduced frequencies and instance sizes. We show that the proposed ML model achieves $R^2$ scores greater than 0.97 and miss-predicts performance by less than 1 iteration per second on average. Experimental results on LLM inference traces show that \textit{throttLL'eM} achieves up to 43.8\% lower energy consumption and an energy efficiency improvement of at least $1.71\times$ under SLOs, when compared to NVIDIA's Triton server.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05235v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Andreas Kosmas Kakolyris, Dimosthenis Masouros, Petros Vavaroutsos, Sotirios Xydis, Dimitrios Soudris</dc:creator>
    </item>
    <item>
      <title>Solving Large Rank-Deficient Linear Least-Squares Problems on Shared-Memory CPU Architectures and GPU Architectures</title>
      <link>https://arxiv.org/abs/2408.05238</link>
      <description>arXiv:2408.05238v1 Announce Type: new 
Abstract: Solving very large linear systems of equations is a key computational task in science and technology. In many cases, the coefficient matrix of the linear system is rank-deficient, leading to systems that may be underdetermined, inconsistent, or both. In such cases, one generally seeks to compute the least squares solution that minimizes the residual of the problem, which can be further defined as the solution with smallest norm in cases where the coefficient matrix has a nontrivial nullspace. This work presents several new techniques for solving least squares problems involving coefficient matrices that are so large that they do not fit in main memory. The implementations include both CPU and GPU variants. All techniques rely on complete orthogonal decompositions that guarantee that both conditions of a least squares solution are met, regardless of the rank properties of the matrix. Specifically, they rely on the recently proposed "randUTV" algorithm that is particularly effective in strongly communication-constrained environments. A detailed precision and performance study reveals that the new methods, that operate on data stored on disk, are competitive with state-of-the-art methods that store all data in main memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05238v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M\'onica Chillar\'on, Gregorio Quintana-Ort\'i, Vicente Vidal, Per-Gunnar Martinsson</dc:creator>
    </item>
    <item>
      <title>Early-Exit meets Model-Distributed Inference at Edge Networks</title>
      <link>https://arxiv.org/abs/2408.05247</link>
      <description>arXiv:2408.05247v1 Announce Type: new 
Abstract: Distributed inference techniques can be broadly classified into data-distributed and model-distributed schemes. In data-distributed inference (DDI), each worker carries the entire deep neural network (DNN) model but processes only a subset of the data. However, feeding the data to workers results in high communication costs, especially when the data is large. An emerging paradigm is model-distributed inference (MDI), where each worker carries only a subset of DNN layers. In MDI, a source device that has data processes a few layers of DNN and sends the output to a neighboring device, i.e., offloads the rest of the layers. This process ends when all layers are processed in a distributed manner. In this paper, we investigate the design and development of MDI with early-exit, which advocates that there is no need to process all the layers of a model for some data to reach the desired accuracy, i.e., we can exit the model without processing all the layers if target accuracy is reached. We design a framework MDI-Exit that adaptively determines early-exit and offloading policies as well as data admission at the source. Experimental results on a real-life testbed of NVIDIA Nano edge devices show that MDI-Exit processes more data when accuracy is fixed and results in higher accuracy for the fixed data rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05247v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Colocrese, Erdem Koyuncu, Hulya Seferoglu</dc:creator>
    </item>
    <item>
      <title>Optimal Dispersion of Silent Robots in a Ring</title>
      <link>https://arxiv.org/abs/2408.05491</link>
      <description>arXiv:2408.05491v1 Announce Type: new 
Abstract: Given a set of co-located mobile robots in an unknown anonymous graph, the robots must relocate themselves in distinct graph nodes to solve the dispersion problem. In this paper, we consider the dispersion problem for silent robots \cite{gorain2024collaborative}, i.e., no direct, explicit communication between any two robots placed in the nodes of an oriented $n$ node ring network. The robots operate in synchronous rounds. The dispersion problem for silent mobile robots has been studied in arbitrary graphs where the robots start from a single source. In this paper, we focus on the dispersion problem for silent mobile robots where robots can start from multiple sources. The robots have unique labels from a range $[0,\;L]$ for some positive integer $L$. Any two co-located robots do not have the information about the label of the other robot. The robots have weak multiplicity detection capability, which means they can determine if it is alone on a node. The robots are assumed to be able to identify an increase or decrease in the number of robots present on a node in a particular round. However, the robots can not get the exact number of increase or decrease in the number of robots. We have proposed a deterministic distributed algorithm that solves the dispersion of $k$ robots in an oriented ring in $O(\log L+k)$ synchronous rounds with $O(\log L)$ bits of memory for each robot. A lower bound $\Omega(\log L+k)$ on time for the dispersion of $k$ robots on a ring network is presented to establish the optimality of the proposed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05491v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bibhuti Das, Barun Gorain, Kaushik Mondal, Krishnendu Mukhopadhyaya, Supantha Pandit</dc:creator>
    </item>
    <item>
      <title>Asynchronous Approximate Agreement with Quadratic Communication</title>
      <link>https://arxiv.org/abs/2408.05495</link>
      <description>arXiv:2408.05495v1 Announce Type: new 
Abstract: We consider an asynchronous network of $n$ message-sending parties, up to $t$ of which are byzantine. We study approximate agreement, where the parties obtain approximately equal outputs in the convex hull of their inputs. The seminal protocol of Abraham, Amit and Dolev [OPODIS '04] achieves approximate agreement in $\mathbb{R}$ with the optimal resilience $t &lt; \frac{n}{3}$ by making each party reliably broadcast its input. This takes $\Omega(n^2)$ messages per reliable broadcast, or $\Omega(n^3)$ messages in total.
  In this work, we present optimally resilient asynchronous approximate agreement protocols which forgo reliable broadcast and thus require communication proportional to $n^2$ instead of $n^3$. First, we achieve $\omega$-dimensional barycentric agreement with $\mathcal{O}(\omega n^2)$ small messages. Then, we achieve edge agreement in a tree of diameter $D$ with $\lceil \log_2 D \rceil$ iterations of a multivalued graded consensus variant for which we design an efficient protocol. This results in a $\mathcal{O}(\log\frac{1}{\varepsilon})$-round protocol for $\varepsilon$-agreement in $[0, 1]$ with $\mathcal{O}(n^2\log\frac{1}{\varepsilon})$ messages and $\mathcal{O}(n^2\log\frac{1}{\varepsilon}\log\log\frac{1}{\varepsilon})$ bits of communication, improving over the state of the art which matches this complexity only when the inputs are all either $0$ or $1$. Finally, we extend our edge agreement protocol to achieve edge agreement in $\mathbb{Z}$ and thus $\varepsilon$-agreement in $\mathbb{R}$ with quadratic communication, in $\mathcal{O}(\log\frac{M}{\varepsilon})$ rounds where $M$ is the maximum honest input magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05495v1</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mose Mizrahi Erbes, Roger Wattenhofer</dc:creator>
    </item>
    <item>
      <title>LLMServingSim: A HW/SW Co-Simulation Infrastructure for LLM Inference Serving at Scale</title>
      <link>https://arxiv.org/abs/2408.05499</link>
      <description>arXiv:2408.05499v1 Announce Type: new 
Abstract: Recently, there has been an extensive research effort in building efficient large language model (LLM) inference serving systems. These efforts not only include innovations in the algorithm and software domains but also constitute developments of various hardware acceleration techniques. Nevertheless, there is a lack of simulation infrastructure capable of accurately modeling versatile hardware-software behaviors in LLM serving systems without extensively extending the simulation time. This paper aims to develop an effective simulation tool, called LLMServingSim, to support future research in LLM serving systems. In designing LLMServingSim, we focus on two limitations of existing simulators: (1) they lack consideration of the dynamic workload variations of LLM inference serving due to its autoregressive nature, and (2) they incur repetitive simulations without leveraging algorithmic redundancies in LLMs. To address these limitations, LLMServingSim simulates the LLM serving in the granularity of iterations, leveraging the computation redundancies across decoder blocks and reusing the simulation results from previous iterations. Additionally, LLMServingSim provides a flexible framework that allows users to plug in any accelerator compiler-and-simulation stacks for exploring various system designs with heterogeneous processors. Our experiments demonstrate that LLMServingSim produces simulation results closely following the performance behaviors of real GPU-based LLM serving system with less than 14.7% error rate, while offering 91.5x faster simulation speed compared to existing accelerator simulators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05499v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaehong Cho, Minsu Kim, Hyunmin Choi, Guseul Heo, Jongse Park</dc:creator>
    </item>
    <item>
      <title>Efficient Federated Learning Using Dynamic Update and Adaptive Pruning with Momentum on Shared Server Data</title>
      <link>https://arxiv.org/abs/2408.05678</link>
      <description>arXiv:2408.05678v1 Announce Type: new 
Abstract: Despite achieving remarkable performance, Federated Learning (FL) encounters two important problems, i.e., low training efficiency and limited computational resources. In this paper, we propose a new FL framework, i.e., FedDUMAP, with three original contributions, to leverage the shared insensitive data on the server in addition to the distributed data in edge devices so as to efficiently train a global model. First, we propose a simple dynamic server update algorithm, which takes advantage of the shared insensitive data on the server while dynamically adjusting the update steps on the server in order to speed up the convergence and improve the accuracy. Second, we propose an adaptive optimization method with the dynamic server update algorithm to exploit the global momentum on the server and each local device for superior accuracy. Third, we develop a layer-adaptive model pruning method to carry out specific pruning operations, which is adapted to the diverse features of each layer so as to attain an excellent trade-off between effectiveness and efficiency. Our proposed FL model, FedDUMAP, combines the three original techniques and has a significantly better performance compared with baseline approaches in terms of efficiency (up to 16.9 times faster), accuracy (up to 20.4% higher), and computational cost (up to 62.6% smaller).</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05678v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ji Liu, Juncheng Jia, Hong Zhang, Yuhui Yun, Leye Wang, Yang Zhou, Huaiyu Dai, Dejing Dou</dc:creator>
    </item>
    <item>
      <title>HiCCL: A Hierarchical Collective Communication Library</title>
      <link>https://arxiv.org/abs/2408.05962</link>
      <description>arXiv:2408.05962v1 Announce Type: new 
Abstract: HiCCL (Hierarchical Collective Communication Library) addresses the growing complexity and diversity in high-performance network architectures. As GPU systems have envolved into networks of GPUs with different multilevel communication hierarchies, optimizing each collective function for a specific system has become a challenging task. Consequently, many collective libraries struggle to adapt to different hardware and software, especially across systems from different vendors. HiCCL's library design decouples the collective communication logic from network-specific optimizations through a compositional API. The communication logic is composed using multicast, reduction, and fence primitives, which are then factorized for a specified network hieararchy using only point-to-point operations within a level. Finally, striping and pipelining optimizations applied as specified for streamlining the execution.
  Performance evaluation of HiCCL across four different machines$\unicode{x2014}$two with Nvidia GPUs, one with AMD GPUs, and one with Intel GPUs$\unicode{x2014}$demonstrates an average 17$\times$ higher throughput than the collectives of highly specialized GPU-aware MPI implementations, and competitive throughput with those of vendor-specific libraries (NCCL, RCCL, and OneCCL), while providing portability across all four machines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05962v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mert Hidayetoglu, Simon Garcia de Gonzalo, Elliott Slaughter, Pinku Surana, Wen-mei Hwu, William Gropp, Alex Aiken</dc:creator>
    </item>
    <item>
      <title>FaasMeter: Energy-First Serverless Computing</title>
      <link>https://arxiv.org/abs/2408.06130</link>
      <description>arXiv:2408.06130v1 Announce Type: new 
Abstract: Functions as a Service has emerged as a popular abstraction for a wide range of cloud applications and an important cloud workload. We present the design and implementation of FaasMeter, a FaaS control plane which provides energy monitoring, accounting, control, and pricing as first-class operations. The highly diverse and dynamic workloads of FaaS create additional complexity to measuring and controlling energy usage which FaasMeter can mitigate.
  We develop a new statistical energy disaggregation approach to provide accurate and complete energy footprints for functions, despite using noisy and coarse-grained system-level power (not just CPU power readings). Our accurate and robust footprints are achieved by combining conventional power models with Kalman filters and Shapley values. FaasMeter is a full-spectrum energy profiler, and fairly attributes energy of shared resources to functions (such as energy used by the control plane itself). We develop new energy profiling validation metrics, and show that FaasMeter's energy footprints are accurate to within 1\% of carefully obtained marginal energy ground truth measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06130v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Abdul Rehman, Alexander Fuerst, Prateek Sharma</dc:creator>
    </item>
    <item>
      <title>Blockchain and Artificial Intelligence: Synergies and Conflicts</title>
      <link>https://arxiv.org/abs/2405.13462</link>
      <description>arXiv:2405.13462v1 Announce Type: cross 
Abstract: Blockchain technology and Artificial Intelligence (AI) have emerged as transformative forces in their respective domains. This paper explores synergies and challenges between these two technologies. Our research analyses the biggest projects combining blockchain and AI, based on market capitalization, and derives a novel framework to categorize contemporary and future use cases. Despite the theoretical compatibility, current real-world applications combining blockchain and AI remain in their infancy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13462v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leon Witt, Armando Teles Fortes, Kentaroh Toyoda, Wojciech Samek, Dan Li</dc:creator>
    </item>
    <item>
      <title>FLASH: Federated Learning-Based LLMs for Advanced Query Processing in Social Networks through RAG</title>
      <link>https://arxiv.org/abs/2408.05242</link>
      <description>arXiv:2408.05242v1 Announce Type: cross 
Abstract: Our paper introduces a novel approach to social network information retrieval and user engagement through a personalized chatbot system empowered by Federated Learning GPT. The system is designed to seamlessly aggregate and curate diverse social media data sources, including user posts, multimedia content, and trending news. Leveraging Federated Learning techniques, the GPT model is trained on decentralized data sources to ensure privacy and security while providing personalized insights and recommendations. Users interact with the chatbot through an intuitive interface, accessing tailored information and real-time updates on social media trends and user-generated content. The system's innovative architecture enables efficient processing of input files, parsing and enriching text data with metadata, and generating relevant questions and answers using advanced language models. By facilitating interactive access to a wealth of social network information, this personalized chatbot system represents a significant advancement in social media communication and knowledge dissemination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05242v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.IR</category>
      <category>cs.SI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sai Puppala, Ismail Hossain, Md Jahangir Alam, Sajedul Talukder</dc:creator>
    </item>
    <item>
      <title>SocFedGPT: Federated GPT-based Adaptive Content Filtering System Leveraging User Interactions in Social Networks</title>
      <link>https://arxiv.org/abs/2408.05243</link>
      <description>arXiv:2408.05243v1 Announce Type: cross 
Abstract: Our study presents a multifaceted approach to enhancing user interaction and content relevance in social media platforms through a federated learning framework. We introduce personalized GPT and Context-based Social Media LLM models, utilizing federated learning for privacy and security. Four client entities receive a base GPT-2 model and locally collected social media data, with federated aggregation ensuring up-to-date model maintenance. Subsequent modules focus on categorizing user posts, computing user persona scores, and identifying relevant posts from friends' lists. A quantifying social engagement approach, coupled with matrix factorization techniques, facilitates personalized content suggestions in real-time. An adaptive feedback loop and readability score algorithm also enhance the quality and relevance of content presented to users. Our system offers a comprehensive solution to content filtering and recommendation, fostering a tailored and engaging social media experience while safeguarding user privacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05243v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.IR</category>
      <category>cs.SI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sai Puppala, Ismail Hossain, Md Jahangir Alam, Sajedul Talukder</dc:creator>
    </item>
    <item>
      <title>Competitive Capacitated Online Recoloring</title>
      <link>https://arxiv.org/abs/2408.05370</link>
      <description>arXiv:2408.05370v1 Announce Type: cross 
Abstract: In this paper, we revisit the online recoloring problem introduced recently by Azar et al. In online recoloring, there is a fixed set $V$ of $n$ vertices and an initial coloring $c_0: V\rightarrow [k]$ for some $k\in \mathbb{Z}^{&gt;0}$. Under an online sequence $\sigma$ of requests where each request is an edge $(u_t,v_t)$, a proper vertex coloring $c$ of the graph $G_t$ induced by requests until time $t$ needs to be maintained for all $t$; i.e., for any $(u,v)\in G_t$, $c(u)\neq c(v)$. The objective is to minimize the total weight of vertices recolored for the sequence $\sigma$.
  We obtain the first competitive algorithms for capacitated online recoloring and fully dynamic recoloring. Our first set of results is for $2$-recoloring using algorithms that are $(1+\varepsilon)$-resource augmented where $\varepsilon\in (0,1)$ is an arbitrarily small constant. Our main result is an $O(\log n)$-competitive deterministic algorithm for weighted bipartite graphs, which is asymptotically optimal in light of an $\Omega(\log n)$ lower bound that holds for an unbounded amount of augmentation. We also present an $O(n\log n)$-competitive deterministic algorithm for fully dynamic recoloring, which is optimal within an $O(\log n)$ factor in light of a $\Omega(n)$ lower bound that holds for an unbounded amount of augmentation.
  Our second set of results is for $\Delta$-recoloring in an $(1+\varepsilon)$-overprovisioned setting where the maximum degree of $G_t$ is bounded by $(1-\varepsilon)\Delta$ for all $t$, and each color assigned to at most $(1+\varepsilon)\frac{n}{\Delta}$ vertices, for an arbitrary $\varepsilon &gt; 0$. Our main result is an $O(1)$-competitive randomized algorithm for $\Delta = O(\sqrt{n/\log n})$. We also present an $O(\Delta)$-competitive deterministic algorithm for $\Delta \le \varepsilon n/2$. Both results are asymptotically optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05370v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajmohan Rajaraman, Omer Wasim</dc:creator>
    </item>
    <item>
      <title>Accelerating In-transit Isosurface Generation With Topology Preserving Compression</title>
      <link>https://arxiv.org/abs/2408.05462</link>
      <description>arXiv:2408.05462v1 Announce Type: cross 
Abstract: Data visualization through isosurface generation is critical in various scientific fields, including computational fluid dynamics, medical imaging, and geophysics. However, the high cost of data sharing between simulation sources and visualization resources poses a significant challenge. This paper introduces a novel framework that leverages lossy compression to accelerate in-transit isosurface generation. Our approach involves a Compressed Hierarchical Representation (CHR) and topology-preserving compression to ensure the fidelity of the isosurface generation. Experimental evaluations demonstrate that our framework can achieve up to 4x speedup in visualization workflows, making it a promising solution for real-time scientific data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05462v1</guid>
      <category>cs.GR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanliang Li, Jieyang Chen</dc:creator>
    </item>
    <item>
      <title>Residual-INR: Communication Efficient On-Device Learning Using Implicit Neural Representation</title>
      <link>https://arxiv.org/abs/2408.05617</link>
      <description>arXiv:2408.05617v1 Announce Type: cross 
Abstract: Edge computing is a distributed computing paradigm that collects and processes data at or near the source of data generation. The on-device learning at edge relies on device-to-device wireless communication to facilitate real-time data sharing and collaborative decision-making among multiple devices. This significantly improves the adaptability of the edge computing system to the changing environments. However, as the scale of the edge computing system is getting larger, communication among devices is becoming the bottleneck because of the limited bandwidth of wireless communication leads to large data transfer latency. To reduce the amount of device-to-device data transmission and accelerate on-device learning, in this paper, we propose Residual-INR, a fog computing-based communication-efficient on-device learning framework by utilizing implicit neural representation (INR) to compress images/videos into neural network weights. Residual-INR enhances data transfer efficiency by collecting JPEG images from edge devices, compressing them into INR format at the fog node, and redistributing them for on-device learning. By using a smaller INR for full image encoding and a separate object INR for high-quality object region reconstruction through residual encoding, our technique can reduce the encoding redundancy while maintaining the object quality. Residual-INR is a promising solution for edge on-device learning because it reduces data transmission by up to 5.16 x across a network of 10 edge devices. It also facilitates CPU-free accelerated on-device learning, achieving up to 2.9 x speedup without sacrificing accuracy. Our code is available at: https://github.com/sharclab/Residual-INR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05617v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanqiu Chen, Xuebin Yao, Pradeep Subedi, Cong Hao</dc:creator>
    </item>
    <item>
      <title>Lancelot: Towards Efficient and Privacy-Preserving Byzantine-Robust Federated Learning within Fully Homomorphic Encryption</title>
      <link>https://arxiv.org/abs/2408.06197</link>
      <description>arXiv:2408.06197v1 Announce Type: cross 
Abstract: In sectors such as finance and healthcare, where data governance is subject to rigorous regulatory requirements, the exchange and utilization of data are particularly challenging. Federated Learning (FL) has risen as a pioneering distributed machine learning paradigm that enables collaborative model training across multiple institutions while maintaining data decentralization. Despite its advantages, FL is vulnerable to adversarial threats, particularly poisoning attacks during model aggregation, a process typically managed by a central server. However, in these systems, neural network models still possess the capacity to inadvertently memorize and potentially expose individual training instances. This presents a significant privacy risk, as attackers could reconstruct private data by leveraging the information contained in the model itself. Existing solutions fall short of providing a viable, privacy-preserving BRFL system that is both completely secure against information leakage and computationally efficient. To address these concerns, we propose Lancelot, an innovative and computationally efficient BRFL framework that employs fully homomorphic encryption (FHE) to safeguard against malicious client activities while preserving data privacy. Our extensive testing, which includes medical imaging diagnostics and widely-used public image datasets, demonstrates that Lancelot significantly outperforms existing methods, offering more than a twenty-fold increase in processing speed, all while maintaining data privacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06197v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siyang Jiang, Hao Yang, Qipeng Xie, Chuan Ma, Sen Wang, Guoliang Xing</dc:creator>
    </item>
    <item>
      <title>Decentralized Intelligence Health Network (DIHN)</title>
      <link>https://arxiv.org/abs/2408.06240</link>
      <description>arXiv:2408.06240v2 Announce Type: cross 
Abstract: Decentralized Health Intelligence Network (DHIN) is a theoretical framework addressing significant challenges of health data sovereignty and AI utilization in healthcare caused by data fragmentation across providers and institutions. It establishes a sovereign architecture for healthcare provision as a prerequisite to a sovereign health network, then facilitates effective AI utilization by overcoming barriers to accessing diverse medical data sources. This comprehensive framework leverages: 1) self-sovereign identity architecture coupled with a personal health record (PHR) as a prerequisite for health data sovereignty; 2) a scalable federated learning (FL) protocol implemented on a public blockchain for decentralized AI training in healthcare, where health data remains with participants and only model parameter updates are shared; and 3) a scalable, trustless rewards mechanism to incentivize participation and ensure fair reward distribution. This framework ensures that no entity can prevent or control access to training on health data offered by participants or determine financial benefits, as these processes operate on a public blockchain with an immutable record and without a third party. It supports effective AI training in healthcare, allowing patients to maintain control over their health data, benefit financially, and contribute to a decentralized, scalable ecosystem that leverages collective AI to develop beneficial healthcare algorithms. Patients receive rewards into their digital wallets as an incentive to opt-in to the FL protocol, with a long-term roadmap to funding decentralized insurance solutions. This approach introduces a novel, self-financed healthcare model that adapts to individual needs, complements existing systems, and redefines universal coverage. It highlights the potential to transform healthcare data management and AI utilization while empowering patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06240v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abraham Nash</dc:creator>
    </item>
    <item>
      <title>MoESys: A Distributed and Efficient Mixture-of-Experts Training and Inference System for Internet Services</title>
      <link>https://arxiv.org/abs/2205.10034</link>
      <description>arXiv:2205.10034v3 Announce Type: replace 
Abstract: While modern internet services, such as chatbots, search engines, and online advertising, demand the use of large-scale deep neural networks (DNNs), distributed training and inference over heterogeneous computing systems are desired to facilitate these DNN models. Mixture-of-Experts (MoE) is one the most common strategies to lower the cost of training subject to the overall size of models/data through gating and parallelism in a divide-and-conquer fashion. While DeepSpeed has made efforts in carrying out large-scale MoE training over heterogeneous infrastructures, the efficiency of training and inference could be further improved from several system aspects, including load balancing, communication/computation efficiency, and memory footprint limits. In this work, we present a novel MoESys that boosts efficiency in both large-scale training and inference. Specifically, in the training procedure, the proposed MoESys adopts an Elastic MoE training strategy with 2D prefetch and Fusion communication over Hierarchical storage, so as to enjoy efficient parallelisms. For scalable inference in a single node, especially when the model size is larger than GPU memory, MoESys builds the CPU-GPU memory jointly into a ring of sections to load the model, and executes the computation tasks across the memory sections in a round-robin manner for efficient inference. We carried out extensive experiments to evaluate MoESys, where MoESys successfully trains a Unified Feature Optimization (UFO) model with a Sparsely-Gated Mixture-of-Experts model of 12B parameters in 8 days on 48 A100 GPU cards. The comparison against the state-of-the-art shows that MoESys outperformed DeepSpeed with 33% higher throughput (tokens per second) in training and 13% higher throughput in inference in general. Particularly, under unbalanced MoE Tasks, e.g., UFO, MoESys achieved 64% higher throughput with 18% lower memory footprints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.10034v3</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dianhai Yu, Liang Shen, Hongxiang Hao, Weibao Gong, Huachao Wu, Jiang Bian, Lirong Dai, Haoyi Xiong</dc:creator>
    </item>
    <item>
      <title>An Experimental Comparison of Partitioning Strategies for Distributed Graph Neural Network Training</title>
      <link>https://arxiv.org/abs/2308.15602</link>
      <description>arXiv:2308.15602v2 Announce Type: replace 
Abstract: Recently, graph neural networks (GNNs) have gained much attention as a growing area of deep learning capable of learning on graph-structured data. However, the computational and memory requirements for training GNNs on large-scale graphs make it necessary to distribute the training. A prerequisite for distributed GNN training is to partition the input graph into smaller parts that are distributed among multiple machines of a compute cluster. Although graph partitioning has been studied with regard to graph analytics and graph databases, its effect on GNN training performance is largely unexplored. As a consequence, it is unclear whether investing computational efforts into high-quality graph partitioning would pay off in GNN training scenarios.
  In this paper, we study the effectiveness of graph partitioning for distributed GNN training. Our study aims to understand how different factors such as GNN parameters, mini-batch size, graph type, features size, and scale-out factor influence the effectiveness of graph partitioning. We conduct experiments with two different GNN systems using vertex and edge partitioning. We found that high-quality graph partitioning is a very effective optimization to speed up GNN training and to reduce memory consumption. Furthermore, our results show that invested partitioning time can quickly be amortized by reduced GNN training time, making it a relevant optimization for most GNN scenarios. Compared to research on distributed graph processing, our study reveals that graph partitioning plays an even more significant role in distributed GNN training, which motivates further research on the graph partitioning problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.15602v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.48786/edbt.2025.14</arxiv:DOI>
      <dc:creator>Nikolai Merkel, Daniel Stoll, Ruben Mayer, Hans-Arno Jacobsen</dc:creator>
    </item>
    <item>
      <title>Sui Lutris: A Blockchain Combining Broadcast and Consensus</title>
      <link>https://arxiv.org/abs/2310.18042</link>
      <description>arXiv:2310.18042v4 Announce Type: replace 
Abstract: Sui Lutris is the first smart-contract platform to sustainably achieve sub-second finality. It achieves this significant decrease by employing consensusless agreement not only for simple payments but for a large variety of transactions. Unlike prior work, Sui Lutris neither compromises expressiveness nor throughput and can run perpetually without restarts. Sui Lutris achieves this by safely integrating consensuless agreement with a high-throughput consensus protocol that is invoked out of the critical finality path but ensures that when a transaction is at risk of inconsistent concurrent accesses, its settlement is delayed until the total ordering is resolved. Building such a hybrid architecture is especially delicate during reconfiguration events, where the system needs to preserve the safety of the consensusless path without compromising the long-term liveness of potentially misconfigured clients. We thus develop a novel reconfiguration protocol, the first to provably show the safe and efficient reconfiguration of a consensusless blockchain. Sui Lutris is currently running in production and underpins the Sui smart-contract platform. Combined with the use of Objects instead of accounts it enables the safe execution of smart contracts that expose objects as a first-class resource. In our experiments Sui Lutris achieves latency lower than 0.5 seconds for throughput up to 5,000 certificates per second (150k ops/s with transaction blocks), compared to the state-of-the-art real-world consensus latencies of 3 seconds. Furthermore, it gracefully handles validators crash-recovery and does not suffer visible performance degradation during reconfiguration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18042v4</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sam Blackshear, Andrey Chursin, George Danezis, Anastasios Kichidis, Lefteris Kokoris-Kogias, Xun Li, Mark Logan, Ashok Menon, Todd Nowacki, Alberto Sonnino, Brandon Williams, Lu Zhang</dc:creator>
    </item>
    <item>
      <title>IoT in the Era of Generative AI: Vision and Challenges</title>
      <link>https://arxiv.org/abs/2401.01923</link>
      <description>arXiv:2401.01923v3 Announce Type: replace 
Abstract: Advancements in Generative AI hold immense promise to push Internet of Things (IoT) to the next level. In this article, we share our vision on IoT in the era of Generative AI. We discuss some of the most important applications of Generative AI in IoT-related domains. We also identify some of the most critical challenges and discuss current gaps as well as promising opportunities on enabling Generative AI for IoT. We hope this article can inspire new research on IoT in the era of Generative AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01923v3</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Wang, Zhongwei Wan, Arvin Hekmati, Mingyu Zong, Samiul Alam, Mi Zhang, Bhaskar Krishnamachari</dc:creator>
    </item>
    <item>
      <title>Evaluation of Programming Models and Performance for Stencil Computation on Current GPU Architectures</title>
      <link>https://arxiv.org/abs/2404.04441</link>
      <description>arXiv:2404.04441v2 Announce Type: replace 
Abstract: Accelerated computing is widely used in high-performance computing. Therefore, it is crucial to experiment and discover how to better utilize GPUGPUs latest generations on relevant applications. In this paper, we present results and share insights about highly tuned stencil-based kernels for NVIDIA Ampere (A100) and Hopper (GH200) architectures. Performance results yield useful insights into the behavior of this type of algorithms for these new accelerators. This knowledge can be leveraged by many scientific applications which involve stencils computations. Further, evaluation of three different programming models: CUDA, OpenACC, and OpenMP target offloading is conducted on aforementioned accelerators. We extensively study the performance and portability of various kernels under each programming model and provide corresponding optimization recommendations. Furthermore, we compare the performance of different programming models on the mentioned architectures. Up to 58% performance improvement was achieved against the previous GPGPU's architecture generation for an highly optimized kernel of the same class, and up to 42% for all classes. In terms of programming models, and keeping portability in mind, optimized OpenACC implementation outperforms OpenMP implementation by 33%. If portability is not a factor, our best tuned CUDA implementation outperforms the optimized OpenACC one by 2.1x.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04441v2</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Baodi Shan, Mauricio Araya-Polo</dc:creator>
    </item>
    <item>
      <title>SAMM: Sharded Automated Market Maker</title>
      <link>https://arxiv.org/abs/2406.05568</link>
      <description>arXiv:2406.05568v3 Announce Type: replace 
Abstract: Automated Market Makers (AMMs) are a cornerstone of decentralized finance (DeFi) blockchain-based platforms. They enable direct exchange of virtual tokens: Traders exchange tokens with the AMM, paying a fee; liquidity comes from liquidity providers, paid by those fees. Despite growing demand, the performance of AMMs is limited. State-of-the-art blockchain platforms allow for parallel execution of transactions. However, we show that AMMs do not enjoy these gains since their operations are not parallelizable.
  We present SAMM, an AMM comprising multiple independent shards. All shards operate in the same chain, but they allow for parallel execution as each is independent. The challenge is that traders are incentivized to split each trade among all AMMs in existing designs, leading to lower throughput. SAMM addresses this issue with a novel design of the trading fees. Traders are incentivized to use only a single smallest shard. We show that all Subgame-Perfect Nash Equilibria (SPNE) fit the desired behavior: Liquidity providers balance the liquidity among all shards, so the system converges to the state where trades are evenly distributed, overcoming destabilization attacks.
  Evaluation in the Sui and Solana blockchains shows that SAMM improves throughput by 5 and by 16, respectively, approaching their limit. SAMM is directly deployable, allowing trading at scale for individuals and DeFi applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05568v3</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongyin Chen, Amit Vaisman, Ittay Eyal</dc:creator>
    </item>
    <item>
      <title>Collaborative Learning Framework to Detect Attacks in Transactions and Smart Contracts</title>
      <link>https://arxiv.org/abs/2308.15804</link>
      <description>arXiv:2308.15804v3 Announce Type: replace-cross 
Abstract: With the escalating prevalence of malicious activities exploiting vulnerabilities in blockchain systems, there is an urgent requirement for robust attack detection mechanisms. To address this challenge, this paper presents a novel collaborative learning framework designed to detect attacks in blockchain transactions and smart contracts by analyzing transaction features. Our framework exhibits the capability to classify various types of blockchain attacks, including intricate attacks at the machine code level (e.g., injecting malicious codes to withdraw coins from users unlawfully), which typically necessitate significant time and security expertise to detect. To achieve that, the proposed framework incorporates a unique tool that transforms transaction features into visual representations, facilitating efficient analysis and classification of low-level machine codes. Furthermore, we propose an advanced collaborative learning model to enable real-time detection of diverse attack types at distributed mining nodes. Our model can efficiently detect attacks in smart contracts and transactions for blockchain systems without the need to gather all data from mining nodes into a centralized server. In order to evaluate the performance of our proposed framework, we deploy a pilot system based on a private Ethereum network and conduct multiple attack scenarios to generate a novel dataset. To the best of our knowledge, our dataset is the most comprehensive and diverse collection of transactions and smart contracts synthesized in a laboratory for cyberattack detection in blockchain systems. Our framework achieves a detection accuracy of approximately 94% through extensive simulations and 91% in real-time experiments with a throughput of over 2,150 transactions per second.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.15804v3</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tran Viet Khoa, Do Hai Son, Chi-Hieu Nguyen, Dinh Thai Hoang, Diep N. Nguyen, Tran Thi Thuy Quynh, Trong-Minh Hoang, Nguyen Viet Ha, Eryk Dutkiewicz, Abu Alsheikh, Nguyen Linh Trung</dc:creator>
    </item>
    <item>
      <title>Efficient Quantum Circuit Simulation by Tensor Network Methods on Modern GPUs</title>
      <link>https://arxiv.org/abs/2310.03978</link>
      <description>arXiv:2310.03978v2 Announce Type: replace-cross 
Abstract: Efficient simulation of quantum circuits has become indispensable with the rapid development of quantum hardware. The primary simulation methods are based on state vectors and tensor networks. As the number of qubits and quantum gates grows larger in current quantum devices, traditional state-vector based quantum circuit simulation methods prove inadequate due to the overwhelming size of the Hilbert space and extensive entanglement. Consequently, brutal force tensor network simulation algorithms become the only viable solution in such scenarios. The two main challenges faced in tensor network simulation algorithms are optimal contraction path finding and efficient execution on modern computing devices, with the latter determines the actual efficiency. In this study, we investigate the optimization of such tensor network simulations on modern GPUs and propose general optimization strategies from two aspects: computational efficiency and accuracy. Firstly, we propose to transform critical Einstein summation operations into GEMM operations, leveraging the specific features of tensor network simulations to amplify the efficiency of GPUs. Secondly, by analyzing the data characteristics of quantum circuits, we employ extended precision to ensure the accuracy of simulation results and mixed precision to fully exploit the potential of GPUs, resulting in faster and more precise simulations. Our numerical experiments demonstrate that our approach can achieve a 3.96x reduction in verification time for random quantum circuit samples in the 18-cycle case of Sycamore, with sustained performance exceeding 21 TFLOPS on one A100. This method can be easily extended to the 20-cycle case, maintaining the same performance, accelerating by 12.5x compared to the state-of-the-art CPU-based results and 4.48-6.78x compared to the state-of-the-art GPU-based results reported in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.03978v2</guid>
      <category>quant-ph</category>
      <category>cs.DC</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Feng Pan, Hanfeng Gu, Lvlin Kuang, Bing Liu, Pan Zhang</dc:creator>
    </item>
    <item>
      <title>Survey on Quality Assurance of Smart Contracts</title>
      <link>https://arxiv.org/abs/2311.00270</link>
      <description>arXiv:2311.00270v3 Announce Type: replace-cross 
Abstract: With the increasing adoption of smart contracts, ensuring their security has become a critical concern. Numerous vulnerabilities and attacks have been identified and exploited, resulting in significant financial losses. In response, researchers have developed various tools and techniques to identify and prevent vulnerabilities in smart contracts. In this survey, we present a systematic overview of the quality assurance of smart contracts, covering vulnerabilities, attacks, defenses, and tool support. By classifying vulnerabilities based on known attacks, we can identify patterns and common weaknesses that need to be addressed. Moreover, in order to effectively protect smart contracts, we have created a labeled dataset to evaluate various vulnerability detection tools and compare their effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00270v3</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyuan Wei, Jing Sun, Zijian Zhang, Xianhao Zhang, Xiaoxuan Yang, Liehuang Zhu</dc:creator>
    </item>
    <item>
      <title>etuner: Redundancy-Aware Efficient Continual Learning on Edge Devices</title>
      <link>https://arxiv.org/abs/2401.16694</link>
      <description>arXiv:2401.16694v4 Announce Type: replace-cross 
Abstract: Many emerging applications, such as robot-assisted eldercare and object recognition, generally employ deep learning neural networks (DNNs) and require the deployment of DNN models on edge devices. These applications naturally require i) handling streaming-in inference requests and ii) fine-tuning the deployed models to adapt to possible deployment scenario changes. Continual learning (CL) is widely adopted to satisfy these needs. CL is a popular deep learning paradigm that handles both continuous model fine-tuning and overtime inference requests. However, an inappropriate model fine-tuning scheme could involve significant redundancy and consume considerable time and energy, making it challenging to apply CL on edge devices. In this paper, we propose ETuner, an efficient edge continual learning framework that optimizes inference accuracy, fine-tuning execution time, and energy efficiency through both inter-tuning and intra-tuning optimizations. Experimental results show that, on average, ETuner reduces overall fine-tuning execution time by 64%, energy consumption by 56%, and improves average inference accuracy by 1.75% over the immediate model fine-tuning approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16694v4</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sheng Li, Geng Yuan, Yawen Wu, Yue Dai, Tianyu Wang, Chao Wu, Alex K. Jones, Jingtong Hu, Yanzhi Wang, Xulong Tang</dc:creator>
    </item>
    <item>
      <title>EdgeAlpha: Distributed Process Discovery at the Data Sources</title>
      <link>https://arxiv.org/abs/2405.03426</link>
      <description>arXiv:2405.03426v3 Announce Type: replace-cross 
Abstract: Process Mining is moving beyond mining traditional event logs and nowadays includes, for example, data sourced from sensors in the Internet of Things (IoT) in a smart home, factory or hospital setting. The volume and velocity of data generated by such sensors makes it increasingly challenging for traditional process discovery algorithms to store and mine such data in traditional event logs. Further, privacy considerations often prevent data collection at a central location in the first place. To address this challenge, this paper introduces EdgeAlpha, an algorithm for distributed process discovery operating directly on sensor nodes on a stream of real-time event data. Based on the Alpha Miner, EdgeAlpha tracks each event and its predecessor and successor events directly on the sensor node where the event is sensed and recorded. From this local view, each node in EdgeAlpha derives a partial footprint matrix, which we then merge at a central location, whenever we query the system to compute a process model. Our analysis shows that even with over 600 activities, like in the Hospital Log, the average number of predecessors per activity remains below 7. This enables efficient determination of predecessors for each event, reducing the communication overhead by up to 96% compared to querying all nodes and enhancing scalability. Additionally, our analysis demonstrates that the number of queried nodes stabilizes after relatively few events, and batching predecessor queries in groups of 40 reduces the average queried nodes per event to less than 2.5%. EdgeAlpha ensures that the resulting process model is identical to the original Alpha Miner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03426v3</guid>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julia Andersen, Patrick Rathje, Olaf Landsiedel</dc:creator>
    </item>
    <item>
      <title>Decentralized Intelligence Network (DIN)</title>
      <link>https://arxiv.org/abs/2407.02461</link>
      <description>arXiv:2407.02461v4 Announce Type: replace-cross 
Abstract: Decentralized Intelligence Network (DIN) is a theoretical framework addressing data fragmentation and siloing challenges, enabling scalable AI through data sovereignty. It facilitates effective AI utilization within sovereign networks by overcoming barriers to accessing diverse data sources, leveraging: 1) personal data stores to ensure data sovereignty, where data remains securely within Participants' control; 2) a scalable federated learning protocol implemented on a public blockchain for decentralized AI training, where only model parameter updates are shared, keeping data within the personal data stores; and 3) a scalable, trustless cryptographic rewards mechanism on a public blockchain to incentivize participation and ensure fair reward distribution through a decentralized auditing protocol. This approach guarantees that no entity can prevent or control access to training data or influence financial benefits, as coordination and reward distribution are managed on the public blockchain with an immutable record. The framework supports effective AI training by allowing Participants to maintain control over their data, benefit financially, and contribute to a decentralized, scalable ecosystem that leverages collective AI to develop beneficial algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02461v4</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abraham Nash</dc:creator>
    </item>
    <item>
      <title>Accelerating Distributed Optimization: A Primal-Dual Perspective on Local Steps</title>
      <link>https://arxiv.org/abs/2407.02689</link>
      <description>arXiv:2407.02689v2 Announce Type: replace-cross 
Abstract: In distributed machine learning, efficient training across multiple agents with different data distributions poses significant challenges. Even with a centralized coordinator, current algorithms that achieve optimal communication complexity typically require either large minibatches or compromise on gradient complexity. In this work, we tackle both centralized and decentralized settings across strongly convex, convex, and nonconvex objectives. We first demonstrate that a basic primal-dual method, (Accelerated) Gradient Ascent Multiple Stochastic Gradient Descent (GA-MSGD), applied to the Lagrangian of distributed optimization inherently incorporates local updates, because the inner loops of running Stochastic Gradient Descent on the primal variable require no inter-agent communication. Notably, for strongly convex objectives, (Accelerated) GA-MSGD achieves linear convergence in communication rounds despite the Lagrangian being only linear in the dual variables. This is due to a structural property where the dual variable is confined to the span of the coupling matrix, rendering the dual problem strongly concave. When integrated with the Catalyst framework, our approach achieves nearly optimal communication complexity across various settings without the need for minibatches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02689v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Junchi Yang, Murat Yildirim, Qiu Feng</dc:creator>
    </item>
  </channel>
</rss>
