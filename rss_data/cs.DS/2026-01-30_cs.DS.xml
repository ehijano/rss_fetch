<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 Jan 2026 05:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Stochastic Indexing Primitives for Non-Deterministic Molecular Archives</title>
      <link>https://arxiv.org/abs/2601.20921</link>
      <description>arXiv:2601.20921v1 Announce Type: new 
Abstract: Random access remains a central bottleneck in DNA-based data storage. Existing systems typically retrieve records by PCR enrichment or other multi-step biochemical procedures, which do not naturally support fast, massively parallel, content-addressable queries.
  We introduce the Holographic Bloom Filter (HBF), a probabilistic indexing primitive that stores key-pointer associations as a single high-dimensional memory vector. HBF binds a key vector and a value (pointer) vector using circular convolution and superposes bindings across all records. A query decodes by correlating the memory with the query key and selecting the best matching value using a margin-based decision rule.
  We give construction and decoding algorithms and a probabilistic analysis under explicit noise models (memory corruption and query/key mismatches). The analysis provides concentration bounds for match and non-match score distributions, explicit threshold and margin settings for a top K decoder, and exponential error decay in the vector dimension under standard randomness assumptions.
  HBF offers a concrete, analyzable alternative to pointer-chasing molecular data structures, enabling one-shot associative retrieval while quantifying trade-offs among dimensionality, dataset size, and noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20921v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Faruk Alpay, Levent Sarioglu</dc:creator>
    </item>
    <item>
      <title>Exact (n + 2) Comparison Complexity for the N-Repeated Element Problem</title>
      <link>https://arxiv.org/abs/2601.21202</link>
      <description>arXiv:2601.21202v1 Announce Type: new 
Abstract: This paper establishes the exact comparison complexity of finding an element repeated $n$ times in a $2n$-element array containing $n+1$ distinct values, under the equality-comparison model with $O(1)$ extra space. We present a simple deterministic algorithm performing exactly $n+2$ comparisons and prove this bound tight: any correct algorithm requires at least $n+2$ comparisons in the worst case. The lower bound follows from an adversary argument using graph-theoretic structure. Equality queries build an inequality graph $I$; its complement $P$ (potential-equalities) must contain either two disjoint $n$-cliques or one $(n+1)$-clique to maintain ambiguity. We show these structures persist up through $n+1$ comparisons via a "pillar matching" construction and edge-flip reconfiguration, but fail at $n+2$. This result provides a concrete, self-contained demonstration of exact lower-bound techniques, bridging toy problems with nontrivial combinatorial reasoning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21202v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Au</dc:creator>
    </item>
    <item>
      <title>Quantifying Noise in Language Generation</title>
      <link>https://arxiv.org/abs/2601.21237</link>
      <description>arXiv:2601.21237v1 Announce Type: new 
Abstract: Kleinberg and Mullainathan recently proposed a formal framework for studying the phenomenon of language generation, called language generation in the limit. In this model, an adversary gives an enumeration of example strings from an unknown target language, and the algorithm is tasked with correctly generating unseen strings from the target language within finite time. Refined notions of non-uniform and uniform generation were later introduced by Li, Raman, and Tewari (2025), and a noisy model was introduced by Raman and Raman (2025), which allows the adversary to insert extraneous strings. A natural question in the noisy model is to quantify the effect of noise, by studying the impact of each additional extraneous string. We show two complementary results in this setting. We first show that for both uniform and non-uniform generation, a single noisy string strictly reduces the set of collections that can be generated, thus answering an open question in Raman and Raman (2025). Then, we show for both uniform and non-uniform generation that generation with a single noisy string is equivalent to generation with any finite amount of noise, sharply contrasting with the strict hierarchy for noisy generation in the limit shown by Bai, Panigrahi, and Zhang (2026). Finally, we leverage our previous results to provide the first known characterization for non-uniform noise-dependent generatability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21237v1</guid>
      <category>cs.DS</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aaron Li, Ian Zhang</dc:creator>
    </item>
    <item>
      <title>Algorithms for the local and the global postage stamp problem</title>
      <link>https://arxiv.org/abs/2601.21423</link>
      <description>arXiv:2601.21423v1 Announce Type: new 
Abstract: We consider stamps with different values (denominations) and same dimensions, and an envelope with a fixed maximum number of stamp positions. The local postage stamp problem is to find the smallest value that cannot be realized by the sum of the stamps on the envelope. The global postage stamp problem is to find the set of denominations that maximize that smallest value for a fixed number of distinct denominations. The local problem is NP-hard and we propose here a novel algorithm that improves on both the time complexity bound and the amount of required memory.  We also propose a polynomial approximation algorithm for the global problem together with its complexity analysis. Finally we show that our algorithms allow to improve secure multi-party computations on sets via a more efficient homomorphic evaluation of polynomials on ciphered values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21423v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'eo Colisson Palais (UGA, LJK, CASC), Jean-Guillaume Dumas (UGA, LJK, CASC), Alexis Galan (UGA, CASC), Bruno Grenet (CASC), Aude Maignan (CASC)</dc:creator>
    </item>
    <item>
      <title>When Local and Non-Local Meet: Quadratic Improvement for Edge Estimation with Independent Set Queries</title>
      <link>https://arxiv.org/abs/2601.21457</link>
      <description>arXiv:2601.21457v1 Announce Type: new 
Abstract: We study the problem of estimating the number of edges in an unknown graph. We consider a hybrid model in which an algorithm may issue independent set, degree, and neighbor queries. We show that this model admits strictly more efficient edge estimation than either access type alone. Specifically, we give a randomized algorithm that outputs a $(1\pm\varepsilon)$-approximation of the number of edges using $O\left(\min\left(\sqrt{m}, \sqrt{\frac{n}{\sqrt{m}}}\right)\cdot\frac{\log n}{\varepsilon^{5/2}}\right)$ queries, and prove a nearly matching lower bound.
  In contrast, prior work shows that in the local query model (Goldreich and Ron, \textit{Random Structures \&amp; Algorithms} 2008) and in the independent set query model (Beame \emph{et al.} ITCS 2018, Chen \emph{et al.} SODA 2020), edge estimation requires $\widetilde{\Theta}(n/\sqrt{m})$ queries in the same parameter regimes. Our results therefore yield a quadratic improvement in the hybrid model, and no asymptotically better improvement is possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21457v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomer Adar, Yahel Hotam, Amit Levi</dc:creator>
    </item>
    <item>
      <title>Improved Approximations for Dial-a-Ride Problems</title>
      <link>https://arxiv.org/abs/2601.21652</link>
      <description>arXiv:2601.21652v1 Announce Type: new 
Abstract: The multi-vehicle dial-a-ride problem (mDaRP) is a fundamental vehicle routing problem with pickups and deliveries, widely applicable in ride-sharing, economics, and transportation. Given a set of $n$ locations, $h$ vehicles of identical capacity $\lambda$ located at various depots, and $m$ ride requests each defined by a source and a destination, the goal is to plan non-preemptive routes that serve all requests while minimizing the total travel distance, ensuring that no vehicle carries more than $\lambda$ passengers at any time. The best-known approximation ratio for the mDaRP remains $\mathcal{O}(\sqrt{\lambda}\log m)$.
  We propose two simple algorithms: the first achieves the same approximation ratio of $\mathcal{O}(\sqrt{\lambda}\log m)$ with improved running time, and the second attains an approximation ratio of $\mathcal{O}(\sqrt{\frac{m}{\lambda}})$. A combination of them yields an approximation ratio of $\mathcal{O}(\sqrt[4]{n}\log^{\frac{1}{2}}n)$ under $m=\Theta(n)$. Moreover, for the case $m\gg n$, by extending our algorithms, we derive an $\mathcal{O}(\sqrt{n\log n})$-approximation algorithm, which also improves the current best-known approximation ratio of $\mathcal{O}(\sqrt{n}\log^2n)$ for the classic (single-vehicle) DaRP, obtained by Gupta et al. (ACM Trans. Algorithms, 2010).</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21652v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyang Zhao, Mingyu Xiao</dc:creator>
    </item>
    <item>
      <title>Improved Approximations for the Unsplittable Capacitated Vehicle Routing Problem</title>
      <link>https://arxiv.org/abs/2601.21660</link>
      <description>arXiv:2601.21660v1 Announce Type: new 
Abstract: The capacitated vehicle routing problem (CVRP) is one of the most extensively studied problems in combinatorial optimization. In this problem, we are given a depot and a set of customers, each with a demand, embedded in a metric space. The objective is to find a set of tours, each starting and ending at the depot, operated by the capacititated vehicle at the depot to serve all customers, such that all customers are served, and the total travel cost is minimized. We consider the unplittable variant, where the demand of each customer must be served entirely by a single tour. Let $\alpha$ denote the current best-known approximation ratio for the metric traveling salesman problem. The previous best approximation ratio was $\alpha+1+\ln 2+\delta&lt;3.1932$ for a small constant $\delta&gt;0$ (Friggstad et al., Math. Oper. Res. 2025), which can be further improved by a small constant using the result of Blauth, Traub, and Vygen (Math. Program. 2023). In this paper, we propose two improved approximation algorithms. The first algorithm focuses on the case of fixed vehicle capacity and achieves an approximation ratio of $\alpha+1+\ln\bigl(2-\frac{1}{2}y_0\bigr)&lt;3.0897$, where $y_0&gt;0.39312$ is the unique root of $\ln\bigl(2-\frac{1}{2}y\bigr)=\frac{3}{2}y$. The second algorithm considers general vehicle capacity and achieves an approximation ratio of $\alpha+1+y_1+\ln\left(2-2y_1\right)+\delta&lt;3.1759$ for a small constant $\delta&gt;0$, where $y_1&gt;0.17458$ is the unique root of $\frac{1}{2} y_1+ 6 (1-y_1)\bigl(1-e^{-\frac{1}{2} y_1}\bigr) =\ln\left(2-2y_1\right)$. Both approximations can be further improved by a small constant using the result of Blauth, Traub, and Vygen (Math. Program. 2023).</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21660v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyang Zhao, Mingyu Xiao</dc:creator>
    </item>
    <item>
      <title>Adaptively Robust Resettable Streaming</title>
      <link>https://arxiv.org/abs/2601.21989</link>
      <description>arXiv:2601.21989v1 Announce Type: new 
Abstract: We study algorithms in the resettable streaming model, where the value of each key can either be increased or reset to zero. The model is suitable for applications such as active resource monitoring with support for deletions and machine unlearning. We show that all existing sketches for this model are vulnerable to adaptive adversarial attacks that apply even when the sketch size is polynomial in the length of the stream.
  To overcome these vulnerabilities, we present the first adaptively robust sketches for resettable streams that maintain polylogarithmic space complexity in the stream length. Our framework supports (sub) linear statistics including $L_p$ moments for $p\in[0,1]$ (in particular, Cardinality and Sum) and Bernstein statistics. We bypass strong impossibility results known for linear and composable sketches by designing dedicated streaming sketches robustified via Differential Privacy. Unlike standard robustification techniques, which provide limited benefits in this setting and still require polynomial space in the stream length, we leverage the Binary Tree Mechanism for continual observation to protect the sketch's internal randomness. This enables accurate prefix-max error guarantees with polylogarithmic space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21989v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edith Cohen, Elena Gribelyuk, Jelani Nelson, Uri Stemmer</dc:creator>
    </item>
    <item>
      <title>Top-k on a Budget: Adaptive Ranking with Weak and Strong Oracles</title>
      <link>https://arxiv.org/abs/2601.20989</link>
      <description>arXiv:2601.20989v1 Announce Type: cross 
Abstract: Identifying the top-$k$ items is fundamental but often prohibitive when exact valuations are expensive. We study a two-oracle setting with a fast, noisy weak oracle and a scarce, high-fidelity strong oracle (e.g., human expert verification or expensive simulation). We first analyze a simple screen-then-certify baseline (STC) and prove it makes at most $m(4\varepsilon_{\max})$ strong calls given jointly valid weak confidence intervals with maximum radius $\varepsilon_{\max}$, where $m(\cdot)$ denotes the near-tie mass around the top-$k$ threshold. We establish a conditional lower bound of $\Omega(m(\varepsilon_{\max}))$ for any algorithm given the same weak uncertainty. Our main contribution is ACE, an adaptive certification algorithm that focuses strong queries on critical boundary items, achieving the same $O(m(4\varepsilon_{\max}))$ bound while reducing strong calls in practice. We then introduce ACE-W, a fully adaptive two-phase method that allocates weak budget adaptively before running ACE, further reducing strong costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20989v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lutz Oettershagen</dc:creator>
    </item>
    <item>
      <title>Efficient Algorithms for Weakly-Interacting Quantum Spin Systems</title>
      <link>https://arxiv.org/abs/2601.21140</link>
      <description>arXiv:2601.21140v1 Announce Type: cross 
Abstract: We establish efficient algorithms for weakly-interacting quantum spin systems at arbitrary temperature. In particular, we obtain a fully polynomial-time approximation scheme for the partition function and an efficient approximate sampling scheme for the thermal distribution over a classical spin space. Our approach is based on the cluster expansion method and a standard reduction from approximate sampling to approximate counting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21140v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryan L. Mann, Gabriel Waite</dc:creator>
    </item>
    <item>
      <title>Deletion-correcting codes for an adversarial nanopore channel</title>
      <link>https://arxiv.org/abs/2601.21236</link>
      <description>arXiv:2601.21236v1 Announce Type: cross 
Abstract: We study deletion-correcting codes for an adversarial nanopore channel in which at most $t$ deletions may occur. We propose an explicit construction of $q$-ary codes of length $n$ for this channel with $2t\log_q n+\Theta(\log\log n)$ redundant symbols. We also show that the optimal redundancy is between $t\log_q n+\Omega(1)$ and $2t\log_q n-\log_q\log_2 n+O(1)$, so our explicit construction matches the existential upper bound to first order. In contrast, for the classical adversarial $q$-ary deletion channel, the smallest redundancy achieved by known explicit constructions that correct up to $t$ deletions is $4t(1+\epsilon)\log_q n+o(\log n)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21236v1</guid>
      <category>cs.IT</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.IT</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huiling Xie, Zitan Chen</dc:creator>
    </item>
    <item>
      <title>Source Coding with Free Bits and the Multi-Way Number Partitioning Problem</title>
      <link>https://arxiv.org/abs/2009.02710</link>
      <description>arXiv:2009.02710v2 Announce Type: replace 
Abstract: We introduce a new variant of variable-length source coding for sending a source over two parallel channels, one of which is costly and the other free. We give a complete solution to this problem. Next, we relate the problem to the number partitioning problem, which is the task of dividing a given list of numbers into a pre-specified number of subsets such that the sum of the numbers in each subset is as nearly equal as possible. We introduce two new objective functions for this problem and show that an adapted version of the Huffman coding algorithm (with a runtime of $\mathcal{O}(n \log n)$ for input size $n$) produces the optimal solution for one objective function, and a nearly optimal solution for the other objective function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2009.02710v2</guid>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.CO</category>
      <category>math.IT</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niloufar Ahmadypour, Amin Gohari</dc:creator>
    </item>
    <item>
      <title>Dynamic Debt Swapping in Financial Networks</title>
      <link>https://arxiv.org/abs/2302.11250</link>
      <description>arXiv:2302.11250v2 Announce Type: replace 
Abstract: A debt swap is an elementary edge swap in a directed, weighted graph, where two edges with the same weight swap their targets. Debt swaps are a natural and appealing operation in financial networks, in which nodes are banks and edges represent debt contracts. They can improve the clearing payments and the stability of these networks. However, their algorithmic properties are not well-understood.
  We analyze the computational complexity of debt swapping. Our main interest lies in semi-positive swaps, in which no creditor strictly suffers and at least one strictly profits. These swaps lead to a Pareto-improvement in the entire network. We consider network optimization via sequences of v-improving debt swaps from which a given bank v strictly profits. For ranking-based clearing, we show that every sequence of semi-positive v-improving swaps has polynomial length. In contrast, for arbitrary v-improving swaps, the problem of reaching a network configuration that allows no further swaps is PLS-complete.
  In global optimization, the goal is to maximize the utility of a given bank $v$ by performing a sequence of debt swaps in the network. This problem is NP-hard to approximate for multiple types of swaps.
  Moreover, we study reachability problems -- deciding if a sequence of swaps exists between given initial and final networks. We design a polynomial-time algorithm to decide this question for arbitrary swaps and derive hardness results for several other types of swaps.
  Many of our results can be extended to networks with arbitrary monotone clearing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.11250v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.GT</category>
      <category>q-fin.RM</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.tcs.2026.115788</arxiv:DOI>
      <dc:creator>Henri Froese, Martin Hoefer, Lisa Wilhelmi</dc:creator>
    </item>
    <item>
      <title>Dominating Set Knapsack: Profit Optimization on Dominating Sets</title>
      <link>https://arxiv.org/abs/2506.24032</link>
      <description>arXiv:2506.24032v3 Announce Type: replace 
Abstract: In a large-scale network, we want to choose some influential nodes to make a profit by paying some cost within a limited budget so that we do not have to spend more budget on some nodes adjacent to the chosen nodes; our problem is the graph-theoretic representation of it. We define our problem, Dominating Set Knapsack, by attaching the knapsack problem with the dominating set on graphs. Each vertex $v~(\in V) $ is associated with a cost factor $w(v)$ and a profit amount $\alpha(v)$. We aim to choose some vertices within a fixed budget $(s)$ that give maximum profit so that we do not need to choose their 1-hop neighbors. We show that the Dominating Set Knapsack problem is strongly NPC even when restricted to bipartite graphs, but weakly NPC for star graphs. We present a pseudo-polynomial time algorithm for trees in time $O(n\cdot min\{s^2, (\alpha(V))^2\})$. We show that Dominating Set Knapsack is unlikely to be Fixed Parameter Tractable (FPT) by proving that it is W[2]-hard parameterized by the solution size. We developed FPT algorithms with running time $O(4^{tw}\cdot n^{O(1)} min\{s^2,{\alpha(V)}^2\})$ and $O(2^{vck-1}\cdot n^{O(1)} min\{s^2,{\alpha(V)}^2\})$, where $tw$ represents the $tw$ of the given graph $G(V,E)$, $vck$ is the solution size of the Vertex Cover Knapsack, $s$ is the capacity or size of the knapsack and $\alpha(V)=\sum_{v\in V}\alpha(v)$. We obtained similar results for other variants $k-$Dominating Set Knapsack and Minimal Dominating Set Knapsack, where $k$ is the size of the dominating set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.24032v3</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sipra Singh</dc:creator>
    </item>
    <item>
      <title>A Generalization of von Neumann's Reduction from the Assignment Problem to Zero-Sum Games</title>
      <link>https://arxiv.org/abs/2410.10767</link>
      <description>arXiv:2410.10767v3 Announce Type: replace-cross 
Abstract: The equivalence between von Neumann's Minimax Theorem for zero-sum games and the LP Duality Theorem connects cornerstone problems of the two fields of game theory and optimization, respectively, and has been the subject of intense scrutiny for seven decades. Yet, as observed in this paper, the proof of the difficult direction of this equivalence is unsatisfactory: It does not assign distinct roles to the two players of the game, as is natural from the definition of a zero-sum game.
  In retrospect, a partial resolution to this predicament was provided in another brilliant paper of von Neumann, which reduced the assignment problem to zero-sum games. However, the underlying LP is highly specialized; all entries of its objective function vector are strictly positive, the constraint vector is all ones, and the constraint matrix is 0/1.
  We generalize von Neumann's result along two directions, each allowing negative entries in certain parts of the LP. Our reductions make explicit the roles of the two players of the reduced game, namely their maximin strategies are to play optimal solutions to the primal and dual LPs. Furthermore, unlike previous reductions, the value of the reduced game reveals the value of the given LP. Our generalizations encompass several basic economic scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10767v3</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>econ.TH</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilan Adler, Martin Bullinger, Vijay V. Vazirani</dc:creator>
    </item>
  </channel>
</rss>
