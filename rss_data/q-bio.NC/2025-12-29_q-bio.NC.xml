<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 29 Dec 2025 05:00:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Metaboplasticity: The Reciprocal Regulation of Neuronal Activity and Cellular Energetics</title>
      <link>https://arxiv.org/abs/2512.21659</link>
      <description>arXiv:2512.21659v1 Announce Type: new 
Abstract: Standard Spiking Neural Network (SNN) models typically neglect metabolic constraints, treating neurons as energetically unconstrained components. We bridge this gap by implementing a conductance-based leaky integrate-and-fire (gLIF) microcircuit (N=5,000) in Brian2, using temperature-dependent Q10 scaling to as a biophysically grounded proxy to couple metabolic state with intrinsic excitability and synaptic plasticity. Our simulations revealed five distinct emergent properties: (1) Dynamics Bifurcation: Learning trajectories diverged significantly, with hypometabolic states plateauing near baseline and hypermetabolic states exhibiting non-linear, runaway potentiation; (2) STDP Window Deformation: Thermal stress structurally deformed the plasticity kernel, where hypermetabolism sharpened coincidence detection and hypometabolism flattened synaptic integration; (3) Signal Degradation: While metabolic rate positively correlated with connectivity strength, high-energy states caused synaptic saturation and a loss of sparse coding specificity; (4) Topological Shift: Network activity transitioned from sparse, asynchronous firing in energy-restricted states to pathological, seizure-like hypersynchronization in high-energy states ; and (5) Parametric Robustness: Sensitivity analysis confirmed these attractor states were intrinsic biophysical properties, robust across random network initializations. Collectively, these results define an "inverted-U" relationship between bioenergetics and learning, demonstrating that metabolic constraints are necessary hardware regulators for network stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21659v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ece \"Oner, Cenk Denkta\c{s}</dc:creator>
    </item>
    <item>
      <title>Numerical Twin with Two Dimensional Ornstein--Uhlenbeck Processes of Transient Oscillations in EEG signal</title>
      <link>https://arxiv.org/abs/2512.21768</link>
      <description>arXiv:2512.21768v1 Announce Type: new 
Abstract: Stochastic burst-like oscillations are common in physiological signals, yet there are few compact generative models that capture their transient structure. We propose a numerical-twin framework that represents transient narrowband activity as a two-dimensional Ornstein-Uhlenbeck (OU) process with three interpretable parameters: decay rate, mean frequency, and noise amplitude. We develop two complementary estimation strategies. The first fits the power spectral density, amplitude distribution, and autocorrelation to recover OU-parameters. The second segments burst events and performs a statistical match between empirical spindle statistics (duration, amplitude, inter-event interval) and simulated OU output via grid search, resolving parameter degeneracies by including event counts. We extend the framework to multiple frequency bands and piecewise-stationary dynamics to track slow parameter drifts. Applied to electroencephalography (EEG) recorded during general anesthesia, the method identifies OU models that reproduce alpha-spindle (8-12 Hz) morphology and band-limited spectra with low residual error, enabling real-time tracking of state changes that are not apparent from band power alone. This decomposition yields a sparse, interpretable representation of transient oscillations and provides interpretable metrics for brain monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21768v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>P. O. Michel, C. Sun, S. Jaffard, D. Longrois, D. Holcman</dc:creator>
    </item>
    <item>
      <title>Learning continually with representational drift</title>
      <link>https://arxiv.org/abs/2512.22045</link>
      <description>arXiv:2512.22045v1 Announce Type: new 
Abstract: Deep artificial neural networks famously struggle to learn from non-stationary streams of data. Without dedicated mitigation strategies, continual learning is associated with continuous forgetting of previous tasks and a progressive loss of plasticity. Current approaches to continual learning have either focused on increasing the stability of representations of past tasks, or on promoting plasticity for future learning. Paradoxically, while animals including humans achieve a desirable stability-plasticity trade-off, the responses of biological neurons to external stimuli that are associated with stable behaviors gradually change over time. This suggests that, although unstable representations have historically been seen as undesirable in artificial systems, they could be a core property of biological neural networks learning continually. Here, we examine how linking representational drift to continual learning in biological neural networks could inform artificial systems. We highlight the existence of representational drift across numerous animal species and brain regions and propose that drift reflects a mixture of homeostatic turnover and learning-related synaptic plasticity. In particular, we evaluate how plasticity induced by learning new tasks could induce drift in the representation of previous tasks, and how such drift could accumulate across brain regions. In deep artificial neural networks, we propose that representational drift is only compatible with approaches that do not explicitly prevent parameter changes to mitigate forgetting. Remarkably, jointly promoting plasticity while mitigating forgetting could in principle induce representational drift in continual learning. While we argue that drift is a byproduct rather than a solution to incremental learning, its investigation could inform approaches to continual learning in artificial systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.22045v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suzanne van der Veldt, Gido M. van de Ven, Sanne Moorman, Guillaume Etter</dc:creator>
    </item>
    <item>
      <title>SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis</title>
      <link>https://arxiv.org/abs/2512.21881</link>
      <description>arXiv:2512.21881v1 Announce Type: cross 
Abstract: Foundation models are emerging as a powerful paradigm for fMRI analysis, but current approaches face a dual bottleneck of data- and training-efficiency. Atlas-based methods aggregate voxel signals into fixed regions of interest, reducing data dimensionality but discarding fine-grained spatial details, and requiring extremely large cohorts to train effectively as general-purpose foundation models. Atlas-free methods, on the other hand, operate directly on voxel-level information - preserving spatial fidelity but are prohibitively memory- and compute-intensive, making large-scale pre-training infeasible. We introduce SLIM-Brain (Sample-efficient, Low-memory fMRI Foundation Model for Human Brain), a new atlas-free foundation model that simultaneously improves both data- and training-efficiency. SLIM-Brain adopts a two-stage adaptive design: (i) a lightweight temporal extractor captures global context across full sequences and ranks data windows by saliency, and (ii) a 4D hierarchical encoder (Hiera-JEPA) learns fine-grained voxel-level representations only from the top-$k$ selected windows, while deleting about 70% masked patches. Extensive experiments across seven public benchmarks show that SLIM-Brain establishes new state-of-the-art performance on diverse tasks, while requiring only 4 thousand pre-training sessions and approximately 30% of GPU memory comparing to traditional voxel-level methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21881v1</guid>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mo Wang, Junfeng Xia, Wenhao Ye, Enyu Liu, Kaining Peng, Jianfeng Feng, Quanying Liu, Hongkai Wen</dc:creator>
    </item>
    <item>
      <title>Coherence in the brain unfolds across separable temporal regimes</title>
      <link>https://arxiv.org/abs/2512.20481</link>
      <description>arXiv:2512.20481v3 Announce Type: replace 
Abstract: Coherence in language requires the brain to satisfy two competing temporal demands: gradual accumulation of meaning across extended context and rapid reconfiguration of representations at event boundaries. Despite their centrality to language and thought, how these processes are implemented in the human brain during naturalistic listening remains unclear. Here, we tested whether these two processes can be captured by annotation-free drift and shift signals and whether their neural expression dissociates across large-scale cortical systems. These signals were derived from a large language model (LLM) and formalized contextual drift and event shifts directly from the narrative input. To enable high-precision voxelwise encoding models with stable parameter estimates, we densely sampled one healthy adult across more than 7 hours of listening to thirteen crime stories while collecting ultra high-field (7T) BOLD data. We then modeled the feature-informed hemodynamic response using a regularized encoding framework validated on independent stories. Drift predictions were prevalent in default-mode network hubs, whereas shift predictions were evident bilaterally in the primary auditory cortex and language association cortex. Furthermore, activity in default-mode and parietal networks was best explained by a signal capturing how meaning accumulates and gradually fades over the course of the narrative. Together, these findings show that coherence during language comprehension is implemented through dissociable neural regimes of slow contextual integration and rapid event-driven reconfiguration, offering a mechanistic entry point for understanding disturbances of language coherence in psychiatric disorders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20481v3</guid>
      <category>q-bio.NC</category>
      <category>cs.CL</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Davide Staub, Finn Rabe, Akhil Misra, Yves Pauli, Roya H\"uppi, Ni Yang, Nils Lang, Lars Michels, Victoria Edkins, Sascha Fr\"uhholz, Iris Sommer, Wolfram Hinzen, Philipp Homan</dc:creator>
    </item>
  </channel>
</rss>
