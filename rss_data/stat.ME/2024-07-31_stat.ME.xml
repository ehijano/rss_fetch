<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 Aug 2024 04:00:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 01 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Bayesian thresholded modeling for integrating brain node and network predictors</title>
      <link>https://arxiv.org/abs/2407.21154</link>
      <description>arXiv:2407.21154v1 Announce Type: new 
Abstract: Progress in neuroscience has provided unprecedented opportunities to advance our understanding of brain alterations and their correspondence to phenotypic profiles. With data collected from various imaging techniques, studies have integrated different types of information ranging from brain structure, function, or metabolism. More recently, an emerging way to categorize imaging traits is through a metric hierarchy, including localized node-level measurements and interactive network-level metrics. However, limited research has been conducted to integrate these different hierarchies and achieve a better understanding of the neurobiological mechanisms and communications. In this work, we address this literature gap by proposing a Bayesian regression model under both vector-variate and matrix-variate predictors. To characterize the interplay between different predicting components, we propose a set of biologically plausible prior models centered on an innovative joint thresholded prior. This captures the coupling and grouping effect of signal patterns, as well as their spatial contiguity across brain anatomy. By developing a posterior inference, we can identify and quantify the uncertainty of signaling node- and network-level neuromarkers, as well as their predictive mechanism for phenotypic outcomes. Through extensive simulations, we demonstrate that our proposed method outperforms the alternative approaches substantially in both out-of-sample prediction and feature selection. By implementing the model to study children's general mental abilities, we establish a powerful predictive mechanism based on the identified task contrast traits and resting-state sub-networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21154v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhe Sun, Wanwan Xu, Tianxi Li, Jian Kang, Gregorio Alanis-Lobato, Yize Zhao</dc:creator>
    </item>
    <item>
      <title>An overview of methods for receiver operating characteristic analysis, with an application to SARS-CoV-2 vaccine-induced humoral responses in solid organ transplant recipients</title>
      <link>https://arxiv.org/abs/2407.21253</link>
      <description>arXiv:2407.21253v1 Announce Type: new 
Abstract: Receiver operating characteristic (ROC) analysis is a tool to evaluate the capacity of a numeric measure to distinguish between groups, often employed in the evaluation of diagnostic tests. Overall classification ability is sometimes crudely summarized by a single numeric measure such as the area under the empirical ROC curve. However, it may also be of interest to estimate the full ROC curve while leveraging assumptions regarding the nature of the data (parametric) or about the ROC curve directly (semiparametric). Although there has been recent interest in methods to conduct comparisons by way of stochastic ordering, nuances surrounding ROC geometry and estimation are not widely known in the broader scientific and statistical community. The overarching goals of this manuscript are to (1) provide an overview of existing frameworks for ROC curve estimation with examples, (2) offer intuition for and considerations regarding methodological trade-offs, and (3) supply sample R code to guide implementation. We utilize simulations to demonstrate the bias-variance trade-off across various methods. As an illustrative example, we analyze data from a recent cohort study in order to compare responses to SARS-CoV-2 vaccination between solid organ transplant recipients and healthy controls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21253v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathaniel P. Dowd, Bryan Blette, James D. Chappell, Natasha B. Halasa, Andrew J. Spieker</dc:creator>
    </item>
    <item>
      <title>Randomized Controlled Trials of Service Interventions: The Impact of Capacity Constraints</title>
      <link>https://arxiv.org/abs/2407.21322</link>
      <description>arXiv:2407.21322v1 Announce Type: new 
Abstract: Randomized controlled trials (RCTs), or experiments, are the gold standard for intervention evaluation. However, the main appeal of RCTs, the clean identification of causal effects, can be compromised by interference, when one subject's treatment assignment can influence another subject's behavior or outcomes. In this paper, we formalise and study a type of interference stemming from the operational implementation of a subclass of interventions we term Service Interventions (SIs): interventions that include an on-demand service component provided by a costly and limited resource (e.g., healthcare providers or teachers).
  We show that in such a system, the capacity constraints induce dependencies across experiment subjects, where an individual may need to wait before receiving the intervention. By modeling these dependencies using a queueing system, we show how increasing the number of subjects without increasing the capacity of the system can lead to a nonlinear decrease in the treatment effect size. This has implications for conventional power analysis and recruitment strategies: increasing the sample size of an RCT without appropriately expanding capacity can decrease the study's power. To address this issue, we propose a method to jointly select the system capacity and number of users using the square root staffing rule from queueing theory. We show how incorporating knowledge of the queueing structure can help an experimenter reduce the amount of capacity and number of subjects required while still maintaining high power. In addition, our analysis of congestion-driven interference provides one concrete mechanism to explain why similar protocols can result in different RCT outcomes and why promising interventions at the RCT stage may not perform well at scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21322v1</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Justin Boutilier, Jonas Oddur Jonasson, Hannah Li, Erez Yoeli</dc:creator>
    </item>
    <item>
      <title>Deep Fr\'echet Regression</title>
      <link>https://arxiv.org/abs/2407.21407</link>
      <description>arXiv:2407.21407v1 Announce Type: new 
Abstract: Advancements in modern science have led to the increasing availability of non-Euclidean data in metric spaces. This paper addresses the challenge of modeling relationships between non-Euclidean responses and multivariate Euclidean predictors. We propose a flexible regression model capable of handling high-dimensional predictors without imposing parametric assumptions. Two primary challenges are addressed: the curse of dimensionality in nonparametric regression and the absence of linear structure in general metric spaces. The former is tackled using deep neural networks, while for the latter we demonstrate the feasibility of mapping the metric space where responses reside to a low-dimensional Euclidean space using manifold learning. We introduce a reverse mapping approach, employing local Fr\'echet regression, to map the low-dimensional manifold representations back to objects in the original metric space. We develop a theoretical framework, investigating the convergence rate of deep neural networks under dependent sub-Gaussian noise with bias. The convergence rate of the proposed regression model is then obtained by expanding the scope of local Fr\'echet regression to accommodate multivariate predictors in the presence of errors in predictors. Simulations and case studies show that the proposed model outperforms existing methods for non-Euclidean responses, focusing on the special cases of probability measures and networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21407v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Su I Iao, Yidong Zhou, Hans-Georg M\"uller</dc:creator>
    </item>
    <item>
      <title>A Bayesian Bootstrap Approach for Dynamic Borrowing for Minimizing Mean Squared Error</title>
      <link>https://arxiv.org/abs/2407.21588</link>
      <description>arXiv:2407.21588v1 Announce Type: new 
Abstract: For dynamic borrowing to leverage external data to augment the control arm of small RCTs, the key step is determining the amount of borrowing based on the similarity of the outcomes in the controls from the trial and the external data sources. A simple approach for this task uses the empirical Bayesian approach, which maximizes the marginal likelihood (maxML) of the amount of borrowing, while a likelihood-independent alternative minimizes the mean squared error (minMSE). We consider two minMSE approaches that differ from each other in the way of estimating the parameters in the minMSE rule. The classical one adjusts for bias due to sample variance, which in some situations is equivalent to the maxML rule. We propose a simplified alternative without the variance adjustment, which has asymptotic properties partially similar to the maxML rule, leading to no borrowing if means of control outcomes from the two data sources are different and may have less bias than that of the maxML rule. In contrast, the maxML rule may lead to full borrowing even when two datasets are moderately different, which may not be a desirable property. For inference, we propose a Bayesian bootstrap (BB) based approach taking the uncertainty of the estimated amount of borrowing and that of pre-adjustment into account. The approach can also be used with a pre-adjustment on the external controls for population difference between the two data sources using, e.g., inverse probability weighting. The proposed approach is computationally efficient and is implemented via a simple algorithm. We conducted a simulation study to examine properties of the proposed approach, including the coverage of 95 CI based on the Bayesian bootstrapped posterior samples, or asymptotic normality. The approach is illustrated by an example of borrowing controls for an AML trial from another study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21588v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jixian Wang, Ram Tiwari</dc:creator>
    </item>
    <item>
      <title>Shape-restricted transfer learning analysis for generalized linear regression model</title>
      <link>https://arxiv.org/abs/2407.21682</link>
      <description>arXiv:2407.21682v1 Announce Type: new 
Abstract: Transfer learning has emerged as a highly sought-after and actively pursued research area within the statistical community. The core concept of transfer learning involves leveraging insights and information from auxiliary datasets to enhance the analysis of the primary dataset of interest. In this paper, our focus is on datasets originating from distinct yet interconnected distributions. We assume that the training data conforms to a standard generalized linear model, while the testing data exhibit a connection to the training data based on a prior probability shift assumption. Ultimately, we discover that the two-sample conditional means are interrelated through an unknown, nondecreasing function. We integrate the power of generalized estimating equations with the shape-restricted score function, creating a robust framework for improved inference regarding the underlying parameters. We theoretically establish the asymptotic properties of our estimator and demonstrate, through simulation studies, that our method yields more accurate parameter estimates compared to those based solely on the testing or training data. Finally, we apply our method to a real-world example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21682v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pengfei Li, Tao Yu, Chixiang Chen, Jing Qin</dc:creator>
    </item>
    <item>
      <title>Unveiling land use dynamics: Insights from a hierarchical Bayesian spatio-temporal modelling of Compositional Data</title>
      <link>https://arxiv.org/abs/2407.21695</link>
      <description>arXiv:2407.21695v1 Announce Type: new 
Abstract: Changes in land use patterns have significant environmental and socioeconomic impacts, making it crucial for policymakers to understand their causes and consequences. This study, part of the European LAMASUS (Land Management for Sustainability) project, aims to support the EU's climate neutrality target by developing a governance model through collaboration between policymakers, land users, and researchers. We present a methodological synthesis for treating land use data using a Bayesian approach within spatial and spatio-temporal modeling frameworks.
  The study tackles the challenges of analyzing land use changes, particularly the presence of zero values and computational issues with large datasets. It introduces joint model structures to address zeros and employs sequential inference and consensus methods for Big Data problems. Spatial downscaling models approximate smaller scales from aggregated data, circumventing high-resolution data complications.
  We explore Beta regression and Compositional Data Analysis (CoDa) for land use data, review relevant spatial and spatio-temporal models, and present strategies for handling zeros. The paper demonstrates the implementation of key models, downscaling techniques, and solutions to Big Data challenges with examples from simulated data and the LAMASUS project, providing a comprehensive framework for understanding and managing land use changes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21695v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mario Figueira, Carmen Guarner, David Conesa, Antonio L\'opez-Qu\'ilez, Tam\'as Krisztin</dc:creator>
    </item>
    <item>
      <title>Potential weights and implicit causal designs in linear regression</title>
      <link>https://arxiv.org/abs/2407.21119</link>
      <description>arXiv:2407.21119v1 Announce Type: cross 
Abstract: When do linear regressions estimate causal effects in quasi-experiments? This paper provides a generic diagnostic that assesses whether a given linear regression specification on a given dataset admits a design-based interpretation. To do so, we define a notion of potential weights, which encode counterfactual decisions a given regression makes to unobserved potential outcomes. If the specification does admit such an interpretation, this diagnostic can find a vector of unit-level treatment assignment probabilities -- which we call an implicit design -- under which the regression estimates a causal effect. This diagnostic also finds the implicit causal effect estimand. Knowing the implicit design and estimand adds transparency, leads to further sanity checks, and opens the door to design-based statistical inference. When applied to regression specifications studied in the causal inference literature, our framework recovers and extends existing theoretical results. When applied to widely-used specifications not covered by existing causal inference literature, our framework generates new theoretical insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21119v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiafeng Chen</dc:creator>
    </item>
    <item>
      <title>Quantile processes and their applications in finite populations</title>
      <link>https://arxiv.org/abs/2407.21238</link>
      <description>arXiv:2407.21238v1 Announce Type: cross 
Abstract: The weak convergence of the quantile processes, which are constructed based on different estimators of the finite population quantiles, is shown under various well-known sampling designs based on a superpopulation model. The results related to the weak convergence of these quantile processes are applied to find asymptotic distributions of the smooth $L$-estimators and the estimators of smooth functions of finite population quantiles. Based on these asymptotic distributions, confidence intervals are constructed for several finite population parameters like the median, the $\alpha$-trimmed means, the interquartile range and the quantile based measure of skewness. Comparisons of various estimators are carried out based on their asymptotic distributions. We show that the use of the auxiliary information in the construction of the estimators sometimes has an adverse effect on the performances of the smooth $L$-estimators and the estimators of smooth functions of finite population quantiles under several sampling designs. Further, the performance of each of the above-mentioned estimators sometimes becomes worse under sampling designs, which use the auxiliary information, than their performances under simple random sampling without replacement (SRSWOR).</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21238v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anurag Dey, Probal Chaudhuri</dc:creator>
    </item>
    <item>
      <title>A Ball Divergence Based Measure For Conditional Independence Testing</title>
      <link>https://arxiv.org/abs/2407.21456</link>
      <description>arXiv:2407.21456v1 Announce Type: cross 
Abstract: In this paper we introduce a new measure of conditional dependence between two random vectors ${\boldsymbol X}$ and ${\boldsymbol Y}$ given another random vector $\boldsymbol Z$ using the ball divergence. Our measure characterizes conditional independence and does not require any moment assumptions. We propose a consistent estimator of the measure using a kernel averaging technique and derive its asymptotic distribution. Using this statistic we construct two tests for conditional independence, one in the model-${\boldsymbol X}$ framework and the other based on a novel local wild bootstrap algorithm. In the model-${\boldsymbol X}$ framework, which assumes the knowledge of the distribution of ${\boldsymbol X}|{\boldsymbol Z}$, applying the conditional randomization test we obtain a method that controls Type I error in finite samples and is asymptotically consistent, even if the distribution of ${\boldsymbol X}|{\boldsymbol Z}$ is incorrectly specified up to distance preserving transformations. More generally, in situations where ${\boldsymbol X}|{\boldsymbol Z}$ is unknown or hard to estimate, we design a double-bandwidth based local wild bootstrap algorithm that asymptotically controls both Type I error and power. We illustrate the advantage of our method, both in terms of Type I error and power, in a range of simulation settings and also in a real data example. A consequence of our theoretical results is a general framework for studying the asymptotic properties of a 2-sample conditional $V$-statistic, which is of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21456v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bilol Banerjee, Bhaswar B. Bhattacharya, Anil K. Ghosh</dc:creator>
    </item>
    <item>
      <title>Identifiability of causal graphs under nonadditive conditionally parametric causal models</title>
      <link>https://arxiv.org/abs/2303.15376</link>
      <description>arXiv:2303.15376v4 Announce Type: replace 
Abstract: Causal discovery from observational data typically requires strong assumptions about the data-generating process. Previous research has established the identifiability of causal graphs under various models, including linear non-Gaussian, post-nonlinear, and location-scale models. However, these models may have limited applicability in real-world situations that involve a mixture of discrete and continuous variables or where the cause affects the variance or tail behavior of the effect. In this study, we introduce a new class of models, called Conditionally Parametric Causal Models (CPCM), which assume that the distribution of the effect, given the cause, belongs to well-known families such as Gaussian, Poisson, Gamma, or heavy-tailed Pareto distributions. These models are adaptable to a wide range of practical situations where the cause can influence the variance or tail behavior of the effect. We demonstrate the identifiability of CPCM by leveraging the concept of sufficient statistics. Furthermore, we propose an algorithm for estimating the causal structure from random samples drawn from CPCM. We evaluate the empirical properties of our methodology on various datasets, demonstrating state-of-the-art performance across multiple benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.15376v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juraj Bodik, Val\'erie Chavez-Demoulin</dc:creator>
    </item>
    <item>
      <title>Interaction Screening and Pseudolikelihood Approaches for Tensor Learning in Ising Models</title>
      <link>https://arxiv.org/abs/2310.13232</link>
      <description>arXiv:2310.13232v2 Announce Type: replace 
Abstract: In this paper, we study two well known methods of Ising structure learning, namely the pseudolikelihood approach and the interaction screening approach, in the context of tensor recovery in $k$-spin Ising models. We show that both these approaches, with proper regularization, retrieve the underlying hypernetwork structure using a sample size logarithmic in the number of network nodes, and exponential in the maximum interaction strength and maximum node-degree. We also track down the exact dependence of the rate of tensor recovery on the interaction order $k$, that is allowed to grow with the number of samples and nodes, for both the approaches. We then provide a comparative discussion of the performance of the two approaches based on simulation studies, which also demonstrates the exponential dependence of the tensor recovery rate on the maximum coupling strength. Our tensor recovery methods are then applied on gene data taken from the Curated Microarray Database (CuMiDa), where we focus on understanding the important genes related to hepatocellular carcinoma.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13232v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tianyu Liu, Somabha Mukherjee</dc:creator>
    </item>
    <item>
      <title>Fast Estimation of the Renshaw-Haberman Model and Its Variants</title>
      <link>https://arxiv.org/abs/2311.14846</link>
      <description>arXiv:2311.14846v2 Announce Type: replace 
Abstract: In mortality modelling, cohort effects are often taken into consideration as they add insights about variations in mortality across different generations. Statistically speaking, models such as the Renshaw-Haberman model may provide a better fit to historical data compared to their counterparts that incorporate no cohort effects. However, when such models are estimated using an iterative maximum likelihood method in which parameters are updated one at a time, convergence is typically slow and may not even be reached within a reasonably established maximum number of iterations. Among others, the slow convergence problem hinders the study of parameter uncertainty through bootstrapping methods. In this paper, we propose an intuitive estimation method that minimizes the sum of squared errors between actual and fitted log central death rates. The complications arising from the incorporation of cohort effects are overcome by formulating part of the optimization as a principal component analysis with missing values. Using mortality data from various populations, we demonstrate that our proposed method produces satisfactory estimation results and is significantly more efficient compared to the traditional likelihood-based approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.14846v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiping Guo, Johnny Siu-Hang Li</dc:creator>
    </item>
    <item>
      <title>Stable Reduced-Rank VAR Identification</title>
      <link>https://arxiv.org/abs/2403.00237</link>
      <description>arXiv:2403.00237v3 Announce Type: replace 
Abstract: The vector autoregression (VAR) has been widely used in system identification, econometrics, natural science, and many other areas. However, when the state dimension becomes large the parameter dimension explodes. So rank reduced modelling is attractive and is well developed. But a fundamental requirement in almost all applications is stability of the fitted model. And this has not been addressed in the rank reduced case. Here, we develop, for the first time, a closed-form formula for an estimator of a rank reduced transition matrix which is guaranteed to be stable. We show that our estimator is consistent and asymptotically statistically efficient and illustrate it in comparative simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00237v3</guid>
      <category>stat.ME</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xinhui Rong, Victor Solo</dc:creator>
    </item>
    <item>
      <title>Population Power Curves in ASCA with Permutation Testing</title>
      <link>https://arxiv.org/abs/2403.00429</link>
      <description>arXiv:2403.00429v2 Announce Type: replace 
Abstract: In this paper, we revisit the Power Curves in ANOVA Simultaneous Component Analysis (ASCA) based on permutation testing, and introduce the Population Curves derived from population parameters describing the relative effect among factors and interactions. We distinguish Relative from Absolute Population Curves, where the former represent statistical power in terms of the normalized effect size between structure and noise, and the latter in terms of the sample size. Relative Population Curves are useful to find the optimal ASCA model (e.g., fixed/random factors, crossed/nested relationships, interactions, the test statistic, transformations, etc.) for the analysis of an experimental design at hand. Absolute Population Curves are useful to determine the sample size and the optimal number of levels for each factor during the planning phase on an experiment. We illustrate both types of curves through simulation. We expect Population Curves to become the go-to approach to plan the optimal analysis pipeline and the required sample size in an omics study analyzed with ASCA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00429v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1002/cem.3596</arxiv:DOI>
      <arxiv:journal_reference>Journal of Chemometrics, 2024</arxiv:journal_reference>
      <dc:creator>Jose Camacho, Michael Sorochan Armstrong</dc:creator>
    </item>
    <item>
      <title>Identifying Sample Size and Accuracy and Precision of the Estimators in Case-Crossover Designs with Distributed Lags of Heteroskedastic Spatiotemporally Varying Continuous Exposures Measured with Simple or Complex Error</title>
      <link>https://arxiv.org/abs/2406.02369</link>
      <description>arXiv:2406.02369v4 Announce Type: replace 
Abstract: Understanding of sample size and the accuracy and precision of the estimator may facilitate power and bias analyses and help in designing reproducible research and interpreting findings. However, such understanding can become complicated in practice for several reasons. First, exposures varying spatiotemporally may be heteroskedastic. Second, distributed lags of exposures may be used to identify critical exposure time-windows. Third, exposure measurement error may exist, impacting the accuracy and/or precision of the estimator. Fourth, different study designs may affect sample size and power differently. For example, case-crossover designs as matched case-control designs, are used to estimate health effects of short-term exposures. Therefore, this article develops approximation equations for sample size, estimates of the estimators and standard errors, including polynomials for non-linear effect estimation in the presence of exposure measurement error. With real-world air pollution estimates, simulation experiments were conducted to examine novel approximation equations. Overall, sample size, the accuracy and precision of the estimators in these complex settings can be accurately approximated. For distributed lags, approximations may perform well if residual confounding due to covariate measurement errors is not severe. This condition may be difficult to identify without exposure validation data, so validation research is recommended in identifying critical exposure time-windows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02369v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Honghyok Kim</dc:creator>
    </item>
    <item>
      <title>Identifying arbitrary transformation between the slopes in functional regression</title>
      <link>https://arxiv.org/abs/2407.19502</link>
      <description>arXiv:2407.19502v2 Announce Type: replace 
Abstract: In this article, we study whether the slope functions of two functional regression models in two samples are associated with any arbitrary transformation (barring constant and linear transformation) or not along the vertical axis. In order to address this issue, a statistical testing of the hypothesis problem is formalized, and the test statistic is formed based on the estimated second derivative of the unknown transformation. The asymptotic properties of the test statistics are investigated using some advanced techniques related to the empirical process. Moreover, to implement the test for small sample size data, a Bootstrap algorithm is proposed, and it is shown that the Bootstrap version of the test is as good as the original test for sufficiently large sample size. Furthermore, the utility of the proposed methodology is shown for simulated data sets, and DTI data is analyzed using this methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19502v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pratim Guha Niyogi, Subhra Sankar Dhar</dc:creator>
    </item>
    <item>
      <title>Adaptive, Rate-Optimal Hypothesis Testing in Nonparametric IV Models</title>
      <link>https://arxiv.org/abs/2006.09587</link>
      <description>arXiv:2006.09587v5 Announce Type: replace-cross 
Abstract: We propose a new adaptive hypothesis test for inequality (e.g., monotonicity, convexity) and equality (e.g., parametric, semiparametric) restrictions on a structural function in a nonparametric instrumental variables (NPIV) model. Our test statistic is based on a modified leave-one-out sample analog of a quadratic distance between the restricted and unrestricted sieve two-stage least squares estimators. We provide computationally simple, data-driven choices of sieve tuning parameters and Bonferroni adjusted chi-squared critical values. Our test adapts to the unknown smoothness of alternative functions in the presence of unknown degree of endogeneity and unknown strength of the instruments. It attains the adaptive minimax rate of testing in $L^2$. That is, the sum of the supremum of type I error over the composite null and the supremum of type II error over nonparametric alternative models cannot be minimized by any other tests for NPIV models of unknown regularities. Confidence sets in $L^2$ are obtained by inverting the adaptive test. Simulations confirm that, across different strength of instruments and sample sizes, our adaptive test controls size and its finite-sample power greatly exceeds existing non-adaptive tests for monotonicity and parametric restrictions in NPIV models. Empirical applications to test for shape restrictions of differentiated products demand and of Engel curves are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2006.09587v5</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Breunig, Xiaohong Chen</dc:creator>
    </item>
    <item>
      <title>An Approximation for the 32-point Discrete Fourier Transform</title>
      <link>https://arxiv.org/abs/2407.12708</link>
      <description>arXiv:2407.12708v2 Announce Type: replace-cross 
Abstract: This brief note aims at condensing some results on the 32-point approximate DFT and discussing its arithmetic complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12708v2</guid>
      <category>eess.SP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>R. J. Cintra</dc:creator>
    </item>
  </channel>
</rss>
