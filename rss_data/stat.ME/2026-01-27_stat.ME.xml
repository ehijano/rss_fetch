<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 28 Jan 2026 02:53:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Varying coefficient model for longitudinal data with informative observation times</title>
      <link>https://arxiv.org/abs/2601.17141</link>
      <description>arXiv:2601.17141v1 Announce Type: new 
Abstract: Varying coefficient models are widely used to characterize dynamic associations between longitudinal outcomes and covariates. Existing work on varying coefficient models, however, all assumes that observation times are independent of the longitudinal outcomes, which is often violated in real-world studies with outcome-driven or otherwise informative visit schedules. Such informative observation times can lead to biased estimation and invalid inference using existing methods. In this article, we develop estimation and inference procedures for varying coefficient models that account for informative observation times. We model the observation time process as a general counting process under a proportional intensity model, with time-varying covariates summarizing the observed history. To address potential bias, we incorporate inverse intensity weighting into a sieve estimation framework, yielding closed-form coefficient function estimators via weighted least squares. We establish consistency, convergence rates, and asymptotic normality of the proposed estimators, and construct pointwise confidence intervals for the coefficient functions. Extensive simulation studies demonstrate that the proposed weighted method substantially outperforms the conventional unweighted method when observation times are informative. Finally, we provide an application of our method to the Alzheimer's Disease Neuroimaging Initiative study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17141v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yu Gu, Yangjianchen Xu, Peijun Sang</dc:creator>
    </item>
    <item>
      <title>Optimal Design under Interference, Homophily, and Robustness Trade-offs</title>
      <link>https://arxiv.org/abs/2601.17145</link>
      <description>arXiv:2601.17145v1 Announce Type: new 
Abstract: To minimize the mean squared error (MSE) in global average treatment effect (GATE) estimation under network interference, a popular approach is to use a cluster-randomized design. However, in the presence of homophily, which is common in social networks, cluster randomization can instead increase the MSE. We develop a novel potential outcomes model that accounts for interference, homophily, and heterogeneous variation. In this setting, we establish a framework for optimizing designs for worst-case MSE under the Horvitz-Thompson estimator. This leads to an optimization problem over the covariance matrices of the treatment assignment, trading off interference, homophily, and robustness. We frame and solve this problem using two complementary approaches. The first involves formulating a semidefinite program (SDP) and employing Gaussian rounding, in the spirit of the Goemans-Williamson approximation algorithm for MAXCUT. The second is an adaptation of the Gram-Schmidt Walk, a vector-balancing algorithm which has recently received much attention. Finally, we evaluate the performance of our designs through various experiments on simulated network data and a real village network dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17145v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vydhourie Thiyageswaran, Alex Kokot, Jennifer Brennan, Marina Meila, Christina Lee Yu, Maryam Fazel</dc:creator>
    </item>
    <item>
      <title>Falsifying Predictive Algorithm</title>
      <link>https://arxiv.org/abs/2601.17146</link>
      <description>arXiv:2601.17146v1 Announce Type: new 
Abstract: Empirical investigations into unintended model behavior often show that the algorithm is predicting another outcome than what was intended. These exposes highlight the need to identify when algorithms predict unintended quantities - ideally before deploying them into consequential settings. We propose a falsification framework that provides a principled statistical test for discriminant validity: the requirement that an algorithm predict intended outcomes better than impermissible ones. Drawing on falsification practices from causal inference, econometrics, and psychometrics, our framework compares calibrated prediction losses across outcomes to assess whether the algorithm exhibits discriminant validity with respect to a specified impermissible proxy. In settings where the target outcome is difficult to observe, multiple permissible proxy outcomes may be available; our framework accommodates both this setting and the case with a single permissible proxy. Throughout we use nonparametric hypothesis testing methods that make minimal assumptions on the data-generating process. We illustrate the method in an admissions setting, where the framework establishes discriminant validity with respect to gender but fails to establish discriminant validity with respect to race. This demonstrates how falsification can serve as an early validity check, prior to fairness or robustness analyses. We also provide analysis in a criminal justice setting, where we highlight the limitations of our framework and emphasize the need for complementary approaches to assess other aspects of construct validity and external validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17146v1</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amanda Coston</dc:creator>
    </item>
    <item>
      <title>Evaluating Aggregated Relational Data Models with Simple Diagnostics</title>
      <link>https://arxiv.org/abs/2601.17153</link>
      <description>arXiv:2601.17153v1 Announce Type: new 
Abstract: Aggregated Relational Data (ARD) contain summary information about individual social networks and are widely used to estimate social network characteristics and the size of populations of interest. Although a variety of ARD estimators exist, practitioners currently lack guidance on how to evaluate whether a selected model adequately fits the data. We introduce a diagnostic framework for ARD models that provides a systematic, reproducible process for assessing covariate structure, distributional assumptions, and correlation. The diagnostics are based on point estimates, using either maximum likelihood or maximum a posteriori optimization, which allows quick evaluation without requiring repeated Bayesian model fitting. Through simulation studies and applications to large ARD datasets, we show that the proposed workflow identifies common sources of model misfit and helps researchers select an appropriate model that adequately explains the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17153v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ian Laga, Benjamin Vogel, Jieyun Wang, Anna Smith, Owen Ward</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for Discrete Markov Random Fields Through Coordinate Rescaling</title>
      <link>https://arxiv.org/abs/2601.17205</link>
      <description>arXiv:2601.17205v1 Announce Type: new 
Abstract: Discrete Markov random fields (MRFs) represent a class of undirected graphical models that capture complex conditional dependencies between discrete variables. Conducting exact posterior inference in these models is computationally challenging due to the intractable partition function, which depends on the model parameters and sums over all possible state configurations in the system. As a result, using the exact likelihood function is infeasible and existing methods, such as Double Metropolis-Hastings or pseudo-likelihood approximations, either scale poorly to large systems or underestimate the variability of the target posterior distribution. To address both computational burden and efficiency loss, we propose a new class of coordinate-rescaling sampling methods, which map the model parameters from the pseudo-likelihood space to the target posterior, preserving computational efficiency while improving posterior inference. Finally, in simulation studies, we compare the proposed method to existing approaches and illustrate that coordinate-rescaling sampling provides more accurate estimates of posterior variability, offering a scalable and robust solution for Bayesian inference in discrete MRFs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17205v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giuseppe Arena, Maarten Marsman</dc:creator>
    </item>
    <item>
      <title>Transfer learning for scalar-on-function regression via control variates</title>
      <link>https://arxiv.org/abs/2601.17217</link>
      <description>arXiv:2601.17217v1 Announce Type: new 
Abstract: Transfer learning (TL) has emerged as a powerful tool for improving estimation and prediction performance by leveraging information from related datasets. In this paper, we repurpose the control-variates (CVS) method for TL in the context of scalar-on-function regression. Our proposed framework relies exclusively on dataset-specific summary statistics, avoiding the need to pool subject-level data and thus remaining applicable in privacy-restricted or decentralized settings. We establish theoretical connections among several existing TL strategies and derive convergence rates for our CVS-based proposals. These rates explicitly account for the typically overlooked smoothing error and reveal how the similarity among covariance functions across datasets influences convergence behavior. Numerical studies support the theoretical findings and demonstrate that the proposed methods achieve competitive estimation and prediction performance compared with existing alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17217v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuping Yang, Zhiyang Zhou</dc:creator>
    </item>
    <item>
      <title>An Empirical Method for Analyzing Count Data</title>
      <link>https://arxiv.org/abs/2601.17233</link>
      <description>arXiv:2601.17233v1 Announce Type: new 
Abstract: Count endpoints are common in clinical trials, particularly for recurrent events such as hypoglycemia. When interest centers on comparing overall event rates between treatment groups, negative binomial (NB) regression is widely used because it accommodates overdispersion and requires only event counts and exposure times. However, NB regression can be numerically unstable when events are sparse, and the efficiency gains from baseline covariate adjustment may be sensitive to model misspecification. We propose an empirical method that targets the same marginal estimand as NB regression -- the ratio of marginal event rates -- while avoiding distributional assumptions on the count outcome. Simulation studies show that the empirical method maintains appropriate Type I error control across diverse scenarios, including extreme overdispersion and zero inflation, achieves power comparable to NB regression, and yields consistent efficiency gains from baseline covariate adjustment. We illustrate the approach using severe hypoglycemia data from the QWINT-5 trial comparing insulin efsitora alfa with insulin degludec in adults with type 1 diabetes. In this sparse-event setting, the empirical method produced stable marginal rate estimates and rate ratios closely aligned with observed rates, while NB regression exhibited greater sensitivity and larger deviations from the observed rates in the sparsest intervals. The proposed empirical method provides a robust and numerically stable alternative to NB regression, particularly when the number of events is low or when numerical stability is a concern.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17233v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiren Sun, Linda Amoafo, Yongming Qu</dc:creator>
    </item>
    <item>
      <title>Capturing Cumulative Disease Burden in Chronic Kidney Disease Outcome Trials: Area Under the Curve and Restricted Mean Time in Favor of Treatment Beyond Conventional Time-to-First Analysis</title>
      <link>https://arxiv.org/abs/2601.17241</link>
      <description>arXiv:2601.17241v1 Announce Type: new 
Abstract: Chronic kidney disease (CKD) affects millions worldwide and progresses irreversibly through stages culminating in end-stage renal disease (ESRD) and death. Outcome trials in CKD traditionally employ time-to-first-event analyses using the Cox models. However, this approach has fundamental limitations for progressive diseases: it assigns equal weight to each composite endpoint component despite clear clinical hierarchy: an eGFR decline threshold receives the same weight as ESRD or death in the analysis, and it captures only the first occurrence while ignoring subsequent progression. Given CKD's gradual evolution over years, comprehensive treatment evaluation requires quantifying cumulative disease burden: integrating both event severity and time spent in each disease state. We propose two complementary approaches to better characterize treatment benefits by incorporating event severity and state occupancy: area under the curve (AUC) and restricted mean time in favor of treatment (RMT-IF). The AUC method assigns ordinal severity scores to disease states and calculates the area under the mean cumulative score curve, quantifying total event-free time lost. Treatment effects are expressed as AUC ratios or differences. The RMT-IF extends restricted mean survival time to multistate processes, measuring average time patients in the treatment arm spend in more favorable states versus the comparator. These methods better capture CKD's progressive nature where treatment benefits extend beyond first-event delay to overall disease trajectory modification. By discriminating between events of differing clinical importance and quantifying the complete disease course, these estimands offer alternative assessment frameworks for kidney-protective therapies, potentially improving efficiency and interpretability of future CKD outcome trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17241v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiren Sun, Tuo Wang, Yu Du</dc:creator>
    </item>
    <item>
      <title>Covariate-assisted Grade of Membership Models via Shared Latent Geometry</title>
      <link>https://arxiv.org/abs/2601.17265</link>
      <description>arXiv:2601.17265v1 Announce Type: new 
Abstract: The grade of membership model is a flexible latent variable model for analyzing multivariate categorical data through individual-level mixed membership scores. In many modern applications, auxiliary covariates are collected alongside responses and encode information about the same latent structure. Traditional approaches to incorporating such covariates typically rely on fully specified joint likelihoods, which are computationally intensive and sensitive to misspecification. We introduce a covariate-assisted grade of membership model that integrates response and covariate information by exploiting their shared low-rank simplex geometry, rather than modeling their joint distribution. We propose a likelihood-free spectral estimation procedure that combines heterogeneous data sources through a balance parameter controlling their relative contribution. To accommodate high-dimensional and heteroskedastic noise, we employ heteroskedastic principal component analysis before performing simplex-based geometric recovery. Our theoretical analysis establishes weaker identifiability conditions than those required in the covariate-free model, and further derives finite-sample, entrywise error bounds for both mixed membership scores and item parameters. These results demonstrate that auxiliary covariates can provably improve latent structure recovery, yielding faster convergence rates in high-dimensional regimes. Simulation studies and an application to educational assessment data illustrate the computational efficiency, statistical accuracy, and interpretability gains of the proposed method. The code for reproducing these results is open-source and available at \texttt{https://github.com/Toby-X/Covariate-Assisted-GoM}</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17265v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiyu Xu, Yuqi Gu</dc:creator>
    </item>
    <item>
      <title>Statistical process control via $p$-values</title>
      <link>https://arxiv.org/abs/2601.17319</link>
      <description>arXiv:2601.17319v1 Announce Type: new 
Abstract: We study statistical process control (SPC) through charting of $p$-values. When in control (IC), any valid sequence $(P_{t})_{t}$ is super-uniform, a requirement that can hold in nonparametric and two-phase designs without parametric modelling of the monitored process. Within this framework, we analyse the Shewhart rule that signals when $P_{t}\le\alpha$. Under super-uniformity alone, and with no assumptions on temporal dependence, we derive universal IC lower bounds for the average run length (ARL) and for the expected time to the $k$th false alarm ($k$-ARL). When conditional super-uniformity holds, these bounds sharpen to the familiar $\alpha^{-1}$ and $k\alpha^{-1}$ rates, giving simple, distribution-free calibration for $p$-value charts. Beyond thresholding, we use merging functions for dependent $p$-values to build EWMA-like schemes that output, at each time $t$, a valid $p$-value for the hypothesis that the process has remained IC up to $t$, enabling smoothing without ad hoc control limits. We also study uniform EWMA processes, giving explicit distribution formulas and left-tail guarantees. Finally, we propose a modular approach to directional and coordinate localisation in multivariate SPC via closed testing, controlling the family-wise error rate at the time of alarm. Numerical examples illustrate the utility and variety of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17319v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hien Duy Nguyen, Dan Wang</dc:creator>
    </item>
    <item>
      <title>Variational autoencoder for inference of nonlinear mixed effect models based on ordinary differential equations</title>
      <link>https://arxiv.org/abs/2601.17400</link>
      <description>arXiv:2601.17400v1 Announce Type: new 
Abstract: We propose a variational autoencoder (VAE) approach for parameter estimation in nonlinear mixed-effects models based on ordinary differential equations (NLME-ODEs) using longitudinal data from multiple subjects. In moderate dimensions, likelihood-based inference via the stochastic approximation EM algorithm (SAEM) is widely used, but it relies on Markov Chain Monte-Carlo (MCMC) to approximate subject-specific posteriors. As model complexity increases or observations per subject are sparse and irregular, performance often deteriorates due to a complex, multimodal likelihood surface which may lead to MCMC convergence difficulties. We instead estimate parameters by maximizing the evidence lower bound (ELBO), a regularized surrogate for the marginal likelihood. A VAE with a shared encoder amortizes inference of subject-specific random effects by avoiding per-subject optimization and the use of MCMC. Beyond pointwise estimation, we quantify parameter uncertainty using observed-information-based variance estimator and verify that practical identifiability of the model parameters is not compromised by nuisance parameters introduced in the encoder. We evaluate the method in three simulation case studies (pharmacokinetics, humoral response to vaccination, and TGF-$\beta$ activation dynamics in asthmatic airways) and on a real-world antibody kinetics dataset, comparing against SAEM baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17400v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.13140/RG.2.2.23271.71841</arxiv:DOI>
      <dc:creator>Zhe Li, M\'elanie Prague, Rodolphe Thi\'ebaut, Quentin Clairon</dc:creator>
    </item>
    <item>
      <title>A Hybrid Latent-Class Item Response Model for Detecting Measurement Non-Invariance in Ordinal Scales</title>
      <link>https://arxiv.org/abs/2601.17612</link>
      <description>arXiv:2601.17612v1 Announce Type: new 
Abstract: Measurement non-invariance arises when the psychometric properties of a scale differ across subgroups, undermining the validity of group comparisons. At the item level, such non-invariance manifests as differential item functioning (DIF), which occurs when the conditional distribution of an item response differs across groups after controlling for the latent trait. This paper introduces a statistical framework for detecting DIF in ordinal scales without requiring known group labels or anchor items. We propose a hybrid latent-class item response model to ordinal data using a proportional-odds formulation, assigning individuals probabilistically to latent classes. DIF is captured through class-specific shifts in item intercepts and slopes, allowing for both uniform and non-uniform DIF. The identification of DIF effects is achieved via an $L_1$-penalised marginal likelihood function under a sparsity assumption, and model estimation is implemented using a tailored EM algorithm. Simulation studies demonstrate strong recovery of item parameters and both uniform and non-uniform types of DIF. An empirical application to a personality test reveals latent subgroups with distinct response patterns and identifies items that may bias group comparisons. The proposed framework provides a flexible approach to assessing measurement invariance in ordinal scales when comparison groups are unobserved or poorly defined.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17612v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gabriel Wallin, Qi Huang</dc:creator>
    </item>
    <item>
      <title>Two-stage Estimation of Latent Variable Regression Models: A General, Root-N Consistent Solution</title>
      <link>https://arxiv.org/abs/2601.17618</link>
      <description>arXiv:2601.17618v1 Announce Type: new 
Abstract: Latent variable (LV) models are widely used in psychological research to investigate relationships among unobservable constructs. When one-stage estimation of the overall LV model is challenging, two-stage factor score regression (FSR) serves as a convenient alternative: the measurement model is fitted to obtain factor scores in the first stage, which are then used to fit the structural model in the subsequent stage. However, naive application of FSR is known to yield biased estimates of structural parameters. In this paper, we develop a generic bias-correction framework for two-stage estimation of parametric statistical models and tailor it specifically to FSR. Unlike existing bias-corrected FSR solutions, the proposed method applies to a broader class of LV models and does not require computing specific types of factor scores. We establish the root-n consistency of the proposed bias-corrected two-stage estimator under mild regularity conditions. To ensure broad applicability and minimize reliance on complex analytical derivations, we introduce a stochastic approximation algorithm for point estimation and a Monte Carlo-based procedure for variance estimation. In a sequence of Monte Carlo experiments, we demonstrate that the bias-corrected FSR estimator performs comparably to the ``gold standard'' one-stage maximum likelihood estimator. These results suggest that our approach offers a straightforward yet effective alternative for estimating LV models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17618v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Liu, Xiaohui Luo, Jieyuan Dong, Youjin Sung, Yueqin Hu, Hongyun Liu, Daniel J. Bauer</dc:creator>
    </item>
    <item>
      <title>Non-parametric finite-sample credible intervals with one-dimensional priors: a middle ground between Bayesian and frequentist intervals</title>
      <link>https://arxiv.org/abs/2601.17621</link>
      <description>arXiv:2601.17621v1 Announce Type: new 
Abstract: We propose a new type of statistical interval obtained by weakening the definition of a p% credible interval: Having observed the interval (rather than the full dataset) we should put at least a p% belief in it. From a decision-theoretical point of view the resulting intervals occupy a middle ground between frequentist and fully Bayesian statistical intervals, both practically and philosophically: To a p% Bayesian credible interval we should assign (at least a) p% belief also after seeing the full dataset, while p% frequentist intervals we in general only assign a p% belief before seeing either the data or the interval.
  We derive concrete implementations for two cases: estimation of the fraction of a distribution that falls below a certain value (i.e., the CDF), and of the mean of a distribution with bounded support. Even though the problems are fully non parametric, these methods require only one-dimensional priors. They share many of the practical advantages of Bayesian methods while avoiding the complexity of assigning high-dimensional priors altogether. Asymptotically they give intervals equivalent to the fully Bayesian approach and somewhat wider intervals, respectively. We discuss promising directions where the proposed type of interval may provide significant advantages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17621v1</guid>
      <category>stat.ME</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tim Ritmeester</dc:creator>
    </item>
    <item>
      <title>Bidirectional causal inference for binary outcomes in the presence of unmeasured confounding</title>
      <link>https://arxiv.org/abs/2601.17695</link>
      <description>arXiv:2601.17695v1 Announce Type: new 
Abstract: Bidirectional causal relationships arising from mutual interactions between variables are commonly observed within biomedical, econometrical, and social science contexts. When such relationships are further complicated by unobserved factors, identifying causal effects in both directions becomes especially challenging. For continuous variables, methods that utilize two instrumental variables from both directions have been proposed to explore bidirectional causal effects in linear models. However, the existing techniques are not applicable when the key variables of interest are binary. To address these issues, we propose a structural equation modeling approach that links observed binary variables to continuous latent variables through a constrained mapping. We further establish identification results for bidirectional causal effects using a pair of instrumental variables. Additionally, we develop an estimation method for the corresponding causal parameters. We also conduct sensitivity analysis under scenarios where certain identification conditions are violated. Finally, we apply our approach to investigate the bidirectional causal relationship between heart disease and diabetes, demonstrating its practical utility in biomedical research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17695v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yafang Deng, Kang Shuai, Shanshan Luo</dc:creator>
    </item>
    <item>
      <title>Group Permutation Testing in Linear Model: Sharp Validity, Power Improvement, and Extension Beyond Exchangeability</title>
      <link>https://arxiv.org/abs/2601.17734</link>
      <description>arXiv:2601.17734v1 Announce Type: new 
Abstract: We consider finite-sample inference for a single regression coefficient in the fixed-design linear model $Y = Z\beta + bX + \varepsilon$, where $\varepsilon\in\mathbb{R}^n$ may exhibit complex dependence or heterogeneity. We develop a group permutation framework, yielding a unified and analyzable randomization structure for linear-model testing. Under exchangeable errors, we place permutation-augmented regression tests within this group-theoretic setting and show that a grouped version of PALMRT controls Type I error at level at most $2\alpha$ for any permutation group; moreover, we provide an worst-case construction demonstrating that the factor $2$ is sharp and cannot be improved without additional assumptions. Second, we relate the Type II error to a design-dependent geometric separation. We formulate it as a combinatorial optimization problem over permutation groups and bound it under additional mild sub-Gaussian assumptions. For the Type II error upper bound control, we propose a constructive algorithm for the permutation strategy that is better (at least no worse) than the i.i.d. permutation, with simulations empirically indicating substantial power gains, especially under heavy-tailed designs. Finally, we extend group-based CPT and PALMRT beyond exchangeability by connecting rank-based randomization arguments to conformal inference. The resulting weighted group tests satisfy finite-sample Type I error bounds that degrade gracefully with a weighted average of total variation distances between $\varepsilon$ and its group-permuted versions, recovering exact validity when these discrepancies vanish and yielding quantitative robustness otherwise. Taken together, the group-permutation viewpoint provides a principled bridge from exact randomization validity to design-adaptive power and quantitative robustness under approximate symmetries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17734v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zonghan Li, Hongyi Zhou, Zhiheng Zhang</dc:creator>
    </item>
    <item>
      <title>Sensitivity analysis for incremental effects, with application to a study of victimization &amp; offending</title>
      <link>https://arxiv.org/abs/2601.17779</link>
      <description>arXiv:2601.17779v1 Announce Type: new 
Abstract: Sensitivity analysis for unmeasured confounding under incremental propensity score interventions remains relatively underdeveloped. Incremental interventions define stochastic treatment regimes by multiplying the odds of treatment, offering a flexible framework for causal effect estimation. To study incremental effects when there are unobserved confounders, we adopt Rosenbaum's sensitivity model in single time point settings, and propose a doubly robust estimator for the resulting effect bounds. The bound estimators are asymptotically normal under mild conditions on nuisance function estimation. We show that incremental effect bounds can be narrower or wider than those for mean potential outcomes, and that the bounds must lie between the expected minimum and maximum of the conditional bounds on E(Y^0|X) and E(Y^1|X). For time-varying treatments, we consider the marginal sensitivity model. Although sharp bounds for incremental effects are identifiable from longitudinal data under this model, practical estimators have not yet been established; we discuss this challenge and provide partial results toward implementation. Finally, we apply our methods to study the effect of victimization on subsequent offending using data from the National Longitudinal Study of Adolescent to Adult Health (Add Health), illustrating the robustness of our findings in an empirical setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17779v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuying Shen, Valerio Bacak, Edward H. Kennedy</dc:creator>
    </item>
    <item>
      <title>Transportability of Regression Calibration with External Validation Studies for Measurement Error Correction</title>
      <link>https://arxiv.org/abs/2601.17961</link>
      <description>arXiv:2601.17961v1 Announce Type: new 
Abstract: In nutritional and environmental epidemiology, exposures are impractical to measure accurately, while practical measures for these exposures are often subject to substantial measurement error. Regression calibration is among the most used measurement error correction methods with external validation studies. The use of external studies to assess the measurement error process always carries the risk of introducing estimation bias into the main study analysis. Although the transportability of regression calibration is usually assumed for practical epidemiology studies, it has not been well studied. In this work, under the measurement error process with a mixture of Berkson-like and classical-like errors, we investigate conditions under which the effect estimate from regression calibration with an external validation study is unbiased for the association between exposure and health outcome. We further examine departures from the transportability assumption, under which the regression calibration estimator is itself biased. However, we theoretically prove that, in most cases, it yields lower bias than the naive method. The derived conditions are confirmed through simulation studies and further verified in an example investigating the association between the risk of cardiovascular disease and moderate physical activity in the health professional follow-up study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17961v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zexiang Li, Donna Spiegelman, Molin Wang, Zuoheng Wang, Xin Zhou</dc:creator>
    </item>
    <item>
      <title>Examining the Efficacy of Coarsen Exact Matching as an Alternative to Propensity Score Matching</title>
      <link>https://arxiv.org/abs/2601.18013</link>
      <description>arXiv:2601.18013v1 Announce Type: new 
Abstract: Coarsened exact matching (CEM) is often promoted as a superior alternative to propensity score matching (PSM) for addressing imbalance, model dependence, bias, and efficiency. However, this recommendation remains uncertain. First, CEM is commonly mischaracterized as exact matching, despite relying on coarsened rather than original variables. This inexactness in matching introduces residual confounding, which necessitates accurate modeling of the outcome-confounder relationship post-matching to mitigate bias, thereby increasing vulnerability to model misspecification. Second, prior studies overlook that any imbalance between treated and untreated subjects matched on the same propensity score is attributable to random variation. Thus, claims that CEM outperforms PSM in reducing imbalance are unfounded, particularly when using metrics like Mahalanobis distance, which do not account for chance imbalance in PSM. Our simulations show that PSM reduces imbalance more effectively than CEM when evaluated with multivariate standardized mean differences (SMD), and unadjusted analyses indicate greater bias with CEM. While adjusted analyses in both CEM with autocoarsening and PSM may perform similarly when matching on few variables, CEM suffers from the curse of dimensionality as the number of factors increases, resulting in substantial data loss and unstable estimates. Increasing the level of coarsening may mitigate data loss but exacerbates residual confounding and model dependence. In contrast, both analytical results and simulations demonstrate that PSM is more robust to model misspecification and thus less model-dependent. Therefore, CEM is not a viable alternative to PSM when matching on a large number of covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18013v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fei Wan</dc:creator>
    </item>
    <item>
      <title>BASTION: A Bayesian Framework for Trend and Seasonality Decomposition</title>
      <link>https://arxiv.org/abs/2601.18052</link>
      <description>arXiv:2601.18052v1 Announce Type: new 
Abstract: We introduce BASTION (Bayesian Adaptive Seasonality and Trend DecompositION), a flexible Bayesian framework for decomposing time series into trend and multiple seasonality components. We cast the decomposition as a penalized nonparametric regression and establish formal conditions under which the trend and seasonal components are uniquely identifiable, an issue only treated informally in the existing literature. BASTION offers three key advantages over existing decomposition methods: (1) accurate estimation of trend and seasonality amidst abrupt changes, (2) enhanced robustness against outliers and time-varying volatility, and (3) robust uncertainty quantification. We evaluate BASTION against established methods, including TBATS, STR, and MSTL, using both simulated and real-world datasets. By effectively capturing complex dynamics while accounting for irregular components such as outliers and heteroskedasticity, BASTION delivers a more nuanced and interpretable decomposition. To support further research and practical applications, BASTION is available as an R package at https://github.com/Jasoncho0914/BASTION</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18052v1</guid>
      <category>stat.ME</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason B. Cho, David S. Matteson</dc:creator>
    </item>
    <item>
      <title>The effect of collinearity and sample size on linear regression results: a simulation study</title>
      <link>https://arxiv.org/abs/2601.18072</link>
      <description>arXiv:2601.18072v1 Announce Type: new 
Abstract: Background: Multicollinearity inflates the variance of OLS coefficients, widening confidence intervals and reducing inferential reliability. Yet fixed variance inflation factor (VIF) cut-offs are often applied uniformly across studies with very different sample sizes, even though collinearity is a finite-sample problem. We quantify how collinearity and sample size jointly affect linear regression performance and provide practical guidance for interpreting VIFs.
  Methods: We simulated data across sample sizes N=100-100,000 and collinearity levels VIF=1-50. For each scenario we generated 1,000 datasets, fitted OLS models, and assessed coverage, mean absolute error (MAE), bias, traditional power (CI excludes 0), and precision assurance (probability the 95% CI lies within a prespecified margin around the true effect). We also evaluated a biased, misspecified setting by omitting a relevant predictor to study bias amplification.
  Results: Under correct specification, collinearity did not materially affect nominal coverage and did not introduce systematic bias, but it reduced precision in small samples: at N=100, even mild collinearity (VIF&lt;2) inflated MAE and markedly reduced both power metrics, whereas at N&gt;=50,000 estimates were robust even at VIF=50. Under misspecification, collinearity strongly amplified bias, increasing errors, reducing coverage, and sharply degrading both precision assurance and traditional power even at low VIF.
  Conclusion: VIF thresholds should not be applied mechanically. Collinearity must be interpreted in relation to sample size and potential sources of bias; removing predictors solely to reduce VIF can worsen inference via omitted-variable bias. The accompanying heatmaps provide a practical reference across study sizes and modelling assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18072v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephanie CC van der Lubbe, Jose M Valderas, Evangelos Kontopantelis</dc:creator>
    </item>
    <item>
      <title>Maximum-Variance-Reduction Stratification for Improved Subsampling</title>
      <link>https://arxiv.org/abs/2601.18075</link>
      <description>arXiv:2601.18075v1 Announce Type: new 
Abstract: Subsampling is a widely used and effective approach for addressing the computational challenges posed by massive datasets. Substantial progress has been made in developing non-uniform, probability-based subsampling schemes that prioritize more informative observations. We propose a novel stratification mechanism that can be combined with existing subsampling designs to further improve estimation efficiency. We establish the estimator's asymptotic normality and quantify the resulting efficiency gains, which enables a principled procedure for selecting stratification variables and interval boundaries that target reductions in asymptotic variance. The resulting algorithm, Maximum-Variance-Reduction Stratification (MVRS), achieves significant improvements in estimation efficiency while incurring only linear additional computational cost. MVRS is applicable to both non-uniform and uniform subsampling methods. Experiments on simulated and real datasets confirm that MVRS markedly reduces estimator variance and improves accuracy compared with existing subsampling methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18075v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dingyi Wang, Haiying Wang, Qingpei Hu</dc:creator>
    </item>
    <item>
      <title>Preference-based Centrality and Ranking in General Metric Spaces</title>
      <link>https://arxiv.org/abs/2601.18412</link>
      <description>arXiv:2601.18412v1 Announce Type: new 
Abstract: Assessing centrality or ranking observations in multivariate or non-Euclidean spaces is challenging because such data lack an intrinsic order and many classical depth notions lose resolution in high-dimensional or structured settings. We propose a preference-based framework that defines centrality through population pairwise proximity comparisons: a point is central if a typical draw from the underlying distribution tends to lie closer to it than to another. This perspective yields a well-defined statistical functional that generalizes data depth to arbitrary metric spaces. To obtain a coherent one-dimensional representation, we study a Bradley-Terry-Luce projection of the induced preferences and develop two finite-sample estimators based on convex M-estimation and spectral aggregation. The resulting procedures are consistent, scalable, and applicable to high-dimensional and non-Euclidean data, and across a range of examples they exhibit stable ranking behavior and improved resolution relative to classical depth-based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18412v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lingfeng Lyu, Doudou Zhou</dc:creator>
    </item>
    <item>
      <title>Vaccine Efficacy Estimands Implied by Common Estimators Used in Individual Randomized Field Trials</title>
      <link>https://arxiv.org/abs/2601.18587</link>
      <description>arXiv:2601.18587v1 Announce Type: new 
Abstract: We review vaccine efficacy (VE) estimands for susceptibility in individual randomized trials with natural (unmeasured) exposure, where individual responses are measured as time from vaccination until an event (e.g., disease from the infectious agent). Common VE estimands are written as $1-\theta$, where $\theta$ is some ratio effect measure (e.g., ratio of incidence rates, cumulative incidences, hazards, or odds) comparing outcomes under vaccination versus control. Although the ratio effects are approximately equal with low control event rates, we explore the quality of that approximation using a nonparametric formulation. Traditionally, the primary endpoint VE estimands are full immunization (or biological) estimands that represent a subset of the intent-to-treat population, excluding those that have the event before the vaccine has been able to ramp-up to its full effect, requiring care for proper causal interpretation. Besides these primary VE estimands that summarize an effect of the vaccine over the full course of the study, we also consider local VE estimands that measure the effect at particular time points. We discuss interpretational difficulties of local VE estimands (e.g., depletion of susceptibles bias), and using frailty models as sensitivity analyses for the individual-level causal effects over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18587v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael P. Fay, Dean Follmann, Bruce J. Swihart, Lauren E. Dang</dc:creator>
    </item>
    <item>
      <title>Goodness-of-Fit Checks for Joint Models</title>
      <link>https://arxiv.org/abs/2601.18598</link>
      <description>arXiv:2601.18598v1 Announce Type: new 
Abstract: Joint models for longitudinal and time-to-event data are widely used in many disciplines. Nonetheless, existing model comparison criteria do not indicate whether a model adequately fits the data or which components may be misspecified. We introduce a Bayesian posterior predictive checks framework for assessing a joint model's fit to the longitudinal and survival processes and their association. The framework supports multiple settings, including existing subjects, new subjects with only covariates, dynamic prediction at intermediate follow-up times, and cross-validated assessment. For the longitudinal component, goodness-of-fit is assessed through the mean, variance, and correlation structure, while the survival component is evaluated using empirical cumulative distributions and probability integral transforms. The association between processes is examined using time-dependent concordance statistics. We apply these checks to the Bio-SHiFT heart failure study, and a simulation study demonstrates that they can identify model misspecification that standard information criteria fail to detect. The proposed methodology is implemented in the freely available R package JMbayes2.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18598v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dimitris Rizopoulos, Jeremy M. G. Taylor, Isabella Kardys</dc:creator>
    </item>
    <item>
      <title>Contrasting Global and Patient-Specific Regression Models via a Neural Network Representation</title>
      <link>https://arxiv.org/abs/2601.18658</link>
      <description>arXiv:2601.18658v1 Announce Type: new 
Abstract: When developing clinical prediction models, it can be challenging to balance between global models that are valid for all patients and personalized models tailored to individuals or potentially unknown subgroups. To aid such decisions, we propose a diagnostic tool for contrasting global regression models and patient-specific (local) regression models. The core utility of this tool is to identify where and for whom a global model may be inadequate. We focus on regression models and specifically suggest a localized regression approach that identifies regions in the predictor space where patients are not well represented by the global model. As localization becomes challenging when dealing with many predictors, we propose modeling in a dimension-reduced latent representation obtained from an autoencoder. Using such a neural network architecture for dimension reduction enables learning a latent representation simultaneously optimized for both good data reconstruction and for revealing local outcome-related associations suitable for robust localized regression. We illustrate the proposed approach with a clinical study involving patients with chronic obstructive pulmonary disease. Our findings indicate that the global model is adequate for most patients but that indeed specific subgroups benefit from personalized models. We also demonstrate how to map these subgroup models back to the original predictors, providing insight into why the global model falls short for these groups. Thus, the principal application and diagnostic yield of our tool is the identification and characterization of patients or subgroups whose outcome associations deviate from the global model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18658v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Behrens, Daiana Stolz, Eleni Papakonstantinou, Janis M. Nolde, Gabriele Bellerino, Angelika Rohde, Moritz Hess, Harald Binder</dc:creator>
    </item>
    <item>
      <title>Learned harmonic mean estimation of the marginal likelihood for multimodal posteriors with flow matching</title>
      <link>https://arxiv.org/abs/2601.18683</link>
      <description>arXiv:2601.18683v1 Announce Type: new 
Abstract: The marginal likelihood, or Bayesian evidence, is a crucial quantity for Bayesian model comparison but its computation can be challenging for complex models, even in parameters space of moderate dimension. The learned harmonic mean estimator has been shown to provide accurate and robust estimates of the marginal likelihood simply using posterior samples. It is agnostic to the sampling strategy, meaning that the samples can be obtained using any method. This enables marginal likelihood calculation and model comparison with whatever sampling is most suitable for the task. However, the internal density estimators considered previously for the learned harmonic mean can struggle with highly multimodal posteriors. In this work we introduce flow matching-based continuous normalizing flows as a powerful architecture for the internal density estimation of the learned harmonic mean. We demonstrate the ability to handle challenging multimodal posteriors, including an example in 20 parameter dimensions, showcasing the method's ability to handle complex posteriors without the need for fine-tuning or heuristic modifications to the base distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18683v1</guid>
      <category>stat.ME</category>
      <category>astro-ph.IM</category>
      <category>cs.LG</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alicja Polanska, Jason D. McEwen</dc:creator>
    </item>
    <item>
      <title>Data-Driven Information-Theoretic Causal Bounds under Unmeasured Confounding</title>
      <link>https://arxiv.org/abs/2601.17160</link>
      <description>arXiv:2601.17160v1 Announce Type: cross 
Abstract: We develop a data-driven information-theoretic framework for sharp partial identification of causal effects under unmeasured confounding. Existing approaches often rely on restrictive assumptions, such as bounded or discrete outcomes; require external inputs (for example, instrumental variables, proxies, or user-specified sensitivity parameters); necessitate full structural causal model specifications; or focus solely on population-level averages while neglecting covariate-conditional treatment effects. We overcome all four limitations simultaneously by establishing novel information-theoretic, data-driven divergence bounds. Our key theoretical contribution shows that the f-divergence between the observational distribution P(Y | A = a, X = x) and the interventional distribution P(Y | do(A = a), X = x) is upper bounded by a function of the propensity score alone. This result enables sharp partial identification of conditional causal effects directly from observational data, without requiring external sensitivity parameters, auxiliary variables, full structural specifications, or outcome boundedness assumptions. For practical implementation, we develop a semiparametric estimator satisfying Neyman orthogonality (Chernozhukov et al., 2018), which ensures square-root-n consistent inference even when nuisance functions are estimated using flexible machine learning methods. Simulation studies and real-world data applications, implemented in the GitHub repository (https://github.com/yonghanjung/Information-Theretic-Bounds), demonstrate that our framework provides tight and valid causal bounds across a wide range of data-generating processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17160v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yonghan Jung, Bogyeong Kang</dc:creator>
    </item>
    <item>
      <title>Tighter confidence intervals for quantiles of heterogeneous data</title>
      <link>https://arxiv.org/abs/2601.17302</link>
      <description>arXiv:2601.17302v1 Announce Type: cross 
Abstract: It is well known that the asymptotic variance of sample quantiles can be reduced under heterogeneity relative to the i.i.d. setting. However, asymptotically correct confidence intervals for quantiles are not yet available. We propose a novel, consistent estimator of the reduced asymptotic variance arising when quantiles are computed from groups of observations, leading to asymptotically correct confidence intervals. Simulation studies show that our confidence intervals are substantially shorter than those in the i.i.d. case and attain nearly correct coverage across a wide range of heterogeneous settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17302v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John H. J. Einmahl, Yi He</dc:creator>
    </item>
    <item>
      <title>Curvelet-Regularized SPDE Inversion on Piecewise-Planar Fractures with Trace-Graph Coupling</title>
      <link>https://arxiv.org/abs/2601.17485</link>
      <description>arXiv:2601.17485v1 Announce Type: cross 
Abstract: We formulate a sparse-to-dense reconstruction layer for fractured media in which sparse point measurements are mapped onto piecewise-planar fracture supports inferred from 3D trace polylines. Each plane is discretized in local coordinates and estimated via a convex objective that combines a grid SPDE/GMRF quadratic prior with an $\ell_1$ penalty on undecimated discrete curvelet coefficients, targeting anisotropic, fracture-aligned structure that is poorly represented by isotropic smoothness alone. We further define an along-fracture distance through trace-network geodesics and express connectivity-driven regularization as a quadratic form $z^\top P^\top L_G P z$, where $L_G$ is a graph Laplacian on the trace network and $P$ maps plane grids to graph nodes; plane intersections are handled by linear consistency constraints sampled along intersection lines. The resulting optimization admits efficient splitting: sparse linear solves for the quadratic block and coefficient-wise shrinkage for the curvelet block, with standard ADMM convergence under convexity. We specify reproducible synthetic benchmarks, baselines, ablations, and sensitivity studies that isolate directional sparsity and connectivity effects, and provide reference code to generate the figures and quantitative tables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17485v1</guid>
      <category>physics.geo-ph</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. J. Segura</dc:creator>
    </item>
    <item>
      <title>Comparisons of policies based on relevation and replacement by a new one unit in reliability</title>
      <link>https://arxiv.org/abs/2601.17518</link>
      <description>arXiv:2601.17518v1 Announce Type: cross 
Abstract: The purpose of this paper is to study the role of the relevation transform, where a failed unit is replaced by a used unit with the same age as the failed one, as an alternative to the policy based on the replacement by a new one. In particular, we compare the stochastic processes arising from a policy based on the replacement of a failed unit by a new one and from the one in which the unit is being continuously subjected to a relevation policy. The comparisons depend on the aging properties of the units under repair.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17518v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s11749-020-00710-6</arxiv:DOI>
      <arxiv:journal_reference>TEST (2021), 30, 211-227</arxiv:journal_reference>
      <dc:creator> Belzunce,  F.,  Mart\'inez-Riquelme,  C.,  Mercader, J. A.,  Ruiz, J. M</dc:creator>
    </item>
    <item>
      <title>Event history analysis with time-dependent covariates via landmarking supermodels and boosted trees</title>
      <link>https://arxiv.org/abs/2601.17605</link>
      <description>arXiv:2601.17605v1 Announce Type: cross 
Abstract: We propose a nonparametric method for dynamic prediction in event history analysis with high-dimensional, time-dependent covariates. The approach estimates future conditional hazards by combining landmarking supermodels with gradient boosted trees. Unlike joint modeling or Cox landmarking models, the proposed estimator flexibly captures interactions and nonlinear effects without imposing restrictive parametric assumptions or requiring the covariate process to be Markovian. We formulate the approach as a sieve M-estimator and establish weak consistency. Computationally, the problem reduces to a Poisson regression, allowing implementation via standard gradient boosting software. A key theoretical advantage is that the method avoids the temporal inconsistencies that arise in landmark Cox models. Simulation studies demonstrate that the method performs well in a variety of settings, and its practical value is illustrated through an analysis of primary biliary cirrhosis data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17605v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oliver Lunding Sandqvist</dc:creator>
    </item>
    <item>
      <title>The Proximal Surrogate Index: Long-Term Treatment Effects under Unobserved Confounding</title>
      <link>https://arxiv.org/abs/2601.17712</link>
      <description>arXiv:2601.17712v1 Announce Type: cross 
Abstract: We study the identification and estimation of long-term treatment effects under unobserved confounding by combining an experimental sample, where the long-term outcome is missing, with an observational sample, where the treatment assignment is unobserved. While standard surrogate index methods fail when unobserved confounders exist, we establish novel identification results by leveraging proxy variables for the unobserved confounders. We further develop multiply robust estimation and inference procedures based on these results. Applying our method to the Job Corps program, we demonstrate its ability to recover experimental benchmarks even when unobserved confounders bias standard surrogate index estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17712v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ting-Chih Hung, Yu-Chang Chen</dc:creator>
    </item>
    <item>
      <title>Best Feasible Conditional Critical Values for a More Powerful Subvector Anderson-Rubin Test</title>
      <link>https://arxiv.org/abs/2601.17843</link>
      <description>arXiv:2601.17843v1 Announce Type: cross 
Abstract: For subvector inference in the linear instrumental variables model under homoskedasticity but allowing for weak instruments, Guggenberger, Kleibergen, and Mavroeidis (2019) (GKM) propose a conditional subvector Anderson and Rubin (1949) (AR) test that uses data-dependent critical values that adapt to the strength of the parameters not under test. This test has correct size and strictly higher power than the test that uses standard asymptotic chi-square critical values. The subvector AR test is the minimum eigenvalue of a data dependent matrix. The GKM critical value function conditions on the largest eigenvalue of this matrix. We consider instead the data dependent critical value function conditioning on the second-smallest eigenvalue, as this eigenvalue is the appropriate indicator for weak identification. We find that the data dependent critical value function of GKM also applies to this conditioning and show that this test has correct size and power strictly higher than the GKM test when the number of parameters not under test is larger than one. Our proposed procedure further applies to the subvector AR test statistic that is robust to an approximate kronecker product structure of conditional heteroskedasticity as proposed by Guggenberger, Kleibergen, and Mavroeidis (2024), carrying over its power advantage to this setting as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17843v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesse Hoekstra, Frank Windmeijer</dc:creator>
    </item>
    <item>
      <title>Asymptotic properties of multivariate Sz\'{a}sz-Mirakyan distribution estimators on the nonnegative orthant</title>
      <link>https://arxiv.org/abs/2601.18178</link>
      <description>arXiv:2601.18178v1 Announce Type: cross 
Abstract: The asymptotic properties of multivariate Sz\'{a}sz-Mirakyan estimators for distribution functions supported on the nonnegative orthant are investigated. Explicit bias and variance expansions are derived on compact subsets of the interior, yielding sharp mean squared error characterizations and optimal smoothing rates. The analysis shows that the proposed Poisson smoothing yields a non-negligible variance reduction relative to the empirical distribution function, leading to asymptotic efficiency gains that can be quantified through local and global deficiency measures. The behavior of the estimator near the boundary of its support is examined separately. Under a boundary-layer scaling that preserves nondegenerate Poisson smoothing as the evaluation point approaches the boundary of $[0,\infty)^d$, bias and variance expansions are obtained that differ fundamentally from those in the interior region. In particular, the variance reduction mechanism disappears at leading order, implying that no asymptotically optimal smoothing parameter exists in the boundary regime. Central limit theorems and almost sure uniform consistency are also established. Together, these results provide a unified asymptotic theory for multivariate Sz\'{a}sz-Mirakyan distribution estimation and clarify the distinct roles of smoothing in the interior and boundary regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18178v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guanjie Lyu, Fr\'ed\'eric Ouimet, Cindy Feng</dc:creator>
    </item>
    <item>
      <title>A varying-coefficient model for characterizing duration-driven heterogeneity in flood-related health impacts</title>
      <link>https://arxiv.org/abs/2601.18656</link>
      <description>arXiv:2601.18656v1 Announce Type: cross 
Abstract: Previous work revealed associations between flood exposure and adverse health outcomes during and in the aftermath of flood events. Floods are highly heterogeneous events, largely owing to vast differences in flood durations, i.e., flash-floods versus slow-moving floods. However, little to no work has incorporated exposure duration into the modeling of flood-related health impacts or has investigated duration-driven effect heterogeneity. To address this gap, we propose an exposure duration varying coefficient modeling (EDVCM) framework for estimating exposure day-specific health effects of consecutive-day environmental exposures that vary in duration. We develop the EDVCM within an area-level self-matched study design to eliminate time-invariant confounding followed by conditional Poisson regression modeling for exposure effect estimation and adjustment of time-varying confounders. Using a Bayesian framework, we introduce duration- and exposure day-specific exposure coefficients within the conditional Poisson model and assign them a two-dimensional Gaussian process prior to allow for sharing of information across both duration and exposure day. This approach enables highly-resolved insights into duration-driven effect heterogeneity while ensuring model stability through information sharing. Through simulations, we demonstrate that the EDVCM out-performs conventional approaches in terms of both effect estimation and uncertainty quantification. We apply the EDVCM to nationwide, multi-decade Medicare claims data linked with high-resolution flood exposure measures to investigate duration-driven heterogeneity in flood effects on musculoskeletal system disease hospitalizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18656v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarika Aggarwal, Phillip B. Nicol, Brent A. Coull, Rachel C. Nethery</dc:creator>
    </item>
    <item>
      <title>Lord's 'paradox' explained: the 50-year warning on the use of 'change scores' in observational data</title>
      <link>https://arxiv.org/abs/2302.01822</link>
      <description>arXiv:2302.01822v2 Announce Type: replace 
Abstract: In 1967, Frederick Lord posed a conundrum that has confused scientists for over half a century. Subsequently named Lord's 'paradox', the puzzle centres on the observation that two different approaches to estimating the effect of an exposure on the 'change' in an outcome can produce radically different results. Approach 1 involves comparing the mean 'change score' between exposure groups and Approach 2 involves comparing the follow-up outcome between exposure groups conditional on the baseline outcome.
  Resolving this puzzle starts with recognising the three reasons that a variable may change value: (A) 'endogenous change', which represents autocorrelation from baseline, (B) 'random change', which represents change from transient random processes, and (C) 'exogenous change', which represents all non-endogenous, non-random change and contains all change that is potentially modifiable by other baseline variables.
  In observational data, neither Approach 1 nor Approach 2 can reliably estimate the causal effect of an exposure on 'exogenous change' in an outcome. Approach 1 is susceptible to diluted or opposite-sign estimates whenever the exposure causes, or is caused by, the baseline outcome. Approach 2 is susceptible to inflated estimates due to measurement error in the baseline outcome and time-varying confounding bias when the baseline outcome is a mediator. The measurement error can be reduced with multiple measures of the baseline outcome, and the time-varying confounding can be reduced using g- methods.
  Lord's 'paradox' offers several enduring lessons for observational data science including the importance of a well-defined research question and the problems with analysing change scores in observational data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.01822v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter W. G. Tennant, Georgia D. Tomova, Eleanor J. Murray, Kellyn F. Arnold, Matthew P. Fox, Mark S. Gilthorpe</dc:creator>
    </item>
    <item>
      <title>Robust Bayesian Inference on Riemannian Submanifold</title>
      <link>https://arxiv.org/abs/2310.18047</link>
      <description>arXiv:2310.18047v2 Announce Type: replace 
Abstract: Manifold-valued parameters routinely arise in modern statistical applications such as in medical imaging, robotics, and computer vision, to name a few. While traditional Bayesian approaches are applicable to such settings by considering an ambient Euclidean space as the parameter space, we demonstrate the benefits of integrating manifold structure into the Bayesian framework, both theoretically and computationally. Moreover, existing Bayesian approaches which are designed specifically for manifold-valued parameters are primarily model-based, which are typically subject to inaccurate uncertainty quantification under model misspecification. In this article, we propose a robust model-free Bayesian inference for parameters defined on a Riemannian submanifold, which is shown to provide valid uncertainty quantification from a frequentist perspective. Computationally, we propose a Markov chain Monte Carlo to sample from the posterior on the Riemannian submanifold, where the mixing time, in the large sample regime, is shown to depend only on the intrinsic dimension of the parameter space instead of the potentially muchlarger ambient dimension. Our numerical results demonstrate the effectiveness of our approach on a variety of problems, such as multiple quantile regression, reduced-rank regression, and Fr\'echet mean estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18047v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rong Tang, Anirban Bhattacharya, Debdeep Pati, Yun Yang</dc:creator>
    </item>
    <item>
      <title>Change Point Inference for Non-Euclidean Data Sequences using Distance Profiles</title>
      <link>https://arxiv.org/abs/2311.16025</link>
      <description>arXiv:2311.16025v2 Announce Type: replace 
Abstract: We introduce a powerful scan statistic and the corresponding test for detecting the presence and pinpointing the location of a change point within the distribution of a data sequence with the data elements residing in a separable metric space $(\Omega, d)$. These change points mark abrupt shifts in the distribution of the data sequence as characterized using distance profiles, where the distance profile of an element $\omega \in \Omega$ is the distribution of distances from $\omega$ as dictated by the data. This approach is tuning parameter free, fully non-parametric and universally applicable to diverse data types, including distributional and network data, as long as distances between the data objects are available. We obtain an explicit characterization of the asymptotic distribution of the test statistic under the null hypothesis of no change points, rigorous guarantees on the consistency of the test in the presence of change points under fixed and local alternatives and near-optimal convergence of the estimated change point location, all under practicable settings. To compare with state-of-the-art methods we conduct simulations covering multivariate data, bivariate distributional data and sequences of graph Laplacians, and illustrate our method on real data sequences of the U.S. electricity generation compositions and Bluetooth proximity networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.16025v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paromita Dubey, Minxing Zheng</dc:creator>
    </item>
    <item>
      <title>Graphical Principal Component Analysis of Multivariate Functional Time Series</title>
      <link>https://arxiv.org/abs/2401.06990</link>
      <description>arXiv:2401.06990v2 Announce Type: replace 
Abstract: In this paper, we consider multivariate functional time series with a two-way dependence structure: a serial dependence across time points and a graphical interaction among the multiple functions within each time point. We develop the notion of dynamic weak separability, a more general condition than those assumed in literature, and use it to characterize the two-way structure in multivariate functional time series. Based on the proposed weak separability, we develop a unified framework for functional graphical models and dynamic principal component analysis, and further extend it to optimally reconstruct signals from contaminated functional data using graphical-level information. We investigate asymptotic properties of the resulting estimators and illustrate the effectiveness of our proposed approach through extensive simulations. We apply our method to hourly air pollution data that were collected from a monitoring network in China.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06990v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2024.2302198</arxiv:DOI>
      <arxiv:journal_reference>Journal of the American Statistical Association (2024): 1-24</arxiv:journal_reference>
      <dc:creator>Jianbin Tan, Decai Liang, Yongtao Guan, Hui Huang</dc:creator>
    </item>
    <item>
      <title>Green's matching: an efficient approach to parameter estimation in complex dynamic systems</title>
      <link>https://arxiv.org/abs/2403.14531</link>
      <description>arXiv:2403.14531v2 Announce Type: replace 
Abstract: Parameters of differential equations are essential to characterize intrinsic behaviors of dynamic systems. Numerous methods for estimating parameters in dynamic systems are computationally and/or statistically inadequate, especially for complex systems with general-order differential operators, such as motion dynamics. This article presents Green's matching, a computationally tractable and statistically efficient two-step method, which only needs to approximate trajectories in dynamic systems but not their derivatives due to the inverse of differential operators by Green's function. This yields a statistically optimal guarantee for parameter estimation in general-order equations, a feature not shared by existing methods, and provides an efficient framework for broad statistical inferences in complex dynamic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14531v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/jrsssb/qkae031</arxiv:DOI>
      <arxiv:journal_reference>Journal of the Royal Statistical Society: Series B, 2024</arxiv:journal_reference>
      <dc:creator>Jianbin Tan, Guoyu Zhang, Xueqin Wang, Hui Huang, Fang Yao</dc:creator>
    </item>
    <item>
      <title>Combining Climate Models using Bayesian Regression Trees and Random Paths</title>
      <link>https://arxiv.org/abs/2407.13169</link>
      <description>arXiv:2407.13169v2 Announce Type: replace 
Abstract: General circulation models (GCMs) are essential tools for climate studies. Such climate models may have varying accuracy across the input domain, but no model is uniformly best. One can improve climate model prediction performance by integrating multiple models using input-dependent weights. Weight functions modeled using Bayesian Additive Regression Trees (BART) were recently shown to be useful in nuclear physics applications. However, a restriction of that approach was the piecewise constant weight functions. To smoothly integrate multiple climate models, we propose a new tree-based model, Random Path BART (RPBART), that incorporates random path assignments in BART to produce smooth weight functions and smooth predictions, all in a matrix-free formulation. RPBART requires a more complex prior specification, for which we introduce a semivariogram to guide hyperparameter selection. This approach is easy to interpret, computationally cheap, and avoids expensive cross-validation. Finally, we propose a posterior projection technique to enable detailed analysis of the fitted weight functions. This allows us to identify a sparse set of climate models that recovers the underlying system within a given spatial region as well as quantifying model discrepancy given the available model set. Our method is demonstrated on an ensemble of 8 GCMs modeling average monthly surface temperature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13169v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John C. Yannotty, Thomas J. Santner, Bo Li, Matthew T. Pratola</dc:creator>
    </item>
    <item>
      <title>Highly Multivariate Large-scale Spatial Stochastic Processes -- A Cross-Markov Random Field Approach</title>
      <link>https://arxiv.org/abs/2408.10396</link>
      <description>arXiv:2408.10396v3 Announce Type: replace 
Abstract: Key challenges in the analysis of highly multivariate large-scale spatial stochastic processes, where both the number of components (p) and spatial locations (n) can be large, include achieving maximal sparsity in the joint precision matrix, ensuring efficient computational cost for its generation, accommodating asymmetric cross-covariance in the joint covariance matrix, and delivering scientific interpretability. We propose a cross-MRF model class, consisting of a mixed spatial graphical model framework and cross-MRF theory, to collectively address these challenges in one unified framework across two modelling stages. The first stage exploits scientifically informed conditional independence (CI) among p component fields and allows for a step-wise parallel generation of joint covariance and precision matrix, enabling a simultaneous accommodation of asymmetric cross-covariance in joint covariance matrix and sparsity in joint precision matrix. The second stage extends the first-stage CI to doubly CI among both p and n and unearths the cross-MRF via an extended Hammersley-Clifford theorem for multivariate spatial stochastic processes. This results in the sparsest possible representation of the joint precision matrix and ensures its lowest generation complexity. We demonstrate with 1D simulated comparative studies and 2D real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10396v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiaoqing Chen, Peter Diggle, James V. Zidek, Gavin Shaddick</dc:creator>
    </item>
    <item>
      <title>Causal Inference Using Augmented Epidemic Models</title>
      <link>https://arxiv.org/abs/2410.11743</link>
      <description>arXiv:2410.11743v3 Announce Type: replace 
Abstract: Epidemic models describe the evolution of a communicable disease over time. These models are often modified to include the effects of interventions (control measures) such as vaccination, social distancing, school closings etc. Many such models were proposed during the COVID-19 epidemic. Inevitably these models are used to answer the question: What is the effect of the intervention on the epidemic? These models can either be interpreted as data generating models describing observed random variables or as causal models for counterfactual random variables. These two interpretations are often conflated in the literature. We discuss the difference between these two types of models, and then we discuss how to estimate the parameters of the model. Our focus is causal inference for parameters in epidemic models by adjusting for confounders, allowing time varying interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11743v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heejong Bong, Val\'erie Ventura, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Scalable calibration of individual-based epidemic models through categorical approximations</title>
      <link>https://arxiv.org/abs/2501.03950</link>
      <description>arXiv:2501.03950v3 Announce Type: replace 
Abstract: Traditional compartmental models capture population-level dynamics but fail to characterize individual-level risk. The computational cost of exact likelihood evaluation for partially observed individual-based models, however, grows exponentially with the population size, necessitating approximate inference. Existing sampling-based methods usually require multiple simulations of the individuals in the population and rely on bespoke proposal distributions or summary statistics. We propose a deterministic approach to approximating the likelihood using categorical distributions. The approximate likelihood is amenable to automatic differentiation so that parameters can be estimated by maximization or posterior sampling using standard software libraries such as Stan or TensorFlow with little user effort. We prove the consistency of the maximum approximate likelihood estimator. We empirically test our approach on several classes of individual-based models for epidemiology: different sets of disease states, individual-specific transition rates, spatial interactions, under-reporting and misreporting. We demonstrate ground truth recovery and comparable marginal log-likelihood values at substantially reduced cost compared to competitor methods. Finally, we show the scalability and effectiveness of our approach with a real-world application on the 2001 UK Foot-and-Mouth outbreak, where the simplicity of the CAL allows us to include 162775 farms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03950v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Rimella, Nick Whiteley, Chris Jewell, Paul Fearnhead, Michael Whitehouse</dc:creator>
    </item>
    <item>
      <title>Doubly Robust and Efficient Calibration of Prediction Sets for Right-Censored Time-to-Event Outcomes</title>
      <link>https://arxiv.org/abs/2501.04615</link>
      <description>arXiv:2501.04615v3 Announce Type: replace 
Abstract: Our objective is to construct well-calibrated prediction sets for a time-to-event outcome subject to right-censoring with guaranteed coverage. Inspired by modern conformal inference, our approach avoids the need for a well-specified parametric or semiparametric survival model. Unlike existing conformal methods for survival data, which assume Type-I censoring with fully observed censoring times, we consider the more common right-censoring setting in which only the censoring time or only the event time is observed, whichever comes first. Under a standard conditional independence censoring condition, we propose and analyze several lower prediction bounds for the survival time of a future observation, including inverse-probability-of-censoring weighting, and its augmented version based on the semiparametric efficient influence function for the relevant marginal quantile of the outcome accounting for dependent censoring. We formally establish asymptotic coverage guarantees of the proposed methods, and demonstrate both theoretically and through empirical experiments, that the augmented approach substantially improves efficiency over all other proposed methods. Specifically, its coverage error bound is doubly robust, and therefore of second order, thus ensuring that it is asymptotically negligible relative to the coverage error of the other methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04615v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rebecca Farina, Eric J. Tchetgen Tchetgen, Arun Kumar Kuchibhotla</dc:creator>
    </item>
    <item>
      <title>Isotropic covariance functions for disparate spatial resolutions</title>
      <link>https://arxiv.org/abs/2502.15146</link>
      <description>arXiv:2502.15146v2 Announce Type: replace 
Abstract: Distances between sets arise naturally in data fusion problems involving both point-referenced and areal observations, as well as in set-indexed stochastic processes more broadly. However, commonly used constructions of distances on sets, including those derived from the Hausdorff distance, generally fail to be conditionally negative definite, precluding their use in isotropic covariance models. We propose the ball-Hausdorff distance, defined as the Hausdorff distance between the minimum enclosing balls of bounded sets in a metric space. For length spaces, we derive an explicit representation of this distance in terms of the associated centers and radii. We show that the ball-Hausdorff distance is conditionally negative definite whenever the underlying metric is conditionally negative definite, which implies, via Schoenberg's theorem, an isometric embedding into a Hilbert space. As a consequence, broad classes of isotropic covariance functions, including the Mat\'ern and powered exponential families, are valid for random fields indexed by sets. The resulting construction reduces set-to-set dependence to low-dimensional geometric summaries, leading to substantial computational simplifications in covariance evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15146v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas da Cunha Godoy, Marcos Oliveira Prates, Fernando Andr\'es Quintana, Jun Yan</dc:creator>
    </item>
    <item>
      <title>Fiducial Inference for Random-Effects Calibration Models: Advancing Reliable Quantification in Environmental Analytical Chemistry</title>
      <link>https://arxiv.org/abs/2503.04588</link>
      <description>arXiv:2503.04588v3 Announce Type: replace 
Abstract: This article addresses calibration challenges in analytical chemistry by employing a random-effects calibration curve model and its generalizations to capture variability in analyte concentrations. The model is motivated by specific issues in analytical chemistry, where measurement errors remain constant at low concentrations but increase proportionally as concentrations rise. To account for this, the model permits the parameters of the calibration curve, which relate instrument responses to true concentrations, to vary across different laboratories, thereby reflecting real-world variability in measurement processes. Traditional large-sample interval estimation methods are inadequate for small samples, leading to the use of an alternative approach, namely the fiducial approach. The calibration curve that accurately captures the heteroscedastic nature of the data, results in more reliable estimates across diverse laboratory conditions. It turns out that the fiducial approach, when used to construct a confidence interval for an unknown concentration, produces a slightly wider width while achieving the desired coverage probability. Applications considered include the determination of the presence of an analyte and the interval estimation of an unknown true analyte concentration. The proposed method is demonstrated for both simulated and real interlaboratory data, including examples involving copper and cadmium in distilled water.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04588v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soumya Sahu, Thomas Mathew, Robert Gibbons, Dulal K. Bhaumik</dc:creator>
    </item>
    <item>
      <title>Assessing the impact of variance heterogeneity and misspecification in mixed-effects location-scale models</title>
      <link>https://arxiv.org/abs/2505.18038</link>
      <description>arXiv:2505.18038v2 Announce Type: replace 
Abstract: Linear Mixed Model (LMM) is a common statistical approach to model the relation between exposure and outcome while capturing individual variability through random effects. However, this model assumes the homogeneity of the error term's variance. Breaking this assumption, known as homoscedasticity, can bias estimates and, consequently, may change a study's conclusions. If this assumption is unmet, the mixed-effect location-scale model (MELSM) offers a solution to account for within-individual variability. Our work explores how LMMs and MELSMs behave when the homoscedasticity assumption is not met. Further, we study how misspecification affects inference for MELSM. To this aim, we propose a simulation study with longitudinal data and evaluate the estimates' bias and coverage. Our simulations show that neglecting heteroscedasticity in LMMs leads to loss of coverage for the estimated coefficients and biases the estimates of the standard deviations of the random effects. In MELSMs, scale misspecification does not bias the location model, but location misspecification alters the scale estimates. Our simulation study illustrates the importance of modelling heteroscedasticity, with potential implications beyond mixed effect models, for generalised linear mixed models for non-normal outcomes and joint models with survival data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18038v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1186/s12874-025-02755-3</arxiv:DOI>
      <dc:creator>Vincent Jeanselme, Marco Palma, Jessica K Barrett</dc:creator>
    </item>
    <item>
      <title>NExON-Bayes: A Bayesian approach to network estimation informed by ordinal covariates</title>
      <link>https://arxiv.org/abs/2508.21649</link>
      <description>arXiv:2508.21649v2 Announce Type: replace 
Abstract: In heterogeneous disease settings, accounting for intrinsic sample variability is crucial for obtaining reliable and interpretable omic network estimates. However, most graphical model analyses of biomedical data assume homogeneous conditional dependence structures, potentially leading to misleading conclusions. To address this, we propose a joint Gaussian graphical model that leverages sample-level ordinal covariates (e.g., disease stage) to account for heterogeneity and improve the estimation of partial correlation structures. Our modelling framework, called NExON-Bayes, extends the graphical spike-and-slab framework to account for ordinal covariates, jointly estimating their relevance to the graph structure and leveraging them to improve the accuracy of network estimation. To scale to high-dimensional omic settings, we develop an efficient variational inference algorithm tailored to our model. Through simulations, we demonstrate that our method outperforms the vanilla graphical spike-and-slab (with no covariate information), as well as other state-of-the-art network approaches which exploit covariate information. Applying our method to reverse phase protein array data from patients diagnosed with stage I, II or III breast carcinoma, we estimate the behaviour of proteomic networks as breast carcinoma progresses. Our model provides insights not only through inspection of the estimated proteomic networks, but also of the estimated ordinal covariate dependencies of key groups of proteins within those networks, offering a comprehensive understanding of how biological pathways shift across disease stages.
  Availability and Implementation: A user-friendly R package for NExON-Bayes with tutorials is available on Github at github.com/jf687/NExON.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21649v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph Feest, H\'el\`ene Ruffieux, Camilla Lingj{\ae}rde, Xiaoyue Xi</dc:creator>
    </item>
    <item>
      <title>Efficient Difference-in-Differences Estimation when Outcomes are Missing at Random</title>
      <link>https://arxiv.org/abs/2509.25009</link>
      <description>arXiv:2509.25009v2 Announce Type: replace 
Abstract: The Difference-in-Differences (DiD) method is a fundamental tool for causal inference, yet its application is often complicated by missing data. Although recent work has developed robust DiD estimators for complex settings like staggered treatment adoption, these methods typically assume complete data and fail to address the critical challenge of outcomes that are missing at random (MAR) -- a common problem that invalidates standard estimators. We develop a rigorous framework, rooted in semiparametric theory, for identifying and efficiently estimating the Average Treatment Effect on the Treated (ATT) when either pre- or post-treatment (or both) outcomes are missing at random. We first establish nonparametric identification of the ATT under two minimal sets of sufficient conditions. For each, we derive the semiparametric efficiency bound, which provides a formal benchmark for asymptotic optimality. We then propose novel estimators that are asymptotically efficient, achieving this theoretical bound. A key feature of our estimators is their multiple robustness, which ensures consistency even if some nuisance function models are misspecified. We validate the properties of our estimators and showcase their broad applicability through an extensive simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25009v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Testa, Edward H. Kennedy, Matthew Reimherr</dc:creator>
    </item>
    <item>
      <title>Maximum softly penalised likelihood in factor analysis</title>
      <link>https://arxiv.org/abs/2510.06465</link>
      <description>arXiv:2510.06465v2 Announce Type: replace 
Abstract: Estimation in exploratory factor analysis often yields estimates on the boundary of the parameter space. Such occurrences, known as Heywood cases, are characterised by non-positive variance estimates and can cause issues in numerical optimisation procedures or convergence failures, which, in turn, can lead to misleading inferences, particularly regarding factor scores and model selection. We derive sufficient conditions on the model and a penalty to the log-likelihood function that i) guarantee the existence of maximum penalised likelihood estimates in the interior of the parameter space, and ii) ensure that the corresponding estimators possess the desirable asymptotic properties expected by the maximum likelihood estimator, namely consistency and asymptotic normality. Consistency and asymptotic normality are achieved when the penalisation is soft enough, in a way that adapts to the information accumulation about the model parameters. We formally show, for the first time, that the penalties of Akaike (1987) and Hirose et al. (2011) to the log-likelihood of the normal linear factor model satisfy the conditions for existence, and, hence, deal with Heywood cases. Their vanilla versions, though, can result in questionable finite-sample properties in estimation, inference, and model selection. The maximum softly-penalised likelihood framework we introduce enables the careful scaling of those penalties to ensure that the resulting estimation and inference procedures inherit the ML estimator's optimal properties. Through comprehensive simulation studies and the analysis of real data sets, we illustrate the desirable finite-sample properties of the maximum softly penalised likelihood estimators and associated procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06465v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philipp Sterzinger, Ioannis Kosmids, Irini Moustaki</dc:creator>
    </item>
    <item>
      <title>Set-valued data analysis for interlaboratory comparisons</title>
      <link>https://arxiv.org/abs/2510.23170</link>
      <description>arXiv:2510.23170v2 Announce Type: replace 
Abstract: This article introduces tools to analyze set-valued data statistically. The tools were initially developed to analyze results from an interlaboratory comparison made by the Electromagnetic Compatibility Working Group of Eurolab France, where the goal was to select a consensual set of injection points on an electrical device. Families based on the Hamming-distance from a consensus set are introduced and Fisher's noncentral hypergeometric distribution is proposed to model the number of deviations. A Bayesian approach is used and two types of techniques are proposed for the inference. Hierarchical models are also considered to quantify a possible within-laboratory effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23170v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S\'ebastien Petit (LNE), S\'ebastien Marmin (LNE), Nicolas Fischer (LNE)</dc:creator>
    </item>
    <item>
      <title>Bias-Aware Conformal Prediction for Metric-Based Imaging Pipelines</title>
      <link>https://arxiv.org/abs/2410.05263</link>
      <description>arXiv:2410.05263v2 Announce Type: replace-cross 
Abstract: Reliable confidence measures of metrics derived from medical imaging reconstruction pipelines would improve the standard of decision-making in many clinical workflows. Conformal Prediction (CP) provides a robust framework for producing calibrated prediction intervals, but standard CP formulations face a critical challenge in the imaging pipeline: common mismatches between image reconstruction objectives and downstream metrics can introduce systematic prediction deviations from ground truth values, known as bias. These biases in turn compromise the efficiency of prediction intervals, which is a problem that has been unexplored in the CP literature. In this study, we formalize the behavior of symmetric (where bounds expand equally in both directions) and asymmetric (where bounds expand unequally) formulations for common non-conformity scores in CP in the presence of bias, and argue that this measurable bias must inform the choice of CP formulation. We theoretically and empirically demonstrate that symmetric intervals are inflated by a factor of two times the magnitude of bias while asymmetric intervals remain unaffected by bias, and provide conditions under which each formulation produces tighter intervals. We empirically validated our theoretical analyses on sparse-view CT reconstruction for downstream radiotherapy planning. Our work enables users of medical imaging pipelines to proactively select optimal CP formulations, thereby improving interval length efficiency for critical downstream metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05263v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matt Y. Cheung, Tucker J. Netherton, Laurence E. Court, Ashok Veeraraghavan, Guha Balakrishnan</dc:creator>
    </item>
    <item>
      <title>Towards Interpretable Deep Generative Models via Causal Representation Learning</title>
      <link>https://arxiv.org/abs/2504.11609</link>
      <description>arXiv:2504.11609v2 Announce Type: replace-cross 
Abstract: Recent developments in generative artificial intelligence (AI) rely on machine learning techniques such as deep learning and generative modeling to achieve state-of-the-art performance across wide-ranging domains. These methods' surprising performance is due in part to their ability to learn implicit "representations" of complex, multi-modal data. Unfortunately, deep neural networks are notoriously black boxes that obscure these representations, making them difficult to interpret or analyze. To resolve these difficulties, one approach is to build new interpretable neural network models from the ground up. This is the goal of the emerging field of causal representation learning (CRL) that uses causality as a vector for building flexible, interpretable, and transferable generative AI. CRL can be seen as a synthesis of three intrinsically statistical ideas: (i) latent variable models such as factor analysis; (ii) causal graphical models with latent variables; and (iii) nonparametric statistics and deep learning. This paper introduces CRL from a statistical perspective, focusing on connections to classical models as well as statistical and causal identifiability results. We also highlights key application areas, implementation strategies, and open statistical questions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11609v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gemma E. Moran, Bryon Aragam</dc:creator>
    </item>
    <item>
      <title>Streaming Sliced Optimal Transport</title>
      <link>https://arxiv.org/abs/2505.06835</link>
      <description>arXiv:2505.06835v3 Announce Type: replace-cross 
Abstract: Sliced optimal transport (SOT), or sliced Wasserstein (SW) distance, is widely recognized for its statistical and computational scalability. In this work, we further enhance computational scalability by proposing the first method for estimating SW from sample streams, called \emph{streaming sliced Wasserstein} (Stream-SW). To define Stream-SW, we first introduce a streaming estimator of the one-dimensional Wasserstein distance (1DW). Since the 1DW has a closed-form expression, given by the absolute difference between the quantile functions of the compared distributions, we leverage quantile approximation techniques for sample streams to define a streaming 1DW estimator. By applying the streaming 1DW to all projections, we obtain Stream-SW. The key advantage of Stream-SW is its low memory complexity while providing theoretical guarantees on the approximation error. We demonstrate that Stream-SW achieves a more accurate approximation of SW than random subsampling, with lower memory consumption, when comparing Gaussian distributions and mixtures of Gaussians from streaming samples. Additionally, we conduct experiments on point cloud classification, point cloud gradient flows, and streaming change point detection to further highlight the favorable performance of the proposed Stream-SW</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06835v3</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khai Nguyen</dc:creator>
    </item>
    <item>
      <title>Bilateral Distribution Compression: Reducing Both Data Size and Dimensionality</title>
      <link>https://arxiv.org/abs/2509.17543</link>
      <description>arXiv:2509.17543v5 Announce Type: replace-cross 
Abstract: Existing distribution compression methods reduce the number of observations in a dataset by minimising the Maximum Mean Discrepancy (MMD) between original and compressed sets, but modern datasets are often large in both sample size and dimensionality. We propose Bilateral Distribution Compression (BDC), a two-stage framework that compresses along both axes while preserving the underlying distribution, with overall linear time and memory complexity in dataset size and dimension. Central to BDC is the Decoded MMD (DMMD), which we introduce to quantify the discrepancy between the original data and a compressed set decoded from a low-dimensional latent space. BDC proceeds by (i) learning a low-dimensional projection using the Reconstruction MMD (RMMD), and (ii) optimising a latent compressed set with the Encoded MMD (EMMD). We show that this procedure minimises the DMMD, guaranteeing that the compressed set faithfully represents the original distribution. Experiments show that BDC can achieve comparable or superior downstream task performance to ambient-space compression at substantially lower cost and with significantly higher rates of compression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17543v5</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dominic Broadbent, Nick Whiteley, Robert Allison, Tom Lovett</dc:creator>
    </item>
  </channel>
</rss>
