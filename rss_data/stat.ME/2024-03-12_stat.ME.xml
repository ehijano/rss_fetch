<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 12 Mar 2024 04:00:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 12 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>TSSS: A Novel Triangulated Spherical Spline Smoothing for Surface-based Data</title>
      <link>https://arxiv.org/abs/2403.05644</link>
      <description>arXiv:2403.05644v1 Announce Type: new 
Abstract: Surface-based data is commonly observed in diverse practical applications spanning various fields. In this paper, we introduce a novel nonparametric method to discover the underlying signals from data distributed on complex surface-based domains. Our approach involves a penalized spline estimator defined on a triangulation of surface patches, which enables effective signal extraction and recovery. The proposed method offers several advantages over existing methods, including superior handling of "leakage" or "boundary effects" over complex domains, enhanced computational efficiency, and potential applications in analyzing sparse and irregularly distributed data on complex objects. We provide rigorous theoretical guarantees for the proposed method, including convergence rates of the estimator in both the $L_2$ and supremum norms, as well as the asymptotic normality of the estimator. We also demonstrate that the convergence rates achieved by our estimation method are optimal within the framework of nonparametric estimation. Furthermore, we introduce a bootstrap method to quantify the uncertainty associated with the proposed estimators accurately. The superior performance of the proposed method is demonstrated through simulation experiments and data applications on cortical surface functional magnetic resonance imaging data and oceanic near-surface atmospheric data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05644v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhiling Gu, Shan Yu, Guannan Wang, Ming-Jun Lai, Li Wang</dc:creator>
    </item>
    <item>
      <title>Minor Issues Escalated to Critical Levels in Large Samples: A Permutation-Based Fix</title>
      <link>https://arxiv.org/abs/2403.05647</link>
      <description>arXiv:2403.05647v1 Announce Type: new 
Abstract: In the big data era, the need to reevaluate traditional statistical methods is paramount due to the challenges posed by vast datasets. While larger samples theoretically enhance accuracy and hypothesis testing power without increasing false positives, practical concerns about inflated Type-I errors persist. The prevalent belief is that larger samples can uncover subtle effects, necessitating dual consideration of p-value and effect size. Yet, the reliability of p-values from large samples remains debated.
  This paper warns that larger samples can exacerbate minor issues into significant errors, leading to false conclusions. Through our simulation study, we demonstrate how growing sample sizes amplify issues arising from two commonly encountered violations of model assumptions in real-world data and lead to incorrect decisions. This underscores the need for vigilant analytical approaches in the era of big data. In response, we introduce a permutation-based test to counterbalance the effects of sample size and assumption discrepancies by neutralizing them between actual and permuted data. We demonstrate that this approach effectively stabilizes nominal Type I error rates across various sample sizes, thereby ensuring robust statistical inferences even amidst breached conventional assumptions in big data.
  For reproducibility, our R codes are publicly available at: \url{https://github.com/ubcxzhang/bigDataIssue}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05647v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xuekui Zhang, Li Xing, Jing Zhang, Soojeong Kim</dc:creator>
    </item>
    <item>
      <title>PROTEST: Nonparametric Testing of Hypotheses Enhanced by Experts' Utility Judgements</title>
      <link>https://arxiv.org/abs/2403.05655</link>
      <description>arXiv:2403.05655v1 Announce Type: new 
Abstract: Instead of testing solely a precise hypothesis, it is often useful to enlarge it with alternatives that are deemed to differ from it negligibly. For instance, in a bioequivalence study one might consider the hypothesis that the concentration of an ingredient is exactly the same in two drugs. In such a context, it might be more relevant to test the enlarged hypothesis that the difference in concentration between the drugs is of no practical significance. While this concept is not alien to Bayesian statistics, applications remain confined to parametric settings and strategies on how to effectively harness experts' intuitions are often scarce or nonexistent. To resolve both issues, we introduce PROTEST, an accessible nonparametric testing framework that seamlessly integrates with Markov Chain Monte Carlo (MCMC) methods. We develop expanded versions of the model adherence, goodness-of-fit, quantile and two-sample tests. To demonstrate how PROTEST operates, we make use of examples, simulated studies - such as testing link functions in a binary regression setting, as well as a comparison between the performance of PROTEST and the PTtest (Holmes et al., 2015) - and an application with data on neuron spikes. Furthermore, we address the crucial issue of selecting the threshold - which controls how much a hypothesis is to be expanded - even when intuitions are limited or challenging to quantify.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05655v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rodrigo F. L. Lassance, Rafael Izbicki, Rafael B. Stern</dc:creator>
    </item>
    <item>
      <title>Debiased Projected Two-Sample Comparisonscfor Single-Cell Expression Data</title>
      <link>https://arxiv.org/abs/2403.05679</link>
      <description>arXiv:2403.05679v1 Announce Type: new 
Abstract: We study several variants of the high-dimensional mean inference problem motivated by modern single-cell genomics data. By taking advantage of low-dimensional and localized signal structures commonly seen in such data, our proposed methods not only have the usual frequentist validity but also provide useful information on the potential locations of the signal if the null hypothesis is rejected. Our method adaptively projects the high-dimensional vector onto a low-dimensional space, followed by a debiasing step using the semiparametric double-machine learning framework. Our analysis shows that debiasing is unnecessary under the global null, but necessary under a ``projected null'' that is of scientific interest. We also propose an ``anchored projection'' to maximize the power while avoiding the degeneracy issue under the null. Experiments on synthetic data and a real single-cell sequencing dataset demonstrate the effectiveness and interpretability of our methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05679v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianyu Zhang, Jing Lei, Kathryn Roeder</dc:creator>
    </item>
    <item>
      <title>Model-Free Local Recalibration of Neural Networks</title>
      <link>https://arxiv.org/abs/2403.05756</link>
      <description>arXiv:2403.05756v1 Announce Type: new 
Abstract: Artificial neural networks (ANNs) are highly flexible predictive models. However, reliably quantifying uncertainty for their predictions is a continuing challenge. There has been much recent work on "recalibration" of predictive distributions for ANNs, so that forecast probabilities for events of interest are consistent with certain frequency evaluations of them. Uncalibrated probabilistic forecasts are of limited use for many important decision-making tasks. To address this issue, we propose a localized recalibration of ANN predictive distributions using the dimension-reduced representation of the input provided by the ANN hidden layers. Our novel method draws inspiration from recalibration techniques used in the literature on approximate Bayesian computation and likelihood-free inference methods. Most existing calibration methods for ANNs can be thought of as calibrating either on the input layer, which is difficult when the input is high-dimensional, or the output layer, which may not be sufficiently flexible. Through a simulation study, we demonstrate that our method has good performance compared to alternative approaches, and explore the benefits that can be achieved by localizing the calibration based on different layers of the network. Finally, we apply our proposed method to a diamond price prediction problem, demonstrating the potential of our approach to improve prediction and uncertainty quantification in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05756v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R. TorresUniversity of Bras\'ilia, D. J. NottNational University of Singapore, S. A. SissonUniversity of New South Wales, Sydney, T. RodriguesUniversity of Bras\'ilia, J. G. ReisUniversity of Bras\'ilia, G. S. RodriguesUniversity of Bras\'ilia</dc:creator>
    </item>
    <item>
      <title>Distributed Conditional Feature Screening via Pearson Partial Correlation with FDR Control</title>
      <link>https://arxiv.org/abs/2403.05792</link>
      <description>arXiv:2403.05792v1 Announce Type: new 
Abstract: This paper studies the distributed conditional feature screening for massive data with ultrahigh-dimensional features. Specifically, three distributed partial correlation feature screening methods (SAPS, ACPS and JDPS methods) are firstly proposed based on Pearson partial correlation. The corresponding consistency of distributed estimation and the sure screening property of feature screening methods are established. Secondly, because using a hard threshold in feature screening will lead to a high false discovery rate (FDR), this paper develops a two-step distributed feature screening method based on knockoff technique to control the FDR. It is shown that the proposed method can control the FDR in the finite sample, and also enjoys the sure screening property under some conditions. Different from the existing screening methods, this paper not only considers the influence of a conditional variable on both the response variable and feature variables in variable screening, but also studies the FDR control issue. Finally, the effectiveness of the proposed methods is confirmed by numerical simulations and a real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05792v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naiwen Pang, Xiaochao Xia</dc:creator>
    </item>
    <item>
      <title>Online Identification of Stochastic Continuous-Time Wiener Models Using Sampled Data</title>
      <link>https://arxiv.org/abs/2403.05899</link>
      <description>arXiv:2403.05899v1 Announce Type: new 
Abstract: It is well known that ignoring the presence of stochastic disturbances in the identification of stochastic Wiener models leads to asymptotically biased estimators. On the other hand, optimal statistical identification, via likelihood-based methods, is sensitive to the assumptions on the data distribution and is usually based on relatively complex sequential Monte Carlo algorithms. We develop a simple recursive online estimation algorithm based on an output-error predictor, for the identification of continuous-time stochastic parametric Wiener models through stochastic approximation. The method is applicable to generic model parameterizations and, as demonstrated in the numerical simulation examples, it is robust with respect to the assumptions on the spectrum of the disturbance process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05899v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohamed Abdalmoaty, Efe C. Balta, John Lygeros, Roy S. Smith</dc:creator>
    </item>
    <item>
      <title>Sample Size Selection under an Infill Asymptotic Domain</title>
      <link>https://arxiv.org/abs/2403.05969</link>
      <description>arXiv:2403.05969v1 Announce Type: new 
Abstract: Experimental studies often fail to appropriately account for the number of collected samples within a fixed time interval for functional responses. Data of this nature appropriately falls under an Infill Asymptotic domain that is constrained by time and not considered infinite. Therefore, the sample size should account for this infill asymptotic domain. This paper provides general guidance on selecting an appropriate size for an experimental study for various simple linear regression models and tuning parameter values of the covariance structure used under an asymptotic domain, an Ornstein-Uhlenbeck process. Selecting an appropriate sample size is determined based on the percent of total variation that is captured at any given sample size for each parameter. Additionally, guidance on the selection of the tuning parameter is given by linking this value to the signal-to-noise ratio utilized for power calculations under design of experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05969v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cory W. Natoli, Edward D. White, Beau A. Nunnally, Alex J. Gutman, Raymond R. Hill</dc:creator>
    </item>
    <item>
      <title>Quantifying the Uncertainty of Imputed Demographic Disparity Estimates: The Dual-Bootstrap</title>
      <link>https://arxiv.org/abs/2403.06238</link>
      <description>arXiv:2403.06238v1 Announce Type: new 
Abstract: Measuring average differences in an outcome across racial or ethnic groups is a crucial first step for equity assessments, but researchers often lack access to data on individuals' races and ethnicities to calculate them. A common solution is to impute the missing race or ethnicity labels using proxies, then use those imputations to estimate the disparity. Conventional standard errors mischaracterize the resulting estimate's uncertainty because they treat the imputation model as given and fixed, instead of as an unknown object that must be estimated with uncertainty. We propose a dual-bootstrap approach that explicitly accounts for measurement uncertainty and thus enables more accurate statistical inference, which we demonstrate via simulation. In addition, we adapt our approach to the commonly used Bayesian Improved Surname Geocoding (BISG) imputation algorithm, where direct bootstrapping is infeasible because the underlying Census Bureau data are unavailable. In simulations, we find that measurement uncertainty is generally insignificant for BISG except in particular circumstances; bias, not variance, is likely the predominant source of error. We apply our method to quantify the uncertainty of prevalence estimates of common health conditions by race using data from the American Family Cohort.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06238v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Lu, Jia Wan, Derek Ouyang, Jacob Goldin, Daniel E. Ho</dc:creator>
    </item>
    <item>
      <title>A doubly robust estimator for the Mann Whitney Wilcoxon Rank Sum Test when applied for causal inference in observational studies</title>
      <link>https://arxiv.org/abs/2403.06783</link>
      <description>arXiv:2403.06783v1 Announce Type: new 
Abstract: The Mann-Whitney-Wilcoxon rank sum test (MWWRST) is a widely used method for comparing two treatment groups in randomized control trials, particularly when dealing with highly skewed data. However, when applied to observational study data, the MWWRST often yields invalid results for causal inference. To address this limitation, Wu et al. (2014) introduced an approach that incorporates inverse probability weighting (IPW) into this rank-based statistics to mitigate confounding effects. Subsequently, Mao (2018), Zhang et al. (2019), and Ai et al. (2020) extended this IPW estimator to develop doubly robust estimators.
  Nevertheless, each of these approaches has notable limitations. Mao's method imposes stringent assumptions that may not align with real-world study data. Zhang et al.'s (2019) estimators rely on bootstrap inference, which suffers from computational inefficiency and lacks known asymptotic properties. Meanwhile, Ai et al. (2020) primarily focus on testing the null hypothesis of equal distributions between two groups, which is a more stringent assumption that may not be well-suited to the primary practical application of MWWRST.
  In this paper, we aim to address these limitations by leveraging functional response models (FRM) to develop doubly robust estimators. We demonstrate the performance of our proposed approach using both simulated and real study data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06783v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruohui Chen, Tuo Lin, Lin Liu, Jinyuan Liu, Ruifeng Chen, Jingjing Zou, Chenyu Liu, Loki Natarajan, Tang Wang, Xinlian Zhang, Xin Tu</dc:creator>
    </item>
    <item>
      <title>Non-robustness of diffusion estimates on networks with measurement error</title>
      <link>https://arxiv.org/abs/2403.05704</link>
      <description>arXiv:2403.05704v1 Announce Type: cross 
Abstract: Network diffusion models are used to study things like disease transmission, information spread, and technology adoption. However, small amounts of mismeasurement are extremely likely in the networks constructed to operationalize these models. We show that estimates of diffusions are highly non-robust to this measurement error. First, we show that even when measurement error is vanishingly small, such that the share of missed links is close to zero, forecasts about the extent of diffusion will greatly underestimate the truth. Second, a small mismeasurement in the identity of the initial seed generates a large shift in the locations of expected diffusion path. We show that both of these results still hold when the vanishing measurement error is only local in nature. Such non-robustness in forecasting exists even under conditions where the basic reproductive number is consistently estimable. Possible solutions, such as estimating the measurement error or implementing widespread detection efforts, still face difficulties because the number of missed links are so small. Finally, we conduct Monte Carlo simulations on simulated networks, and real networks from three settings: travel data from the COVID-19 pandemic in the western US, a mobile phone marketing campaign in rural India, and in an insurance experiment in China.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05704v1</guid>
      <category>econ.EM</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Arun G. Chandrasekhar, Paul Goldsmith-Pinkham, Tyler H. McCormick, Samuel Thau, Jerry Wei</dc:creator>
    </item>
    <item>
      <title>Membership Testing in Markov Equivalence Classes via Independence Query Oracles</title>
      <link>https://arxiv.org/abs/2403.05759</link>
      <description>arXiv:2403.05759v1 Announce Type: cross 
Abstract: Understanding causal relationships between variables is a fundamental problem with broad impact in numerous scientific fields. While extensive research has been dedicated to learning causal graphs from data, its complementary concept of testing causal relationships has remained largely unexplored. While learning involves the task of recovering the Markov equivalence class (MEC) of the underlying causal graph from observational data, the testing counterpart addresses the following critical question: Given a specific MEC and observational data from some causal graph, can we determine if the data-generating causal graph belongs to the given MEC?
  We explore constraint-based testing methods by establishing bounds on the required number of conditional independence tests. Our bounds are in terms of the size of the maximum undirected clique ($s$) of the given MEC. In the worst case, we show a lower bound of $\exp(\Omega(s))$ independence tests. We then give an algorithm that resolves the task with $\exp(O(s))$ tests, matching our lower bound. Compared to the learning problem, where algorithms often use a number of independence tests that is exponential in the maximum in-degree, this shows that testing is relatively easier. In particular, it requires exponentially less independence tests in graphs featuring high in-degrees and small clique sizes. Additionally, using the DAG associahedron, we provide a geometric interpretation of testing versus learning and discuss how our testing result can aid learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05759v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaqi Zhang, Kirankumar Shiragur, Caroline Uhler</dc:creator>
    </item>
    <item>
      <title>Semiparametric Inference for Regression-Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2403.05803</link>
      <description>arXiv:2403.05803v1 Announce Type: cross 
Abstract: Treatment effects in regression discontinuity designs (RDDs) are often estimated using local regression methods. However, global approximation methods are generally deemed inefficient. In this paper, we propose a semiparametric framework tailored for estimating treatment effects in RDDs. Our global approach conceptualizes the identification of treatment effects within RDDs as a partially linear modeling problem, with the linear component capturing the treatment effect. Furthermore, we utilize the P-spline method to approximate the nonparametric function and develop procedures for inferring treatment effects within this framework. We demonstrate through Monte Carlo simulations that the proposed method performs well across various scenarios. Furthermore, we illustrate using real-world datasets that our global approach may result in more reliable inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05803v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rong J. B. Zhu, Weiwei Jiang</dc:creator>
    </item>
    <item>
      <title>Estimating Causal Effects of Discrete and Continuous Treatments with Binary Instruments</title>
      <link>https://arxiv.org/abs/2403.05850</link>
      <description>arXiv:2403.05850v1 Announce Type: cross 
Abstract: We propose an instrumental variable framework for identifying and estimating average and quantile effects of discrete and continuous treatments with binary instruments. The basis of our approach is a local copula representation of the joint distribution of the potential outcomes and unobservables determining treatment assignment. This representation allows us to introduce an identifying assumption, so-called copula invariance, that restricts the local dependence of the copula with respect to the treatment propensity. We show that copula invariance identifies treatment effects for the entire population and other subpopulations such as the treated. The identification results are constructive and lead to straightforward semiparametric estimation procedures based on distribution regression. An application to the effect of sleep on well-being uncovers interesting patterns of heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05850v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Chernozhukov, Iv\'an Fern\'andez-Val, Sukjin Han, Kaspar W\"uthrich</dc:creator>
    </item>
    <item>
      <title>Estimating Factor-Based Spot Volatility Matrices with Noisy and Asynchronous High-Frequency Data</title>
      <link>https://arxiv.org/abs/2403.06246</link>
      <description>arXiv:2403.06246v1 Announce Type: cross 
Abstract: We propose a new estimator of high-dimensional spot volatility matrices satisfying a low-rank plus sparse structure from noisy and asynchronous high-frequency data collected for an ultra-large number of assets. The noise processes are allowed to be temporally correlated, heteroskedastic, asymptotically vanishing and dependent on the efficient prices. We define a kernel-weighted pre-averaging method to jointly tackle the microstructure noise and asynchronicity issues, and we obtain uniformly consistent estimates for latent prices. We impose a continuous-time factor model with time-varying factor loadings on the price processes, and estimate the common factors and loadings via a local principal component analysis. Assuming a uniform sparsity condition on the idiosyncratic volatility structure, we combine the POET and kernel-smoothing techniques to estimate the spot volatility matrices for both the latent prices and idiosyncratic errors. Under some mild restrictions, the estimated spot volatility matrices are shown to be uniformly consistent under various matrix norms. We provide Monte-Carlo simulation and empirical studies to examine the numerical performance of the developed estimation methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06246v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Degui Li, Oliver Linton, Haoxuan Zhang</dc:creator>
    </item>
    <item>
      <title>Seasonal and Periodic Patterns in US COVID-19 Mortality using the Variable Bandpass Periodic Block Bootstrap</title>
      <link>https://arxiv.org/abs/2403.06343</link>
      <description>arXiv:2403.06343v1 Announce Type: cross 
Abstract: Since the emergence of the SARS-CoV-2 virus, research into the existence, extent, and pattern of seasonality has been of the highest importance for public health preparation. This study uses a novel bandpass bootstrap approach called the Variable Bandpass Periodic Block Bootstrap (VBPBB) to investigate the periodically correlated (PC) components including seasonality within US COVID-19 mortality. Bootstrapping to produce confidence intervals (CI) for periodic characteristics such as the seasonal mean requires preservation of the PC component's correlation structure during resampling. While existing bootstrap methods can preserve the PC component correlation structure, filtration of that PC component's frequency from interference is critical to bootstrap the PC component's characteristics accurately and efficiently. The VBPBB filters the PC time series to reduce interference from other components such as noise. This greatly reduces bootstrapped CI size and outperforms the statistical power and accuracy of other methods when estimating the periodic mean sampling distribution. VBPBB analysis of US COVID-19 mortality PC components are provided and compared against alternative bootstrapping methods. These results reveal crucial evidence supporting the presence of a seasonal PC pattern and existence of additional PC components, their timing, and CIs for their effect which will aid prediction and preparation for future COVID-19 responses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06343v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edward Valachovic, Ekaterina Shishova</dc:creator>
    </item>
    <item>
      <title>Ensemble Kalman filter in geoscience meets model predictive control</title>
      <link>https://arxiv.org/abs/2403.06371</link>
      <description>arXiv:2403.06371v1 Announce Type: cross 
Abstract: Although data assimilation originates from control theory, the relationship between modern data assimilation methods in geoscience and model predictive control has not been extensively explored. In the present paper, I discuss that the modern data assimilation methods in geoscience and model predictive control essentially minimize the similar quadratic cost functions. Inspired by this similarity, I propose a new ensemble Kalman filter (EnKF)-based method for controlling spatio-temporally chaotic systems, which can readily be applied to high-dimensional and nonlinear Earth systems. In this method, the reference vector, which serves as the control target, is assimilated into the state space as a pseudo-observation by ensemble Kalman smoother to obtain the appropriate perturbation to be added to a system. A proof-of-concept experiment using the Lorenz 63 model is presented. The system is constrained in one wing of the butterfly attractor without tipping to the other side by reasonably small control perturbations which are comparable with previous works.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06371v1</guid>
      <category>physics.geo-ph</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yohei Sawada</dc:creator>
    </item>
    <item>
      <title>Estimating spillovers using imprecisely measured networks</title>
      <link>https://arxiv.org/abs/1904.00136</link>
      <description>arXiv:1904.00136v4 Announce Type: replace 
Abstract: In many experimental contexts, whether and how network interactions impact the outcome of interest for both treated and untreated individuals are key concerns. Networks data is often assumed to perfectly represent these possible interactions. This paper considers the problem of estimating treatment effects when measured connections are, instead, a noisy representation of the true spillover pathways. We show that existing methods, using the potential outcomes framework, yield biased estimators in the presence of this mismeasurement. We develop a new method, using a class of mixture models, that can account for missing connections and discuss its estimation via the Expectation-Maximization algorithm. We check our method's performance by simulating experiments on real network data from 43 villages in India. Finally, we use data from a previously published study to show that estimates using our method are more robust to the choice of network measure.</description>
      <guid isPermaLink="false">oai:arXiv.org:1904.00136v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Morgan Hardy, Rachel M. Heath, Wesley Lee, Tyler H. McCormick</dc:creator>
    </item>
    <item>
      <title>Longitudinal Network Models and Permutation-Uniform Markov Chains</title>
      <link>https://arxiv.org/abs/2108.05555</link>
      <description>arXiv:2108.05555v2 Announce Type: replace 
Abstract: Consider longitudinal networks whose edges turn on and off according to a discrete-time Markov chain with exponential-family transition probabilities. We characterize when their joint distributions are also exponential families with the same parameter, improving data reduction. Further we show that the permutation-uniform subclass of these chains permit interpretation as an independent, identically distributed sequence on the same state space. We then apply these ideas to temporal exponential random graph models, for which permutation uniformity is well suited, and discuss mean-parameter convergence, dyadic independence, and exchangeability. Our framework facilitates our introducing a new network model; simplifies analysis of some network and autoregressive models from the literature, including by permitting closed-form expressions for maximum likelihood estimates for some models; and facilitates applying standard tools to longitudinal-network Markov chains from either asymptotics or single-observation exponential random graph models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.05555v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1111/sjos.12630</arxiv:DOI>
      <arxiv:journal_reference>Scandinavian Journal of Statistics 50.3 (September 2023) 1201-1231</arxiv:journal_reference>
      <dc:creator>William K. Schwartz, Sonja Petrovi\'c, Hemanshu Kaul</dc:creator>
    </item>
    <item>
      <title>Nonlinear Regression with Residuals: Causal Estimation with Time-varying Treatments and Covariates</title>
      <link>https://arxiv.org/abs/2201.13451</link>
      <description>arXiv:2201.13451v3 Announce Type: replace 
Abstract: Standard regression adjustment gives inconsistent estimates of causal effects when there are time-varying treatment effects and time-varying covariates. Loosely speaking, the issue is that some covariates are post-treatment variables because they may be affected by prior treatment status, and regressing out post-treatment variables causes bias. More precisely, the bias is due to certain non-confounding latent variables that create colliders in the causal graph. These latent variables, which we call phantoms, do not harm the identifiability of the causal effect, but they render naive regression estimates inconsistent. Motivated by this, we ask: how can we modify regression methods so that they hold up even in the presence of phantoms? We develop an estimator for this setting based on regression modeling (linear, log-linear, probit and Cox regression), proving that it is consistent for a reasonable causal estimand. In particular, the estimator is a regression model fit with a simple adjustment for collinearity, making it easy to understand and implement with standard regression software. The proposed estimators are instances of the parametric g-formula, extending the regression-with-residuals approach to several canonical nonlinear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.13451v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephen Bates, Edward Kennedy, Robert Tibshirani, Valerie Ventura, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Near-Optimal Non-Parametric Sequential Tests and Confidence Sequences with Possibly Dependent Observations</title>
      <link>https://arxiv.org/abs/2212.14411</link>
      <description>arXiv:2212.14411v5 Announce Type: replace 
Abstract: Sequential tests and their implied confidence sequences, which are valid at arbitrary stopping times, promise flexible statistical inference and on-the-fly decision making. However, strong guarantees are limited to parametric sequential tests that under-cover in practice or concentration-bound-based sequences that over-cover and have suboptimal rejection times. In this work, we consider classic delayed-start normal-mixture sequential probability ratio tests, and we provide the first asymptotic type-I-error and expected-rejection-time guarantees under general non-parametric data generating processes, where the asymptotics are indexed by the test's burn-in time. The type-I-error results primarily leverage a martingale strong invariance principle and establish that these tests (and their implied confidence sequences) have type-I error rates asymptotically equivalent to the desired (possibly varying) $\alpha$-level. The expected-rejection-time results primarily leverage an identity inspired by It\^o's lemma and imply that, in certain asymptotic regimes, the expected rejection time is asymptotically equivalent to the minimum possible among $\alpha$-level tests. We show how to apply our results to sequential inference on parameters defined by estimating equations, such as average treatment effects. Together, our results establish these (ostensibly parametric) tests as general-purpose, non-parametric, and near-optimal. We illustrate this via numerical simulations and a real-data application to A/B testing at Netflix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.14411v5</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aurelien Bibaut, Nathan Kallus, Michael Lindon</dc:creator>
    </item>
    <item>
      <title>Estimation of Grouped Time-Varying Network Vector Autoregression Models</title>
      <link>https://arxiv.org/abs/2303.10117</link>
      <description>arXiv:2303.10117v2 Announce Type: replace 
Abstract: This paper introduces a flexible time-varying network vector autoregressive model framework for large-scale time series. A latent group structure is imposed on the heterogeneous and node-specific time-varying momentum and network spillover effects so that the number of unknown time-varying coefficients to be estimated can be reduced considerably. A classic agglomerative clustering algorithm with nonparametrically estimated distance matrix is combined with a ratio criterion to consistently estimate the latent group number and membership. A post-grouping local linear smoothing method is proposed to estimate the group-specific time-varying momentum and network effects, substantially improving the convergence rates of the preliminary estimates which ignore the latent structure. We further modify the methodology and theory to allow for structural breaks in either the group membership, group number or group-specific coefficient functions. Numerical studies including Monte-Carlo simulation and an empirical application are presented to examine the finite-sample performance of the developed model and methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.10117v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Degui Li, Bin Peng, Songqiao Tang, Weibiao Wu</dc:creator>
    </item>
    <item>
      <title>Off-policy evaluation beyond overlap: partial identification through smoothness</title>
      <link>https://arxiv.org/abs/2305.11812</link>
      <description>arXiv:2305.11812v2 Announce Type: replace 
Abstract: Off-policy evaluation (OPE) is the problem of estimating the value of a target policy using historical data collected under a different logging policy. OPE methods typically assume overlap between the target and logging policy, enabling solutions based on importance weighting and/or imputation. In this work, we approach OPE without assuming either overlap or a well-specified model by considering a strategy based on partial identification under non-parametric assumptions on the conditional mean function, focusing especially on Lipschitz smoothness. Under such smoothness assumptions, we formulate a pair of linear programs whose optimal values upper and lower bound the contributions of the no-overlap region to the off-policy value. We show that these linear programs have a concise closed form solution that can be computed efficiently and that their solutions converge, under the Lipschitz assumption, to the sharp partial identification bounds on the off-policy value. Furthermore, we show that the rate of convergence is minimax optimal, up to log factors. We deploy our methods on two semi-synthetic examples, and obtain informative and valid bounds that are tighter than those possible without smoothness assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.11812v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samir Khan, Martin Saveski, Johan Ugander</dc:creator>
    </item>
    <item>
      <title>Doubly Robust Proximal Causal Learning for Continuous Treatments</title>
      <link>https://arxiv.org/abs/2309.12819</link>
      <description>arXiv:2309.12819v3 Announce Type: replace 
Abstract: Proximal causal learning is a promising framework for identifying the causal effect under the existence of unmeasured confounders. Within this framework, the doubly robust (DR) estimator was derived and has shown its effectiveness in estimation, especially when the model assumption is violated. However, the current form of the DR estimator is restricted to binary treatments, while the treatment can be continuous in many real-world applications. The primary obstacle to continuous treatments resides in the delta function present in the original DR estimator, making it infeasible in causal effect estimation and introducing a heavy computational burden in nuisance function estimation. To address these challenges, we propose a kernel-based DR estimator that can well handle continuous treatments. Equipped with its smoothness, we show that its oracle form is a consistent approximation of the influence function. Further, we propose a new approach to efficiently solve the nuisance functions. We then provide a comprehensive convergence analysis in terms of the mean square error. We demonstrate the utility of our estimator on synthetic datasets and real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.12819v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yong Wu, Yanwei Fu, Shouyan Wang, Xinwei Sun</dc:creator>
    </item>
    <item>
      <title>Incorporating nonparametric methods for estimating causal excursion effects in mobile health with zero-inflated count outcomes</title>
      <link>https://arxiv.org/abs/2310.18905</link>
      <description>arXiv:2310.18905v2 Announce Type: replace 
Abstract: In mobile health, tailoring interventions for real-time delivery is of paramount importance. Micro-randomized trials have emerged as the "gold-standard" methodology for developing such interventions. Analyzing data from these trials provides insights into the efficacy of interventions and the potential moderation by specific covariates. The "causal excursion effect", a novel class of causal estimand, addresses these inquiries. Yet, existing research mainly focuses on continuous or binary data, leaving count data largely unexplored. The current work is motivated by the Drink Less micro-randomized trial from the UK, which focuses on a zero-inflated proximal outcome, i.e., the number of screen views in the subsequent hour following the intervention decision point. To be specific, we revisit the concept of causal excursion effect, specifically for zero-inflated count outcomes, and introduce novel estimation approaches that incorporate nonparametric techniques. Bidirectional asymptotics are established for the proposed estimators. Simulation studies are conducted to evaluate the performance of the proposed methods. As an illustration, we also implement these methods to the Drink Less trial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18905v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xueqing Liu, Tianchen Qian, Lauren Bell, Bibhas Chakraborty</dc:creator>
    </item>
    <item>
      <title>Interaction tests with covariate-adaptive randomization</title>
      <link>https://arxiv.org/abs/2311.17445</link>
      <description>arXiv:2311.17445v2 Announce Type: replace 
Abstract: Treatment-covariate interaction tests are commonly applied by researchers to examine whether the treatment effect varies across patient subgroups defined by baseline characteristics. The objective of this study is to explore treatment-covariate interaction tests involving covariate-adaptive randomization. Without assuming a parametric data generating model, we investigate usual interaction tests and observe that they tend to be conservative: specifically, their limiting rejection probabilities under the null hypothesis do not exceed the nominal level and are typically strictly lower than it. To address this problem, we propose modifications to the usual tests to obtain corresponding valid tests. Moreover, we introduce a novel class of stratified-adjusted interaction tests that are simple, more powerful than the usual and modified tests, and broadly applicable to most covariate-adaptive randomization methods. The results are general to encompass two types of interaction tests: one involving stratification covariates and the other involving additional covariates that are not used for randomization. Our study clarifies the application of interaction tests in clinical trials and offers valuable tools for revealing treatment heterogeneity, crucial for advancing personalized medicine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.17445v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Likun Zhang, Wei Ma</dc:creator>
    </item>
    <item>
      <title>Who Are We Missing? A Principled Approach to Characterizing the Underrepresented Population</title>
      <link>https://arxiv.org/abs/2401.14512</link>
      <description>arXiv:2401.14512v3 Announce Type: replace 
Abstract: Randomized controlled trials (RCTs) serve as the cornerstone for understanding causal effects, yet extending inferences to target populations presents challenges due to effect heterogeneity and underrepresentation. Our paper addresses the critical issue of identifying and characterizing underrepresented subgroups in RCTs, proposing a novel framework for refining target populations to improve generalizability. We introduce an optimization-based approach, Rashomon Set of Optimal Trees (ROOT), to characterize underrepresented groups. ROOT optimizes the target subpopulation distribution by minimizing the variance of the target average treatment effect estimate, ensuring more precise treatment effect estimations. Notably, ROOT generates interpretable characteristics of the underrepresented population, aiding researchers in effective communication. Our approach demonstrates improved precision and interpretability compared to alternatives, as illustrated with synthetic data experiments. We apply our methodology to extend inferences from the Starting Treatment with Agonist Replacement Therapies (START) trial -- investigating the effectiveness of medication for opioid use disorder -- to the real-world population represented by the Treatment Episode Dataset: Admissions (TEDS-A). By refining target populations using ROOT, our framework offers a systematic approach to enhance decision-making accuracy and inform future trials in diverse populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14512v3</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Harsh Parikh, Rachael Ross, Elizabeth Stuart, Kara Rudolph</dc:creator>
    </item>
    <item>
      <title>Fusing Individualized Treatment Rules Using Secondary Outcomes</title>
      <link>https://arxiv.org/abs/2402.08828</link>
      <description>arXiv:2402.08828v3 Announce Type: replace 
Abstract: An individualized treatment rule (ITR) is a decision rule that recommends treatments for patients based on their individual feature variables. In many practices, the ideal ITR for the primary outcome is also expected to cause minimal harm to other secondary outcomes. Therefore, our objective is to learn an ITR that not only maximizes the value function for the primary outcome, but also approximates the optimal rule for the secondary outcomes as closely as possible. To achieve this goal, we introduce a fusion penalty to encourage the ITRs based on different outcomes to yield similar recommendations. Two algorithms are proposed to estimate the ITR using surrogate loss functions. We prove that the agreement rate between the estimated ITR of the primary outcome and the optimal ITRs of the secondary outcomes converges to the true agreement rate faster than if the secondary outcomes are not taken into consideration. Furthermore, we derive the non-asymptotic properties of the value function and misclassification rate for the proposed method. Finally, simulation studies and a real data example are used to demonstrate the finite-sample performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08828v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daiqi Gao, Yuanjia Wang, Donglin Zeng</dc:creator>
    </item>
    <item>
      <title>Horseshoe Priors for Sparse Dirichlet-Multinomial Models</title>
      <link>https://arxiv.org/abs/2402.09583</link>
      <description>arXiv:2402.09583v2 Announce Type: replace 
Abstract: Bayesian inference for Dirichlet-Multinomial (DM) models has a long and important history. The concentration parameter $\alpha$ is pivotal in smoothing category probabilities within the multinomial distribution and is crucial for the inference afterward. Due to the lack of a tractable form of its marginal likelihood, $\alpha$ is often chosen in an ad-hoc manner, or estimated using approximation algorithms. A constant $\alpha$ often leads to inadequate smoothing of probabilities, particularly for sparse compositional count datasets. In this paper, we introduce a novel class of prior distributions facilitating conjugate updating of the concentration parameter, allowing for full Bayesian inference for DM models. Our methodology is based on fast residue computation and admits closed-form posterior moments in specific scenarios. Additionally, our prior provides continuous shrinkage with its heavy tail and substantial mass around zero, ensuring adaptability to the sparsity or quasi-sparsity of the data. We demonstrate the usefulness of our approach on both simulated examples and on real-world applications. Finally, we conclude with directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09583v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuexi Wang, Nicholas G. Polson</dc:creator>
    </item>
    <item>
      <title>Scalable couplings for the random walk Metropolis algorithm</title>
      <link>https://arxiv.org/abs/2211.12585</link>
      <description>arXiv:2211.12585v2 Announce Type: replace-cross 
Abstract: There has been a recent surge of interest in coupling methods for Markov chain Monte Carlo algorithms: they facilitate convergence quantification and unbiased estimation, while exploiting embarrassingly parallel computing capabilities. Motivated by these, we consider the design and analysis of couplings of the random walk Metropolis algorithm which scale well with the dimension of the target measure. Methodologically, we introduce a low-rank modification of the synchronous coupling that is provably optimally contractive in standard high-dimensional asymptotic regimes. We expose a shortcoming of the reflection coupling, the status quo at time of writing, and we propose a modification which mitigates the issue. Our analysis bridges the gap to the optimal scaling literature and builds a framework of asymptotic optimality which may be of independent interest. We illustrate the applicability of our proposed couplings, and the potential for extending our ideas, with various numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.12585v2</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tam\'as P. Papp, Chris Sherlock</dc:creator>
    </item>
    <item>
      <title>Bayesian Federated Inference for estimating Statistical Models based on Non-shared Multicenter Data sets</title>
      <link>https://arxiv.org/abs/2302.07677</link>
      <description>arXiv:2302.07677v2 Announce Type: replace-cross 
Abstract: Identifying predictive factors for an outcome of interest via a multivariable analysis is often difficult when the data set is small. Combining data from different medical centers into a single (larger) database would alleviate this problem, but is in practice challenging due to regulatory and logistic problems. Federated Learning (FL) is a machine learning approach that aims to construct from local inferences in separate data centers what would have been inferred had the data sets been merged. It seeks to harvest the statistical power of larger data sets without actually creating them. The FL strategy is not always efficient and precise. Therefore, in this paper we refine and implement an alternative Bayesian Federated Inference (BFI) framework for multicenter data with the same aim as FL. The BFI framework is designed to cope with small data sets by inferring locally not only the optimal parameter values, but also additional features of the posterior parameter distribution, capturing information beyond what is used in FL. BFI has the additional benefit that a single inference cycle across the centers is sufficient, whereas FL needs multiple cycles. We quantify the performance of the proposed methodology on simulated and real life data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.07677v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marianne A. Jonker, Hassan Pazira, Anthony CC Coolen</dc:creator>
    </item>
    <item>
      <title>Estimation and inference for minimizer and minimum of convex functions: optimality, adaptivity and uncertainty principles</title>
      <link>https://arxiv.org/abs/2305.00164</link>
      <description>arXiv:2305.00164v2 Announce Type: replace-cross 
Abstract: Optimal estimation and inference for both the minimizer and minimum of a convex regression function under the white noise and nonparametric regression models are studied in a nonasymptotic local minimax framework, where the performance of a procedure is evaluated at individual functions. Fully adaptive and computationally efficient algorithms are proposed and sharp minimax lower bounds are given for both the estimation accuracy and expected length of confidence intervals for the minimizer and minimum.
  The nonasymptotic local minimax framework brings out new phenomena in simultaneous estimation and inference for the minimizer and minimum. We establish a novel uncertainty principle that provides a fundamental limit on how well the minimizer and minimum can be estimated simultaneously for any convex regression function. A similar result holds for the expected length of the confidence intervals for the minimizer and minimum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.00164v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1214/24-AOS2355</arxiv:DOI>
      <arxiv:journal_reference>Ann. Statist. 52(1): 392-411 (February 2024)</arxiv:journal_reference>
      <dc:creator>T. Tony Cai, Ran Chen, Yuancheng Zhu</dc:creator>
    </item>
    <item>
      <title>Uniform error bound for PCA matrix denoising</title>
      <link>https://arxiv.org/abs/2306.12690</link>
      <description>arXiv:2306.12690v2 Announce Type: replace-cross 
Abstract: Principal component analysis (PCA) is a simple and popular tool for processing high-dimensional data. We investigate its effectiveness for matrix denoising.
  We consider the clean data are generated from a low-dimensional subspace, but masked by independent high-dimensional sub-Gaussian noises with standard deviation $\sigma$. Under the low-rank assumption on the clean data with a mild spectral gap assumption, we prove that the distance between each pair of PCA-denoised data point and the clean data point is uniformly bounded by $O(\sigma \log n)$. To illustrate the spectral gap assumption, we show it can be satisfied when the clean data are independently generated with a non-degenerate covariance matrix. We then provide a general lower bound for the error of the denoised data matrix, which indicates PCA denoising gives a uniform error bound that is rate-optimal. Furthermore, we examine how the error bound impacts downstream applications such as clustering and manifold learning. Numerical results validate our theoretical findings and reveal the importance of the uniform error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.12690v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin T. Tong, Wanjie Wang, Yuguan Wang</dc:creator>
    </item>
    <item>
      <title>Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization</title>
      <link>https://arxiv.org/abs/2310.02679</link>
      <description>arXiv:2310.02679v3 Announce Type: replace-cross 
Abstract: We tackle the problem of sampling from intractable high-dimensional density functions, a fundamental task that often appears in machine learning and statistics. We extend recent sampling-based approaches that leverage controlled stochastic processes to model approximate samples from these target densities. The main drawback of these approaches is that the training objective requires full trajectories to compute, resulting in sluggish credit assignment issues due to use of entire trajectories and a learning signal present only at the terminal time. In this work, we present Diffusion Generative Flow Samplers (DGFS), a sampling-based framework where the learning process can be tractably broken down into short partial trajectory segments, via parameterizing an additional "flow function". Our method takes inspiration from the theory developed for generative flow networks (GFlowNets), allowing us to make use of intermediate learning signals. Through various challenging experiments, we demonstrate that DGFS achieves more accurate estimates of the normalization constant than closely-related prior methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.02679v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dinghuai Zhang, Ricky T. Q. Chen, Cheng-Hao Liu, Aaron Courville, Yoshua Bengio</dc:creator>
    </item>
    <item>
      <title>Worst-Case Optimal Multi-Armed Gaussian Best Arm Identification with a Fixed Budget</title>
      <link>https://arxiv.org/abs/2310.19788</link>
      <description>arXiv:2310.19788v3 Announce Type: replace-cross 
Abstract: This study investigates the experimental design problem for identifying the arm with the highest expected outcome, referred to as best arm identification (BAI). In our experiments, the number of treatment-allocation rounds is fixed. During each round, a decision-maker allocates an arm and observes a corresponding outcome, which follows a Gaussian distribution with variances that can differ among the arms. At the end of the experiment, the decision-maker recommends one of the arms as an estimate of the best arm. To design an experiment, we first discuss lower bounds for the probability of misidentification. Our analysis highlights that the available information on the outcome distribution, such as means (expected outcomes), variances, and the choice of the best arm, significantly influences the lower bounds. Because available information is limited in actual experiments, we develop a lower bound that is valid under the unknown means and the unknown choice of the best arm, which are referred to as the worst-case lower bound. We demonstrate that the worst-case lower bound depends solely on the variances of the outcomes. Then, under the assumption that the variances are known, we propose the Generalized-Neyman-Allocation (GNA)-empirical-best-arm (EBA) strategy, an extension of the Neyman allocation proposed by Neyman (1934). We show that the GNA-EBA strategy is asymptotically optimal in the sense that its probability of misidentification aligns with the lower bounds as the sample size increases infinitely and the differences between the expected outcomes of the best and other suboptimal arms converge to the same values across arms. We refer to such strategies as asymptotically worst-case optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.19788v3</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>Active learning with biased non-response to label requests</title>
      <link>https://arxiv.org/abs/2312.08150</link>
      <description>arXiv:2312.08150v2 Announce Type: replace-cross 
Abstract: Active learning can improve the efficiency of training prediction models by identifying the most informative new labels to acquire. However, non-response to label requests can impact active learning's effectiveness in real-world contexts. We conceptualise this degradation by considering the type of non-response present in the data, demonstrating that biased non-response is particularly detrimental to model performance. We argue that biased non-response is likely in contexts where the labelling process, by nature, relies on user interactions. To mitigate the impact of biased non-response, we propose a cost-based correction to the sampling strategy--the Upper Confidence Bound of the Expected Utility (UCB-EU)--that can, plausibly, be applied to any active learning algorithm. Through experiments, we demonstrate that our method successfully reduces the harm from labelling non-response in many settings. However, we also characterise settings where the non-response bias in the annotations remains detrimental under UCB-EU for specific sampling methods and data generating processes. Finally, we evaluate our method on a real-world dataset from an e-commerce platform. We show that UCB-EU yields substantial performance improvements to conversion models that are trained on clicked impressions. Most generally, this research serves to both better conceptualise the interplay between types of non-response and model improvements via active learning, and to provide a practical, easy-to-implement correction that mitigates model degradation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.08150v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Robinson, Niek Tax, Richard Mudd, Ido Guy</dc:creator>
    </item>
    <item>
      <title>Assessment of Sports Concussion in Female Athletes: A Role for Neuroinformatics?</title>
      <link>https://arxiv.org/abs/2401.13045</link>
      <description>arXiv:2401.13045v2 Announce Type: replace-cross 
Abstract: Over the past decade, the intricacies of sports-related concussions among female athletes have become readily apparent. Traditional clinical methods for diagnosing concussions suffer limitations when applied to female athletes, often failing to capture subtle changes in brain structure and function. Advanced neuroinformatics techniques and machine learning models have become invaluable assets in this endeavor. While these technologies have been extensively employed in understanding concussion in male athletes, there remains a significant gap in our comprehension of their effectiveness for female athletes. With its remarkable data analysis capacity, machine learning offers a promising avenue to bridge this deficit. By harnessing the power of machine learning, researchers can link observed phenotypic neuroimaging data to sex-specific biological mechanisms, unraveling the mysteries of concussions in female athletes. Furthermore, embedding methods within machine learning enable examining brain architecture and its alterations beyond the conventional anatomical reference frame. In turn, allows researchers to gain deeper insights into the dynamics of concussions, treatment responses, and recovery processes. To guarantee that female athletes receive the optimal care they deserve, researchers must employ advanced neuroimaging techniques and sophisticated machine-learning models. These tools enable an in-depth investigation of the underlying mechanisms responsible for concussion symptoms stemming from neuronal dysfunction in female athletes. This paper endeavors to address the crucial issue of sex differences in multimodal neuroimaging experimental design and machine learning approaches within female athlete populations, ultimately ensuring that they receive the tailored care they require when facing the challenges of concussions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13045v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rachel Edelstein, Sterling Gutterman, Benjamin Newman, John Darrell Van Horn</dc:creator>
    </item>
    <item>
      <title>Semi-Supervised U-statistics</title>
      <link>https://arxiv.org/abs/2402.18921</link>
      <description>arXiv:2402.18921v2 Announce Type: replace-cross 
Abstract: Semi-supervised datasets are ubiquitous across diverse domains where obtaining fully labeled data is costly or time-consuming. The prevalence of such datasets has consistently driven the demand for new tools and methods that exploit the potential of unlabeled data. Responding to this demand, we introduce semi-supervised U-statistics enhanced by the abundance of unlabeled data, and investigate their statistical properties. We show that the proposed approach is asymptotically Normal and exhibits notable efficiency gains over classical U-statistics by effectively integrating various powerful prediction tools into the framework. To understand the fundamental difficulty of the problem, we derive minimax lower bounds in semi-supervised settings and showcase that our procedure is semi-parametrically efficient under regularity conditions. Moreover, tailored to bivariate kernels, we propose a refined approach that outperforms the classical U-statistic across all degeneracy regimes, and demonstrate its optimality properties. Simulation studies are conducted to corroborate our findings and to further demonstrate our framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18921v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ilmun Kim, Larry Wasserman, Sivaraman Balakrishnan, Matey Neykov</dc:creator>
    </item>
    <item>
      <title>Improving generalisation via anchor multivariate analysis</title>
      <link>https://arxiv.org/abs/2403.01865</link>
      <description>arXiv:2403.01865v2 Announce Type: replace-cross 
Abstract: We introduce a causal regularisation extension to anchor regression (AR) for improved out-of-distribution (OOD) generalisation. We present anchor-compatible losses, aligning with the anchor framework to ensure robustness against distribution shifts. Various multivariate analysis (MVA) algorithms, such as (Orthonormalized) PLS, RRR, and MLR, fall within the anchor framework. We observe that simple regularisation enhances robustness in OOD settings. Estimators for selected algorithms are provided, showcasing consistency and efficacy in synthetic and real-world climate science problems. The empirical validation highlights the versatility of anchor regularisation, emphasizing its compatibility with MVA approaches and its role in enhancing replicability while guarding against distribution shifts. The extended AR framework advances causal inference methodologies, addressing the need for reliable OOD generalisation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01865v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Homer Durand, Gherardo Varando, Nathan Mankovich, Gustau Camps-Valls</dc:creator>
    </item>
  </channel>
</rss>
