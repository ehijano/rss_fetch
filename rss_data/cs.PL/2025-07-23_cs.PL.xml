<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PL</link>
    <description>cs.PL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 24 Jul 2025 01:27:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>RightTyper: Effective and Efficient Type Annotation for Python</title>
      <link>https://arxiv.org/abs/2507.16051</link>
      <description>arXiv:2507.16051v1 Announce Type: new 
Abstract: Python type annotations bring the benefits of static type checking to the language. However, manually writing annotations can be time-consuming and tedious. The result is that most real-world Python code remains largely untyped. Past approaches to annotating types in Python code fall short in a number of ways. Static approaches struggle with dynamic features and infer overly broad types. AI-based methods are inherently unsound and can miss rare or user-defined types. Dynamic methods can impose extreme runtime overheads, degrading performance by up to 270x, abort execution as they exhaust resources, and even infer incorrect types that lead to runtime errors. Crucially, all prior work assumes implicitly that the code to be annotated is already correct. This assumption is generally unwarranted, especially for large codebases that have been untyped.
  This paper presents RightTyper, a novel approach for Python that overcomes these disadvantages. RightTyper not only generates precise type annotations based on actual program behavior, improving recall in type checking relative to prior approaches. It also turns type checking into anomaly detection, allowing the type checker to identify corner cases that the programmer can audit for unintended behavior. RightTyper is also fast and space-efficient, imposing just 30% performance overhead on average. RightTyper achieves these characteristics by a principled yet pervasive use of sampling--guided by self-profiling--along with statistical filtering and careful resolution and aggregation of type information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16051v1</guid>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juan Altmayer Pizzorno, Emery D. Berger</dc:creator>
    </item>
    <item>
      <title>Understanding Haskell-style Overloading via Open Data and Open Functions</title>
      <link>https://arxiv.org/abs/2507.16086</link>
      <description>arXiv:2507.16086v1 Announce Type: new 
Abstract: We present a new, uniform semantics for Haskell-style overloading. We realize our approach in a new core language, System F$_\mathrm{D}$, whose metatheory we mechanize in the Lean4 interactive theorem prover. System F$_\mathrm{D}$ is distinguished by its open data types and open functions, each given by a collection of instances rather than by a single definition. We show that System F$_\mathrm{D}$ can encode advanced features of Haskell's of type class systems, more expressively than current semantics of these features, and without assuming additional type equality axioms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16086v1</guid>
      <category>cs.PL</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Marmaduke, Apoorv Ingle, J. Garrett Morris</dc:creator>
    </item>
    <item>
      <title>Querying Graph-Relational Data</title>
      <link>https://arxiv.org/abs/2507.16089</link>
      <description>arXiv:2507.16089v2 Announce Type: new 
Abstract: For applications that store structured data in relational databases, there is an impedance mismatch between the flat representations encouraged by relational data models and the deeply nested information that applications expect to receive. In this work, we present the graph-relational database model, which provides a flexible, compositional, and strongly-typed solution to this "object-relational mismatch." We formally define the graph-relational database model and present a static and dynamic semantics for queries. In addition, we discuss the realization of the graph-relational database model in EdgeQL, a general-purpose SQL-style query language, and the Gel system, which compiles EdgeQL schemas and queries into PostgreSQL queries. Gel facilitates the kind of object-shaped data manipulation that is frequently provided inefficiently by object-relational mapping (ORM) technologies, while achieving most of the efficiency that comes from writing complex PostgreSQL queries directly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16089v2</guid>
      <category>cs.PL</category>
      <category>cs.DB</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael J. Sullivan, Zhibo Chen, Elvis Pranskevichus, Robert J. Simmons, Victor Petrovykh, Alja\v{z} Mur Er\v{z}en, Yury Selivanov</dc:creator>
    </item>
    <item>
      <title>Enhancing Compiler Optimization Efficiency through Grammatical Decompositions of Control-Flow Graphs</title>
      <link>https://arxiv.org/abs/2507.16660</link>
      <description>arXiv:2507.16660v1 Announce Type: new 
Abstract: This thesis addresses the complexities of compiler optimizations, such as register allocation and Lifetime-optimal Speculative Partial Redundancy Elimination (LOSPRE), which are often handled using tree decomposition algorithms. However, these methods frequently overlook important sparsity aspects of Control Flow Graphs (CFGs) and result in high computational costs. We introduce the SPL (Series-Parallel-Loop) decomposition, a novel framework that offers optimal solutions to these challenges. A key contribution is the formulation of a general solution for Partial Constraint Satisfaction Problems (PCSPs) within graph structures, applied to three optimization problems. First, SPL decomposition enhances register allocation by accurately modeling variable interference graphs, leading to efficient register assignments and improved performance across benchmarks. Second, it optimizes LOSPRE by effectively identifying and eliminating redundancies in program execution. Finally, the thesis focuses on optimizing the placement of bank selection instructions to enhance data retrieval efficiency and reduce latency. Extensive experimentation demonstrates significant performance improvements over existing methods, establishing SPL decomposition as a powerful tool for complex compiler optimizations, including register allocation, LOSPRE, and bank selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16660v1</guid>
      <category>cs.PL</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xuran Cai</dc:creator>
    </item>
    <item>
      <title>Abstracting Extensible Recursive Functions</title>
      <link>https://arxiv.org/abs/2410.11742</link>
      <description>arXiv:2410.11742v2 Announce Type: replace 
Abstract: We explore recursive programming with extensible data types. Row types make the structure of data types first class, and can express a variety of type system features including record subtyping and combination of case branches. Our goal is the modular combination of recursive types and of recursive functions over them. The most significant challenge is in recursive function calls, which may need to account for new cases in a combined type. We introduce extensible histomorphisms, Mendler-style descriptions of recursive functions in which recursive calls can happen at larger types, and show that they provide expressive recursion over extensible data types. We formalize our approach in R$\omega\mu$, a row type theory with support for recursive terms and types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11742v2</guid>
      <category>cs.PL</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Hubers, Apoorv Ingle, Andrew Marmaduke, J. Garrett Morris</dc:creator>
    </item>
    <item>
      <title>What does it take to certify a conversion checker?</title>
      <link>https://arxiv.org/abs/2502.15500</link>
      <description>arXiv:2502.15500v2 Announce Type: replace 
Abstract: We report on a detailed exploration of the properties of conversion (definitional equality) in dependent type theory, with the goal of certifying decision procedures for it. While in that context the property of normalisation has attracted the most light, we instead emphasize the importance of injectivity properties, showing that they alone are both crucial and sufficient to certify most desirable properties of conversion checkers. We also explore the certification of a fully untyped conversion checker, with respect to a typed specification, and show that the story is mostly unchanged, although the exact injectivity properties needed are subtly different.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15500v2</guid>
      <category>cs.PL</category>
      <category>cs.LO</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.FSCD.2025.27</arxiv:DOI>
      <dc:creator>Meven Lennon-Bertrand</dc:creator>
    </item>
    <item>
      <title>Bialgebraic Reasoning on Stateful Languages</title>
      <link>https://arxiv.org/abs/2503.10955</link>
      <description>arXiv:2503.10955v2 Announce Type: replace 
Abstract: Reasoning about program equivalence in imperative languages is notoriously challenging, as the presence of states (in the form of variable stores) fundamentally increases the observational power of program terms. The key desideratum for any notion of equivalence is compositionality, guaranteeing that subprograms can be safely replaced by equivalent subprograms regardless of the context. To facilitate compositionality proofs and avoid boilerplate work, one would hope to employ the abstract bialgebraic methods provided by Turi and Plotkin's powerful theory of mathematical operational semantics (a.k.a. abstract GSOS) or its recent extension by Goncharov et al. to higher-order languages. However, multiple attempts to apply abstract GSOS to stateful languages have thus failed. We propose a novel approach to the operational semantics of stateful languages based on the formal distinction between readers (terms that expect an initial input store before being executed), and writers (running terms that have already been provided with a store). In contrast to earlier work, this style of semantics is fully compatible with abstract GSOS, and we can thus leverage the existing theory to obtain coinductive reasoning techniques. We demonstrate that our approach generates non-trivial compositionality results for stateful languages with first-order and higher-order store and that it flexibly applies to program equivalences at different levels of granularity, such as trace, cost, and natural equivalence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10955v2</guid>
      <category>cs.PL</category>
      <category>cs.LO</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3747513</arxiv:DOI>
      <dc:creator>Sergey Goncharov, Stefan Milius, Lutz Schr\"oder, Stelios Tsampas, Henning Urbat</dc:creator>
    </item>
    <item>
      <title>Autocomp: LLM-Driven Code Optimization for Tensor Accelerators</title>
      <link>https://arxiv.org/abs/2505.18574</link>
      <description>arXiv:2505.18574v3 Announce Type: replace 
Abstract: Hardware accelerators, especially those designed for tensor processing, have become ubiquitous in today's computing landscape. However, even with significant efforts in building compilers, programming these tensor accelerators remains challenging, leaving much of their potential underutilized. Recently, large language models (LLMs), trained on large amounts of code, have shown significant promise in code generation and optimization tasks, but generating low-resource languages like specialized tensor accelerator code still poses a significant challenge. We tackle this challenge with Autocomp, an approach that empowers accelerator programmers to leverage domain knowledge and hardware feedback to optimize code via an automated LLM-driven search. We accomplish this by: 1) formulating each optimization pass as a structured two-phase prompt, divided into planning and code generation phases, 2) inserting domain knowledge during planning via a concise and adaptable optimization menu, and 3) integrating correctness and performance metrics from hardware as feedback at each search iteration. Across three categories of representative workloads and two different accelerators, we demonstrate that Autocomp-optimized code runs 5.6x (GEMM) and 2.7x (convolution) faster than the vendor-provided library, and outperforms expert-level hand-tuned code by 1.4x (GEMM), 1.1x (convolution), and 1.3x (fine-grained linear algebra). Additionally, we demonstrate that optimization schedules generated from Autocomp can be reused across similar tensor operations, improving speedups by up to 24% under a fixed sample budget.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18574v3</guid>
      <category>cs.PL</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles Hong, Sahil Bhatia, Alvin Cheung, Yakun Sophia Shao</dc:creator>
    </item>
    <item>
      <title>Hear Your Code Fail, Voice-Assisted Debugging for Python</title>
      <link>https://arxiv.org/abs/2507.15007</link>
      <description>arXiv:2507.15007v2 Announce Type: replace 
Abstract: This research introduces an innovative voice-assisted debugging plugin for Python that transforms silent runtime errors into actionable audible diagnostics. By implementing a global exception hook architecture with pyttsx3 text-to-speech conversion and Tkinter-based GUI visualization, the solution delivers multimodal error feedback through parallel auditory and visual channels. Empirical evaluation demonstrates 37% reduced cognitive load (p&lt;0.01, n=50) compared to traditional stack-trace debugging, while enabling 78% faster error identification through vocalized exception classification and contextualization. The system achieves sub-1.2 second voice latency with under 18% CPU overhead during exception handling, vocalizing error types and consequences while displaying interactive tracebacks with documentation deep links. Criteria validate compatibility across Python 3.7+ environments on Windows, macOS, and Linux platforms. Needing only two lines of integration code, the plugin significantly boosts availability for aesthetically impaired designers and supports multitasking workflows through hands-free error medical diagnosis. Educational applications show particular promise, with pilot studies indicating 45% faster debugging skill acquisition among novice programmers. Future development will incorporate GPT-based repair suggestions and real-time multilingual translation to further advance auditory debugging paradigms. The solution represents a fundamental shift toward human-centric error diagnostics, bridging critical gaps in programming accessibility while establishing new standards for cognitive efficiency in software development workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15007v2</guid>
      <category>cs.PL</category>
      <category>cs.CL</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sayed Mahbub Hasan Amiri, Md. Mainul Islam, Mohammad Shakhawat Hossen, Sayed Majhab Hasan Amiri, Mohammad Shawkat Ali Mamun, Sk. Humaun Kabir, Naznin Akter</dc:creator>
    </item>
    <item>
      <title>Hydra: Virtualized Multi-Language Runtime for High-Density Serverless Platforms</title>
      <link>https://arxiv.org/abs/2212.10131</link>
      <description>arXiv:2212.10131v3 Announce Type: replace-cross 
Abstract: Serverless is an attractive computing model that offers seamless scalability and elasticity; it takes the infrastructure management burden away from users and enables a pay-as-you-use billing model. As a result, serverless is becoming increasingly popular to support highly elastic and bursty workloads. However, existing platforms are supported by bloated virtualization stacks, which, combined with bursty and irregular invocations, lead to high memory and latency overheads.
  To reduce the virtualization stack bloat, we propose Hydra, a virtualized multi-language runtime and platform capable of hosting multiple sandboxes running concurrently. To fully leverage Hydra's virtualized runtime, we revisit the existing serverless platform design to make it colocation-aware across owners and functions, and to feature a caching layer of pre-allocated Hydra instances that can be used by different functions written in different languages to reduce cold starts. We also propose a snapshotting mechanism to checkpoint and restore individual sandboxes.
  By consolidating multiple serverless function invocations through Hydra, we improve the overall function density (ops/GB-sec) by 2.41x on average compared to OpenWhisk runtimes, the state-of-the-art single-language runtimes used in most serverless platforms, and by 1.43x on average compared to Knative runtimes supporting invocation colocation within the same function. When reproducing the Azure Functions trace, our serverless platform operating Hydra instances reduces the overall memory footprint by 21.3-43.9% compared to operating OpenWhisk instances and by 14.5-30% compared to operating Knative instances. Hydra eliminates cold starts thanks to the pool of pre-warmed runtime instances, reducing p99 latency by 45.3-375.5x compared to OpenWhisk and by 1.9-51.4x compared to Knative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.10131v3</guid>
      <category>cs.DC</category>
      <category>cs.PL</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Serhii Ivanenko, Vasyl Lanko, Rudi Horn, Vojin Jovanovic, Rodrigo Bruno</dc:creator>
    </item>
    <item>
      <title>GitChameleon 2.0: Evaluating AI Code Generation Against Python Library Version Incompatibilities</title>
      <link>https://arxiv.org/abs/2507.12367</link>
      <description>arXiv:2507.12367v2 Announce Type: replace-cross 
Abstract: The rapid evolution of software libraries poses a considerable hurdle for code generation, necessitating continuous adaptation to frequent version updates while preserving backward compatibility. While existing code evolution benchmarks provide valuable insights, they typically lack execution-based evaluation for generating code compliant with specific library versions. To address this, we introduce GitChameleon 2.0, a novel, meticulously curated dataset comprising 328 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests. GitChameleon 2.0 rigorously evaluates the capacity of contemporary large language models (LLMs), LLM-powered agents, code assistants, and RAG systems to perform version-conditioned code generation that demonstrates functional accuracy through execution. Our extensive evaluations indicate that state-of-the-art systems encounter significant challenges with this task; enterprise models achieving baseline success rates in the 48-51% range, underscoring the intricacy of the problem. By offering an execution-based benchmark emphasizing the dynamic nature of code libraries, GitChameleon 2.0 enables a clearer understanding of this challenge and helps guide the development of more adaptable and dependable AI code generation methods. We make the dataset and evaluation code publicly available at https://github.com/mrcabbage972/GitChameleonBenchmark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12367v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.PL</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diganta Misra, Nizar Islah, Victor May, Brice Rauby, Zihan Wang, Justine Gehring, Antonio Orvieto, Muawiz Chaudhary, Eilif B. Muller, Irina Rish, Samira Ebrahimi Kahou, Massimo Caccia</dc:creator>
    </item>
  </channel>
</rss>
