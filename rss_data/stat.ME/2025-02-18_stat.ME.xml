<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Feb 2025 05:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Bayesian Multivariate Spatial Point Pattern Model: Application to Oral Microbiome FISH Image Data</title>
      <link>https://arxiv.org/abs/2502.10513</link>
      <description>arXiv:2502.10513v1 Announce Type: new 
Abstract: Advances in cellular imaging technologies, especially those based on fluorescence in situ hybridization (FISH) now allow detailed visualization of the spatial organization of human or bacterial cells. Quantifying this spatial organization is crucial for understanding the function of multicellular tissues or biofilms, with implications for human health and disease. To address the need for better methods to achieve such quantification, we propose a flexible multivariate point process model that characterizes and estimates complex spatial interactions among multiple cell types. The proposed Bayesian framework is appealing due to its unified estimation process and the ability to directly quantify uncertainty in key estimates of interest, such as those of inter-type correlation and the proportion of variance due to inter-type relationships. To ensure stable and interpretable estimation, we consider shrinkage priors for coefficients associated with latent processes. Model selection and comparison are conducted by using a deviance information criterion designed for models with latent variables, effectively balancing the risk of overfitting with that of oversimplifying key quantities. Furthermore, we develop a hierarchical modeling approach to integrate multiple image-specific estimates from a given subject, allowing inference at both the global and subject-specific levels. We apply the proposed method to microbial biofilm image data from the human tongue dorsum and find that specific taxon pairs, such as Streptococcus mitis-Streptococcus salivarius and Streptococcus mitis-Veillonella, exhibit strong positive spatial correlations, while others, such as Actinomyces-Rothia, show slight negative correlations. For most of the taxa, a substantial portion of spatial variance can be attributed to inter-taxon relationships.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10513v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyu Ha Lee, Brent A. Coull, Suman Majumder, Patrick J. La Riviere, Jessica L. Mark Welch, Jacqueline R. Starr</dc:creator>
    </item>
    <item>
      <title>Cohering Disaggregation and Uncertainty Quantification for Spatially Misaligned Data</title>
      <link>https://arxiv.org/abs/2502.10584</link>
      <description>arXiv:2502.10584v1 Announce Type: new 
Abstract: Spatial misalignments arise from data aggregation or attempts to align misaligned data, leading to information loss. We propose a disaggregation framework that combines the finite element method (FEM) with a first-order Taylor approximation via integrated nested Laplace approximation (INLA).
  In landslide studies, landslide occurrences are often aggregated into counts based on slope units, reducing spatial detail. Our framework examines point pattern and aggregated count models under four covariate field scenarios: \textit{Raster at Full Resolution (RastFull), Raster Aggregation (RastAgg), Polygon Aggregation (PolyAgg), and Point Values (PointVal)}. The first three involve aggregation, while the latter two have incomplete fields. For these, we estimate the full covariate field using \textit{Value Plugin, Joint Uncertainty, and Uncertainty Plugin} methods, with the latter two accounting for uncertainty propagation and showing superior performance. Even under model misspecification (i.e.\ modelling a nonlinear field as linear), these methods remain more robust.
  Whenever possible, point pattern observations and full-resolution covariate fields should be prioritized. For incomplete fields, methods incorporating uncertainty propagation are preferred. This framework supports landslide susceptibility and other spatial mapping, integrating seamlessly with R-INLA \ extension packages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10584v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Man Ho Suen, Mark Naylor, Finn Lindgren</dc:creator>
    </item>
    <item>
      <title>C-learning in estimation of optimal individualized treatment regimes for recurrent disease</title>
      <link>https://arxiv.org/abs/2502.10658</link>
      <description>arXiv:2502.10658v1 Announce Type: new 
Abstract: Recurrent events, characterized by the repeated occurrence of the same event in an individual, are a common type of data in medical research. Motivated by cancer recurrences, we aim to estimate the optimal individualized treatment regime (ITR) that effectively mitigates such recurrent events. An ITR is a decision rule that assigns the optimal treatment to each patient, based on personalized information, with the aim of maximizing the overall therapeutic benefits. However, existing studies of estimating ITR mainly focus on first-time events rather than recurrent events. To address the issue of determining the optimal ITR for recurrent events, we propose the Recurrent C-learning (ReCL) method to identify the optimal ITR from two or multiple treatment options. The proposed method reformulates the optimization problem into a weighted classification problem. We introduce three estimators for the misclassification cost: the outcome regression estimator, the inverse probability weighting estimator, and the augmented inverse probability weighting estimator. The ReCL method leverages classification techniques to generate an interpretable optimal ITR tailored for recurrent event data. The advantages of the ReCL method are demonstrated through simulations under various scenarios. Furthermore, based on real data on colorectal cancer treatments, we employ this novel method to derive interpretable tree treatment regimes for colorectal cancer, thus providing a practical framework for enhancing treatment strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10658v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zi-Shu Zhan, Jin-Lun Zhang, Chen Shi, Xiao-Han Xu, Chun-Quan Ou</dc:creator>
    </item>
    <item>
      <title>Robust Estimation of Item Parameters via Divergence Measures in Item Response Theory</title>
      <link>https://arxiv.org/abs/2502.10741</link>
      <description>arXiv:2502.10741v1 Announce Type: new 
Abstract: Marginal maximum likelihood estimation (MMLE) in item response theory (IRT) is highly sensitive to aberrant responses, such as careless answering and random guessing, which can reduce estimation accuracy. To address this issue, this study introduces robust estimation methods for item parameters in IRT. Instead of empirically minimizing Kullback--Leibler divergence as in MMLE, the proposed approach minimizes the objective functions based on robust divergences, specifically density power divergence and {\gamma}-divergence. The resulting estimators are statistically consistent and asymptotically normal under appropriate regularity conditions. Furthermore, they offer a flexible trade-off between robustness and efficiency through hyperparameter tuning, forming a generalized estimation framework encompassing MMLE as a special case. To evaluate the effectiveness of the proposed methods, we conducted simulation experiments under various conditions, including scenarios with aberrant responses. The results demonstrated that the proposed methods surpassed existing ones in performance across various conditions. Moreover, numerical analysis of influence functions verified that increasing the hyperparameters effectively suppressed the impact of responses with low occurrence probabilities, which are potentially aberrant. These findings highlight that the proposed approach offers a robust alternative to MMLE, significantly enhancing measurement accuracy in testing and survey contexts prone to aberrant responses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10741v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuki Itaya, Kenichi Hayashi</dc:creator>
    </item>
    <item>
      <title>Dynamic spectral co-clustering of directed networks to unveil latent community paths in VAR-type models</title>
      <link>https://arxiv.org/abs/2502.10849</link>
      <description>arXiv:2502.10849v1 Announce Type: new 
Abstract: Identifying network Granger causality in large vector autoregressive (VAR) models enhances explanatory power by capturing complex interdependencies among variables. Instead of constructing network structures solely through sparse estimation of coefficients, we explore latent community structures to uncover the underlying network dynamics. We propose a dynamic network framework that embeds directed connectivity within the transition matrices of VAR-type models, enabling tracking of evolving community structures over time. To incorporate network directionality, we employ degree-corrected stochastic co-block models for each season or cycle, integrating spectral co-clustering with singular vector smoothing to refine latent community transitions. For greater model parsimony, we adopt periodic VAR (PVAR) and vector heterogeneous autoregressive (VHAR) models as alternatives to high-lag VAR models. We provide theoretical justifications for the proposed methodology and demonstrate its effectiveness through applications to the cyclic evolution of US nonfarm payroll employment and the temporal progression of realized stock market volatilities. Indeed, spectral co-clustering of directed networks reveals dynamic latent community trajectories, offering deeper insights into the evolving structure of high-dimensional time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10849v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Younghoon Kim, Changryong Baek</dc:creator>
    </item>
    <item>
      <title>Model-assisted inference for dynamic causal effects in staggered rollout cluster randomized experiments</title>
      <link>https://arxiv.org/abs/2502.10939</link>
      <description>arXiv:2502.10939v1 Announce Type: new 
Abstract: Staggered rollout cluster randomized experiments (SR-CREs) are increasingly used for their practical feasibility and logistical convenience. These designs involve staggered treatment adoption across clusters, requiring analysis methods that account for an exhaustive class of dynamic causal effects, anticipation, and non-ignorable cluster-period sizes. Without imposing outcome modeling assumptions, we study regression estimators using individual data, cluster-period averages, and scaled cluster-period totals, with and without covariate adjustment from a design-based perspective, where only the treatment adoption time is random. We establish consistency and asymptotic normality of each regression estimator under a finite-population framework and formally prove that the associated variance estimators are asymptotically conservative in the Lowner ordering. Furthermore, we conduct a unified efficiency comparison of the estimators and provide practical recommendations. We highlight the efficiency advantage of using estimators based on scaled cluster-period totals with covariate adjustment over their counterparts using individual-level data and cluster-period averages. Our results rigorously justify linear regression estimators as model-assisted methods to address an entire class of dynamic causal effects in SR-CREs and significantly expand those developed for parallel-arm CREs by Su and Ding (JRSSB, 2021) to accommodate a wider class of complex experimental settings with staggered randomization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10939v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyuan Chen, Fan Li</dc:creator>
    </item>
    <item>
      <title>Estimation of Treatment Effects based on Kernel Matching</title>
      <link>https://arxiv.org/abs/2502.10958</link>
      <description>arXiv:2502.10958v1 Announce Type: new 
Abstract: The treatment effect represents the average causal impact or outcome difference between treatment and control groups. Treatment effects can be
  estimated through social experiments, regression models, matching estimators, and instrumental variables. In this paper, we introduce a novel
  kernel-matching estimator for treatment effect estimation. This method is
  particularly beneficial in observational studies where randomized control
  trials are not feasible, as it uses the full sample to increase the efficiency
  and robustness of treatment effect estimates. We demonstrate that the
  proposed estimator is consistent and asymptotically efficient under certain
  conditions. Through Monte Carlo simulations, we show that the estimator
  performs favorably against other estimators in the literature. Finally, we
  apply our method to data from the National Supported Work Demonstration to illustrate its practical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10958v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chong Ding, Zheng Li, Hon Keung Tony Ng, Wei Gao</dc:creator>
    </item>
    <item>
      <title>Exact variance estimation for model-assisted survey estimators using U- and V-statistics</title>
      <link>https://arxiv.org/abs/2502.11032</link>
      <description>arXiv:2502.11032v1 Announce Type: new 
Abstract: Model-assisted estimation combines sample survey data with auxiliary information to increase precision when estimating finite population quantities. Accurately estimating the variance of model-assisted estimators is challenging: the classical approach ignores uncertainty from estimating the working model for the functional relationship between survey and auxiliary variables. This approach may be asymptotically valid, but can underestimate variance in practical settings with limited sample sizes. In this work, we develop a connection between model-assisted estimation and the theory of U- and V-statistics. We demonstrate that when predictions from the working model for the variable of interest can be represented as a U- or V-statistic, the resulting model-assisted estimator also admits a U- or V-statistic representation. We exploit this connection to derive an improved estimator of the exact variance of such model-assisted estimators. The class of working models for which this strategy can be used is broad, ranging from linear models to modern ensemble methods. We apply our approach to the model-assisted estimator constructed with a linear regression working model, commonly referred to as the generalized regression estimator, show that it can be re-written as a U-statistic, and propose an estimator of its exact variance. We illustrate our proposal and compare it against the classical asymptotic variance estimator using household survey data from the American Community Survey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11032v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ameer Dharamshi, Peter Gao, Jon Wakefield</dc:creator>
    </item>
    <item>
      <title>Box Confidence Depth: simulation-based inference with hyper-rectangles</title>
      <link>https://arxiv.org/abs/2502.11072</link>
      <description>arXiv:2502.11072v1 Announce Type: new 
Abstract: This work presents a novel simulation-based approach for constructing confidence regions in parametric models, which is particularly suited for generative models and situations where limited data and conventional asymptotic approximations fail to provide accurate results. The method leverages the concept of data depth and depends on creating ran- dom hyper-rectangles, i.e. boxes, in the sample space generated through simulations from the model, varying the input parameters. A probabilistic acceptance rule allows to retrieve a Depth-Confidence Distribution for the model parameters from which point estimators as well as calibrated confidence sets can be read-off. The method is designed to address cases where both the parameters and test statistics are multivariate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11072v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Bortolato, Laura Ventura</dc:creator>
    </item>
    <item>
      <title>Regression Modeling of the Count Relational Data with Exchangeable Dependencies</title>
      <link>https://arxiv.org/abs/2502.11255</link>
      <description>arXiv:2502.11255v1 Announce Type: new 
Abstract: Relational data characterized by directed edges with count measurements are common in social science. Most existing methods either assume the count edges are derived from continuous random variables or model the edge dependency by parametric distributions. In this paper, we develop a latent multiplicative Poisson model for relational data with count edges. Our approach directly models the edge dependency of count data by the pairwise dependence of latent errors, which are assumed to be weakly exchangeable. This assumption not only covers a variety of common network effects, but also leads to a concise representation of the error covariance. In addition, the identification and inference of the mean structure, as well as the regression coefficients, depend on the errors only through their covariance. Such a formulation provides substantial flexibility for our model. Based on this, we propose a pseudo-likelihood based estimator for the regression coefficients, demonstrating its consistency and asymptotic normality. The newly suggested method is applied to a food-sharing network, revealing interesting network effects in gift exchange behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11255v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenqin Du, Bailey K. Fosdick, Wen Zhou</dc:creator>
    </item>
    <item>
      <title>Transfer Learning of CATE with Kernel Ridge Regression</title>
      <link>https://arxiv.org/abs/2502.11331</link>
      <description>arXiv:2502.11331v1 Announce Type: new 
Abstract: The proliferation of data has sparked significant interest in leveraging findings from one study to estimate treatment effects in a different target population without direct outcome observations. However, the transfer learning process is frequently hindered by substantial covariate shift and limited overlap between (i) the source and target populations, as well as (ii) the treatment and control groups within the source. We propose a novel method for overlap-adaptive transfer learning of conditional average treatment effect (CATE) using kernel ridge regression (KRR). Our approach involves partitioning the labeled source data into two subsets. The first one is used to train candidate CATE models based on regression adjustment and pseudo-outcomes. An optimal model is then selected using the second subset and unlabeled target data, employing another pseudo-outcome-based strategy. We provide a theoretical justification for our method through sharp non-asymptotic MSE bounds, highlighting its adaptivity to both weak overlaps and the complexity of CATE function. Extensive numerical studies confirm that our method achieves superior finite-sample efficiency and adaptability. We conclude by demonstrating the effectiveness of our approach using a 401(k) eligibility dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11331v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seok-Jin Kim, Hongjie Liu, Molei Liu, Kaizheng Wang</dc:creator>
    </item>
    <item>
      <title>Stochastic Block Covariance Matrix Estimation</title>
      <link>https://arxiv.org/abs/2502.11332</link>
      <description>arXiv:2502.11332v1 Announce Type: new 
Abstract: Motivated by a neuroscience application we study the problem of statistical estimation of a high-dimensional covariance matrix with a block structure. The block model embeds a structural assumption: the population of items (neurons) can be divided into latent sub-populations with shared associative covariation within blocks and shared associative or dis-associative covariation across blocks. Unlike the block diagonal assumption, our block structure incorporates positive or negative pairwise correlation between blocks. In addition to offering reasonable modeling choices in neuroscience and economics, the block covariance matrix assumption is interesting purely from the perspective of statistical estimation theory: (a) it offers in-built dimension reduction and (b) it resembles a regularized factor model without the need of choosing the number of factors. We discuss a hierarchical Bayesian estimation method to simultaneously recover the latent blocks and estimate the overall covariance matrix. We show with numerical experiments that a hierarchical structure and a shrinkage prior are essential to accurate recovery when several blocks are present.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11332v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunran Chen, Surya T Tokdar, Jennifer M Groh</dc:creator>
    </item>
    <item>
      <title>A Bi-failure Mode Model for Competing Risk Modeling with HMC-Driven Bayesian Framework</title>
      <link>https://arxiv.org/abs/2502.11507</link>
      <description>arXiv:2502.11507v1 Announce Type: new 
Abstract: Bathtub failure rate (BFR) and roller-coaster failure rate (RCFR - a sequence of BFR and inverted BFR (IBFR)) shapes are among the non-monotone failure rate function (FRF) behaviors often observed in complex or competing risks (CR) datasets. Recent studies have introduced varied bathtub failure rate models for reliability modeling of such datasets. However, limited attention is paid to the reliability study of CR datasets characterized by RCFR. Motivated by this drawback, this paper proposes the so-called Bi-Failure Modes (BFM) model for robust reliability analysis of CR data exhibiting BFR, RCFR, and several other FRF shapes. The mean residual life function (MRLF) and cause-specific failure probabilities are studied in detail. The fundamental reciprocal relationships between the MRLF and FRF are established. We propose the Hamiltonian Monte Carlo (HMC)-based Bayesian framework for estimating the BFM parameters and its reliability attributes to offer greater computational efficiency and faster inference. Two CR datasets from electrode voltage endurance life and electrical appliance tests, respectively, characterized by BFR and RCFR behaviors, are employed to demonstrate the BFM adequacy. The recently introduced Bridge Criterion (BC) metric and other metrics are used to evaluate the BFM modeling performance against five recent methodologies under the maximum likelihood technique. The BFM compatibility with the two datasets is also examined. The findings portrayed the BFM's advantage over other competing candidates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11507v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Badamasi Abba, Mustapha Muhammad, Muhammad Salihu Isa, Wu Jingbiao</dc:creator>
    </item>
    <item>
      <title>Change-point problem: Direct estimation using a geometry inspired re-parametrization</title>
      <link>https://arxiv.org/abs/2502.11679</link>
      <description>arXiv:2502.11679v1 Announce Type: new 
Abstract: Estimation of mean shift in a temporally ordered sequence of random variables with a possible existence of change-point is an important problem in many disciplines. In the available literature of more than fifty years the estimation methods of the mean shift is usually dealt as a two-step problem. A test for the existence of a change-point is followed by an estimation process of the mean shift, which is known as testimator. The problem suffers from over parametrization. When viewed as an estimation problem, we establish that the maximum likelihood estimator (MLE) always gives a false alarm indicting an existence of a change-point in the given sequence even though there is no change-point at all. After modelling the parameter space as a modified horn torus. We introduce a new method of estimation of the parameters. The newly introduced estimation method of the mean shift is assessed with a proper Riemannian metric on that conic manifold. It is seen that its performance is superior compared to that of the MLE. The proposed method is implemented on Bitcoin data and compared its performance with the performance of the MLE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11679v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Buddhananda Banerjee, Arnab Kumar Laha</dc:creator>
    </item>
    <item>
      <title>Surrogate-based ABC matches generalized Bayesian inference under specific discrepancy and kernel choices</title>
      <link>https://arxiv.org/abs/2502.11738</link>
      <description>arXiv:2502.11738v1 Announce Type: new 
Abstract: Generalized Bayesian inference (GBI) is an alternative inference framework motivated by robustness to modeling errors, where a specific loss function is used to link the model parameters with observed data, instead of the log-likelihood used in standard Bayesian inference. Approximate Bayesian Computation (ABC) refers in turn to a family of methods approximating the posterior distribution via a discrepancy function between the observed and simulated data instead of using the likelihood. In this paper we discuss the connection between ABC and GBI, when the loss function is defined as an expected discrepancy between the observed and simulated data from the model under consideration. We show that the resulting generalized posterior corresponds to an ABC-posterior when the latter is obtained under a Gaussian process -based surrogate model. We illustrate the behavior of the approximations as a function of specific discrepancy and kernel choices to provide insights of the relationships between these different approximate inference paradigms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11738v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marko J\"arvenp\"a\"a, Jukka Corander, Henri Pesonen</dc:creator>
    </item>
    <item>
      <title>Bi-invariant Geodesic Regression with Data from the Osteoarthritis Initiative</title>
      <link>https://arxiv.org/abs/2502.11826</link>
      <description>arXiv:2502.11826v1 Announce Type: new 
Abstract: Many phenomena are naturally characterized by measuring continuous transformations such as shape changes in medicine or articulated systems in robotics. Modeling the variability in such datasets requires performing statistics on Lie groups, that is, manifolds carrying an additional group structure. As the Lie group captures the symmetries in the data, it is essential from a theoretical and practical perspective to ask for statistical methods that respect these symmetries; this way they are insensitive to confounding effects, e.g., due to the choice of reference coordinate systems. In this work, we investigate geodesic regression -- a generalization of linear regression originally derived for Riemannian manifolds. While Lie groups can be endowed with Riemannian metrics, these are generally incompatible with the group structure. We develop a non-metric estimator using an affine connection setting. It captures geodesic relationships respecting the symmetries given by left and right translations. For its computation, we propose an efficient fixed point algorithm requiring simple differential expressions that can be calculated through automatic differentiation. We perform experiments on a synthetic example and evaluate our method on an open-access, clinical dataset studying knee joint configurations under the progression of osteoarthritis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11826v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johannes Schade, Christoph von Tycowicz, Martin Hanik</dc:creator>
    </item>
    <item>
      <title>Phylogenetic latent space models for network data</title>
      <link>https://arxiv.org/abs/2502.11868</link>
      <description>arXiv:2502.11868v1 Announce Type: new 
Abstract: Latent space models for network data characterize each node through a vector of latent features whose pairwise similarities define the edge probabilities among pairs of nodes. Although this formulation has led to successful implementations and impactful extensions, the overarching focus has been on directly inferring node embeddings through the latent features rather than learning the generative process underlying the embedding. This focus prevents from borrowing information among the features of different nodes and fails to infer complex higher-level architectures regulating the formation of the network itself. For example, routinely-studied networks often exhibit multiscale structures informing on nested modular hierarchies among nodes that could be learned via tree-based representations of dependencies among latent features. We pursue this direction by developing an innovative phylogenetic latent space model that explicitly characterizes the generative process of the nodes' feature vectors via a branching Brownian motion, with branching structure parametrized by a phylogenetic tree. This tree constitutes the main object of interest and is learned under a Bayesian perspective to infer tree-based modular hierarchies among nodes that explain heterogenous multiscale patterns in the network. Identifiability results are derived along with posterior consistency theory, and the inference potentials of the newly-proposed model are illustrated in simulations and two real-data applications from criminology and neuroscience, where our formulation learns core structures hidden to state-of-the-art alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11868v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federico Pavone, Daniele Durante, Robin J. Ryder</dc:creator>
    </item>
    <item>
      <title>Unified Multivariate Ordinal Model for analysis of sensory attributes</title>
      <link>https://arxiv.org/abs/2502.11990</link>
      <description>arXiv:2502.11990v1 Announce Type: new 
Abstract: Experiments involving sensory analysis of foods and beverages are beneficial for selecting healthy products and assessing the preferences of potential consumers. They are generally planned in incomplete blocks, and their attributes, such as aroma, colour, and flavour, are evaluated using a 9-point hedonic scale, characterising an ordinal variable response. Also, the generalised logit model with random effects for panellists is one of the appropriate models to relate the multivariate response to the covariates. This study aims to present a method for analysing sensory attributes through a unified multivariate model. Due to the nature of the variable, each separate model already corresponds to a multivariate analysis, so our proposal would incorporate a complete analysis with solely one model. This proposal is based on multivariate methods for categorical data and maximum likelihood theory. Our method was evaluated through a simulation study, in which we consider three distinct formulations with two attributes to represent various formulation selection scenarios via mixed discrete models. The simulated results demonstrated overall concordance rates exceeding 80\% for the unified model compared to the separate models. Moreover, as motivation is presented, a study of 13 prebiotic beverages based on cashew nut almonds added to grape juice, with 130 potential consumers. The attributes evaluated were overall impression, aroma, Body, sweetness and flavour, using a 9-point hedonic scale. The selected unified model considering all attributes was the non-proportional odds mixed-effect model. According to this model, the prebiotic beverage formulations most likely to be accepted were: 8\% sugar and 40\% grape juice ($F_4$), 6\% sugar and 44\% grape juice ($F_6$), and 9\% sugar and 30\% grape juice ($F_{13}$). The unified analysis and computational time showed the advantages of this proposal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11990v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jana\'ina Marques e Melo, Jo\~ao C\'esar Reis Alves, Gabriel Rodrigues Palma, S\'ilvia Maria de Freitas, Idemauro Antonio Rodrigues de Lara</dc:creator>
    </item>
    <item>
      <title>Forecasting time series with constraints</title>
      <link>https://arxiv.org/abs/2502.10485</link>
      <description>arXiv:2502.10485v1 Announce Type: cross 
Abstract: Time series forecasting presents unique challenges that limit the effectiveness of traditional machine learning algorithms. To address these limitations, various approaches have incorporated linear constraints into learning algorithms, such as generalized additive models and hierarchical forecasting. In this paper, we propose a unified framework for integrating and combining linear constraints in time series forecasting. Within this framework, we show that the exact minimizer of the constrained empirical risk can be computed efficiently using linear algebra alone. This approach allows for highly scalable implementations optimized for GPUs. We validate the proposed methodology through extensive benchmarking on real-world tasks, including electricity demand forecasting and tourism forecasting, achieving state-of-the-art performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10485v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathan Doum\`eche (LPSM, EDF R&amp;D OSIRIS), Francis Bach (DI-ENS, SIERRA), \'Eloi Bedek (EDF R&amp;D OSIRIS), G\'erard Biau (LPSM, IUF), Claire Boyer (LMO, IUF), Yannig Goude (EDF R&amp;D OSIRIS, LMO)</dc:creator>
    </item>
    <item>
      <title>Variational empirical Bayes variable selection in high-dimensional logistic regression</title>
      <link>https://arxiv.org/abs/2502.10532</link>
      <description>arXiv:2502.10532v1 Announce Type: cross 
Abstract: Logistic regression involving high-dimensional covariates is a practically important problem. Often the goal is variable selection, i.e., determining which few of the many covariates are associated with the binary response. Unfortunately, the usual Bayesian computations can be quite challenging and expensive. Here we start with a recently proposed empirical Bayes solution, with strong theoretical convergence properties, and develop a novel and computationally efficient variational approximation thereof. One such novelty is that we develop this approximation directly for the marginal distribution on the model space, rather than on the regression coefficients themselves. We demonstrate the method's strong performance in simulations, and prove that our variational approximation inherits the strong selection consistency property satisfied by the posterior distribution that it is approximating.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10532v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiqi Tang, Ryan Martin</dc:creator>
    </item>
    <item>
      <title>E-TRIALS: Empowering Data-Driven Decisions to Enhance Computer-Based Learning Platforms</title>
      <link>https://arxiv.org/abs/2502.10545</link>
      <description>arXiv:2502.10545v1 Announce Type: cross 
Abstract: Computer-based learning platforms (CBLPs) have become a common medium in schools, transferring how students learn and interact with education content. A recent report by the Institute of Education Sciences revealed that 94\% of public schools now provide a computer for every student. The high percentage of students having their own computers highlights the need for researchers to investigate how students learn in CBLPs and identify instructional designs that can enhance their performance. In this paper, we introduce E-TRIALS, a tool by ASSISTments designed to conduct experiments and analyze student performance. Using data from three experiments, we propose a generalizable approach for analyzing experimental data and modeling results. We evaluate three Average Treatment Effect (ATE) estimators: Student's t-test $\hat{\tau}_{t-test}$, Ordinary Least Squares regression $\hat{\tau}_{OLS}$, and Leave-One-Out Potential outcomes (LOOP) $\hat{\tau}_{LOOP}$. Our approach can be used for future E-TRIALS experiments. Code used for this work is available at https://osf.io/xp6ch/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10545v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abubakir Siedahmed, Yanping Pei, Adam C Sales, Neil T Heffernan, Johann Gagnon-Bartsch</dc:creator>
    </item>
    <item>
      <title>Generative Adversarial Networks for High-Dimensional Item Factor Analysis: A Deep Adversarial Learning Algorithm</title>
      <link>https://arxiv.org/abs/2502.10650</link>
      <description>arXiv:2502.10650v1 Announce Type: cross 
Abstract: Advances in deep learning and representation learning have transformed item factor analysis (IFA) in the item response theory (IRT) literature by enabling more efficient and accurate parameter estimation. Variational Autoencoders (VAEs) have been one of the most impactful techniques in modeling high-dimensional latent variables in this context. However, the limited expressiveness of the inference model based on traditional VAEs can still hinder the estimation performance. This study introduces Adversarial Variational Bayes (AVB) algorithms as an improvement to VAEs for IFA with improved flexibility and accuracy. By bridging the strengths of VAEs and Generative Adversarial Networks (GANs), AVB incorporates an auxiliary discriminator network to reframe the estimation process as a two-player adversarial game and removes the restrictive assumption of standard normal distributions in the inference model. Theoretically, AVB can achieve similar or higher likelihood compared to VAEs. A further enhanced algorithm, Importance-weighted Adversarial Variational Bayes (IWAVB) is proposed and compared with Importance-weighted Autoencoders (IWAE). In an exploratory analysis of real empirical data, IWAVB demonstrated superior expressiveness by achieving a higher likelihood compared to IWAE. In confirmatory studies with simulated data, IWAVB achieved similar mean-square error results to IWAE while consistently achieving higher likelihoods. Moreover, in simulations where latent variables followed a multimodal distribution, IWAVB outperformed IWAE by providing more accurate parameter estimates. With its innovative use of GANs, IWAVB is shown to have the potential to extend IFA to handle large-scale data, facilitating the potential integration of psychometrics and multimodal data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10650v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nanyu Luo, Feng Ji</dc:creator>
    </item>
    <item>
      <title>Policy Learning with Confidence</title>
      <link>https://arxiv.org/abs/2502.10653</link>
      <description>arXiv:2502.10653v1 Announce Type: cross 
Abstract: This paper proposes a framework for selecting policies that maximize expected benefit in the presence of estimation uncertainty, by controlling for estimation risk and incorporating risk aversion. The proposed method explicitly balances the size of the estimated benefit against the uncertainty inherent in its estimation, ensuring that chosen policies meet a reporting guarantee, namely that the actual benefit of the implemented policy is guaranteed not to fall below the reported estimate with a pre-specified confidence level. This approach applies to a variety of settings, including the selection of policy rules that allocate individuals to treatments based on observed characteristics, using both experimental and non-experimental data; and the allocation of limited budgets among competing social programs; as well as many others. Across these applications, the framework offers a principled and robust method for making data-driven policy choices under uncertainty. In broader terms, it focuses on policies that are on the efficient decision frontier, describing policies that offer maximum estimated benefit for a given acceptable level of estimation risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10653v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victor Chernozhukov, Sokbae Lee, Adam M. Rosen, Liyang Sun</dc:creator>
    </item>
    <item>
      <title>Learning Identifiable Structures Helps Avoid Bias in DNN-based Supervised Causal Learning</title>
      <link>https://arxiv.org/abs/2502.10883</link>
      <description>arXiv:2502.10883v1 Announce Type: cross 
Abstract: Causal discovery is a structured prediction task that aims to predict causal relations among variables based on their data samples. Supervised Causal Learning (SCL) is an emerging paradigm in this field. Existing Deep Neural Network (DNN)-based methods commonly adopt the "Node-Edge approach", in which the model first computes an embedding vector for each variable-node, then uses these variable-wise representations to concurrently and independently predict for each directed causal-edge. In this paper, we first show that this architecture has some systematic bias that cannot be mitigated regardless of model size and data size. We then propose SiCL, a DNN-based SCL method that predicts a skeleton matrix together with a v-tensor (a third-order tensor representing the v-structures). According to the Markov Equivalence Class (MEC) theory, both the skeleton and the v-structures are identifiable causal structures under the canonical MEC setting, so predictions about skeleton and v-structures do not suffer from the identifiability limit in causal discovery, thus SiCL can avoid the systematic bias in Node-Edge architecture, and enable consistent estimators for causal discovery. Moreover, SiCL is also equipped with a specially designed pairwise encoder module with a unidirectional attention layer to model both internal and external relationships of pairs of nodes. Experimental results on both synthetic and real-world benchmarks show that SiCL significantly outperforms other DNN-based SCL approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10883v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaru Zhang, Rui Ding, Qiang Fu, Bojun Huang, Zizhen Deng, Yang Hua, Haibing Guan, Shi Han, Dongmei Zhang</dc:creator>
    </item>
    <item>
      <title>Is Elo Rating Reliable? A Study Under Model Misspecification</title>
      <link>https://arxiv.org/abs/2502.10985</link>
      <description>arXiv:2502.10985v1 Announce Type: cross 
Abstract: Elo rating, widely used for skill assessment across diverse domains ranging from competitive games to large language models, is often understood as an incremental update algorithm for estimating a stationary Bradley-Terry (BT) model. However, our empirical analysis of practical matching datasets reveals two surprising findings: (1) Most games deviate significantly from the assumptions of the BT model and stationarity, raising questions on the reliability of Elo. (2) Despite these deviations, Elo frequently outperforms more complex rating systems, such as mElo and pairwise models, which are specifically designed to account for non-BT components in the data, particularly in terms of win rate prediction. This paper explains this unexpected phenomenon through three key perspectives: (a) We reinterpret Elo as an instance of online gradient descent, which provides no-regret guarantees even in misspecified and non-stationary settings. (b) Through extensive synthetic experiments on data generated from transitive but non-BT models, such as strongly or weakly stochastic transitive models, we show that the ''sparsity'' of practical matching data is a critical factor behind Elo's superior performance in prediction compared to more complex rating systems. (c) We observe a strong correlation between Elo's predictive accuracy and its ranking performance, further supporting its effectiveness in ranking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10985v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shange Tang, Yuanhao Wang, Chi Jin</dc:creator>
    </item>
    <item>
      <title>Advances in Bayesian Modeling: Applications and Methods</title>
      <link>https://arxiv.org/abs/2502.11321</link>
      <description>arXiv:2502.11321v1 Announce Type: cross 
Abstract: This paper explores the versatility and depth of Bayesian modeling by presenting a comprehensive range of applications and methods, combining Markov chain Monte Carlo (MCMC) techniques and variational approximations. Covering topics such as hierarchical modeling, spatial modeling, higher-order Markov chains, and Bayesian nonparametrics, the study emphasizes practical implementations across diverse fields, including oceanography, climatology, epidemiology, astronomy, and financial analysis. The aim is to bridge theoretical underpinnings with real-world applications, illustrating the formulation of Bayesian models, elicitation of priors, computational strategies, and posterior and predictive analyses. By leveraging different computational methods, this paper provides insights into model fitting, goodness-of-fit evaluation, and predictive accuracy, addressing computational efficiency and methodological challenges across various datasets and domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11321v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifei Yan, Juan Sosa, Carlos A. Mart\'inez</dc:creator>
    </item>
    <item>
      <title>Exact Upper and Lower Bounds for the Output Distribution of Neural Networks with Random Inputs</title>
      <link>https://arxiv.org/abs/2502.11672</link>
      <description>arXiv:2502.11672v1 Announce Type: cross 
Abstract: We derive exact upper and lower bounds for the cumulative distribution function (cdf) of the output of a neural network over its entire support subject to noisy (stochastic) inputs. The upper and lower bounds converge to the true cdf over its domain as the resolution increases. Our method applies to any feedforward NN using continuous monotonic piecewise differentiable activation functions (e.g., ReLU, tanh and softmax) and convolutional NNs, which were beyond the scope of competing approaches. The novelty and an instrumental tool of our approach is to bound general NNs with ReLU NNs. The ReLU NN based bounds are then used to derive upper and lower bounds of the cdf of the NN output. Experiments demonstrate that our method delivers guaranteed bounds of the predictive output distribution over its support, thus providing exact error guarantees, in contrast to competing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11672v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrey Kofnov, Daniel Kapla, Ezio Bartocci, Efstathia Bura</dc:creator>
    </item>
    <item>
      <title>Energy load forecasting using Terna public data: a free lunch multi-task combination approach</title>
      <link>https://arxiv.org/abs/2502.11873</link>
      <description>arXiv:2502.11873v1 Announce Type: cross 
Abstract: We propose a quick-and-simple procedure to augment the accuracy of 15-minutes Italian load forecasts disaggregated by bidding zones published by Terna, the operator of the Italian electricity system. We show that a stacked-regression multi-task combination approach using Terna and daily random walk naive forecasts, is able to produce significantly more accurate forecasts immediately after Terna publishes on its data portal the energy load measurements for the previous day, and the forecasts for the current day.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11873v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Girolimetto, Tommaso Di Fonzo</dc:creator>
    </item>
    <item>
      <title>Forecasting Italian daily electricity generation disaggregated by geographical zones and energy sources using coherent forecast combination</title>
      <link>https://arxiv.org/abs/2502.11878</link>
      <description>arXiv:2502.11878v1 Announce Type: cross 
Abstract: A novel approach is applied for improving forecast accuracy and achieving coherence in forecasting the Italian daily energy generation time series. In hierarchical frameworks such as national energy generation disaggregated by geographical zones and energy sources, independently generated base forecasts often result in inconsistencies across the constraints. We deal with this issue through a coherent balanced multi-task forecast combination approach, which combines unbiased forecasts from multiple experts while ensuring coherence. Applied to the daily Italian electricity generation data, our method shows superior accuracy compared to single-task base and combined forecasts, and a state-of-the-art single-expert reconciliation technique, demonstrating to be an effective approach to forecasting linearly constrained multiple time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11878v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Girolimetto, Tommaso Di Fonzo</dc:creator>
    </item>
    <item>
      <title>Low-Rank Thinning</title>
      <link>https://arxiv.org/abs/2502.12063</link>
      <description>arXiv:2502.12063v1 Announce Type: cross 
Abstract: The goal in thinning is to summarize a dataset using a small set of representative points. Remarkably, sub-Gaussian thinning algorithms like Kernel Halving and Compress can match the quality of uniform subsampling while substantially reducing the number of summary points. However, existing guarantees cover only a restricted range of distributions and kernel-based quality measures and suffer from pessimistic dimension dependence. To address these deficiencies, we introduce a new low-rank analysis of sub-Gaussian thinning that applies to any distribution and any kernel, guaranteeing high-quality compression whenever the kernel or data matrix is approximately low-rank. To demonstrate the broad applicability of the techniques, we design practical sub-Gaussian thinning approaches that improve upon the best known guarantees for approximating attention in transformers, accelerating stochastic gradient training through reordering, and distinguishing distributions in near-linear time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12063v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annabelle Michael Carrell, Albert Gong, Abhishek Shetty, Raaz Dwivedi, Lester Mackey</dc:creator>
    </item>
    <item>
      <title>Potato Potahto in the FAO-GAEZ Productivity Measures? Nonclassical Measurement Error with Multiple Proxies</title>
      <link>https://arxiv.org/abs/2502.12141</link>
      <description>arXiv:2502.12141v1 Announce Type: cross 
Abstract: The FAO-GAEZ crop productivity data are widely used in Economics. However, the existence of measurement error is rarely recognized in the empirical literature. We propose a novel method to partially identify the effect of agricultural productivity, deriving bounds that allow for nonclassical measurement error by leveraging two proxies. These bounds exhaust all the information contained in the first two moments of the data. We reevaluate three influential studies, documenting that measurement error matters and that the impact of agricultural productivity on economic outcomes may be smaller than previously reported. Our methodology has broad applications in empirical research involving mismeasured variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12141v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rafael Araujo, Vitor Possebom</dc:creator>
    </item>
    <item>
      <title>Factor copula models for non-Gaussian longitudinal data</title>
      <link>https://arxiv.org/abs/2402.00668</link>
      <description>arXiv:2402.00668v3 Announce Type: replace 
Abstract: This article presents factor copula approaches to model temporal dependency of non-Gaussian (continuous/discrete) longitudinal data. Factor copula models are canonical vine copulas which explain the underlying dependence structure of a multivariate data through latent variables, and therefore can be easily interpreted and implemented to unbalanced longitudinal data. We develop regression models for continuous, binary and ordinal longitudinal data including covariates, by using factor copula constructions with subject-specific latent variables. Considering homogeneous within-subject dependence, our proposed models allow for feasible parametric inference in moderate to high dimensional situations, using two-stage (IFM) estimation method. We assess the finite sample performance of the proposed models with extensive simulation studies. In the empirical analysis, the proposed models are applied for analysing different longitudinal responses of two real world data sets. Moreover, we compare the performances of these models with some widely used random effect models using standard model selection techniques and find substantial improvements. Our studies suggest that factor copula models can be good alternatives to random effect models and can provide better insights to temporal dependency of longitudinal data of arbitrary nature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00668v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhajit Chattopadhyay</dc:creator>
    </item>
    <item>
      <title>Combining Evidence Across Filtrations</title>
      <link>https://arxiv.org/abs/2402.09698</link>
      <description>arXiv:2402.09698v3 Announce Type: replace 
Abstract: In sequential anytime-valid inference, any admissible procedure must be based on e-processes: generalizations of test martingales that quantify the accumulated evidence against a composite null hypothesis at any stopping time. This paper proposes a method for combining e-processes constructed in different filtrations but for the same null. Although e-processes in the same filtration can be combined effortlessly (by averaging), e-processes in different filtrations cannot because their validity in a coarser filtration does not translate to a finer filtration. This issue arises in sequential tests of randomness and independence, as well as in the evaluation of sequential forecasters. We establish that a class of functions called adjusters can lift arbitrary e-processes across filtrations. The result yields a generally applicable "adjust-then-combine" procedure, which we demonstrate on the problem of testing randomness in real-world financial data. Furthermore, we prove a characterization theorem for adjusters that formalizes a sense in which using adjusters is necessary. There are two major implications. First, if we have a powerful e-process in a coarsened filtration, then we readily have a powerful e-process in the original filtration. Second, when we coarsen the filtration to construct an e-process, there is a logarithmic cost to recovering validity in the original filtration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09698v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yo Joong Choe, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Modeling temporal dependency of longitudinal data: use of multivariate geometric skew-normal copula</title>
      <link>https://arxiv.org/abs/2404.03420</link>
      <description>arXiv:2404.03420v2 Announce Type: replace 
Abstract: Use of copula for the purpose of modeling dependence has been receiving considerable attention in recent times. On the other hand, search for multivariate copulas with desirable dependence properties also is an important area of research. When fitting regression models to non-Gaussian longitudinal data, multivariate Gaussian copula is commonly used to account for temporal dependence of the repeated measurements. But using symmetric multivariate Gaussian copula is not preferable in every situation, since it can not capture non-exchangeable dependence or tail dependence, if present in the data. Hence to ensure reliable inference, it is important to look beyond the Gaussian dependence assumption. In this paper, we construct geometric skew-normal copula from multivariate geometric skew-normal (MGSN) distribution proposed by Kundu (2014) and Kundu (2017) in order to model temporal dependency of non-Gaussian longitudinal data. First we investigate the theoretical properties of the proposed multivariate copula, and then develop regression models for both continuous and discrete longitudinal data. The quantile function of this copula is independent of the correlation matrix of its respective multivariate distribution, which provides computational advantage in terms of likelihood inference compared to the class of copulas derived from skew-elliptical distributions by Azzalini &amp; Valle (1996). Moreover, composite likelihood inference is possible for this multivariate copula, which facilitates to estimate parameters from ordered probit model with same dependence structure as geometric skew-normal distribution. We conduct extensive simulation studies to validate our proposed models and therefore apply them to analyze the longitudinal dependence of two real world data sets. Finally, we report our findings in terms of improvements over multivariate Gaussian copula based regression models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03420v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhajit Chattopadhyay</dc:creator>
    </item>
    <item>
      <title>Compressive Bayesian non-negative matrix factorization for mutational signatures analysis</title>
      <link>https://arxiv.org/abs/2404.10974</link>
      <description>arXiv:2404.10974v2 Announce Type: replace 
Abstract: Non-negative matrix factorization (NMF) is widely used in many applications for dimensionality reduction. Inferring an appropriate number of factors for NMF is a challenging problem, and several approaches based on information criteria or sparsity-inducing priors have been proposed. However, inference in these models is often complicated and computationally challenging. In this paper, we introduce a novel methodology for overfitted Bayesian NMF models using ``compressive hyperpriors'' that force unneeded factors down to negligible values while only imposing mild shrinkage on needed factors. The method is based on using simple semi-conjugate priors to facilitate inference, while setting the strength of the hyperprior in a data-dependent way to achieve this compressive property. We apply our method to mutational signatures analysis in cancer genomics, where we find that it outperforms state-of-the-art alternatives. In particular, we illustrate how our compressive hyperprior enables the use of biologically informed priors on the signatures, yielding significantly improved accuracy. We provide theoretical results characterizing the posterior and its concentration, and we demonstrate the method in simulations and on real data from cancer applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10974v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Zito, Jeffrey W. Miller</dc:creator>
    </item>
    <item>
      <title>MM Algorithms for Statistical Estimation in Quantile Regression</title>
      <link>https://arxiv.org/abs/2407.12348</link>
      <description>arXiv:2407.12348v3 Announce Type: replace 
Abstract: Quantile regression \parencite{Koenker1978} is a robust and practically useful way to efficiently model quantile varying correlation and predict varied response quantiles of interest. This article constructs and tests MM algorithms, which are simple to code and have been suggested superior to some other prominent quantile regression methods in nonregularized problems \parencite{Pietrosanu2017}, in an array of linear quantile regression settings. Simulation studies comparing MM to existing tested methods and applications to various real data sets have corroborated our algorithms' effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12348v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifan Cheng, Anthony Yung Cheung Kuk</dc:creator>
    </item>
    <item>
      <title>Admissible online closed testing must employ e-values</title>
      <link>https://arxiv.org/abs/2407.15733</link>
      <description>arXiv:2407.15733v3 Announce Type: replace 
Abstract: In contemporary research, data scientists often test an infinite sequence of hypotheses $H_1,H_2,\ldots $ one by one, and are required to make real-time decisions without knowing the future hypotheses or data. In this paper, we consider such an online multiple testing problem with the goal of providing simultaneous lower bounds for the number of true discoveries in data-adaptively chosen rejection sets. In offline multiple testing, it has been recently established that such simultaneous inference is admissible iff it proceeds through (offline) closed testing. We establish an analogous result in this paper using the recent online closure principle. In particular, we show that it is necessary to use an anytime-valid test for each intersection hypothesis. This connects two distinct branches of the literature: online testing of multiple hypotheses (where the hypotheses appear online), and sequential anytime-valid testing of a single hypothesis (where the data for a fixed hypothesis appears online). Motivated by this result, we construct a new online closed testing procedure and a corresponding short-cut with a true discovery guarantee based on multiplying sequential e-values. This general but simple procedure gives uniform improvements over the state-of-the-art methods but also allows to construct entirely new and powerful procedures. In addition, we introduce new ideas for hedging and boosting of sequential e-values that provably increase power. Finally, we also propose the first online true discovery procedures for exchangeable and arbitrarily dependent e-values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15733v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lasse Fischer, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Functional Singular Value Decomposition</title>
      <link>https://arxiv.org/abs/2410.03619</link>
      <description>arXiv:2410.03619v4 Announce Type: replace 
Abstract: Heterogeneous functional data commonly arise in time series and longitudinal studies. To uncover the statistical structures of such data, we propose Functional Singular Value Decomposition (FSVD), a unified framework encompassing various tasks for the analysis of functional data with potential heterogeneity. We establish the mathematical foundation of FSVD by proving its existence and providing its fundamental properties. We then develop an implementation approach for noisy and irregularly observed functional data based on a novel alternating minimization scheme and provide theoretical guarantees for its convergence and estimation accuracy. The FSVD framework also introduces the concepts of intrinsic basis functions and intrinsic basis vectors, representing two fundamental structural aspects of random functions. These concepts enable FSVD to provide new and improved solutions to tasks including functional principal component analysis, factor models, functional clustering, functional linear regression, and functional completion, while effectively handling heterogeneity and irregular temporal sampling. Through extensive simulations, we demonstrate that FSVD-based methods consistently outperform existing methods across these tasks. To showcase the value of FSVD in real-world datasets, we apply it to extract temporal patterns from a COVID-19 case count dataset and perform data completion on an electronic health record dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03619v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jianbin Tan, Pixu Shi, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Dynamic Supervised Principal Component Analysis for Classification</title>
      <link>https://arxiv.org/abs/2411.01820</link>
      <description>arXiv:2411.01820v2 Announce Type: replace 
Abstract: This paper introduces a novel framework for dynamic classification in high dimensional spaces, addressing the evolving nature of class distributions over time or other index variables. Traditional discriminant analysis techniques are adapted to learn dynamic decision rules with respect to the index variable. In particular, we propose and study a new supervised dimension reduction method employing kernel smoothing to identify the optimal subspace, and provide a comprehensive examination of this approach for both linear discriminant analysis and quadratic discriminant analysis. We illustrate the effectiveness of the proposed methods through numerical simulations and real data examples. The results show considerable improvements in classification accuracy and computational efficiency. This work contributes to the field by offering a robust and adaptive solution to the challenges of scalability and non-staticity in high-dimensional data classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01820v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/10618600.2025.2452935</arxiv:DOI>
      <dc:creator>Wenbo Ouyang, Ruiyang Wu, Ning Hao, Hao Helen Zhang</dc:creator>
    </item>
    <item>
      <title>Univariate-Guided Sparse Regression</title>
      <link>https://arxiv.org/abs/2501.18360</link>
      <description>arXiv:2501.18360v4 Announce Type: replace 
Abstract: In this paper, we introduce ``UniLasso'' -- a novel statistical method for regression. This two-stage approach preserves the signs of the univariate coefficients and leverages their magnitude. Both of these properties are attractive for stability and interpretation of the model. Through comprehensive simulations and applications to real-world datasets, we demonstrate that UniLasso outperforms Lasso in various settings, particularly in terms of sparsity and model interpretability. We prove asymptotic support recovery and mean-squared error consistency under a set of conditions different from the well-known irrepresentability conditions for the Lasso. Extensions to generalized linear models (GLMs) and Cox regression are also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18360v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourav Chatterjee, Trevor Hastie, Robert Tibshirani</dc:creator>
    </item>
    <item>
      <title>Generalized Simple Graphical Rules for Assessing Selection Bias</title>
      <link>https://arxiv.org/abs/2502.00924</link>
      <description>arXiv:2502.00924v3 Announce Type: replace 
Abstract: Selection bias is a major obstacle toward valid causal inference in epidemiology. Over the past decade, several simple graphical rules based on causal diagrams have been proposed as the sufficient identification conditions for addressing selection bias and recovering causal effects. However, these simple graphical rules are usually coupled with specific identification strategies and estimators. In this article, we show two important cases of selection bias that cannot be addressed by these simple rules and their estimators: one case where selection is a descendant of a collider of the treatment and the outcome, and the other case where selection is affected by the mediator. To address selection bias in these two cases, we construct identification formulas by the g-computation and the inverse probability weighting (IPW) methods based on single-world intervention graphs (SWIGs). They are generalized to recover the average treatment effect by adjusting for post-treatment upstream causes of selection. We propose two IPW estimators and their variance estimators to recover the average treatment effect in the presence of selection bias in these two cases. We conduct simulation studies to verify the performance of the estimators when the traditional crude selected-sample analysis returns erroneous contradictory conclusions to the truth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00924v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yichi Zhang, Haidong Lu</dc:creator>
    </item>
    <item>
      <title>Causal Inference under Interference: Regression Adjustment and Optimality</title>
      <link>https://arxiv.org/abs/2502.06008</link>
      <description>arXiv:2502.06008v2 Announce Type: replace 
Abstract: In randomized controlled trials without interference, regression adjustment is widely used to enhance the efficiency of treatment effect estimation. This paper extends this efficiency principle to settings with network interference, where a unit's response may depend on the treatments assigned to its neighbors in a network. We make three key contributions: (1) we establish a central limit theorem for a linear regression-adjusted estimator and prove its optimality in achieving the smallest asymptotic variance within a class of linear adjustments; (2) we develop a novel, consistent estimator for the asymptotic variance of this linear estimator; and (3) we propose a nonparametric estimator that integrates kernel smoothing and trimming techniques, demonstrating its asymptotic normality and its optimality in minimizing asymptotic variance within a broader class of nonlinear adjustments. Extensive simulations validate the superior performance of our estimators, and a real-world data application illustrates their practical utility. Our findings underscore the power of regression-based methods and reveal the potential of kernel-and-trimming-based approaches for further enhancing efficiency under network interference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06008v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyuan Fan, Chenlei Leng, Weichi Wu</dc:creator>
    </item>
    <item>
      <title>Network regression and supervised centrality estimation</title>
      <link>https://arxiv.org/abs/2111.12921</link>
      <description>arXiv:2111.12921v2 Announce Type: replace-cross 
Abstract: The centrality in a network is often used to measure nodes' importance and model network effects on a certain outcome. Empirical studies widely adopt a two-stage procedure, which first estimates the centrality from the observed noisy network and then infers the network effect from the estimated centrality, even though it lacks theoretical understanding. We propose a unified modeling framework to study the properties of centrality estimation and inference and the subsequent network regression analysis with noisy network observations. Furthermore, we propose a supervised centrality estimation methodology, which aims to simultaneously estimate both centrality and network effect. We showcase the advantages of our method compared with the two-stage method both theoretically and numerically via extensive simulations and a case study in predicting currency risk premiums from the global trade network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.12921v2</guid>
      <category>econ.EM</category>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junhui Cai, Ran Chen, Dan Yang, Wu Zhu, Haipeng Shen, Linda Zhao</dc:creator>
    </item>
    <item>
      <title>Conditional Aalen--Johansen estimation</title>
      <link>https://arxiv.org/abs/2303.02119</link>
      <description>arXiv:2303.02119v3 Announce Type: replace-cross 
Abstract: The conditional Aalen--Johansen estimator, a general-purpose non-parametric estimator of conditional state occupation probabilities, is introduced. The estimator is applicable for any finite-state jump process and supports conditioning on external as well as internal covariate information. The conditioning feature permits for a much more detailed analysis of the distributional characteristics of the process. The estimator reduces to the conditional Kaplan--Meier estimator in the special case of a survival model and also englobes other, more recent, landmark estimators when covariates are discrete. Strong uniform consistency and asymptotic normality are established under lax moment conditions on the multivariate counting process, allowing in particular for an unbounded number of transitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.02119v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Bladt, Christian Furrer</dc:creator>
    </item>
    <item>
      <title>On the Efficiency of Finely Stratified Experiments</title>
      <link>https://arxiv.org/abs/2307.15181</link>
      <description>arXiv:2307.15181v5 Announce Type: replace-cross 
Abstract: This paper studies the use of finely stratified designs for the efficient estimation of a large class of treatment effect parameters that arise in the analysis of experiments. By a "finely stratified" design, we mean experiments in which units are divided into groups of a fixed size and a proportion within each group is assigned to a binary treatment uniformly at random. The class of parameters considered are those that can be expressed as the solution to a set of moment conditions constructed using a known function of the observed data. They include, among other things, average treatment effects, quantile treatment effects, and local average treatment effects as well as the counterparts to these quantities in experiments in which the unit is itself a cluster. In this setting, we establish three results. First, we show that under a finely stratified design, the na\"ive method of moments estimator achieves the same asymptotic variance as what could typically be attained under alternative treatment assignment mechanisms only through ex post covariate adjustment. Second, we argue that the na\"ive method of moments estimator under a finely stratified design is asymptotically efficient by deriving a lower bound on the asymptotic variance of regular estimators of the parameter of interest in the form of a convolution theorem. Finally, we establish conditions under which a "fast-balancing" property of finely stratified designs is in fact necessary for the na\"ive method of moments estimator to attain the efficiency bound. In this sense, finely stratified experiments are attractive because they lead to efficient estimators of treatment effect parameters "by design."</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.15181v5</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuehao Bai, Jizhou Liu, Azeem M. Shaikh, Max Tabord-Meehan</dc:creator>
    </item>
    <item>
      <title>Censored extreme value estimation</title>
      <link>https://arxiv.org/abs/2312.10499</link>
      <description>arXiv:2312.10499v5 Announce Type: replace-cross 
Abstract: A novel and comprehensive methodology designed to tackle the challenges posed by extreme values in the context of random censorship is introduced. The main focus is on the analysis of integrals based on the product-limit estimator of normalized upper order statistics, called extreme Kaplan--Meier integrals. These integrals allow for the transparent derivation of various important asymptotic distributional properties, offering an alternative approach to conventional plug-in estimation methods. Notably, this methodology demonstrates robustness and wide applicability among various tail regimes. A noteworthy by-product is the extension of generalized Hill-type estimators of extremes to encompass arbitrary tail behavior, which is of independent interest. The theoretical framework is applied to construct novel estimators for real-valued extreme value indices for right-censored data. Simulation studies confirm the asymptotic results and, in a competitor case, mostly show superiority in mean square error. An application to brain cancer data demonstrates that censoring effects are properly accounted for, even when focusing solely on tail classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10499v5</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Bladt, Igor Rodionov</dc:creator>
    </item>
    <item>
      <title>Causal reasoning in difference graphs</title>
      <link>https://arxiv.org/abs/2411.01292</link>
      <description>arXiv:2411.01292v2 Announce Type: replace-cross 
Abstract: Understanding causal mechanisms across different populations is essential for designing effective public health interventions. Recently, difference graphs have been introduced as a tool to visually represent causal variations between two distinct populations. While there has been progress in inferring these graphs from data through causal discovery methods, there remains a gap in systematically leveraging their potential to enhance causal reasoning. This paper addresses that gap by establishing conditions for identifying causal changes and effects using difference graphs. It specifically focuses on identifying total causal changes and total effects in a nonparametric setting, as well as direct causal changes and direct effects in a linear setting. In doing so, it provides a novel approach to causal reasoning that holds potential for various public health applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01292v2</guid>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles K. Assaad</dc:creator>
    </item>
    <item>
      <title>LITE: Efficiently Estimating Gaussian Probability of Maximality</title>
      <link>https://arxiv.org/abs/2501.13535</link>
      <description>arXiv:2501.13535v2 Announce Type: replace-cross 
Abstract: We consider the problem of computing the probability of maximality (PoM) of a Gaussian random vector, i.e., the probability for each dimension to be maximal. This is a key challenge in applications ranging from Bayesian optimization to reinforcement learning, where the PoM not only helps with finding an optimal action, but yields a fine-grained analysis of the action domain, crucial in tasks such as drug discovery. Existing techniques are costly, scaling polynomially in computation and memory with the vector size. We introduce LITE, the first approach for estimating Gaussian PoM with almost-linear time and memory complexity. LITE achieves SOTA accuracy on a number of tasks, while being in practice several orders of magnitude faster than the baselines. This also translates to a better performance on downstream tasks such as entropy estimation and optimal control of bandits. Theoretically, we cast LITE as entropy-regularized UCB and connect it to prior PoM estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13535v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Menet (ETH Z\"urich), Jonas H\"ubotter (ETH Z\"urich), Parnian Kassraie (ETH Z\"urich), Andreas Krause (ETH Z\"urich)</dc:creator>
    </item>
  </channel>
</rss>
