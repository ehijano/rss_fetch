<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Nov 2024 04:07:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The CUSUM Test with Observation-Adjusted Control Limits in Parameters Change Detection for the Extremely Heavy-Tailed Distributions Sequences</title>
      <link>https://arxiv.org/abs/2411.14706</link>
      <description>arXiv:2411.14706v1 Announce Type: new 
Abstract: In this paper, we propose an new the CUSUM sequential test (control chart, stopping time) with the observation-adjusted control limits (CUSUM-OAL) for monitoring quickly and adaptively the change in distribution of a sequential observations. We give the estimation of the in-control and the out-of-control average run lengths (ARLs) of the CUSUM-OAL test. The theoretical results are illustrated by numerical simulations in detecting $\alpha$ shifts of the extreme heavy-tailed distribution observations sequence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14706v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>F. Tang, D. Han</dc:creator>
    </item>
    <item>
      <title>Bayesian dynamic mode decomposition for real-time ship motion digital twinning</title>
      <link>https://arxiv.org/abs/2411.14839</link>
      <description>arXiv:2411.14839v1 Announce Type: new 
Abstract: Digital twins are widely considered enablers of groundbreaking changes in the development, operation, and maintenance of novel generations of products. They are meant to provide reliable and timely predictions to inform decisions along the entire product life cycle. One of their most interesting applications in the naval field is the digital twinning of ship performances in waves, a crucial aspect in design and operation safety. In this paper, a Bayesian extension of the Hankel dynamic mode decomposition method is proposed for ship motion's nowcasting as a prediction tool for naval digital twins. The proposed algorithm meets all the requirements for formulations devoted to digital twinning, being able to adapt the resulting models with the data incoming from the physical system, using a limited amount of data, producing real-time predictions, and estimating their reliability. Results are presented and discussed for the course-keeping of the 5415M model in beam-quartering sea state 7 irregular waves at Fr = 0.33, using data from three different CFD solvers. The results show predictions keeping good accuracy levels up to five wave encounter periods, with the Bayesian formulation improving the deterministic forecasts. In addition, a connection between the predicted uncertainty and prediction accuracy is found.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14839v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>math.DS</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giorgio Palma, Andrea Serani, Kevin McTaggart, Shawn Aram, David W. Wundrow, David Drazen, Matteo Diez</dc:creator>
    </item>
    <item>
      <title>A fast approach for analyzing spatio-temporal patterns in ischemic heart disease mortality across US counties (1999-2021)</title>
      <link>https://arxiv.org/abs/2411.14849</link>
      <description>arXiv:2411.14849v1 Announce Type: new 
Abstract: Ischaemic heart disease (IHD) remains the primary cause of mortality in the US. This study focuses on using spatio-temporal disease mapping models to explore the temporal trends of IHD at the county level from 1999 to 2021. To manage the computational burden arising from the high-dimensional data, we employ scalable Bayesian models using a "divide and conquer" strategy. This approach allows for fast model fitting and serves as an efficient procedure for screening spatio-temporal patterns.
  Additionally, we analyze trends in four regional subdivisions, West, Midwest, South and Northeast, and in urban and rural areas. The dataset on IHD contains missing data, and we propose a procedure to impute the omitted information. The results show a slowdown in the decrease of IHD mortality in the US after 2014 with a slight increase noted after 2019. However, differences exists among the counties, the four big geographical regions, and rural and urban areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14849v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>A. Urdangarin, T. Goicoa, P. Congdon, MD. Ugarte</dc:creator>
    </item>
    <item>
      <title>The Effects of Major League Baseball's Ban on Infield Shifts: A Quasi-Experimental Analysis</title>
      <link>https://arxiv.org/abs/2411.15075</link>
      <description>arXiv:2411.15075v1 Announce Type: new 
Abstract: From 2020 to 2023, Major League Baseball changed rules affecting team composition, player positioning, and game time. Understanding the effects of these rules is crucial for leagues, teams, players, and other relevant parties to assess their impact and to advocate either for further changes or undoing previous ones. Panel data and quasi-experimental methods provide useful tools for causal inference in these settings. I demonstrate this potential by analyzing the effect of the 2023 shift ban at both the league-wide and player-specific levels. Using difference-in-differences analysis, I show that the policy increased batting average on balls in play and on-base percentage for left-handed batters by a modest amount (nine points). For individual players, synthetic control analyses identify several players whose offensive performance (on-base percentage, on-base plus slugging percentage, and weighted on-base average) improved substantially (over 70 points in several cases) because of the rule change, and other players with previously high shift rates for whom it had little effect. This article both estimates the impact of this specific rule change and demonstrates how these methods for causal inference are potentially valuable for sports analytics -- at the player, team, and league levels -- more broadly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15075v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lee Kennedy-Shaffer</dc:creator>
    </item>
    <item>
      <title>Exploring the Potential Role of Generative AI in the TRAPD Procedure for Survey Translation</title>
      <link>https://arxiv.org/abs/2411.14472</link>
      <description>arXiv:2411.14472v1 Announce Type: cross 
Abstract: This paper explores and assesses in what ways generative AI can assist in translating survey instruments. Writing effective survey questions is a challenging and complex task, made even more difficult for surveys that will be translated and deployed in multiple linguistic and cultural settings. Translation errors can be detrimental, with known errors rendering data unusable for its intended purpose and undetected errors leading to incorrect conclusions. A growing number of institutions face this problem as surveys deployed by private and academic organizations globalize, and the success of their current efforts depends heavily on researchers' and translators' expertise and the amount of time each party has to contribute to the task. Thus, multilinguistic and multicultural surveys produced by teams with limited expertise, budgets, or time are at significant risk for translation-based errors in their data. We implement a zero-shot prompt experiment using ChatGPT to explore generative AI's ability to identify features of questions that might be difficult to translate to a linguistic audience other than the source language. We find that ChatGPT can provide meaningful feedback on translation issues, including common source survey language, inconsistent conceptualization, sensitivity and formality issues, and nonexistent concepts. In addition, we provide detailed information on the practicality of the approach, including accessing the necessary software, associated costs, and computational run times. Lastly, based on our findings, we propose avenues for future research that integrate AI into survey translation practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14472v1</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erica Ann Metheney, Lauren Yehle</dc:creator>
    </item>
    <item>
      <title>Combining missing data imputation and internal validation in clinical risk prediction models</title>
      <link>https://arxiv.org/abs/2411.14542</link>
      <description>arXiv:2411.14542v1 Announce Type: cross 
Abstract: Methods to handle missing data have been extensively explored in the context of estimation and descriptive studies, with multiple imputation being the most widely used method in clinical research. However, in the context of clinical risk prediction models, where the goal is often to achieve high prediction accuracy and to make predictions for future patients, there are different considerations regarding the handling of missing data. As a result, deterministic imputation is better suited to the setting of clinical risk prediction models, since the outcome is not included in the imputation model and the imputation method can be easily applied to future patients. In this paper, we provide a tutorial demonstrating how to conduct bootstrapping followed by deterministic imputation of missing data to construct and internally validate the performance of a clinical risk prediction model in the presence of missing data. Extensive simulation study results are provided to help guide decision-making in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14542v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junhui Mi, Rahul D. Tendulkar, Sarah M. C. Sittenfeld, Sujata Patil, Emily C. Zabor</dc:creator>
    </item>
    <item>
      <title>An Investigation of the Relationship Between Crime Rate and Police Compensation</title>
      <link>https://arxiv.org/abs/2411.14632</link>
      <description>arXiv:2411.14632v1 Announce Type: cross 
Abstract: The goal of this paper is to assess whether there is any correlation between police salaries and crime rates. Using public data sources that contain Baltimore Crime Rates and Baltimore Police Department (BPD) salary information from 2011 to 2021, our research uses a variety of techniques to capture and measure any correlation between the two. Based on that correlation, the paper then uses established social theories to make recommendations on how this data can potentially be used by State Leadership. Our initial results show a negative correlation between salary/compensation levels and crime rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14632v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jhancy Amarsingh, Likhith Kumar Reddy Appakondreddigari, Ashish Nunna, Charishma Choudary Tummala, John Winship, Alex Zhou, Huthaifa I. Ashqar</dc:creator>
    </item>
    <item>
      <title>Modelling Loss of Complexity in Intermittent Time Series and its Application</title>
      <link>https://arxiv.org/abs/2411.14635</link>
      <description>arXiv:2411.14635v1 Announce Type: cross 
Abstract: In this paper, we developed a nonparametric relative entropy (RlEn) for modelling loss of complexity in intermittent time series. This technique consists of two steps. First, we carry out a nonlinear autoregressive model where the lag order is determined by a Bayesian Information Criterion (BIC), and complexity of each intermittent time series is obtained by our novel relative entropy. Second, change-points in complexity were detected by using the cumulative sum (CUSUM) based method. Using simulations and compared to the popular method appropriate entropy (ApEN), the performance of RlEn was assessed for its (1) ability to localise complexity change-points in intermittent time series; (2) ability to faithfully estimate underlying nonlinear models. The performance of the proposal was then examined in a real analysis of fatigue-induced changes in the complexity of human motor outputs. The results demonstrated that the proposed method outperformed the ApEn in accurately detecting complexity changes in intermittent time series segments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14635v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jie Li, Jian Zhang, Samantha L. Winter, Mark Burnley</dc:creator>
    </item>
    <item>
      <title>mmWave Radar for Sit-to-Stand Analysis: A Comparative Study with Wearables and Kinect</title>
      <link>https://arxiv.org/abs/2411.14656</link>
      <description>arXiv:2411.14656v1 Announce Type: cross 
Abstract: This study explores a novel approach for analyzing Sit-to-Stand (STS) movements using millimeter-wave (mmWave) radar technology. The goal is to develop a non-contact sensing, privacy-preserving, and all-day operational method for healthcare applications, including fall risk assessment. We used a 60GHz mmWave radar system to collect radar point cloud data, capturing STS motions from 45 participants. By employing a deep learning pose estimation model, we learned the human skeleton from Kinect built-in body tracking and applied Inverse Kinematics (IK) to calculate joint angles, segment STS motions, and extract commonly used features in fall risk assessment. Radar extracted features were then compared with those obtained from Kinect and wearable sensors. The results demonstrated the effectiveness of mmWave radar in capturing general motion patterns and large joint movements (e.g., trunk). Additionally, the study highlights the advantages and disadvantages of individual sensors and suggests the potential of integrated sensor technologies to improve the accuracy and reliability of motion analysis in clinical and biomedical research settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14656v1</guid>
      <category>eess.SP</category>
      <category>cs.ET</category>
      <category>stat.AP</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuting Hu, Peggy Ackun, Xiang Zhang, Siyang Cao, Jennifer Barton, Melvin G. Hector, Mindy J. Fain, Nima Toosizadeh</dc:creator>
    </item>
    <item>
      <title>Double Machine Learning for Adaptive Causal Representation in High-Dimensional Data</title>
      <link>https://arxiv.org/abs/2411.14665</link>
      <description>arXiv:2411.14665v1 Announce Type: cross 
Abstract: Adaptive causal representation learning from observational data is presented, integrated with an efficient sample splitting technique within the semiparametric estimating equation framework. The support points sample splitting (SPSS), a subsampling method based on energy distance, is employed for efficient double machine learning (DML) in causal inference. The support points are selected and split as optimal representative points of the full raw data in a random sample, in contrast to the traditional random splitting, and providing an optimal sub-representation of the underlying data generating distribution. They offer the best representation of a full big dataset, whereas the unit structural information of the underlying distribution via the traditional random data splitting is most likely not preserved. Three machine learning estimators were adopted for causal inference, support vector machine (SVM), deep learning (DL), and a hybrid super learner (SL) with deep learning (SDL), using SPSS. A comparative study is conducted between the proposed SVM, DL, and SDL representations using SPSS, and the benchmark results from Chernozhukov et al. (2018), which employed random forest, neural network, and regression trees with a random k-fold cross-fitting technique on the 401(k)-pension plan real data. The simulations show that DL with SPSS and the hybrid methods of DL and SL with SPSS outperform SVM with SPSS in terms of computational efficiency and the estimation quality, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14665v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lynda Aouar, Han Yu</dc:creator>
    </item>
    <item>
      <title>Summarizing Bayesian Nonparametric Mixture Posterior -- Sliced Optimal Transport Metrics for Gaussian Mixtures</title>
      <link>https://arxiv.org/abs/2411.14674</link>
      <description>arXiv:2411.14674v2 Announce Type: cross 
Abstract: Existing methods to summarize posterior inference for mixture models focus on identifying a point estimate of the implied random partition for clustering, with density estimation as a secondary goal (Wade and Ghahramani, 2018; Dahl et al., 2022). We propose a novel approach for summarizing posterior inference in nonparametric Bayesian mixture models, prioritizing density estimation of the mixing measure (or mixture) as an inference target. One of the key features is the model-agnostic nature of the approach, which remains valid under arbitrarily complex dependence structures in the underlying sampling model. Using a decision-theoretic framework, our method identifies a point estimate by minimizing posterior expected loss. A loss function is defined as a discrepancy between mixing measures. Estimating the mixing measure implies inference on the mixture density and the random partition. Exploiting the discrete nature of the mixing measure, we use a version of sliced Wasserstein distance. We introduce two specific variants for Gaussian mixtures. The first, mixed sliced Wasserstein, applies generalized geodesic projections on the product of the Euclidean space and the manifold of symmetric positive definite matrices. The second, sliced mixture Wasserstein, leverages the linearity of Gaussian mixture measures for efficient projection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14674v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khai Nguyen, Peter Mueller</dc:creator>
    </item>
    <item>
      <title>Deep Gaussian Process Emulation and Uncertainty Quantification for Large Computer Experiments</title>
      <link>https://arxiv.org/abs/2411.14690</link>
      <description>arXiv:2411.14690v1 Announce Type: cross 
Abstract: Computer models are used as a way to explore complex physical systems. Stationary Gaussian process emulators, with their accompanying uncertainty quantification, are popular surrogates for computer models. However, many computer models are not well represented by stationary Gaussian processes models. Deep Gaussian processes have been shown to be capable of capturing non-stationary behaviors and abrupt regime changes in the computer model response. In this paper, we explore the properties of two deep Gaussian process formulations within the context of computer model emulation. For one of these formulations, we introduce a new parameter that controls the amount of smoothness in the deep Gaussian process layers. We adapt a stochastic variational approach to inference for this model, allowing for prior specification and posterior exploration of the smoothness of the response surface. Our approach can be applied to a large class of computer models, and scales to arbitrarily large simulation designs. The proposed methodology was motivated by the need to emulate an astrophysical model of the formation of binary black hole mergers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14690v1</guid>
      <category>stat.ME</category>
      <category>astro-ph.IM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Faezeh Yazdi, Derek Bingham, Daniel Williamson</dc:creator>
    </item>
    <item>
      <title>Bayesian "Deep" Process Convolutions: An Application in Cosmology</title>
      <link>https://arxiv.org/abs/2411.14747</link>
      <description>arXiv:2411.14747v1 Announce Type: cross 
Abstract: The nonlinear matter power spectrum in cosmology describes how matter density fluctuations vary with scale in the universe, providing critical insights into large-scale structure formation. The matter power spectrum includes both smooth regions and highly oscillatory features. Cosmologists rely on noisy, multi-resolution realizations of large N-body simulations to study these phenomena, which require appropriate smoothing techniques to learn about underlying structures. We introduce a Bayesian Deep Process Convolution (DPC) model that flexibly adapts its smoothness parameter across the input space, enabling it to capture both smooth and variable structure within a single framework. The DPC model leverages common patterns across related functions to improve estimation in regions with sparse data. Compared to existing methods, the DPC model offers superior accuracy and uncertainty quantification in simulated data, and qualitatively superior performance with the cosmological data. This methodology will be useful in cosmology and other fields requiring flexible modeling of smooth nonstationary surfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14747v1</guid>
      <category>astro-ph.CO</category>
      <category>astro-ph.IM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kelly R. Moran, Richard Payne, Earl Lawrence, David Higdon, Stephen A. Walsh, Annie S. Booth, Juliana Kwan, Amber Day, Salman Habib, Katrin Heitmann</dc:creator>
    </item>
    <item>
      <title>Is there a robust effect of mainland mutualism rates on species richness of oceanic islands?</title>
      <link>https://arxiv.org/abs/2411.15105</link>
      <description>arXiv:2411.15105v1 Announce Type: cross 
Abstract: In island biogeography, it is widely accepted that species richness on island depends on the area and isolation of the island as well as the species pool on the mainland. Delavaux et al. (2024) suggest that species richness on oceanic islands also depends on the proportion of mutualists on the mainland, based on the idea that mutualists require specific interaction partners for their survival and thus have lower chances of establishment after successful immigration. As the proportion of mutualists increases towards the tropics, this effect could explain a weaker latitudinal diversity gradient (LDG) for oceanic islands. However, after re-analyzing their data, we have doubts if these conclusions are supported by the available data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15105v1</guid>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maximilian Pichler, Florian Hartig</dc:creator>
    </item>
    <item>
      <title>Bayesian Evidence Synthesis for Modeling SARS-CoV-2 Transmission</title>
      <link>https://arxiv.org/abs/2309.03122</link>
      <description>arXiv:2309.03122v2 Announce Type: replace 
Abstract: The acute phase of the Covid-19 pandemic has made apparent the need for decision support based upon accurate epidemic modeling. This process is substantially hampered by under-reporting of cases and related data incompleteness issues. In this article we adopt the Bayesian paradigm and synthesize publicly available data via a discrete-time stochastic epidemic modeling framework. The models allow for estimating the total number of infections while accounting for the endemic phase of the pandemic. We assess the prediction of the infection rate utilizing mobility information, notably the principal components of the mobility data. We evaluate variational Bayes in this context and find that Hamiltonian Monte Carlo offers a robust inference alternative for such models. We elaborate upon vector analysis of the epidemic dynamics, thus enriching the traditional tools used for decision making. In particular, we show how certain 2-dimensional plots on the phase plane may yield intuitive information regarding the speed and the type of transmission dynamics. We investigate the potential of a two-stage analysis as a consequence of cutting feedback, for inference on certain functionals of the model parameters. Finally, we show that a point mass on critical parameters is overly restrictive and investigate informative priors as a suitable alternative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.03122v2</guid>
      <category>stat.AP</category>
      <category>physics.soc-ph</category>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anastasios Apsemidis, Nikolaos Demiris</dc:creator>
    </item>
    <item>
      <title>Unraveling stochastic fundamental diagrams considering empirical knowledge: modeling, limitation and further discussion</title>
      <link>https://arxiv.org/abs/2404.09318</link>
      <description>arXiv:2404.09318v3 Announce Type: replace 
Abstract: Traffic flow modeling relies heavily on fundamental diagrams. However, deterministic fundamental diagrams, such as single or multi-regime models, cannot capture the uncertainty pattern that underlies traffic flow. To address this limitation, a sparse non-parametric regression model is proposed in this paper to formulate the stochastic fundamental diagram. Unlike parametric stochastic fundamental diagram models, a non-parametric model is insensitive to parameters, flexible, and applicable. The computation complexity and the huge memory required for training in the Gaussian process regression have been reduced by introducing the sparse Gaussian process regression. The paper also discusses how empirical knowledge influences the modeling process. The paper analyzes the influence of modeling empirical knowledge in the prior of the stochastic fundamental diagram model and whether empirical knowledge can improve the robustness and accuracy of the proposed model. By introducing several well-known single-regime fundamental diagram models as the prior and testing the model's robustness and accuracy with different sampling methods given real-world data, the authors find that empirical knowledge can only benefit the model under small inducing samples given a relatively clean and large dataset. A pure data-driven approach is sufficient to estimate and describe the pattern of the density-speed relationship.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09318v3</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.trc.2024.104851</arxiv:DOI>
      <arxiv:journal_reference>Transportation Research Part C: Emerging Technologies, 169, 104851 (2024)</arxiv:journal_reference>
      <dc:creator>Yuan-Zheng Lei, Yaobang Gong, Xianfeng Terry Yang</dc:creator>
    </item>
    <item>
      <title>Considerations for Single-Arm Trials to Support Accelerated Approval of Oncology Drugs</title>
      <link>https://arxiv.org/abs/2405.12437</link>
      <description>arXiv:2405.12437v2 Announce Type: replace 
Abstract: In the last two decades, single-arm trials (SATs) have been effectively used to study anticancer therapies in well-defined patient populations using durable response rates as an objective and interpretable clinical endpoints. With a growing trend of regulatory accelerated approval (AA) requiring randomized controlled trials (RCTs), some confusions have arisen about the roles of SATs in AA. This paper is intended to elucidate conditions under which an SAT may be considered reasonable for AA. Specifically, the paper describes (1) two necessary conditions for designing an SAT, (2) three sufficient conditions that help either optimize the study design or interpret the study results, (3) four conditions that demonstrate substantial evidence of clinical benefits of the drug, and (4) a plan of a confirmatory RCT to verify the clinical benefits. Some further considerations are discussed to help design a scientifically sound SAT and communicate with regulatory agencies. Conditions presented in this paper may serve as a set of references for sponsors using SATs for regulatory decision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12437v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Feinan Lu, Tao Wang, Ying Lu, Jie Chen</dc:creator>
    </item>
    <item>
      <title>Stable Survival Extrapolation via Transfer Learning</title>
      <link>https://arxiv.org/abs/2409.16044</link>
      <description>arXiv:2409.16044v2 Announce Type: replace 
Abstract: The mean survival is the key ingredient of the decision process in several applications, notably in health economic evaluations. It is defined as the area under the complete survival curve, thus necessitating extrapolation of the observed data. This may be achieved in a more stable manner by borrowing long term evidence from registry and demographic data. Such borrowing can be seen as an implicit bias-variance trade-off in unseen data. In this article we employ a Bayesian mortality model and transfer its projections in order to construct the baseline population that acts as an anchor of the survival model. We then propose extrapolation methods based on flexible parametric polyhazard models which can naturally accommodate diverse shapes, including non-proportional hazards and crossing survival curves, while typically maintaining a natural interpretation. We estimate the mean survival and related estimands in three cases, namely breast cancer, cardiac arrhythmia and advanced melanoma. Specifically, we evaluate the survival disadvantage of triple-negative breast cancer cases, the efficacy of combining immunotherapy with mRNA cancer therapeutics for melanoma treatment and the suitability of implantable cardioverter defibrilators for cardiac arrhythmia. The latter is conducted in a competing risks context illustrating how working on the cause-specific hazard alone minimizes potential instability. The results suggest that the proposed approach offers a flexible, interpretable and robust approach when survival extrapolation is required.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16044v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anastasios Apsemidis, Nikolaos Demiris</dc:creator>
    </item>
    <item>
      <title>Causal Representation Learning with Generative Artificial Intelligence: Application to Texts as Treatments</title>
      <link>https://arxiv.org/abs/2410.00903</link>
      <description>arXiv:2410.00903v2 Announce Type: replace 
Abstract: In this paper, we demonstrate how to enhance the validity of causal inference with unstructured high-dimensional treatments like texts, by leveraging the power of generative Artificial Intelligence. Specifically, we propose to use a deep generative model such as large language models (LLMs) to efficiently generate treatments and use their internal representation for subsequent causal effect estimation. We show that the knowledge of this true internal representation helps disentangle the treatment features of interest, such as specific sentiments and certain topics, from other possibly unknown confounding features. Unlike the existing methods, our proposed approach eliminates the need to learn causal representation from the data and hence produces more accurate and efficient estimates. We formally establish the conditions required for the nonparametric identification of the average treatment effect, propose an estimation strategy that avoids the violation of the overlap assumption, and derive the asymptotic properties of the proposed estimator through the application of double machine learning. Finally, using an instrumental variables approach, we extend the proposed methodology to the settings, in which the treatment feature is based on human perception rather than is assumed to be fixed given the treatment object. The proposed methodology is also applicable to text reuse where an LLM is used to regenerate the existing texts. We conduct simulation and empirical studies, using the generated text data from an open-source LLM, Llama 3, to illustrate the advantages of our estimator over the state-of-the-art causal representation learning algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00903v2</guid>
      <category>stat.AP</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kosuke Imai, Kentaro Nakamura</dc:creator>
    </item>
    <item>
      <title>Rank-adaptive covariance testing with applications to genomics and neuroimaging</title>
      <link>https://arxiv.org/abs/2309.10284</link>
      <description>arXiv:2309.10284v2 Announce Type: replace-cross 
Abstract: In biomedical studies, testing for differences in covariance offers scientific insights beyond mean differences, especially when differences are driven by complex joint behavior between features. However, when differences in joint behavior are weakly dispersed across many dimensions and arise from differences in low-rank structures within the data, as is often the case in genomics and neuroimaging, existing two-sample covariance testing methods may suffer from power loss. The Ky-Fan(k) norm, defined by the sum of the top Ky-Fan(k) singular values, is a simple and intuitive matrix norm able to capture signals caused by differences in low-rank structures between matrices, but its statistical properties in hypothesis testing have not been studied well. In this paper, we investigate the behavior of the Ky-Fan(k) norm in two-sample covariance testing. Ultimately, we propose a novel methodology, Rank-Adaptive Covariance Testing (RACT), which is able to leverage differences in low-rank structures found in the covariance matrices of two groups in order to maximize power. RACT uses permutation for statistical inference, ensuring an exact Type I error control. We validate RACT in simulation studies and evaluate its performance when testing for differences in gene expression networks between two types of lung cancer, as well as testing for covariance heterogeneity in diffusion tensor imaging (DTI) data taken on two different scanner types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.10284v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>David Veitch, Yinqiu He, Jun Young Park</dc:creator>
    </item>
    <item>
      <title>HAL-based Plugin Estimation of the Causal Dose-Response Curve</title>
      <link>https://arxiv.org/abs/2406.05607</link>
      <description>arXiv:2406.05607v3 Announce Type: replace-cross 
Abstract: Estimating and obtaining reliable inference for the marginally adjusted causal dose-response curve for continuous treatments without relying on parametric assumptions is a well-known statistical challenge. Parametric models risk introducing significant bias through model misspecification, compromising the accurate representation of the underlying data and dose-response relationship. On the other hand, nonparametric models face difficulties as the dose-response curve is not pathwise differentiable, preventing consistent estimation at standard rates. The Highly Adaptive Lasso (HAL) maximum likelihood estimator offers a promising approach to this issue. In this paper, we introduce a HAL-based plug-in estimator for the causal dose-response curve and assess its empirical performance against other estimators. Through comprehensive simulations, we evaluate the accuracy of the estimation and the quality of the inference, particularly in terms of coverage, using robust standard error estimators. Our results demonstrate the finite-sample effectiveness of the HAL-based estimator, utilizing an undersmoothed and smoothness-adaptive fit for the conditional outcome model. Additionally, the simulations reveal that the HAL-based estimator consistently outperforms existing methods for estimating the causal dose-response curve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05607v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junming Shi, Wenxin Zhang, Alan E. Hubbard, Mark van der Laan</dc:creator>
    </item>
  </channel>
</rss>
