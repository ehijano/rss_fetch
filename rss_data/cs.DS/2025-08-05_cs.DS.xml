<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 06 Aug 2025 01:34:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Learned LSM-trees: Two Approaches Using Learned Bloom Filters</title>
      <link>https://arxiv.org/abs/2508.00882</link>
      <description>arXiv:2508.00882v1 Announce Type: new 
Abstract: Modern key-value stores rely heavily on Log-Structured Merge (LSM) trees for write optimization, but this design introduces significant read amplification. Auxiliary structures like Bloom filters help, but impose memory costs that scale with tree depth and dataset size. Recent advances in learned data structures suggest that machine learning models can augment or replace these components, trading handcrafted heuristics for data-adaptive behavior. In this work, we explore two approaches for integrating learned predictions into the LSM-tree lookup path. The first uses a classifier to selectively bypass Bloom filter probes for irrelevant levels, aiming to reduce average-case query latency. The second replaces traditional Bloom filters with compact learned models and small backup filters, targeting memory footprint reduction without compromising correctness. We implement both methods atop a Monkey-style LSM-tree with leveled compaction, per-level Bloom filters, and realistic workloads. Our experiments show that the classifier reduces GET latency by up to 2.28x by skipping over 30% of Bloom filter checks with high precision, though it incurs a modest false-negative rate. The learned Bloom filter design achieves zero false negatives and retains baseline latency while cutting memory usage per level by 70-80%. Together, these designs illustrate complementary trade-offs between latency, memory, and correctness, and highlight the potential of learned index components in write-optimized storage systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00882v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas Fidalgo, Puyuan Ye</dc:creator>
    </item>
    <item>
      <title>Efficient Direct-Access Ranked Retrieval</title>
      <link>https://arxiv.org/abs/2508.01108</link>
      <description>arXiv:2508.01108v1 Announce Type: new 
Abstract: We study the problem of Direct-Access Ranked Retrieval (DAR) for interactive data tooling, where evolving data exploration practices, combined with large-scale and high-dimensional datasets, create new challenges. DAR concerns the problem of enabling efficient access to arbitrary rank positions according to a ranking function, without enumerating all preceding tuples. To address this need, we formalize the DAR problem and propose a theoretically efficient algorithm based on geometric arrangements, achieving logarithmic query time. However, this method suffers from exponential space complexity in high dimensions. Therefore, we develop a second class of algorithms based on $\varepsilon$-sampling, which consume a linear space. Since exactly locating the tuple at a specific rank is challenging due to its connection to the range counting problem, we introduce a relaxed variant called Conformal Set Ranked Retrieval (CSR), which returns a small subset guaranteed to contain the target tuple. To solve the CSR problem efficiently, we define an intermediate problem, Stripe Range Retrieval (SRR), and design a hierarchical sampling data structure tailored for narrow-range queries. Our method achieves practical scalability in both data size and dimensionality. We prove near-optimal bounds on the efficiency of our algorithms and validate their performance through extensive experiments on real and synthetic datasets, demonstrating scalability to millions of tuples and hundreds of dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01108v1</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <category>cs.DB</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Dehghankar, Raghav Mittal, Suraj Shetiya, Abolfazl Asudeh, Gautam Das</dc:creator>
    </item>
    <item>
      <title>PageRank Centrality in Directed Graphs with Bounded In-Degree</title>
      <link>https://arxiv.org/abs/2508.01257</link>
      <description>arXiv:2508.01257v1 Announce Type: new 
Abstract: We study the computational complexity of locally estimating a node's PageRank centrality in a directed graph $G$. For any node $t$, its PageRank centrality $\pi(t)$ is defined as the probability that a random walk in $G$, starting from a uniformly chosen node, terminates at $t$, where each step terminates with a constant probability $\alpha\in(0,1)$.
  To obtain a multiplicative $\big(1\pm O(1)\big)$-approximation of $\pi(t)$ with probability $\Omega(1)$, the previously best upper bound is $O(n^{1/2}\min\{ \Delta_{in}^{1/2},\Delta_{out}^{1/2},m^{1/4}\})$ from [Wang, Wei, Wen, Yang STOC '24], where $n$ and $m$ denote the number of nodes and edges in $G$, and $\Delta_{in}$ and $\Delta_{out}$ upper bound the in-degrees and out-degrees of $G$, respectively. The same paper implicitly gives the previously best lower bound of $\Omega(n^{1/2}\min\{\Delta_{in}^{1/2}/n^{\gamma},\Delta_{out}^{1/2}/n^{\gamma},m^{1/4}\})$, where $\gamma=\frac{\log(1/(1-\alpha))}{4\log\Delta_{in}-2\log(1/(1-\alpha))}$ if $\Delta_{in}&gt;1/(1-\alpha)$, and $\gamma=1/2$ if $\Delta_{in}\le1/(1-\alpha)$. As $\gamma$ only depends on $\Delta_{in}$, the known upper bound is tight if we only parameterize the complexity by $n$, $m$, and $\Delta_{out}$. However, there remains a gap of $\Omega(n^{\gamma})$ when considering $\Delta_{in}$, and this gap is large when $\Delta_{in}$ is small. In the extreme case where $\Delta_{in}\le1/(1-\alpha)$, we have $\gamma=1/2$, leading to a gap of $\Omega(n^{1/2})$ between the bounds $O(n^{1/2})$ and $\Omega(1)$.
  In this paper, we present a new algorithm that achieves the above lower bound (up to logarithmic factors). The algorithm assumes that $n$ and the bounds $\Delta_{in}$ and $\Delta_{out}$ are known in advance. Our key technique is a novel randomized backwards propagation process which only propagates selectively based on Monte Carlo estimated PageRank scores.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01257v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mikkel Thorup, Hanzhi Wang, Zhewei Wei, Mingji Yang</dc:creator>
    </item>
    <item>
      <title>Towards Faster Feasible Matrix Multiplication by Trilinear Aggregation</title>
      <link>https://arxiv.org/abs/2508.01748</link>
      <description>arXiv:2508.01748v1 Announce Type: new 
Abstract: Matrix multiplication is a fundamental kernel in high performance computing. Many algorithms for fast matrix multiplication can only be applied to enormous matrices ($n&gt;10^{100}$) and thus cannot be used in practice. Of all algorithms applicable to feasible input, Pan's $O(n^{2.773372})$ algorithm (1982) is asymptotically the fastest. We obtain an $O(n^{2.773203})$ algorithm applicable to the same input sizes as Pan's algorithm. This algorithm is the fastest matrix multiplication algorithm with base case smaller than $1000$. Further, our method obtains the best asymptotic complexity for many small base cases, starting at $n_0=28$. We also obtain better exponents for larger base cases. To construct our algorithm, we use the trilinear aggregation method. We find parts of the algorithms that are equivalent to matrix multiplication with smaller base case, and use the de Groote equivalence to replace these parts in a way that allows further optimization of our algorithms. Finally, we improve the additive complexity of our algorithms by finding a sparse decomposition and reducing the leading coefficient. These mark a fundamental step towards outperforming existing fast matrix multiplication algorithms in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01748v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oded Schwartz, Eyal Zwecher</dc:creator>
    </item>
    <item>
      <title>Near-Optimal Differentially Private Graph Algorithms via the Multidimensional AboveThreshold Mechanism</title>
      <link>https://arxiv.org/abs/2508.02182</link>
      <description>arXiv:2508.02182v1 Announce Type: new 
Abstract: Many differentially private and classical non-private graph algorithms rely crucially on determining whether some property of each vertex meets a threshold. For example, for the $k$-core decomposition problem, the classic peeling algorithm iteratively removes a vertex if its induced degree falls below a threshold. The sparse vector technique (SVT) is generally used to transform non-private threshold queries into private ones with only a small additive loss in accuracy. However, a naive application of SVT in the graph setting leads to an amplification of the error by a factor of $n$ due to composition, as SVT is applied to every vertex. In this paper, we resolve this problem by formulating a novel generalized sparse vector technique which we call the Multidimensional AboveThreshold (MAT) Mechanism which generalizes SVT (applied to vectors with one dimension) to vectors with multiple dimensions. As an application, we solve a number of important graph problems with better bounds than previous work.
  We apply our MAT mechanism to obtain a set of improved bounds for a variety of problems including $k$-core decomposition, densest subgraph, low out-degree ordering, and vertex coloring. We give a tight local edge DP algorithm for $k$-core decomposition with $O(\epsilon^{-1}\log n)$ additive error and no multiplicative error in $O(n)$ rounds. We also give a new $(2+\eta)$-factor multiplicative, $O(\epsilon^{-1}\log n)$ additive error algorithm in $O(\log^2 n)$ rounds for any constant $\eta &gt; 0$. Both of these results are asymptotically tight against our new lower bound of $\Omega(\log n)$ for any constant-factor approximation algorithm for $k$-core decomposition. Our new algorithms for $k$-core also directly lead to new algorithms for densest subgraph and low out-degree ordering. Our novel private defective coloring algorithms uses number of colors proportional to the arboricity of the graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02182v1</guid>
      <category>cs.DS</category>
      <category>cs.CR</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Laxman Dhulipala, Monika Henzinger, George Z. Li, Quanquan C. Liu, A. R. Sricharan, Leqi Zhu</dc:creator>
    </item>
    <item>
      <title>Testing Quasiperiodicity</title>
      <link>https://arxiv.org/abs/2508.02231</link>
      <description>arXiv:2508.02231v1 Announce Type: new 
Abstract: A cover (or quasiperiod) of a string $S$ is a shorter string $C$ such that every position of $S$ is contained in some occurrence of $C$ as a substring. The notion of covers was introduced by Apostolico and Ehrenfeucht over 30 years ago [Theor. Comput. Sci. 1993] and it has received significant attention from the combinatorial pattern matching community. In this note, we show how to efficiently test whether $S$ admits a cover. Our tester can also be translated into a streaming algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02231v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christine Awofeso, Ben Bals, Oded Lachish, Solon P. Pissis</dc:creator>
    </item>
    <item>
      <title>Facility Location and $k$-Median with Fair Outliers</title>
      <link>https://arxiv.org/abs/2508.02572</link>
      <description>arXiv:2508.02572v1 Announce Type: new 
Abstract: Classical clustering problems such as \emph{Facility Location} and \emph{$k$-Median} aim to efficiently serve a set of clients from a subset of facilities -- minimizing the total cost of facility openings and client assignments in Facility Location, and minimizing assignment (service) cost under a facility count constraint in $k$-Median. These problems are highly sensitive to outliers, and therefore researchers have studied variants that allow excluding a small number of clients as outliers to reduce cost. However, in many real-world settings, clients belong to different demographic or functional groups, and unconstrained outlier removal can disproportionately exclude certain groups, raising fairness concerns.
  We study \emph{Facility Location with Fair Outliers}, where each group is allowed a specified number of outliers, and the objective is to minimize total cost while respecting group-wise fairness constraints. We present a bicriteria approximation with a $O(1/\epsilon)$ approximation factor and $(1+ 2\epsilon)$ factor violation in outliers per group. For \emph{$k$-Median with Fair Outliers}, we design a bicriteria approximation with a $4(1+\omega/\epsilon)$ approximation factor and $(\omega + \epsilon)$ violation in outliers per group improving on prior work by avoiding dependence on $k$ in outlier violations. We also prove that the problems are W[1]-hard parameterized by $\omega$, assuming the Exponential Time Hypothesis.
  We complement our algorithmic contributions with a detailed empirical analysis, demonstrating that fairness can be achieved with negligible increase in cost and that the integrality gap of the standard LP is small in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02572v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajni Dabas, Samir Khuller, Emilie Rivkin</dc:creator>
    </item>
    <item>
      <title>Instance-Optimal Uniformity Testing and Tracking</title>
      <link>https://arxiv.org/abs/2508.02637</link>
      <description>arXiv:2508.02637v1 Announce Type: new 
Abstract: In the uniformity testing task, an algorithm is provided with samples from an unknown probability distribution over a (known) finite domain, and must decide whether it is the uniform distribution, or, alternatively, if its total variation distance from uniform exceeds some input distance parameter. This question has received a significant amount of interest and its complexity is, by now, fully settled. Yet, we argue that it fails to capture many scenarios of interest, and that its very definition as a gap problem in terms of a prespecified distance may lead to suboptimal performance.
  To address these shortcomings, we introduce the problem of uniformity tracking, whereby an algorithm is required to detect deviations from uniformity (however they may manifest themselves) using as few samples as possible, and be competitive against an optimal algorithm knowing the distribution profile in hindsight. Our main contribution is a $\operatorname{polylog}(\operatorname{opt})$-competitive uniformity tracking algorithm. We obtain this result by leveraging new structural results on Poisson mixtures, which we believe to be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02637v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guy Blanc, Cl\'ement L. Canonne, Erik Waingarten</dc:creator>
    </item>
    <item>
      <title>A Myhill-Nerode Theorem for Generalized Automata, with Applications to Pattern Matching and Compression</title>
      <link>https://arxiv.org/abs/2302.06506</link>
      <description>arXiv:2302.06506v3 Announce Type: cross 
Abstract: The model of generalized automata, introduced by Eilenberg in 1974, allows representing a regular language more concisely than conventional automata by allowing edges to be labeled not only with characters, but also strings. Giammaresi and Montalbano introduced a notion of determinism for generalized automata [STACS 1995]. While generalized deterministic automata retain many properties of conventional deterministic automata, the uniqueness of a minimal generalized deterministic automaton is lost.
  In the first part of the paper, we show that the lack of uniqueness can be explained by introducing a set $ \mathcal{W(A)} $ associated with a generalized automaton $ \mathcal{A} $. In this way, we derive for the first time a full Myhill-Nerode theorem for generalized automata, which contains the textbook Myhill-Nerode theorem for conventional automata as a degenerate case. In the second part of the paper, we show that the set $ \mathcal{W(A)} $ leads to applications for pattern matching and data compression. We show that a Wheeler generalized automata can be stored using $ \mathfrak{e} \log \sigma (1 + o(1)) + O(e) $ bits so that pattern matching queries can be solved in $ O(m \log \log \sigma) $ time, where $ \mathfrak{e} $ is the total length of all edge labels, $ e $ is the number of edges, $ \sigma $ is the size of the alphabet and $ m $ is the length of the pattern.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.06506v3</guid>
      <category>cs.FL</category>
      <category>cs.DS</category>
      <category>cs.LO</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nicola Cotumaccio</dc:creator>
    </item>
    <item>
      <title>Deterministic Fault-Tolerant Local Load Balancing and its Applications against Adaptive Adversaries</title>
      <link>https://arxiv.org/abs/2508.01373</link>
      <description>arXiv:2508.01373v1 Announce Type: cross 
Abstract: Load balancing is among the basic primitives in distributed computing. In this paper, we consider this problem when executed locally on a network with nodes prone to failures. We show that there exist lightweight network topologies that are immune to message delivery failures incurred by (at most) a constant fraction of all nodes. More precisely, we design a novel deterministic fault-tolerant local load balancing (LLB) algorithm, which, similarly to their classical counterparts working in fault-free networks, has a relatively simple structure and guarantees exponentially fast convergence to the average value despite crash and omission failures.
  As the second part of our contribution, we show three applications of the newly developed fault-tolerant local load balancing protocol. We give a randomized consensus algorithm, working against $t &lt; n / 3$ crash failures, that improves over the best-known consensus solution by Hajiaghayi et al. with respect to communication complexity, yet with an arguable simpler technique of combining a randomly and locally selected virtual communication graph with a deterministic fault-tolerant local load balancing on this graph.
  We also give a new solution for consensus for networks with omission failures. Our solution works against $t &lt; \frac{n}{C\log{n} (\log\log n)^2}$ omissions, for some constant $C$, is nearly optimal in terms of time complexity, but most notably -- it has communication complexity $O((t^2 + n)\text{ polylog } {n})$, matching, within a polylogarithmic factor, the lower bound by Abraham et. al. with respect to both terms depending on $t$ and $n$. Ours is the first algorithm in the literature that is simultaneously nearly optimal, in terms of $n,t$, with respect to both complexity measures, against the adaptive omission-causing adversary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01373v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dariusz R. Kowalski, Jan Olkowski</dc:creator>
    </item>
    <item>
      <title>Towards EXPTIME One Way Functions: Bloom Filters, Succinct Graphs, Cliques, &amp; Self Masking</title>
      <link>https://arxiv.org/abs/2508.01649</link>
      <description>arXiv:2508.01649v1 Announce Type: cross 
Abstract: Consider graphs of n nodes, and use a Bloom filter of length 2 log3 n bits. An edge between nodes i and j, with i &lt; j, turns on a certain bit of the Bloom filter according to a hash function on i and j. Pick a set of log n nodes and turn on all the bits of the Bloom filter required for these log n nodes to form a clique. As a result, the Bloom filter implies the existence of certain other edges, those edges (x, y), with x &lt; y, such that all the bits selected by applying the hash functions to x and y happen to have been turned on due to hashing the clique edges into the Bloom filter.
  Constructing the graph consisting of the clique-selected edges and those edges mapped to the turned-on bits of the Bloom filter can be performed in polynomial time in n. Choosing a large enough polylogarithmic in n Bloom filter yields that the graph has only one clique of size log n, the planted clique. When the hash function is black-boxed, finding that clique is intractable and, therefore, inverting the function that maps log n nodes to a graph is not (likely to be) possible in polynomial time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01649v1</guid>
      <category>cs.CC</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shlomi Dolev</dc:creator>
    </item>
    <item>
      <title>Faster Distributed $\Delta$-Coloring via a Reduction to MIS</title>
      <link>https://arxiv.org/abs/2508.01762</link>
      <description>arXiv:2508.01762v1 Announce Type: cross 
Abstract: Recent improvements on the deterministic complexities of fundamental graph problems in the LOCAL model of distributed computing have yielded state-of-the-art upper bounds of $\tilde{O}(\log^{5/3} n)$ rounds for maximal independent set (MIS) and $(\Delta + 1)$-coloring [Ghaffari, Grunau, FOCS'24] and $\tilde{O}(\log^{19/9} n)$ rounds for the more restrictive $\Delta$-coloring problem [Ghaffari, Kuhn, FOCS'21; Ghaffari, Grunau, FOCS'24; Bourreau, Brandt, Nolin, STOC'25]. In our work, we show that $\Delta$-coloring can be solved deterministically in $\tilde{O}(\log^{5/3} n)$ rounds as well, matching the currently best bound for $(\Delta + 1)$-coloring.
  We achieve our result by developing a reduction from $\Delta$-coloring to MIS that guarantees that the (asymptotic) complexity of $\Delta$-coloring is at most the complexity of MIS, unless MIS can be solved in sublogarithmic time, in which case, due to the $\Omega(\log n)$-round $\Delta$-coloring lower bound from [BFHKLRSU, STOC'16], our reduction implies a tight complexity of $\Theta(\log n)$ for $\Delta$-coloring. In particular, any improvement on the complexity of the MIS problem will yield the same improvement for the complexity of $\Delta$-coloring (up to the true complexity of $\Delta$-coloring).
  Our reduction yields improvements for $\Delta$-coloring in the randomized LOCAL model and when complexities are parameterized by both $n$ and $\Delta$. We obtain a randomized complexity bound of $\tilde{O}(\log^{5/3} \log n)$ rounds (improving over the state of the art of $\tilde{O}(\log^{8/3} \log n)$ rounds) on general graphs and tight complexities of $\Theta(\log n)$ and $\Theta(\log \log n)$ for the deterministic, resp.\ randomized, complexity on bounded-degree graphs. In the special case of graphs of constant clique number (which for instance include bipartite graphs), we also give a reduction to the $(\Delta+1)$-coloring problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01762v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yann Bourreau, Sebastian Brandt, Alexandre Nolin</dc:creator>
    </item>
    <item>
      <title>Fast Gaussian process inference by exact Mat\'ern kernel decomposition</title>
      <link>https://arxiv.org/abs/2508.01864</link>
      <description>arXiv:2508.01864v1 Announce Type: cross 
Abstract: To speed up Gaussian process inference, a number of fast kernel matrix-vector multiplication (MVM) approximation algorithms have been proposed over the years. In this paper, we establish an exact fast kernel MVM algorithm based on exact kernel decomposition into weighted empirical cumulative distribution functions, compatible with a class of kernels which includes multivariate Mat\'ern kernels with half-integer smoothness parameter. This algorithm uses a divide-and-conquer approach, during which sorting outputs are stored in a data structure. We also propose a new algorithm to take into account some linear fixed effects predictor function. Our numerical experiments confirm that our algorithm is very effective for low-dimensional Gaussian process inference problems with hundreds of thousands of data points. An implementation of our algorithm is available at https://gitlab.com/warin/fastgaussiankernelregression.git.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01864v1</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Langren\'e, Xavier Warin, Pierre Gruet</dc:creator>
    </item>
    <item>
      <title>An Improved Bound for the Beck-Fiala Conjecture</title>
      <link>https://arxiv.org/abs/2508.01937</link>
      <description>arXiv:2508.01937v1 Announce Type: cross 
Abstract: In 1981, Beck and Fiala [Discrete Appl. Math, 1981] conjectured that given a set system $A \in \{0,1\}^{m \times n}$ with degree at most $k$ (i.e., each column of $A$ has at most $k$ non-zeros), its combinatorial discrepancy $\mathsf{disc}(A) := \min_{x \in \{\pm 1\}^n} \|Ax\|_\infty$ is at most $O(\sqrt{k})$. Previously, the best-known bounds for this conjecture were either $O(k)$, first established by Beck and Fiala [Discrete Appl. Math, 1981], or $O(\sqrt{k \log n})$, first proved by Banaszczyk [Random Struct. Algor., 1998].
  We give an algorithmic proof of an improved bound of $O(\sqrt{k \log\log n})$ whenever $k \geq \log^5 n$, thus matching the Beck-Fiala conjecture up to $O(\sqrt{\log \log n})$ for almost the full regime of $k$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01937v1</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikhil Bansal, Haotian Jiang</dc:creator>
    </item>
    <item>
      <title>Robust Detection of Planted Subgraphs in Semi-Random Models</title>
      <link>https://arxiv.org/abs/2508.02158</link>
      <description>arXiv:2508.02158v1 Announce Type: cross 
Abstract: Detection of planted subgraphs in Erd\"os-R\'enyi random graphs has been extensively studied, leading to a rich body of results characterizing both statistical and computational thresholds. However, most prior work assumes a purely random generative model, making the resulting algorithms potentially fragile in the face of real-world perturbations. In this work, we initiate the study of semi-random models for the planted subgraph detection problem, wherein an adversary is allowed to remove edges outside the planted subgraph before the graph is revealed to the statistician. Crucially, the statistician remains unaware of which edges have been removed, introducing fundamental challenges to the inference task. We establish fundamental statistical limits for detection under this semi-random model, revealing a sharp dichotomy. Specifically, for planted subgraphs with strongly sub-logarithmic maximum density detection becomes information-theoretically impossible in the presence of an adversary, despite being possible in the classical random model. In stark contrast, for subgraphs with super-logarithmic density, the statistical limits remain essentially unchanged; we prove that the optimal (albeit computationally intractable) likelihood ratio test remains robust. Beyond these statistical boundaries, we design a new computationally efficient and robust detection algorithm, and provide rigorous statistical guarantees for its performance. Our results establish the first robust framework for planted subgraph detection and open new directions in the study of semi-random models, computational-statistical trade-offs, and robustness in graph inference problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02158v1</guid>
      <category>cs.IT</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dor Elimelech, Wasim Huleihel</dc:creator>
    </item>
    <item>
      <title>A Threshold Phenomenon for the Shortest Lattice Vector Problem in the Infinity Norm</title>
      <link>https://arxiv.org/abs/2508.02249</link>
      <description>arXiv:2508.02249v1 Announce Type: cross 
Abstract: One important question in the theory of lattices is to detect a shortest vector: given a norm and a lattice, what is the smallest norm attained by a non-zero vector contained in the lattice? We focus on the infinity norm and work with lattices of the form $A\mathbb{Z}^n$, where $A$ has integer entries and is of full column rank. Finding a shortest vector is NP-hard. We show that this task is fixed parameter tractable in the parameter $\Delta$, the largest absolute value of the determinant of a full rank submatrix of $A$. The algorithm is based on a structural result that can be interpreted as a threshold phenomenon: whenever the dimension $n$ exceeds a certain value determined only by $\Delta$, then a shortest lattice vector attains an infinity norm value of one. This threshold phenomenon has several applications. In particular, it reveals that integer optimal solutions lie on faces of the given polyhedron whose dimensions are bounded only in terms of $\Delta$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02249v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stefan Kuhlmann, Robert Weismantel</dc:creator>
    </item>
    <item>
      <title>A Maximum Linear Arrangement Problem on Directed Graphs</title>
      <link>https://arxiv.org/abs/1810.12277</link>
      <description>arXiv:1810.12277v2 Announce Type: replace 
Abstract: We propose a new arrangement problem on directed graphs, Maximum Directed Linear Arrangement (MaxDLA). This is a directed variant of a similar problem for undirected graphs, in which however one seeks maximum and not minimum; this problem known as the Minimum Linear Arrangement Problem (MinLA) has been much studied in the literature. We establish a number of theorems illustrating the behavior and complexity of MaxDLA. First, we relate MaxDLA to Maximum Directed Cut (MaxDiCut) by proving that every simple digraph $D$ on $n$ vertices satisfies $\frac{n}{2}$$maxDiCut(D) \leq MaxDLA(D) \leq (n-1)MaxDiCut(D)$. Next, we prove that MaxDiCut is NP-Hard for planar digraphs (even with the added restriction of maximum degree 15); it follows from the above bounds that MaxDLA is also NP-Hard for planar digraphs. In contrast, Hadlock (1975) and Dorfman and Orlova (1972) showed that the undirected Maximum Cut problem is solvable in polynomial time on planar graphs.
  On the positive side, we present a polynomial-time algorithm for solving MaxDLA on orientations of trees with degree bounded by a constant, which translates to a polynomial-time algorithm for solving MinLA on the complements of those trees. This pairs with results by Goldberg and Klipker (1976), Shiloach (1979) and Chung (1984) solving MinLA in polynomial time on trees. Finally, analogues of Harper's famous isoperimetric inequality for the hypercube, in the setting of MaxDLA, are shown for tournaments, orientations of graphs with degree at most two, and transitive acyclic digraphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:1810.12277v2</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matt DeVos, Kathryn Nurse</dc:creator>
    </item>
    <item>
      <title>Range (R\'enyi) Entropy Queries and Partitioning</title>
      <link>https://arxiv.org/abs/2312.15959</link>
      <description>arXiv:2312.15959v3 Announce Type: replace 
Abstract: Data partitioning that maximizes/minimizes the Shannon entropy, or more generally the R\'enyi entropy is a crucial subroutine in data compression, columnar storage, and cardinality estimation algorithms. These partition algorithms can be accelerated if we have a data structure to compute the entropy in different subsets of data when the algorithm needs to decide what block to construct. Such a data structure will also be useful for data analysts exploring different subsets of data to identify areas of interest. While it is generally known how to compute the Shannon or the R\'enyi entropy of a discrete distribution in the offline or streaming setting efficiently, we focus on the query setting where we aim to efficiently derive the entropy among a subset of data that satisfy some linear predicates. We solve this problem in a typical setting when we deal with real data, where data items are geometric points and each requested area is a query (hyper)rectangle. More specifically, we consider a set $P$ of $n$ weighted and colored points in $\mathbb{R}^d$. For the range S-entropy (resp. R-entropy) query problem, the goal is to construct a low space data structure, such that given a query (hyper)rectangle $R$, it computes the Shannon (resp. R\'enyi) entropy based on the colors and the weights of the points in $P\cap R$, in sublinear time. We show conditional lower bounds proving that we cannot hope for data structures with near-linear space and near-constant query time for both the range S-entropy and R-entropy query problems. Then, we propose exact data structures for $d=1$ and $d&gt;1$ with $o(n^{2d})$ space and $o(n)$ query time for both problems. Finally, we propose near linear space data structures for returning either an additive or a multiplicative approximation of the Shannon (resp. R\'enyi) entropy in $P\cap R$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15959v3</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aryan Esmailpour, Sanjay Krishnan, Stavros Sintos</dc:creator>
    </item>
    <item>
      <title>On the enumeration of signatures of XOR-CNF's</title>
      <link>https://arxiv.org/abs/2402.18537</link>
      <description>arXiv:2402.18537v2 Announce Type: replace 
Abstract: Given a CNF formula $\varphi$ with clauses $C_1, \dots, C_m$ over a set of variables $V$, a truth assignment $\mathbf{a} : V \to \{0, 1\}$ generates a binary sequence $\sigma_\varphi(\mathbf{a})=(C_1(\mathbf{a}), \ldots, C_m(\mathbf{a}))$, called a signature of $\varphi$, where $C_i(\mathbf{a})=1$ if clause $C_i$ evaluates to 1 under assignment $\mathbf{a}$, and $C_i(\mathbf{a})=0$ otherwise. Signatures and their associated generation problems have given rise to new yet promising research questions in algorithmic enumeration. In a recent paper, B\'erczi et al. interestingly proved that generating signatures of a CNF is tractable despite the fact that verifying a solution is hard. They also showed the hardness of finding maximal signatures of an arbitrary CNF due to the intractability of satisfiability in general. Their contribution leaves open the problem of efficiently generating maximal signatures for tractable classes of CNFs, i.e., those for which satisfiability can be solved in polynomial time. Stepping into that direction, we completely characterize the complexity of generating all, minimal, and maximal signatures for XOR-CNFs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18537v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nadia Creignou, Oscar Defrain, Fr\'ed\'eric Olive, Simon Vilmin</dc:creator>
    </item>
    <item>
      <title>Fast In-Spectrum Graph Watermarks</title>
      <link>https://arxiv.org/abs/2502.04182</link>
      <description>arXiv:2502.04182v2 Announce Type: replace 
Abstract: We address the problem of watermarking graph objects, which consists in hiding information within them, to prove their origin. The two existing methods to watermark graphs use subgraph matching or graph isomorphism techniques, which are known to be intractable for large graphs. To reduce the operational complexity, we propose FFG, a new graph watermarking scheme adapted from an image watermarking scheme, since graphs and images can be represented as matrices. We analyze and compare FFG, whose novelty lies in embedding the watermark in the Fourier transform of the adjacency matrix of a graph. Our technique enjoys a much lower complexity than that of related works (i.e. in $\mathcal{O}\left(N^2 \log N\right)$), while performing better or at least as well as the two state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04182v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jade Garcia Bourr\'ee, Anne-Marie Kermarrec, Erwan Le Merrer, Othmane Safsafi</dc:creator>
    </item>
    <item>
      <title>Enumerating minimal dominating sets and variants in chordal bipartite graphs</title>
      <link>https://arxiv.org/abs/2502.14611</link>
      <description>arXiv:2502.14611v3 Announce Type: replace 
Abstract: Enumerating minimal dominating sets with polynomial delay in bipartite graphs is a long-standing open problem. To date, even the subcase of chordal bipartite graphs is open, with the best known algorithm due to Golovach, Heggernes, Kant\'e, Kratsch, Saether, and Villanger running in incremental-polynomial time. We improve on this result by providing a polynomial delay and space algorithm enumerating minimal dominating sets in chordal bipartite graphs. Additionally, we show that the total and connected variants admit polynomial and incremental-polynomial delay algorithms, respectively, within the same class. This provides an alternative proof of a result by Golovach et al. for total dominating sets, and answers an open question for the connected variant. Finally, we give evidence that the techniques used in this paper cannot be generalized to bipartite graphs for (total) minimal dominating sets, unless P = NP, and show that enumerating minimal connected dominating sets in bipartite graphs is harder than enumerating minimal transversals in general hypergraphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14611v3</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emanuel Castelo, Oscar Defrain, Guilherme C. M. Gomes</dc:creator>
    </item>
    <item>
      <title>Location-Restricted Stable Matching</title>
      <link>https://arxiv.org/abs/2505.03680</link>
      <description>arXiv:2505.03680v2 Announce Type: replace 
Abstract: Motivated by group-project distribution, we introduce and study stable matching under the constraint of applicants needing to share a location to be matched with the same institute, which we call the Location-Restricted Stable Matching problem (LRSM). We show that finding a feasible matching is NP-hard, making finding a feasible and stable matching automatically NP-hard. We then analyze the subproblem where all the projects have the same capacity, and the applicant population of each location is a multiple of the universal project capacity, which mimics more realistic constraints and makes finding a feasible matching in P. Even under these conditions, a stable matching (a matching without blocking pairs) may not exist, so we look for a matching that minimizes the number of blocking pairs. We find that the blocking pair minimization problem for this subproblem is inapproximable within $|A|^{1-\epsilon}$ for $|A|$ agents and provide an $|A|$-approximation algorithm to show this result is almost tight. We extend this result to show that the problem of minimizing the number of agents in blocking pairs is also inapproximable within $|A|^{1-\epsilon}$, and since there are only $|A|$ agents, this result is also almost tight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03680v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Garret Castro</dc:creator>
    </item>
    <item>
      <title>Ultra-Resilient Superimposed Codes: Near-Optimal Construction and Applications</title>
      <link>https://arxiv.org/abs/2506.13489</link>
      <description>arXiv:2506.13489v2 Announce Type: replace 
Abstract: A superimposed code is a collection of binary vectors (codewords) with the property that no vector is contained in the Boolean sum of any $k$ others, enabling unique identification of codewords within any group of $k$. Superimposed codes are foundational combinatorial tools with applications in areas ranging from distributed computing and data retrieval to fault-tolerant communication. However, classical superimposed codes rely on strict alignment assumptions, limiting their effectiveness in asynchronous and fault-prone environments, which are common in modern systems and applications.
  We introduce Ultra-Resilient Superimposed Codes (URSCs), a new class of codes that extends the classic superimposed framework by ensuring a stronger codewords' isolation property and resilience to two types of adversarial perturbations: arbitrary cyclic shifts and partial bitwise corruption (flips). Additionally, URSCs exhibit universality, adapting seamlessly to any number $k$ of concurrent codewords without prior knowledge. This is a combination of properties not achieved in any previous construction.
  We provide the first polynomial-time construction of URSCs with near-optimal length, significantly outperforming previous constructions with less general features, all without requiring prior knowledge of the number of concurrent codewords, $k$. % We demonstrate that our URSCs significantly advance the state of the art in multiple applications, including uncoordinated beeping networks, where our codes reduce time complexity for local broadcast by nearly two orders of magnitude, and generalized contention resolution in multi-access channel communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13489v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gianluca De Marco, Dariusz R. Kowalski</dc:creator>
    </item>
    <item>
      <title>LZSE: an LZ-style compressor supporting $O(\log n)$-time random access</title>
      <link>https://arxiv.org/abs/2506.20107</link>
      <description>arXiv:2506.20107v2 Announce Type: replace 
Abstract: An LZ-like factorization of a string is a factorization in which each factor is either a single character or a copy of a substring that occurs earlier in the string. While grammar-based compression schemes support efficient random access with linear space in the size of the compressed representation, such methods are not known for general LZ-like factorizations. This has led to the development of restricted LZ-like schemes such as LZ-End [Kreft and Navarro, 2013] and height-bounded (LZHB) [Bannai et al., 2024], which trade off some compression efficiency for faster access. We introduce LZ-Start-End (LZSE), a new variant of LZ-like factorizations in which each copy factor refers to a contiguous sequence of preceding factors. By its nature, any context-free grammar can easily be converted into an LZSE factorization of equal size. Further, we study the greedy LZSE factorization, in which each copy factor is taken as long as possible. We show how the greedy LZSE factorization can be computed in linear time with respect to the input string length, and that there exists a family of strings for which the size of the greedy LZSE factorization is of strictly lower order than that of the smallest grammar. These imply that our LZSE scheme is stronger than grammar-based compressions in the context of repetitiveness measures. To support fast queries, we propose a data structure for LZSE-compressed strings that permits $O(\log n)$-time random access within space linear in the compressed size, where $n$ is the length of the input string.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20107v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiroki Shibata, Yuto Nakashima, Yutaro Yamaguchi, Shunsuke Inenaga</dc:creator>
    </item>
    <item>
      <title>High-dimensional Linear Bandits with Knapsacks</title>
      <link>https://arxiv.org/abs/2311.01327</link>
      <description>arXiv:2311.01327v2 Announce Type: replace-cross 
Abstract: We investigate the contextual bandits with knapsack (CBwK) problem in a high-dimensional linear setting, where the feature dimension can be very large. Our goal is to harness sparsity to obtain sharper regret guarantees. To this end, we first develop an online variant of the hard thresholding algorithm that performs the sparse estimation in an online manner. We then embed this estimator in a primal-dual scheme: every knapsack constraint is paired with a dual variable, which is updated by an online learning rule to keep the cumulative resource consumption within budget. This integrated approach achieves a two-phase sub-linear regret that scales only logarithmically with the feature dimension, improving on the polynomial dependency reported in prior work. Furthermore, we show that either of the following structural assumptions is sufficient for a sharper regret bound of $\tilde{O}(s_{0} \sqrt{T})$: (i) a diverse-covariate condition; and (ii) a margin condition. When both conditions hold simultaneously, we can further control the regret to $O(s_{0}^{2} \log(dT)\log T)$ by a dual resolving scheme. As a by-product, applying our framework to high-dimensional contextual bandits without knapsack constraints recovers the optimal regret rates in both the data-poor and data-rich regimes. Finally, numerical experiments confirm the empirical efficiency of our algorithms in high-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01327v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wanteng Ma, Dong Xia, Jiashuo Jiang</dc:creator>
    </item>
    <item>
      <title>When are quarnets sufficient to reconstruct semi-directed phylogenetic networks?</title>
      <link>https://arxiv.org/abs/2408.12997</link>
      <description>arXiv:2408.12997v3 Announce Type: replace-cross 
Abstract: Phylogenetic networks are graphs that are used to represent evolutionary relationships between different taxa. They generalize phylogenetic trees since for example, unlike trees, they permit lineages to combine. Recently, there has been rising interest in semi-directed phylogenetic networks, which are mixed graphs in which certain lineage combination events are represented by directed edges coming together, whereas the remaining edges are left undirected. One reason to consider such networks is that it can be difficult to root a network using real data. In this paper, we consider the problem of when a semi-directed phylogenetic network is defined or encoded by the smaller networks that it induces on the 4-leaf subsets of its leaf set. These smaller networks are called quarnets. We prove that semi-directed binary level-2 phylogenetic networks are encoded by their quarnets, but that this is not the case for level-3. In addition, we prove that the so-called blob tree of a semi-directed binary network, a tree that give the coarse-grained structure of the network, is always encoded by the quarnets of the network. These results are relevant for proving the statistical consistency of programs that are currently being developed for reconstructing phylogenetic networks from practical data, such as the recently developed Squirrel software tool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12997v3</guid>
      <category>q-bio.PE</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Katharina T. Huber, Leo van Iersel, Mark Jones, Vincent Moulton, Leonie Veenema - Nipius</dc:creator>
    </item>
    <item>
      <title>Certificate-Sensitive Subset Sum: Realizing Instance Complexity</title>
      <link>https://arxiv.org/abs/2507.15511</link>
      <description>arXiv:2507.15511v4 Announce Type: replace-cross 
Abstract: We present, to our knowledge, the first deterministic, certificate-sensitive algorithm for a canonical NP-complete problem whose runtime provably adapts to the structure of each input. For a Subset-Sum instance $(S, t)$, let $\Sigma(S)$ denote the set of distinct subset sums and define $U = |\Sigma(S)|$. This set serves as an information-theoretically minimal witness, the instance-complexity (IC) certificate.
  Our solver, IC-SubsetSum, enumerates every element of $\Sigma(S)$ in deterministic time $O(U \cdot n^2)$ and space $O(U \cdot n)$. A randomized variant achieves expected runtime $O(U \cdot n)$. The algorithm's complexity is thus directly governed by the certificate size, and this structure-sensitive performance is paired with a guaranteed worst-case runtime of $O^*(2^{n/2 - \varepsilon})$ for some constant $\varepsilon &gt; 0$, the first such result to strictly outperform classical methods on every instance.
  We revisit fine-grained reductions that rely on the classical $2^{n/2}$ hardness of SubsetSum and show that these arguments hold only for collision-free instances where $U$ is maximal. IC-SubsetSum reframes this barrier structurally and introduces a new paradigm for certificate-sensitive algorithms across NP-complete problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15511v4</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesus Salas</dc:creator>
    </item>
  </channel>
</rss>
