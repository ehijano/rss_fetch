<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 30 Jun 2025 04:00:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Simultaneously Fair Allocation of Indivisible Items Across Multiple Dimensions</title>
      <link>https://arxiv.org/abs/2506.21727</link>
      <description>arXiv:2506.21727v1 Announce Type: new 
Abstract: This paper explores the fair allocation of indivisible items in a multidimensional setting, motivated by the need to address fairness in complex environments where agents assess bundles according to multiple criteria. Such multidimensional settings are not merely of theoretical interest but are central to many real-world applications. For example, cloud computing resources are evaluated based on multiple criteria such as CPU cores, memory, and network bandwidth. In such cases, traditional one dimensional fairness notions fail to capture fairness across multiple attributes. To address these challenges, we study two relaxed variants of envy-freeness: weak simultaneously envy-free up to c goods (weak sEFc) and strong simultaneously envy-free up to c goods (strong sEFc), which accommodate the multidimensionality of agents' preferences. Under the weak notion, for every pair of agents and for each dimension, any perceived envy can be eliminated by removing, if necessary, a different set of goods from the envied agent's allocation. In contrast, the strong version requires selecting a single set of goods whose removal from the envied bundle simultaneously eliminates envy in every dimension. We provide upper and lower bounds on the relaxation parameter c that guarantee the existence of weak or strong sEFc allocations, where these bounds are independent of the total number of items. In addition, we present algorithms for checking whether a weak or strong sEFc allocation exists. Moreover, we establish NP-hardness results for checking the existence of weak sEF1 and strong sEF1 allocations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21727v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yasushi Kawase, Bodhayan Roy, Mohammad Azharuddin Sanpui</dc:creator>
    </item>
    <item>
      <title>Pseudo-Equilibria, or: How to Stop Worrying About Crypto and Just Analyze the Game</title>
      <link>https://arxiv.org/abs/2506.22089</link>
      <description>arXiv:2506.22089v1 Announce Type: new 
Abstract: We consider the problem of a game theorist analyzing a game that uses cryptographic protocols. Ideally, a theorist abstracts protocols as ideal, implementation-independent primitives, letting conclusions in the "ideal world" carry over to the "real world." This is crucial, since the game theorist cannot--and should not be expected to--handle full cryptographic complexity. In today's landscape, the rise of distributed ledgers makes a shared language between cryptography and game theory increasingly necessary.
  The security of cryptographic protocols hinges on two types of assumptions: state-of-the-world (e.g., "factoring is hard") and behavioral (e.g., "honest majority"). We observe that for protocols relying on behavioral assumptions (e.g., ledgers), our goal is unattainable in full generality. For state-of-the-world assumptions, we show that standard solution concepts, e.g., ($\epsilon$-)Nash equilibria, are not robust to transfer from the ideal to the real world.
  We propose a new solution concept: the pseudo-Nash equilibrium. Informally, a profile $s=(s_1,\dots,s_n)$ is a pseudo-Nash equilibrium if, for any player $i$ and deviation $s'_i$ with higher expected utility, $i$'s utility from $s_i$ is (computationally) indistinguishable from that of $s'_i$. Pseudo-Nash is simpler and more accessible to game theorists than prior notions addressing the mismatch between (asymptotic) cryptography and game theory. We prove that Nash equilibria in games with ideal, unbreakable cryptography correspond to pseudo-Nash equilibria when ideal cryptography is instantiated with real protocols (under state-of-the-world assumptions). Our translation is conceptually simpler and more general: it avoids tuning or restricting utility functions in the ideal game to fit quirks of cryptographic implementations. Thus, pseudo-Nash lets us study game-theoretic and cryptographic aspects separately and seamlessly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22089v1</guid>
      <category>cs.GT</category>
      <category>cs.CR</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alexandros Psomas, Athina Terzoglou, Yu Wei, Vassilis Zikas</dc:creator>
    </item>
    <item>
      <title>A few good choices</title>
      <link>https://arxiv.org/abs/2506.22133</link>
      <description>arXiv:2506.22133v1 Announce Type: new 
Abstract: A Condorcet winning set addresses the Condorcet paradox by selecting a few candidates--rather than a single winner--such that no unselected alternative is preferred to all of them by a majority of voters. This idea extends to $\alpha$-undominated sets, which ensure the same property for any $\alpha$-fraction of voters and are guaranteed to exist in constant size for any $\alpha$. However, the requirement that an outsider be preferred to every member of the set can be overly restrictive and difficult to justify in many applications. Motivated by this, we introduce a more flexible notion: $(t, \alpha)$-undominated sets. Here, each voter compares an outsider to their $t$-th most preferred member of the set, and the set is undominated if no outsider is preferred by more than an $\alpha$-fraction of voters. This framework subsumes prior definitions, recovering Condorcet winning sets when $(t = 1, \alpha = 1/2)$ and $\alpha$-undominated sets when $t = 1$, and introduces a new, tunable notion of collective acceptability for $t &gt; 1$. We establish three main results:
  1. We prove that a $(t, \alpha)$-undominated set of size $O(t/\alpha)$ exists for all values of $t$ and $\alpha$.
  2. We show that as $t$ becomes large, the minimum size of such a set approaches $t/\alpha$, which is asymptotically optimal.
  3. In the special case $t = 1$, we improve the bound on the size of an $\alpha$-undominated set given by Charikar, Lassota, Ramakrishnan, Vetta, and Wang (STOC 2025). As a consequence, we show that a Condorcet winning set of five candidates exists, improving their bound of six.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22133v1</guid>
      <category>cs.GT</category>
      <category>math.CO</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thanh Nguyen, Haoyu Song, Young-San Lin</dc:creator>
    </item>
    <item>
      <title>CyGym: A Simulation-Based Game-Theoretic Analysis Framework for Cybersecurity</title>
      <link>https://arxiv.org/abs/2506.21688</link>
      <description>arXiv:2506.21688v1 Announce Type: cross 
Abstract: We introduce a novel cybersecurity encounter simulator between a network defender and an attacker designed to facilitate game-theoretic modeling and analysis while maintaining many significant features of real cyber defense. Our simulator, built within the OpenAI Gym framework, incorporates realistic network topologies, vulnerabilities, exploits (including-zero-days), and defensive mechanisms. Additionally, we provide a formal simulation-based game-theoretic model of cyberdefense using this simulator, which features a novel approach to modeling zero-days exploits, and a PSRO-style approach for approximately computing equilibria in this game. We use our simulator and associated game-theoretic framework to analyze the Volt Typhoon advanced persistent threat (APT). Volt Typhoon represents a sophisticated cyber attack strategy employed by state-sponsored actors, characterized by stealthy, prolonged infiltration and exploitation of network vulnerabilities. Our experimental results demonstrate the efficacy of game-theoretic strategies in understanding network resilience against APTs and zero-days, such as Volt Typhoon, providing valuable insight into optimal defensive posture and proactive threat mitigation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21688v1</guid>
      <category>cs.CR</category>
      <category>cs.GT</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Lanier, Yevgeniy Vorobeychik</dc:creator>
    </item>
    <item>
      <title>OpenAlpha: A Community-Led Adversarial Strategy Validation Mechanism for Decentralised Capital Management</title>
      <link>https://arxiv.org/abs/2506.21809</link>
      <description>arXiv:2506.21809v1 Announce Type: cross 
Abstract: We propose \textit{OpenAlpha}, a community-led strategy validation framework for decentralised capital management on a host blockchain network, which integrates game-theoretic validation, adversarial auditing, and market-based belief aggregation. This work formulates treasury deployment as a capital optimisation problem under verification costs and strategic misreporting, and operationalises it through a decision waterfall that sequences intention declaration, strategy proposal, prediction-market validation, dispute resolution, and capital allocation. Each phase of this framework's validation process embeds economic incentives to align proposer, verifier, and auditor behaviour, producing confidence scores that may feed into a capital allocation rule. While OpenAlpha is designed for capital strategy assessment, its validation mechanisms are composable and extend naturally to evaluating external decentralised applications (DApps), enabling on-chain scrutiny of DApp performance, reliability, and integration risk. This architecture allows for adaptive, trust-minimised capital deployment without reliance on centralised governance or static audits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21809v1</guid>
      <category>q-fin.GN</category>
      <category>cs.GT</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Arman Abgaryan, Utkarsh Sharma</dc:creator>
    </item>
    <item>
      <title>A Principled Approach to Randomized Selection under Uncertainty: Applications to Peer Review and Grant Funding</title>
      <link>https://arxiv.org/abs/2506.19083</link>
      <description>arXiv:2506.19083v2 Announce Type: replace 
Abstract: Many decision-making processes involve evaluating and then selecting items; examples include scientific peer review, job hiring, school admissions, and investment decisions. The eventual selection is performed by applying rules or deliberations to the raw evaluations, and then deterministically selecting the items deemed to be the best. These domains feature error-prone evaluations and uncertainty about future outcomes, which undermine the reliability of such deterministic selection rules. As a result, selection mechanisms involving explicit randomization that incorporate the uncertainty are gaining traction in practice. However, current randomization approaches are ad hoc, and as we prove, inappropriate for their purported objectives. In this paper, we propose a principled framework for randomized decision-making based on interval estimates of the quality of each item. We introduce MERIT (Maximin Efficient Randomized Interval Top-k), an optimization-based method that maximizes the worst-case expected number of top candidates selected, under uncertainty represented by overlapping intervals (e.g., confidence intervals or min-max intervals). MERIT provides an optimal resource allocation scheme under an interpretable notion of robustness. We develop a polynomial-time algorithm to solve the optimization problem and demonstrate empirically that the method scales to over 10,000 items. We prove that MERIT satisfies desirable axiomatic properties not guaranteed by existing approaches. Finally, we empirically compare algorithms on synthetic peer review data. Our experiments demonstrate that MERIT matches the performance of existing algorithms in expected utility under fully probabilistic review data models used in previous work, while outperforming previous methods with respect to our novel worst-case formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19083v2</guid>
      <category>cs.GT</category>
      <category>cs.CY</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Goldberg, Giulia Fanti, Nihar B. Shah</dc:creator>
    </item>
  </channel>
</rss>
