<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 Jan 2025 02:32:58 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Mirror Descent Methods with Weighting Scheme for Outputs for Constrained Variational Inequality Problems</title>
      <link>https://arxiv.org/abs/2501.04034</link>
      <description>arXiv:2501.04034v1 Announce Type: new 
Abstract: This paper is devoted to the variational inequality problems. We consider two classes of problems, the first is classical constrained variational inequality and the second is the same problem with functional (inequality type) constraints. To solve these problems, we propose mirror descent-type methods with a weighting scheme for the generated points in each iteration of the algorithms. This scheme assigns smaller weights to the initial points and larger weights to the most recent points, thus it improves the convergence rate of the proposed methods. For the variational inequality problem with functional constraints, the proposed method switches between adaptive and non-adaptive steps in the dependence on the values of the functional constraints at iterations. We analyze the proposed methods for the time-varying step sizes and prove the optimal convergence rate for variational inequality problems with bounded and monotone operators. The results of numerical experiments of the proposed methods for classical constrained variational inequality problems show a significant improvement over the modified projection method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04034v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad S. Alkousa, Belal A. Alashqar, Fedor S. Stonyakin, Tarek Nabhani, Seydamet S. Ablaev</dc:creator>
    </item>
    <item>
      <title>Efficient LP warmstarting for linear modifications of the constraint matrix</title>
      <link>https://arxiv.org/abs/2501.04151</link>
      <description>arXiv:2501.04151v1 Announce Type: new 
Abstract: We consider the problem of computing the optimal solution and objective of a linear program under linearly changing linear constraints. More specifically, we want to compute the optimal solution of a linear optimization where the constraint matrix linearly depends on a paramater that can take p different values. Based on the information given by a precomputed basis, we present three efficient LP warm-starting algorithms. Each algorithm is either based on the eigenvalue decomposition, the Schur decomposition, or a tweaked eigenvalue decomposition to evaluate the optimal solution and optimal objective of these problems. The three algorithms have an overall complexity O(m^3 + pm^2) where m is the number of constraints of the original problem and p the number of values of the parameter that we want to evaluate. We also provide theorems related to the optimality conditions to verify when a basis is still optimal and a local bound on the objective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04151v1</guid>
      <category>math.OC</category>
      <category>cs.CC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Guillaume Derval, Bardhyl Miftari, Damien Ernst, Quentin Louveaux</dc:creator>
    </item>
    <item>
      <title>Unifying restart accelerated gradient and proximal bundle methods</title>
      <link>https://arxiv.org/abs/2501.04165</link>
      <description>arXiv:2501.04165v1 Announce Type: new 
Abstract: This paper presents a novel restarted version of Nesterov's accelerated gradient method and establishes its optimal iteration-complexity for solving convex smooth composite optimization problems. The proposed restart accelerated gradient method is shown to be a specific instance of the accelerated inexact proximal point framework introduced in "An accelerated hybrid proximal extragradient method for convex optimization and its implications to second-order methods" by Monteiro and Svaiter, SIAM Journal on Optimization, 2013. Furthermore, this work examines the proximal bundle method within the inexact proximal point framework, demonstrating that it is an instance of the framework. Notably, this paper provides new insights into the underlying algorithmic principle that unifies two seemingly disparate optimization methods, namely, the restart accelerated gradient and the proximal bundle methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04165v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaming Liang</dc:creator>
    </item>
    <item>
      <title>A black-box optimization method with polynomial-based kernels and quadratic-optimization annealing</title>
      <link>https://arxiv.org/abs/2501.04225</link>
      <description>arXiv:2501.04225v1 Announce Type: new 
Abstract: We introduce kernel-QA, a black-box optimization (BBO) method that constructs surrogate models analytically using low-order polynomial kernels within a quadratic unconstrained binary optimization (QUBO) framework, enabling efficient utilization of Ising machines. The method has been evaluated on artificial landscapes, ranging from uni-modal to multi-modal, with input dimensions extending to 80 for real variables and 640 for binary variables. The results demonstrate that kernel-QA is particularly effective for optimizing black-box functions characterized by local minima and high-dimensional inputs, showcasing its potential as a robust and scalable BBO approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04225v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuki Minamoto, Yuya Sakamoto</dc:creator>
    </item>
    <item>
      <title>Integrated Offline and Online Learning to Solve a Large Class of Scheduling Problems</title>
      <link>https://arxiv.org/abs/2501.04253</link>
      <description>arXiv:2501.04253v1 Announce Type: new 
Abstract: In this paper, we develop a unified machine learning (ML) approach to predict high-quality solutions for single-machine scheduling problems with a non-decreasing min-sum objective function with or without release times. Our ML approach is novel in three major aspects. First, our approach is developed for the entire class of the aforementioned problems. To achieve this, we exploit the fact that the entire class of the problems considered can be formulated as a time-indexed formulation in a unified manner. We develop a deep neural network (DNN) which uses the cost parameters in the time-indexed formulation as the inputs to effectively predict a continuous solution to this formulation, based on which a feasible discrete solution is easily constructed. The second novel aspect of our approach lies in how the DNN model is trained. In view of the NP-hard nature of the problems, labels (i.e., optimal solutions) are hard to generate for training. To overcome this difficulty, we generate and utilize a set of special instances, for which optimal solutions can be found with little computational effort, to train the ML model offline. The third novel idea we employ in our approach is that we develop an online single-instance learning approach to fine tune the parameters in the DNN for a given online instance, with the goal of generating an improved solution for the given instance. To this end, we develop a feasibility surrogate that approximates the objective value of a given instance as a continuous function of the outputs of the DNN, which then enables us to derive gradients and update the learnable parameters in the DNN. Numerical results show that our approach can efficiently generate high-quality solutions for a variety of single-machine scheduling min-sum problems with up to 1000 jobs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04253v1</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anbang Liu, Zhi-Long Chen, Jinyang Jiang, Xi Chen</dc:creator>
    </item>
    <item>
      <title>A truncated {\epsilon}-subdifferential method for global DC optimization</title>
      <link>https://arxiv.org/abs/2501.04291</link>
      <description>arXiv:2501.04291v1 Announce Type: new 
Abstract: We consider the difference of convex (DC) optimization problem subject to box-constraints. Utilizing {\epsilon}-subdifferentials of DC components of the objective, we develop a new method for finding global solutions to this problem. The method combines a local search approach with a special procedure for escaping non-global solutions by identifying improved initial points for a local search. The method terminates when the solution cannot be improved further. The escaping procedure is designed using subsets of the {\epsilon}-subdifferentials of DC components. We compute the deviation between these subsets and determine {\epsilon}-subgradients providing this deviation. Using these specific {\epsilon}-subgradients, we formulate a subproblem with a convex objective function. The solution to this subproblem serves as a starting point for a local search. We study the convergence of the conceptual version of the proposed method and discuss its implementation. A large number of academic test problems demonstrate that the method requires reasonable computational effort to find higher quality solutions than other local DC optimization methods. Additionally, we apply the new method to find global solutions to DC optimization problems and compare its performance with two benchmark global optimization solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04291v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Adil M. Bagirov, Kaisa Joki, Marko M. Makela, Sona Taheri</dc:creator>
    </item>
    <item>
      <title>State-dependent preconditioning for the inner-loop in Variational Data Assimilation using Machine Learning</title>
      <link>https://arxiv.org/abs/2501.04369</link>
      <description>arXiv:2501.04369v1 Announce Type: new 
Abstract: Data Assimilation is the process in which we improve the representation of the state of a physical system by combining information coming from a numerical model, real-world observations, and some prior modelling. It is widely used to model and to improve forecast systems in Earth science fields such as meteorology, oceanography and environmental sciences. One key aspect of Data assimilation is the analysis step, where the output of the numerical model is adjusted in order to account for the observational data. In Variational Data Assimilation and under Gaussian assumptions, the analysis step comes down to solving a high-dimensional non-linear least-square problem. In practice, this minimization involves successive inversions of large, and possibly ill-conditioned matrices constructed using linearizations of the forward model. In order to improve the convergence rate of these methods, and thus reduce the computational burden, preconditioning techniques are often used to get better-conditioned matrices, but require either the sparsity pattern of the matrix to inverse, or some spectral information. We propose to use Deep Neural Networks in order to construct a preconditioner. This surrogate is trained using some properties of the singular value decomposition, and is based on a dataset which can be constructed online to reduce the storage requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04369v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Trappler (AIRSEA, ICJ, PSPM), Arthur Vidard (AIRSEA)</dc:creator>
    </item>
    <item>
      <title>Revisiting LocalSGD and SCAFFOLD: Improved Rates and Missing Analysis</title>
      <link>https://arxiv.org/abs/2501.04443</link>
      <description>arXiv:2501.04443v1 Announce Type: new 
Abstract: LocalSGD and SCAFFOLD are widely used methods in distributed stochastic optimization, with numerous applications in machine learning, large-scale data processing, and federated learning. However, rigorously establishing their theoretical advantages over simpler methods, such as minibatch SGD (MbSGD), has proven challenging, as existing analyses often rely on strong assumptions, unrealistic premises, or overly restrictive scenarios.
  In this work, we revisit the convergence properties of LocalSGD and SCAFFOLD under a variety of existing or weaker conditions, including gradient similarity, Hessian similarity, weak convexity, and Lipschitz continuity of the Hessian. Our analysis shows that (i) LocalSGD achieves faster convergence compared to MbSGD for weakly convex functions without requiring stronger gradient similarity assumptions; (ii) LocalSGD benefits significantly from higher-order similarity and smoothness; and (iii) SCAFFOLD demonstrates faster convergence than MbSGD for a broader class of non-quadratic functions. These theoretical insights provide a clearer understanding of the conditions under which LocalSGD and SCAFFOLD outperform MbSGD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04443v1</guid>
      <category>math.OC</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ruichen Luo, Sebastian U Stich, Samuel Horv\'ath, Martin Tak\'a\v{c}</dc:creator>
    </item>
    <item>
      <title>A fast iterative thresholding and support-and-scale shrinking algorithm (fits3) for non-lipschitz group sparse optimization (i): the case of least-squares fidelity</title>
      <link>https://arxiv.org/abs/2501.04491</link>
      <description>arXiv:2501.04491v1 Announce Type: new 
Abstract: We consider to design a new efficient and easy-to-implement algorithm to solve a general group sparse optimization model with a class of non-convex non-Lipschitz regularizations, named as fast iterative thresholding and support-and-scale shrinking algorithm (FITS3). In this paper we focus on the case of a least-squares fidelity. FITS3 is designed from a lower bound theory of such models and by integrating thresholding operation, linearization and extrapolation techniques. The FITS3 has two advantages. Firstly, it is quite efficient and especially suitable for large-scale problems, because it adopts support-and-scale shrinking and does not need to solve any linear or nonlinear system. For two important special cases, the FITS3 contains only simple calculations like matrix-vector multiplication and soft thresholding. Secondly, the FITS3 algorithm has a sequence convergence guarantee under proper assumptions. The numerical experiments and comparisons to recent existing non-Lipschitz group recovery algorithms demonstrate that, the proposed FITS3 achieves similar recovery accuracies, but costs only around a half of the CPU time by the second fastest compared algorithm for median or large-scale problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04491v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanan Zhao, Qiaoli Dong, Yufei Zhao, Chunlin Wu</dc:creator>
    </item>
    <item>
      <title>Scalable Derivative-Free Optimization Algorithms with Low-Dimensional Subspace Techniques</title>
      <link>https://arxiv.org/abs/2501.04536</link>
      <description>arXiv:2501.04536v1 Announce Type: new 
Abstract: We re-introduce a derivative-free subspace optimization framework originating from Chapter 5 of the Ph.D. thesis [Z. Zhang, On Derivative-Free Optimization Methods, Ph.D. thesis, Chinese Academy of Sciences, Beijing, 2012] of the author under the supervision of Ya-xiang Yuan. At each iteration, the framework defines a (low-dimensional) subspace based on an approximate gradient, and then solves a subproblem in this subspace to generate a new iterate. We sketch the global convergence and worst-case complexity analysis of the framework, elaborate on its implementation, and present some numerical results on solving problems with dimensions as high as 10^4 using only inaccurate function values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04536v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zaikun Zhang</dc:creator>
    </item>
    <item>
      <title>Optimal Control of the Navier-Stokes equations via Pressure Boundary Conditions</title>
      <link>https://arxiv.org/abs/2501.04548</link>
      <description>arXiv:2501.04548v1 Announce Type: new 
Abstract: In this work we study an optimal control problem subject to the instationary Navier-Stokes equations, where the control enters via an inhomogeneous Neumann/Do-Nothing boundary condition. Despite the Navier-Stokes equations with these boundary conditions not being well-posed for large times and/or data, we obtain wellposedness of the optimal control problem by choosing a proper tracking type term. In order to discuss the regularity of the optimal control, state and adjoint state, we present new results on $L^2(I;H^2(\Omega))$ regularity of solutions to a Stokes problem with mixed inhomogeneous boundary conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04548v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Boris Vexler, Jakob Wagner</dc:creator>
    </item>
    <item>
      <title>On Branch-and-Price for Project Scheduling</title>
      <link>https://arxiv.org/abs/2501.04563</link>
      <description>arXiv:2501.04563v1 Announce Type: new 
Abstract: Integer programs for resource-constrained project scheduling problems are notoriously hard to solve due to their weak linear relaxations. Several papers have proposed reformulating project scheduling problems via Dantzig-Wolfe decomposition to strengthen their linear relaxation and decompose large problem instances. The reformulation gives rise to a master problem that has a large number of variables. Therefore, the master problem is solved by a column generation procedure embedded in a branching framework, also known as branch-and-price. While branch-and-price has been successfully applied to many problem classes, it turns out to be ineffective for most project scheduling problems. This paper identifies drivers of the ineffectiveness by analyzing the structure of the reformulated problem and the strength of different branching schemes. Our analysis shows that the reformulated problem has an unfavorable structure for column generation: It is highly degenerate, slowing down the convergence of column generation, and for many project scheduling problems, it yields the same or only slightly stronger linear relaxations as classical formulations at the expense of large increases in runtime. Our computational experiments complement our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04563v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maximilian Kolter, Martin Grunow, Rainer Kolisch</dc:creator>
    </item>
    <item>
      <title>Accelerated Extragradient-Type Methods -- Part 2: Generalization and Sublinear Convergence Rates under Co-Hypomonotonicity</title>
      <link>https://arxiv.org/abs/2501.04585</link>
      <description>arXiv:2501.04585v1 Announce Type: new 
Abstract: Following the first part of our project, this paper comprehensively studies two types of extragradient-based methods: anchored extragradient and Nesterov's accelerated extragradient for solving [non]linear inclusions (and, in particular, equations), primarily under the Lipschitz continuity and the co-hypomonotonicity assumptions. We unify and generalize a class of anchored extragradient methods for monotone inclusions to a wider range of schemes encompassing existing algorithms as special cases. We establish $\mathcal{O}(1/k)$ last-iterate convergence rates on the residual norm of the underlying mapping for this general framework and then specialize it to obtain convergence guarantees for specific instances, where $k$ denotes the iteration counter. We extend our approach to a class of anchored Tseng's forward-backward-forward splitting methods to obtain a broader class of algorithms for solving co-hypomonotone inclusions. Again, we analyze $\mathcal{O}(1/k)$ last-iterate convergence rates for this general scheme and specialize it to obtain convergence results for existing and new variants. We generalize and unify Nesterov's accelerated extra-gradient method to a new class of algorithms that covers existing schemes as special instances while generating new variants. For these schemes, we can prove $\mathcal{O}(1/k)$ last-iterate convergence rates for the residual norm under co-hypomonotonicity, covering a class of nonmonotone problems. We propose another novel class of Nesterov's accelerated extragradient methods to solve inclusions. Interestingly, these algorithms achieve both $\mathcal{O}(1/k)$ and $o(1/k)$ last-iterate convergence rates, and also the convergence of iterate sequences under co-hypomonotonicity and Lipschitz continuity. Finally, we provide a set of numerical experiments encompassing different scenarios to validate our algorithms and theoretical guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04585v1</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Quoc Tran-Dinh, Nghia Nguyen-Trung</dc:creator>
    </item>
    <item>
      <title>Infinite Horizon Fully Coupled Nonlinear Forward-Backward Stochastic Difference Equations and their Application to LQ Optimal Control Problems</title>
      <link>https://arxiv.org/abs/2501.04603</link>
      <description>arXiv:2501.04603v1 Announce Type: new 
Abstract: This paper focuses on the study of infinite horizon fully coupled nonlinear forward-backward stochastic difference equations (FBS$\bigtriangleup$Es). Firstly, we establish a pair of priori estimates for the solutions to forward stochastic difference equations (FS$\bigtriangleup$Es) and backward stochastic difference equations (BS$\bigtriangleup$Es) respectively. Then, to achieve broader applicability, we utilize a set of domination-monotonicity conditions which are more lenient than general ones. Using these conditions, we apply continuation methods to prove the unique solvability of infinite horizon fully coupled FBS$\bigtriangleup$Es and derive a set of solution estimates. Furthermore, our results have considerable implications for a variety of related linear quadratic (LQ) problems, especially when the stochastic Hamiltonian system is consistent with FBS$\bigtriangleup$Es satisfying these introduced domination-monotonicity conditions. Thus, by solving the associated stochastic Hamiltonian system, we can derive an explicit expression for the unique optimal control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04603v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyu Ma, Xun Li, Qingxin Meng</dc:creator>
    </item>
    <item>
      <title>Characterizations of Variational Convexity and Tilt Stability via Quadratic Bundles</title>
      <link>https://arxiv.org/abs/2501.04629</link>
      <description>arXiv:2501.04629v1 Announce Type: new 
Abstract: In this paper, we establish characterizations of variational $s$-convexity and tilt stability for prox-regular functions in the absence of subdifferential continuity via quadratic bundles, a kind of primal-dual generalized second-order derivatives recently introduced by Rockafellar. Deriving such characterizations in the effective pointbased form requires a certain revision of quadratic bundles investigated below. Our device is based on the notion of generalized twice differentiability and its novel characterization via classical twice differentiability of the associated Moreau envelopes combined with various limiting procedures for functions and sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04629v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Pham Duy Khanh, Boris S. Mordukhovich, Vo Thanh Phat, Le Duc Viet</dc:creator>
    </item>
    <item>
      <title>Semilinear Dynamic Programming: Analysis, Algorithms, and Certainty Equivalence Properties</title>
      <link>https://arxiv.org/abs/2501.04668</link>
      <description>arXiv:2501.04668v1 Announce Type: new 
Abstract: We consider a broad class of dynamic programming (DP) problems that involve a partially linear structure and some positivity properties in their system equation and cost function. We address deterministic and stochastic problems, possibly with Markov jump parameters. We focus primarily on infinite horizon problems and prove that under our assumptions, the optimal cost function is linear, and that an optimal policy can be computed efficiently with standard DP algorithms. Moreover, we show that forms of certainty equivalence hold for our stochastic problems, in analogy with the classical linear quadratic optimal control problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04668v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchao Li, Dimitri Bertsekas</dc:creator>
    </item>
    <item>
      <title>DeepVIVONet: Using deep neural operators to optimize sensor locations with application to vortex-induced vibrations</title>
      <link>https://arxiv.org/abs/2501.04105</link>
      <description>arXiv:2501.04105v1 Announce Type: cross 
Abstract: We introduce DeepVIVONet, a new framework for optimal dynamic reconstruction and forecasting of the vortex-induced vibrations (VIV) of a marine riser, using field data. We demonstrate the effectiveness of DeepVIVONet in accurately reconstructing the motion of an off--shore marine riser by using sparse spatio-temporal measurements. We also show the generalization of our model in extrapolating to other flow conditions via transfer learning, underscoring its potential to streamline operational efficiency and enhance predictive accuracy. The trained DeepVIVONet serves as a fast and accurate surrogate model for the marine riser, which we use in an outer--loop optimization algorithm to obtain the optimal locations for placing the sensors. Furthermore, we employ an existing sensor placement method based on proper orthogonal decomposition (POD) to compare with our data-driven approach. We find that that while POD offers a good approach for initial sensor placement, DeepVIVONet's adaptive capabilities yield more precise and cost-effective configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04105v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>physics.flu-dyn</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruyin Wan, Ehsan Kharazmi, Michael S Triantafyllou, George Em Karniadakis</dc:creator>
    </item>
    <item>
      <title>Mixing Times and Privacy Analysis for the Projected Langevin Algorithm under a Modulus of Continuity</title>
      <link>https://arxiv.org/abs/2501.04134</link>
      <description>arXiv:2501.04134v1 Announce Type: cross 
Abstract: We study the mixing time of the projected Langevin algorithm (LA) and the privacy curve of noisy Stochastic Gradient Descent (SGD), beyond nonexpansive iterations. Specifically, we derive new mixing time bounds for the projected LA which are, in some important cases, dimension-free and poly-logarithmic on the accuracy, closely matching the existing results in the smooth convex case. Additionally, we establish new upper bounds for the privacy curve of the subsampled noisy SGD algorithm. These bounds show a crucial dependency on the regularity of gradients, and are useful for a wide range of convex losses beyond the smooth case. Our analysis relies on a suitable extension of the Privacy Amplification by Iteration (PABI) framework (Feldman et al., 2018; Altschuler and Talwar, 2022, 2023) to noisy iterations whose gradient map is not necessarily nonexpansive. This extension is achieved by designing an optimization problem which accounts for the best possible R\'enyi divergence bound obtained by an application of PABI, where the tractability of the problem is crucially related to the modulus of continuity of the associated gradient mapping. We show that, in several interesting cases -- including the nonsmooth convex, weakly smooth and (strongly) dissipative -- such optimization problem can be solved exactly and explicitly. This yields the tightest possible PABI-based bounds, where our results are either new or substantially sharper than those in previous works.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04134v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Bravo, Juan P. Flores-Mella, Crist\'obal Guzm\'an</dc:creator>
    </item>
    <item>
      <title>Linear Optimization for the Perfect Meal: A Data-Driven Approach to Optimising the Perfect Meal Using Gurobi</title>
      <link>https://arxiv.org/abs/2501.04143</link>
      <description>arXiv:2501.04143v1 Announce Type: cross 
Abstract: This study aims to optimize meal planning for nutritional health and cost efficiency using linear programming. Linear optimization provides an effective framework for addressing the problem of an optimal diet, as the composition of food can be naturally modeled as a linearly additive system. Leveraging a comprehensive nutrition dataset, our model minimizes meal costs while meeting specific nutritional requirements. We explore additional complexities, such as fractional weights and nutrient ratio constraints, enhancing the robustness of the solution. Case studies address common nutritional challenges, providing tailored diet plans. The significance lies in aiding individuals to form balanced, cost-effective dietary schedules, considering fitness goals and caloric needs. This research contributes to efficient, sustainable, and time-sensitive meal planning, emphasizing the intersection of nutrition, optimization, and real-world applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04143v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Utkarsh Prajapati, Tanushree Jain, Abhishek Machiraju, Divyam Kaushik</dc:creator>
    </item>
    <item>
      <title>Collaborative Spacecraft Servicing under Partial Feedback using Lyapunov-based Deep Neural Networks</title>
      <link>https://arxiv.org/abs/2501.04160</link>
      <description>arXiv:2501.04160v2 Announce Type: cross 
Abstract: Multi-agent systems are increasingly applied in space missions, including distributed space systems, resilient constellations, and autonomous rendezvous and docking operations. A critical emerging application is collaborative spacecraft servicing, which encompasses on-orbit maintenance, space debris removal, and swarm-based satellite repositioning. These missions involve servicing spacecraft interacting with malfunctioning or defunct spacecraft under challenging conditions, such as limited state information, measurement inaccuracies, and erratic target behaviors. Existing approaches often rely on assumptions of full state knowledge or single-integrator dynamics, which are impractical for real-world applications involving second-order spacecraft dynamics. This work addresses these challenges by developing a distributed state estimation and tracking framework that requires only relative position measurements and operates under partial state information. A novel $\rho$-filter is introduced to reconstruct unknown states using locally available information, and a Lyapunov-based deep neural network adaptive controller is developed that adaptively compensates for uncertainties stemming from unknown spacecraft dynamics. To ensure the collaborative spacecraft regulation problem is well-posed, a trackability condition is defined. A Lyapunov-based stability analysis is provided to ensure exponential convergence of errors in state estimation and spacecraft regulation to a neighborhood of the origin under the trackability condition. The developed method eliminates the need for expensive velocity sensors or extensive pre-training, offering a practical and robust solution for spacecraft servicing in complex, dynamic environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04160v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cristian F. Nino, Omkar Sudhir Patil, Christopher D. Petersen, Sean Phillips, Warren E. Dixon</dc:creator>
    </item>
    <item>
      <title>An algorithm for a constrained P-spline</title>
      <link>https://arxiv.org/abs/2501.04335</link>
      <description>arXiv:2501.04335v1 Announce Type: cross 
Abstract: Regression splines are largely used to investigate and predict data behavior, attracting the interest of mathematicians for their beautiful numerical properties, and of statisticians for their versatility with respect to the applications. Several penalized spline regression models are available in the literature, and the most commonly used ones in real-world applications are P-splines, which enjoy the advantages of penalized models while being easy to generalize across different functional spaces and higher degree order, because of their discrete penalty term. To face the different requirements imposed by the nature of the problem or the physical meaning of the expected values, the P-spline definition is often modified by additional hypotheses, often translated into constraints on the solution or its derivatives. In this framework, our work is motivated by the aim of getting approximation models that fall within pre-established thresholds. Specifically, starting from a set of observed data, we consider a P-spline constrained between some prefixed bounds. In our paper, we just consider 0 as lower bound, although our approach applies to more general cases. We propose to get nonnegativity by imposing lower bounds on selected sample points. The spline can be computed through a sequence of linearly constrained problems. We suggest a strategy to dynamically select the sample points, to avoid extremely dense sampling, and therefore try to reduce as much as possible the computational burden. We show through some computational experiments the reliability of our approach and the accuracy of the results compared to some state-of-the-art models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04335v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rosanna Campagna, Serena Crisci, Gabriele Santin, Gerardo Toraldo, Marco Viola</dc:creator>
    </item>
    <item>
      <title>Regret Analysis: a control perspective</title>
      <link>https://arxiv.org/abs/2501.04572</link>
      <description>arXiv:2501.04572v2 Announce Type: cross 
Abstract: Online learning and model reference adaptive control have many interesting intersections. One area where they differ however is in how the algorithms are analyzed and what objective or metric is used to discriminate "good" algorithms from "bad" algorithms. In adaptive control there are usually two objectives: 1) prove that all time varying parameters/states of the system are bounded, and 2) that the instantaneous error between the adaptively controlled system and a reference system converges to zero over time (or at least a compact set). For online learning the performance of algorithms is often characterized by the regret the algorithm incurs. Regret is defined as the cumulative loss (cost) over time from the online algorithm minus the cumulative loss (cost) of the single optimal fixed parameter choice in hindsight. Another significant difference between the two areas of research is with regard to the assumptions made in order to obtain said results. Adaptive control makes assumptions about the input-output properties of the control problem and derives solutions for a fixed error model or optimization task. In the online learning literature results are derived for classes of loss functions (i.e. convex) while a priori assuming that all time varying parameters are bounded, which for many optimization tasks is not unrealistic, but is a non starter in control applications. In this work we discuss these differences in detail through the regret based analysis of gradient descent for convex functions and the control based analysis of a streaming regression problem. We close with a discussion about the newly defined paradigm of online adaptive control and ask the following question "Are regret optimal control strategies deployable?"</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04572v2</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Travis E. Gibson, Sawal Acharya</dc:creator>
    </item>
    <item>
      <title>Quadratic-form Optimal Transport</title>
      <link>https://arxiv.org/abs/2501.04658</link>
      <description>arXiv:2501.04658v1 Announce Type: cross 
Abstract: We introduce the framework of quadratic-form optimal transport (QOT), whose transport cost has the form $\iint c\,\mathrm{d}\pi \otimes\mathrm{d}\pi$ for some coupling $\pi$ between two marginals. Interesting examples of quadratic-form transport cost and their optimization include the variance of a bivariate function, covariance, Kendall's tau, the Gromov--Wasserstein distance, quadratic assignment problems, and quadratic regularization of classic optimal transport. QOT leads to substantially different mathematical structures compared to classic transport problems and many technical challenges. We illustrate the fundamental properties of QOT, provide several cases where explicit solutions are obtained, and give general lower bounds of the optimal transport costs. For a wide class of cost functions, including the rectangular cost functions, the QOT problem is solved by a new coupling called the diamond transport, whose copula is supported on a diamond in the unit square.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04658v1</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruodu Wang, Zhenyuan Zhang</dc:creator>
    </item>
    <item>
      <title>Optimal Stabilization of Periodic Orbits</title>
      <link>https://arxiv.org/abs/2211.11955</link>
      <description>arXiv:2211.11955v2 Announce Type: replace 
Abstract: In this contribution, the optimal stabilization problem of periodic orbits is studied via invariant manifold theory and symplectic geometry. The stable manifold theory for the optimal point stabilization case is generalized to the case of periodic orbit stabilization, where a normally hyperbolic invariant manifold (NHIM) plays the role of a hyperbolic equilibrium.
  A sufficient condition for the existence of an NHIM of an associated Hamiltonian system is derived in terms of a periodic Riccati differential equation. It is shown that the problem of optimal orbit stabilization has a solution if a linearized periodic system satisfies stabilizability and detectability. A moving orthogonal coordinate system is employed along the periodic orbit which is a natural framework for orbital stabilization and linearization argument.
  Examples illustrated include an optimal control problem for a spring-mass oscillator system, which should be stabilized at a certain energy level, and an orbit transfer problem for a satellite, which constitutes a typical control problem of orbital mechanics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.11955v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabian Beck, Noboru Sakamoto</dc:creator>
    </item>
    <item>
      <title>Output-Positive Adaptive Control of Hyperbolic PDE-ODE Cascades</title>
      <link>https://arxiv.org/abs/2309.05596</link>
      <description>arXiv:2309.05596v3 Announce Type: replace 
Abstract: In this paper, we propose a new adaptive Control Barrier Function (aCBF) method to design the output-positive adaptive control law for a hyperbolic PDE-ODE cascade with parametric uncertainties. This method employs the recent adaptive control approach with batch least-squares identification (BaLSI, pronounced "ballsy") that completes perfect parameter identification in finite time and offers a previously unforeseen advantage in safe control design with aCBF, which we elucidate in this paper. Since the true challenge is exhibited for CBF of a high relative degree, we undertake a control design in this paper for a class of systems that possess a particularly extreme relative degree: $2\times2$ hyperbolic PDEs sandwiched by a strict-feedback nonlinear ODE and a linear ODE, where the unknown coefficients are associated with the PDE in-domain coupling terms and with the input signal of the distal ODE. The designed output-positive adaptive controller guarantees the positivity of the output signal that is the furthermost state from the control input as well as the exponential regulation of the overall plant state to zero. The effectiveness of the proposed method is illustrated by numerical simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.05596v3</guid>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ji Wang, Miroslav Krstic</dc:creator>
    </item>
    <item>
      <title>Boosting Column Generation with Graph Neural Networks for Joint Rider Trip Planning and Crew Shift Scheduling</title>
      <link>https://arxiv.org/abs/2401.03692</link>
      <description>arXiv:2401.03692v3 Announce Type: replace 
Abstract: Optimizing service schedules is pivotal to the reliable, efficient, and inclusive on-demand mobility. This pressing challenge is further exacerbated by the increasing needs of an aging population, the oversubscription of existing services, and the lack of effective solution methods. This study addresses the intricacies of service scheduling, by jointly optimizing rider trip planning and crew scheduling for a complex dynamic mobility service. The resulting optimization problems are extremely challenging computationally for state-of-the-art methods. To address this fundamental gap, this paper introduces the Joint Rider Trip Planning and Crew Shift Scheduling Problem (JRTPCSSP) and a novel solution method, called Attention and Gated GNN-Informed Column Generation (AGGNNI-CG), that hybridizes column generation and machine learning to obtain near-optimal solutions to the JRTPCSSP with real-life constraints of the application. The key idea of the machine-learning component is to dramatically reduce the number of paths to explore in the pricing problem, accelerating the most time-consuming component of the column generation. The machine learning component is a graph neural network with an attention mechanism and a gated architecture, which is particularly suited to cater for the different input sizes coming from daily operations. AGGNNI-CG has been applied to a challenging, real-world dataset from the Paratransit system of Chatham County in Georgia. It produces substantial improvements compared to the baseline column generation approach, which typically cannot produce high-quality feasible solutions in reasonable time on large-scale complex instances. AGGNNI-CG also produces significant improvements in service quality compared to the existing system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03692v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawei Lu, Tinghan Ye, Wenbo Chen, Pascal Van Hentenryck</dc:creator>
    </item>
    <item>
      <title>A Unified Algorithmic Framework for Dynamic Assortment Optimization under MNL Choice</title>
      <link>https://arxiv.org/abs/2404.03604</link>
      <description>arXiv:2404.03604v3 Announce Type: replace 
Abstract: We consider assortment and inventory planning problems with dynamic stockout-based substitution effects, and without replenishment, in two different settings: (1) Customers can see all available products when they arrive, a typical scenario in physical stores. (2) The seller can choose to offer a subset of available products to each customer, which is more common on online platforms. Both settings are known to be computationally challenging, and the current approximation algorithms for the two settings are quite different. We develop a unified algorithm framework under the MNL choice model for both settings. Our algorithms improve on the state-of-the-art algorithms in terms of approximation guarantee and runtime, and the ability to manage uncertainty in the total number of customers and handle more complex constraints. In the process, we establish various novel properties of dynamic assortment planning (for the MNL choice model) that may be useful more broadly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03604v3</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuo Sun, Rajan Udwani, Zuo-Jun Max Shen</dc:creator>
    </item>
    <item>
      <title>Hardness of circuit and monotone diameters of polytopes</title>
      <link>https://arxiv.org/abs/2404.04158</link>
      <description>arXiv:2404.04158v3 Announce Type: replace 
Abstract: The Circuit diameter of polytopes was introduced by Borgwardt, Finhold and Hemmecke as a fundamental tool for the study of circuit augmentation schemes for linear programming and for estimating combinatorial diameters. Determining the complexity of computing the circuit diameter of polytopes was posed as an open problem by Sanit\`a as well as by Kafer, and was recently reiterated by Borgwardt, Grewe, Kafer, Lee and Sanit\`a.
  In this paper, we solve this problem by showing that computing the circuit diameter of a polytope given in halfspace-description is strongly NP-hard. To prove this result, we show that computing the combinatorial diameter of the perfect matching polytope of a bipartite graph is NP-hard. This complements a result by Sanit\`a (FOCS 2018) on the NP-hardness of computing the diameter of fractional matching polytopes and implies the new result that computing the diameter of a $\{0,1\}$-polytope is strongly NP-hard, which may be of independent interest. In our second main result, we give a precise graph-theoretic description of the monotone diameter of perfect matching polytopes and use this description to prove that computing the monotone (circuit) diameter of a given input polytope is strongly NP-hard as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04158v3</guid>
      <category>math.OC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian N\"obel, Raphael Steiner</dc:creator>
    </item>
    <item>
      <title>Non-asymptotic Global Convergence Analysis of BFGS with the Armijo-Wolfe Line Search</title>
      <link>https://arxiv.org/abs/2404.16731</link>
      <description>arXiv:2404.16731v2 Announce Type: replace 
Abstract: In this paper, we present the first explicit and non-asymptotic global convergence rates of the BFGS method when implemented with an inexact line search scheme satisfying the Armijo-Wolfe conditions. We show that BFGS achieves a global linear convergence rate of $(1 - \frac{1}{\kappa})^t$ for $\mu$-strongly convex functions with $L$-Lipschitz gradients, where $\kappa = \frac{L}{\mu}$ represents the condition number. Additionally, if the objective function's Hessian is Lipschitz, BFGS with the Armijo-Wolfe line search achieves a linear convergence rate that depends solely on the line search parameters, independent of the condition number. We also establish a global superlinear convergence rate of $\mathcal{O}((\frac{1}{t})^t)$. These global bounds are all valid for any starting point $x_0$ and any symmetric positive definite initial Hessian approximation matrix $B_0$, though the choice of $B_0$ impacts the number of iterations needed to achieve these rates. By synthesizing these results, we outline the first global complexity characterization of BFGS with the Armijo-Wolfe line search. Additionally, we clearly define a mechanism for selecting the step size to satisfy the Armijo-Wolfe conditions and characterize its overall complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16731v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiujiang Jin, Ruichen Jiang, Aryan Mokhtari</dc:creator>
    </item>
    <item>
      <title>Ensemble Control for Stochastic Systems with Asymmetric Laplace Noises</title>
      <link>https://arxiv.org/abs/2405.09973</link>
      <description>arXiv:2405.09973v4 Announce Type: replace 
Abstract: This paper presents an adaptive ensemble control for stochastic systems subject to asymmetric noises and outliers. Asymmetric noises skew system observations, and outliers with large amplitude deteriorate the observations even further. Such disturbances induce poor system estimation and degraded stochastic system control. In this work, we model the asymmetric noises and outliers by mixed asymmetric Laplace distributions (ALDs), and propose an optimal control for stochastic systems with mixed ALD noises. Particularly, we segregate the system disturbed by mixed ALD noises into subsystems, each of which is subject to a specific ALD noise. For each subsystem, we design an iterative quantile filter (IQF) to estimate the system parameters using system observations. With the estimated parameters by IQF, we derive the certainty equivalence (CE) control law for each subsystem. Then we use the Bayesian approach to ensemble the subsystem CE controllers, with each of the controllers weighted by their posterior probability. We finalize our control law as the weighted sum of the control signals by the sub-system CE controllers. To demonstrate our approach, we conduct numerical simulations and Monte Carlo analyses. The results show improved tracking performance by our approach for skew noises and its robustness to outliers, compared with single ALD based and RLS-based control policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09973v4</guid>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yajie Yu, Xuehui Ma, Shiliang Zhang, Zhuzhu Wang, Xubing Shi, Yushuai Li, Tingwen Huang</dc:creator>
    </item>
    <item>
      <title>Variance-reduction for Variational Inequality Problems with Bregman Distance Function</title>
      <link>https://arxiv.org/abs/2405.10735</link>
      <description>arXiv:2405.10735v2 Announce Type: replace 
Abstract: In this paper, we address variational inequalities (VI) with a finite-sum structure. We introduce a novel single-loop stochastic variance-reduced algorithm, incorporating the Bregman distance function, and establish an optimal convergence guarantee under a monotone setting. Additionally, we explore a structured class of non-monotone problems that exhibit weak Minty solutions, and analyze the complexity of our proposed method, highlighting a significant improvement over existing approaches. Numerical experiments are presented to demonstrate the performance of our algorithm compared to state-of-the-art methods</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10735v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeinab Alizadeh, Erfan Yazdandoost Hamedani, Afrooz Jalilzadeh</dc:creator>
    </item>
    <item>
      <title>State-of-the-art Methods for Pseudo-Boolean Solving with SCIP</title>
      <link>https://arxiv.org/abs/2501.03390</link>
      <description>arXiv:2501.03390v2 Announce Type: replace 
Abstract: The Pseudo-Boolean problem deals with linear or polynomial constraints with integer coefficients over Boolean variables. The objective lies in optimizing a linear objective function, or finding a feasible solution, or finding a solution that satisfies as many constraints as possible. In the 2024 Pseudo-Boolean competition, solvers incorporating the SCIP framework won five out of six categories it was competing in. From a total of 1,207 instances, SCIP successfully solved 759, while its parallel version FiberSCIP solved 776. Based on the results from the competition, we further enhanced SCIP's Pseudo-Boolean capabilities. This article discusses the results and presents the winning algorithmic ideas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03390v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gioni Mexi, Dominik Kamp, Yuji Shinano, Shanwen Pu, Alexander Hoen, Ksenia Bestuzheva, Christopher Hojny, Matthias Walter, Marc E. Pfetsch, Sebastian Pokutta, Thorsten Koch</dc:creator>
    </item>
    <item>
      <title>Scalable Second-Order Optimization Algorithms for Minimizing Low-rank Functions</title>
      <link>https://arxiv.org/abs/2501.03718</link>
      <description>arXiv:2501.03718v2 Announce Type: replace 
Abstract: We present a random-subspace variant of cubic regularization algorithm that chooses the size of the subspace adaptively, based on the rank of the projected second derivative matrix. Iteratively, our variant only requires access to (small-dimensional) projections of first- and second-order problem derivatives and calculates a reduced step inexpensively. The ensuing method maintains the optimal global rate of convergence of (full-dimensional) cubic regularization, while showing improved scalability both theoretically and numerically, particularly when applied to low-rank functions. When applied to the latter, our algorithm naturally adapts the subspace size to the true rank of the function, without knowing it a priori.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03718v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edward Tansley, Coralia Cartis</dc:creator>
    </item>
    <item>
      <title>Hydrogen Network Expansion Planning considering the Chicken-and-egg Dilemma and Market Uncertainty</title>
      <link>https://arxiv.org/abs/2501.03744</link>
      <description>arXiv:2501.03744v2 Announce Type: replace 
Abstract: Green hydrogen is thought to be a game changer for reaching sustainability targets. However, the transition to a green hydrogen economy faces a critical challenge known as the `chicken-and-egg dilemma', wherein establishing a hydrogen supply network relies on demand, while demand only grows with reliable supply. In addition, as the hydrogen market is in the early stage, predicting demand distributions is challenging due to lack of data availability. This paper addresses these complex issues through a risk-averse framework with the introduction of a distributionally robust hydrogen network expansion planning problem under decision-dependent demand ambiguity. The problem optimizes location and production capacity decisions of the suppliers considering the moments of the stochastic hydrogen demand as a function of these investment decisions. To obtain tractable representations of this problem, we derive two different reformulations that consider continuous and discrete hydrogen demand support sets under different forms of decision dependencies. To efficiently solve the reformulations, we develop a tailored algorithm based on the column-and-constraint generation approach, and enhance the computational performance through solving the master problems to a relative optimality gap, decomposing the subproblems, and integrating pre-generated columns and constraints. To validate the effectiveness of our approach, we investigate a real case study leveraging data from the "Hydrogen Energy Applications in Valley Environments for Northern Netherlands (HEAVENN)" project. The results reveal that considering the chicken-and-egg dilemma under uncertain hydrogen market conditions leads to earlier and more diverse investments, providing critical insights for policymakers based on the degree of decision dependency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03744v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sezen Ece Kayac{\i}k, Beste Basciftci, Albert H. Schrotenboer, Iris F. A. Vis, Evrim Ursavas</dc:creator>
    </item>
    <item>
      <title>Deep Policy Iteration with Integer Programming for Inventory Management</title>
      <link>https://arxiv.org/abs/2112.02215</link>
      <description>arXiv:2112.02215v3 Announce Type: replace-cross 
Abstract: We present a Reinforcement Learning (RL) based framework for optimizing long-term discounted reward problems with large combinatorial action space and state dependent constraints. These characteristics are common to many operations management problems, e.g., network inventory replenishment, where managers have to deal with uncertain demand, lost sales, and capacity constraints that results in more complex feasible action spaces. Our proposed Programmable Actor Reinforcement Learning (PARL) uses a deep-policy iteration method that leverages neural networks (NNs) to approximate the value function and combines it with mathematical programming (MP) and sample average approximation (SAA) to solve the per-step-action optimally while accounting for combinatorial action spaces and state-dependent constraint sets. We show how the proposed methodology can be applied to complex inventory replenishment problems where analytical solutions are intractable. We also benchmark the proposed algorithm against state-of-the-art RL algorithms and commonly used replenishment heuristics and find it considerably outperforms existing methods by as much as 14.7% on average in various complex supply chain settings. We find that this improvement of PARL over benchmark algorithms can be directly attributed to better inventory cost management, especially in inventory constrained settings. Furthermore, in the simpler setting where optimal replenishment policy is tractable or known near optimal heuristics exist, we find that the RL approaches can learn near optimal policies. Finally, to make RL algorithms more accessible for inventory management researchers, we also discuss the development of a modular Python library that can be used to test the performance of RL algorithms with various supply chain structures and spur future research in developing practical and near-optimal algorithms for inventory management problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.02215v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1287/msom.2022.0617</arxiv:DOI>
      <dc:creator>Pavithra Harsha, Ashish Jagmohan, Jayant Kalagnanam, Brian Quanz, Divya Singhvi</dc:creator>
    </item>
    <item>
      <title>GCBF+: A Neural Graph Control Barrier Function Framework for Distributed Safe Multi-Agent Control</title>
      <link>https://arxiv.org/abs/2401.14554</link>
      <description>arXiv:2401.14554v2 Announce Type: replace-cross 
Abstract: Distributed, scalable, and safe control of large-scale multi-agent systems is a challenging problem. In this paper, we design a distributed framework for safe multi-agent control in large-scale environments with obstacles, where a large number of agents are required to maintain safety using only local information and reach their goal locations. We introduce a new class of certificates, termed graph control barrier function (GCBF), which are based on the well-established control barrier function theory for safety guarantees and utilize a graph structure for scalable and generalizable distributed control of MAS. We develop a novel theoretical framework to prove the safety of an arbitrary-sized MAS with a single GCBF. We propose a new training framework GCBF+ that uses graph neural networks to parameterize a candidate GCBF and a distributed control policy. The proposed framework is distributed and is capable of taking point clouds from LiDAR, instead of actual state information, for real-world robotic applications. We illustrate the efficacy of the proposed method through various hardware experiments on a swarm of drones with objectives ranging from exchanging positions to docking on a moving target without collision. Additionally, we perform extensive numerical experiments, where the number and density of agents, as well as the number of obstacles, increase. Empirical results show that in complex environments with agents with nonlinear dynamics (e.g., Crazyflie drones), GCBF+ outperforms the hand-crafted CBF-based method with the best performance by up to 20% for relatively small-scale MAS with up to 256 agents, and leading reinforcement learning (RL) methods by up to 40% for MAS with 1024 agents. Furthermore, the proposed method does not compromise on the performance, in terms of goal reaching, for achieving high safety rates, which is a common trade-off in RL-based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14554v2</guid>
      <category>cs.RO</category>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Songyuan Zhang, Oswin So, Kunal Garg, Chuchu Fan</dc:creator>
    </item>
    <item>
      <title>Rethinking the Capacity of Graph Neural Networks for Branching Strategy</title>
      <link>https://arxiv.org/abs/2402.07099</link>
      <description>arXiv:2402.07099v3 Announce Type: replace-cross 
Abstract: Graph neural networks (GNNs) have been widely used to predict properties and heuristics of mixed-integer linear programs (MILPs) and hence accelerate MILP solvers. This paper investigates the capacity of GNNs to represent strong branching (SB), the most effective yet computationally expensive heuristic employed in the branch-and-bound algorithm. In the literature, message-passing GNN (MP-GNN), as the simplest GNN structure, is frequently used as a fast approximation of SB and we find that not all MILPs's SB can be represented with MP-GNN. We precisely define a class of "MP-tractable" MILPs for which MP-GNNs can accurately approximate SB scores. Particularly, we establish a universal approximation theorem: for any data distribution over the MP-tractable class, there always exists an MP-GNN that can approximate the SB score with arbitrarily high accuracy and arbitrarily high probability, which lays a theoretical foundation of the existing works on imitating SB with MP-GNN. For MILPs without the MP-tractability, unfortunately, a similar result is impossible, which can be illustrated by two MILP instances with different SB scores that cannot be distinguished by any MP-GNN, regardless of the number of parameters. Recognizing this, we explore another GNN structure called the second-order folklore GNN (2-FGNN) that overcomes this limitation, and the aforementioned universal approximation theorem can be extended to the entire MILP space using 2-FGNN, regardless of the MP-tractability. A small-scale numerical experiment is conducted to directly validate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07099v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziang Chen, Jialin Liu, Xiaohan Chen, Xinshang Wang, Wotao Yin</dc:creator>
    </item>
    <item>
      <title>Learning Neural Contracting Dynamics: Extended Linearization and Global Guarantees</title>
      <link>https://arxiv.org/abs/2402.08090</link>
      <description>arXiv:2402.08090v4 Announce Type: replace-cross 
Abstract: Global stability and robustness guarantees in learned dynamical systems are essential to ensure well-behavedness of the systems in the face of uncertainty. We present Extended Linearized Contracting Dynamics (ELCD), the first neural network-based dynamical system with global contractivity guarantees in arbitrary metrics. The key feature of ELCD is a parametrization of the extended linearization of the nonlinear vector field. In its most basic form, ELCD is guaranteed to be (i) globally exponentially stable, (ii) equilibrium contracting, and (iii) globally contracting with respect to some metric. To allow for contraction with respect to more general metrics in the data space, we train diffeomorphisms between the data space and a latent space and enforce contractivity in the latent space, which ensures global contractivity in the data space. We demonstrate the performance of ELCD on the high dimensional LASA, multi-link pendulum, and Rosenbrock datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08090v4</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sean Jaffe, Alexander Davydov, Deniz Lapsekili, Ambuj Singh, Francesco Bullo</dc:creator>
    </item>
    <item>
      <title>Generalized sparsity-promoting solvers for Bayesian inverse problems: Versatile sparsifying transforms and unknown noise variances</title>
      <link>https://arxiv.org/abs/2402.16623</link>
      <description>arXiv:2402.16623v2 Announce Type: replace-cross 
Abstract: Bayesian hierarchical models can provide efficient algorithms for finding sparse solutions to ill-posed inverse problems. The models typically comprise a conditionally Gaussian prior model for the unknown which is augmented by a generalized gamma hyper-prior model for variance hyper-parameters. This investigation generalizes these models and their efficient maximum a posterior (MAP) estimation using the iterative alternating sequential (IAS) algorithm in two ways: (1) General sparsifying transforms: Diverging from conventional methods, our approach permits the use of sparsifying transformations with nontrivial kernels; (2) Unknown noise variances: We treat the noise variance as a random variable that is estimated during the inference procedure. This is important in applications where the noise estimate cannot be accurately estimated a priori. Remarkably, these augmentations neither significantly burden the computational expense of the algorithm nor compromise its efficacy. We include convexity and convergence analysis for the method and demonstrate its efficacy in several numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16623v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1088/1361-6420/ada17f</arxiv:DOI>
      <dc:creator>Jonathan Lindbloom, Jan Glaubitz, Anne Gelb</dc:creator>
    </item>
    <item>
      <title>Off-the-grid regularisation for Poisson inverse problems</title>
      <link>https://arxiv.org/abs/2404.00810</link>
      <description>arXiv:2404.00810v2 Announce Type: replace-cross 
Abstract: Off-the-grid regularisation has been extensively employed over the last decade in the context of ill-posed inverse problems formulated in the continuous setting of the space of Radon measures $\mathcal{M}(\mathcal{X})$. These approaches enjoy convexity and counteract the discretisation biases as well the numerical instabilities typical of their discrete counterparts. In the framework of sparse reconstruction of discrete point measures (sum of weighted Diracs), a Total Variation regularisation norm in $\mathcal{M}(\mathcal{X})$ is typically combined with an $L^2$ data term modelling additive Gaussian noise. To asses the framework of off-the-grid regularisation in the presence of signal-dependent Poisson noise, we consider in this work a variational model coupling the Total Variation regularisation with a Kullback-Leibler data term under a non-negativity constraint. Analytically, we study the optimality conditions of the composite functional and analyse its dual problem. Then, we consider an homotopy strategy to select an optimal regularisation parameter and use it within a Sliding Frank-Wolfe algorithm. Several numerical experiments on both 1D/2D simulated and real 3D fluorescent microscopy data are reported.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00810v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marta Lazzaretti, Claudio Estatico, Alejandro Melero, Luca Calatroni</dc:creator>
    </item>
    <item>
      <title>Conjugate-Gradient-like Based Adaptive Moment Estimation Optimization Algorithm for Deep Learning</title>
      <link>https://arxiv.org/abs/2404.01714</link>
      <description>arXiv:2404.01714v4 Announce Type: replace-cross 
Abstract: Training deep neural networks is a challenging task. In order to speed up training and enhance the performance of deep neural networks, we rectify the vanilla conjugate gradient as conjugate-gradient-like and incorporate it into the generic Adam, and thus propose a new optimization algorithm named CG-like-Adam for deep learning. Specifically, both the first-order and the second-order moment estimation of generic Adam are replaced by the conjugate-gradient-like. Convergence analysis handles the cases where the exponential moving average coefficient of the first-order moment estimation is constant and the first-order moment estimation is unbiased. Numerical experiments show the superiority of the proposed algorithm based on the CIFAR10/100 dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01714v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawu Tian, Liwei Xu, Xiaowei Zhang, Yongqi Li</dc:creator>
    </item>
    <item>
      <title>A Game Between Two Identical Dubins Cars: Evading a Conic Sensor in Minimum Time</title>
      <link>https://arxiv.org/abs/2406.08637</link>
      <description>arXiv:2406.08637v2 Announce Type: replace-cross 
Abstract: A fundamental task in mobile robotics is keeping an intelligent agent under surveillance with an autonomous robot as it travels in the environment. This work studies a theoretical version of that problem involving one of the most popular vehicle platforms in robotics. In particular, we consider two identical Dubins cars moving on a plane without obstacles. One of them plays as the pursuer, and it is equipped with a limited field-of-view detection region modeled as a semi-infinite cone with its apex at the pursuer's position. The pursuer aims to maintain the other Dubins car, which plays as the evader, as much time as possible inside its detection region. On the contrary, the evader wants to escape as soon as possible. In this work, employing differential game theory, we find the time-optimal motion strategies near the game's end. The analysis of those trajectories reveals the existence of at least two singular surfaces: a Transition Surface (also known as a Switch Surface) and an Evader's Universal Surface. We also found that the barrier's standard construction produces a surface that partially lies outside the playing space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08637v2</guid>
      <category>cs.RO</category>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ubaldo Ruiz</dc:creator>
    </item>
    <item>
      <title>A three-stage method for reconstructing multiple coefficients in coupled photoacoustic and diffuse optical imaging</title>
      <link>https://arxiv.org/abs/2408.03496</link>
      <description>arXiv:2408.03496v2 Announce Type: replace-cross 
Abstract: This paper studies inverse problems in quantitative photoacoustic tomography with additional optical current data supplemented from diffuse optical tomography. We propose a three-stage image reconstruction method for the simultaneous recovery of the absorption, diffusion, and Gr\"uneisen coefficients. We demonstrate, through numerical simulations, that: (i) when the Gr\"uneisen coefficient is known, the addition of the optical measurements allows a more accurate reconstruction of the scattering and absorption coefficients; and (ii) when the Gr\"uneisen coefficient is not known, the addition of optical current measurements allows us to reconstruct uniquely the Gr\"uneisen, the scattering and absorption coefficients. Numerical simulations based on synthetic data are presented to demonstrate the effectiveness of the proposed idea.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03496v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <category>physics.comp-ph</category>
      <category>physics.med-ph</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinxi Pan, Kui Ren, Shanyin Tong</dc:creator>
    </item>
    <item>
      <title>Euclidean distance discriminants and Morse attractors</title>
      <link>https://arxiv.org/abs/2412.16957</link>
      <description>arXiv:2412.16957v3 Announce Type: replace-cross 
Abstract: Our study concerns the Euclidean distance function in case of complex plane curves. We decompose the ED discriminant into 3 parts which are responsible for the 3 types of behavior of the Morse points, and we find the structure of each one. In particular we shed light on the ``atypical discriminant'' which is due to the loss of Morse points at infinity. We find formulas for the number of Morse singularities which abut to the corresponding 3 types of attractors when moving the centre of the distance function toward a point of the discriminant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16957v3</guid>
      <category>math.AG</category>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cezar Joi\c{t}a, Dirk Siersma, Mihai Tib\u{a}r</dc:creator>
    </item>
    <item>
      <title>Central limit theorems for vector-valued composite functionals with smoothing and applications</title>
      <link>https://arxiv.org/abs/2412.19367</link>
      <description>arXiv:2412.19367v3 Announce Type: replace-cross 
Abstract: This paper focuses on vector-valued composite functionals, which may be nonlinear in probability. Our primary goal is to establish central limit theorems for these functionals when mixed estimators are employed. Our study is relevant to the evaluation and comparison of risk in decision-making contexts and extends to functionals that arise in machine learning methods. A generalized family of composite risk functionals is presented, which encompasses most of the known coherent risk measures including systemic measures of risk. The paper makes two main contributions. First, we analyze vector-valued functionals, providing a framework for evaluating high-dimensional risks. This framework facilitates the comparison of multiple risk measures, as well as the estimation and asymptotic analysis of systemic risk and its optimal value in decision-making problems. Second, we derive novel central limit theorems for optimized composite functionals when mixed types of estimators: empirical and smoothed estimators are used. We provide verifiable sufficient conditions for the central limit formulae and show their applicability to several popular measures of risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19367v3</guid>
      <category>math.ST</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huihui Chen, Darinka Dentcheva, Yang Lin, Gregory J. Stock</dc:creator>
    </item>
  </channel>
</rss>
