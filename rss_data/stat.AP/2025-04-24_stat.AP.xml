<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Apr 2025 04:01:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Weighted-likelihood framework for class imbalance in Bayesian prediction models</title>
      <link>https://arxiv.org/abs/2504.17013</link>
      <description>arXiv:2504.17013v1 Announce Type: new 
Abstract: Class imbalance occurs when data used for training classification models has a different number of observations or samples within each category or class. Models built on such data can be biased towards the majority class and have poor predictive performance and generalisation for the minority class. We propose a Bayesian weighted-likelihood (power-likelihood) approach to deal with class imbalance: each observation's likelihood is raised to a weight inversely proportional to its class proportion, with weights normalized to sum to the number of samples. This embeds cost-sensitive learning directly into Bayesian updating and is applicable to binary, multinomial and ordered logistic prediction models. Example models are implemented in Stan, PyMC, and Turing.jl, and all code and reproducible scripts are archived on Github: https://github.com/stanlazic/weighted_likelihoods. This approach is simple to implement and extends naturally to arbitrary error-cost matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17013v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Stanley E. Lazic</dc:creator>
    </item>
    <item>
      <title>MOOSE ProbML: Parallelized Probabilistic Machine Learning and Uncertainty Quantification for Computational Energy Applications</title>
      <link>https://arxiv.org/abs/2504.17101</link>
      <description>arXiv:2504.17101v1 Announce Type: new 
Abstract: This paper presents the development and demonstration of massively parallel probabilistic machine learning (ML) and uncertainty quantification (UQ) capabilities within the Multiphysics Object-Oriented Simulation Environment (MOOSE), an open-source computational platform for parallel finite element and finite volume analyses. In addressing the computational expense and uncertainties inherent in complex multiphysics simulations, this paper integrates Gaussian process (GP) variants, active learning, Bayesian inverse UQ, adaptive forward UQ, Bayesian optimization, evolutionary optimization, and Markov chain Monte Carlo (MCMC) within MOOSE. It also elaborates on the interaction among key MOOSE systems -- Sampler, MultiApp, Reporter, and Surrogate -- in enabling these capabilities. The modularity offered by these systems enables development of a multitude of probabilistic ML and UQ algorithms in MOOSE. Example code demonstrations include parallel active learning and parallel Bayesian inference via active learning. The impact of these developments is illustrated through five applications relevant to computational energy applications: UQ of nuclear fuel fission product release, using parallel active learning Bayesian inference; very rare events analysis in nuclear microreactors using active learning; advanced manufacturing process modeling using multi-output GPs (MOGPs) and dimensionality reduction; fluid flow using deep GPs (DGPs); and tritium transport model parameter optimization for fusion energy, using batch Bayesian optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17101v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Somayajulu L. N. Dhulipala, Peter German, Yifeng Che, Zachary M. Prince, Pierre-Clement A. Simon, Xianjian Xie, Vincent M. Laboure, Hao Yan</dc:creator>
    </item>
    <item>
      <title>Probabilistic modeling of delays for train journeys with transfers</title>
      <link>https://arxiv.org/abs/2504.17479</link>
      <description>arXiv:2504.17479v1 Announce Type: new 
Abstract: Reliability plays a key role in the experience of a rail traveler. The reliability of journeys involving transfers is affected by the reliability of the transfers and the consequences of missing a transfer, as well as the possible delay of the train used to reach the destination. In this paper, we propose a flexible method to model the reliability of train journeys with any number of transfers. The method combines a transfer reliability model based on gradient boosting responsible for predicting the reliability of transfers between trains and a delay model based on probabilistic Bayesian regression, which is used to model train arrival delays. The models are trained on delay data from four Swedish train stations and evaluated on delay data from another two stations, in order to evaluate the generalization performance of the models. We show that the probabilistic delay model, which models train delays following a mixture distribution with two lognormal components, allows to much more realistically model the distribution of actual train delays compared to a standard lognormal model. Finally, we show how these models can be used together to sample the arrival delay at the final destination of the entire journey. The results indicate that the method accurately predicts the reliability for nine out of ten tested journeys. The method could be used to improve journey planners by providing reliability information to travelers. Further applications include timetable planning and transport modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17479v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nikolaus Stratil-Sauer (Link\"oping University), Nils Breyer (Link\"oping University)</dc:creator>
    </item>
    <item>
      <title>ColonScopeX: Leveraging Explainable Expert Systems with Multimodal Data for Improved Early Diagnosis of Colorectal Cancer</title>
      <link>https://arxiv.org/abs/2504.08824</link>
      <description>arXiv:2504.08824v1 Announce Type: cross 
Abstract: Colorectal cancer (CRC) ranks as the second leading cause of cancer-related deaths and the third most prevalent malignant tumour worldwide. Early detection of CRC remains problematic due to its non-specific and often embarrassing symptoms, which patients frequently overlook or hesitate to report to clinicians. Crucially, the stage at which CRC is diagnosed significantly impacts survivability, with a survival rate of 80-95\% for Stage I and a stark decline to 10\% for Stage IV. Unfortunately, in the UK, only 14.4\% of cases are diagnosed at the earliest stage (Stage I).
  In this study, we propose ColonScopeX, a machine learning framework utilizing explainable AI (XAI) methodologies to enhance the early detection of CRC and pre-cancerous lesions. Our approach employs a multimodal model that integrates signals from blood sample measurements, processed using the Savitzky-Golay algorithm for fingerprint smoothing, alongside comprehensive patient metadata, including medication history, comorbidities, age, weight, and BMI. By leveraging XAI techniques, we aim to render the model's decision-making process transparent and interpretable, thereby fostering greater trust and understanding in its predictions. The proposed framework could be utilised as a triage tool or a screening tool of the general population.
  This research highlights the potential of combining diverse patient data sources and explainable machine learning to tackle critical challenges in medical diagnostics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08824v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Natalia Sikora, Robert L. Manschke, Alethea M. Tang, Peter Dunstan, Dean A. Harris, Su Yang</dc:creator>
    </item>
    <item>
      <title>A Sensitivity Analysis Framework for Quantifying Confidence in Decisions in the Presence of Data Uncertainty</title>
      <link>https://arxiv.org/abs/2504.17043</link>
      <description>arXiv:2504.17043v1 Announce Type: cross 
Abstract: Nearly all statistical analyses that inform policy-making are based on imperfect data. As examples, the data may suffer from measurement errors, missing values, sample selection bias, or record linkage errors. Analysts have to decide how to handle such data imperfections, e.g., analyze only the complete cases or impute values for the missing items via some posited model. Their choices can influence estimates and hence, ultimately, policy decisions. Thus, it is prudent for analysts to evaluate the sensitivity of estimates and policy decisions to the assumptions underlying their choices. To facilitate this goal, we propose that analysts define metrics and visualizations that target the sensitivity of the ultimate decision to the assumptions underlying their approach to handling the data imperfections. Using these visualizations, the analyst can assess their confidence in the policy decision under their chosen analysis. We illustrate metrics and corresponding visualizations with two examples, namely considering possible measurement error in the inputs of predictive models of presidential vote share and imputing missing values when evaluating the percentage of children exposed to high levels of lead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17043v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adway S. Wadekar, Jerome P. Reiter</dc:creator>
    </item>
    <item>
      <title>A Novel Hybrid Approach Using an Attention-Based Transformer + GRU Model for Predicting Cryptocurrency Prices</title>
      <link>https://arxiv.org/abs/2504.17079</link>
      <description>arXiv:2504.17079v1 Announce Type: cross 
Abstract: In this article, we introduce a novel deep learning hybrid model that integrates attention Transformer and Gated Recurrent Unit (GRU) architectures to improve the accuracy of cryptocurrency price predictions. By combining the Transformer's strength in capturing long-range patterns with the GRU's ability to model short-term and sequential trends, the hybrid model provides a well-rounded approach to time series forecasting. We apply the model to predict the daily closing prices of Bitcoin and Ethereum based on historical data that include past prices, trading volumes, and the Fear and Greed index. We evaluate the performance of our proposed model by comparing it with four other machine learning models: two are non-sequential feedforward models: Radial Basis Function Network (RBFN) and General Regression Neural Network (GRNN), and two are bidirectional sequential memory-based models: Bidirectional Long-Short-Term Memory (BiLSTM) and Bidirectional Gated Recurrent Unit (BiGRU). The performance of the model is assessed using several metrics, including Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE), along with statistical validation through the nonparametric Friedman test followed by a post hoc Wilcoxon signed rank test. The results demonstrate that our hybrid model consistently achieves superior accuracy, highlighting its effectiveness for financial prediction tasks. These findings provide valuable insights for improving real-time decision making in cryptocurrency markets and support the growing use of hybrid deep learning models in financial analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17079v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Esam Mahdi, C. Martin-Barreiro, X. Cabezas</dc:creator>
    </item>
    <item>
      <title>Target trial emulation without matching: a more efficient approach for evaluating vaccine effectiveness using observational data</title>
      <link>https://arxiv.org/abs/2504.17104</link>
      <description>arXiv:2504.17104v1 Announce Type: cross 
Abstract: Real-world vaccine effectiveness has increasingly been studied using matching-based approaches, particularly in observational cohort studies following the target trial emulation framework. Although matching is appealing in its simplicity, it suffers important limitations in terms of clarity of the target estimand and the efficiency or precision with which is it estimated. Scientifically justified causal estimands of vaccine effectiveness may be difficult to define owing to the fact that vaccine uptake varies over calendar time when infection dynamics may also be rapidly changing. We propose a causal estimand of vaccine effectiveness that summarizes vaccine effectiveness over calendar time, similar to how vaccine efficacy is summarized in a randomized controlled trial. We describe the identification of our estimand, including its underlying assumptions, and propose simple-to-implement estimators based on two hazard regression models. We apply our proposed estimator in simulations and in a study to assess the effectiveness of the Pfizer-BioNTech COVID-19 vaccine to prevent infections with SARS-CoV2 in children 5-11 years old. In both settings, we find that our proposed estimator yields similar scientific inferences while providing significant efficiency gains over commonly used matching-based estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17104v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emily Wu, Elizabeth Rogawski McQuade, Mats Stensrud, Razieh Nabi, David Benkeser</dc:creator>
    </item>
    <item>
      <title>The Loser's Curse and the Critical Role of the Utility Function</title>
      <link>https://arxiv.org/abs/2411.10400</link>
      <description>arXiv:2411.10400v4 Announce Type: replace 
Abstract: A longstanding question in the judgment and decision making literature is whether experts, even in high-stakes environments, exhibit the same cognitive biases observed in controlled experiments with inexperienced participants. Massey and Thaler (2013) claim to have found an example of bias and irrationality in expert decision making: general managers' behavior in the National Football League draft pick trade market. They argue that general managers systematically overvalue top draft picks, which generate less surplus value on average than later first-round picks, a phenomenon known as the loser's curse. Their conclusion hinges on the assumption that general managers should use expected surplus value as their utility function for evaluating draft picks. This assumption, however, is neither explicitly justified nor necessarily aligned with the strategic complexities of constructing a National Football League roster. In this paper, we challenge their framework by considering alternative utility functions, particularly those that emphasize the acquisition of transformational players--those capable of dramatically increasing a team's chances of winning the Super Bowl. Under a decision rule that prioritizes the probability of acquiring elite players, which we construct from a novel Bayesian hierarchical Beta regression model, general managers' draft trade behavior appears rational rather than systematically flawed. More broadly, our findings highlight the critical role of carefully specifying a utility function when evaluating the quality of decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10400v4</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan S. Brill, Abraham J. Wyner</dc:creator>
    </item>
    <item>
      <title>Theoretical and Practical Limits of Signal Strength Estimate Precision for Kolmogorov-Zurbenko Periodograms with Dynamic Smoothing</title>
      <link>https://arxiv.org/abs/2412.07735</link>
      <description>arXiv:2412.07735v2 Announce Type: replace 
Abstract: This investigation establishes the theoretical and practical limits of signal strength estimate precision for Kolmogorov-Zurbenko periodograms with dynamic smoothing and compares them to those of standard log-periodograms with static smoothing. Previous research has established the sensitivity, accuracy, resolution, and robustness of Kolmogorov-Zurbenko periodograms with dynamic smoothing in estimating signal frequencies. However, the precision with which they estimate signal strength has never been evaluated. To this point, the width of the confidence interval for a signal strength estimate can serve as a criterion for assessing the precision of such estimates: the narrower the confidence interval, the more precise the estimate. The statistical background for confidence intervals of periodograms is presented, followed by candidate functions to compute and plot them when using Kolmogorov-Zurbenko periodograms with dynamic smoothing. Given an identified signal frequency, a static smoothing window and its smoothing window width can be selected such that its confidence interval is narrower and, thus, its signal strength estimate more precise, than that of dynamic smoothing windows, all while maintaining a level of frequency resolution as good as or better than that of a dynamic smoothing window. These findings suggest the need for a two-step protocol in spectral analysis: computation of a Kolmogorov-Zurbenko periodogram with dynamic smoothing to detect, identify, and separate signal frequencies, followed by computation of a Kolmogorov-Zurbenko periodogram with static smoothing to precisely estimate signal strength and compute its confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07735v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Barry Loneck, Igor Zurbenko, Edward Valachovic</dc:creator>
    </item>
    <item>
      <title>Conformal prediction of future insurance claims in the regression problem</title>
      <link>https://arxiv.org/abs/2503.03659</link>
      <description>arXiv:2503.03659v2 Announce Type: replace-cross 
Abstract: In the current insurance literature, prediction of insurance claims in the regression problem is often performed with a statistical model. This model-based approach may potentially suffer from several drawbacks: (i) model misspecification, (ii) selection effect, and (iii) lack of finite-sample validity. This article addresses these three issues simultaneously by employing conformal prediction -- a general machine learning strategy for valid predictions. The proposed method is both model-free and tuning-parameter-free. It also guarantees finite-sample validity at a pre-assigned coverage probability level. Examples, based on both simulated and real data, are provided to demonstrate the excellent performance of the proposed method and its applications in insurance, especially regarding meeting the solvency capital requirement of European insurance regulation, Solvency II.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03659v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liang Hong</dc:creator>
    </item>
    <item>
      <title>Comparison of Bayesian methods for extrapolation of treatment effects: a large scale simulation study</title>
      <link>https://arxiv.org/abs/2504.01949</link>
      <description>arXiv:2504.01949v2 Announce Type: replace-cross 
Abstract: Extrapolating treatment effects from related studies is a promising strategy for designing and analyzing clinical trials in situations where achieving an adequate sample size is challenging. Bayesian methods are well-suited for this purpose, as they enable the synthesis of prior information through the use of prior distributions. While the operating characteristics of Bayesian approaches for borrowing data from control arms have been extensively studied, methods that borrow treatment effects -- quantities derived from the comparison between two arms -- remain less well understood. In this paper, we present the findings of an extensive simulation study designed to address this gap. We evaluate the frequentist operating characteristics of these methods, including the probability of success, mean squared error, bias, precision, and credible interval coverage. Our results provide insights into the strengths and limitations of existing methods in the context of confirmatory trials. In particular, we show that the Conditional Power Prior and the Robust Mixture Prior perform better overall, while the test-then-pool variants and the p-value-based power prior display suboptimal performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01949v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tristan Fauvel, Julien Tanniou, Pascal Godbillot, Marie G\'enin, Billy Amzal</dc:creator>
    </item>
  </channel>
</rss>
