<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 17 Mar 2025 04:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>OCPM$^2$: Extending the Process Mining Methodology for Object-Centric Event Data Extraction</title>
      <link>https://arxiv.org/abs/2503.10735</link>
      <description>arXiv:2503.10735v1 Announce Type: new 
Abstract: Object-Centric Process Mining (OCPM) enables business process analysis from multiple perspectives. For example, an educational path can be examined from the viewpoints of students, teachers, and groups. This analysis depends on Object-Centric Event Data (OCED), which captures relationships between events and object types, representing different perspectives. Unlike traditional process mining techniques, extracting OCED minimizes the need for repeated log extractions when shifting the analytical focus. However, recording these complex relationships increases the complexity of the log extraction process. To address this challenge, this paper proposes a method for extracting OCED based on PM\inst{2}, a well-established process mining framework. Our approach introduces a structured framework that guides data analysts and engineers in extracting OCED for process analysis. We validate this framework by applying it in a real-world educational setting, demonstrating its effectiveness in extracting an Object-Centric Event Log (OCEL), which serves as the standard format for recording OCED, from a learning management system and an administrative grading system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10735v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Najmeh Miri, Shahrzad Khayatbashi, Jelena Zdravkovic, Amin Jalali</dc:creator>
    </item>
    <item>
      <title>Elimination of annotation dependencies in validation for Modern JSON Schema</title>
      <link>https://arxiv.org/abs/2503.11288</link>
      <description>arXiv:2503.11288v1 Announce Type: new 
Abstract: JSON Schema is a logical language used to define the structure of JSON values. JSON Schema syntax is based on nested schema objects. In all versions of JSON Schema until Draft-07, collectively known as Classical JSON Schema, the semantics of a schema was entirely described by the set of JSON values that it validates. This semantics was the basis for a thorough theoretical study and for the development of tools to decide satisfiability and equivalence of schemas. Unfortunately, Classical JSON Schema suffered a severe limitation in its ability to express extensions of object schemas, which caused the introduction, with Draft 2019-09, of two disruptive features: annotation dependency and dynamic references.
  These new features undermine the previously developed semantic theory, and the algorithms used to decide satisfiability for Classical JSON Schema are not easy to extend. One possible solution is rewriting a schema written in Modern JSON Schema into an equivalent schema in Classical JSON Schema.
  In this paper we prove that the elimination of annotation dependent keywords cannot, in general, avoid an exponential increase of the schema dimension. We provide an algorithm to eliminate these keywords that, despite the theoretical lower bound, behaves quite well in practice, as we verify with an extensive set of experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11288v1</guid>
      <category>cs.DB</category>
      <category>cs.PL</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lyes Attouche, Mohamed-Amine Baazizi, Dario Colazzo, Giorgio Ghelli, Stefan Klessinger, Carlo Sartiani, Stefanie Scherzinger</dc:creator>
    </item>
    <item>
      <title>Step-by-Step Data Cleaning Recommendations to Improve ML Prediction Accuracy</title>
      <link>https://arxiv.org/abs/2503.11366</link>
      <description>arXiv:2503.11366v1 Announce Type: new 
Abstract: Data quality is crucial in machine learning (ML) applications, as errors in the data can significantly impact the prediction accuracy of the underlying ML model. Therefore, data cleaning is an integral component of any ML pipeline. However, in practical scenarios, data cleaning incurs significant costs, as it often involves domain experts for configuring and executing the cleaning process. Thus, efficient resource allocation during data cleaning can enhance ML prediction accuracy while controlling expenses.
  This paper presents COMET, a system designed to optimize data cleaning efforts for ML tasks. COMET gives step-by-step recommendations on which feature to clean next, maximizing the efficiency of data cleaning under resource constraints. We evaluated COMET across various datasets, ML algorithms, and data error types, demonstrating its robustness and adaptability. Our results show that COMET consistently outperforms feature importance-based, random, and another well-known cleaning method, achieving up to 52 and on average 5 percentage points higher ML prediction accuracy than the proposed baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11366v1</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.48786/edbt.2025.43</arxiv:DOI>
      <arxiv:journal_reference>Proceedings 28th International Conference on Extending Database Technology (EDBT) 2025, Barcelona, Spain, March 25-28, 2025, 542-554</arxiv:journal_reference>
      <dc:creator>Sedir Mohammed, Felix Naumann, Hazar Harmouch</dc:creator>
    </item>
    <item>
      <title>mobilityDCAT-AP: a Metadata Specification for Enhanced Cross-border Mobility Data Sharing</title>
      <link>https://arxiv.org/abs/2503.11535</link>
      <description>arXiv:2503.11535v1 Announce Type: new 
Abstract: Integrated and efficient mobility requires data sharing among the involved stakeholders. In this direction, regulators and transport authorities have been defining policies to foster the digitalisation and online publication of mobility data. However, the creation of several heterogeneous data portals for mobility data resulted in a fragmented ecosystem that challenges data accessibility. In this context, metadata is a key enabler to foster the findability and reusability of relevant datasets, but their interoperability across different data portals should be ensured. Moreover, each domain presents specificities on the relevant information that should be encoded through metadata. To solve these issues within the mobility domain, we present mobilityDCAT-AP, a reference metadata specification for mobility data portals specified by putting together domain experts and the Semantic Web community. We report on the work done to develop the metadata model behind mobilityDCAT-AP and the best practices followed in its implementation and publication. Finally, we describe the available educational resources and the activities performed to ensure broader adoption of mobilityDCAT-AP across mobility data portals. We present success stories from early adopters and discuss the challenges they encountered in implementing a metadata specification based on Semantic Web technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11535v1</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mario Scrocca, Lina Molinas Comet, Benjamin Witsch, Daham Mohammed Mustafa, Christoph Lange, Marco Comerio, Peter Lubrich</dc:creator>
    </item>
    <item>
      <title>Finding a Fair Scoring Function for Top-$k$ Selection: Hardness, Algorithms, and Experiments</title>
      <link>https://arxiv.org/abs/2503.11575</link>
      <description>arXiv:2503.11575v1 Announce Type: new 
Abstract: Selecting a subset of the $k$ "best" items from a dataset of $n$ items, based on a scoring function, is a key task in decision-making. Given the widespread use of automated decision-making software nowadays, it is important that the outcome of this process, called top-$k$ selection, is fair. Here we consider the problem of identifying a linear scoring function for top-$k$ selection that is fair. The function computes a score for each item as a weighted sum of its (numerical) attribute values. Additionally, the function must ensure that the subset selected is a faithful representative of the entire dataset for a minority or historically disadvantaged group. Existing algorithms do not scale effectively on large, high-dimensional datasets. Our theoretical analysis shows that in more than two dimensions, no algorithm is likely to achieve good scalability with respect to dataset size (i.e., a run time of $O(n\cdot \text{polylog}(n))$), and the computational complexity is likely to increase rapidly with dimensionality. However, there are exceptions for small values of $k$ and for this case we provide significantly faster algorithms. We also provide efficient practical variants of these algorithms. Our implementations of these take advantage of modern hardware (e.g., exploiting parallelism). For large values of $k$, we give an alternative algorithm that, while theoretically worse, performs better in practice. Experimental results on real-world datasets demonstrate the efficiency of our proposed algorithms, which achieve speed-ups of up to several orders of magnitude compared to the state of the art (SoTA).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11575v1</guid>
      <category>cs.DB</category>
      <category>cs.CC</category>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guangya Cai</dc:creator>
    </item>
    <item>
      <title>Advancing the Database of Cross-Linguistic Colexifications with New Workflows and Data</title>
      <link>https://arxiv.org/abs/2503.11377</link>
      <description>arXiv:2503.11377v1 Announce Type: cross 
Abstract: Lexical resources are crucial for cross-linguistic analysis and can provide new insights into computational models for natural language learning. Here, we present an advanced database for comparative studies of words with multiple meanings, a phenomenon known as colexification. The new version includes improvements in the handling, selection and presentation of the data. We compare the new database with previous versions and find that our improvements provide a more balanced sample covering more language families worldwide, with an enhanced data quality, given that all word forms are provided in phonetic transcription. We conclude that the new Database of Cross-Linguistic Colexifications has the potential to inspire exciting new studies that link cross-linguistic data to open questions in linguistic typology, historical linguistics, psycholinguistics, and computational linguistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11377v1</guid>
      <category>cs.CL</category>
      <category>cs.DB</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annika Tjuka, Robert Forkel, Christoph Rzymski, Johann-Mattis List</dc:creator>
    </item>
  </channel>
</rss>
