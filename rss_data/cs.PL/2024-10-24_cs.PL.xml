<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PL</link>
    <description>cs.PL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 24 Oct 2024 04:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Charon: An Analysis Framework for Rust</title>
      <link>https://arxiv.org/abs/2410.18042</link>
      <description>arXiv:2410.18042v1 Announce Type: new 
Abstract: With the explosion in popularity of the Rust programming language, a wealth of tools have recently been developed to analyze, verify, and test Rust programs. Alas, the Rust ecosystem remains relatively young, meaning that every one of these tools has had to re-implement difficult, time-consuming machinery to interface with the Rust compiler and its cargo build system, to hook into the Rust compiler's internal representation, and to expose an abstract syntax tree (AST) that is suitable for analysis rather than optimized for efficiency. We address this missing building block of the Rust ecosystem, and propose Charon, an analysis framework for Rust. Charon acts as a swiss-army knife for analyzing Rust programs, and deals with all of the tedium above, providing clients with a clean, stable AST that can serve as the foundation of many analyses. We demonstrate the usefulness of Charon through a series of case studies, ranging from a Rust verification framework (Aeneas), a compiler from Rust to C (Eurydice), and a novel taint-checker for cryptographic code. To drive the point home, we also re-implement a popular existing analysis (Rudra), and show that it can be replicated by leveraging the Charon framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18042v1</guid>
      <category>cs.PL</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Son Ho, Guillaume Boisseau, Lucas Franceschino, Yoann Prak, Aymeric Fromherz, Jonathan Protzenko</dc:creator>
    </item>
    <item>
      <title>Full Version: (De/Re)-Composition of Data-Parallel Computations via Multi-Dimensional Homomorphisms</title>
      <link>https://arxiv.org/abs/2405.05118</link>
      <description>arXiv:2405.05118v3 Announce Type: replace 
Abstract: We formally introduce a systematic (de/re)-composition approach, based on the algebraic formalism of "Multi-Dimensional Homomorphisms (MDHs)". Our approach is designed as general enough to be applicable to a wide range of data-parallel computations and for various kinds of target parallel architectures. To efficiently target the deep and complex memory and core hierarchies of contemporary architectures, we exploit our introduced (de/re)-composition approach for a correct-by-construction, parametrized cache blocking and parallelization strategy. We show that our approach is powerful enough to express, in the same formalism, the (de/re)-composition strategies of different classes of state-of-the-art approaches (scheduling-based, polyhedral, etc), and we demonstrate that the parameters of our strategies enable systematically generating code that can be fully automatically optimized (auto-tuned) for the particular target architecture and characteristics of the input and output data (e.g., their sizes and memory layouts). Particularly, our experiments confirm that via auto-tuning, we achieve higher performance than state-of-the-art approaches, including hand-optimized solutions provided by vendors (such as NVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a variety of data-parallel computations, including: linear algebra routines, stencil and quantum chemistry computations, data mining algorithms, and computations that recently gained high attention due to their relevance for deep learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05118v3</guid>
      <category>cs.PL</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3665643</arxiv:DOI>
      <arxiv:journal_reference>ACM Trans. Program. Lang. Syst. (May 2024)</arxiv:journal_reference>
      <dc:creator>Ari Rasch</dc:creator>
    </item>
    <item>
      <title>On Computational Indistinguishability and Logical Relations</title>
      <link>https://arxiv.org/abs/2408.17340</link>
      <description>arXiv:2408.17340v2 Announce Type: replace 
Abstract: A $\lambda$-calculus is introduced in which all programs can be evaluated in probabilistic polynomial time and in which there is sufficient structure to represent sequential cryptographic constructions and adversaries for them, even when the latter are oracle-based. A notion of observational equivalence capturing computational indistinguishability and a class of approximate logical relations are then presented, showing that the latter represent a sound proof technique for the former. The work concludes with the presentation of an example of a security proof in which the encryption scheme induced by a pseudorandom function is proven secure against active adversaries in a purely equational style.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17340v2</guid>
      <category>cs.PL</category>
      <category>cs.CR</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ugo Dal Lago, Zeinab Galal, Giulia Giusti</dc:creator>
    </item>
    <item>
      <title>Structural temporal logic for mechanized program verification</title>
      <link>https://arxiv.org/abs/2410.14906</link>
      <description>arXiv:2410.14906v2 Announce Type: replace 
Abstract: Mechanized verification of liveness properties for programs with effects, nondeterminism, and nontermination is difficult. Existing temporal reasoning frameworks operate on the level of models (traces, automata) not executable code, creating a verification gap and losing the benefits of modularity and composition enjoyed by structural program logics. Reasoning about infinite traces and automata requires complex (co-)inductive proof techniques and familiarity with proof assistant mechanics (e.g., guardedness checker). We propose a structural approach to the verification of temporal properties with a new temporal logic that we call Ticl. Using Ticl, we internalize complex (co-)inductive proof techniques to structural lemmas and reasoning about variants and invariants. We show that it is possible to perform mechanized proofs of general temporal properties, while working in a high-level of abstraction. We demonstrate the benefits of ticl by giving short, structural proofs of safety and liveness properties for programs with queues, secure memory, and distributed consensus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14906v2</guid>
      <category>cs.PL</category>
      <category>cs.LO</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eleftherios Ioannidis, Yannick Zakowski, Steve Zdancewic, Sebastian Angel</dc:creator>
    </item>
    <item>
      <title>C-lisp and Flexible Macro Programming with S-expressions</title>
      <link>https://arxiv.org/abs/2410.16690</link>
      <description>arXiv:2410.16690v2 Announce Type: replace 
Abstract: Llama$.$lisp is a compiler framework intended to target offload processor backends such as GPUs, using intermediate representation languages (IRs) that are device-agnostic. The Llama$.$lisp IRs are formulated as S-expressions. This makes them easy to generate using higher level programming languages, which is one of the primary goals for Llama$.$lisp. The highest IR layer currently implemented in Llama$.$lisp is C-Lisp. In this paper, we describe the macro system developed for the Llama$.$lisp compiler framework. We show how we implemented FFI bindings as an example of this system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16690v2</guid>
      <category>cs.PL</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Vedanth Padmaraman, Sasank Chilamkurthy</dc:creator>
    </item>
    <item>
      <title>Understanding the Effectiveness of Large Language Models in Detecting Security Vulnerabilities</title>
      <link>https://arxiv.org/abs/2311.16169</link>
      <description>arXiv:2311.16169v3 Announce Type: replace-cross 
Abstract: While automated vulnerability detection techniques have made promising progress in detecting security vulnerabilities, their scalability and applicability remain challenging. The remarkable performance of Large Language Models (LLMs), such as GPT-4 and CodeLlama, on code-related tasks has prompted recent works to explore if LLMs can be used to detect vulnerabilities. In this paper, we perform a more comprehensive study by concurrently examining a higher number of datasets, languages and LLMs, and qualitatively evaluating performance across prompts and vulnerability classes while addressing the shortcomings of existing tools. Concretely, we evaluate the effectiveness of 16 pre-trained LLMs on 5,000 code samples from five diverse security datasets. These balanced datasets encompass both synthetic and real-world projects in Java and C/C++ and cover 25 distinct vulnerability classes.
  Overall, LLMs across all scales and families show modest effectiveness in detecting vulnerabilities, obtaining an average accuracy of 62.8% and F1 score of 0.71 across datasets. They are significantly better at detecting vulnerabilities only requiring intra-procedural analysis, such as OS Command Injection and NULL Pointer Dereference. Moreover, they report higher accuracies on these vulnerabilities than popular static analysis tools, such as CodeQL.
  We find that advanced prompting strategies that involve step-by-step analysis significantly improve performance of LLMs on real-world datasets in terms of F1 score (by upto 0.18 on average). Interestingly, we observe that LLMs show promising abilities at performing parts of the analysis correctly, such as identifying vulnerability-related specifications and leveraging natural language information to understand code behavior (e.g., to check if code is sanitized). We expect our insights to guide future work on LLM-augmented vulnerability detection systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.16169v3</guid>
      <category>cs.CR</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Avishree Khare, Saikat Dutta, Ziyang Li, Alaia Solko-Breslin, Rajeev Alur, Mayur Naik</dc:creator>
    </item>
    <item>
      <title>Dependency-Aware Compilation for Surface Code Quantum Architectures</title>
      <link>https://arxiv.org/abs/2311.18042</link>
      <description>arXiv:2311.18042v2 Announce Type: replace-cross 
Abstract: Practical applications of quantum computing depend on fault-tolerant devices with error correction. Today, the most promising approach is a class of error-correcting codes called surface codes. We study the problem of compiling quantum circuits for quantum computers implementing surface codes. Optimal or near-optimal compilation is critical for both efficiency and correctness. The compilation problem requires (1) mapping circuit qubits to the device qubits and (2) routing execution paths between interacting qubits. We solve this problem efficiently and near-optimally with a novel algorithm that exploits the dependency structure of circuit operations to formulate discrete optimization problems that can be approximated via simulated annealing, a classic and simple algorithm. Our extensive evaluation shows that our approach is powerful and flexible for compiling realistic workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18042v2</guid>
      <category>quant-ph</category>
      <category>cs.PL</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Abtin Molavi, Amanda Xu, Swamit Tannu, Aws Albarghouthi</dc:creator>
    </item>
  </channel>
</rss>
