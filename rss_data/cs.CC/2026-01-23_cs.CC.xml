<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 Jan 2026 05:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Verified polynomial-time reductions in Lean 4: formalizing the complexity of decision-relevant information</title>
      <link>https://arxiv.org/abs/2601.15571</link>
      <description>arXiv:2601.15571v1 Announce Type: new 
Abstract: We present a Lean 4 framework for polynomial-time reductions and complexity-theory proofs, and use it to formalize the complexity of identifying decision-relevant information. Problem: given a decision problem, which coordinates suffice to compute an optimal action? (SUFFICIENCY-CHECK; explicit encodings). Verified complexity results (Lean): coNP-complete; $(1-\varepsilon)\ln n$ inapproximable (from SET-COVER); $2^{\Omega(n)}$ lower bounds under ETH for succinct encodings; W[2]-hard for a natural parameterization; and a dichotomy between explicit and succinct models. Formalization contributions: bundled Karp reductions with polynomial-time witnesses; composition lemmas/tactics; and templates for NP/coNP and $\Sigma_2^P$ membership and hardness. Scale: about 5,600 lines of Lean across 36 files, with 230+ theorems and explicit polynomial bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15571v1</guid>
      <category>cs.CC</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tristan Simas</dc:creator>
    </item>
    <item>
      <title>DNF formulas are efficiently testable with relative error</title>
      <link>https://arxiv.org/abs/2601.16076</link>
      <description>arXiv:2601.16076v1 Announce Type: new 
Abstract: We give a poly$(s,1/\epsilon)$-query algorithm for testing whether an unknown and arbitrary function $f: \{0,1\}^n \to \{0,1\}$ is an $s$-term DNF, in the challenging relative-error framework for Boolean function property testing that was recently introduced and studied in a number of works [CDH+25b, CPPS25a, CPPS25b, CDH+25a]. This gives the first example of a rich and natural class of functions which may depend on a super-constant number of variables and yet is efficiently testable in the relative-error model with constant query complexity.
  A crucial new ingredient enabling our approach is a novel decomposition of any $s$-term DNF formula into ``local clusters'' of terms. Our results demonstrate that this new decomposition can be usefully exploited for algorithms even when the $s$-term DNF is not explicitly given; we believe that this decomposition may have applications in other contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.16076v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xi Chen, William Pires, Toniann Pitassi, Rocco A. Servedio</dc:creator>
    </item>
    <item>
      <title>The computational two-way quantum capacity</title>
      <link>https://arxiv.org/abs/2601.15393</link>
      <description>arXiv:2601.15393v1 Announce Type: cross 
Abstract: Quantum channel capacities are fundamental to quantum information theory. Their definition, however, does not limit the computational resources of sender and receiver. In this work, we initiate the study of computational quantum capacities. These quantify how much information can be reliably transmitted when imposing the natural requirement that en- and decoding have to be computationally efficient. We focus on the computational two-way quantum capacity and showcase that it is closely related to the computational distillable entanglement of the Choi state of the channel. This connection allows us to show a stark computational capacity separation. Under standard cryptographic assumptions, there exists a quantum channel of polynomial complexity whose computational two-way quantum capacity vanishes while its unbounded counterpart is nearly maximal. More so, we show that there exists a sharp transition in computational quantum capacity from nearly maximal to zero when the channel complexity leaves the polynomial realm. Our results demonstrate that the natural requirement of computational efficiency can radically alter the limits of quantum communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15393v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.CR</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johannes Jakob Meyer, Jacopo Rizzo, Asad Raza, Lorenzo Leone, Sofiene Jerbi, Jens Eisert</dc:creator>
    </item>
    <item>
      <title>An Efficient Algorithm to Generate all Labeled Triangle-free Graphs with a given Graphical Degree Sequence</title>
      <link>https://arxiv.org/abs/2601.15943</link>
      <description>arXiv:2601.15943v1 Announce Type: cross 
Abstract: We extend our previous algorithm that generates all labeled graphs with a given graphical degree sequence to generate all labeled triangle-free graphs with a given graphical degree sequence. The algorithm uses various pruning techniques to avoid having to first generate all labeled realizations of the input sequence and then testing whether each labeled realization is triangle-free. It can be further extended to generate all labeled bipartite graphs with a given graphical degree sequence by adding a simple test whether each generated triangle-free realization is a bipartite graph. All output graphs are generated in the lexicographical ordering as in the original algorithm. The algorithms can also be easily parallelized.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15943v1</guid>
      <category>math.CO</category>
      <category>cs.CC</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kai Wang</dc:creator>
    </item>
    <item>
      <title>Graph-Based Deterministic Polynomial Algorithm for NP Problems</title>
      <link>https://arxiv.org/abs/2508.13166</link>
      <description>arXiv:2508.13166v4 Announce Type: replace 
Abstract: The P versus NP problem asks whether every problem in NP, whose membership can be verified in polynomial time given a suitable certificate, can be decided by a deterministic Turing machine in polynomial time. In this paper, we present a proof that P = NP by constructing a deterministic polynomial-time algorithm for NP problems based on a graph-based computation framework.
  We introduce a structured computation model in which the transitions of a deterministic Turing machine are incrementally realized in the corresponding computation graph via edge extensions. Each extension step enforces a local feasibility condition that preserves consistency with valid NP verification paths across all possible certificates, ensuring that the maintained computation graph remains feasible at every stage. The total number of extension steps is polynomially bounded in the input size, and each step can be verified in polynomial time.
  As a result, the entire graph construction process runs in deterministic polynomial time and decides NP problems without enumerating certificates. This provides a direct and constructive resolution of the P versus NP question. Our result has significant implications for cryptography, combinatorial optimization, and artificial intelligence, where NP-complete problems play a central role.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13166v4</guid>
      <category>cs.CC</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changryeol Lee (Department of Software, Yonsei University, Mirae Campus)</dc:creator>
    </item>
  </channel>
</rss>
