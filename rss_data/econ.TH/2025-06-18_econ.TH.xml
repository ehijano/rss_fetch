<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.TH</link>
    <description>econ.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Jun 2025 01:29:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Smart contracts and reaction-function games</title>
      <link>https://arxiv.org/abs/2506.14413</link>
      <description>arXiv:2506.14413v1 Announce Type: new 
Abstract: Blockchain-based smart contracts offer a new take on credible commitment, where players can commit to actions in reaction to actions of others. Such reaction-function games extend on strategic games with players choosing reaction functions instead of strategies. We formalize a solution concept in terms of fixed points for such games, akin to Nash equilibrium, and prove equilibrium existence. Reaction functions can mimic "trigger" strategies from folk theorems on infinitely repeated games -- but now in a one-shot setting. We introduce a refinement in terms of safe play. We apply our theoretical framework to symmetric investment games, which includes two prominent classes of games, namely weakest-link and public-good games. In both cases, we identify a safe and optimal reaction function. In this way, our findings highlight how blockchain-based commitment can overcome trust and free-riding barriers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14413v1</guid>
      <category>econ.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jens Gudmundsson, Jens Leth Hougaard</dc:creator>
    </item>
    <item>
      <title>Revealed Information</title>
      <link>https://arxiv.org/abs/2411.13293</link>
      <description>arXiv:2411.13293v2 Announce Type: replace 
Abstract: An analyst observes the frequency with which a decision maker (DM) takes actions, but not the frequency conditional on payoff-relevant states. We ask when the analyst can rationalize the DM's choices as if the DM first learns something about the state before acting. We provide a support-function characterization of the triples of utility functions, prior beliefs, and (marginal) distributions over actions such that the DM's action distribution is consistent with information given the DM's prior and utility function. Assumptions on the cardinality of the state space and the utility function allow us to refine this characterization, obtaining a sharp system of finitely many inequalities the utility function, prior, and action distribution must satisfy. We apply our characterization to study comparative statics and to identify conditions under which a single information structure rationalizes choices across multiple decision problems. We characterize the set of distributions over posterior beliefs that are consistent with the DM's choices. We extend our results to settings with a continuum of actions and states assuming the first-order approach applies, and to simple multi-agent settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13293v2</guid>
      <category>econ.TH</category>
      <category>econ.EM</category>
      <category>math.OC</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laura Doval, Ran Eilat, Tianhao Liu, Yangfan Zhou</dc:creator>
    </item>
    <item>
      <title>Non-Monetary Mechanism Design without Distributional Information: Using Scarce Audits Wisely</title>
      <link>https://arxiv.org/abs/2502.08412</link>
      <description>arXiv:2502.08412v2 Announce Type: replace-cross 
Abstract: We study a repeated resource allocation problem with strategic agents where monetary transfers are disallowed and the central planner has no prior information on agents' utility distributions. In light of Arrow's impossibility theorem, acquiring information about agent preferences through some form of feedback is necessary. We assume that the central planner can request powerful but expensive audits on the winner in any round, revealing the true utility of the winner in that round. We design a mechanism achieving $T$-independent $O(K^2)$ social welfare regret while only requesting $O(K^3 \log T)$ audits in expectation, where $K$ is the number of agents and $T$ is the number of rounds. We also show an $\Omega(K)$ lower bound on the regret and an $\Omega(1)$ lower bound on the number of audits when having low regret. Algorithmically, we show that incentive-compatibility can be mostly enforced via the imposition of adaptive future punishments, where the audit probability is inversely proportional to the winner's future winning probability. To accurately estimate such probabilities in presence of strategic agents, who may adversely react to any potential misestimate, we introduce a flagging component that allows agents to flag any biased estimate (we show that doing so aligns with individual incentives). On the technical side, without a unique and known distribution, one cannot apply the revelation principle and conclude that truthful reporting is exactly an equilibrium. Instead, we characterize the equilibrium via a reduction to a simpler auxiliary game, in which agents cannot strategize until close to the end of the game; we show equilibria in this game can induce equilibria in the actual, fully strategic game. The tools developed therein may be of independent interest for other mechanism design problems in which the revelation principle cannot be readily applied.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08412v2</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Dai, Moise Blanchard, Patrick Jaillet</dc:creator>
    </item>
    <item>
      <title>The Backfiring Effect of Weak AI Safety Regulation</title>
      <link>https://arxiv.org/abs/2503.20848</link>
      <description>arXiv:2503.20848v2 Announce Type: replace-cross 
Abstract: Recent policy proposals aim to improve the safety of general-purpose AI, but there is little understanding of the efficacy of different regulatory approaches to AI safety. We present a strategic model that explores the interactions between safety regulation, the general-purpose AI creators, and domain specialists--those who adapt the technology for specific applications. Our analysis examines how different regulatory measures, targeting different parts of the AI development chain, affect the outcome of this game. In particular, we assume AI technology is characterized by two key attributes: safety and performance. The regulator first sets a minimum safety standard that applies to one or both players, with strict penalties for non-compliance. The general-purpose creator then invests in the technology, establishing its initial safety and performance levels. Next, domain specialists refine the AI for their specific use cases, updating the safety and performance levels and taking the product to market. The resulting revenue is then distributed between the specialist and generalist through a revenue-sharing parameter. Our analysis reveals two key insights: First, weak safety regulation imposed predominantly on domain specialists can backfire. While it might seem logical to regulate AI use cases, our analysis shows that weak regulations targeting domain specialists alone can unintentionally reduce safety. This effect persists across a wide range of settings. Second, in sharp contrast to the previous finding, we observe that stronger, well-placed regulation can in fact mutually benefit all players subjected to it. When regulators impose appropriate safety standards on both general-purpose AI creators and domain specialists, the regulation functions as a commitment device, leading to safety and performance gains, surpassing what is achieved under no regulation or regulating one player alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20848v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>econ.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Laufer, Jon Kleinberg, Hoda Heidari</dc:creator>
    </item>
  </channel>
</rss>
