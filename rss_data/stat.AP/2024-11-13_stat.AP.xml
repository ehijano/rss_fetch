<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 14 Nov 2024 02:46:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 13 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Targeting mediating mechanisms of social disparities with an interventional effects framework, applied to the gender pay gap in West Germany</title>
      <link>https://arxiv.org/abs/2411.07368</link>
      <description>arXiv:2411.07368v1 Announce Type: new 
Abstract: The Oaxaca-Blinder decomposition is a widely used method to explain social disparities. However, assigning causal meaning to its estimated components requires strong assumptions that often lack explicit justification. This paper emphasizes the importance of clearly defined estimands and their identification when targeting mediating mechanisms of social disparities. Three approaches are distinguished based on their scientific questions and assumptions: a mediation approach and two interventional approaches. The Oaxaca-Blinder decomposition and Monte Carlo simulation-based g-computation are discussed for estimation in relation to these approaches. The latter method is used in an interventional effects analysis of the observed gender pay gap in West Germany, using data from the 2017 German Socio-Economic Panel. Ten mediators, including indicators of human capital and job characteristics, are considered. Key findings indicate that the gender pay gap in log hourly wages could be reduced by up to 86% if these mediators were evenly distributed between women and men. Substantial reductions could be achieved by aligning full-time employment and work experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07368v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 13 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christiane Didden</dc:creator>
    </item>
    <item>
      <title>Review and Validation of Stochastic Ground Motion Models: which one does it better?</title>
      <link>https://arxiv.org/abs/2411.07401</link>
      <description>arXiv:2411.07401v1 Announce Type: new 
Abstract: Stochastic ground motion models (GMMs) are gaining popularity and momentum among engineers to perform time-history analysis of structures and infrastructures. This paper aims to review and validate hierarchical stochastic GMMs, with a focus on identifying their ''optimal'' configuration. We introduce the word ''hierarchical'' as its formulation contains two steps:(1) selecting a modulated filtered white noise model (MFWNM) to replicate a target record and (2) constructing a joint probability density function (PDF) for the parameters of the selected MFWNM, accounting for the record-to-record variability. In the first step, we review the development of MFWNMs and explore the ''optimal'' modeling of time-varying spectral content. Specifically, we investigate different frequency filters (single- and multi-mode) and various trends (constant, linear, and non-parametric) to describe the filters' time-varying properties. In the second step, the joint PDF is decomposed into a product of marginal distributions and a correlation structure, represented by copula models. We explore two copula models: the Gaus-sian copula and the R-vine copula. The hierarchical GMMs are evaluated by comparing specific statistical metrics, calculated from 1,001 real strong motions, with those derived from their corresponding synthetic dataset. Based on the selected validation metrics, we conclude that (1) Hierarchical stochastic GMMs can generate ground motions with high statistical compatibility to the real datasets, in terms of four key intensity measures and linear- and nonlinear-response spectra; (2) A parsimonious 11-parameter MFWNM, incorporating either the Gaussian copula or the R-vine copula, offers sufficient and similar accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07401v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 13 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maijia Su, Mayssa Dabaghi, Marco Broccardo</dc:creator>
    </item>
    <item>
      <title>Cluster globally, Reduce locally: Scalable efficient dictionary compression for magnetic resonance fingerprinting</title>
      <link>https://arxiv.org/abs/2411.07415</link>
      <description>arXiv:2411.07415v1 Announce Type: new 
Abstract: With the rapid advancements in medical data acquisition and production, increasingly richer representations exist to characterize medical information. However, such large-scale data do not usually meet computing resource constraints or algorithmic complexity, and can only be processed after compression or reduction, at the potential loss of information. In this work, we consider specific Gaussian mixture models (HD-GMM), tailored to deal with high dimensional data and to limit information loss by providing component-specific lower dimensional representations. We also design an incremental algorithm to compute such representations for large data sets, overcoming hardware limitations of standard methods. Our procedure is illustrated in a magnetic resonance fingerprinting study, where it achieves a 97% dictionary compression for faster and more accurate map reconstructions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07415v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 13 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geoffroy Oudoumanessah, Thomas Coudert, Luc Meyer, Aurelien Delphin, Michel Dojat, Carole Lartizien, Florence Forbes</dc:creator>
    </item>
    <item>
      <title>Modeling Alzheimer's Disease: Bayesian Copula Graphical Model from Demographic, Cognitive, and Neuroimaging Data</title>
      <link>https://arxiv.org/abs/2411.07745</link>
      <description>arXiv:2411.07745v1 Announce Type: new 
Abstract: The early detection of Alzheimer's disease (AD) requires the understanding of the relations between a wide range of disease-related features. Analyses that estimate these relations and evaluate their uncertainty are still rare. We address this gap by presenting a Bayesian approach using a Gaussian copula graphical model (GCGM). This model is able to estimate the relations between both continuous, discrete, and binary variables and compute the uncertainty of these estimates. Our method estimates the relations between brain-region specific gray matter volume and glucose uptake, amyloid levels, demographic information, and cognitive test scores. We applied our model to 1022 participants across different stages of AD. We found three indirect pathways through which old age reduces cognition: hippocampal volume loss, posterior cingulate cortex (PCC) volume loss, and amyloid accumulation. Corrected for other variables, we found that women perform better on cognitive tests, but also discovered four indirect pathways that dampen this association in women: lower hippocampal volume, lower PCC volume, more amyloid accumulation and less education. We found limited relations between brain-region specific glucose uptake and cognition, but did discover that the hippocampus and PCC volumes are related to cognition. These results showcase that the novel use of GCGMs can offer valuable insights into AD pathogenesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07745v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 13 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Lucas Vogels, Reza Mohammadi, Marit Schoonhoven, S. Ilker Birbil, Martin Dyrba</dc:creator>
    </item>
    <item>
      <title>A dynamic latent space time series model to assess the spread of mumps in England</title>
      <link>https://arxiv.org/abs/2411.07749</link>
      <description>arXiv:2411.07749v1 Announce Type: new 
Abstract: This work is motivated by an original dataset of reported mumps cases across nine regions of England, and focuses on the modeling of temporal dynamics and time-varying dependency patterns between the observed time series. The goal is to discover the possible presence of latent routes of contagion that go beyond the geographical locations of the regions, and instead may be explained through other non directly observable socio-economic factors. We build upon the recent statistics literature and extend the existing count time series network models by adopting a time-varying latent distance network model. This approach can efficiently capture across-series and across-time dependencies, which are both not directly observed from the data. We adopt a Bayesian hierarchical framework and perform parameter estimation using L-BFGS optimization and Hamiltonian Monte Carlo. We demonstrate with several simulation experiments that the model parameters can be accurately estimated under a variety of realistic dependency settings. Our real data application on mumps cases leads to a detailed view of some possible contagion routes. A critical advantage of our methodology is that it permits clear and interpretable visualizations of the complex relations between the time series and how these relations may evolve over time. The geometric nature of the latent embedding provides useful model based summaries. In particular, we show how to extract a measure of contraction of the inferred latent space, which can be interpreted as an overall risk for the escalation of contagion, at each point in time. Ultimately, the results highlight some possible critical transmission pathways and the role of key regions in driving infection dynamics, offering valuable perspectives that may be considered when designing public health strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07749v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 13 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hardeep Kaur, Riccardo Rastelli</dc:creator>
    </item>
    <item>
      <title>Robust estimation of carbon dioxide airborne fraction under measurement errors</title>
      <link>https://arxiv.org/abs/2411.07836</link>
      <description>arXiv:2411.07836v1 Announce Type: new 
Abstract: This paper discusses the effect of measurement errors in the estimation of the carbon dioxide (CO$_2$) airborne fraction. We are the first to present regression-based estimates and standard errors that are robust to measurement errors for the extended model, the preferred specification to estimate the CO$_2$ airborne fraction. To achieve this goal, we add to the literature in three ways: $i)$ We generalise the Deming regression to handle multiple variables. $ii)$ We introduce a bootstrap approach to construct confidence intervals for Deming regression in both univariate and multivariate scenarios. $iii)$ Propose to estimate the airborne fraction using instrumental variables (IV), taking advantage of the variation of additional measurements, to obtain consistent estimates that are robust to measurement errors. IV estimates for the airborne fraction are 44.8%($\pm$ 1.4%; 1$\sigma$) for the simple specification, and 47.3%($\pm$ 1.1%; 1$\sigma$) for the extended specification. We show that these estimates are not statistically different from the ordinary least squares (OLS) estimates, while being robust to measurement errors without relying on additional assumptions. In contrast, OLS estimates are shown to fall outside the confidence interval of the Deming regression estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07836v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 13 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. Eduardo Vera-Vald\'es, Charisios Grivas</dc:creator>
    </item>
    <item>
      <title>Markov Processes for Enhanced Deepfake Generation and Detection</title>
      <link>https://arxiv.org/abs/2411.07993</link>
      <description>arXiv:2411.07993v1 Announce Type: new 
Abstract: New and existing methods for generating, and especially detecting, deepfakes are investigated and compared on the simple problem of authenticating coin flip data. Importantly, an alternative approach to deepfake generation and detection, which uses a Markov Observation Model (MOM) is introduced and compared on detection ability to the traditional Generative Adversarial Network (GAN) approach as well as Support Vector Machine (SVM), Branching Particle Filtering (BPF) and human alternatives. MOM was also compared on generative and discrimination ability to GAN, filtering and humans (as SVM does not have generative ability). Humans are shown to perform the worst, followed in order by GAN, SVM, BPF and MOM, which was the best at the detection of deepfakes. Unsurprisingly, the order was maintained on the generation problem with removal of SVM as it does not have generation ability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07993v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 13 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jyoti Bhadana, Michael A. Kouritzin, Seoyeon Park, Ian Zhang</dc:creator>
    </item>
    <item>
      <title>R Package moodlequizR: Fully Randomized Moodle Tests</title>
      <link>https://arxiv.org/abs/2411.08032</link>
      <description>arXiv:2411.08032v1 Announce Type: new 
Abstract: This article describes the R package moodlequizR, which allows the user to easily create fully randomized quizzes and exams for Moodle, or indeed any online assessment platform that uses XML files for importing questions. In such a quiz the students are presented with the essentially same problem, but with various parts sufficiently different to make cheating very difficult. For example, the problem might require the students to find the sample mean but each student is presented with a different data set. Moodle does include some facilities for randomization, but these are rudimentary and wholly insufficient for a course in Statistics. The package is available on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08032v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 13 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wolfgang Rolke</dc:creator>
    </item>
    <item>
      <title>Ozone level forecasting in Mexico City with temporal features and interactions</title>
      <link>https://arxiv.org/abs/2411.07259</link>
      <description>arXiv:2411.07259v1 Announce Type: cross 
Abstract: Tropospheric ozone is an atmospheric pollutant that negatively impacts human health and the environment. Precise estimation of ozone levels is essential for preventive measures and mitigating its effects. This work compares the accuracy of multiple regression models in forecasting ozone levels in Mexico City, first without adding temporal features and interactions, and then with these features included. Our findings show that incorporating temporal features and interactions improves the accuracy of the models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07259v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 13 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>J. M. S\'anchez Cerritos, J. A. Mart\'inez-Cadena, A. Mar\'in-L\'opez, J. Delgado-Fern\'andez</dc:creator>
    </item>
    <item>
      <title>Cumulative differences between subpopulations versus body mass index in the Behavioral Risk Factor Surveillance System data</title>
      <link>https://arxiv.org/abs/2411.07399</link>
      <description>arXiv:2411.07399v1 Announce Type: cross 
Abstract: Prior works have demonstrated many advantages of cumulative statistics over the classical methods of reliability diagrams, ECEs (empirical, estimated, or expected calibration errors), and ICIs (integrated calibration indices). The advantages pertain to assessing calibration of predicted probabilities, comparison of responses from a subpopulation to the responses from the full population, and comparison of responses from one subpopulation to those from a separate subpopulation. The cumulative statistics include graphs of cumulative differences as a function of the scalar covariate, as well as metrics due to Kuiper and to Kolmogorov and Smirnov that summarize the graphs into single scalar statistics and associated P-values (also known as "attained significance levels" for significance tests). However, the prior works have not yet treated data from biostatistics.
  Fortunately, the advantages of the cumulative statistics extend to the Behavioral Risk Factor Surveillance System (BRFSS) of the Centers for Disease Control and Prevention. This is unsurprising, since the mathematics is the same as in earlier works. Nevertheless, detailed analysis of the BRFSS data is revealing and corroborates the findings of earlier works.
  Two methodological extensions beyond prior work that facilitate analysis of the BRFSS are (1) empirical estimators of uncertainty for graphs of the cumulative differences between two subpopulations, such that the estimators are valid for any real-valued responses, and (2) estimators of the weighted average treatment effect for the differences in the responses between the subpopulations. Both of these methods concern the case in which none of the covariate's observed values for one subpopulation is equal to any of the covariate's values for the other subpopulation. The data analysis presented reports results for this case as well as several others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07399v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 13 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mark Tygert</dc:creator>
    </item>
    <item>
      <title>A Theoretical Review of Area Production Rates as Test Statistics for Detecting Nonequilibrium Dynamics in Ornstein-Uhlenbeck Processes</title>
      <link>https://arxiv.org/abs/2411.07613</link>
      <description>arXiv:2411.07613v1 Announce Type: cross 
Abstract: A stochastic process is at thermodynamic equilibrium if it obeys time-reversal symmetry; forward and reverse time are statistically indistinguishable at steady state. Non-equilibrium processes break time-reversal symmetry by maintaining circulating probability currents. In physical processes, these currents require a continual use and exchange of energy. Accordingly, signatures of non-equilibrium behavior are important markers of energy use in biophysical systems. In this article we consider a particular signature of nonequilibrium behavior: area production rates. These are, the average rate at which a stochastic process traces out signed area in its projections onto coordinate planes. Area production is an example of a linear observable: a path integral over an observed trajectory against a linear vector field. We provide a summary review of area production rates in Ornstein-Uhlenbeck (OU) processes. Then, we show that, given an OU process, a weighted Frobenius norm of the area production rate matrix is the optimal test statistic for detecting nonequilibrium behavior in the sense that its coefficient of variation decays faster in the length of time observed than the coefficient of variation of any other linear observable. We conclude by showing that this test statistic estimates the entropy production rate of the process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07613v1</guid>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Wed, 13 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Strang</dc:creator>
    </item>
    <item>
      <title>Language Models as Causal Effect Generators</title>
      <link>https://arxiv.org/abs/2411.08019</link>
      <description>arXiv:2411.08019v1 Announce Type: cross 
Abstract: We present a framework for large language model (LLM) based data generation with controllable causal structure. In particular, we define a procedure for turning any language model and any directed acyclic graph (DAG) into a sequence-driven structural causal model (SD-SCM). Broadly speaking, an SD-SCM is a causal model with user-defined structure and LLM-defined structural equations. We characterize how an SD-SCM allows sampling from observational, interventional, and counterfactual distributions according to the desired causal structure. We then leverage this procedure to propose a new type of benchmark for causal inference methods, generating individual-level counterfactual data without needing to manually specify functional relationships between variables. We create an example benchmark consisting of thousands of datasets, and test a suite of popular estimation methods on these datasets for average, conditional average, and individual treatment effect estimation, both with and without hidden confounding. Apart from generating data, the same procedure also allows us to test for the presence of a causal effect that might be encoded in an LLM. This procedure can underpin auditing LLMs for misinformation, discrimination, or otherwise undesirable behavior. We believe SD-SCMs can serve as a useful tool in any application that would benefit from sequential data with controllable causal structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08019v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 13 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lucius E. J. Bynum, Kyunghyun Cho</dc:creator>
    </item>
    <item>
      <title>A Likelihood Approach to Incorporating Self-Report Data in HIV Recency Classification</title>
      <link>https://arxiv.org/abs/2309.02430</link>
      <description>arXiv:2309.02430v2 Announce Type: replace 
Abstract: Estimating new HIV infections is significant yet challenging due to the difficulty in distinguishing between recent and long-term infections. We demonstrate that HIV recency status (recent v.s. long-term) could be determined from the combination of self-report testing history and biomarkers, which are increasingly available in bio-behavioral surveys. HIV recency status is partially observed, given the self-report testing history. For example, people who tested positive for HIV over one year ago should have a long-term infection. Based on the nationally representative samples collected by the Population-based HIV Impact Assessment (PHIA) Project, we propose a likelihood-based probabilistic model for HIV recency classification. The model incorporates both labeled and unlabeled data and integrates the mechanism of how HIV recency status depends on biomarkers and the mechanism of how HIV recency status, together with the self-report time of the most recent HIV test, impacts the test results, via a set of logistic regression models. We compare our method to logistic regression and the binary classification tree (current practice) on Malawi, Zimbabwe, and Zambia PHIA data, as well as on simulated data. Our model obtains more efficient and less biased parameter estimates and is relatively robust to potential reporting error and model misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.02430v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 13 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenlong Yang, Danping Liu, Le Bao, Runze Li</dc:creator>
    </item>
    <item>
      <title>A Bayesian Framework for Causal Analysis of Recurrent Events with Timing Misalignment</title>
      <link>https://arxiv.org/abs/2304.03247</link>
      <description>arXiv:2304.03247v2 Announce Type: replace-cross 
Abstract: Observational studies of recurrent event rates are common in biomedical statistics. Broadly, the goal is to estimate differences in event rates under two treatments within a defined target population over a specified followup window. Estimation with observational data is challenging because, while membership in the target population is defined in terms of eligibility criteria, treatment is rarely observed exactly at the time of eligibility. Ad-hoc solutions to this timing misalignment can induce bias by incorrectly attributing prior event counts and person-time to treatment. Even if eligibility and treatment are aligned, a terminal event process (e.g. death) often stops the recurrent event process of interest. In practice, both processes can be censored so that events are not observed over the entire followup window. Our approach addresses misalignment by casting it as a time-varying treatment problem: some patients are on treatment at eligibility while others are off treatment but may switch to treatment at a specified time - if they survive long enough. We define and identify an average causal effect estimand under right-censoring. Estimation is done using a g-computation procedure with a joint semiparametric Bayesian model for the death and recurrent event processes. We apply the method to contrast hospitalization rates among patients with different opioid treatments using Medicare insurance claims data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.03247v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 13 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/biomtc/ujae145</arxiv:DOI>
      <dc:creator>Arman Oganisian, Anthony Girard, Jon A. Steingrimsson, Patience Moyo</dc:creator>
    </item>
  </channel>
</rss>
