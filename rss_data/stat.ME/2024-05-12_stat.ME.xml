<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 13 May 2024 04:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 13 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Local Longitudinal Modified Treatment Policies</title>
      <link>https://arxiv.org/abs/2405.06135</link>
      <description>arXiv:2405.06135v1 Announce Type: new 
Abstract: Longitudinal Modified Treatment Policies (LMTPs) provide a framework for defining a broad class of causal target parameters for continuous and categorical exposures. We propose Local LMTPs, a generalization of LMTPs to settings where the target parameter is conditional on subsets of units defined by the treatment or exposure. Such parameters have wide scientific relevance, with well-known parameters such as the Average Treatment Effect on the Treated (ATT) falling within the class. We provide a formal causal identification result that expresses the Local LMTP parameter in terms of sequential regressions, and derive the efficient influence function of the parameter which defines its semi-parametric and local asymptotic minimax efficiency bound. Efficient semi-parametric inference of Local LMTP parameters requires estimating the ratios of functions of complex conditional probabilities (or densities). We propose an estimator for Local LMTP parameters that directly estimates these required ratios via empirical loss minimization, drawing on the theory of Riesz representers. The estimator is implemented using a combination of ensemble machine learning algorithms and deep neural networks, and evaluated via simulation studies. We illustrate in simulation that estimation of the density ratios using Riesz representation might provide more stable estimators in finite samples in the presence of empirical violations of the overlap/positivity assumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06135v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Herbert Susmann, Iv\'an D\'iaz</dc:creator>
    </item>
    <item>
      <title>Bayesian factor zero-inflated Poisson model for multiple grouped count data</title>
      <link>https://arxiv.org/abs/2405.06335</link>
      <description>arXiv:2405.06335v1 Announce Type: new 
Abstract: This paper proposes a computationally efficient Bayesian factor model for multiple grouped count data. Adopting the link function approach, the proposed model can capture the association within and between the at-risk probabilities and Poisson counts over multiple dimensions. The likelihood function for the grouped count data consists of the differences of the cumulative distribution functions evaluated at the endpoints of the groups, defining the probabilities of each data point falling in the groups. The combination of the data augmentation of underlying counts, the P\'{o}lya-Gamma augmentation to approximate the Poisson distribution, and parameter expansion for the factor components is used to facilitate posterior computing. The efficacy of the proposed factor model is demonstrated using the simulated data and real data on the involvement of youths in the nineteen illegal activities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06335v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Genya Kobayashi, Yuta Yamauchi</dc:creator>
    </item>
    <item>
      <title>Next generation clinical trials: Seamless designs and master protocols</title>
      <link>https://arxiv.org/abs/2405.06353</link>
      <description>arXiv:2405.06353v1 Announce Type: new 
Abstract: Background: Drug development is often inefficient, costly and lengthy, yet it is essential for evaluating the safety and efficacy of new interventions. Compared with other disease areas, this is particularly true for Phase II / III cancer clinical trials where high attrition rates and reduced regulatory approvals are being seen. In response to these challenges, seamless clinical trials and master protocols have emerged to streamline the drug development process. Methods: Seamless clinical trials, characterized by their ability to transition seamlessly from one phase to another, can lead to accelerating the development of promising therapies while Master protocols provide a framework for investigating multiple treatment options and patient subgroups within a single trial. Results: We discuss the advantages of these methods through real trial examples and the principals that lead to their success while also acknowledging the associated regulatory considerations and challenges. Conclusion: Seamless designs and Master protocols have the potential to improve confirmatory clinical trials. In the disease area of cancer, this ultimately means that patients can receive life-saving treatments sooner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06353v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abigail Burdon, Thomas Jaki, Xijin Chen, Pavel Mozgunov, Haiyan Zheng, Richard Baird</dc:creator>
    </item>
    <item>
      <title>Accounting for selection biases in population analyses: equivalence of the in-likelihood and post-processing approaches</title>
      <link>https://arxiv.org/abs/2405.06366</link>
      <description>arXiv:2405.06366v1 Announce Type: new 
Abstract: In this paper I show the equivalence, under appropriate assumptions, of two alternative methods to account for the presence of selection biases (also called selection effects) in population studies: one is to include the selection effects in the likelihood directly; the other follows the procedure of first inferring the observed distribution and then removing selection effects a posteriori. Moreover, I investigate a potential bias allegedly induced by the latter approach: I show that this procedure, if applied under the appropriate assumptions, does not produce the aforementioned bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06366v1</guid>
      <category>stat.ME</category>
      <category>astro-ph.IM</category>
      <category>gr-qc</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefano Rinaldi</dc:creator>
    </item>
    <item>
      <title>Informativeness of Weighted Conformal Prediction</title>
      <link>https://arxiv.org/abs/2405.06479</link>
      <description>arXiv:2405.06479v1 Announce Type: new 
Abstract: Weighted conformal prediction (WCP), a recently proposed framework, provides uncertainty quantification with the flexibility to accommodate different covariate distributions between training and test data. However, it is pointed out in this paper that the effectiveness of WCP heavily relies on the overlap between covariate distributions; insufficient overlap can lead to uninformative prediction intervals. To enhance the informativeness of WCP, we propose two methods for scenarios involving multiple sources with varied covariate distributions. We establish theoretical guarantees for our proposed methods and demonstrate their efficacy through simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06479v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mufang Ying, Wenge Guo, Koulik Khamaru, Ying Hung</dc:creator>
    </item>
    <item>
      <title>The landscapemetrics and motif packages for measuring landscape patterns and processes</title>
      <link>https://arxiv.org/abs/2405.06559</link>
      <description>arXiv:2405.06559v1 Announce Type: new 
Abstract: This book chapter emphasizes the significance of categorical raster data in ecological studies, specifically land use or land cover (LULC) data, and highlights the pivotal role of landscape metrics and pattern-based spatial analysis in comprehending environmental patterns and their dynamics. It explores the usage of R packages, particularly landscapemetrics and motif, for quantifying and analyzing landscape patterns using LULC data from three distinct European regions. It showcases the computation, visualization, and comparison of landscape metrics, while also addressing additional features such as patch value extraction, sub-region sampling, and moving window computation. Furthermore, the chapter delves into the intricacies of pattern-based spatial analysis, explaining how spatial signatures are computed and how the motif package facilitates comparisons and clustering of landscape patterns. The chapter concludes by discussing the potential of customization and expansion of the presented tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06559v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jakub Nowosad, Maximilian H. K. Hesselbarth</dc:creator>
    </item>
    <item>
      <title>Simultaneously detecting spatiotemporal changes with penalized Poisson regression models</title>
      <link>https://arxiv.org/abs/2405.06613</link>
      <description>arXiv:2405.06613v1 Announce Type: new 
Abstract: In the realm of large-scale spatiotemporal data, abrupt changes are commonly occurring across both spatial and temporal domains. This study aims to address the concurrent challenges of detecting change points and identifying spatial clusters within spatiotemporal count data. We introduce an innovative method based on the Poisson regression model, employing doubly fused penalization to unveil the underlying spatiotemporal change patterns. To efficiently estimate the model, we present an iterative shrinkage and threshold based algorithm to minimize the doubly penalized likelihood function. We establish the statistical consistency properties of the proposed estimator, confirming its reliability and accuracy. Furthermore, we conduct extensive numerical experiments to validate our theoretical findings, thereby highlighting the superior performance of our method when compared to existing competitive approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06613v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zerui Zhang, Xin Wang, Xin Zhang, Jing Zhang</dc:creator>
    </item>
    <item>
      <title>Multivariate Interval-Valued Models in Frequentist and Bayesian Schemes</title>
      <link>https://arxiv.org/abs/2405.06635</link>
      <description>arXiv:2405.06635v1 Announce Type: new 
Abstract: In recent years, addressing the challenges posed by massive datasets has led researchers to explore aggregated data, particularly leveraging interval-valued data, akin to traditional symbolic data analysis. While much recent research, with the exception of Samdai et al. (2023) who focused on the bivariate case, has primarily concentrated on parameter estimation in single-variable scenarios, this paper extends such investigations to the multivariate domain for the first time. We derive maximum likelihood (ML) estimators for the parameters and establish their asymptotic distributions. Additionally, we pioneer a theoretical Bayesian framework, previously confined to the univariate setting, for multivariate data. We provide a detailed exposition of the proposed estimators and conduct comparative performance analyses. Finally, we validate the effectiveness of our estimators through simulations and real-world data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06635v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ali Sadeghkhani, Abdolnasser Sadeghkhani</dc:creator>
    </item>
    <item>
      <title>Variational Inference for Acceleration of SN Ia Photometric Distance Estimation with BayeSN</title>
      <link>https://arxiv.org/abs/2405.06013</link>
      <description>arXiv:2405.06013v1 Announce Type: cross 
Abstract: Type Ia supernovae (SNe Ia) are standarizable candles whose observed light curves can be used to infer their distances, which can in turn be used in cosmological analyses. As the quantity of observed SNe Ia grows with current and upcoming surveys, increasingly scalable analyses are necessary to take full advantage of these new datasets for precise estimation of cosmological parameters. Bayesian inference methods enable fitting SN Ia light curves with robust uncertainty quantification, but traditional posterior sampling using Markov Chain Monte Carlo (MCMC) is computationally expensive. We present an implementation of variational inference (VI) to accelerate the fitting of SN Ia light curves using the BayeSN hierarchical Bayesian model for time-varying SN Ia spectral energy distributions (SEDs). We demonstrate and evaluate its performance on both simulated light curves and data from the Foundation Supernova Survey with two different forms of surrogate posterior -- a multivariate normal and a custom multivariate zero-lower-truncated normal distribution -- and compare them with the Laplace Approximation and full MCMC analysis. To validate of our variational approximation, we calculate the pareto-smoothed importance sampling (PSIS) diagnostic, and perform variational simulation-based calibration (VSBC). The VI approximation achieves similar results to MCMC but with an order-of-magnitude speedup for the inference of the photometric distance moduli. Overall, we show that VI is a promising method for scalable parameter inference that enables analysis of larger datasets for precision cosmology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06013v1</guid>
      <category>astro-ph.IM</category>
      <category>astro-ph.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ana Sof\'ia M. Uzsoy, Stephen Thorp, Matthew Grayling, Kaisey S. Mandel</dc:creator>
    </item>
    <item>
      <title>Separating States in Astronomical Sources Using Hidden Markov Models: With a Case Study of Flaring and Quiescence on EV Lac</title>
      <link>https://arxiv.org/abs/2405.06540</link>
      <description>arXiv:2405.06540v1 Announce Type: cross 
Abstract: We present a new method to distinguish between different states (e.g., high and low, quiescent and flaring) in astronomical sources with count data. The method models the underlying physical process as latent variables following a continuous-space Markov chain that determines the expected Poisson counts in observed light curves in multiple passbands. For the underlying state process, we consider several autoregressive processes, yielding continuous-space hidden Markov models of varying complexity. Under these models, we can infer the state that the object is in at any given time. The state predictions from these models are then dichotomized with the help of a finite-mixture model to produce state classifications. We apply these techniques to X-ray data from the active dMe flare star EV Lac, splitting the data into quiescent and flaring states. We find that a first-order vector autoregressive process efficiently separates flaring from quiescence: flaring occurs over 30-40% of the observation durations, a well-defined persistent quiescent state can be identified, and the flaring state is characterized by higher temperatures and emission measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06540v1</guid>
      <category>astro-ph.SR</category>
      <category>astro-ph.HE</category>
      <category>astro-ph.IM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert Zimmerman, David A. van Dyk, Vinay L. Kashyap, Aneta Siemiginowska</dc:creator>
    </item>
    <item>
      <title>Random matrix theory improved Fr\'echet mean of symmetric positive definite matrices</title>
      <link>https://arxiv.org/abs/2405.06558</link>
      <description>arXiv:2405.06558v1 Announce Type: cross 
Abstract: In this study, we consider the realm of covariance matrices in machine learning, particularly focusing on computing Fr\'echet means on the manifold of symmetric positive definite matrices, commonly referred to as Karcher or geometric means. Such means are leveraged in numerous machine-learning tasks. Relying on advanced statistical tools, we introduce a random matrix theory-based method that estimates Fr\'echet means, which is particularly beneficial when dealing with low sample support and a high number of matrices to average. Our experimental evaluation, involving both synthetic and real-world EEG and hyperspectral datasets, shows that we largely outperform state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06558v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florent Bouchard, Ammar Mian, Malik Tiomoko, Guillaume Ginolhac, Fr\'ed\'eric Pascal</dc:creator>
    </item>
    <item>
      <title>Forecasting high-dimensional functional time series with dual-factor structures</title>
      <link>https://arxiv.org/abs/2109.04146</link>
      <description>arXiv:2109.04146v2 Announce Type: replace 
Abstract: We propose a dual-factor model for high-dimensional functional time series (HDFTS) that considers multiple populations. The HDFTS is first decomposed into a collection of functional time series (FTS) in a lower dimension and a group of population-specific basis functions. The system of basis functions describes cross-sectional heterogeneity, while the reduced-dimension FTS retains most of the information common to multiple populations. The low-dimensional FTS is further decomposed into a product of common functional loadings and a matrix-valued time series that contains the most temporal dynamics embedded in the original HDFTS. The proposed general-form dual-factor structure is connected to several commonly used functional factor models. We demonstrate the finite-sample performances of the proposed method in recovering cross-sectional basis functions and extracting common features using simulated HDFTS. An empirical study shows that the proposed model produces more accurate point and interval forecasts for subnational age-specific mortality rates in Japan. The financial benefits associated with the improved mortality forecasts are translated into a life annuity pricing scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.04146v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chen Tang, Han Lin Shang, Yanrong Yang, Yang Yang</dc:creator>
    </item>
    <item>
      <title>When Respondents Don't Care Anymore: Identifying the Onset of Careless Responding</title>
      <link>https://arxiv.org/abs/2303.07167</link>
      <description>arXiv:2303.07167v2 Announce Type: replace 
Abstract: Questionnaires in the behavioral and organizational sciences tend to be lengthy: survey measures comprising hundreds of items are the norm rather than the exception. However, literature suggests that the longer a questionnaire takes, the higher the probability that participants lose interest and start responding carelessly. Consequently, in long surveys a large number of participants may engage in careless responding, posing a major threat to internal validity. We propose a novel method for identifying the onset of careless responding (or an absence thereof) for each participant. It is based on combined measurements of multiple dimensions in which carelessness may manifest, such as inconsistency and invariability. Since a structural break in either dimension is potentially indicative of carelessness, the proposed method searches for evidence for changepoints along the combined measurements. It is highly flexible, based on machine learning, and provides statistical guarantees on its performance. An empirical application on data from a seminal study on the incidence of careless responding reveals that the reported incidence has likely been substantially underestimated due to the presence of respondents that were careless for only parts of the questionnaire. In simulation experiments, we find that the proposed method achieves high reliability in correctly identifying carelessness onset, discriminates well between careless and attentive respondents, and captures a variety of careless response types, even when a large number of careless respondents are present. Furthermore, we provide freely available open source software to enhance accessibility and facilitate adoption by empirical researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.07167v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Welz, Andreas Alfons</dc:creator>
    </item>
    <item>
      <title>Analysis of Active/Inactive Patterns in the NHANES Data using Generalized Multilevel Functional Principal Component Analysis</title>
      <link>https://arxiv.org/abs/2311.14054</link>
      <description>arXiv:2311.14054v3 Announce Type: replace 
Abstract: Between 2011 and 2014 NHANES collected objectively measured physical activity data using wrist-worn accelerometers for tens of thousands of individuals for up to seven days. Here we analyze the minute-level indicators of being active, which can be viewed as binary (because there is an active indicator at every minute), multilevel (because there are multiple days of data for each study participant), functional (because within-day data can be viewed as a function of time) data. To extract within- and between-participant directions of variation in the data, we introduce Generalized Multilevel Functional Principal Component Analysis (GM-FPCA), an approach based on the dimension reduction of the linear predictor. Scores associated with specific patterns of activity are shown to be strongly associated with time to death. Extensive simulation studies indicate that GM-FPCA provides accurate estimation of model parameters, is computationally stable, and is scalable in the number of study participants, visits, and observations within visits. R code for implementing the method is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.14054v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xinkai Zhou, Julia Wrobel, Ciprian M. Crainiceanu, Andrew Leroux</dc:creator>
    </item>
    <item>
      <title>Logistic-beta processes for dependent random probabilities with beta marginals</title>
      <link>https://arxiv.org/abs/2402.07048</link>
      <description>arXiv:2402.07048v2 Announce Type: replace 
Abstract: The beta distribution serves as a canonical tool for modelling probabilities in statistics and machine learning. However, there is limited work on flexible and computationally convenient stochastic process extensions for modelling dependent random probabilities. We propose a novel stochastic process called the logistic-beta process, whose logistic transformation yields a stochastic process with common beta marginals. Logistic-beta processes can model dependence on both discrete and continuous domains, such as space or time, and have a flexible dependence structure through correlation kernels. Moreover, its normal variance-mean mixture representation leads to effective posterior inference algorithms. We illustrate the benefits through nonparametric binary regression and conditional density estimation examples, both in simulation studies and in a pregnancy outcome application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07048v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changwoo J. Lee, Alessandro Zito, Huiyan Sang, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>On foundation of generative statistics with F-entropy: a gradient-based approach</title>
      <link>https://arxiv.org/abs/2405.05389</link>
      <description>arXiv:2405.05389v2 Announce Type: replace 
Abstract: This paper explores the interplay between statistics and generative artificial intelligence. Generative statistics, an integral part of the latter, aims to construct models that can {\it generate} efficiently and meaningfully new data across the whole of the (usually high dimensional) sample space, e.g. a new photo. Within it, the gradient-based approach is a current favourite that exploits effectively, for the above purpose, the information contained in the observed sample, e.g. an old photo. However, often there are missing data in the observed sample, e.g. missing bits in the old photo. To handle this situation, we have proposed a gradient-based algorithm for generative modelling. More importantly, our paper underpins rigorously this powerful approach by introducing a new F-entropy that is related to Fisher's divergence. (The F-entropy is also of independent interest.) The underpinning has enabled the gradient-based approach to expand its scope. For example, it can now provide a tool for Possible future projects include discrete data and Bayesian variational inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05389v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bing Cheng, Howell Tong</dc:creator>
    </item>
    <item>
      <title>Distributionally robust halfspace depth</title>
      <link>https://arxiv.org/abs/2101.00726</link>
      <description>arXiv:2101.00726v2 Announce Type: replace-cross 
Abstract: Tukey's halfspace depth can be seen as a stochastic program and as such it is not guarded against optimizer's curse, so that a limited training sample may easily result in a poor out-of-sample performance. We propose a generalized halfspace depth concept relying on the recent advances in distributionally robust optimization, where every halfspace is examined using the respective worst-case distribution in the Wasserstein ball of radius $\delta\geq 0$ centered at the empirical law. This new depth can be seen as a smoothed and regularized classical halfspace depth which is retrieved as $\delta\downarrow 0$. It inherits most of the main properties of the latter and, additionally, enjoys various new attractive features such as continuity and strict positivity beyond the convex hull of the support. We provide numerical illustrations of the new depth and its advantages, and develop some fundamental theory. In particular, we study the upper level sets and the median region including their breakdown properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2101.00726v2</guid>
      <category>math.OC</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jevgenijs Ivanovs, Pavlo Mozharovskyi</dc:creator>
    </item>
    <item>
      <title>Flexible and efficient spatial extremes emulation via variational autoencoders</title>
      <link>https://arxiv.org/abs/2307.08079</link>
      <description>arXiv:2307.08079v3 Announce Type: replace-cross 
Abstract: Many real-world processes have complex tail dependence structures that cannot be characterized using classical Gaussian processes. More flexible spatial extremes models exhibit appealing extremal dependence properties but are often exceedingly prohibitive to fit and simulate from in high dimensions. In this paper, we aim to push the boundaries on computation and modeling of high-dimensional spatial extremes via integrating a new spatial extremes model that has flexible and non-stationary dependence properties in the encoding-decoding structure of a variational autoencoder called the XVAE. The XVAE can emulate spatial observations and produce outputs that have the same statistical properties as the inputs, especially in the tail. Our approach also provides a novel way of making fast inference with complex extreme-value processes. Through extensive simulation studies, we show that our XVAE is substantially more time-efficient than traditional Bayesian inference while outperforming many spatial extremes models with a stationary dependence structure. Lastly, we analyze a high-resolution satellite-derived dataset of sea surface temperature in the Red Sea, which includes 30 years of daily measurements at 16703 grid cells. We demonstrate how to use XVAE to identify regions susceptible to marine heatwaves under climate change and examine the spatial and temporal variability of the extremal dependence structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.08079v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Likun Zhang, Xiaoyu Ma, Christopher K. Wikle, Rapha\"el Huser</dc:creator>
    </item>
  </channel>
</rss>
