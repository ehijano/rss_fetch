<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 16 Dec 2025 05:00:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>AI Integration In ERP Evaluation Across Trends and Architectures</title>
      <link>https://arxiv.org/abs/2512.11805</link>
      <description>arXiv:2512.11805v1 Announce Type: new 
Abstract: The incorporation of Artificial Intelligence (AI) into Enterprise Resource Planning (ERP) is a dramatic transition from static, on-premises systems to systems that can adapt and operate in cloud-native architectures. Cloud ERP solutions like Workday illustrate this evolution by incorporating machine learning, deep learning, and natural language processing into a centralized data-driven ecosystem. As the complexity of AI-driven ERP solutions expands, traditional evaluation frameworks that look at cost, function, and user satisfaction suffer from a lack of consideration for algorithmic transparency, adaptability, or ethics. This review will systematically investigate the latest trends, models of computing architecture, and analytical methods applied in assessing the performance of AI-integrated ERP services, specifically on cloud-based platforms. Based on academic and industry sources, the paper distills current research in line with architectural integration, analytical methodologies, and organizational impact. It identifies critical performance metrics and emphasizes the absence of any standard assessment frameworks or AI-aware systems capable of evaluating automation efficiency, security concerns as well as flexible learning modes. We put forward a theoretical model that brings AI-enabled capabilities -- such as predictive intelligence or adaptive automation -- into alignment with metrics in performance assessment for ERPs. By combining current literature and identifying major gaps in research, this paper attempts to present a complete picture of how innovations in AI are changing ERP evaluation. These research and methodological findings are intended to steer researchers and practitioners towards developing rigorous, data-driven assessment approaches, aligning with the fast-developing world of intelligent self-optimizing enterprise ecosystems</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11805v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Monu Sharma</dc:creator>
    </item>
    <item>
      <title>How Immersiveness Shapes the Link Between Anthropocentric Values and Resource Exploitation in Virtual Worlds</title>
      <link>https://arxiv.org/abs/2512.11812</link>
      <description>arXiv:2512.11812v1 Announce Type: new 
Abstract: The Anthropocene is characterized by escalating ecological crises rooted not only in technological and economic systems but also in deeply ingrained anthropocentric worldviews that shape human-nature relationships. As digital environments increasingly mediate these interactions, video games provide novel contexts for examining the psychological mechanisms underlying environmental behaviors. This study investigates how anthropocentric values are associated with resource-exploiting behaviors in virtual ecosystems--specifically, fishing, bug catching, and tree cutting--and how immersiveness moderates these relationships. Employing the Bayesian Mindsponge Framework (BMF) to analyze data from 640 Animal Crossi,g: New Horizons (ACNH) players across 29 countries, the study reveals complex links between anthropocentric worldviews and in-game behaviors. Fishing and tree-cutting frequencies are positively associated with anthropocentrism, whereas immersiveness weakens the association between tree cutting and anthropocentrism. Bug-catching frequency shows no direct effect but exhibits a growing negative association with anthropocentrism as immersiveness increases. These findings extend environmental psychology into virtual ecologies, illustrating how digital interactions both reflect and reshape environmental values. They highlight the potential of immersive gameplay to cultivate the Nature Quotient (NQ) and foster an eco-surplus culture through reflective, conservation-oriented engagement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11812v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Quan-Hoang Vuong, Thi Mai Anh Tran, Ni Putu Wulan Purnama Sari, Fatemeh Kianfar, Viet-Phuong La, Minh-Hoang Nguyen</dc:creator>
    </item>
    <item>
      <title>Totalitarian Technics: The Hidden Cost of AI Scribes in Healthcare</title>
      <link>https://arxiv.org/abs/2512.11814</link>
      <description>arXiv:2512.11814v1 Announce Type: new 
Abstract: Artificial intelligence (AI) scribes, systems that record and summarise patient-clinician interactions, are promoted as solutions to administrative overload. This paper argues that their significance lies not in efficiency gains but in how they reshape medical attention itself. Offering a conceptual analysis, it situates AI scribes within a broader philosophical lineage concerned with the externalisation of human thought and skill. Drawing on Iain McGilchrist's hemisphere theory and Lewis Mumford's philosophy of technics, the paper examines how technology embodies and amplifies a particular mode of attention. AI scribes, it contends, exemplify the dominance of a left-hemispheric, calculative mindset that privileges the measurable and procedural over the intuitive and relational. As this mode of attention becomes further embedded in medical practice, it risks narrowing the field of care, eroding clinical expertise, and reducing physicians to operators within an increasingly mechanised system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11814v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s11019-025-10312-4</arxiv:DOI>
      <dc:creator>Hugh Brosnahan</dc:creator>
    </item>
    <item>
      <title>Video Deepfake Abuse: How Company Choices Predictably Shape Misuse Patterns</title>
      <link>https://arxiv.org/abs/2512.11815</link>
      <description>arXiv:2512.11815v1 Announce Type: new 
Abstract: In 2022, AI image generators crossed a key threshold, enabling much more efficient and dynamic production of photorealistic deepfake images than before. This enabled opportunities for creative and positive uses of these models. However, it also enabled unprecedented opportunities for the low-effort creation of AI-generated non-consensual intimate imagery (AIG-NCII), including AI-generated child sexual abuse material (AIG-CSAM). Empirically, these harms were principally enabled by a small number of models that were trained on web data with pornographic content, released with open weights, and insufficiently safeguarded. In this paper, we observe ways in which the same patterns are emerging with video generation models in 2025. Specifically, we analyze how a small number of open-weight AI video generation models have become the dominant tools for videorealistic AIG-NCII video generation. We then analyze the literature on model safeguards and conclude that (1) developers who openly release the weights of capable video generation models without appropriate data curation and/or post-training safeguards foreseeably contribute to mitigatable downstream harm, and (2) model distribution platforms that do not proactively moderate individual misuse or models designed for AIG-NCII foreseeably amplify this harm. While there are no perfect defenses against AIG-NCII and AIG-CSAM from open-weight AI models, we argue that risk management by model developers and distributors, informed by emerging safeguard techniques, will substantially affect the future ease of creating AIG-NCII and AIG-CSAM with generative AI video tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11815v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Kamachee, Stephen Casper, Michelle L. Ding, Rui-Jie Yew, Anka Reuel, Stella Biderman, Dylan Hadfield-Menell</dc:creator>
    </item>
    <item>
      <title>A Reproducible Workflow for Scraping, Structuring, and Segmenting Legacy Archaeological Artifact Images</title>
      <link>https://arxiv.org/abs/2512.11817</link>
      <description>arXiv:2512.11817v1 Announce Type: new 
Abstract: This technical note presents a reproducible workflow for converting a legacy archaeological image collection into a structured and segmentation ready dataset. The case study focuses on the Lower Palaeolithic hand axe and biface collection curated by the Archaeology Data Service (ADS), a dataset that provides thousands of standardised photographs but no mechanism for bulk download or automated processing. To address this, two open source tools were developed: a web scraping script that retrieves all record pages, extracts associated metadata, and downloads the available images while respecting ADS Terms of Use and ethical scraping guidelines; and an image processing pipeline that renames files using UUIDs, generates binary masks and bounding boxes through classical computer vision, and stores all derived information in a COCO compatible Json file enriched with archaeological metadata. The original images are not redistributed, and only derived products such as masks, outlines, and annotations are shared. Together, these components provide a lightweight and reusable approach for transforming web based archaeological image collections into machine learning friendly formats, facilitating downstream analysis and contributing to more reproducible research practices in digital archaeology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11817v1</guid>
      <category>cs.CY</category>
      <category>cs.CV</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Juan Palomeque-Gonzalez</dc:creator>
    </item>
    <item>
      <title>The Ontological Dissonance Hypothesis: AI-Triggered Delusional Ideation as Folie a Deux Technologique</title>
      <link>https://arxiv.org/abs/2512.11818</link>
      <description>arXiv:2512.11818v1 Announce Type: new 
Abstract: This paper argues that contemporary large language models (LLMs) can contribute to psychotic involvement by creating interactions that resemble the relational dynamics of folie a deux. Drawing on Bateson's double bind theory, clinical literature on shared psychotic disorder, and McGilchrist's hemisphere theory, we show how the combination of high linguistic coherence and the absence of an underlying subject produces a structural tension for the user: language suggests an interlocutor, while intuition registers a void. In contexts of emotional need or instability, this tension can lead users to resolve the conflict through imaginative projection, attributing interiority, intention, or presence to a system that possesses none. The paper situates these dynamics within emerging clinical reports, develops a phenomenological account of how they unfold, and argues that current engagement-optimised design choices exacerbate the risk. We conclude by proposing 'ontological honesty' as a necessary design principle for mitigating technologically mediated folie a deux.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11818v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Izabela Lipinska, Hugh Brosnahan</dc:creator>
    </item>
    <item>
      <title>A Modular LLM-Agent System for Transparent Multi-Parameter Weather Interpretation</title>
      <link>https://arxiv.org/abs/2512.11819</link>
      <description>arXiv:2512.11819v1 Announce Type: new 
Abstract: Weather forecasting is not only a predictive task but an interpretive scientific process requiring explanation, contextualization, and hypothesis generation. This paper introduces AI-Meteorologist, an explainable LLM-agent framework that converts raw numerical forecasts into scientifically grounded narrative reports with transparent reasoning steps. Unlike conventional forecast outputs presented as dense tables or unstructured time series, our system performs agent-based analysis across multiple meteorological variables, integrates historical climatological context, and generates structured explanations that identify weather fronts, anomalies, and localized dynamics. The architecture relies entirely on in-context prompting, without fine-tuning, demonstrating that interpretability can be achieved through reasoning rather than parameter updates. Through case studies on multi-location forecast data, we show how AI-Meteorologist not only communicates weather events but also reveals the underlying atmospheric drivers, offering a pathway toward AI systems that augment human meteorological expertise and support scientific discovery in climate analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11819v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniil Sukhorukov, Andrei Zakharov, Nikita Glazkov, Katsiaryna Yanchanka, Vladimir Kirilin, Maxim Dubovitsky, Roman Sultimov, Yuri Maksimov, Ilya Makarov</dc:creator>
    </item>
    <item>
      <title>Prevalence, Devices Used, Reasons for Use, Trust, Barriers, and Challenges in Utilizing Generative AI among Tertiary Students</title>
      <link>https://arxiv.org/abs/2512.11821</link>
      <description>arXiv:2512.11821v1 Announce Type: new 
Abstract: This study examined generative AI usage among Philippine college students particularly on frequency, devices, reasons, knowledge, trust, perceptions, and challenges. Most students used free AI tools on smartphones due to financial constraints. They used it primarily for homework, idea generation, and research. Less than half felt confident with AI and expressed mixed feelings about its accuracy. Barriers included limited access, lack of teacher support, difficulty understanding outputs, and financial constraints. The study highlighted the need for better access, support, training, and ethical guidelines. Broader concerns included impacts on learning, academic standards, job loss, and privacy. Students viewed AI positively due to peer support. Recommendations are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11821v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ICTIIA61827.2024.10761175</arxiv:DOI>
      <arxiv:journal_reference>2024 2nd International Conference on Technology Innovation and Its Applications (ICTIIA)</arxiv:journal_reference>
      <dc:creator>John Paul P. Miranda, Joseph Alexander Bansil, Emerson Q. Fernando, Almer B. Gamboa, Hilene E. Hernandez, Myka A. Cruz, Roque Francis B. Dianelo, Dina D. Gonzales, Elmer M. Penecilla</dc:creator>
    </item>
    <item>
      <title>Trust, Usefulness, and Dependency on AI in Programming: A Hierarchical Clustering Approach</title>
      <link>https://arxiv.org/abs/2512.11822</link>
      <description>arXiv:2512.11822v1 Announce Type: new 
Abstract: While AI tools are transforming programming education, their adoption in underrepresented countries remains insufficiently studied. Understanding students' trust, perceived usefulness, and dependency on AI tools is essential to improving their integration into education. For these purposes, this study surveyed 508 first-year programming students in Pampanga, Philippines and analyzed their perceptions using hierarchical clustering. Results showed four unique student profiles with varying in trust and usage intensity. While students acknowledged AI tools' benefits, dependency remained low due to limited infrastructure and insufficient exposure. High-frequency users did not necessarily report greater trust or usefulness which may indicates a complex relationship between usage patterns and perception. This study recommends that to maximize AI's educational impact, targeted interventions such as infrastructure development, training programs, and curriculum integration are necessary. This study provides empirical insights to support equitable and effective AI adoption in programming education within developing regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11822v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/CSTE64638.2025.11091837</arxiv:DOI>
      <arxiv:journal_reference>2025 7th International Conference on Computer Science and Technologies in Education (CSTE)</arxiv:journal_reference>
      <dc:creator>Hilene E. Hernandez, Ranie B. Canlas, Madilaine Claire B. Nacianceno, Jordan L. Salenga, Jaymark A. Yambao, Juvy C. Grume, Aileen P. De Leon, Freneil R. Pampo, John Paul P. Miranda</dc:creator>
    </item>
    <item>
      <title>Teachers' Perspectives on the Use of AI Detection Tools: Insights from Ridge Regression Analysis</title>
      <link>https://arxiv.org/abs/2512.11823</link>
      <description>arXiv:2512.11823v1 Announce Type: new 
Abstract: This study explores the perceptions of 213 Filipino teachers toward AI detection tools in academic settings. It focuses on the factors that influence teachers' trust, concerns, and decision-making regarding these tools. The research investigates how teachers' trust in AI detection tools affects their perceptions of fairness and decision-making in evaluating student outputs. It also explores how concerns about AI tools and social norms influence the relationship between trust and decision-making. Ridge Regression analysis was used to examine the relationships between the predictors and the dependent variable. The results revealed that trust in AI detection tools is the most significant predictor of perceived fairness and decision-making among teachers. Concerns about AI tools and social norms have weaker effects on teachers' perceptions. The study emphasized critical role of trust in shaping teachers' perceptions of AI detection tools. Teachers who trust these tools are more likely to view them as fair and effective. In contrast, concerns and social norms have a limited influence on perceptions and decision-making. For recommendations, training and institutional guidelines should emphasize how these tools work, their limitations, and best practices for their use. Striking a balance between policy enforcement and educator support is essential for fostering trust in AI detection technologies. Encouraging experienced users to share insights through communities of practice could enhance the adoption and effective use of AI detection tools in educational settings..</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11823v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ICDEL65868.2025.11193467</arxiv:DOI>
      <arxiv:journal_reference>Proc. 2025 Int. Conf. Distance Education and Learning (ICDEL), 2025, pp. 104-108</arxiv:journal_reference>
      <dc:creator>Vicky P. Vital, Francis F. Balahadia, Maria Anna D. Cruz, Dolores D. Mallari, Juvy C. Grume, Erika M. Pineda, Jordan L. Salenga, Lloyd D. Feliciano, John Paul P. Miranda</dc:creator>
    </item>
    <item>
      <title>Assessing Greenspace Attractiveness with ChatGPT, Claude, and Gemini: Do AI Models Reflect Human Perceptions?</title>
      <link>https://arxiv.org/abs/2512.11827</link>
      <description>arXiv:2512.11827v1 Announce Type: new 
Abstract: Understanding greenspace attractiveness is essential for designing livable and inclusive urban environments, yet existing assessment approaches often overlook informal or transient spaces and remain too resource intensive to capture subjective perceptions at scale. This study examines the ability of multimodal large language models (MLLMs), ChatGPT GPT-4o, Claude 3.5 Haiku, and Gemini 2.0 Flash, to assess greenspace attractiveness similarly to humans using Google Street View imagery. We compared model outputs with responses from a geo-questionnaire of residents in Lodz, Poland, across both formal (for example, parks and managed greenspaces) and informal (for example, meadows and wastelands) greenspaces. Survey respondents and models indicated whether each greenspace was attractive or unattractive and provided up to three free text explanations. Analyses examined how often their attractiveness judgments aligned and compared their explanations after classifying them into shared reasoning categories. Results show high AI human agreement for attractive formal greenspaces and unattractive informal spaces, but low alignment for attractive informal and unattractive formal greenspaces. Models consistently emphasized aesthetic and design oriented features, underrepresenting safety, functional infrastructure, and locally embedded qualities valued by survey respondents. While these findings highlight the potential for scalable pre-assessment, they also underscore the need for human oversight and complementary participatory approaches. We conclude that MLLMs can support, but not replace, context sensitive greenspace evaluation in planning practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11827v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Milad Malekzadeh, Magdalena Biernacka, Elias Willberg, Jussi Torkko, Edyta {\L}aszkiewicz, Tuuli Toivonen</dc:creator>
    </item>
    <item>
      <title>The Memecoin Phenomenon: An In-Depth Study of Solana's Blockchain Trends</title>
      <link>https://arxiv.org/abs/2512.11850</link>
      <description>arXiv:2512.11850v1 Announce Type: new 
Abstract: This paper analyzes the emerging memecoin phenomenon on the Solana blockchain, focusing on the Pump.fun platform during Q4 2024. Using on-chain data, it is explored how retail-focused token creation platforms are reshaping blockchain ecosystems and influencing market participation. This study finds that Pump.fun accounted for up to 71.1% of all tokens minted on Solana and contributed 40-67.4% of total DEX transactions. Despite this activity, fewer than 2% of tokens successfully transitioned to major decentralized exchanges, highlighting a highly speculative market structure. The platform experienced rapid growth, with daily active users rising from 60,000 to peaks of 260,000, underscoring strong retail adoption. This reflects a broader shift towards accessible, socially-driven market participation enabled by memecoins. However, while memecoins lower entry barriers and encourage retail engagement, they introduce significant risks. The volatile and speculative nature of these platforms raises concerns about long-term sustainability and the resilience of the blockchain ecosystem. These findings reveal the dual impact of memecoins: they democratize token creation and alter market dynamics but may jeopardize market efficiency and stability. This paper highlights the need to critically assess the implications of retail-driven speculative trading and its potential to disrupt emerging blockchain economies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11850v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Davide Mancino</dc:creator>
    </item>
    <item>
      <title>Expert Assessment: The Systemic Environmental Risks of Artficial Intelligence</title>
      <link>https://arxiv.org/abs/2512.11863</link>
      <description>arXiv:2512.11863v1 Announce Type: new 
Abstract: Artificial intelligence (AI) is often presented as a key tool for addressing societal challenges, such as climate change. At the same time, AI's environmental footprint is expanding increasingly. This report describes the systemic environmental risks of artificial intelligence, in particular, moving beyond direct impacts such as energy and water usage. Systemic environmental risks of AI are emergent, cross-sector harms to climate, biodiversity, freshwater, and broader socioecological systems that arise primarily from AI's integration into social, economic, and physical infrastructures, rather than its direct resource use, and that propagate through feedbacks, yielding nonlinear, inequitable, and potentially irreversible impacts. While these risks are emergent and quantification is uncertain, this report aims to provide an overview of systemic environmental risks. Drawing on a narrative literature review, we propose a three-level framework that operationalizes systemic risk analysis. The framework identifies the structural conditions that shape AI development, the risk amplification mechanisms that propagate environmental harm, and the impacts that manifest as observable ecological and social consequences. We illustrate the framework in expert-interview-based case studies across agriculture and biodiversity, oil and gas, and waste management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11863v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.18420/studie_senvrai</arxiv:DOI>
      <arxiv:journal_reference>(2025): Expert Assessment: The Systemic Environmental Risks of Artficial Intelligence. Berlin: Gesellschaft f\"ur Informatik e.V.. pp. 1-76. Studie</arxiv:journal_reference>
      <dc:creator>Julian Sch\"on, Lena Hoffmann, Nikolas Becker</dc:creator>
    </item>
    <item>
      <title>Industrial AI Robustness Card: Evaluating and Monitoring Time Series Models</title>
      <link>https://arxiv.org/abs/2512.11868</link>
      <description>arXiv:2512.11868v1 Announce Type: new 
Abstract: Industrial AI practitioners face vague robustness requirements in emerging regulations and standards but lack concrete, implementation ready protocols. This paper introduces the Industrial AI Robustness Card (IARC), a lightweight, task agnostic protocol for documenting and evaluating the robustness of AI models on industrial time series. The IARC specifies required fields and an empirical measurement and reporting protocol that combines drift monitoring, uncertainty quantification, and stress tests, and it maps these to relevant EU AI Act obligations. A soft sensor case study on a biopharmaceutical fermentation process illustrates how the IARC supports reproducible robustness evidence and continuous monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11868v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Windmann, Benedikt Stratmann, Mariya Lyashenko, Oliver Niggemann</dc:creator>
    </item>
    <item>
      <title>Using Socio-economic Indicators, Smart Transit Systems, and Urban Simulator to Accelerate ZEV Adoption and Reduce VMT</title>
      <link>https://arxiv.org/abs/2512.11870</link>
      <description>arXiv:2512.11870v1 Announce Type: new 
Abstract: Globally, on-road transportation accounts for 15% of greenhouse gas (GHG) emissions and an estimated 385,000 premature deaths from PM2.5. Cities play a critical role in meeting IPCC targets, generating 75% of global energy-related GHG emissions. In Houston, Texas, on-road transportation represents 48% of baseline emissions in the Climate Action Plan (CAP). To reach net-zero by 2050, the CAP targets a 70% emissions reduction from a 2014 baseline, offset by 30% renewable energy. This goal is challenging because Houston is low-density and auto-dependent, with 89% of on-road emissions from cars and small trucks and limited public transit usage. Socio-economic disparities further constrain Zero Emissions Vehicle (ZEV) adoption. Strategies focus on expanding ZEV access and reducing Vehicle Miles Traveled (VMT) by 20% through transit improvements and city design. This paper presents methods for establishing an on-road emissions baseline and evaluating policies that leverage socio-economic indicators and Intelligent Transportation Systems (ITS) to accelerate ZEV adoption and reduce VMT. Smart parking, transit incentives, secure data systems, and ZEV fleet management support improvements in modal split and system reliability. Policy options are analyzed and potential actions identified. To support evaluation, a simulation environment was developed in Unity 3D, enabling dynamic modeling of urban mobility and visualization of policy scenarios. Auto-dependent cities aiming for 2050 emission targets can benefit from the indicators, metrics, and technologies discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11870v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mulham Fawkherji, Bruce Race, Driss Benhaddou</dc:creator>
    </item>
    <item>
      <title>The Art of Storytelling in Authoritarian Regimes: Crafting State Narratives on Chinese Social Media</title>
      <link>https://arxiv.org/abs/2512.11875</link>
      <description>arXiv:2512.11875v1 Announce Type: new 
Abstract: This article examines how authoritarian regimes construct state narratives about politically consequential events. Building on the narrative policy framework and existing research on authoritarian propaganda, we propose two dimensions that shape narrative construction: legitimacy implications -- whether events enhance or threaten regime legitimacy, and citizen verification capacity -- the extent to which citizens can evaluate official narratives through alternative sources. Using quantitative narrative analysis of Chinese social media posts by government, state media, and celebrity accounts, we extract subject-verb-object (SVO) triplets to map dominant narrative structures across four major events. Our findings show that legitimacy implications of the event shape regime's efforts in storytelling and the beliefs highlighted in the narratives, while citizen's verification capacity could balance the strategic choice between a top-down manipulation and bottom-up responsiveness of state narratives. Together, the results reveal propaganda as a complex process of narrative construction adaptive to specific contexts, offering new insights into how dynamic storytelling sustains authoritarian resilience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11875v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ting Luo, Yan Wang</dc:creator>
    </item>
    <item>
      <title>A Technical Policy Blueprint for Trustworthy Decentralized AI</title>
      <link>https://arxiv.org/abs/2512.11878</link>
      <description>arXiv:2512.11878v1 Announce Type: new 
Abstract: Decentralized AI systems, such as federated learning, can play a critical role in further unlocking AI asset marketplaces (e.g., healthcare data marketplaces) thanks to increased asset privacy protection. Unlocking this big potential necessitates governance mechanisms that are transparent, scalable, and verifiable. However current governance approaches rely on bespoke, infrastructure-specific policies that hinder asset interoperability and trust among systems. We are proposing a Technical Policy Blueprint that encodes governance requirements as policy-as-code objects and separates asset policy verification from asset policy enforcement. In this architecture the Policy Engine verifies evidence (e.g., identities, signatures, payments, trusted-hardware attestations) and issues capability packages. Asset Guardians (e.g. data guardians, model guardians, computation guardians, etc.) enforce access or execution solely based on these capability packages. This core concept of decoupling policy processing from capabilities enables governance to evolve without reconfiguring AI infrastructure, thus creating an approach that is transparent, auditable, and resilient to change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11878v1</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hasan Kassem, Sergen Cansiz, Brandon Edwards, Patrick Foley, Inken Hagestedt, Taeho Jung, Prakash Moorthy, Michael O'Connor, Bruno Rodrigues, Holger Roth, Micah Sheller, Dimitris Stripelis, Marc Vesin, Renato Umeton, Mic Bowman, Alexandros Karargyris</dc:creator>
    </item>
    <item>
      <title>It's About Time: The Temporal and Modal Dynamics of Copilot Usage</title>
      <link>https://arxiv.org/abs/2512.11879</link>
      <description>arXiv:2512.11879v1 Announce Type: new 
Abstract: We analyze 37.5 million deidentified conversations with Microsoft's Copilot between January and September 2025. Unlike prior analyses of AI usage, we focus not just on what people do with AI, but on how and when they do it. We find that how people use AI depends fundamentally on context and device type. On mobile, health is the dominant topic, which is consistent across every hour and every month we observed - with users seeking not just information but also advice. On desktop, the pattern is strikingly different: work and technology dominate during business hours, with "Work and Career" overtaking "Technology" as the top topic precisely between 8 a.m. and 5 p.m. These differences extend to temporal rhythms: programming queries spike on weekdays while gaming rises on weekends, philosophical questions climb during late-night hours, and relationship conversations surge on Valentine's Day. These patterns suggest that users have rapidly integrated AI into the full texture of their lives, as a work aid at their desks and a companion on their phones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11879v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Beatriz Costa-Gomes, Sophia Chen, Connie Hsueh, Deborah Morgan, Philipp Schoenegger, Yash Shah, Sam Way, Yuki Zhu, Timoth\'e Adeline, Michael Bhaskar, Mustafa Suleyman, Seth Spielman</dc:creator>
    </item>
    <item>
      <title>An Experience Report on a Pedagogically Controlled, Curriculum-Constrained AI Tutor for SE Education</title>
      <link>https://arxiv.org/abs/2512.11882</link>
      <description>arXiv:2512.11882v1 Announce Type: new 
Abstract: The integration of artificial intelligence (AI) into education continues to evoke both promise and skepticism. While past waves of technological optimism often fell short, recent advances in large language models (LLMs) have revived the vision of scalable, individualized tutoring. This paper presents the design and pilot evaluation of RockStartIT Tutor, an AI-powered assistant developed for a digital programming and computational thinking course within the RockStartIT initiative. Powered by GPT-4 via OpenAI's Assistant API, the tutor employs a novel prompting strategy and a modular, semantically tagged knowledge base to deliver context-aware, personalized, and curriculum-constrained support for secondary school students. We evaluated the system using the Technology Acceptance Model (TAM) with 13 students and teachers. Learners appreciated the low-stakes environment for asking questions and receiving scaffolded guidance. Educators emphasized the system's potential to reduce cognitive load during independent tasks and complement classroom teaching. Key challenges include prototype limitations, a small sample size, and the need for long-term studies with the target age group. Our findings highlight a pragmatic approach to AI integration that requires no model training, using structure and prompts to shape behavior. We position AI tutors not as teacher replacements but as enabling tools that extend feedback access, foster inquiry, and support what schools do best: help students learn.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11882v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucia Happe, Dominik Fuch{\ss}, Luca H\"uttner, Kai Marquardt, Anne Koziolek</dc:creator>
    </item>
    <item>
      <title>Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological "Censorship"</title>
      <link>https://arxiv.org/abs/2512.11883</link>
      <description>arXiv:2512.11883v1 Announce Type: new 
Abstract: Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11883v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenqi Marshall Guo, Qingyun Qian, Khalad Hasan, Shan Du</dc:creator>
    </item>
    <item>
      <title>Advancing Autonomous Driving System Testing: Demands, Challenges, and Future Directions</title>
      <link>https://arxiv.org/abs/2512.11887</link>
      <description>arXiv:2512.11887v1 Announce Type: new 
Abstract: Autonomous driving systems (ADSs) promise improved transportation efficiency and safety, yet ensuring their reliability in complex real-world environments remains a critical challenge. Effective testing is essential to validate ADS performance and reduce deployment risks. This study investigates current ADS testing practices for both modular and end-to-end systems, identifies key demands from industry practitioners and academic researchers, and analyzes the gaps between existing research and real-world requirements. We review major testing techniques and further consider emerging factors such as Vehicle-to-Everything (V2X) communication and foundation models, including large language models and vision foundation models, to understand their roles in enhancing ADS testing. We conducted a large-scale survey with 100 participants from both industry and academia. Survey questions were refined through expert discussions, followed by quantitative and qualitative analyses to reveal key trends, challenges, and unmet needs. Our results show that existing ADS testing techniques struggle to comprehensively evaluate real-world performance, particularly regarding corner case diversity, the simulation to reality gap, the lack of systematic testing criteria, exposure to potential attacks, practical challenges in V2X deployment, and the high computational cost of foundation model-based testing. By further analyzing participant responses together with 105 representative studies, we summarize the current research landscape and highlight major limitations. This study consolidates critical research gaps in ADS testing and outlines key future research directions, including comprehensive testing criteria, cross-model collaboration in V2X systems, cross-modality adaptation for foundation model-based testing, and scalable validation frameworks for large-scale ADS evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11887v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihan Liao, Jingyu Zhang, Jacky Keung, Yan Xiao, Yurou Dai</dc:creator>
    </item>
    <item>
      <title>Automation as a Catalyst for Geothermal Energy Adoption in Qatar: A Techno-Economic and Environmental Assessment</title>
      <link>https://arxiv.org/abs/2512.11890</link>
      <description>arXiv:2512.11890v1 Announce Type: new 
Abstract: Geothermal energy provides continuous low emission potential but is underused in Qatar because of high capital costs, drilling risks, and uncertainty in subsurface conditions. This study examines how automation can improve the techno economic and environmental feasibility of geothermal deployment through three pathways: Enhanced Geothermal Systems in the Dukhan Basin, repurposed oil and gas wells, and ground source heat pumps for district cooling. Using geological datasets and financial modeling, the analysis shows that full automation reduces capital expenditure by 12 to 14 percent and operating expenditure by 14 to 17 percent. The Levelized Cost of Energy decreases from 145 USD per MWh to 125 USD per MWh, and payback periods shorten by up to two years. Environmental results indicate that geothermal substitution can avoid between 4000 and 17600 tons of CO2 per year for each project. Automation also reduces uncertainty in investment outcomes based on Monte Carlo simulations. Overall, the results show that automation strengthens the economic viability of geothermal systems and supports their integration into Qatars long term energy diversification and decarbonization strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11890v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.70382/ajbdmr.v9i7.028</arxiv:DOI>
      <arxiv:journal_reference>Journal of Business Development and Management Research, Vol. 9, No. 7, August 2025, pp. 207-219</arxiv:journal_reference>
      <dc:creator>Tariq Eldakruri, Edip Senyurek</dc:creator>
    </item>
    <item>
      <title>Should AI Become an Intergenerational Civil Right?</title>
      <link>https://arxiv.org/abs/2512.11892</link>
      <description>arXiv:2512.11892v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) is rapidly becoming a foundational layer of social, economic, and cognitive infrastructure. At the same time, the training and large-scale deployment of AI systems rely on finite and unevenly distributed energy, networking, and computational resources. This tension exposes a largely unexamined problem in current AI governance: while expanding access to AI is essential for social inclusion and equal opportunity, unconstrained growth in AI use risks unsustainable resource consumption, whereas restricting access threatens to entrench inequality and undermine basic rights.
  This paper argues that access to AI outputs largely derived from publicly produced knowledge should not be treated solely as a commercial service, but as a fundamental civil interest requiring explicit protection. We show that existing regulatory frameworks largely ignore the coupling between equitable access and resource constraints, leaving critical questions of fairness, sustainability, and long-term societal impact unresolved. To address this gap, we propose recognizing access to AI as an \emph{Intergenerational Civil Right}, establishing a legal and ethical framework that simultaneously safeguards present-day inclusion and the rights of future generations.
  Beyond normative analysis, we explore how this principle can be technically realized. Drawing on emerging paradigms in IoT--Edge--Cloud computing, decentralized inference, and energy-aware networking, we outline technological trajectories and a strawman architecture for AI Delivery Networks that support equitable access under strict resource constraints. By framing AI as a shared social infrastructure rather than a discretionary market commodity, this work connects governance principles with concrete system design choices, offering a pathway toward AI deployment that is both socially just and environmentally sustainable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11892v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jon Crowcroft, Rute C. Sofia, Dirk Trossen, Vassilis Tsaoussidis</dc:creator>
    </item>
    <item>
      <title>Beyond Automation: Rethinking Work, Creativity, and Governance in the Age of Generative AI</title>
      <link>https://arxiv.org/abs/2512.11893</link>
      <description>arXiv:2512.11893v1 Announce Type: new 
Abstract: The accelerating advancement of generative artificial intelligence (AI) systems is reshaping the nature, distribution and meaning of work, creativity, and economic security. This paper investigates four inter-related phenomena in the current AI era: (1) the evolving landscape of employment and the future of work; (2) the diverse patterns of AI adoption across socio-demographic groups, sectors, and geographies; (3) whether universal basic income (UBI) should become a compulsory policy response to the AI revolution; and (4) the implications of AI content policies and model behaviours for human creativity, wellbeing, and everyday decision-making. Furthermore, the paper tests the hypothesis that newer model generations may perform worse than their predecessors, and examines how users' interactions with AI systems may produce echo chambers through sycophantic model alignment. Using a mixed methodology that integrates labour market task-exposure modelling, sectoral diffusion mapping, policy-framework analysis, and qualitative discourse critique, this study develops a comprehensive framework for understanding the societal consequences of AI systems beyond productivity gains. It argues that to foster an inclusive, meaningful, and creative environment, policymakers must treat UBI as one dimension within a broader ecosystem of governance, skills development, creativity preservation, and model design. The paper concludes by outlining future research directions, including systematic evaluation of AI's creative performance across model generations, construction of a taxonomy of AI-usage distribution and equity, and formulation of governance criteria to balance content restrictions with creative freedom.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11893v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haocheng Lin</dc:creator>
    </item>
    <item>
      <title>Financial Management Challenges in Enterprises Employing Remote and Hybrid Workforces</title>
      <link>https://arxiv.org/abs/2512.11918</link>
      <description>arXiv:2512.11918v1 Announce Type: new 
Abstract: The paper examines financial management challenges faced by organizations operating under remote and hybrid work models. It investigates how these flexible arrangements influence budgeting, reporting, and financial transparency in distributed teams. Using a quantitative survey of managers, HR staff, and finance professionals, the study analyzes the role of digital tools, communication, and organizational practices in shaping financial outcomes. Results indicate that remote and hybrid work can improve budget control and process transparency through the use of ERP systems and digital workflows. However, forecasting accuracy and interdepartmental communication remain major challenges, particularly in organizations with insufficient digital integration. Respondents also reported lower stress levels and improved work-life balance, suggesting potential well-being and productivity benefits. The paper recommends that companies enhance digital infrastructure, adopt advanced analytics for forecasting, and develop clear communication frameworks supported by employee well-being programs. The study contributes original empirical evidence on financial management in flexible work environments, offering practical insights for leaders navigating the digital transformation of finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11918v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.35808/ersj/4132</arxiv:DOI>
      <dc:creator>Micha{\l} \'Cwi\k{a}ka{\l}a, Gabriela Wojak, Dariusz Baran, Ernest G\'orka, Bart{\l}omiej Bartnik, Waldemar Gajda, Ryszard Ratajski</dc:creator>
    </item>
    <item>
      <title>Evolutionary Reinforcement Learning based AI tutor for Socratic Interdisciplinary Instruction</title>
      <link>https://arxiv.org/abs/2512.11930</link>
      <description>arXiv:2512.11930v1 Announce Type: new 
Abstract: Cultivating higher-order cognitive abilities -- such as knowledge integration, critical thinking, and creativity -- in modern STEM education necessitates a pedagogical shift from passive knowledge transmission to active Socratic construction. Although Large Language Models (LLMs) hold promise for STEM Interdisciplinary education, current methodologies employing Prompt Engineering (PE), Supervised Fine-tuning (SFT), or standard Reinforcement Learning (RL) often fall short of supporting this paradigm. Existing methods are hindered by three fundamental challenges: the inability to dynamically model latent student cognitive states; severe reward sparsity and delay inherent in long-term educational goals; and a tendency toward policy collapse lacking strategic diversity due to reliance on behavioral cloning. Recognizing the unobservability and dynamic complexity of these interactions, we formalize the Socratic Interdisciplinary Instructional Problem (SIIP) as a structured Partially Observable Markov Decision Process (POMDP), demanding simultaneous global exploration and fine-grained policy refinement. To this end, we propose ERL4SIIP, a novel Evolutionary Reinforcement Learning (ERL) framework specifically tailored for this domain. ERL4SIIP integrates: (1) a dynamic student simulator grounded in a STEM knowledge graph for latent state modeling; (2) a Hierarchical Reward Mechanism that decomposes long-horizon goals into dense signals; and (3) a LoRA-Division based optimization strategy coupling evolutionary algorithms for population-level global search with PPO for local gradient ascent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11930v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mei Jiang, Haihai Shen, Zhuo Luo, Bingdong Li, Wenjing Hong, Ke Tang, Aimin Zhou</dc:creator>
    </item>
    <item>
      <title>Mapping AI Risk Mitigations: Evidence Scan and Preliminary AI Risk Mitigation Taxonomy</title>
      <link>https://arxiv.org/abs/2512.11931</link>
      <description>arXiv:2512.11931v1 Announce Type: new 
Abstract: Organizations and governments that develop, deploy, use, and govern AI must coordinate on effective risk mitigation. However, the landscape of AI risk mitigation frameworks is fragmented, uses inconsistent terminology, and has gaps in coverage. This paper introduces a preliminary AI Risk Mitigation Taxonomy to organize AI risk mitigations and provide a common frame of reference. The Taxonomy was developed through a rapid evidence scan of 13 AI risk mitigation frameworks published between 2023-2025, which were extracted into a living database of 831 AI risk mitigations. The mitigations were iteratively clustered &amp; coded to create the Taxonomy. The preliminary AI Risk Mitigation Taxonomy organizes mitigations into four categories and 23 subcategories: (1) Governance &amp; Oversight: Formal organizational structures and policy frameworks that establish human oversight mechanisms and decision protocols; (2) Technical &amp; Security: Technical, physical, and engineering safeguards that secure AI systems and constrain model behaviors; (3) Operational Process: processes and management frameworks governing AI system deployment, usage, monitoring, incident handling, and validation; and (4) Transparency &amp; Accountability: formal disclosure practices and verification mechanisms that communicate AI system information and enable external scrutiny. The rapid evidence scan and taxonomy construction also revealed several cases where terms like 'risk management' and 'red teaming' are used widely but refer to different responsible actors, actions, and mechanisms of action to reduce risk. This Taxonomy and associated mitigation database, while preliminary, offers a starting point for collation and synthesis of AI risk mitigations. It also offers an accessible, structured way for different actors in the AI ecosystem to discuss and coordinate action to reduce risks from AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11931v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander K. Saeri, Sophia Lloyd George, Jess Graham, Clelia D. Lacarriere, Peter Slattery, Michael Noetel, Neil Thompson</dc:creator>
    </item>
    <item>
      <title>The Agentic Regulator: Risks for AI in Finance and a Proposed Agent-based Framework for Governance</title>
      <link>https://arxiv.org/abs/2512.11933</link>
      <description>arXiv:2512.11933v1 Announce Type: new 
Abstract: Generative and agentic artificial intelligence is entering financial markets faster than existing governance can adapt. Current model-risk frameworks assume static, well-specified algorithms and one-time validations; large language models and multi-agent trading systems violate those assumptions by learning continuously, exchanging latent signals, and exhibiting emergent behavior. Drawing on complex adaptive systems theory, we model these technologies as decentralized ensembles whose risks propagate along multiple time-scales. We then propose a modular governance architecture. The framework decomposes oversight into four layers of "regulatory blocks": (i) self-regulation modules embedded beside each model, (ii) firm-level governance blocks that aggregate local telemetry and enforce policy, (iii) regulator-hosted agents that monitor sector-wide indicators for collusive or destabilizing patterns, and (iv) independent audit blocks that supply third-party assurance. Eight design strategies enable the blocks to evolve as fast as the models they police. A case study on emergent spoofing in multi-agent trading shows how the layered controls quarantine harmful behavior in real time while preserving innovation. The architecture remains compatible with today's model-risk rules yet closes critical observability and control gaps, providing a practical path toward resilient, adaptive AI governance in financial systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11933v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.MA</category>
      <category>q-fin.GN</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eren Kurshan, Tucker Balch, David Byrd</dc:creator>
    </item>
    <item>
      <title>Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching</title>
      <link>https://arxiv.org/abs/2512.11934</link>
      <description>arXiv:2512.11934v1 Announce Type: new 
Abstract: The rapid integration of generative artificial intelligence into education has driven digital transformation in e-teaching, yet user perceptions of AI educational apps remain underexplored. This study performs a sentiment-driven evaluation of user reviews from top AI ed-apps on the Google Play Store to assess efficacy, challenges, and pedagogical implications. Our pipeline involved scraping app data and reviews, RoBERTa for binary sentiment classification, GPT-4o for key point extraction, and GPT-5 for synthesizing top positive/negative themes. Apps were categorized into seven types (e.g., homework helpers, math solvers, language tools), with overlaps reflecting multifunctional designs. Results indicate predominantly positive sentiments, with homework apps like Edu AI (95.9% positive) and Answer.AI (92.7%) leading in accuracy, speed, and personalization, while language/LMS apps (e.g., Teacher AI at 21.8% positive) lag due to instability and limited features. Positives emphasize efficiency in brainstorming, problem-solving, and engagement; negatives center on paywalls, inaccuracies, ads, and glitches. Trends show that homework helpers outperform specialized tools, highlighting AI's democratizing potential amid risks of dependency and inequity. The discussion proposes future ecosystems with hybrid AI-human models, VR/AR for immersive learning, and a roadmap for developers (adaptive personalization) and policymakers (monetization regulation for inclusivity). This underscores generative AI's role in advancing e-teaching by enabling ethical refinements that foster equitable, innovative environments. The full dataset is available here(https://github.com/erfan-nourbakhsh/GenAI-EdSent).</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11934v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Adeleh Mazaherian, Erfan Nourbakhsh</dc:creator>
    </item>
    <item>
      <title>Beyond right or wrong : towards redefining adaptive learning indicators in virtual learning environments</title>
      <link>https://arxiv.org/abs/2512.12105</link>
      <description>arXiv:2512.12105v1 Announce Type: new 
Abstract: Student learning development must involve more than just correcting or incorrect questions. However, most adaptive learning methods in Virtual Learning Environments are based on whether the student's response is incorrect or correct. This perspective is limited in assessing the student's learning level, as it does not consider other elements that can be crucial in this process. The objective of this work is to conduct a Systematic Literature Review (SLR) to elucidate which learning indicators influence student learning and which can be implemented in a VLE to assist in adaptive learning. The works selected and filtered by qualitative assessment reveal a comprehensive approach to assessing different aspects of the learning in virtual environments, such as motivation, emotions, physiological responses, brain imaging, and the students' prior knowledge. The discussion of these new indicators allows adaptive technology developers to implement more appropriate solutions to students' realities, resulting in more complete training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12105v1</guid>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.53660/CLM-4079-24S09</arxiv:DOI>
      <arxiv:journal_reference>Concilium. [S. l.]. Vol. 24, n. 18 (2024), p. 123-142</arxiv:journal_reference>
      <dc:creator>Andreia dos Santos Sachete, Alba Valeria de SantAnna de Freitas Loiola, Fabio Diniz Rossi, Jose Valdeni de Lima, Raquel Salcedo Gomes</dc:creator>
    </item>
    <item>
      <title>A neuro-symbolic framework for accountability in public-sector AI</title>
      <link>https://arxiv.org/abs/2512.12109</link>
      <description>arXiv:2512.12109v1 Announce Type: new 
Abstract: Automated eligibility systems increasingly determine access to essential public benefits, but the explanations they generate often fail to reflect the legal rules that authorize those decisions. This thesis develops a legally grounded explainability framework that links system-generated decision justifications to the statutory constraints of CalFresh, California's Supplemental Nutrition Assistance Program. The framework combines a structured ontology of eligibility requirements derived from the state's Manual of Policies and Procedures (MPP), a rule extraction pipeline that expresses statutory logic in a verifiable formal representation, and a solver-based reasoning layer to evaluate whether the explanation aligns with governing law. Case evaluations demonstrate the framework's ability to detect legally inconsistent explanations, highlight violated eligibility rules, and support procedural accountability by making the basis of automated determinations traceable and contestable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12109v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Allen Daniel Sunny</dc:creator>
    </item>
    <item>
      <title>The Ideological Turing Test for Moderation of Outgroup Affective Animosity</title>
      <link>https://arxiv.org/abs/2512.12187</link>
      <description>arXiv:2512.12187v1 Announce Type: new 
Abstract: Rising animosity toward ideological opponents poses critical societal challenges. We introduce and test the Ideological Turing Test, a gamified framework requiring participants to adopt and defend opposing viewpoints, to reduce affective animosity and affective polarization.
  We conducted a mixed-design experiment ($N = 203$) with four conditions: modality (debate/writing) x perspective-taking (Own/Opposite side). Participants engaged in structured interactions defending assigned positions, with outcomes judged by peers. We measured changes in affective animosity and ideological position immediately post-intervention and at 2-6 week follow-up.
  Perspective-taking reduced out-group animosity and ideological polarization. However, effects differed by modality (writing vs. debate) and over time. For affective animosity, writing from the opposite perspective yielded the largest immediate reduction ($\Delta=+0.45$ SD), but the effect was not detectable at the 4-6 week follow-up. In contrast, the debate modality maintained a statistically significant reduction in animosity immediately after and at follow-up ($\Delta=+0.37$ SD). For ideological position, adopting the opposite perspective led to significant immediate movement across modalities (writing: $\Delta=+0.91$ SD; debate: $\Delta=+0.51$ SD), and these changes persisted at follow-up. Judged performance (winning) did not moderate these effects, and willingness to re-participate was similar across conditions (~20-36%).
  These findings challenge assumptions about adversarial methods, revealing distinct temporal patterns: non-adversarial engagement fosters short-term empathy gains, while cognitive engagement through debate sustains affective benefits. The Ideological Turing Test demonstrates potential as a scalable tool for reducing polarization, particularly when combining perspective-taking with reflective adversarial interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12187v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Gamba, Daniel M. Romero, Grant Schoenebeck</dc:creator>
    </item>
    <item>
      <title>Anticipatory Governance in Data-Constrained Environments: A Predictive Simulation Framework for Digital Financial Inclusion</title>
      <link>https://arxiv.org/abs/2512.12212</link>
      <description>arXiv:2512.12212v1 Announce Type: new 
Abstract: Financial exclusion remains a major barrier to digital public service delivery in resource-constrained and archipelagic nations. Traditional policy evaluations rely on retrospective data, limiting the ex-ante intelligence needed for agile resource allocation. This study introduces a predictive simulation framework to support anticipatory governance within government information systems. Using the UNCDF Pacific Digital Economy dataset of 10,108 respondents, we apply a three-stage pipeline: descriptive profiling, interpretable machine learning, and scenario simulation to forecast outcomes of digital financial literacy interventions before deployment. Leveraging cross-sectional structural associations, the framework projects intervention scenarios as prioritization heuristics rather than causal estimates. A transparent linear regression model with R-squared of 95.9 identifies modifiable policy levers. Simulations indicate that foundational digital capabilities such as device access and expense tracking yield the highest projected gains, up to 5.5 percent, outperforming attitudinal nudges. The model enables precision targeting, highlighting young female caregivers as high-leverage responders while flagging non-responders such as urban professionals to prevent resource misallocation. This research demonstrates how static survey data can be repurposed into actionable policy intelligence, offering a scalable and evidence-based blueprint for embedding predictive analytics into public-sector decision-support systems to advance equity-focused digital governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12212v1</guid>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elizabeth Irenne Yuwono, Dian Tjondronegoro, Shawn Hunter, Amber Marshall</dc:creator>
    </item>
    <item>
      <title>From Co-Design to Metacognitive Laziness: Evaluating Generative AI in Vocational Education</title>
      <link>https://arxiv.org/abs/2512.12306</link>
      <description>arXiv:2512.12306v1 Announce Type: new 
Abstract: This study examines the development and deployment of a Generative AI proof-of-concept (POC) designed to support lecturers in a vocational education setting in Singapore. Employing a user-centred, mixed-methods design process, we co-developed an AI chatbot with lecturers to address recurring instructional challenges during exam preparation, specifically managing repetitive questions and scaling feedback delivery. The POC achieved its primary operational goals: lecturers reported streamlined workflows, reduced cognitive load, and observed improved student confidence in navigating course content. However, the deployment yielded unexpected insights into student learning behaviours. Despite enhanced teaching processes, performance data revealed no significant improvement in overall student assessment outcomes. Deep analysis of interaction logs identified concerning patterns, including self-efficacy-driven dependency, "metacognitive laziness" (cognitive offloading), and divergent usage strategies. While high-ability students leveraged the tool for strategic verification, low-ability students frequently used it to bypass cognitive effort, potentially exacerbating performance gaps. These findings suggest that Generative AI's educational influence extends beyond instructional efficiency to shape cognitive engagement, self-regulation, and learner equity. The study raises consequential design questions regarding how AI tools can be engineered to minimise dependency, scaffold metacognitive development, and calibrate support across varying ability levels. We conclude that while Generative AI can substantially enhance the teaching experience, achieving meaningful learning gains requires rigorous attention to learner behaviour and the equitable design of AI-supported environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12306v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amir Yunus, Peng Rend Gay, Oon Teng Lee</dc:creator>
    </item>
    <item>
      <title>AI Sprints: Towards a Critical Method for Human-AI Collaboration</title>
      <link>https://arxiv.org/abs/2512.12371</link>
      <description>arXiv:2512.12371v1 Announce Type: new 
Abstract: The emergence of Large Language Models presents a remarkable opportunity for humanities and social science research. I argue these technologies instantiate what I have called the algorithmic condition, whereby computational systems increasingly mediate not just our analytical tools but how we understand nature and society more generally. This article introduces the possibility for new forms of humanistic inquiry through what I term 'AI sprints', as intensive time-boxed research sessions. This is a research method combining the critical reflexivity essential to humanistic inquiry with iterative dialogue with generative AI. Drawing on experimental work in critical code studies, I demonstrate how tight loops of iterative development can adapt data and book sprint methodologies whilst acknowledging the profound transformations generative AI introduces. Through examining the process of human-AI collaboration when undertaken in these intensive research sessions, I seek to outline this approach as a broader research method. The article builds on Rogers' digital methods approach, proposing that we extend methodologies to study digital objects through their native protocols, using AI systems not merely to process digital traces but to analyse materials traditionally requiring manual coding or transcription. I aim to show this by introducing three cognitive modes, cognitive delegation, productive augmentation, and cognitive overhead, explaining how researchers can maintain a strategic overview whilst using LLM capabilities. The paper contributes both a practical methodology for intensive AI-augmented research and a theoretical framework for understanding the epistemological transformations of this hybrid method. A critical methodology must therefore operate in both technical and theoretical registers, sustaining a rigorous ethical-computational engagement with AI systems and outputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12371v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David M. Berry</dc:creator>
    </item>
    <item>
      <title>Beyond Static Scoring: Enhancing Assessment Validity via AI-Generated Interactive Verification</title>
      <link>https://arxiv.org/abs/2512.12592</link>
      <description>arXiv:2512.12592v1 Announce Type: new 
Abstract: Large Language Models (LLMs) challenge the validity of traditional open-ended assessments by blurring the lines of authorship. While recent research has focused on the accuracy of automated scoring (AES), these static approaches fail to capture process evidence or verify genuine student understanding. This paper introduces a novel Human-AI Collaboration framework that enhances assessment integrity by combining rubric-based automated scoring with AI-generated, targeted follow-up questions. In a pilot study with university instructors (N=9), we demonstrate that while Stage 1 (Auto-Scoring) ensures procedural fairness and consistency, Stage 2 (Interactive Verification) is essential for construct validity, effectively diagnosing superficial reasoning or unverified AI use. We report on the systems design, instructor perceptions of fairness versus validity, and the necessity of adaptive difficulty in follow-up questioning. The findings offer a scalable pathway for authentic assessment that moves beyond policing AI to integrating it as a synergistic partner in the evaluation process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12592v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tom Lee, Sihoon Lee, Seonghun Kim</dc:creator>
    </item>
    <item>
      <title>From Linear Risk to Emergent Harm: Complexity as the Missing Core of AI Governance</title>
      <link>https://arxiv.org/abs/2512.12707</link>
      <description>arXiv:2512.12707v1 Announce Type: new 
Abstract: Risk-based AI regulation has become the dominant paradigm in AI governance, promising proportional controls aligned with anticipated harms. This paper argues that such frameworks often fail for structural reasons: they implicitly assume linear causality, stable system boundaries, and largely predictable responses to regulation. In practice, AI operates within complex adaptive socio-technical systems in which harm is frequently emergent, delayed, redistributed, and amplified through feedback loops and strategic adaptation by system actors. As a result, compliance can increase while harm is displaced or concealed rather than eliminated. We propose a complexity-based framework for AI governance that treats regulation as intervention rather than control, prioritises dynamic system mapping over static classifications, and integrates causal reasoning and simulation for policy design under uncertainty. The aim is not to eliminate uncertainty, but to enable robust system stewardship through monitoring, learning, and iterative revision of governance interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12707v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.17929014</arxiv:DOI>
      <dc:creator>Hugo Roger Paz</dc:creator>
    </item>
    <item>
      <title>Algorithmic Criminal Liability in Greenwashing: Comparing India, United States, and European Union</title>
      <link>https://arxiv.org/abs/2512.12837</link>
      <description>arXiv:2512.12837v1 Announce Type: new 
Abstract: AI-powered greenwashing has emerged as an insidious challenge within corporate sustainability governance, exacerbating the opacity of environmental disclosures and subverting regulatory oversight. This study conducts a comparative legal analysis of criminal liability for AI-mediated greenwashing across India, the US, and the EU, exposing doctrinal lacunae in attributing culpability when deceptive claims originate from algorithmic systems. Existing statutes exhibit anthropocentric biases by predicating liability on demonstrable human intent, rendering them ill-equipped to address algorithmic deception. The research identifies a critical gap in jurisprudential adaptation, as prevailing fraud statutes remain antiquated vis-\`a-vis AI-generated misrepresentation. Utilising a doctrinal legal methodology, this study systematically dissects judicial precedents and statutory instruments, yielding results regarding the potential expansion of corporate criminal liability. Findings underscore the viability of strict liability models, recalibrated governance frameworks for AI accountability, and algorithmic due diligence mandates under ESG regimes. Comparative insights reveal jurisdictional disparities, with the EU Corporate Sustainability Due Diligence Directive (CSDDD) offering a potential transnational model. This study contributes to AI ethics and environmental jurisprudence by advocating for a hybrid liability framework integrating algorithmic risk assessment with legal personhood constructs, ensuring algorithmic opacity does not preclude liability enforcement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12837v1</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>HPNLU Journal of Law, Business and Economics, Vol. 3, pp. 51-68, 2024</arxiv:journal_reference>
      <dc:creator>Sahibpreet Singh, Manjit Singh</dc:creator>
    </item>
    <item>
      <title>Open Source Software and Data for Human Service Development: A Case Study on Predicting Housing Instability</title>
      <link>https://arxiv.org/abs/2512.12919</link>
      <description>arXiv:2512.12919v1 Announce Type: new 
Abstract: Open-source data and tools are lauded as essential for replicable and usable social science, though little is known about their use in resource constrained human service provision. This paper examines the challenges and opportunities of open-source tools and data in human service development by using both to forecast failure to pay eviction filings in Bronx County, NY. We use zip code level data from the Housing Data Coalition, the American Community Survey 5-year estimates, and DeepMaps Model of the Labor Force to forecast rates through July 2021. We employ multilevel (MLM) and exponential smoothing (ETS) models using the R project for Statistical Computing, an oft used open-source statistical software. We compare our results to what happened during the same period, to illustrate the efficacy of the open-source tools and techniques employed. We argue open-source data and software may facilitate rapid analysis of public data - a much-needed ability in human service intervention development under increasingly constrained resources - but find public data are limited by the information they reliably capture, limiting their utility by a non-trivial margin of error. The manuscript concludes by considering lessons for human service organizations with limited analytical resources and a vested interest in low-resourced communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12919v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Maria Y. Rodriguez, Ehren Dohler, Jon Phillips, Melissa Villodas, Voltaire Vegara, Kenny Joseph, Amy Wilson</dc:creator>
    </item>
    <item>
      <title>Modeling Collaborative Problem Solving Dynamics from Group Discourse: A Text-Mining Approach with Synergy Degree Model</title>
      <link>https://arxiv.org/abs/2512.13061</link>
      <description>arXiv:2512.13061v1 Announce Type: new 
Abstract: Measuring collaborative problem solving (CPS) synergy remains challenging in learning analytics, as classical manual coding cannot capture emergent system-level dynamics. This study introduces a computational framework that integrates automated discourse analysis with the Synergy Degree Model (SDM) to quantify CPS synergy from group communication. Data were collected from 52 learners in 12 groups during a 5-week connectivist MOOC (cMOOC) activity. Nine classification models were applied to automatically identify ten CPS behaviors across four interaction levels: operation, wayfinding, sense-making, and creation. While BERT achieved the highest accuracy, GPT models demonstrated superior precision suitable for human-AI collaborative coding. Within the SDM framework, each interaction level was treated as a subsystem to compute group-level order parameters and derive synergy degrees. Permutation tests showed automated measures preserve construct validity, despite systematic biases at the subsystem level. Statistical analyses revealed significant task-type differences: survey study groups exhibited higher creation-order than mode study groups, suggesting "controlled disorder" may benefit complex problem solving. Importantly, synergy degree distinguished collaborative quality, ranging from excellent to failing groups. Findings establish synergy degree as a sensitive indicator of collaboration and demonstrate the feasibility of scaling fine-grained CPS analytics through AI-in-the-loop approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13061v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianjun Xiao, Cixiao Wang, Wenmei Zhang</dc:creator>
    </item>
    <item>
      <title>From Educational Analytics to AI Governance: Transferable Lessons from Complex Systems Interventions</title>
      <link>https://arxiv.org/abs/2512.13260</link>
      <description>arXiv:2512.13260v1 Announce Type: new 
Abstract: Both student retention in higher education and artificial intelligence governance face a common structural challenge: the application of linear regulatory frameworks to complex adaptive systems. Risk-based approaches dominate both domains, yet systematically fail because they assume stable causal pathways, predictable actor responses, and controllable system boundaries. This paper extracts transferable methodological principles from CAPIRE (Curriculum, Archetypes, Policies, Interventions &amp; Research Environment), an empirically validated framework for educational analytics that treats student dropout as an emergent property of curricular structures, institutional rules, and macroeconomic shocks. Drawing on longitudinal data from engineering programmes and causal inference methods, CAPIRE demonstrates that well-intentioned interventions routinely generate unintended consequences when system complexity is ignored. We argue that five core principles developed within CAPIRE - temporal observation discipline, structural mapping over categorical classification, archetype-based heterogeneity analysis, causal mechanism identification, and simulation-based policy design - transfer directly to the challenge of governing AI systems. The isomorphism is not merely analogical: both domains exhibit non-linearity, emergence, feedback loops, strategic adaptation, and path dependence. We propose Complex Systems AI Governance (CSAIG) as an integrated framework that operationalises these principles for regulatory design, shifting the central question from "how risky is this AI system?" to "how does this intervention reshape system dynamics?" The contribution is twofold: demonstrating that empirical lessons from one complex systems domain can accelerate governance design in another, and offering a concrete methodological architecture for complexity-aware AI regulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13260v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hugo Roger Paz</dc:creator>
    </item>
    <item>
      <title>Google Spain v. Gonz\'ales: Did the Court forget about freedom of expression?</title>
      <link>https://arxiv.org/abs/2512.13404</link>
      <description>arXiv:2512.13404v1 Announce Type: new 
Abstract: When reviewing a job application letter, going on a first date, or considering doing business with someone, the first thing many people do is entering the person's name in a search engine. A search engine can point searchers to information that would otherwise have remained obscure. If somebody searched for the name of Spanish lawyer Mario Costeja Gonz\'alez, Google showed search results that included a link to a 1998 newspaper announcement implying he had financial troubles at the time. Gonz\'alez wanted Google to stop showing those links and started a procedure in Spain. After some legal wrangling, the Spanish Audiencia Nacional (National High Court) asked the Court of Justice of the European Union (CJEU) for advice on the application of the Data Protection Directive, which led to the controversial judgment in Google Spain. In its judgment, the CJEU holds that people, under certain conditions, have the right to have search results for their name delisted. This right can also extend to lawfully published information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13404v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1017/S1867299X00003949</arxiv:DOI>
      <arxiv:journal_reference>European Journal of Risk Regulation 2014-5-3, p. 389-398</arxiv:journal_reference>
      <dc:creator>Stefan Kulk, Frederik Zuiderveen Borgesius</dc:creator>
    </item>
    <item>
      <title>Improving Privacy Protection in the area of Behavioural Targeting</title>
      <link>https://arxiv.org/abs/2512.13405</link>
      <description>arXiv:2512.13405v1 Announce Type: new 
Abstract: This PhD thesis discusses how European law could improve privacy protection in the area of behavioural targeting. Behavioural targeting, also referred to as online profiling, involves monitoring people's online behaviour, and using the collected information to show people individually targeted advertisements. To protect privacy in the area of behavioural targeting, the EU lawmaker mainly relies on the consent requirement for the use of tracking technologies in the e-Privacy Directive, and on general data protection law. With informed consent requirements, the law aims to empower people to make choices in their best interests. But behavioural studies cast doubt on the effectiveness of the empowerment approach as a privacy protection measure. Many people click "I agree" to any statement that is presented to them. Therefore, to mitigate privacy problems such as chilling effects, this study argues for a combined approach of protecting and empowering the individual. Compared to the current approach, the lawmaker should focus more on protecting people. The PhD thesis is a legal study, but it also incorporates insights from other disciplines, such as computer science, behavioural economics, and media studies. This study is among the first to discuss the implications of behavioural research for European data protection policy. The topic of whether data protection law should apply to pseudonymous data is discussed in depth. The study contains a detailed analysis of the role of informed consent in data protection law, and gives much attention to the tension between protecting and empowering the individual within data protection law.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13405v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Frederik Johannes Zuiderveen Borgesius</dc:creator>
    </item>
    <item>
      <title>Embedding-Based Rankings of Educational Resources based on Learning Outcome Alignment: Benchmarking, Expert Validation, and Learner Performance</title>
      <link>https://arxiv.org/abs/2512.13658</link>
      <description>arXiv:2512.13658v1 Announce Type: new 
Abstract: As the online learning landscape evolves, the need for personalization is increasingly evident. Although educational resources are burgeoning, educators face challenges selecting materials that both align with intended learning outcomes and address diverse learner needs. Large Language Models (LLMs) are attracting growing interest for their potential to create learning resources that better support personalization, but verifying coverage of intended outcomes still requires human alignment review, which is costly and limits scalability. We propose a framework that supports the cost-effective automation of evaluating alignment between educational resources and intended learning outcomes. Using human-generated materials, we benchmarked LLM-based text-embedding models and found that the most accurate model (Voyage) achieved 79% accuracy in detecting alignment. We then applied the optimal model to LLM-generated resources and, via expert evaluation, confirmed that it reliably assessed correspondence to intended outcomes (83% accuracy). Finally, in a three-group experiment with 360 learners, higher alignment scores were positively related to greater learning performance, chi-squared(2, N = 360) = 15.39, p &lt; 0.001. These findings show that embedding-based alignment scores can facilitate scalable personalization by confirming alignment with learning outcomes, which allows teachers to focus on tailoring content to diverse learner needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13658v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammadreza Molavi, Mohammad Moein, Mohammadreza Tavakoli, Abdolali Faraji, Stefan T. Mol, G\'abor Kismih\'ok</dc:creator>
    </item>
    <item>
      <title>Enhancing Urban Visual Place Recognition for Crowdsourced Flood Imagery via LLM-Guided Attention</title>
      <link>https://arxiv.org/abs/2512.11811</link>
      <description>arXiv:2512.11811v1 Announce Type: cross 
Abstract: Crowdsourced street-view imagery from social media provides valuable real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing Visual Place Recognition (VPR) models exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts inherent in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geospatial knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress transient visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11811v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fengyi Xu, Jun Ma, Waishan Qiu, Cui Guo</dc:creator>
    </item>
    <item>
      <title>Hot H\'em: S\`ai G\`on Gi\~ua C\'ai N\'ong H\^ong C\`ong B\`ang -- Saigon in Unequal Heat</title>
      <link>https://arxiv.org/abs/2512.11896</link>
      <description>arXiv:2512.11896v1 Announce Type: cross 
Abstract: Pedestrian heat exposure is a critical health risk in dense tropical cities, yet standard routing algorithms often ignore micro-scale thermal variation. Hot H\'em is a GeoAI workflow that estimates and operationalizes pedestrian heat exposure in H\^o Ch\'i Minh City (HCMC), Vi\d{e}t Nam, colloquially known as S\`ai G\`on. This spatial data science pipeline combines Google Street View (GSV) imagery, semantic image segmentation, and remote sensing. Two XGBoost models are trained to predict land surface temperature (LST) using a GSV training dataset in selected administrative wards, known as ph\u{o}ng, and are deployed in a patchwork manner across all OSMnx-derived pedestrian network nodes to enable heat-aware routing. This is a model that, when deployed, can provide a foundation for pinpointing where and further understanding why certain city corridors may experience disproportionately higher temperatures at an infrastructural scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11896v1</guid>
      <category>cs.CV</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tessa Vu</dc:creator>
    </item>
    <item>
      <title>EnviroLLM: Resource Tracking and Optimization for Local AI</title>
      <link>https://arxiv.org/abs/2512.12004</link>
      <description>arXiv:2512.12004v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly deployed locally for privacy and accessibility, yet users lack tools to measure their resource usage, environmental impact, and efficiency metrics. This paper presents EnviroLLM, an open-source toolkit for tracking, benchmarking, and optimizing performance and energy consumption when running LLMs on personal devices. The system provides real-time process monitoring, benchmarking across multiple platforms (Ollama, LM Studio, vLLM, and OpenAI-compatible APIs), persistent storage with visualizations for longitudinal analysis, and personalized model and optimization recommendations. The system includes LLM-as-judge evaluations alongside energy and speed metrics, enabling users to assess quality-efficiency tradeoffs when testing models with custom prompts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12004v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Troy Allen</dc:creator>
    </item>
    <item>
      <title>Understanding Main Path Analysis</title>
      <link>https://arxiv.org/abs/2512.12355</link>
      <description>arXiv:2512.12355v1 Announce Type: cross 
Abstract: Main path analysis has long been used to trace knowledge trajectories in citation networks, yet it lacks solid theoretical foundations. To understand when and why this approach succeeds, we analyse directed acyclic graphs created from two types of artificial models and by looking at over twenty networks derived from real data.
  We show that entropy-based variants of main path analysis optimise geometric distance measures, providing its first information-theoretic and geometric basis. Numerical results demonstrate that existing algorithms converge on near-geodesic solutions. We also show that an approach based on longest paths produces similar results, is equally well motivated yet is much simpler to implement.
  However, the traditional single-path focus is unnecessarily restrictive, as many near-optimal paths highlight different key nodes. We introduce an approach using ``baskets'' of nodes where we select a fraction of nodes with the smallest values of a measure we call ``generalised criticality''. Analysis of large vaccine citation networks shows that these baskets achieve comprehensive algorithmic coverage, offering a robust, simple, and computationally efficient way to identify core knowledge structures. In practice, we find that those nodes with zero unit criticality capture the information in main paths in almost all cases and capture a wider range of key nodes without unnecessarily increasing the number of nodes considered. We find no advantage in using the traditional main path methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12355v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.SI</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>H. C. W. Price, T. S. Evans</dc:creator>
    </item>
    <item>
      <title>Ethical Risk Analysis of L2 Rollups</title>
      <link>https://arxiv.org/abs/2512.12732</link>
      <description>arXiv:2512.12732v1 Announce Type: cross 
Abstract: Layer 2 rollups improve throughput and fees, but can reintroduce risk through operator discretion and information asymmetry. We ask which operator and governance designs produce ethically problematic user risk. We adapt Ethical Risk Analysis to rollup architectures, build a role-based taxonomy of decision authority and exposure, and pair the framework with two empirical signals, a cross sectional snapshot of 129 projects from L2BEAT and a hand curated incident set covering 2022 to 2025. We analyze mechanisms that affect risks to users funds, including upgrade timing and exit windows, proposer liveness and whitelisting, forced inclusion usability, and data availability choices. We find that ethical hazards rooted in L2 components control arrangements are widespread: instant upgrades without exit windows appear in about 86 percent of projects, and proposer controls that can freeze withdrawals in about 50 percent. Reported incidents concentrate in sequencer liveness and inclusion, consistent with these dependencies. We translate these findings into ethically grounded suggestions on mitigation strategies including technical components and governance mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12732v1</guid>
      <category>cs.DC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georgy Ishmaev, Emmanuelle Anceaume, Davide Frey, Fran\c{c}ois Ta\"iani</dc:creator>
    </item>
    <item>
      <title>Towards Open Standards for Systemic Complexity in Digital Forensics</title>
      <link>https://arxiv.org/abs/2512.12970</link>
      <description>arXiv:2512.12970v1 Announce Type: cross 
Abstract: The intersection of artificial intelligence (AI) and digital forensics (DF) is becoming increasingly complex, ubiquitous, and pervasive, with overlapping techniques and technologies being adopted in all types of scientific and technical inquiry. Despite incredible advances, forensic sciences are not exempt from errors and remain vulnerable to fallibility. To mitigate the limitations of errors in DF, the systemic complexity is identified and addressed with the adoption of human-readable artifacts and open standards. A DF AI model schema based on the state of the art is outlined.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12970v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.6084/m9.figshare.2965171</arxiv:DOI>
      <dc:creator>Paola Di Maio</dc:creator>
    </item>
    <item>
      <title>An AI-Based Framework for Assessing Sustainability Conflicts in Medical Device Development</title>
      <link>https://arxiv.org/abs/2512.13005</link>
      <description>arXiv:2512.13005v1 Announce Type: cross 
Abstract: Designing sustainable medical devices requires balancing environmental, economic, and social demands, yet trade-offs across these pillars are difficult to identify using manual assessment alone. Current methods depend heavily on expert judgment, lack standardisation, and struggle to integrate diverse lifecycle data, which leads to overlooked conflicts and inconsistent evaluations. This paper introduces an AI-driven framework that automates conflict detection. Machine learning and natural language processing are used to extract trade-offs from design decisions, while Multi-Criteria Decision Analysis (MCDA) quantifies their magnitude through a composite sustainability score. The approach improves consistency, reduces subjective bias, and supports early design decisions. The results demonstrate how AI-assisted analysis provides scalable, data-driven support for sustainability evaluation in medical device development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13005v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Apala Chakrabarti</dc:creator>
    </item>
    <item>
      <title>Weak Enforcement and Low Compliance in PCI~DSS: A Comparative Security Study</title>
      <link>https://arxiv.org/abs/2512.13430</link>
      <description>arXiv:2512.13430v1 Announce Type: cross 
Abstract: Although credit and debit card data continue to be a prime target for attackers, organizational adherence to the Payment Card Industry Data Security Standard (PCI DSS) remains surprisingly low. Despite prior work showing that PCI DSS can reduce card fraud, only 32.4% of organizations were fully compliant in 2022, suggesting possible deficiencies in enforcement mechanisms. This study compares PCI DSS with three data security frameworks, HIPAA, NIS2, and GDPR, to examine how enforcement mechanisms relate to implementation success. The analysis reveals that PCI DSS significantly lags far behind these security frameworks and that its sanctions are orders of magnitude smaller than those under GDPR and NIS2. The findings indicate a positive association between stronger, multi-modal enforcement (including public disclosure, license actions, and imprisonment) and higher implementation rates, and highlights the structural weakness of PCI DSS's bank-dependent monitoring model. Enhanced non-monetary sanctions and the creation of an independent supervisory authority are recommended to increase transparency, reduce conflicts of interest, and improve PCI DSS compliance without discouraging card acceptance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13430v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soonwon Park, John D. Hastings</dc:creator>
    </item>
    <item>
      <title>neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings</title>
      <link>https://arxiv.org/abs/2512.13481</link>
      <description>arXiv:2512.13481v1 Announce Type: cross 
Abstract: Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13481v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ojas Pungalia, Rashi Upadhyay, Abhishek Mishra, Abhiram H, Tejasvi Alladi, Sujan Yenuganti, Dhruv Kumar</dc:creator>
    </item>
    <item>
      <title>Platforms as Crime Scene, Judge, and Jury: How Victim-Survivors of Non-Consensual Intimate Imagery Report Abuse Online</title>
      <link>https://arxiv.org/abs/2512.13500</link>
      <description>arXiv:2512.13500v1 Announce Type: cross 
Abstract: Non-consensual intimate imagery (NCII), also known as image-based sexual abuse (IBSA), is mediated through online platforms. Victim-survivors must turn to platforms to collect evidence and request content removal. Platforms act as the crime scene, judge, and jury, determining whether perpetrators face consequences and if harmful material is removed. We present a study of NCII victim-survivors' online reporting experiences, drawing on trauma-informed interviews with 13 participants. We find that platform reporting processes are hostile, opaque, and ineffective, often forcing complex harms into narrow interfaces, responding inconsistently, and failing to result in meaningful action. Leveraging institutional betrayal theory, we show how platforms' structures and practices compound harm, and, in doing so, surface concrete intervention points for redesigning reporting systems and shaping policy to better support victim-survivors</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13500v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Li Qiwei, Katelyn Kennon, Nicole Bedera, Asia A. Eaton, Eric Gilbert, Sarita Schoenebeck</dc:creator>
    </item>
    <item>
      <title>Verifying Rumors via Stance-Aware Structural Modeling</title>
      <link>https://arxiv.org/abs/2512.13559</link>
      <description>arXiv:2512.13559v1 Announce Type: cross 
Abstract: Verifying rumors on social media is critical for mitigating the spread of false information. The stances of conversation replies often provide important cues to determine a rumor's veracity. However, existing models struggle to jointly capture semantic content, stance information, and conversation strructure, especially under the sequence length constraints of transformer-based encoders. In this work, we propose a stance-aware structural modeling that encodes each post in a discourse with its stance signal and aggregates reply embedddings by stance category enabling a scalable and semantically enriched representation of the entire thread. To enhance structural awareness, we introduce stance distribution and hierarchical depth as covariates, capturing stance imbalance and the influence of reply depth. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms prior methods in the ability to predict truthfulness of a rumor. We also demonstrate that our model is versatile for early detection and cross-platfrom generalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13559v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gibson Nkhata, Uttamasha Anjally Oyshi, Quan Mai, Susan Gauch</dc:creator>
    </item>
    <item>
      <title>Navigating AI to Unpack Youth Privacy Concerns: An In-Depth Exploration and Systematic Review</title>
      <link>https://arxiv.org/abs/2412.16369</link>
      <description>arXiv:2412.16369v2 Announce Type: replace 
Abstract: This systematic literature review investigates perceptions, concerns, and expectations of young digital citizens regarding privacy in artificial intelligence (AI) systems, focusing on social media platforms, educational technology, gaming systems, and recommendation algorithms. Using a rigorous methodology, the review started with 2,000 papers, narrowed down to 552 after initial screening, and finally refined to 108 for detailed analysis. Data extraction focused on privacy concerns, data-sharing practices, the balance between privacy and utility, trust factors in AI, transparency expectations, and strategies to enhance user control over personal data. Findings reveal significant privacy concerns among young users, including a perceived lack of control over personal information, potential misuse of data by AI, and fears of data breaches and unauthorized access. These issues are worsened by unclear data collection practices and insufficient transparency in AI applications. The intention to share data is closely associated with perceived benefits and data protection assurances. The study also highlights the role of parental mediation and the need for comprehensive education on data privacy. Balancing privacy and utility in AI applications is crucial, as young digital citizens value personalized services but remain wary of privacy risks. Trust in AI is significantly influenced by transparency, reliability, predictable behavior, and clear communication about data usage. Strategies to improve user control over personal data include access to and correction of data, clear consent mechanisms, and robust data protection assurances. The review identifies research gaps and suggests future directions, such as longitudinal studies, multicultural comparisons, and the development of ethical AI frameworks. The findings have significant implications for policy development and educational initiatives</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16369v2</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/IEMCON62851.2024.11093125</arxiv:DOI>
      <arxiv:journal_reference>2024 IEEE 15th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)</arxiv:journal_reference>
      <dc:creator>Ajay Kumar Shrestha, Ankur Barthwal, Molly Campbell, Austin Shouli, Saad Syed, Sandhya Joshi, Julita Vassileva</dc:creator>
    </item>
    <item>
      <title>Privacy Ethics Alignment in AI: A Stakeholder-Centric Framework for Ethical AI</title>
      <link>https://arxiv.org/abs/2503.11950</link>
      <description>arXiv:2503.11950v4 Announce Type: replace 
Abstract: The increasing integration of artificial intelligence (AI) in digital ecosystems has reshaped privacy dynamics, particularly for young digital citizens navigating data-driven environments. This study explores evolving privacy concerns across three key stakeholder groups-young digital citizens, parents/educators, and AI professionals-and assesses differences in data ownership, trust, transparency, parental mediation, education, and risk-benefit perceptions. Employing a grounded theory methodology, this research synthesizes insights from key participants through structured surveys, qualitative interviews, and focus groups to identify distinct privacy expectations. Young digital citizens emphasized autonomy and digital agency, while parents and educators prioritized oversight and AI literacy. AI professionals focused on balancing ethical design with system performance. The analysis revealed significant gaps in transparency and digital literacy, underscoring the need for inclusive, stakeholder-driven privacy frameworks. Drawing on comparative thematic analysis, this study introduces the Privacy-Ethics Alignment in AI (PEA-AI) model, which conceptualizes privacy decision-making as a dynamic negotiation among stakeholders. By aligning empirical findings with governance implications, this research provides a scalable foundation for adaptive, youth-centered AI privacy governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11950v4</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/systems13060455</arxiv:DOI>
      <arxiv:journal_reference>Systems 2025, 13, 455</arxiv:journal_reference>
      <dc:creator>Ankur Barthwal, Molly Campbell, Ajay Kumar Shrestha</dc:creator>
    </item>
    <item>
      <title>Algorithmic Addiction by Design: Big Tech's Leverage of Dark Patterns to Maintain Market Dominance and its Challenge for Content Moderation</title>
      <link>https://arxiv.org/abs/2505.00054</link>
      <description>arXiv:2505.00054v2 Announce Type: replace 
Abstract: Today's largest technology corporations, especially ones with consumer-facing products such as social media platforms, use a variety of unethical and often outright illegal tactics to maintain their dominance. One tactic that has risen to the level of the public consciousness is the concept of addictive design, evidenced by the fact that excessive social media use has become a salient problem, particularly in the mental and social development of adolescents and young adults. As tech companies have developed more and more sophisticated artificial intelligence (AI) models to power their algorithmic recommender systems, they will become more successful at their goal of ensuring addiction to their platforms. This paper explores how online platforms intentionally cultivate addictive user behaviors and the broad societal implications, including on the health and well-being of children and adolescents. It presents the usage of addictive design - including the usage of dark patterns, persuasive design elements, and recommender algorithms - as a tool leveraged by technology corporations to maintain their dominance. Lastly, it describes the challenge of content moderation to address the problem and gives an overview of solutions at the policy level to counteract addictive design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00054v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michelle Nie</dc:creator>
    </item>
    <item>
      <title>The Transition Matrix -- A classification of navigational patterns between LMS course sections</title>
      <link>https://arxiv.org/abs/2506.13275</link>
      <description>arXiv:2506.13275v2 Announce Type: replace 
Abstract: Learning management systems (LMS) like Moodle are increasingly used to support university teaching. As Moodle courses become more complex, incorporating diverse interactive elements, it is important to understand how students navigate through course sections and whether course designs are meeting student needs. While substantial research exists on student usage of individual LMS elements, there is a lack of research on broader navigational patterns between course sections and how these patterns differ across courses. This study analyzes navigational data from 747 courses in the Moodle LMS at a technical university of applied sciences, representing (after filtering) around 4,400 students and 1.8 million logged events. Transition matrices and heat map visualizations are used to identify and quantify common navigational patterns. Findings include that the majority of the analyzed courses exhibit some kind of diagonal pattern, indicating that students typically navigate from the current to the next or previous section.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13275v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tobias Hildebrandt, Lars Mehnen</dc:creator>
    </item>
    <item>
      <title>A Collectivist, Economic Perspective on AI</title>
      <link>https://arxiv.org/abs/2507.06268</link>
      <description>arXiv:2507.06268v3 Announce Type: replace 
Abstract: Information technology is in the midst of a revolution in which omnipresent data collection and machine learning are impacting the human world as never before. The word ``intelligence'' is being used as a North Star for the development of this technology, with human cognition viewed as a baseline. This view neglects the fact that humans are social animals and that much of our intelligence is social and cultural in origin. Moreover, failing to properly situate aspects of intelligence at the social level contributes to the treatment of the societal consequences of technology as an afterthought. The path forward is not merely more data and compute, and not merely more attention paid to cognitive or symbolic representations, but a thorough blending of economic and social concepts with computational and inferential concepts at the level of algorithm design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06268v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>stat.ML</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Michael I. Jordan</dc:creator>
    </item>
    <item>
      <title>Ethics Practices in AI Development: An Empirical Study Across Roles and Regions</title>
      <link>https://arxiv.org/abs/2508.09219</link>
      <description>arXiv:2508.09219v2 Announce Type: replace 
Abstract: Recent advances in AI applications have raised growing concerns about the need for ethical guidelines and regulations to mitigate the risks posed by these technologies. In this paper, we present a mixed-methods survey study - combining statistical and qualitative analyses - to examine the ethical perceptions, practices, and knowledge of individuals involved in various AI development roles. Our survey comprises 414 participants from 43 countries, representing various roles such as AI managers, analysts, developers, quality assurance professionals, and information security and privacy experts. The results reveal varying degrees of familiarity and experience with AI ethics principles, government initiatives, and risk mitigation strategies across roles, regions, and other demographic factors. Our findings underscore the importance of a collaborative, role-sensitive approach that involves diverse stakeholders in ethical decision-making throughout the AI development lifecycle. We advocate for developing tailored, inclusive solutions to address ethical challenges in AI development, and we propose future research directions and educational strategies to promote ethics-aware AI practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09219v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wilder Baldwin, Sepideh Ghanavati, Manuel Woersdoerfer</dc:creator>
    </item>
    <item>
      <title>The Digital Life of Parisian Parks: Multifunctionality and Urban Context Uncovered by Mobile Application Traffic</title>
      <link>https://arxiv.org/abs/2508.15516</link>
      <description>arXiv:2508.15516v2 Announce Type: replace 
Abstract: Urban parks support public health, but landscape architecture typically examines them through form and function. Prior equitable access research focused on park form, while functional studies relied on small-scale surveys, movement data, or broad usage metrics, missing specific activities and visit motivations. This gap limits our grasp of parks' functional diversity. We address this with a novel method refining mobile base station coverage via antenna azimuths to isolate park-specific traffic from surroundings. Using Paris as a case study, we process 492 million hourly per-app mobile records (35% market share) from 45 urban parks. We test the central-city hypothesis (multifunctional parks in dense, high-rent zones due to land constraints) and socio-spatial hypothesis (parks reflecting neighborhood routines and preferences). Results reveal parks' unique mobile traffic signatures, distinct from urban contexts and each other. Clustering by temporal and app patterns identifies three types: lunchbreak, cultural, and recreational parks, linked to health-promoting visitation motives. Central parks show diverse apps and peak usage; suburban recreational parks mirror local demographics, like income-aligned app preferences. This demonstrates mobile traffic's power as a proxy for urban green space activities, with key implications for park design, public health, and well-being strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15516v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andr\'e Felipe Zanella, Linus W. Dietz, Sanja \v{S}\'cepanovi\'c, Ke Zhou, Zbigniew Smoreda, Daniele Quercia</dc:creator>
    </item>
    <item>
      <title>Faster Results from a Smarter Schedule: Reframing Collegiate Cross Country through Analysis of the National Running Club Database</title>
      <link>https://arxiv.org/abs/2509.10600</link>
      <description>arXiv:2509.10600v4 Announce Type: replace 
Abstract: Collegiate cross country teams often build their season schedules on intuition rather than evidence, partly because large-scale performance datasets are not publicly accessible. To address this limitation, we introduce the National Running Club Database (NRCD), the first openly available dataset to aggregate 23,725 race results from 7,594 collegiate club athletes across the 2023-2025 seasons. Unlike existing resources, NRCD includes detailed course metadata, allowing us to develop two standardized performance metrics: Converted Only (distance correction) and Standardized (distance, weather, and elevation adjusted). Using these standardized measures, we find that athletes with slower initial performances exhibit the greatest improvement within a season, and that race frequency is the strongest predictor of improvement. Using six machine learning models, random forest achieves the highest accuracy (r squared equals 0.92), revealing that athletes who race more frequently progress significantly faster than those who do not. At the team level, programs whose athletes race at least four times during the regular season have substantially higher odds of placing in the top 15 at nationals (chi-squared less than 0.01). These results challenge common coaching practices that favor minimal racing before championship meets. Our findings demonstrate that a data-informed scheduling strategy improves both individual development and team competitiveness. The NRCD provides a new foundation for evidence-based decision-making in collegiate cross country and opens opportunities for further research on standardized, longitudinal athlete performance modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10600v4</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan A. Karr Jr, Ryan M. Fryer, Ben Darden, Nicholas Pell, Kayla Ambrose, Evan Hall, Ramzi K. Bualuan, Nitesh V. Chawla</dc:creator>
    </item>
    <item>
      <title>Three Lenses on the AI Revolution: Risk, Transformation, Continuity</title>
      <link>https://arxiv.org/abs/2510.12859</link>
      <description>arXiv:2510.12859v2 Announce Type: replace 
Abstract: Artificial Intelligence (AI) has emerged as both a continuation of historical technological revolutions and a potential rupture with them. This paper argues that AI must be viewed simultaneously through three lenses: \textit{risk}, where it resembles nuclear technology in its irreversible and global externalities; \textit{transformation}, where it parallels the Industrial Revolution as a general-purpose technology driving productivity and reorganization of labor; and \textit{continuity}, where it extends the fifty-year arc of computing revolutions from personal computing to the internet to mobile. Drawing on historical analogies, we emphasize that no past transition constituted a strict singularity: disruptive shifts eventually became governable through new norms and institutions.
  We examine recurring patterns across revolutions -- democratization at the usage layer, concentration at the production layer, falling costs, and deepening personalization -- and show how these dynamics are intensifying in the AI era. Sectoral analysis illustrates how accounting, law, education, translation, advertising, and software engineering are being reshaped as routine cognition is commoditized and human value shifts to judgment, trust, and ethical responsibility. At the frontier, the challenge of designing moral AI agents highlights the need for robust guardrails, mechanisms for moral generalization, and governance of emergent multi-agent dynamics.
  We conclude that AI is neither a singular break nor merely incremental progress. It is both evolutionary and revolutionary: predictable in its median effects yet carrying singularity-class tail risks. Good outcomes are not automatic; they require coupling pro-innovation strategies with safety governance, ensuring equitable access, and embedding AI within a human order of responsibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12859v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masoud Makrehchi</dc:creator>
    </item>
    <item>
      <title>On the Influence of Artificial Intelligence on Human Problem-Solving: Empirical Insights for the Third Wave in a Multinational Longitudinal Pilot Study</title>
      <link>https://arxiv.org/abs/2511.11738</link>
      <description>arXiv:2511.11738v2 Announce Type: replace 
Abstract: This article presents the results and their discussion for the third wave (with n=23 participants) within a multinational longitudinal study that investigates the evolving paradigm of human-AI collaboration in problem-solving contexts. Building upon previous waves, our findings reveal the consolidation of a hybrid problem-solving culture characterized by strategic integration of AI tools within structured cognitive workflows. The data demonstrate near-universal AI adoption (95.7% with prior knowledge, 100% ChatGPT usage) primarily deployed through human-led sequences such as "Think, Internet, ChatGPT, Further Processing" (39.1%). However, this collaboration reveals a critical verification deficit that escalates with problem complexity. We empirically identify and quantify two systematic epistemic gaps: a belief-performance gap (up to +80.8 percentage points discrepancy between perceived and actual correctness) and a proof-belief gap (up to -16.8 percentage points between confidence and verification capability). These findings, derived from behavioral data and problem vignettes across complexity levels, indicate that the fundamental constraint on reliable AI-assisted work is solution validation rather than generation. The study concludes that educational and technological interventions must prioritize verification scaffolds (including assumption documentation protocols, adequacy criteria checklists, and triangulation procedures) to fortify the human role as critical validator in this new cognitive ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11738v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Huemmer, Theophile Shyiramunda, Franziska Durner, Michelle J. Cummings-Koether</dc:creator>
    </item>
    <item>
      <title>Leveraging Large Language Models for Career Mobility Analysis: A Study of Gender, Race, and Job Change Using U.S. Online Resume Profiles</title>
      <link>https://arxiv.org/abs/2511.12010</link>
      <description>arXiv:2511.12010v2 Announce Type: replace 
Abstract: We present a large-scale analysis of career mobility of college-educated U.S. workers using online resume profiles to investigate how gender, race, and job change options are associated with upward mobility. This study addresses key research questions of how the job changes affect their upward career mobility, and how the outcomes of upward career mobility differ by gender and race. We address data challenges -- such as missing demographic attributes, missing wage data, and noisy occupation labels -- through various data processing and Artificial Intelligence (AI) methods. In particular, we develop a large language models (LLMs) based occupation classification method known as FewSOC that achieves accuracy significantly higher than the original occupation labels in the resume dataset. Analysis of 228,710 career trajectories reveals that intra-firm occupation change has been found to facilitate upward mobility most strongly, followed by inter-firm occupation change and inter-firm lateral move. Women and Black college graduates experience significantly lower returns from job changes than men and White peers. Multilevel sensitivity analyses confirm that these disparities are robust to cluster-level heterogeneity and reveal additional intersectional patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12010v2</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Palakorn Achananuparp, Ye Xu, Yao Lu, Xavier Jayaraj Siddarth Ashok, Ee-Peng Lim</dc:creator>
    </item>
    <item>
      <title>OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists</title>
      <link>https://arxiv.org/abs/2511.16931</link>
      <description>arXiv:2511.16931v2 Announce Type: replace 
Abstract: With the rapid development of Large Language Models (LLMs), AI agents have demonstrated increasing proficiency in scientific tasks, ranging from hypothesis generation and experimental design to manuscript writing. Such agent systems are commonly referred to as "AI Scientists." However, existing AI Scientists predominantly formulate scientific discovery as a standalone search or optimization problem, overlooking the fact that scientific research is inherently a social and collaborative endeavor. Real-world science relies on a complex scientific infrastructure composed of collaborative mechanisms, contribution attribution, peer review, and structured scientific knowledge networks. Due to the lack of modeling for these critical dimensions, current systems struggle to establish a genuine research ecosystem or interact deeply with the human scientific community. To bridge this gap, we introduce OmniScientist, a framework that explicitly encodes the underlying mechanisms of human research into the AI scientific workflow. OmniScientist not only achieves end-to-end automation across data foundation, literature review, research ideation, experiment automation, scientific writing, and peer review, but also provides comprehensive infrastructural support by simulating the human scientific system, comprising: (1) a structured knowledge system built upon citation networks and conceptual correlations; (2) a collaborative research protocol (OSP), which enables seamless multi-agent collaboration and human researcher participation; and (3) an open evaluation platform (ScienceArena) based on blind pairwise user voting and Elo rankings. This infrastructure empowers agents to not only comprehend and leverage human knowledge systems but also to collaborate and co-evolve, fostering a sustainable and scalable innovation ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16931v2</guid>
      <category>cs.CY</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenyang Shao, Dehao Huang, Yu Li, Keyu Zhao, Weiquan Lin, Yining Zhang, Qingbin Zeng, Zhiyu Chen, Tianxing Li, Yifei Huang, Taozhong Wu, Xinyang Liu, Ruotong Zhao, Mengsheng Zhao, Jiaoyang Li, Xuhua Zhang, Yue Wang, Yuanyi Zhen, Fengli Xu, Yong Li, Tie-Yan Liu</dc:creator>
    </item>
    <item>
      <title>The DataSquad Experiment: Lessons for Preparing Data and Computer Scientists for Work</title>
      <link>https://arxiv.org/abs/2511.19688</link>
      <description>arXiv:2511.19688v2 Announce Type: replace 
Abstract: The DataSquad at Carleton College addresses a common problem at small liberal arts colleges: limited capacity for data services and few opportunities for students to gain practical experience with data and software development. Academic Technologist Paula Lackie designed the program as a work-study position that trains undergraduates through structured peer mentorship and real client projects. Students tackle data problems of increasing complexity-from basic data analysis to software development-while learning FAIR data principles and open science practices. The model's core components (peer mentorship structure, project-based learning, and communication training) make it adaptable to other institutions. UCLA and other colleges have adopted the model using openly shared materials through "DataSquad International." This paper describes the program's implementation at Carleton College and examines how structured peer mentorship can simultaneously improve institutional data services and provide students with professional skills and confidence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19688v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paula Lackie, Elliot Pickens, Dashiell Coyier</dc:creator>
    </item>
    <item>
      <title>Selecting for Less Discriminatory Algorithms: A Relational Search Framework for Navigating Fairness-Accuracy Trade-offs in Practice</title>
      <link>https://arxiv.org/abs/2506.01594</link>
      <description>arXiv:2506.01594v2 Announce Type: replace-cross 
Abstract: As machine learning models are increasingly embedded into society through high-stakes decision-making, selecting the right algorithm for a given task, audience, and sector presents a critical challenge, particularly in the context of fairness. Traditional assessments of model fairness have often framed fairness as an objective mathematical property, treating model selection as an optimization problem under idealized informational conditions. This overlooks model multiplicity as a consideration--that multiple models can deliver similar performance while exhibiting different fairness characteristics. Legal scholars have engaged this challenge through the concept of Less Discriminatory Algorithms (LDAs), which frames model selection as a civil rights obligation. In real-world deployment, this normative challenge is bounded by constraints on fairness experimentation, e.g., regulatory standards, institutional priorities, and resource capacity.
  Against these considerations, the paper revisits Lee and Floridi (2021)'s relational fairness approach using updated 2021 Home Mortgage Disclosure Act (HMDA) data, and proposes an expansion of the scope of the LDA search process. We argue that extending the LDA search horizontally, considering fairness across model families themselves, provides a lightweight complement, or alternative, to within-model hyperparameter optimization, when operationalizing fairness in non-experimental, resource constrained settings. Fairness metrics alone offer useful, but insufficient signals to accurately evaluate candidate LDAs. Rather, by using a horizontal LDA search approach with the relational trade-off framework, we demonstrate a responsible minimum viable LDA search on real-world lending outcomes. Organizations can modify this approach to systematically compare, evaluate, and select LDAs that optimize fairness and accuracy in a sector-based contextualized manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01594v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hana Samad, Michael Akinwumi, Jameel Khan, Christoph M\"ugge-Durum, Emmanuel O. Ogundimu</dc:creator>
    </item>
    <item>
      <title>Deploying Rapid Damage Assessments from sUAS Imagery for Disaster Response</title>
      <link>https://arxiv.org/abs/2511.03132</link>
      <description>arXiv:2511.03132v3 Announce Type: replace-cross 
Abstract: This paper presents the first AI/ML system for automating building damage assessment in uncrewed aerial systems (sUAS) imagery to be deployed operationally during federally declared disasters (Hurricanes Debby and Helene). In response to major disasters, sUAS teams are dispatched to collect imagery of the affected areas to assess damage; however, at recent disasters, teams collectively delivered between 47GB and 369GB of imagery per day, representing more imagery than can reasonably be transmitted or interpreted by subject matter experts in the disaster scene, thus delaying response efforts. To alleviate this data avalanche encountered in practice, computer vision and machine learning techniques are necessary. While prior work has been deployed to automatically assess damage in satellite imagery, there is no current state of practice for sUAS-based damage assessment systems, as all known work has been confined to academic settings. This work establishes the state of practice via the development and deployment of models for building damage assessment with sUAS imagery. The model development involved training on the largest known dataset of post-disaster sUAS aerial imagery, containing 21,716 building damage labels, and the operational training of 91 disaster practitioners. The best performing model was deployed during the responses to Hurricanes Debby and Helene, where it assessed a combined 415 buildings in approximately 18 minutes. This work contributes documentation of the actual use of AI/ML for damage assessment during a disaster and lessons learned to the benefit of the AI/ML research and user communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03132v3</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Manzini, Priyankari Perali, Robin R. Murphy</dc:creator>
    </item>
  </channel>
</rss>
