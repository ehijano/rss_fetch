<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Sep 2024 04:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Design of a Reformed Array Logic Binary Multiplier for High-Speed Computations</title>
      <link>https://arxiv.org/abs/2409.16405</link>
      <description>arXiv:2409.16405v1 Announce Type: new 
Abstract: Binary multipliers have long been a staple component in digital circuitry, serving crucial roles in microprocessor design, digital signal processing units and many more applications. This work presents a unique design for a multiplier that utilizes a reformed-array-logic approach to compute the product of two unsigned binary numbers. We employed a multiplexer and a barrel shifter to multiply partial products in a single clock cycle to speed up the traditional array logic. In addition, we have employed a combination of Carry Save Adders (CSA) and Ripple Carry Adders (RCA) to accumulate the partial products instead of using standalone RCAs to speed up the multiplication process further. Finally, we have demonstrated our design to perform multiplication of two 16-bit unsigned binary numbers on Cadence Virtuoso. Our design is modular and can be scaled up or down to accommodate the multiplication of any n-bit unsigned numbers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16405v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sakib Mohammad, Themistoklis Haniotakis</dc:creator>
    </item>
    <item>
      <title>PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences</title>
      <link>https://arxiv.org/abs/2409.16633</link>
      <description>arXiv:2409.16633v1 Announce Type: new 
Abstract: Deep Learning Recommendation Models (DLRMs) have become increasingly popular and prevalent in today's datacenters, consuming most of the AI inference cycles. The performance of DLRMs is heavily influenced by available bandwidth due to their large vector sizes in embedding tables and concurrent accesses. To achieve substantial improvements over existing solutions, novel approaches towards DLRM optimization are needed, especially, in the context of emerging interconnect technologies like CXL. This study delves into exploring CXL-enabled systems, implementing a process-in-fabric-switch (PIFS) solution to accelerate DLRMs while optimizing their memory and bandwidth scalability. We present an in-depth characterization of industry-scale DLRM workloads running on CXL-ready systems, identifying the predominant bottlenecks in existing CXL systems. We, therefore, propose PIFS-Rec, a PIFS-based scheme that implements near-data processing through downstream ports of the fabric switch. PIFS-Rec achieves a latency that is 3.89x lower than Pond, an industry-standard CXL-based system, and also outperforms BEACON, a state-of-the-art scheme, by 2.03x.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16633v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pingyi Huo, Anusha Devulapally, Hasan Al Maruf, Minseo Park, Krishnakumar Nair, Meena Arunachalam, Gulsum Gudukbay Akbulut, Mahmut Taylan Kandemir, Vijaykrishnan Narayanan</dc:creator>
    </item>
    <item>
      <title>HURRY: Highly Utilized, Reconfigurable ReRAM-based In-situ Accelerator with Multifunctionality</title>
      <link>https://arxiv.org/abs/2409.16640</link>
      <description>arXiv:2409.16640v1 Announce Type: new 
Abstract: Resistive random-access memory (ReRAM) crossbar arrays are suitable for efficient inference computations in neural networks due to their analog general matrix-matrix multiplication (GEMM) capabilities. However, traditional ReRAM-based accelerators suffer from spatial and temporal underutilization. We present HURRY, a reconfigurable and multifunctional ReRAM-based in-situ accelerator. HURRY uses a block activation scheme for concurrent activation of dynamically sized ReRAM portions, enhancing spatial utilization. Additionally, it incorporates functional blocks for convolution, ReLU, max pooling, and softmax computations to improve temporal utilization. System-level scheduling and data mapping strategies further optimize performance. Consequently, HURRY achieves up to 3.35x speedup, 5.72x higher energy efficiency, and 7.91x greater area efficiency compared to current ReRAM-based accelerators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16640v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hery Shin, Jae-Young Kim, Donghyuk Kim, Joo-Young Kim</dc:creator>
    </item>
    <item>
      <title>Omni 3D: BEOL-Compatible 3D Logic with Omnipresent Power, Signal, and Clock</title>
      <link>https://arxiv.org/abs/2409.16608</link>
      <description>arXiv:2409.16608v1 Announce Type: cross 
Abstract: This paper presents Omni 3D - a 3D-stacked device architecture that is naturally enabled by back-end-of-line (BEOL)-compatible transistors. Omni 3D arbitrarily interleaves metal layers for both signal/power with FETs in 3D (i.e., nFETs and pFETs are stacked in 3D). Thus, signal/power routing layers have fine-grained, all-sided access to the FET active regions maximizing 3D standard cell design flexibility. This is in sharp contrast to approaches such as back-side power delivery networks (BSPDNs), complementary FETs (CFETs), and stacked FETs. Importantly, the routing flexibility of Omni 3D is enabled by double-side routing and an interleaved metal (IM) layer for inter- and intra-cell routing, respectively. In this work, we explore Omni 3D variants (e.g., both with and without the IM layer) and optimize these variants using a virtual-source BEOL-FET compact model. We establish a physical design flow that efficiently utilizes the double-side routing in Omni 3D and perform a thorough design-technology-co-optimization (DTCO) of Omni 3D device architecture on several design points. From our design flow, we project 2.0x improvement in the energy-delay product and 1.5x reduction in area compared to the state-of-the-art CFETs with BSPDNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16608v1</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suhyeong Choi, Carlo Gilardi, Paul Gutwin, Robert M. Radway, Tathagata Srimani, Subhasish Mitra</dc:creator>
    </item>
    <item>
      <title>Ascend HiFloat8 Format for Deep Learning</title>
      <link>https://arxiv.org/abs/2409.16626</link>
      <description>arXiv:2409.16626v1 Announce Type: cross 
Abstract: This preliminary white paper proposes a novel 8-bit floating-point data format HiFloat8 (abbreviated as HiF8) for deep learning. HiF8 features tapered precision. For normal value encoding, it provides 7 exponents with 3-bit mantissa, 8 exponents with 2-bit mantissa, and 16 exponents with 1-bit mantissa. For denormal or subnormal value encoding, it extends the dynamic range by 7 extra powers of 2, from 31 to 38 binades (notice that FP16 covers 40 binades). Meanwhile, HiF8 encodes all the special values except that positive zero and negative zero are represented by only one bit-pattern. Thanks to the better balance between precision and dynamic range, HiF8 can be simultaneously used in both forward and backward passes of AI training. In this paper, we will describe the definition and rounding methods of HiF8, as well as the tentative training and inference solutions. To demonstrate the efficacy of HiF8 format, massive simulation results on various neural networks, including traditional neural networks and large language models (LLMs), will also be presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16626v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuanyong Luo, Zhongxing Zhang, Richard Wu, Hu Liu, Ying Jin, Kai Zheng, Minmin Wang, Zhanying He, Guipeng Hu, Luyao Chen, Tianchi Hu, Junsong Wang, Minqi Chen, Mikhaylov Dmitry, Korviakov Vladimir, Bobrin Maxim, Yuhao Hu, Guanfu Chen, Zeyi Huang</dc:creator>
    </item>
    <item>
      <title>PhD Forum: Efficient Privacy-Preserving Processing via Memory-Centric Computing</title>
      <link>https://arxiv.org/abs/2409.16777</link>
      <description>arXiv:2409.16777v1 Announce Type: cross 
Abstract: Privacy-preserving computation techniques like homomorphic encryption (HE) and secure multi-party computation (SMPC) enhance data security by enabling processing on encrypted data. However, the significant computational and CPU-DRAM data movement overhead resulting from the underlying cryptographic algorithms impedes the adoption of these techniques in practice. Existing approaches focus on improving computational overhead using specialized hardware like GPUs and FPGAs, but these methods still suffer from the same processor-DRAM overhead. Novel hardware technologies that support in-memory processing have the potential to address this problem. Memory-centric computing, or processing-in-memory (PIM), brings computation closer to data by introducing low-power processors called data processing units (DPUs) into memory. Besides its in-memory computation capability, PIM provides extensive parallelism, resulting in significant performance improvement over state-of-the-art approaches. We propose a framework that uses recently available PIM hardware to achieve efficient privacy-preserving computation. Our design consists of a four-layer architecture: (1) an application layer that decouples privacy-preserving applications from the underlying protocols and hardware; (2) a protocol layer that implements existing secure computation protocols (HE and MPC); (3) a data orchestration layer that leverages data compression techniques to mitigate the data transfer overhead between DPUs and host memory; (4) a computation layer which implements DPU kernels on which secure computation algorithms are built.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16777v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mpoki Mwaisela</dc:creator>
    </item>
    <item>
      <title>Benchmarking Deep Learning Models for Object Detection on Edge Computing Devices</title>
      <link>https://arxiv.org/abs/2409.16808</link>
      <description>arXiv:2409.16808v1 Announce Type: cross 
Abstract: Modern applications, such as autonomous vehicles, require deploying deep learning algorithms on resource-constrained edge devices for real-time image and video processing. However, there is limited understanding of the efficiency and performance of various object detection models on these devices. In this paper, we evaluate state-of-the-art object detection models, including YOLOv8 (Nano, Small, Medium), EfficientDet Lite (Lite0, Lite1, Lite2), and SSD (SSD MobileNet V1, SSDLite MobileDet). We deployed these models on popular edge devices like the Raspberry Pi 3, 4, and 5 with/without TPU accelerators, and Jetson Orin Nano, collecting key performance metrics such as energy consumption, inference time, and Mean Average Precision (mAP). Our findings highlight that lower mAP models such as SSD MobileNet V1 are more energy-efficient and faster in inference, whereas higher mAP models like YOLOv8 Medium generally consume more energy and have slower inference, though with exceptions when accelerators like TPUs are used. Among the edge devices, Jetson Orin Nano stands out as the fastest and most energy-efficient option for request handling, despite having the highest idle energy consumption. These results emphasize the need to balance accuracy, speed, and energy efficiency when deploying deep learning models on edge devices, offering valuable guidance for practitioners and researchers selecting models and devices for their applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16808v1</guid>
      <category>cs.CV</category>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.SE</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Daghash K. Alqahtani, Aamir Cheema, Adel N. Toosi</dc:creator>
    </item>
    <item>
      <title>System-Level Design Space Exploration for High-Level Synthesis under End-to-End Latency Constraints</title>
      <link>https://arxiv.org/abs/2408.10431</link>
      <description>arXiv:2408.10431v2 Announce Type: replace 
Abstract: Many modern embedded systems have end-to-end (EtoE) latency constraints that necessitate precise timing to ensure high reliability and functional correctness. The combination of High-Level Synthesis (HLS) and Design Space Exploration (DSE) enables the rapid generation of embedded systems using various constraints/directives to find Pareto-optimal configurations. Current HLS DSE approaches often address latency by focusing on individual components, without considering the EtoE latency during the system-level optimization process. However, to truly optimize the system under EtoE latency, we need a holistic approach that analyzes individual system components' timing constraints in the context of how the different components interact and impact the overall design. This paper presents a novel system-level HLS DSE approach, called EtoE-DSE, that accommodates EtoE latency and variable timing constraints for complex multi-component application-specific embedded systems. EtoE-DSE employs a latency estimation model and a pathfinding algorithm to identify and estimate the EtoE latency for paths between any endpoints. It also uses a frequency-based segmentation process to segment and prune the design space, alongside a latency-constrained optimization algorithm for efficiently and accurately exploring the system-level design space. We evaluate our approach using a real-world use case of an autonomous driving subsystem compared to the state-of-the-art in HLS DSE. We show that our approach yields substantially better optimization results than prior DSE approaches, improving the quality of results by up to 89.26%, while efficiently identifying Pareto-optimal configurations in terms of energy and area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10431v2</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuchao Liao, Tosiron Adegbija, Roman Lysecky</dc:creator>
    </item>
    <item>
      <title>Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting</title>
      <link>https://arxiv.org/abs/2402.13827</link>
      <description>arXiv:2402.13827v2 Announce Type: replace-cross 
Abstract: 3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms the neural radiance field (NeRF) in terms of both speed and image quality. 3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects these Gaussians onto the 2D image plane for rendering. However, during the rendering process, a substantial number of unnecessary 3D Gaussians exist for the current view direction, resulting in significant computation costs associated with their identification. In this paper, we propose a computational reduction technique that quickly identifies unnecessary 3D Gaussians in real-time for rendering the current view without compromising image quality. This is accomplished through the offline clustering of 3D Gaussians that are close in distance, followed by the projection of these clusters onto a 2D image plane during runtime. Additionally, we analyze the bottleneck associated with the proposed technique when executed on GPUs and propose an efficient hardware architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360 dataset, the proposed technique excludes 63% of 3D Gaussians on average before the 2D image projection, which reduces the overall rendering computation by almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The proposed accelerator also achieves a speedup of 10.7x compared to a GPU.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13827v2</guid>
      <category>cs.CV</category>
      <category>cs.AR</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joongho Jo, Hyeongwon Kim, Jongsun Park</dc:creator>
    </item>
  </channel>
</rss>
