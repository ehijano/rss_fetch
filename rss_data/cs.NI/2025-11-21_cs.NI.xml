<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Nov 2025 10:39:56 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Improving Resiliency of Vital Services in Flood-Affected Regions of Bangladesh Using Next-Generation Opportunistic DTN Edge Ad Hoc Networks</title>
      <link>https://arxiv.org/abs/2511.15710</link>
      <description>arXiv:2511.15710v1 Announce Type: new 
Abstract: Opportunistic routing architectures offer a resilient communication paradigm in environments where conventional networks fail due to disrupted infrastructure, dynamic node mobility, and intermittent connectivity conditions that commonly arise during large-scale disasters. In Bangladesh, recurring floods severely hinder communication systems, isolating affected populations and obstructing emergency response efforts. To address these challenges, there is a growing demand for intelligent and adaptive routing solutions capable of sustaining critical communication and services without relying on fixed infrastructure. This research presents AZIZA (Adaptive Zone-based Intelligent Fully Distributed Trust-Aware Routing Protocol), a next-generation opportunistic protocol designed to improve the resiliency of critical communication and services in disaster-prone and flood-affected regions. AZIZA supports adaptive data delivery for emergency alerts, sensor readings, and inter-zone coordination by integrating (1) zone-based forwarding to optimize localized transmission, (2) trust-aware logic to bypass uncooperative or malicious nodes, and (3) context-driven decision-making based on trust metrics, residual energy, and historical delivery patterns. AZIZA operates over lightweight, infrastructure-less edge ad hoc networks comprising mobile phones, UAVs, and ground vehicles acting as decentralized service relays. Simulation results using The Opportunistic Network Environment (ONE) Simulator configured with real-world mobility traces and flood data from Bangladesh demonstrate that AZIZA significantly outperforms benchmark approaches in delivery reliability, energy efficiency, and routing resilience. As a scalable and deployable framework, AZIZA advances the use of next-generation opportunistic routing in environments where traditional systems routinely collapse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15710v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Main Uddin Hasan, Milena Radenkovic</dc:creator>
    </item>
    <item>
      <title>Inter-Satellite Link Configuration for Fast Delivery in Low-Earth-Orbit Constellations</title>
      <link>https://arxiv.org/abs/2511.15861</link>
      <description>arXiv:2511.15861v1 Announce Type: new 
Abstract: End-to-end latency in large low-Earth-orbit (LEO) constellations is dominated by propagation delay, making total delay roughly proportional to the network diameter, the longest shortest path in hops. Current inter-satellite link (ISL) layouts have rarely been optimized to minimize network diameter while simultaneously satisfying physical and operational constraints, including maximum link distance, line-of-sight, per-satellite hardware limits, and long-term link viability over orbital periods. In this study, the selection and assignment of inter-plane ISLs is formulated as a diameter-minimization problem on a Starlink-inspired Walker-Delta constellation in which each satellite is equipped with two fixed intra-plane links and may activate up to two inter-plane links. Beginning with a feasible baseline, the topology is iteratively refined by a local-search procedure that replaces or reinforces links to shrink the diameter. The resulting ISL configuration meets all geometric and hardware limits, preserves link stability across multiple orbital periods, and yields a sparse, diameter-aware graph with potential for centralized routing capabilities. Simulations demonstrate that the proposed algorithm achieves low worst-case latency without compromising ISL stability, and the trade-off between hop count and long-term link stability is empirically measured for guidance of future LEO network deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15861v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Arman Mollakhani, Jerayu Tiamraj, Shu-Jie Cao, Dongning Guo</dc:creator>
    </item>
    <item>
      <title>Graph-Aware Temporal Encoder Based Service Migration and Resource Allocation in Satellite Networks</title>
      <link>https://arxiv.org/abs/2511.16011</link>
      <description>arXiv:2511.16011v1 Announce Type: new 
Abstract: The rapid expansion of latency-sensitive applications has sparked renewed interest in deploying edge computing capabilities aboard satellite constellations, aiming to achieve truly global and seamless service coverage. On one hand, it is essential to allocate the limited onboard computational and communication resources efficiently to serve geographically distributed users. On the other hand, the dynamic nature of satellite orbits necessitates effective service migration strategies to maintain service continuity and quality as the coverage areas of satellites evolve. We formulate this problem as a spatio-temporal Markov decision process, where satellites, ground users, and flight users are modeled as nodes in a time-varying graph. The node features incorporate queuing dynamics to characterize packet loss probabilities. To solve this problem, we propose a Graph-Aware Temporal Encoder (GATE) that jointly models spatial correlations and temporal dynamics. GATE uses a two-layer graph convolutional network to extract inter-satellite and user dependencies and a temporal convolutional network to capture their short-term evolution, producing unified spatio-temporal representations. The resulting spatial-temporal representations are passed into a Hybrid Proximal Policy Optimization (HPPO) framework. This framework features a multi-head actor that outputs both discrete service migration decisions and continuous resource allocation ratios, along with a critic for value estimation. We conduct extensive simulations involving both persistent and intermittent users distributed across real-world population centers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16011v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haotong Wang, Jun Du, Chunxiao Jiang, Jintao Wang, M\'erouane Debbah, Zhu Han</dc:creator>
    </item>
    <item>
      <title>Modeling Pointing, Acquisition, and Tracking Delays in Free-Space Optical Satellite Networks</title>
      <link>https://arxiv.org/abs/2511.16063</link>
      <description>arXiv:2511.16063v1 Announce Type: new 
Abstract: Free-space optical inter-satellite links (OISLs) enable high-capacity space communications but require precise Pointing, Acquisition, and Tracking (PAT) between links. Current scheduling approaches often overlook or oversimplify PAT delays, leading to inefficient contact planning and overestimated network capacities. We present a validated model for quantifying retargeting delays, defined as the delay-inducing portion of PAT before data transmission begins, encompassing coarse pointing, fine pointing, and the handover to tracking. The model is grounded in mission data from NASA TBIRD, LLCD, DSOC, and ESA's Lunar Optical Communication Terminal. We find that PAT delays exhibit multimodal distributions based on prior link geometry and scale nonlinearly with initial pointing uncertainty and optical beam width. Integrating these delay models into routing and scheduling algorithms will enable more accurate contact planning and higher utilization in optical networks. The proposed model provides a foundation for evaluating performance and designing algorithms for future large-scale optical satellite networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16063v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jason Gerard, Juan A. Fraire, Sandra C\'espedes</dc:creator>
    </item>
    <item>
      <title>Bio-inspired Integrated Networking and Control for Large-Scale Swarm: A Hierarchical Co-design</title>
      <link>https://arxiv.org/abs/2511.16067</link>
      <description>arXiv:2511.16067v1 Announce Type: new 
Abstract: Unmanned aerial vehicle (UAV) swarms encounter the challenge of high overhead due to both network management and formation control requirements. In this paper, we propose a Bio-inspired Integrated Networking and Control (BINC) scheme, enabling efficient formation management for swarms comprising thousands of UAVs. The scheme forms a two-layer hierarchical structure, where network clusters and formations share the same groups so that cross-cluster control is eliminated. For networking, we design a fused routing message together with control information to reduce overhead, and limit clusters' size to local two-hop topologies for fast command transmission. For controlling, we develop a hybrid bio-inspired control approach, including a pigeon-like leader-follower algorithm within formations under the consideration of cluster topology maintenance, and a starling-like algorithm among formations that helps to improve the ability of obstacle avoidance. We establish a simulation platform for UAV swarms with over 1000 nodes, and experimental results show that the proposed BINC scheme can achieve highly maneuverable swarm formation marching with significant reduction on communication overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16067v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huan Lin, Dakai Liu, Lianghui Ding, Lin Wang, Feng Yang</dc:creator>
    </item>
    <item>
      <title>Green Distributed AI Training: Orchestrating Compute Across Renewable-Powered Micro Datacenters</title>
      <link>https://arxiv.org/abs/2511.16182</link>
      <description>arXiv:2511.16182v1 Announce Type: new 
Abstract: The accelerating expansion of AI workloads is colliding with an energy landscape increasingly dominated by intermittent renewable generation. While vast quantities of zero-carbon energy are routinely curtailed, today's centralized datacenter architectures remain poorly matched to this reality in both energy proportionality and geographic flexibility. This work envisions a shift toward a distributed fabric of renewable-powered micro-datacenters that dynamically follow the availability of surplus green energy through live workload migration.
  At the core of this vision lies a formal feasibility-domain model that delineates when migratory AI computation is practically achievable. By explicitly linking checkpoint size, wide-area bandwidth, and renewable-window duration, the model reveals that migration is almost always energetically justified, and that time-not energy-is the dominant constraint shaping feasibility. This insight enables the design of a feasibility-aware orchestration framework that transforms migration from a best-effort heuristic into a principled control mechanism. Trace-driven evaluation shows that such orchestration can simultaneously reduce non-renewable energy use and improve performance stability, overcoming the tradeoffs of purely energy-driven strategies.
  Beyond the immediate feasibility analysis, the extended version explores the architectural horizon of renewable-aware AI infrastructures. It examines the role of emerging ultra-efficient GPU-enabled edge platforms, anticipates integration with grid-level control and demand-response ecosystems, and outlines paths toward supporting partially migratable and distributed workloads. The work positions feasibility-aware migration as a foundational building block for a future computing paradigm in which AI execution becomes fluid, geographically adaptive, and aligned with renewable energy availability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16182v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giuseppe Tomei, Andrea Mayer, Giuseppe Alcini, Stefano Salsano</dc:creator>
    </item>
    <item>
      <title>Multi-Band Wireless Access-and-Backhaul (WAB) for 5G: Implementation and Experiments</title>
      <link>https://arxiv.org/abs/2511.16259</link>
      <description>arXiv:2511.16259v1 Announce Type: new 
Abstract: The growing demand for wireless capacity and coverage has driven research into new radio architectures and higher frequency bands. The recently standardized Wireless Access Backhaul (WAB) architecture represents a key evolution, enabling cost-effective network densification through wireless relaying. This paper presents the first experimental realization of a multi-band WAB testbed, combining an FR2 backhaul and an FR1 access link using open-source software and commercial off-the-shelf components. The proposed framework validates the feasibility of end-to-end WAB operation and demonstrates its ability to extend FR2 coverage while maintaining compatibility with legacy FR1 user equipment. Experimental campaigns conducted in vehicular and outdoor-to-indoor scenarios confirm that WAB effectively mitigates the limitations of FR2 links, particularly in uplink and Non-Line-of-Sight conditions. These results highlight WAB as a practical and scalable approach for next-generation wireless networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16259v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Rubaltelli, Marcello Morini, Eugenio Moro, Ilario Filippini</dc:creator>
    </item>
    <item>
      <title>Toward hyper-adaptive AI-enabled 6G networks for energy efficiency: techniques, classifications and tradeoffs</title>
      <link>https://arxiv.org/abs/2511.16296</link>
      <description>arXiv:2511.16296v1 Announce Type: new 
Abstract: Energy efficiency is shaping up to be one of the most challenging issues for 6G networks. The reason is fairly straightforward: Networks will need to meet extreme service demands while remaining sustainable and traditional optimization techniques are too limited. With users moving, traffic swinging unpredictably and services pulling in different directions, management has to be adaptive and AI may offer a way forward. This survey looks at how well AI-based methods actually deliver on that promise. We organize the review around practical use cases. For each use case, we examine how AI techniques contribute to feedback-driven adaptability and rapid decision-making under dynamic conditions. We then evaluate them against seven central dynamic aspects that we consider unavoidable in 6G. The survey also discusses crucial tradeoffs between energy efficiency and the remaining 6G main objectives such as latency, reliability, fairness and coverage, and finally identifies gaps and future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16296v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mariem Zayene, Oussama Habachi, Gerard Chalhoub</dc:creator>
    </item>
    <item>
      <title>Payment-failure times for random Lightning paths</title>
      <link>https://arxiv.org/abs/2511.16376</link>
      <description>arXiv:2511.16376v1 Announce Type: new 
Abstract: We study a random process over graphs inspired by the way payments are executed in the Lightning Network, the main layer-two solution on top of Bitcoin. We first prove almost tight upper and lower bounds on the time it takes for a payment failure to occur, as a function of the number of nodes and the edge capacities, when the underlying graph is complete. Then, we show how such a random process is related to the edge-betweenness centrality measure and we prove upper and lower bounds for arbitrary graphs as a function of edge-betweenness and capacity. Finally, we validate our theoretical results by running extensive simulations over some classes of graphs, including snapshots of the real Lightning Network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16376v1</guid>
      <category>cs.NI</category>
      <category>math.PR</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taki E. M. Abedesselam, Fabio Giacomelli, Francesco Pasquale, Michele Salvi</dc:creator>
    </item>
    <item>
      <title>Machine Learning Epidemic Predictions Using Agent-based Wireless Sensor Network Models</title>
      <link>https://arxiv.org/abs/2511.15982</link>
      <description>arXiv:2511.15982v1 Announce Type: cross 
Abstract: The lack of epidemiological data in wireless sensor networks (WSNs) is a fundamental difficulty in constructing robust models to forecast and mitigate threats such as viruses and worms. Many studies have examined different epidemic models for WSNs, focusing on how malware infections spread given the network's specific properties, including energy limits and node mobility. In this study, an agent-based implementation of the susceptible-exposed-infected-recovered-vaccinated (SEIRV) mathematical model was employed for machine learning (ML) predictions. Using tools such as NetLogo's BehaviorSpace and Python, two epidemic synthetic datasets were generated and prepared for the application of several ML algorithms. Posed as a regression problem, the infected and recovered nodes were predicted, and the performance of these algorithms is compared using the error metrics of the train and test sets. The predictions performed well, with low error metrics and high R^2 values (0.997, 1.000, 0.999, 1.000), indicating an effective fit to the training set. The validation values were lower (0.992, 0.998, 0.971, and 0.999), as is typical when evaluating model performance on unseen data. Based on the recorded performances, support vector, linear, Lasso, Ridge, and ElasticNet regression were among the worst-performing algorithms, while Random Forest, XGBoost, Decision Trees, and k-nearest neighbors achieved the best results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15982v1</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chukwunonso Henry Nwokoye, Blessing Oluchi, Sharna Waldron, Peace Ezzeh</dc:creator>
    </item>
    <item>
      <title>Reasoning Meets Representation: Envisioning Neuro-Symbolic Wireless Foundation Models</title>
      <link>https://arxiv.org/abs/2511.16369</link>
      <description>arXiv:2511.16369v1 Announce Type: cross 
Abstract: Recent advances in Wireless Physical Layer Foundation Models (WPFMs) promise a new paradigm of universal Radio Frequency (RF) representations. However, these models inherit critical limitations found in deep learning such as the lack of explainability, robustness, adaptability, and verifiable compliance with physical and regulatory constraints. In addition, the vision for an AI-native 6G network demands a level of intelligence that is deeply embedded into the systems and is trustworthy. In this vision paper, we argue that the neuro-symbolic paradigm, which integrates data-driven neural networks with rule- and logic-based symbolic reasoning, is essential for bridging this gap. We envision a novel Neuro-Symbolic framework that integrates universal RF embeddings with symbolic knowledge graphs and differentiable logic layers. This hybrid approach enables models to learn from large datasets while reasoning over explicit domain knowledge, enabling trustworthy, generalizable, and efficient wireless AI that can meet the demands of future networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16369v1</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jaron Fontaine, Mohammad Cheraghinia, John Strassner, Adnan Shahid, Eli De Poorter</dc:creator>
    </item>
    <item>
      <title>Optimizing Quantum Key Distribution Network Performance using Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2511.16468</link>
      <description>arXiv:2511.16468v1 Announce Type: cross 
Abstract: This paper proposes an optimization of Quantum Key Distribution (QKD) Networks using Graph Neural Networks (GNN) framework. Today, the development of quantum computers threatens the security systems of classical cryptography. Moreover, as QKD networks are designed for protecting secret communication, they suffer from multiple operational difficulties: adaptive to dynamic conditions, optimization for multiple parameters and effective resource utilization. In order to overcome these obstacles, we propose a GNN-based framework which can model QKD networks as dynamic graphs and extracts exploitable characteristics from these networks' structure. The graph contains not only topological information but also specific characteristics associated with quantum communication (the number of edges between nodes, etc). Experimental results demonstrate that the GNN-optimized QKD network achieves a substantial increase in total key rate (from 27.1 Kbits/s to 470 Kbits/s), a reduced average QBER (from 6.6% to 6.0%), and maintains path integrity with a slight reduction in average transmission distance (from 7.13 km to 6.42 km). Furthermore, we analyze network performance across varying scales (10 to 250 nodes), showing improved link prediction accuracy and enhanced key generation rate in medium-sized networks. This work introduces a novel operation mode for QKD networks, shifting the paradigm of network optimization through adaptive and scalable quantum communication systems that enhance security and performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16468v1</guid>
      <category>quant-ph</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akshit Pramod Anchan, Ameiy Acharya, Leki Chom Thungon</dc:creator>
    </item>
    <item>
      <title>Towards a Wireless Physical-Layer Foundation Model: Challenges and Strategies</title>
      <link>https://arxiv.org/abs/2403.12065</link>
      <description>arXiv:2403.12065v2 Announce Type: replace 
Abstract: Artificial intelligence (AI) plays an important role in the dynamic landscape of wireless communications, solving challenges unattainable by traditional approaches. This paper discusses the evolution of wireless AI, emphasizing the transition from isolated task-specific models to more generalizable and adaptable AI models inspired by recent successes in large language models (LLMs) and computer vision. To overcome task-specific AI strategies in wireless networks, we propose a unified wireless physical-layer foundation model (WPFM). Challenges include the design of effective pre-training tasks, support for embedding heterogeneous time series and human-understandable interaction. The paper presents a strategic framework, focusing on embedding wireless time series, self-supervised pre-training, and semantic representation learning. The proposed WPFM aims to understand and describe diverse wireless signals, allowing human interactivity with wireless networks. The paper concludes by outlining next research steps for WPFMs, including the integration with LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12065v2</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ICCWorkshops59551.2024.10615509</arxiv:DOI>
      <dc:creator>Jaron Fontaine, Adnan Shahid, Eli De Poorter</dc:creator>
    </item>
  </channel>
</rss>
