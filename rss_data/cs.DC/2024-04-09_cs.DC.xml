<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Apr 2024 04:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Achieving High-Performance Fault-Tolerant Routing in HyperX Interconnection Networks</title>
      <link>https://arxiv.org/abs/2404.04315</link>
      <description>arXiv:2404.04315v1 Announce Type: new 
Abstract: Interconnection networks are key actors that condition the performance of current large datacenter and supercomputer systems. Both topology and routing are critical aspects that must be carefully considered for a competitive system network design. Moreover, when daily failures are expected, this tandem should exhibit resilience and robustness. Low-diameter networks, including HyperX, are cheaper than typical Fat Trees. But, to be really competitive, they have to employ evolved routing algorithms to both balance traffic and tolerate failures.
  In this paper, SurePath, an efficient fault-tolerant routing mechanism for HyperX topology is introduced and evaluated. SurePath leverages routes provided by standard routing algorithms and a deadlock avoidance mechanism based on an Up/Down escape subnetwork. This mechanism not only prevents deadlock but also allows for a fault-tolerant solution for these networks. SurePath is thoroughly evaluated in the paper under different traffic patterns, showing no performance degradation under extremely faulty scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04315v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Crist\'obal Camarero, Alejandro Cano, Carmen Mart\'inez, Ram\'on Beivide</dc:creator>
    </item>
    <item>
      <title>Sharding Distributed Data Databases: A Critical Review</title>
      <link>https://arxiv.org/abs/2404.04384</link>
      <description>arXiv:2404.04384v1 Announce Type: new 
Abstract: This article examines the significant challenges encountered in implementing sharding within distributed replication systems. It identifies the impediments of achieving consensus among large participant sets, leading to scalability, throughput, and performance limitations. These issues primarily arise due to the message complexity inherent in consensus mechanisms. In response, we investigate the potential of sharding to mitigate these challenges, analyzing current implementations within distributed replication systems. Additionally, we offer a comprehensive review of replication systems, encompassing both classical distributed databases as well as Distributed Ledger Technologies (DLTs) employing sharding techniques. Through this analysis, the article aims to provide insights into addressing the scalability and performance concerns in distributed replication systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04384v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siamak Solat</dc:creator>
    </item>
    <item>
      <title>Stable Blockchain Sharding under Adversarial Transaction Generation</title>
      <link>https://arxiv.org/abs/2404.04438</link>
      <description>arXiv:2404.04438v1 Announce Type: new 
Abstract: Sharding is used to improve the scalability and performance of blockchain systems. We investigate the stability of blockchain sharding, where transactions are continuously generated by an adversarial model. The system consists of $n$ processing nodes that are divided into $s$ shards. Following the paradigm of classical adversarial queuing theory, transactions are continuously received at injection rate $\rho \leq 1$ and burstiness $b &gt; 0$. We give an absolute upper bound $\max\{ \frac{2}{k+1}, \frac{2}{ \left\lfloor\sqrt{2s}\right\rfloor}\}$ on the maximum injection rate for which any scheduler could guarantee bounded queues and latency of transactions, where $k$ is the number of shards that each transaction accesses. We next give a basic distributed scheduling algorithm for uniform systems where shards are equally close to each other. To guarantee stability, the injection rate is limited to $\rho \leq \max\{ \frac{1}{18k}, \frac{1}{\lceil 18 \sqrt{s} \rceil} \}$. We then provide a fully distributed scheduling algorithm for non-uniform systems where shards are arbitrarily far from each other. By using a hierarchical clustering of the shards, stability is guaranteed with injection rate $\rho \leq \frac{1}{c_1d \log^2 s} \cdot \max\{ \frac{1}{k}, \frac{1}{\sqrt{s}} \}$, where $d$ is the worst distance of any transaction to the shards it will access, and $c_1$ is some positive constant. We also conduct simulations to evaluate the algorithms and measure the average queue sizes and latency throughout the system. To our knowledge, this is the first adversarial stability analysis of sharded blockchain systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04438v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ramesh Adhikari, Costas Busch, Dariusz Kowalski</dc:creator>
    </item>
    <item>
      <title>Evaluation of Programming Models and Performance for Stencil Computation on Current GPU Architectures</title>
      <link>https://arxiv.org/abs/2404.04441</link>
      <description>arXiv:2404.04441v1 Announce Type: new 
Abstract: Accelerated computing is widely used in high-performance computing. Therefore, it is crucial to experiment and discover how to better utilize GPUGPUs latest generations on relevant applications. In this paper, we present results and share insights about highly tuned stencil-based kernels for NVIDIA Ampere (A100) and Hopper (GH200) architectures. Performance results yield useful insights into the behavior of this type of algorithms for these new accelerators. This knowledge can be leveraged by many scientific applications which involve stencils computations. Further, evaluation of three different programming models: CUDA, OpenACC, and OpenMP target offloading is conducted on aforementioned accelerators. We extensively study the performance and portability of various kernels under each programming model and provide corresponding optimization recommendations. Furthermore, we compare the performance of different programming models on the mentioned architectures. Up to 58% performance improvement was achieved against the previous GPGPU's architecture generation for an highly optimized kernel of the same class, and up to 42% for all classes. In terms of programming models, and keeping portability in mind, optimized OpenACC implementation outperforms OpenMP implementation by 33%. If portability is not a factor, our best tuned CUDA implementation outperforms the optimized OpenACC one by 2.1x.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04441v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Baodi Shan, Mauricio Araya-Polo</dc:creator>
    </item>
    <item>
      <title>Towards Reconfigurable Linearizable Reads</title>
      <link>https://arxiv.org/abs/2404.05470</link>
      <description>arXiv:2404.05470v1 Announce Type: new 
Abstract: Linearizable datastores are desirable because they provide users with the illusion that the datastore is run on a single machine that performs client operations one at a time. To reduce the performance cost of providing this illusion, many specialized algorithms for linearizable reads have been proposed which significantly improve read performance compared to write performance. The main difference between these specialized algorithms is their performance under different workloads. Unfortunately, since a datastore's workload is often unknown or changes over time and system designers must decide on a single read algorithm to implement ahead of time, a datastore's performance is often suboptimal as it cannot adapt to workload changes. In this paper, we lay the groundwork for addressing this problem by proposing Chameleon, an algorithm for linearizable reads that provides a principled approach for datastores to switch between existing read algorithms at runtime. The key observation that enables this generalization is that all existing algorithms are specific read-write quorum systems. Chameleon constructs a generic read-write quorum system, by using tokens that are included to complete write and read operations. This token quorum system enables Chameleon to mimic existing read algorithms and switch between them by transferring these tokens between processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05470v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Myles Thiessen, Aleksey Panas, Guy Khazma, Eyal de Lara</dc:creator>
    </item>
    <item>
      <title>Efficient Distributed Data Structures for Future Many-core Architectures</title>
      <link>https://arxiv.org/abs/2404.05515</link>
      <description>arXiv:2404.05515v1 Announce Type: new 
Abstract: We study general techniques for implementing distributed data structures on top of future many-core architectures with non cache-coherent or partially cache-coherent memory. With the goal of contributing towards what might become, in the future, the concurrency utilities package in Java collections for such architectures, we end up with a comprehensive collection of data structures by considering different variants of these techniques. To achieve scalability, we study a generic scheme which makes all our implementations hierarchical. We consider a collection of known techniques for improving the scalability of concurrent data structures and we adjust them to work in our setting. We have performed experiments which illustrate that some of these techniques have indeed high impact on achieving scalability. Our experiments also reveal the performance and scalability power of the hierarchical approach. We finally present experiments to study energy consumption aspects of the proposed techniques by using an energy model recently proposed for such architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05515v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Panagiota Fatourou, Nikolaos D. Kallimanis, Eleni Kanellou, Odysseas Makridakis, Christi Symeonidou</dc:creator>
    </item>
    <item>
      <title>KaMPIng: Flexible and (Near) Zero-overhead C++ Bindings for MPI</title>
      <link>https://arxiv.org/abs/2404.05610</link>
      <description>arXiv:2404.05610v1 Announce Type: new 
Abstract: The Message-Passing Interface (MPI) and C++ form the backbone of high-performance computing, but MPI only provides C and Fortran bindings. While this offers great language interoperability, high-level programming languages like C++ make software development quicker and less error-prone.
  We propose novel C++ language bindings that cover all abstraction levels from low-level MPI calls to convenient STL-style bindings, where most parameters are inferred from a small subset of parameters, by bringing named parameters to C++. This enables rapid prototyping and fine-tuning runtime behavior and memory management. A flexible type system and additional safeness guarantees help to prevent programming errors.
  By exploiting C++'s template-metaprogramming capabilities, this has (near) zero-overhead, as only required code paths are generated at compile time.
  We demonstrate that our library is a strong foundation for a future distributed standard library using multiple application benchmarks, ranging from text-book sorting algorithms to phylogenetic interference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05610v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Demian Hespe, Lukas H\"ubner, Florian Kurpicz, Peter Sanders, Matthias Schimek, Daniel Seemaier, Christoph Stelz, Tim Niklas Uhl</dc:creator>
    </item>
    <item>
      <title>VTR: An Optimized Vision Transformer for SAR ATR Acceleration on FPGA</title>
      <link>https://arxiv.org/abs/2404.04527</link>
      <description>arXiv:2404.04527v1 Announce Type: cross 
Abstract: Synthetic Aperture Radar (SAR) Automatic Target Recognition (ATR) is a key technique used in military applications like remote-sensing image recognition. Vision Transformers (ViTs) are the current state-of-the-art in various computer vision applications, outperforming their CNN counterparts. However, using ViTs for SAR ATR applications is challenging due to (1) standard ViTs require extensive training data to generalize well due to their low locality; the standard SAR datasets, however, have a limited number of labeled training data which reduces the learning capability of ViTs; (2) ViTs have a high parameter count and are computation intensive which makes their deployment on resource-constrained SAR platforms difficult. In this work, we develop a lightweight ViT model that can be trained directly on small datasets without any pre-training by utilizing the Shifted Patch Tokenization (SPT) and Locality Self-Attention (LSA) modules. We directly train this model on SAR datasets which have limited training samples to evaluate its effectiveness for SAR ATR applications. We evaluate our proposed model, that we call VTR (ViT for SAR ATR), on three widely used SAR datasets: MSTAR, SynthWakeSAR, and GBSAR. Further, we propose a novel FPGA accelerator for VTR, in order to enable deployment for real-time SAR ATR applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04527v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sachini Wickramasinghe, Dhruv Parikh, Bingyi Zhang, Rajgopal Kannan, Viktor Prasanna, Carl Busart</dc:creator>
    </item>
    <item>
      <title>Adapting Multi-objectivized Software Configuration Tuning</title>
      <link>https://arxiv.org/abs/2404.04744</link>
      <description>arXiv:2404.04744v1 Announce Type: cross 
Abstract: When tuning software configuration for better performance (e.g., latency or throughput), an important issue that many optimizers face is the presence of local optimum traps, compounded by a highly rugged configuration landscape and expensive measurements. To mitigate these issues, a recent effort has shifted to focus on the level of optimization model (called meta multi-objectivization or MMO) instead of designing better optimizers as in traditional methods. This is done by using an auxiliary performance objective, together with the target performance objective, to help the search jump out of local optima. While effective, MMO needs a fixed weight to balance the two objectives-a parameter that has been found to be crucial as there is a large deviation of the performance between the best and the other settings. However, given the variety of configurable software systems, the "sweet spot" of the weight can vary dramatically in different cases and it is not possible to find the right setting without time-consuming trial and error. In this paper, we seek to overcome this significant shortcoming of MMO by proposing a weight adaptation method, dubbed AdMMO. Our key idea is to adaptively adjust the weight at the right time during tuning, such that a good proportion of the nondominated configurations can be maintained. Moreover, we design a partial duplicate retention mechanism to handle the issue of too many duplicate configurations without losing the rich information provided by the "good" duplicates.
  Experiments on several real-world systems, objectives, and budgets show that, for 71% of the cases, AdMMO is considerably superior to MMO and a wide range of state-of-the-art optimizers while achieving generally better efficiency with the best speedup between 2.2x and 20x.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04744v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3643751</arxiv:DOI>
      <dc:creator>Tao Chen, Miqing Li</dc:creator>
    </item>
    <item>
      <title>Iniva: Inclusive and Incentive-compatible Vote Aggregation</title>
      <link>https://arxiv.org/abs/2404.04948</link>
      <description>arXiv:2404.04948v1 Announce Type: cross 
Abstract: Many blockchain platforms use committee-based consensus for scalability, finality, and security. In this consensus scheme, a committee decides which blocks get appended to the chain, typically through several voting phases. Platforms typically leverage the committee members' recorded votes to reward, punish, or detect failures. A common approach is to let the block proposer decide which votes to include, opening the door to possible attacks. For example, a malicious proposer can omit votes from targeted committee members, resulting in lost profits and, ultimately, their departure from the system.
  This paper presents Iniva, an inclusive and incentive-compatible vote aggregation scheme that prevents such vote omission attacks. Iniva relies on a tree overlay with carefully selected fallback paths, making it robust against process failures without needing reconfiguration or additional redundancy. Our analysis shows that Iniva significantly reduces the chance to omit individual votes while ensuring that omitting many votes incurs a significant cost. In addition, our experimental results show that Iniva enjoys robustness, scalability, and reasonable throughput.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04948v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Arian Baloochestani, Hanish Gogada, Leander Jehl, Hein Meling</dc:creator>
    </item>
    <item>
      <title>Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts</title>
      <link>https://arxiv.org/abs/2404.05019</link>
      <description>arXiv:2404.05019v1 Announce Type: cross 
Abstract: Expert parallelism has been introduced as a strategy to distribute the computational workload of sparsely-gated mixture-of-experts (MoE) models across multiple computing devices, facilitating the execution of these increasingly large-scale models. However, the All-to-All communication intrinsic to expert parallelism constitutes a significant overhead, diminishing the MoE models' efficiency. Current optimization approaches offer some relief, yet they are constrained by the sequential interdependence of communication and computation operations. To address this limitation, we present a novel shortcut-connected MoE architecture with overlapping parallel strategy, designated as ScMoE, which effectively decouples communication from its conventional sequence, allowing for a substantial overlap of 70% to 100% with computation. When compared with the prevalent top-2 MoE architecture, ScMoE demonstrates training speed improvements of 30% and 11%, and inference improvements of 40% and 15%, in our PCIe and NVLink hardware environments, respectively, where communication constitutes 60% and 15% of the total MoE time consumption. On the other hand, extensive experiments and theoretical analyses indicate that ScMoE not only achieves comparable but in some instances surpasses the model quality of existing approaches in vision and language tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05019v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weilin Cai, Juyong Jiang, Le Qin, Junwei Cui, Sunghun Kim, Jiayi Huang</dc:creator>
    </item>
    <item>
      <title>Fork is All You Needed in Heterogeneous Systems</title>
      <link>https://arxiv.org/abs/2404.05085</link>
      <description>arXiv:2404.05085v1 Announce Type: cross 
Abstract: We present a unified programming model for heterogeneous computing systems. Such systems integrate multiple computing accelerators and memory units to deliver higher performance than CPU-centric systems. Although heterogeneous systems have been adopted by modern workloads such as machine learning, programming remains a critical limiting factor. Conventional heterogeneous programming techniques either impose heavy modifications to the code base or require rewriting the program in a different language. Such programming complexity stems from the lack of a unified abstraction layer for computing and data exchange, which forces each programming model to define its abstractions. However, with the emerging cache-coherent interconnections such as Compute Express Link, we see an opportunity to standardize such architecture heterogeneity and provide a unified programming model. We present CodeFlow, a language runtime system for heterogeneous computing. CodeFlow abstracts architecture computation in programming language runtime and utilizes CXL as a unified data exchange protocol. Workloads written in high-level languages such as C++ and Rust can be compiled to CodeFlow, which schedules different parts of the workload to suitable accelerators without requiring the developer to implement code or call APIs for specific accelerators. CodeFlow reduces programmers' effort in utilizing heterogeneous systems and improves workload performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05085v1</guid>
      <category>cs.ET</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zixuan Wang, Jishen Zhao</dc:creator>
    </item>
    <item>
      <title>Optimal Allocation of Tasks and Price of Anarchy of Distributed Optimization in Networked Computing Facilities</title>
      <link>https://arxiv.org/abs/2404.05543</link>
      <description>arXiv:2404.05543v1 Announce Type: cross 
Abstract: The allocation of computing tasks for networked distributed services poses a question to service providers on whether centralized allocation management be worth its cost. Existing analytical models were conceived for users accessing computing resources with practically indistinguishable (hence irrelevant for the allocation decision) delays, which is typical of services located in the same distant data center. However, with the rise of the edge-cloud continuum, a simple analysis of the sojourn time that computing tasks observe at the server misses the impact of diverse latency values imposed by server locations. We therefore study the optimization of computing task allocation with a new model that considers both distance of servers and sojourn time in servers. We derive exact algorithms to optimize the system and we show, through numerical analysis and real experiments, that differences in server location in the edge-cloud continuum cannot be neglected. By means of algorithmic game theory, we study the price of anarchy of a distributed implementation of the computing task allocation problem and unveil important practical properties such as the fact that the price of anarchy tends to be small -- except when the system is overloaded -- and its maximum can be computed with low complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05543v1</guid>
      <category>cs.GT</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Vincenzo Mancuso, Paolo Castagno, Leonardo Badia, Matteo Sereno, Marco Ajmone Marsan</dc:creator>
    </item>
    <item>
      <title>Hook-in Privacy Techniques for gRPC-based Microservice Communication</title>
      <link>https://arxiv.org/abs/2404.05598</link>
      <description>arXiv:2404.05598v1 Announce Type: cross 
Abstract: gRPC is at the heart of modern distributed system architectures. Based on HTTP/2 and Protocol Buffers, it provides highly performant, standardized, and polyglot communication across loosely coupled microservices and is increasingly preferred over REST- or GraphQL-based service APIs in practice. Despite its widespread adoption, gRPC lacks any advanced privacy techniques beyond transport encryption and basic token-based authentication. Such advanced techniques are, however, increasingly important for fulfilling regulatory requirements. For instance, anonymizing or otherwise minimizing (personal) data before responding to requests, or pre-processing data based on the purpose of the access may be crucial in certain usecases. In this paper, we therefore propose a novel approach for integrating such advanced privacy techniques into the gRPC framework in a practically viable way. Specifically, we present a general approach along with a working prototype that implements privacy techniques, such as data minimization and purpose limitation, in a configurable, extensible, and gRPC-native way utilizing a gRPC interceptor. We also showcase how to integrate this contribution into a realistic example of a food delivery use case. Alongside these implementations, a preliminary performance evaluation shows practical applicability with reasonable overheads. Altogether, we present a viable solution for integrating advanced privacy techniques into real-world gRPC-based microservice architectures, thereby facilitating regulatory compliance ``by design''.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05598v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Louis Loechel, Siar-Remzi Akbayin, Elias Gr\"unewald, Jannis Kiesel, Inga Strelnikova, Thomas Janke, Frank Pallas</dc:creator>
    </item>
    <item>
      <title>Accelerating the Delivery of Data Services over Uncertain Mobile Crowdsensing Networks</title>
      <link>https://arxiv.org/abs/2210.04410</link>
      <description>arXiv:2210.04410v3 Announce Type: replace 
Abstract: The challenge of exchanging and processing of big data over mobile crowdsensing (MCS) networks calls for designing seamless data service provisioning mechanisms to enable utilization of resources of mobile devices/users for crowdsensing tasks. Although conventional onsite spot trading of resources based on real-time network conditions can facilitate data sharing, it often suffers from prohibitively long service provisioning delays and unavoidable trading failures due to requiring timely analysis of dynamic network environment. These limitations motivate us to investigate an integrated forward and spot trading mechanism (iFAST), which entails a novel hybrid data trading protocol with time efficiency, over uncertain MCS ecosystems. In iFAST, the sellers (i.e., mobile devices who can contribute data) can provide long-term or temporary sensing services to the buyers (i.e., sensing tasks). Specifically, it enables signing long-term contracts in advance of future transactions through a forward trading mode, via analyzing historical statistics of the network/market, for which the notion of overbooking is introduced and promoted. iFAST further encourages the buyers with unsatisfying service quality to recruit temporary sellers through a spot trading mode, considering the current network/market conditions. We analyze the fundamental blocks of iFAST and provide a case study to demonstrate its performance. Inspirations for future research directions of next-generation sensing and communication are summarized.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.04410v3</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minghui Liwang, Zhipeng Cheng, Wei Gong, Li Li, Yuhan Su, Zhenzhen Jiao, Seyyedali Hosseinalipour, Xianbin Wang, Huaiyu Dai</dc:creator>
    </item>
    <item>
      <title>Stitching Satellites to the Edge: Pervasive and Efficient Federated LEO Satellite Learning</title>
      <link>https://arxiv.org/abs/2401.15541</link>
      <description>arXiv:2401.15541v2 Announce Type: replace 
Abstract: In the ambitious realm of space AI, the integration of federated learning (FL) with low Earth orbit (LEO) satellite constellations holds immense promise. However, many challenges persist in terms of feasibility, learning efficiency, and convergence. These hurdles stem from the bottleneck in communication, characterized by sporadic and irregular connectivity between LEO satellites and ground stations, coupled with the limited computation capability of satellite edge computing (SEC). This paper proposes a novel FL-SEC framework that empowers LEO satellites to execute large-scale machine learning (ML) tasks onboard efficiently. Its key components include i) personalized learning via divide-and-conquer, which identifies and eliminates redundant satellite images and converts complex multi-class classification problems to simple binary classification, enabling rapid and energy-efficient training of lightweight ML models suitable for IoT/edge devices on satellites; ii) orbital model retraining, which generates an aggregated "orbital model" per orbit and retrains it before sending to the ground station, significantly reducing the required communication rounds. We conducted experiments using Jetson Nano, an edge device closely mimicking the limited compute on LEO satellites, and a real satellite dataset. The results underscore the effectiveness of our approach, highlighting SEC's ability to run lightweight ML models on real and high-resolution satellite imagery. Our approach dramatically reduces FL convergence time by nearly 30 times, and satellite energy consumption down to as low as 1.38 watts, all while maintaining an exceptional accuracy of up to 96%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15541v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Mohamed Elmahallawy, Tie Luo</dc:creator>
    </item>
    <item>
      <title>Design and Implementation of an Analysis Pipeline for Heterogeneous Data</title>
      <link>https://arxiv.org/abs/2403.15721</link>
      <description>arXiv:2403.15721v3 Announce Type: replace 
Abstract: Managing and preparing complex data for deep learning, a prevalent approach in large-scale data science can be challenging. Data transfer for model training also presents difficulties, impacting scientific fields like genomics, climate modeling, and astronomy. A large-scale solution like Google Pathways with a distributed execution environment for deep learning models exists but is proprietary. Integrating existing open-source, scalable runtime tools and data frameworks on high-performance computing (HPC) platforms is crucial to address these challenges. Our objective is to establish a smooth and unified method of combining data engineering and deep learning frameworks with diverse execution capabilities that can be deployed on various high-performance computing platforms, including cloud and supercomputers. We aim to support heterogeneous systems with accelerators, where Cylon and other data engineering and deep learning frameworks can utilize heterogeneous execution. To achieve this, we propose Radical-Cylon, a heterogeneous runtime system with a parallel and distributed data framework to execute Cylon as a task of Radical Pilot. We thoroughly explain Radical-Cylon's design and development and the execution process of Cylon tasks using Radical Pilot. This approach enables the use of heterogeneous MPI-communicators across multiple nodes. Radical-Cylon achieves better performance than Bare-Metal Cylon with minimal and constant overhead. Radical-Cylon achieves (4~15)% faster execution time than batch execution while performing similar join and sort operations with 35 million and 3.5 billion rows with the same resources. The approach aims to excel in both scientific and engineering research HPC systems while demonstrating robust performance on cloud infrastructures. This dual capability fosters collaboration and innovation within the open-source scientific research community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15721v3</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arup Kumar Sarker, Aymen Alsaadi, Niranda Perera, Mills Staylor, Gregor von Laszewski, Matteo Turilli, Ozgur Ozan Kilic, Mikhail Titov, Andre Merzky, Shantenu Jha, Geoffrey Fox</dc:creator>
    </item>
    <item>
      <title>Optimizing Communication for Latency Sensitive HPC Applications on up to 48 FPGAs Using ACCL</title>
      <link>https://arxiv.org/abs/2403.18374</link>
      <description>arXiv:2403.18374v2 Announce Type: replace 
Abstract: Most FPGA boards in the HPC domain are well-suited for parallel scaling because of the direct integration of versatile and high-throughput network ports. However, the utilization of their network capabilities is often challenging and error-prone because the whole network stack and communication patterns have to be implemented and managed on the FPGAs. Also, this approach conceptually involves a trade-off between the performance potential of improved communication and the impact of resource consumption for communication infrastructure, since the utilized resources on the FPGAs could otherwise be used for computations. In this work, we investigate this trade-off, firstly, by using synthetic benchmarks to evaluate the different configuration options of the communication framework ACCL and their impact on communication latency and throughput. Finally, we use our findings to implement a shallow water simulation whose scalability heavily depends on low-latency communication. With a suitable configuration of ACCL, good scaling behavior can be shown to all 48 FPGAs installed in the system. Overall, the results show that the availability of inter-FPGA communication frameworks as well as the configurability of framework and network stack are crucial to achieve the best application performance with low latency communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18374v2</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marius Meyer, Tobias Kenter, Lucian Petrica, Kenneth O'Brien, Michaela Blott, Christian Plessl</dc:creator>
    </item>
    <item>
      <title>GeoT: Tensor Centric Library for Graph Neural Network via Efficient Segment Reduction on GPU</title>
      <link>https://arxiv.org/abs/2404.03019</link>
      <description>arXiv:2404.03019v2 Announce Type: replace 
Abstract: In recent years, Graph Neural Networks (GNNs) have ignited a surge of innovation, significantly enhancing the processing of geometric data structures such as graphs, point clouds, and meshes. As the domain continues to evolve, a series of frameworks and libraries are being developed to push GNN efficiency to new heights. While graph-centric libraries have achieved success in the past, the advent of efficient tensor compilers has highlighted the urgent need for tensor-centric libraries. Yet, efficient tensor-centric frameworks for GNNs remain scarce due to unique challenges and limitations encountered when implementing segment reduction in GNN contexts.
  We introduce GeoT, a cutting-edge tensor-centric library designed specifically for GNNs via efficient segment reduction. GeoT debuts innovative parallel algorithms that not only introduce new design principles but also expand the available design space. Importantly, GeoT is engineered for straightforward fusion within a computation graph, ensuring compatibility with contemporary tensor-centric machine learning frameworks and compilers. Setting a new performance benchmark, GeoT marks a considerable advancement by showcasing an average operator speedup of 1.80x and an end-to-end speedup of 1.68x.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03019v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongming Yu, Genghan Zhang, Hanxian Huang, Xin Chen, Jishen Zhao</dc:creator>
    </item>
    <item>
      <title>Bodyless Block Propagation: TPS Fully Scalable Blockchain with Pre-Validation</title>
      <link>https://arxiv.org/abs/2204.08769</link>
      <description>arXiv:2204.08769v3 Announce Type: replace-cross 
Abstract: Despite numerous prior attempts to boost transaction per second (TPS) of blockchain systems, many sacrifice decentralization and security. This paper proposes a bodyless block propagation (BBP) scheme for which the blockbody is not validated and transmitted during block propagation, to increase TPS without compromising security. Nodes in the blockchain network anticipate the transactions and their ordering in the next upcoming block so that these transactions can be pre-executed and pre-validated before the block is born. For a network with $N$ nodes, our theoretical analysis reveals that BBP can improve TPS scalability from $O(1/log(N))$ to $O(1)$.
  Ensuring consensus on the next block's transaction content is crucial. We propose a transaction selection, ordering, and synchronization algorithm to drive this consensus. To address the undetermined Coinbase address issue, we further present an algorithm for such unresolvable transactions, ensuring a consistent and TPS-efficient scheme. With BBP, most transactions require neither validation nor transmission during block propagation, liberating system from transaction-block dependencies and rendering TPS scalable. Both theoretical analysis and experiments underscore BBP's potential for full TPS scalability. Experimental results reveal a 4x reduction in block propagation time compared to Ethereum blockchain, with TPS performance being limited by node hardware rather than block propagation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.08769v3</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chonghe Zhao, Shengli Zhang, Taotao Wang, Soung Chang Liew</dc:creator>
    </item>
    <item>
      <title>Partial Selfish Mining for More Profits</title>
      <link>https://arxiv.org/abs/2207.13478</link>
      <description>arXiv:2207.13478v2 Announce Type: replace-cross 
Abstract: Mining attacks aim to gain an unfair share of extra rewards in the blockchain mining. Selfish mining can preserve discovered blocks and strategically release them, wasting honest miners' computing resources and getting higher profits. Previous mining attacks either conceal the mined whole blocks (hiding or discarding), or release them completely in a particular time slot (e.g., causing a fork). In this paper, we extend the mining attack's strategy space to partial block sharing, and propose a new and feasible Partial Selfish Mining (PSM) attack. We show that by releasing partial block data publicly and attracting rational miners to work on attacker's private branch, attackers and these attracted miners can gain an unfair share of mining rewards. We then propose Advanced PSM (A-PSM) attack that can further improve attackers' profits to be no less than the selfish mining. Both theoretical and experimental results show that PSM attackers can be more profitable than selfish miners under a certain range of mining power and network conditions. A-PSM attackers can gain even higher profits than both selfish mining and honest mining with attracted rational miners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.13478v2</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaping Yu, Shang Gao, Rui Song, Zhiping Cai, Bin Xiao</dc:creator>
    </item>
    <item>
      <title>Fedstellar: A Platform for Decentralized Federated Learning</title>
      <link>https://arxiv.org/abs/2306.09750</link>
      <description>arXiv:2306.09750v4 Announce Type: replace-cross 
Abstract: In 2016, Google proposed Federated Learning (FL) as a novel paradigm to train Machine Learning (ML) models across the participants of a federation while preserving data privacy. Since its birth, Centralized FL (CFL) has been the most used approach, where a central entity aggregates participants' models to create a global one. However, CFL presents limitations such as communication bottlenecks, single point of failure, and reliance on a central server. Decentralized Federated Learning (DFL) addresses these issues by enabling decentralized model aggregation and minimizing dependency on a central entity. Despite these advances, current platforms training DFL models struggle with key issues such as managing heterogeneous federation network topologies. To overcome these challenges, this paper presents Fedstellar, a platform extended from p2pfl library and designed to train FL models in a decentralized, semi-decentralized, and centralized fashion across diverse federations of physical or virtualized devices. The Fedstellar implementation encompasses a web application with an interactive graphical interface, a controller for deploying federations of nodes using physical or virtual devices, and a core deployed on each device which provides the logic needed to train, aggregate, and communicate in the network. The effectiveness of the platform has been demonstrated in two scenarios: a physical deployment involving single-board devices such as Raspberry Pis for detecting cyberattacks, and a virtualized deployment comparing various FL approaches in a controlled environment using MNIST and CIFAR-10 datasets. In both scenarios, Fedstellar demonstrated consistent performance and adaptability, achieving F1 scores of 91%, 98%, and 91.2% using DFL for detecting cyberattacks and classifying MNIST and CIFAR-10, respectively, reducing training time by 32% compared to centralized approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.09750v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.eswa.2023.122861</arxiv:DOI>
      <dc:creator>Enrique Tom\'as Mart\'inez Beltr\'an, \'Angel Luis Perales G\'omez, Chao Feng, Pedro Miguel S\'anchez S\'anchez, Sergio L\'opez Bernal, G\'er\^ome Bovet, Manuel Gil P\'erez, Gregorio Mart\'inez P\'erez, Alberto Huertas Celdr\'an</dc:creator>
    </item>
    <item>
      <title>MimiC: Combating Client Dropouts in Federated Learning by Mimicking Central Updates</title>
      <link>https://arxiv.org/abs/2306.12212</link>
      <description>arXiv:2306.12212v4 Announce Type: replace-cross 
Abstract: Federated learning (FL) is a promising framework for privacy-preserving collaborative learning, where model training tasks are distributed to clients and only the model updates need to be collected at a server. However, when being deployed at mobile edge networks, clients may have unpredictable availability and drop out of the training process, which hinders the convergence of FL. This paper tackles such a critical challenge. Specifically, we first investigate the convergence of the classical FedAvg algorithm with arbitrary client dropouts. We find that with the common choice of a decaying learning rate, FedAvg oscillates around a stationary point of the global loss function, which is caused by the divergence between the aggregated and desired central update. Motivated by this new observation, we then design a novel training algorithm named MimiC, where the server modifies each received model update based on the previous ones. The proposed modification of the received model updates mimics the imaginary central update irrespective of dropout clients. The theoretical analysis of MimiC shows that divergence between the aggregated and central update diminishes with proper learning rates, leading to its convergence. Simulation results further demonstrate that MimiC maintains stable convergence performance and learns better models than the baseline methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.12212v4</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TMC.2023.3338021</arxiv:DOI>
      <dc:creator>Yuchang Sun, Yuyi Mao, Jun Zhang</dc:creator>
    </item>
    <item>
      <title>Synthetic data shuffling accelerates the convergence of federated learning under data heterogeneity</title>
      <link>https://arxiv.org/abs/2306.13263</link>
      <description>arXiv:2306.13263v2 Announce Type: replace-cross 
Abstract: In federated learning, data heterogeneity is a critical challenge. A straightforward solution is to shuffle the clients' data to homogenize the distribution. However, this may violate data access rights, and how and when shuffling can accelerate the convergence of a federated optimization algorithm is not theoretically well understood. In this paper, we establish a precise and quantifiable correspondence between data heterogeneity and parameters in the convergence rate when a fraction of data is shuffled across clients. We prove that shuffling can quadratically reduce the gradient dissimilarity with respect to the shuffling percentage, accelerating convergence. Inspired by the theory, we propose a practical approach that addresses the data access rights issue by shuffling locally generated synthetic data. The experimental results show that shuffling synthetic data improves the performance of multiple existing federated learning algorithms by a large margin.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.13263v2</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bo Li, Yasin Esfandiari, Mikkel N. Schmidt, Tommy S. Alstr{\o}m, Sebastian U. Stich</dc:creator>
    </item>
    <item>
      <title>Cross-Silo Federated Learning Across Divergent Domains with Iterative Parameter Alignment</title>
      <link>https://arxiv.org/abs/2311.04818</link>
      <description>arXiv:2311.04818v3 Announce Type: replace-cross 
Abstract: Learning from the collective knowledge of data dispersed across private sources can provide neural networks with enhanced generalization capabilities. Federated learning, a method for collaboratively training a machine learning model across remote clients, achieves this by combining client models via the orchestration of a central server. However, current approaches face two critical limitations: i) they struggle to converge when client domains are sufficiently different, and ii) current aggregation techniques produce an identical global model for each client. In this work, we address these issues by reformulating the typical federated learning setup: rather than learning a single global model, we learn N models each optimized for a common objective. To achieve this, we apply a weighted distance minimization to model parameters shared in a peer-to-peer topology. The resulting framework, Iterative Parameter Alignment, applies naturally to the cross-silo setting, and has the following properties: (i) a unique solution for each participant, with the option to globally converge each model in the federation, and (ii) an optional early-stopping mechanism to elicit fairness among peers in collaborative learning settings. These characteristics jointly provide a flexible new framework for iteratively learning from peer models trained on disparate datasets. We find that the technique achieves competitive results on a variety of data partitions compared to state-of-the-art approaches. Further, we show that the method is robust to divergent domains (i.e. disjoint classes across peers) where existing approaches struggle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04818v3</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matt Gorbett, Hossein Shirazi, Indrakshi Ray</dc:creator>
    </item>
    <item>
      <title>Have Your Cake and Eat It Too: Toward Efficient and Accurate Split Federated Learning</title>
      <link>https://arxiv.org/abs/2311.13163</link>
      <description>arXiv:2311.13163v2 Announce Type: replace-cross 
Abstract: Due to its advantages in resource constraint scenarios, Split Federated Learning (SFL) is promising in AIoT systems. However, due to data heterogeneity and stragglers, SFL suffers from the challenges of low inference accuracy and low efficiency. To address these issues, this paper presents a novel SFL approach, named Sliding Split Federated Learning (S$^2$FL), which adopts an adaptive sliding model split strategy and a data balance-based training mechanism. By dynamically dispatching different model portions to AIoT devices according to their computing capability, S$^2$FL can alleviate the low training efficiency caused by stragglers. By combining features uploaded by devices with different data distributions to generate multiple larger batches with a uniform distribution for back-propagation, S$^2$FL can alleviate the performance degradation caused by data heterogeneity. Experimental results demonstrate that, compared to conventional SFL, S$^2$FL can achieve up to 16.5\% inference accuracy improvement and 3.54X training acceleration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13163v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dengke Yan, Ming Hu, Zeke Xia, Yanxin Yang, Jun Xia, Xiaofei Xie, Mingsong Chen</dc:creator>
    </item>
    <item>
      <title>Federated reinforcement learning for robot motion planning with zero-shot generalization</title>
      <link>https://arxiv.org/abs/2403.13245</link>
      <description>arXiv:2403.13245v2 Announce Type: replace-cross 
Abstract: This paper considers the problem of learning a control policy for robot motion planning with zero-shot generalization, i.e., no data collection and policy adaptation is needed when the learned policy is deployed in new environments. We develop a federated reinforcement learning framework that enables collaborative learning of multiple learners and a central server, i.e., the Cloud, without sharing their raw data. In each iteration, each learner uploads its local control policy and the corresponding estimated normalized arrival time to the Cloud, which then computes the global optimum among the learners and broadcasts the optimal policy to the learners. Each learner then selects between its local control policy and that from the Cloud for next iteration. The proposed framework leverages on the derived zero-shot generalization guarantees on arrival time and safety. Theoretical guarantees on almost-sure convergence, almost consensus, Pareto improvement and optimality gap are also provided. Monte Carlo simulation is conducted to evaluate the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13245v2</guid>
      <category>eess.SY</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenyuan Yuan, Siyuan Xu, Minghui Zhu</dc:creator>
    </item>
  </channel>
</rss>
