<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 16 Sep 2025 02:43:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Computational Study of New Record-Based Transmuted Chen Distribution and Its Applications to the Failure Time and Iron Sheet Data</title>
      <link>https://arxiv.org/abs/2509.09756</link>
      <description>arXiv:2509.09756v1 Announce Type: new 
Abstract: This study is considered to introduce a novel distribution as an alternative to Chen distribution via the record-based transmutation method. This technique is based on the distributions of first two upper record values. Thus, we suggest a new special case based on Chen distribution in the family of record based transmuted distributions. We explore various distributional properties of the proposed model namely, quantile function, hazard function, median, moments, and stochastic ordering. Our distribution has three parameters and to estimate these parameters, we utilize nine different and well-known estimators. Then, we compare the performances of these estimators via a comprehensive simulation study. Also, we provide two real-world data examples to assess the fits the data sets the suggested model and its some competitors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09756v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Caner Tan{\i}\c{s}</dc:creator>
    </item>
    <item>
      <title>Record-based transmuted log-logistic distribution: Properties, simulation, and applications to petroleum rock and reactor pump data</title>
      <link>https://arxiv.org/abs/2509.09757</link>
      <description>arXiv:2509.09757v1 Announce Type: new 
Abstract: This study aims to introduce a new lifetime distribution, called the record-based transformed log-logistic distribution, to the literature. We obtain this distribution using a record-based transformation map based on the distributions of upper record values. We explore some mathematical properties of the suggested distribution, namely the quantile function, hazard function, moments, order statistics, and stochastic ordering. We discuss the point estimation via seven different methods such as maximum likelihood, least squares, weighted least squares, Anderson-Darling, Cramer-von Mises, maximum product spacings, and right tail Anderson Darling. Then, we perform a Monte Carlo simulation study to evaluate the performances of these estimators. Also, we present two practical data examples, reactor pump failure and petroleum rock data to compare the fits of the proposed distribution with its rivals. As a result of data analysis, we conclude that the best-fitted distribution is the record-based transmuted log-logistic distribution for reactor pump failure and petroleum rock data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09757v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Caner Tan{\i}\c{s}</dc:creator>
    </item>
    <item>
      <title>Optimal Inference of the Mean Outcome under Optimal Treatment Regime</title>
      <link>https://arxiv.org/abs/2509.09773</link>
      <description>arXiv:2509.09773v1 Announce Type: new 
Abstract: When an optimal treatment regime (OTR) is considered, we need to evaluate the OTR in a valid and efficient way. The classical inference applied to the mean outcome under OTR, assuming the OTR is the same as the estimated OTR, might be biased when the regularity assumption that OTR is unique is violated. Although several methods have been proposed to allow nonregularity in such inference, its optimality is unclear due to challenges in deriving semiparametric efficiency bounds under potential nonregularity. In this paper, we address the bias issue via adaptive smoothing over the estimated OTR and develop a valid inference procedure on the mean outcome under OTR regardless of whether regularity is satisfied. We establish the optimality of the proposed method by deriving a lower bound of the asymptotic variance for the robust asymptotically linear unbiased estimator to the mean outcome under OTR and showing that our proposed estimator achieves the variance lower bound. The considered estimator class is general and the derived variance lower bound paves a novel way to establish efficiency optimality theories for OTR in a more general scenario allowing nonregularity. The merit of the proposed method is demonstrated by re-analyzing the ACTG 175 trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09773v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shuoxun Xu (Department of Mathematics, The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong, China, Division of Biostatistics, School of Public Health, University of California, Berkeley, Berkeley, CA, USA), Xinzhou Guo (Department of Mathematics, The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong, China)</dc:creator>
    </item>
    <item>
      <title>Meta-Analysis with JASP, Part I: Classical Approaches</title>
      <link>https://arxiv.org/abs/2509.09845</link>
      <description>arXiv:2509.09845v1 Announce Type: new 
Abstract: Meta-analyses play a crucial part in empirical science, enabling researchers to synthesize evidence across studies and draw more precise and generalizable conclusions. Despite their importance, access to advanced meta-analytic methodology is often limited to scientists and students with considerable expertise in computer programming. To lower the barrier for adoption, we have developed the Meta-Analysis module in JASP (https://jasp-stats.org/), a free and open-source software for statistical analyses. The module offers standard and advanced meta-analytic techniques through an easy-to-use graphical user interface (GUI), allowing researchers with diverse technical backgrounds to conduct state-of-the-art analyses. This manuscript presents an overview of the meta-analytic tools implemented in the module and showcases how JASP supports a meta-analytic practice that is rigorous, relevant, and reproducible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09845v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Franti\v{s}ek Barto\v{s}, Eric-Jan Wagenmakers, Wolfgang Viechtbauer</dc:creator>
    </item>
    <item>
      <title>Meta-Analysis with JASP, Part II: Bayesian Approaches</title>
      <link>https://arxiv.org/abs/2509.09850</link>
      <description>arXiv:2509.09850v1 Announce Type: new 
Abstract: Bayesian inference is on the rise, partly because it allows researchers to quantify parameter uncertainty, evaluate evidence for competing hypotheses, incorporate model ambiguity, and seamlessly update knowledge as information accumulates. All of these advantages apply to the meta-analytic settings; however, advanced Bayesian meta-analytic methodology is often restricted to researchers with programming experience. In order to make these tools available to a wider audience, we implemented state-of-the-art Bayesian meta-analysis methods in the Meta-Analysis module of JASP, a free and open-source statistical software package (https://jasp-stats.org/). The module allows researchers to conduct Bayesian estimation, hypothesis testing, and model averaging with models such as meta-regression, multilevel meta-analysis, and publication bias adjusted meta-analysis. Results can be interpreted using forest plots, bubble plots, and estimated marginal means. This manuscript provides an overview of the Bayesian meta-analysis tools available in JASP and demonstrates how the software enables researchers of all technical backgrounds to perform advanced Bayesian meta-analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09850v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Franti\v{s}ek Barto\v{s}, Eric-Jan Wagenmakers</dc:creator>
    </item>
    <item>
      <title>Confidence Intervals for Extinction Risk: Validating Population Viability Analysis with Limited Data</title>
      <link>https://arxiv.org/abs/2509.09965</link>
      <description>arXiv:2509.09965v1 Announce Type: new 
Abstract: Quantitative assessment of extinction risk requires not only point estimates but also confidence intervals (CIs) that remain informative with limited data. Their reliability has been debated, as short observation spans can inflate uncertainty and reduce usefulness. I derive new CIs for extinction probability $G$ under the Wiener process with drift, a canonical model of population viability analysis. The method uses correlated noncentral-$t$ distributions for the transformed statistics $\widehat{w}$ and $\widehat{z}$, derived from drift and variance estimators, and constructs CIs of the extinction probability by exploiting the geometric properties of $G(\widehat{w},\widehat{z})$ in parameter space. Monte Carlo experiments show that the proposed intervals attain nominal coverage with narrower widths than common approximate methods, including the delta method, moment-based approaches, and bootstrap. A key result is that even with short time series, extinction probabilities that are very small or very large can be estimated reliably. This resolves a long-standing concern that population viability analysis fails under data scarcity. Applied to three 64-year catch series for Japanese eel (Anguilla japonica), the analysis indicates extinction risk well below the IUCN Criterion E thresholds for Critically Endangered and Endangered, with narrow CIs. These findings demonstrate that extinction-risk CIs can be both statistically rigorous and practical for Red List evaluations, even when data are limited.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09965v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiroshi Hakoyama</dc:creator>
    </item>
    <item>
      <title>A Bayesian Framework for Regularized Estimation in Multivariate Models Integrating Approximate Computing Concepts</title>
      <link>https://arxiv.org/abs/2509.10045</link>
      <description>arXiv:2509.10045v1 Announce Type: new 
Abstract: This paper discusses regularized estimators in the multivariate statistical model as tools naturally arising within a Bayesian framework. First, a link is established between Bayesian estimation and inference under parameter rounding (quantization), thereby connecting two distinct paradigms: Bayesian inference and approximate computing. Next, Bayesian estimation of the means from two independent multivariate normal samples is employed to justify shrinkage estimators, i.e., means shrunk toward the pooled mean. Finally, regularized linear discriminant analysis (LDA) is considered. Various shrinkage strategies for the mean are justified from a Bayesian perspective, and novel algorithms for their computation are proposed. The proposed methods are illustrated by numerical experiments on real and simulated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10045v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Kalina</dc:creator>
    </item>
    <item>
      <title>Weakening assumptions in the evaluation of treatment effects in longitudinal randomized trials with truncation by death or other intercurrent events</title>
      <link>https://arxiv.org/abs/2509.10067</link>
      <description>arXiv:2509.10067v1 Announce Type: new 
Abstract: Intercurrent events, such as treatment switching, rescue medication, or truncation by death, can complicate the interpretation of intention-to-treat (ITT) analyses in randomized clinical trials. Recent advances in causal inference address these challenges by targeting alternative estimands, such as hypothetical estimands or principal stratum estimands (e.g., survivor average causal effects). However, such approaches often require strong, unverifiable assumptions, partly due to limited data on time-varying confounders and the difficulty of adjusting for them. Additionally, strict trial protocols frequently lead to (near) violations of the positivity assumption, resulting in limited information for identifying these estimands.
  In this paper, we propose a novel approach that sidesteps these difficulties by focusing on testing the null hypothesis of no treatment effect in the presence of arbitrary intercurrent events, including truncation by death, using longitudinal trial data. Our key idea is to compare treated and untreated individuals, matched on baseline covariates, at the most recent time point before either experiences an intercurrent event. We refer to such contrasts as Pairwise Last Observation Time (PLOT) estimands. These estimands can be identified in randomized clinical trials without requiring additional structural assumptions, and even in the presence of the aforementioned positivity violations. However, they may still be susceptible to a form of residual selection bias. We show that this bias vanishes under the conditions typically required by alternative methods, and find it to be more generally small in extensive simulation studies. Building on this, we develop asymptotically efficient, model-free tests using data-adaptive estimation of nuisance parameters. We evaluate the method's performance via simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10067v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georgi Baklicharov, Kelly Van Lancker, Stijn Vansteelandt</dc:creator>
    </item>
    <item>
      <title>A sampling method based on highest density regions: Applications to surrogate models for rare events estimation</title>
      <link>https://arxiv.org/abs/2509.10149</link>
      <description>arXiv:2509.10149v1 Announce Type: new 
Abstract: This paper introduces a practical sampling method for training surrogate models in the context of uncertainty propagation. We propose a heuristic method to uniformly draw samples within highest density regions of the density given by the random vector describing the uncertainty of the model parameters. The resulting experimental design aims to provide a better approximation of the underlying true model compared to the cases where experimental designs have been drawn according to the distribution of the random vector itself. To assess the quality of our approach, three error metrics are considered: The first is the leave-one-out error, the second the relative mean square error and the third is the error generated by the surrogate model when estimating the probability of failure of the system compared to its reference value. The highest density region-based designs are shown to globally outperform the random vector-based designs both in terms of relative mean square error as well as in estimating the probability of failure. The proposed method is applicable within a black-box context and is compatible with existing uncertainty quantification frameworks for low dimensional and moderately correlated inputs. It may thus be useful in case of reliability problems, Bayesian inverse analysis, or whenever the surrogate model is used in a predictor mode.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10149v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jocelyn Minini, Micha Wasem</dc:creator>
    </item>
    <item>
      <title>Simultaneous testing of hypotheses and alternatives</title>
      <link>https://arxiv.org/abs/2509.10197</link>
      <description>arXiv:2509.10197v1 Announce Type: new 
Abstract: To identify statistically significant conclusions, it is proposed to simultaneously test hypotheses and alternatives. It is shown that, under the condition of free combination of hypotheses and alternatives, the closure method leads to single-step procedures for the simultaneous testing of hypotheses and alternatives. Using an example it is shown the result is lost when hypotheses and alternatives do not satisfy the condition of free combination of hypotheses and alternatives. It is shown that the average number of insignificant conclusions is an uncontrolled part of the risk function of a single-step procedure with an appropriate choice of the loss function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10197v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>P. A. Koldanov, A. P. Koldanov</dc:creator>
    </item>
    <item>
      <title>Using joint models in phase I dose-finding designs in oncology: considerations for frequentist approaches</title>
      <link>https://arxiv.org/abs/2509.10238</link>
      <description>arXiv:2509.10238v1 Announce Type: new 
Abstract: Dose-finding trials for oncology studies are traditionally designed to assess safety in the early stages of drug development. With the rise of molecularly targeted therapies and immuno-oncology compounds, biomarker-driven approaches have gained significant importance. In this paper, we propose a novel approach that incorporates multiple values of a predictive biomarker to assist in evaluating binary toxicity outcomes using the factorization of a joint model in phase I dose-finding oncology trials. The proposed joint model framework, which utilizes additional repeated biomarker values as an early predictive marker for potential toxicity, is compared to the likelihood-based continual reassessment method (CRM) using only binary toxicity data, across various dose-toxicity relationship scenarios. Our findings highlight a critical limitation of likelihood-based approaches in early-phase dose-finding studies with small sample sizes: estimation challenges that have been previously overlooked in the phase I dose-escalation setting. We explore potential remedies to address these challenges and emphasize the appropriate use of likelihood-based methods. Simulation results demonstrate that the proposed joint model framework, by integrating biomarker information, can alleviate estimation problems in the the likelihood-based continual reassessment method (CRM) and improve the proportion of correct selection. However, we highlight that the inherent data limitations in early-phase dose-finding studies remain a significant challenge that cannot fully be overcomed in the frequentist framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10238v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xijin Chen, Pavel Mozgunov, Richard D. Baird, Thomas Jaki</dc:creator>
    </item>
    <item>
      <title>Conformal prediction without knowledge of labeled calibration data</title>
      <link>https://arxiv.org/abs/2509.10321</link>
      <description>arXiv:2509.10321v1 Announce Type: new 
Abstract: We extend the method of conformal prediction beyond the case relying on labeled calibration data. Replacing the calibration scores by suitable estimates, we identify conformity sets $C$ for classification and regression models that rely on unlabeled calibration data. Given a classification model with accuracy $1-\beta$, we prove that the conformity sets guarantee a coverage of $P(Y \in C) \geq 1-\alpha-\beta$ for an arbitrary parameter $\alpha \in (0,1)$. The same coverage guarantee also holds for regression models, if we replace the accuracy by a similar exactness measure. Finally, we describe how to use the theoretical results in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10321v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Flechsig, Maximilian Pilz</dc:creator>
    </item>
    <item>
      <title>Using the rejection sampling for finding tests</title>
      <link>https://arxiv.org/abs/2509.10325</link>
      <description>arXiv:2509.10325v1 Announce Type: new 
Abstract: A new method based on the rejection sampling for finding statistical tests is proposed. This method is conceptually intuitive, easy to implement, and applicable for arbitrary dimension. To illustrate its potential applicability, three distinct empirical examples are presented: (1) examine the differences between group means of correlated (repeated) or independent samples, (2) examine if a mean vector equals to a specific fixed vector, and (3) investigate if samples come from a specific population distribution. The simulation examples indicate that the new test has similar statistical power as uniformly the most powerful (unbiased) tests. Moreover, these examples demonstrate that the new test is a powerful goodness-of-fit test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10325v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Markku Kuismin</dc:creator>
    </item>
    <item>
      <title>Bayesian Semiparametric Joint Modeling of Gap-Time Distribution for Multitype Recurrent Events and a Terminal Event</title>
      <link>https://arxiv.org/abs/2509.10354</link>
      <description>arXiv:2509.10354v1 Announce Type: new 
Abstract: In biomedical settings, multitype recurrent events such as stroke and heart failure occur frequently, often concluding with a terminal event such as death. Understanding the links between these recurring and terminal events is fundamental to developing interventions that delay detrimental outcomes. Joint modeling is needed to quantify the dependence between event types and between recurrent events and mortality. We propose a Bayesian semiparametric joint model on the gap-time scale for multitype recurrent events and a terminal event. The model includes a shared frailty that links all recurrent types and the terminal event. Each baseline hazard is assigned a gamma-process prior, while regression and frailty parameters receive standard parametric priors. This ensures flexible baselines and familiar effect measures. The construction gives closed-form expressions for the cumulative hazard and frailty component and connects to Breslow-Aalen type estimators as a special case of our estimator, linking the Bayesian procedure to the classical approach. Computationally, we develop a simple MCMC sampler that avoids large matrix factorizations and scales nearly linearly in sample size. A comprehensive simulation evaluates four criteria: accuracy, prediction, robustness, and computation. There is no exact frequentist version of our specification; for comparison, we fit the same model with an EM algorithm in a frequentist framework. Our model and MCMC algorithm demonstrate superior performance on each criterion. We illustrate the approach with data from the Antihypertensive and Lipid-Lowering Treatment to Prevent Heart Attack Trial (ALLHAT), jointly analyzing acute and chronic cardiovascular recurrences and death.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10354v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mithun Kumar Acharjee, AKM Fazlur Rahman</dc:creator>
    </item>
    <item>
      <title>Network Meta-Analysis of survival outcomes with non-proportional hazards using flexible M-splines</title>
      <link>https://arxiv.org/abs/2509.10383</link>
      <description>arXiv:2509.10383v1 Announce Type: new 
Abstract: Network meta-analysis (NMA) is widely used in healthcare decision-making, where estimates of the effect of multiple treatments on outcomes are required. For time-to-event outcomes such as survival or disease progression the most common approach is to model log hazard ratios; however, this relies on the proportional hazards assumption. Novel treatments such as immunotherapies are expected to display complex hazard functions that cannot be captured by standard parametric models, which results in non-proportional hazards when comparing treatments from different classes. As a result, alternative models such as fractional polynomials or restricted cubic splines are often used. These allow substantial flexibility on the shape of the baseline hazard, but require time-consuming model selection or are intractable for Bayesian analysis. We propose a flexible NMA model using M-splines on the baseline hazard, with a novel weighted random walk prior distribution that provides shrinkage to avoid overfitting and is invariant to the choice of knots and timescale. Non-proportional hazards are modelled either by stratifying by treatment or by introducing treatment effects on the spline coefficients, and covariates may be included on the log hazard rate and spline coefficients. Treatment and covariate effects on the spline coefficients are given random walk prior distributions to smoothly model departures from proportionality over time. The methods are implemented in the user-friendly R package multinma, which supports analyses with aggregate data, individual participant data, or mixtures of both. We apply the methods to a NMA of progression-free survival with treatments for non-small cell lung cancer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10383v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David M. Phillippo (University of Bristol, Bristol, UK), Ayman Sadek (University of Bristol, Bristol, UK), Hugo Pedder (University of Bristol, Bristol, UK), Nicky J. Welton (University of Bristol, Bristol, UK)</dc:creator>
    </item>
    <item>
      <title>ALIGNS: Unlocking nomological networks in psychological measurement through a large language model</title>
      <link>https://arxiv.org/abs/2509.09723</link>
      <description>arXiv:2509.09723v1 Announce Type: cross 
Abstract: Psychological measurement is critical to many disciplines. Despite advances in measurement, building nomological networks, theoretical maps of how concepts and measures relate to establish validity, remains a challenge 70 years after Cronbach and Meehl proposed them as fundamental to validation. This limitation has practical consequences: clinical trials may fail to detect treatment effects, and public policy may target the wrong outcomes. We introduce Analysis of Latent Indicators to Generate Nomological Structures (ALIGNS), a large language model-based system trained with validated questionnaire measures. ALIGNS provides three comprehensive nomological networks containing over 550,000 indicators across psychology, medicine, social policy, and other fields. This represents the first application of large language models to solve a foundational problem in measurement validation. We report classification accuracy tests used to develop the model, as well as three evaluations. In the first evaluation, the widely used NIH PROMIS anxiety and depression instruments are shown to converge into a single dimension of emotional distress. The second evaluation examines child temperament measures and identifies four potential dimensions not captured by current frameworks, and questions one existing dimension. The third evaluation, an applicability check, engages expert psychometricians who assess the system's importance, accessibility, and suitability. ALIGNS is freely available at nomologicalnetwork.org, complementing traditional validation methods with large-scale nomological analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09723v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kai R. Larsen, Sen Yan, Roland M\"uller, Lan Sang, Mikko R\"onkk\"o, Ravi Starzl, Donald Edmondson</dc:creator>
    </item>
    <item>
      <title>AI Harmonics: a human-centric and harms severity-adaptive AI risk assessment framework</title>
      <link>https://arxiv.org/abs/2509.10104</link>
      <description>arXiv:2509.10104v1 Announce Type: cross 
Abstract: The absolute dominance of Artificial Intelligence (AI) introduces unprecedented societal harms and risks. Existing AI risk assessment models focus on internal compliance, often neglecting diverse stakeholder perspectives and real-world consequences. We propose a paradigm shift to a human-centric, harm-severity adaptive approach grounded in empirical incident data. We present AI Harmonics, which includes a novel AI harm assessment metric (AIH) that leverages ordinal severity data to capture relative impact without requiring precise numerical estimates. AI Harmonics combines a robust, generalized methodology with a data-driven, stakeholder-aware framework for exploring and prioritizing AI harms. Experiments on annotated incident data confirm that political and physical harms exhibit the highest concentration and thus warrant urgent mitigation: political harms erode public trust, while physical harms pose serious, even life-threatening risks, underscoring the real-world relevance of our approach. Finally, we demonstrate that AI Harmonics consistently identifies uneven harm distributions, enabling policymakers and organizations to target their mitigation efforts effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10104v1</guid>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sofia Vei, Paolo Giudici, Pavlos Sermpezis, Athena Vakali, Adelaide Emma Bernardelli</dc:creator>
    </item>
    <item>
      <title>Joint Optimization and Statistical Inference for Zero-th Order Simulation Optimization</title>
      <link>https://arxiv.org/abs/2210.06737</link>
      <description>arXiv:2210.06737v3 Announce Type: replace 
Abstract: We consider stochastic optimization problems with the dual tasks of (i) effectively finding the optimizer and (ii) reliably conducting statistical inference for the optimal objective function value. We find that classical simulation optimization and stochastic optimization algorithms, despite of their fast convergence rates to the optimizer under strong convexity assumptions, may not come with a valid central limit theorem (CLT) with a vanishing bias. This non-vanishing bias can harm statistical inference and the construction of asymptotically valid confidence intervals. We fix this issue by providing a new stochastic optimization algorithm that on one hand maintains the same fast convergence rate and on the other hand permits the establishment of a valid CLT with vanishing bias. We discuss practical implementations of the proposed algorithm and conduct numerical experiments to illustrate the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.06737v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhang Wu, Zeyu Zheng, Yingfei Wang, Guangyu Zhang, Zuohua Zhang, Chu Wang</dc:creator>
    </item>
    <item>
      <title>Finite Population Survey Sampling: An Unapologetic Bayesian Perspective</title>
      <link>https://arxiv.org/abs/2306.10635</link>
      <description>arXiv:2306.10635v4 Announce Type: replace 
Abstract: This article attempts to offer some perspectives on Bayesian inference for finite population quantities when the units in the population are assumed to exhibit complex dependencies. Beginning with an overview of Bayesian hierarchical models, including some that yield design-based Horvitz-Thompson estimators, the article proceeds to introduce dependence in finite populations and sets out inferential frameworks for ignorable and nonignorable responses. Multivariate dependencies using graphical models and spatial processes are discussed and some salient features of two recent analyses for spatial finite populations are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.10635v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s13171-024-00348-8</arxiv:DOI>
      <arxiv:journal_reference>Sankhya A 86 (Suppl 1), 95--124 (2024)</arxiv:journal_reference>
      <dc:creator>Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>Zero-inflation in the Multivariate Poisson Lognormal Family</title>
      <link>https://arxiv.org/abs/2405.14711</link>
      <description>arXiv:2405.14711v2 Announce Type: replace 
Abstract: Analyzing high-dimensional count data is a challenge and statistical model-based approaches provide an adequate and efficient framework that preserves explainability. The (multivariate) Poisson-Log-Normal (PLN) model is one such model: it assumes count data are driven by an underlying structured latent Gaussian variable, so that the dependencies between counts solely stems from the latent dependencies. However PLN doesn't account for zero-inflation, a feature frequently observed in real-world datasets. Here we introduce the Zero-Inflated PLN (ZIPLN) model, adding a multivariate zero-inflated component to the model, as an additional Bernoulli latent variable. The Zero-Inflation can be fixed, site-specific, feature-specific or depends on covariates. We estimate model parameters using variational inference that scales up to datasets with a few thousands variables and compare two approximations: (i) independent Gaussian and Bernoulli variational distributions or (ii) Gaussian variational distribution conditioned on the Bernoulli one. The method is assessed on synthetic data and the efficiency of ZIPLN is established even when zero-inflation concerns up to 90% of the observed counts. We then apply both ZIPLN and PLN to a cow microbiome dataset, containing 90.6% of zeroes. Accounting for zero-inflation significantly increases log-likelihood and reduces dispersion in the latent space, thus leading to improved group discrimination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14711v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bastien Batardi\`ere, Julien Chiquet, Fran\c{c}ois Gindraud, Mahendra Mariadassou</dc:creator>
    </item>
    <item>
      <title>Generalizing Difference-in-Differences to Non-Canonical Settings: Identifying an Array of Estimands</title>
      <link>https://arxiv.org/abs/2408.16039</link>
      <description>arXiv:2408.16039v4 Announce Type: replace 
Abstract: Consider a general setting in which data on an outcome is collected in two `groups' at two time periods, with certain group-periods deemed `treated' and others `untreated'. A special case is the canonical Difference-in-Differences (DiD) setting in which one group is treated only in the second period while the other is treated in neither period. Then it is well known that under a parallel trends assumption across the two groups the classic DiD formula (subtracting the average change in outcome across periods in the treated group by the average change in the outcome across periods in the untreated group) identifies the average treatment effect on the treated in the second period. But other relations between group, period, and treatment are possible. For example, the groups might be demographic (or other baseline covariate) categories with all units in both groups treated in the second period and none treated in the first, i.e. a pre-post design. Or one group might be treated in both periods while the other is treated in neither. Furthermore, other parallel trends assumptions under other treatment regimes are possible. For example, we could assume the two groups' potential outcomes would evolve in parallel under a regime of `do not switch treatment in the second period'. In fact, there is a literal array of data structures and parallel trends assumptions. The difference between the changes in outcomes of the two groups, which we dub the `group DiD' (gDiD) formula, identifies different causal estimands depending on the data structure and parallel trends assumption adopted. Here, we determine under which combinations of data structure and assumptions the gDiD formula identifies meaningful causal estimands. We also explore when parallel trends assumptions are amenable to empirical check or structural justification via Single World Intervention Graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16039v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zach Shahn, Laura Hatfield</dc:creator>
    </item>
    <item>
      <title>Priors from Envisioned Posterior Judgments: A Novel Elicitation Approach With Application to Bayesian Clinical Trials</title>
      <link>https://arxiv.org/abs/2409.05271</link>
      <description>arXiv:2409.05271v3 Announce Type: replace 
Abstract: Background: The uptake of formalized prior elicitation from experts in Bayesian clinical trials has been limited due to challenges such as complex statistical modeling, lack of practical tools, and the cognitive burden placed on experts arising from needing to quantify their uncertainty probabilistically. Existing methods also fail to address prior-posterior coherence, i.e., how do we ensure that the posterior distribution, obtained mathematically from combining the estimated prior with the trial data, reflects the expert's actual posterior beliefs?
  Method: In this study, we propose a new elicitation approach that effectuates prior-posterior coherence and reduces cognitive burden. This is achieved by eliciting expert responses, comprising point estimates only, about envisioned posterior judgments under various data outcomes and inferring the prior distribution by minimizing discrepancies between these responses and expected responses derived from the posterior distribution. Via an iterative process, experts receive feedback on the degree of coherency of their responses, and are invited to revise their responses to achieve greater coherency. The feasibility and potential value of this new approach are illustrated through an application to an ongoing trial.
  Results: We involved 10 experts from Walk 'n watch trial research team. Experts were presented with 16 hypothetical outcome scenarios to experts and elicit the priors followed by the developed elicitation framework. Following two rounds of elicitation, experts' judgments showed substantial improvement in coherency, demonstrating the practical applicability of the proposed elicitation approach.
  Conclusion: The proposed method provides a practical solution to the challenges of formalized prior elicitation in Bayesian clinical trials by addressing prior-posterior coherence and reducing cognitive demands on experts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05271v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yongdong Ouyang, Janice J Eng, Denghuang Zhan, Hubert Wong, The WnW Research Team</dc:creator>
    </item>
    <item>
      <title>Evolving Voices Based on Temporal Poisson Factorisation</title>
      <link>https://arxiv.org/abs/2410.18486</link>
      <description>arXiv:2410.18486v2 Announce Type: replace 
Abstract: The world is evolving and so is the vocabulary used to discuss topics in speech. Analysing political speech data from more than 30 years requires the use of flexible topic models to uncover the latent topics and their change in prevalence over time as well as the change in the vocabulary of the topics. We propose the temporal Poisson factorisation (TPF) model as an extension to the Poisson factorisation model to model sparse count data matrices obtained based on the bag-of-words assumption from text documents with time stamps. We discuss and empirically compare different model specifications for the time-varying latent variables consisting either of a flexible auto-regressive structure of order one or a random walk. Estimation is based on variational inference where we consider a combination of coordinate ascent updates with automatic differentiation using batching of documents. Suitable variational families are proposed to ease inference. We compare results obtained using independent univariate variational distributions for the time-varying latent variables to those obtained with a multivariate variant. We discuss in detail the results of the TPF model when analysing speeches from 18 sessions in the U.S. Senate (1981-2016).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18486v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1177/1471082X251355682</arxiv:DOI>
      <arxiv:journal_reference>Statistical Modelling. 2025;0(0)</arxiv:journal_reference>
      <dc:creator>Jan V\'avra (Vienna University of Economics and Business, Paris-Lodron University of Salzburg), Bettina Gr\"un (Vienna University of Economics and Business), Paul Hofmarcher (Paris-Lodron University of Salzburg)</dc:creator>
    </item>
    <item>
      <title>Detecting State Changes in Functional Neuronal Connectivity using Factorial Switching Linear Dynamical Systems</title>
      <link>https://arxiv.org/abs/2411.04229</link>
      <description>arXiv:2411.04229v2 Announce Type: replace 
Abstract: A key question in brain sciences is how to identify time-evolving functional connectivity, such as that obtained from recordings of neuronal activity over time. We wish to explain the observed phenomena in terms of latent states which, in the case of neuronal activity, might correspond to subnetworks of neurons within a brain or organoid. Many existing approaches assume that only one latent state can be active at a time, in contrast to our domain knowledge. We propose a switching dynamical system based on the factorial hidden Markov model. Unlike existing approaches, our model acknowledges that neuronal activity can be caused by multiple subnetworks, which may be activated either jointly or independently. A change in one part of the network does not mean that the entire connectivity pattern will change. We pair our model with scalable variational inference algorithm, using a concrete relaxation of the underlying factorial hidden Markov model, to effectively infer the latent states and model parameters. We show that our algorithm can recover ground-truth structure and yield insights about the maturation of neuronal activity in microelectrode array recordings from in vitro neuronal cultures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04229v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiwei Gong, Susanna B. Mierau, Sinead A. Williamson</dc:creator>
    </item>
    <item>
      <title>The ladder of abstraction in statistical graphics</title>
      <link>https://arxiv.org/abs/2501.06920</link>
      <description>arXiv:2501.06920v3 Announce Type: replace 
Abstract: Graphical forms such as scatterplots, line plots, and histograms are so familiar that it can be easy to forget how abstract they are. As a result, we often produce graphs that are difficult to follow. We propose a strategy for graphical communication by climbing a ladder of abstraction (a term from linguistics that we borrow from Hayakawa, 1939), starting with simple plots of special cases and then at each step embedding a graph into a more general framework. We demonstrate with two examples, first graphing a set of equations related to a modeled trajectory and then graphing data from an analysis of income and voting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06920v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Gelman</dc:creator>
    </item>
    <item>
      <title>Efficient Inference for Time-to-Event Outcomes by Integrating Right-Censored and Current Status Data</title>
      <link>https://arxiv.org/abs/2508.10357</link>
      <description>arXiv:2508.10357v2 Announce Type: replace 
Abstract: We propose a semiparametric data fusion framework for efficient inference on survival probabilities by integrating right-censored and current status data. Existing data fusion methods focus largely on fusing right-censored data only, while standard meta-analysis approaches are inadequate for combining right-censored and current status data, as estimators based on current status data alone typically converge at slower rates and have non-normal limiting distributions. In this work, we consider a semiparametric model under exchangeable event time distribution across data sources. We derive the canonical gradient of the survival probability at a given time, and develop one-step estimators along with the corresponding inference procedure. Specifically, we propose a doubly robust estimator and an efficient estimator that attains the semiparametric efficiency bound under mild conditions. Importantly, we show that incorporating current status data can lead to meaningful efficiency gains despite the slower convergence rate of current status-only estimators. We demonstrate the performance of our proposed method in simulations and discuss extensions to settings with covariate shift. We believe that this work has the potential to open new directions in data fusion methodology, particularly for settings involving mixed censoring types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10357v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiudi Li, Sijia Li</dc:creator>
    </item>
    <item>
      <title>Simulation-based Inference via Langevin Dynamics with Score Matching</title>
      <link>https://arxiv.org/abs/2509.03853</link>
      <description>arXiv:2509.03853v2 Announce Type: replace 
Abstract: Simulation-based inference (SBI) enables Bayesian analysis when the likelihood is intractable but model simulations are available. Recent advances in statistics and machine learning, including Approximate Bayesian Computation and deep generative models, have expanded the applicability of SBI, yet these methods often face challenges in moderate to high-dimensional parameter spaces. Motivated by the success of gradient-based Monte Carlo methods in Bayesian sampling, we propose a novel SBI method that integrates score matching with Langevin dynamics to explore complex posterior landscapes more efficiently in such settings. Our approach introduces tailored score-matching procedures for SBI, including a localization scheme that reduces simulation costs and an architectural regularization that embeds the statistical structure of log-likelihood scores to improve score-matching accuracy. We provide theoretical analysis of the method and illustrate its practical benefits on benchmark tasks and on more challenging problems in moderate to high dimensions, where it performs favorably compared to existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03853v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyu Jiang, Yuexi Wang, Yun Yang</dc:creator>
    </item>
    <item>
      <title>Dealing with Logs and Zeros in Regression Models</title>
      <link>https://arxiv.org/abs/2203.11820</link>
      <description>arXiv:2203.11820v2 Announce Type: replace-cross 
Abstract: The log transformation is widely used in linear regression, mainly because coefficients are interpretable as proportional effects. Yet this practice has fundamental limitations, most notably that the log is undefined at zero, creating an identification problem. We propose a new estimator, iterated OLS (iOLS), which targets the normalized average treatment effect, preserving the percentage-change interpretation while addressing these limitations. Our procedure is the theoretically justified analogue of the ad-hoc log(1+Y) transformation and delivers a consistent and asymptotically normal estimator of the parameters of the exponential conditional mean model. iOLS is computationally efficient, globally convergent, and free of the incidental-parameter bias, while extending naturally to endogenous regressors through iterated 2SLS. We illustrate the methods with simulations and revisit three influential publications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.11820v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christophe Bell\'ego, David Benatia, Louis Pape</dc:creator>
    </item>
    <item>
      <title>Compositionality in algorithms for smoothing</title>
      <link>https://arxiv.org/abs/2303.13865</link>
      <description>arXiv:2303.13865v4 Announce Type: replace-cross 
Abstract: Backward Filtering Forward Guiding (BFFG) is a bidirectional algorithm proposed in Mider et al. [2021] and studied more in depth in a general setting in Van der Meulen and Schauer [2022]. In category theory, optics have been proposed for modelling systems with bidirectional data flow. We connect BFFG with optics by demonstrating that the forward and backwards map together define a functor from a category of Markov kernels into a category of optics, which is furthermore lax monoidal in the case when the guiding kernels coincide with the generative dynamics</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.13865v4</guid>
      <category>math.CT</category>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moritz Schauer, Frank van der Meulen, Andi Q. Wang</dc:creator>
    </item>
    <item>
      <title>Constructive Universal Approximation and Sure Convergence for Multi-Layer Neural Networks</title>
      <link>https://arxiv.org/abs/2507.04779</link>
      <description>arXiv:2507.04779v2 Announce Type: replace-cross 
Abstract: We propose o1Neuro, a new neural network model built on sparse indicator activation neurons, with two key statistical properties. (1) Constructive universal approximation: At the population level, a deep o1Neuro can approximate any measurable function of $\boldsymbol{X}$, while a shallow o1Neuro suffices for additive models with two-way interaction components, including XOR and univariate terms, assuming $\boldsymbol{X} \in [0,1]^p$ has bounded density. Combined with prior work showing that a single-hidden-layer non-sparse network is a universal approximator, this highlights a trade-off between activation sparsity and network depth in approximation capability. (2) Sure convergence: At the sample level, the optimization of o1Neuro reaches an optimal model with probability approaching one after sufficiently many update rounds, and we provide an example showing that the required number of updates is well bounded under linear data-generating models. Empirically, o1Neuro is compared with XGBoost, Random Forests, and TabNet for learning complex regression functions with interactions, demonstrating superior predictive performance on several benchmark datasets from OpenML and the UCI Machine Learning Repository with $n = 10000$, as well as on synthetic datasets with $100 \le n \le 20000$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04779v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chien-Ming Chi</dc:creator>
    </item>
    <item>
      <title>Counterfactual Probabilistic Diffusion with Expert Models</title>
      <link>https://arxiv.org/abs/2508.13355</link>
      <description>arXiv:2508.13355v2 Announce Type: replace-cross 
Abstract: Predicting counterfactual distributions in complex dynamical systems is essential for scientific modeling and decision-making in domains such as public health and medicine. However, existing methods often rely on point estimates or purely data-driven models, which tend to falter under data scarcity. We propose a time series diffusion-based framework that incorporates guidance from imperfect expert models by extracting high-level signals to serve as structured priors for generative modeling. Our method, ODE-Diff, bridges mechanistic and data-driven approaches, enabling more reliable and interpretable causal inference. We evaluate ODE-Diff across semi-synthetic COVID-19 simulations, synthetic pharmacological dynamics, and real-world case studies, demonstrating that it consistently outperforms strong baselines in both point prediction and distributional accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13355v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenhao Mu, Zhi Cao, Mehmed Uludag, Alexander Rodr\'iguez</dc:creator>
    </item>
  </channel>
</rss>
