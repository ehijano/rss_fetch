<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Feb 2026 05:00:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Maximum Likelihood Particle Tracking in Turbulent Flows via Sparse Optimization</title>
      <link>https://arxiv.org/abs/2602.22257</link>
      <description>arXiv:2602.22257v1 Announce Type: new 
Abstract: Lagrangian particle tracking is essential for characterizing turbulent flows, but inferring particle acceleration from inherently noisy position data remains a significant challenge. Fluid particles in turbulence experience extreme, intermittent accelerations, resulting in heavy-tailed probability density functions (PDFs) that deviate strongly from Gaussian predictions. Existing filtering techniques, such as Gaussian kernels and penalized B-splines, implicitly assume Gaussian-distributed jerk, thereby penalizing sparse, high-magnitude acceleration changes and artificially suppressing the intermittent tails. In this work, we develop a novel maximum likelihood estimation (MLE) framework that explicitly accounts for this non-Gaussian intermittency. By formulating a modified Gaussian process to model the random incremental forcing, we introduce a sparse optimization scheme utilizing a convex 1-norm relaxation. To overcome the numerical stiffness associated with high-order difference operators, the problem is efficiently solved using an iteratively reweighted least squares (IRLS) algorithm. The proposed filter is evaluated against direct numerical simulation (DNS) data of homogeneous, isotropic turbulence (Re approx. 310). Results demonstrate that the IRLS approach consistently outperforms state-of-the-art discrete MLE, continuous MLE, and B-spline methods, yielding systematic reductions in root-mean-squared error (RMSE) across position, velocity, and acceleration. Most importantly, the proposed framework succeeds in better recovering the heavy-tailed statistical structure of both acceleration and acceleration differences (jerk) across temporal scales, preserving the physical intermittency characteristic of high-Reynoldsnumber turbulent flows that baseline methods severely attenuate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22257v1</guid>
      <category>physics.data-an</category>
      <category>physics.flu-dyn</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Griffin M Kearney, Kasey M Laurent, Makan Fardad</dc:creator>
    </item>
    <item>
      <title>Titanic overconfidence -- dark uncertainty can sink hybrid metrology for semiconductor manufacturing</title>
      <link>https://arxiv.org/abs/2602.23131</link>
      <description>arXiv:2602.23131v1 Announce Type: new 
Abstract: Hybrid metrology for semiconductor manufacturing is on a collision course with dark uncertainty. An IEEE technology roadmap for this venture has targeted a linewidth uncertainty of +/- 0.17 nm at 95 % coverage and advised the hybridization of results from different measurement methods to hit this target. Related studies have applied statistical models that require consistent results to compel a lower uncertainty, whereas inconsistent results are prevalent. We illuminate this lurking issue, studying how standard methods of uncertainty evaluation fail to account for the causes and effects of dark uncertainty. We revisit a comparison of imaging and scattering methods to measure linewidths of approximately 13 nm, applying contrasting statistical models to highlight the potential effect of dark uncertainty on hybrid metrology. A random effects model allows the combination of inconsistent results, accounting for dark uncertainty and estimating a total uncertainty of +/- 0.8 nm at 95 % coverage. In contrast, a common mean model requires consistent results for combination, ignoring dark uncertainty and underestimating the total uncertainty by as much as a factor of five. To avoid such titanic overconfidence, which can sink a venture, we outline good practices to reduce dark uncertainty and guide the combination of indeterminately consistent results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.23131v1</guid>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ronald G. Dixson, Adam L. Pintar, R. Joseph. Kline, Thomas A. Germer, J. Alexander Liddle, John S. Villarrubia, Samuel M. Stavis</dc:creator>
    </item>
    <item>
      <title>A Thermodynamic Structure of Asymptotic Inference</title>
      <link>https://arxiv.org/abs/2602.22605</link>
      <description>arXiv:2602.22605v1 Announce Type: cross 
Abstract: A thermodynamic framework for asymptotic inference is developed in which sample size and parameter variance define a state space. Within this description, Shannon information plays the role of entropy, and an integrating factor organizes its variation into a first-law-type balance equation. The framework supports a cyclic inequality analogous to a reversed second law, derived for the estimation of the mean. A non-trivial third-law-type result emerges as a lower bound on entropy set by representation noise. Optimal inference paths, global bounds on information gain, and a natural Carnot-like information efficiency follow from this structure, with efficiency fundamentally limited by a noise floor. Finally, de Bruijn's identity and the I-MMSE relation in the Gaussian-limit case appear as coordinate projections of the same underlying thermodynamic structure. This framework suggests that ensemble physics and inferential physics constitute shadow processes evolving in opposite directions within a unified thermodynamic description.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22605v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>physics.data-an</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Willy Wong</dc:creator>
    </item>
    <item>
      <title>Optimization-based Unfolding in High-Energy Physics</title>
      <link>https://arxiv.org/abs/2602.22776</link>
      <description>arXiv:2602.22776v1 Announce Type: cross 
Abstract: In High-Energy Physics, unfolding is the process of reconstructing true distributions of physical observables from detector-distorted measurements. Starting from its reformulation as a regularized quadratic optimization, we develop a framework to tackle this problem using both classical and quantum-compatible methods. In particular, we derive a Quadratic Unconstrained Binary Optimization (QUBO) representation of the unfolding objective, allowing direct implementation on quantum annealing and hybrid quantum-classical solvers. The proposed approach is implemented in QUnfold, an open-source Python package integrating classical mixed-integer solvers and D-Wave's hybrid quantum solver. We benchmark the method against widely used unfolding techniques in RooUnfold, including response Matrix Inversion, Iterative Bayesian Unfolding, and Singular Value Decomposition unfolding, using synthetic dataset with controlled distortion effects. Our results demonstrate that the optimization-based approach achieves competitive reconstruction accuracy across multiple distributions while naturally accommodating regularization within the objective function. This work establishes a unified optimization perspective on unfolding and provides a practical pathway for exploring quantum-enhanced methods in experimental HEP data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22776v1</guid>
      <category>quant-ph</category>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simone Gasperini, Gianluca Bianco, Marco Lorusso, Carla Rieger, Michele Grossi</dc:creator>
    </item>
    <item>
      <title>Fluid dynamics meet network science: two cases of temporal network eigendecomposition</title>
      <link>https://arxiv.org/abs/2509.03135</link>
      <description>arXiv:2509.03135v2 Announce Type: replace 
Abstract: Temporal networks, defined as sequences of time-aggregated adjacency matrices, sample latent graph dynamics and trace trajectories in graph space. By interpreting each adjacency matrix as a different time snapshot of a scalar field, fluid-mechanics theories can be applied to construct two distinct eigendecompositions of temporal networks. The first builds on the proper orthogonal decomposition (POD) of flowfields and decomposes the evolution of a network in terms of a basis of orthogonal network eigenmodes which are ordered in terms of their relative importance, hence enabling compression of temporal networks as well as their reconstruction from low-dimensional embeddings. The second proposes a numerical approximation of the Koopman operator, a linear operator acting on a suitable observable of the graph space which provides the best linear approximation of the latent graph dynamics. Its eigendecomposition provides a data-driven spectral description of the temporal network dynamics, in terms of dynamic modes which grow, decay or oscillate over time. Both eigendecompositions are illustrated and validated in a suite of synthetic generative models of temporal networks with varying complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03135v2</guid>
      <category>physics.data-an</category>
      <category>physics.soc-ph</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas Lacasa</dc:creator>
    </item>
    <item>
      <title>Geometric Autoencoder Priors for Bayesian Inversion: Learn First Observe Later</title>
      <link>https://arxiv.org/abs/2509.19929</link>
      <description>arXiv:2509.19929v2 Announce Type: replace-cross 
Abstract: Uncertainty Quantification (UQ) is paramount for inference in engineering. A common inference task is to recover full-field information of physical systems from a small number of noisy observations, a usually highly ill-posed problem. Sharing information from multiple distinct yet related physical systems can alleviate this ill-possendess. Critically, engineering systems often have complicated variable geometries prohibiting the use of standard multi-system Bayesian UQ. In this work, we introduce Geometric Autoencoders for Bayesian Inversion (GABI), a framework for learning geometry-aware generative models of physical responses that serve as highly informative geometry-conditioned priors for Bayesian inversion. Following a ''learn first, observe later'' paradigm, GABI distills information from large datasets of systems with varying geometries, without requiring knowledge of governing PDEs, boundary conditions, or observation processes, into a rich latent prior. At inference time, this prior is seamlessly combined with the likelihood of a specific observation process, yielding a geometry-adapted posterior distribution. Our proposed framework is architecture agnostic. A creative use of Approximate Bayesian Computation (ABC) sampling yields an efficient implementation that utilizes modern GPU hardware. We test our method on: steady-state heat over rectangular domains; Reynold-Averaged Navier-Stokes (RANS) flow around airfoils; Helmholtz resonance and source localization on 3D car bodies; RANS airflow over terrain. We find: the predictive accuracy to be comparable to deterministic supervised learning approaches in the restricted setting where supervised learning is applicable; UQ to be well calibrated and robust on challenging problems with complex geometries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19929v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arnaud Vadeboncoeur, Gregory Duth\'e, Mark Girolami, Eleni Chatzi</dc:creator>
    </item>
    <item>
      <title>Constraining Power of Wavelet vs. Power Spectrum Statistics for CMB Lensing and Weak Lensing with Learned Binning</title>
      <link>https://arxiv.org/abs/2510.13968</link>
      <description>arXiv:2510.13968v2 Announce Type: replace-cross 
Abstract: We present forecasts for constraints on the matter density ($\Omega_m$) and the amplitude of matter density fluctuations at 8h$^{-1}$Mpc ($\sigma_8$) from CMB lensing convergence maps and galaxy weak lensing convergence maps. For CMB lensing convergence auto statistics, we compare the angular power spectra ($C_\ell$'s) to the wavelet scattering transform (WST) coefficients. For CMB lensing convergence $\times$ galaxy weak lensing convergence statistics, we compare the cross angular power spectra to wavelet phase harmonics (WPH). This work also serves as the first application of WST and WPH to these probes. For CMB lensing convergence, we find that WST and $C_\ell$'s yield similar constraints in forecasts for all surveys considered in this work. When CMB lensing convergence is crossed with galaxy weak lensing convergence projected from $\textit{Euclid}$ Data Release 2 (DR2), we find that WPH outperforms cross-$C_\ell$'s by factors between $2.2$ and $3.4$ for individual parameter constraints. To compare these different summary statistics, we develop a novel learned binning approach. This method compresses summary statistics while maintaining interpretability. We find this leads to improved constraints compared to more naive binning schemes for our wavelet-based statistics, but not for $C_\ell$'s. By learning the binning and measuring constraints on distinct data sets, our method is robust to overfitting by construction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13968v2</guid>
      <category>astro-ph.CO</category>
      <category>astro-ph.IM</category>
      <category>physics.data-an</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyle Boone, Georgios Valogiannis, Marco Gatti, Cora Dvorkin</dc:creator>
    </item>
    <item>
      <title>Physics-Aware, Shannon-Optimal Compression via Arithmetic Coding for Distributional Fidelity</title>
      <link>https://arxiv.org/abs/2602.19476</link>
      <description>arXiv:2602.19476v2 Announce Type: replace-cross 
Abstract: Assessing whether two datasets are distributionally consistent has become a central theme in modern scientific analysis, particularly as generative artificial intelligence is increasingly used to produce synthetic datasets whose fidelity must be rigorously validated against the original data on which they are trained, a task made more challenging by the continued growth in data volume and problem dimensionality. In this work, we propose the use of arithmetic coding to provide a lossless and invertible compression of datasets under a physics-informed probabilistic representation. Datasets that share the same underlying physical correlations admit comparable optimal descriptions, while discrepancies in those correlations-arising from miscalibration, mismodeling, or bias-manifest as an irreducible excess in code length. This excess codelength defines an operational fidelity metric, quantified directly in bits through differences in achievable compression length relative to a physics-inspired reference distribution. We demonstrate that this metric is global, interpretable, additive across components, and asymptotically optimal in the Shannon sense. Moreover, we show that differences in codelength correspond to differences in expected negative log-likelihood evaluated under the same physics-informed reference model. As a byproduct, we also demonstrate that our compression approach achieves a higher compression ratio than traditional general-purpose algorithms such as gzip. Our results establish lossless, physics-aware compression based on arithmetic coding not as an end in itself, but as a measurement instrument for testing the fidelity between datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19476v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>physics.data-an</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cristiano Fanelli</dc:creator>
    </item>
  </channel>
</rss>
