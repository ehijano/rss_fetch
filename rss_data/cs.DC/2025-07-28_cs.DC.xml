<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 29 Jul 2025 02:18:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>CUTHERMO: Understanding GPU Memory Inefficiencies with Heat Map Profiling</title>
      <link>https://arxiv.org/abs/2507.18729</link>
      <description>arXiv:2507.18729v1 Announce Type: new 
Abstract: GPUs have become indispensable in high-performance computing, machine learning, and many other domains. Efficiently utilizing the memory subsystem on GPUs is critical for maximizing computing power through massive parallelism. Analyzing memory access patterns has proven to be an effective method for understanding memory bottlenecks in applications. However, comprehensive runtime and fine-grained memory profiling support is lacking on GPU architectures. In this work, we introduce cuThermo, a lightweight and practical profiling tool for GPU memory analysis. It operates on GPU binaries without requiring any modifications to hardware, operating system, or application source code. Given a CUDA application, cuThermo identifies memory inefficiencies at runtime via a heat map based on distinct visited warp counts to represent word-sector-level data sharing and provides optimization guidance in performance tuning iterations. Through our experiments on six applications, we identified five memory access patterns that are portable across different GPU architectures. By evaluating optimization on two GPUs, cuThermo achieves up to $721.79\%$ performance improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18729v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanbo Zhao, Jinku Cui, Zecheng Li, Shuyin Jiao, Xu Liu, Jiajia Li</dc:creator>
    </item>
    <item>
      <title>PPipe: Efficient Video Analytics Serving on Heterogeneous GPU Clusters via Pool-Based Pipeline Parallelism</title>
      <link>https://arxiv.org/abs/2507.18748</link>
      <description>arXiv:2507.18748v1 Announce Type: new 
Abstract: With the rapid innovation of GPUs, heterogeneous GPU clusters in both public clouds and on-premise data centers have become increasingly commonplace. In this paper, we demonstrate how pipeline parallelism, a technique wellstudied for throughput-oriented deep learning model training, can be used effectively for serving latency-bound model inference, e.g., in video analytics systems, on heterogeneous GPU clusters. Our work exploits the synergy between diversity in model layers and diversity in GPU architectures, which results in comparable inference latency for many layers when running on low-class and high-class GPUs. We explore how such overlooked capability of low-class GPUs can be exploited using pipeline parallelism and present a novel inference serving system, PPipe, that employs pool-based pipeline parallelism via an MILP-based control plane and a data plane that performs resource reservation-based adaptive batching. Evaluation results on diverse workloads (18 CNN models) show that PPipe achieves 41.1% - 65.5% higher utilization of low-class GPUs while maintaining high utilization of high-class GPUs, leading to 32.2% - 75.1% higher serving throughput compared to various baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18748v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>2025 USENIX Annual Technical Conference (USENIX ATC 25)</arxiv:journal_reference>
      <dc:creator>Z. Jonny Kong, Qiang Xu, Y. Charlie Hu</dc:creator>
    </item>
    <item>
      <title>Deadline-Aware Joint Task Scheduling and Offloading in Mobile Edge Computing Systems</title>
      <link>https://arxiv.org/abs/2507.18864</link>
      <description>arXiv:2507.18864v1 Announce Type: new 
Abstract: The demand for stringent interactive quality-of-service has intensified in both mobile edge computing (MEC) and cloud systems, driven by the imperative to improve user experiences. As a result, the processing of computation-intensive tasks in these systems necessitates adherence to specific deadlines or achieving extremely low latency. To optimize task scheduling performance, existing research has mainly focused on reducing the number of late jobs whose deadlines are not met. However, the primary challenge with these methods lies in the total search time and scheduling efficiency. In this paper, we present the optimal job scheduling algorithm designed to determine the optimal task order for a given set of tasks. In addition, users are enabled to make informed decisions for offloading tasks based on the information provided by servers. The details of performance analysis are provided to show its optimality and low complexity with the linearithmic time O(nlogn), where $n$ is the number of tasks. To tackle the uncertainty of the randomly arriving tasks, we further develop an online approach with fast outage detection that achieves rapid acceptance times with time complexity of O(n). Extensive numerical results are provided to demonstrate the effectiveness of the proposed algorithm in terms of the service ratio and scheduling cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18864v1</guid>
      <category>cs.DC</category>
      <category>cs.CC</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/JIOT.2024.3425854</arxiv:DOI>
      <arxiv:journal_reference>IEEE Internet of Things Journal, vol. 11, no. 20, pp. 33282-33295, Oct. 15, 2024</arxiv:journal_reference>
      <dc:creator>Ngoc Hung Nguyen, Van-Dinh Nguyen, Anh Tuan Nguyen, Nguyen Van Thieu, Hoang Nam Nguyen, Symeon Chatzinotas</dc:creator>
    </item>
    <item>
      <title>GPUnion: Autonomous GPU Sharing on Campus</title>
      <link>https://arxiv.org/abs/2507.18928</link>
      <description>arXiv:2507.18928v1 Announce Type: new 
Abstract: A pronounced imbalance in GPU resources exists on campus, where some laboratories own underutilized servers while others lack the compute needed for AI research. GPU sharing can alleviate this disparity, while existing platforms typically rely on centralized oversight and persistent allocation models, conflicting with the voluntary and autonomous nature of academic resource ownership. We present GPUnion, a campus-scale GPU sharing platform enabling voluntary participation while preserving full provider autonomy. GPUnion incorporates three core mechanisms: i) container-based task dispatching and execution, ii) resource provider-first architecture, and iii) resilient execution featuring automatic checkpointing and migration. GPUnion also supports custom data storage and integrates the non-root execution and image attestation for isolation and security improvement for containerization. Case studies across multiple campus scenarios demonstrate 30% more GPU utilization improvement, 40% increase in interactive sessions, and 94% successful workload migration during provider departures. GPUnion demonstrates that provider autonomy and platform reliability can coexist, challenging conventional centralized paradigms and democratizing access to computational resources within campus networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18928v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yufang Li (The Hong Kong University of Science,Technology), Yuanbo Zhang (Sun Yat-sen University), Hanlong Liao (National University of Defense Technology), Guoming Tang (The Hong Kong University of Science,Technology), Deke Guo (Sun Yat-sen University)</dc:creator>
    </item>
    <item>
      <title>The Case for Time-Shared Computing Resources</title>
      <link>https://arxiv.org/abs/2507.19287</link>
      <description>arXiv:2507.19287v2 Announce Type: new 
Abstract: The environmental impact of Information and Communication Technologies (ICT) continues to grow, driven notably by increasing usage, rebound effects, and emerging demands. However, despite the virtual nature of its services, the sector remains inherently constrained by its materiality and cannot rely on an infinite pool of resources. As a result, the wide variety of supported services may need to be managed under stricter limits within hosting facilities in the future. Contrary to common assumptions, we show that tenants typically do not share computing resources, even in environments commonly perceived as mutualized, such as cloud platforms. Time-sharing has been progressively phased out for reasons of performance, security, predictability, and, perhaps more importantly, due to the decreasing cost of computing resources. This paper advocates for managing fewer physical resources by improving resource sharing between tenants. It represents a paradigm shift, moving beyond traditional time-sharing at the hardware level to a higher abstraction. This approach entails "doing with fewer resources" under conditions of "reduced performance". Nonetheless, enhancing the mutualization of infrastructure can reduce cluster sizes (through consolidation) and improve energy efficiency, with gains related to the accepted performance trade-off, a situation potentially more socially acceptable than eliminating services. We review the current state of the art, identify challenges and opportunities, propose interpretations of Time-Shared Computing, and outline key research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19287v2</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pierre Jacquet, Adrien Luxey-Bitri</dc:creator>
    </item>
    <item>
      <title>RailX: A Flexible, Scalable, and Low-Cost Network Architecture for Hyper-Scale LLM Training Systems</title>
      <link>https://arxiv.org/abs/2507.18889</link>
      <description>arXiv:2507.18889v1 Announce Type: cross 
Abstract: Increasingly large AI workloads are calling for hyper-scale infrastructure; however, traditional interconnection network architecture is neither scalable nor cost-effective enough. Tree-based topologies such as the \textit{Rail-optimized} network are extremely expensive, while direct topologies such as \textit{Torus} have insufficient bisection bandwidth and flexibility. In this paper, we propose \textit{RailX}, a reconfigurable network architecture based on intra-node direct connectivity and inter-node circuit switching. Nodes and optical switches are physically 2D-organized, achieving better scalability than existing centralized circuit switching networks. We propose a novel interconnection method based on \textit{Hamiltonian Decomposition} theory to organize separate rail-based rings into \textit{all-to-all} topology, simultaneously optimizing ring-collective and all-to-all communication. More than $100$K chips with hyper bandwidth can be interconnected with a flat switching layer, and the diameter is only $2\sim4$ inter-node hops. The network cost per injection/All-Reduce bandwidth of \textit{RailX} is less than $10\%$ of the Fat-Tree, and the cost per bisection/All-to-All bandwidth is less than $50\%$ of the Fat-Tree. Specifically, only $\sim$\$$1.3$B is required to interconnect 200K chips with 1.8TB bandwidth. \textit{RailX} can also be used in the ML-as-a-service (MLaaS) scenario, where single or multiple training workloads with various shapes, scales, and parallelism strategies can be flexibly mapped, and failures can be worked around.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18889v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinxiao Feng, Tiancheng Chen, Yuchen Wei, Siyuan Shen, Shiju Wang, Wei Li, Kaisheng Ma, Torsten Hoefler</dc:creator>
    </item>
    <item>
      <title>A New One-Shot Federated Learning Framework for Medical Imaging Classification with Feature-Guided Rectified Flow and Knowledge Distillation</title>
      <link>https://arxiv.org/abs/2507.19045</link>
      <description>arXiv:2507.19045v1 Announce Type: cross 
Abstract: In multi-center scenarios, One-Shot Federated Learning (OSFL) has attracted increasing attention due to its low communication overhead, requiring only a single round of transmission. However, existing generative model-based OSFL methods suffer from low training efficiency and potential privacy leakage in the healthcare domain. Additionally, achieving convergence within a single round of model aggregation is challenging under non-Independent and Identically Distributed (non-IID) data. To address these challenges, in this paper a modified OSFL framework is proposed, in which a new Feature-Guided Rectified Flow Model (FG-RF) and Dual-Layer Knowledge Distillation (DLKD) aggregation method are developed. FG-RF on the client side accelerates generative modeling in medical imaging scenarios while preserving privacy by synthesizing feature-level images rather than pixel-level images. To handle non-IID distributions, DLKD enables the global student model to simultaneously mimic the output logits and align the intermediate-layer features of client-side teacher models during aggregation. Experimental results on three non-IID medical imaging datasets show that our new framework and method outperform multi-round federated learning approaches, achieving up to 21.73% improvement, and exceeds the baseline FedISCA by an average of 21.75%. Furthermore, our experiments demonstrate that feature-level synthetic images significantly reduce privacy leakage risks compared to pixel-level synthetic images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19045v1</guid>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yufei Ma, Hanwen Zhang, Qiya Yang, Guibo Luo, Yuesheng Zhu</dc:creator>
    </item>
    <item>
      <title>Big Data Energy Systems: A Survey of Practices and Associated Challenges</title>
      <link>https://arxiv.org/abs/2507.19154</link>
      <description>arXiv:2507.19154v1 Announce Type: cross 
Abstract: Energy systems generate vast amounts of data in extremely short time intervals, creating challenges for efficient data management. Traditional data management methods often struggle with scalability and accessibility, limiting their usefulness. More advanced solutions, such as NoSQL databases and cloud-based platforms, have been adopted to address these issues. Still, even these advanced solutions can encounter bottlenecks, which can impact the efficiency of data storage, retrieval, and analysis. This review paper explores the research trends in big data management for energy systems, highlighting the practices, opportunities and challenges. Also, the data regulatory demands are highlighted using chosen reference architectures. The review, in particular, explores the limitations of current storage and data integration solutions and examines how new technologies are applied to the energy sector. Novel insights into emerging technologies, including data spaces, various data management architectures, peer-to-peer data management, and blockchains, are provided, along with practical recommendations for achieving enhanced data sharing and regulatory compliance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19154v1</guid>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lunodzo J. Mwinuka, Massimo Cafaro, Lucas Pereira, Hugo Morais</dc:creator>
    </item>
    <item>
      <title>SDVDiag: A Modular Platform for the Diagnosis of Connected Vehicle Functions</title>
      <link>https://arxiv.org/abs/2507.19403</link>
      <description>arXiv:2507.19403v1 Announce Type: cross 
Abstract: Connected and software-defined vehicles promise to offer a broad range of services and advanced functions to customers, aiming to increase passenger comfort and support autonomous driving capabilities. Due to the high reliability and availability requirements of connected vehicles, it is crucial to resolve any occurring failures quickly. To achieve this however, a complex cloud/edge architecture with a mesh of dependencies must be navigated to diagnose the responsible root cause. As such, manual analyses become unfeasible since they would significantly delay the troubleshooting.
  To address this challenge, this paper presents SDVDiag, an extensible platform for the automated diagnosis of connected vehicle functions. The platform enables the creation of pipelines that cover all steps from initial data collection to the tracing of potential root causes. In addition, SDVDiag supports self-adaptive behavior by the ability to exchange modules at runtime. Dependencies between functions are detected and continuously updated, resulting in a dynamic graph view of the system. In addition, vital system metrics are monitored for anomalies. Whenever an incident is investigated, a snapshot of the graph is taken and augmented by relevant anomalies. Finally, the analysis is performed by traversing the graph and creating a ranking of the most likely causes.
  To evaluate the platform, it is deployed inside an 5G test fleet environment for connected vehicle functions. The results show that injected faults can be detected reliably. As such, the platform offers the potential to gain new insights and reduce downtime by identifying problems and their causes at an early stage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19403v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Matthias Wei{\ss}, Falk Dettinger, Michael Weyrich</dc:creator>
    </item>
    <item>
      <title>An OpenSource CI/CD Pipeline for Variant-Rich Software-Defined Vehicles</title>
      <link>https://arxiv.org/abs/2507.19446</link>
      <description>arXiv:2507.19446v1 Announce Type: cross 
Abstract: Software-defined vehicles (SDVs) offer a wide range of connected functionalities, including enhanced driving behavior and fleet management. These features are continuously updated via over-the-air (OTA) mechanisms, resulting in a growing number of software versions and variants due to the diversity of vehicles, cloud/edge environments, and stakeholders involved. The lack of a unified integration environment further complicates development, as connected mobility solutions are often built in isolation. To ensure reliable operations across heterogeneous systems, a dynamic orchestration of functions that considers hardware and software variability is essential. This paper presents an open-source CI/CD pipeline tailored for SDVs. It automates the build, test, and deployment phases using a combination of containerized open-source tools, creating a standardized, portable, and scalable ecosystem accessible to all stakeholders. Additionally, a custom OTA middleware distributes software updates and supports rollbacks across vehicles and backend services. Update variants are derived based on deployment target dependencies and hardware configurations. The pipeline also supports continuous development and deployment of AI models for autonomous driving features. Its effectiveness is evaluated using an automated valet parking (AVP) scenario involving TurtleBots and a coordinating backend server. Two object detection variants are developed and deployed to match hardware-specific requirements. Results demonstrate seamless OTA updates, correct variant selection, and successful orchestration across all targets. Overall, the proposed pipeline provides a scalable and efficient solution for managing software variants and OTA updates in SDVs, contributing to the advancement of future mobility technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19446v1</guid>
      <category>cs.SE</category>
      <category>cs.DC</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Matthias Wei{\ss}, Anish Navalgund, Johannes St\"umpfle, Falk Dettinger, Michael Weyrich</dc:creator>
    </item>
    <item>
      <title>Fully Energy-Efficient Randomized Backoff: Slow Feedback Loops Yield Fast Contention Resolution</title>
      <link>https://arxiv.org/abs/2302.07751</link>
      <description>arXiv:2302.07751v5 Announce Type: replace 
Abstract: Contention resolution addresses the problem of coordinating access to a shared channel. Time proceeds in slots, and a packet transmission can be made in any slot. A packet is successfully sent if no other packet is also transmitted during that slot. If two or more packets are sent in the same slot, then none of these transmissions succeed. Listening during a slot gives ternary feedback, indicating if that slot had (0) silence, (1) a successful transmission, or (2+) noise. No other feedback is available. Packets are (adversarially) injected into the system over time. A packet departs the system once it is successful. The goal is to send all packets while optimizing throughput, which is roughly the fraction of successful slots.
  Most prior algorithms with constant throughput require a short feedback loop, in the sense that a packet's sending probability in slot t+1 is fully determined by its internal state at slot t and the channel feedback at slot t. An open question is whether these short feedback loops are necessary; that is, how often must listening and updating occur in order to achieve constant throughput? This question addresses energy efficiency, since both listening and sending consume significant energy. The channel can also suffer adversarial noise ("jamming"), which causes any listener to hear noise, even when no packets are sent. How does jamming affect our goal of long feedback loops/energy efficiency?
  Connecting these questions, we ask: what does a contention-resolution algorithm have to sacrifice to reduce channel accesses? Must we give up on constant throughput or robustness to noise? Here, we show that we need not concede anything. Suppose there are N packets and J jammed slots, where the input is determined by an adaptive adversary. We give an algorithm that, with high probability in N+J, has constant throughput and polylog(N+J) channel accesses per packet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.07751v5</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael A. Bender, Jeremy T. Fineman, Seth Gilbert, John Kuszmaul, Maxwell Young</dc:creator>
    </item>
    <item>
      <title>Urban Green Governance: IoT-Driven Management and Enhancement of Urban Green Spaces in Campobasso</title>
      <link>https://arxiv.org/abs/2507.12106</link>
      <description>arXiv:2507.12106v4 Announce Type: replace 
Abstract: The efficient design and management of public green spaces is a key factor in promoting the health and well-being of urban population, as emphasized by the WHO, UNEP, and EEA. These areas serve as the "green lungs" of the urban ecosystem, playing a vital role in enhancing quality of life thanks to the provision of ecosystem services. In this context, the Smart Green City use case in Campobasso municipality, funded by the Italian Ministry of Enterprises (MIMIT), emerges as an innovative model for the sustainable management of green urban areas through the adoption of an advanced system of emerging technologies integrated and interoperable. The project integrates IoT systems and data-driven governance platforms, enabling real-time monitoring of the health status of trees and green areas via a Decision Support System (DSS). It also facilitates the collection and analysis of data from diverse sources, including weather conditions, air quality, soil moisture, pollution levels. The resulting cloud-based platform supports a holistic real time decision making for green urban managers, technical experts and operational staff. It enables intelligent control and management of urban green spaces using Tree Talker sensors, integrated with soil moisture and water potential monitoring systems. Thanks to predictive models based on machine learning algorithms and real time data provided by IoT sensors, irrigation of public parks can be optimized by providing suggestions on when and how much water to apply. Customized alerts layers are also activated warning users when monitored parameters, such as soil temperature, humidity, or water potential, exceed predefined thresholds. This Use Case demonstrates how digitalization, IoT sensors fusion and technological innovation can support sustainable urban governance, fostering environmental resilience and improving citizens quality of life.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12106v4</guid>
      <category>cs.DC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonio Salis, Gabriele Troina, Gianluca Boanelli, Marco Ottaviano, Paola Fortini, Soraya Versace</dc:creator>
    </item>
    <item>
      <title>FedVSR: Towards Model-Agnostic Federated Learning in Video Super-Resolution</title>
      <link>https://arxiv.org/abs/2503.13745</link>
      <description>arXiv:2503.13745v2 Announce Type: replace-cross 
Abstract: Video super-resolution aims to enhance low-resolution videos by leveraging both spatial and temporal information. While deep learning has led to impressive progress, it typically requires centralized data, which raises privacy concerns. Federated learning offers a privacy-friendly solution, but general FL frameworks often struggle with low-level vision tasks, resulting in blurry, low-quality outputs. To address this, we introduce FedVSR, the first FL framework specifically designed for VSR. It is model-agnostic and stateless, and introduces a lightweight loss function based on the DWT to better preserve high-frequency details during local training. Additionally, a loss-aware aggregation strategy combines both DWT-based and task-specific losses to guide global updates effectively. Extensive experiments across multiple VSR models and datasets demonstrate that FedVSR consistently outperforms existing FL methods, achieving up to 0.82 dB higher PSNR, 0.0327 higher SSIM, and 0.0251 lower LPIPS. These results underscore FedVSR's ability to bridge the gap between privacy and performance, setting a new benchmark for federated learning in low-level vision tasks. The code is available at: https://github.com/alimd94/FedVSR</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13745v2</guid>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Mollaahmadi Dehaghi, Hossein KhademSohi, Reza Razavi, Steve Drew, Mohammad Moshirpour</dc:creator>
    </item>
    <item>
      <title>Performance in solving the Hermitian and pseudo-Hermitian Bethe-Salpeter equation with the Yambo code</title>
      <link>https://arxiv.org/abs/2504.10096</link>
      <description>arXiv:2504.10096v2 Announce Type: replace-cross 
Abstract: We analyze the performance of two strategies in solving the structured eigenvalue problem deriving from the Bethe-Salpeter equation (BSE) in condensed matter physics. The BSE matrix is constructed with the \texttt{Yambo} code, and the two strategies are implemented by interfacing \texttt{Yambo} with the ScaLAPACK and ELPA libraries for direct diagonalization, and with the SLEPc library for the iterative approach. We consider both the Hermitian (Tamm-Dancoff approximation) and pseudo-Hermitian forms, addressing dense matrices of three different sizes. A description of the implementation is also provided, with details for the pseudo-Hermitian case. Timing and memory utilization are analyzed on both CPU and GPU clusters. Our results demonstrate that it is now feasible to handle dense BSE matrices of the order of 10$^5$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10096v2</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.DC</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Petru Milev, Blanca Mellado-Pinto, Muralidhar Nalabothula, Ali Esquembre Kucukalic, Fernando Alvarruiz, Enrique Ramos, Alejandro Molina-Sanchez, Ludger Wirtz, Jose E. Roman, Davide Sangalli</dc:creator>
    </item>
    <item>
      <title>AI Flow: Perspectives, Scenarios, and Approaches</title>
      <link>https://arxiv.org/abs/2506.12479</link>
      <description>arXiv:2506.12479v3 Announce Type: replace-cross 
Abstract: Pioneered by the foundational information theory by Claude Shannon and the visionary framework of machine intelligence by Alan Turing, the convergent evolution of information and communication technologies (IT/CT) has created an unbroken wave of connectivity and computation. This synergy has sparked a technological revolution, now reaching its peak with large artificial intelligence (AI) models that are reshaping industries and redefining human-machine collaboration. However, the realization of ubiquitous intelligence faces considerable challenges due to substantial resource consumption in large models and high communication bandwidth demands. To address these challenges, AI Flow has been introduced as a multidisciplinary framework that integrates cutting-edge IT and CT advancements, with a particular emphasis on the following three key points. First, device-edge-cloud framework serves as the foundation, which integrates end devices, edge servers, and cloud clusters to optimize scalability and efficiency for low-latency model inference. Second, we introduce the concept of familial models, which refers to a series of different-sized models with aligned hidden features, enabling effective collaboration and the flexibility to adapt to varying resource constraints and dynamic scenarios. Third, connectivity- and interaction-based intelligence emergence is a novel paradigm of AI Flow. By leveraging communication networks to enhance connectivity, the collaboration among AI models across heterogeneous nodes achieves emergent intelligence that surpasses the capability of any single model. The innovations of AI Flow provide enhanced intelligence, timely responsiveness, and ubiquitous accessibility to AI services, paving the way for the tighter fusion of AI techniques and communication systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12479v3</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <category>eess.SP</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongjun An, Wenhan Hu, Sida Huang, Siqi Huang, Ruanjun Li, Yuanzhi Liang, Jiawei Shao, Yiliang Song, Zihan Wang, Cheng Yuan, Chi Zhang, Hongyuan Zhang, Wenhao Zhuang, Xuelong Li</dc:creator>
    </item>
  </channel>
</rss>
