<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 31 Oct 2024 02:04:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>An Actor-Critic Approach to Boosting Text-to-SQL Large Language Model</title>
      <link>https://arxiv.org/abs/2410.22082</link>
      <description>arXiv:2410.22082v1 Announce Type: new 
Abstract: Text-To-SQL (T2S) conversion based on large language models (LLMs) has found a wide range of applications, by leveraging the capabilities of LLMs in interpreting the query intent expressed in natural language. Existing research focuses on suitable representations for data schema and/or questions, task-specific instructions and representative examples, and complicated inference pipelines. All these methods are empirical and task specific, without a theoretical bound on performance. In this paper, we propose a simple, general, and performance guaranteed T2S enhancement approach called Actor-Critic (AC). Specifically, we design two roles using the same LLM: an Actor to produce SQL queries and a Critic to evaluate the produced SQL. If the Critic believes the produced SQL is wrong, it notifies the Actor to reproduce the SQL and perform evaluation again. By this simple iterative process, expected performance can be derived in theory. We conducted extensive experiments on the Spider and related datasets with eleven LLMs, and demonstrated that the Actor-Critic method consistently improves the performance of T2S, thus serving as a general enhancement approach for T2S conversion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22082v1</guid>
      <category>cs.DB</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyang Zheng, Haipeng Jing, Canyu Rui, Askar Hamdulla, Dong Wang</dc:creator>
    </item>
    <item>
      <title>DAGE: DAG Query Answering via Relational Combinator with Logical Constraints</title>
      <link>https://arxiv.org/abs/2410.22105</link>
      <description>arXiv:2410.22105v1 Announce Type: new 
Abstract: Predicting answers to queries over knowledge graphs is called a complex reasoning task because answering a query requires subdividing it into subqueries. Existing query embedding methods use this decomposition to compute the embedding of a query as the combination of the embedding of the subqueries. This requirement limits the answerable queries to queries having a single free variable and being decomposable, which are called tree-form queries and correspond to the $\mathcal{SROI}^-$ description logic. In this paper, we define a more general set of queries, called DAG queries and formulated in the $\mathcal{ALCOIR}$ description logic, propose a query embedding method for them, called DAGE, and a new benchmark to evaluate query embeddings on them. Given the computational graph of a DAG query, DAGE combines the possibly multiple paths between two nodes into a single path with a trainable operator that represents the intersection of relations and learns DAG-DL from tautologies. We show that it is possible to implement DAGE on top of existing query embedding methods, and we empirically measure the improvement of our method over the results of vanilla methods evaluated in tree-form queries that approximate the DAG queries of our proposed benchmark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22105v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yunjie He, Bo Xiong, Daniel Hern\'andez, Yuqicheng Zhu, Evgeny Kharlamov, Steffen Staab</dc:creator>
    </item>
    <item>
      <title>Accelerating Privacy-Preserving Medical Record Linkage: A Three-Party MPC Approach</title>
      <link>https://arxiv.org/abs/2410.21605</link>
      <description>arXiv:2410.21605v1 Announce Type: cross 
Abstract: Motivation: Record linkage is a crucial concept for integrating data from multiple sources, particularly when datasets lack exact identifiers, and it has diverse applications in real-world data analysis. Privacy-Preserving Record Linkage (PPRL) ensures this integration occurs securely, protecting sensitive information from unauthorized access. This is especially important in sectors such as healthcare, where datasets include private identity information (IDAT) governed by strict privacy laws. However, maintaining both privacy and efficiency in large-scale record linkage poses significant challenges. Consequently, researchers must develop advanced methods to protect data privacy while optimizing processing performance. This paper presents a novel and efficient PPRL method based on a secure 3-party computation (MPC) framework. Our approach allows multiple parties to compute linkage results without exposing their private inputs and significantly improves the speed of linkage process compared to existing privacy-preserving solutions. Results: We demonstrated that our method preserves the linkage quality of the state-of-the-art PPRL method while achieving up to 14 times faster performance. For example, linking a record against a database of 10,000 records takes just 8.74 seconds in a realistic network with 700 Mbps bandwidth and 60 ms latency. Even on a slower internet connection with 100 Mbps bandwidth and 60 ms latency, the linkage completes in 28 seconds, highlighting the scalability and efficiency of our solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21605v1</guid>
      <category>cs.CR</category>
      <category>cs.DB</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>\c{S}eyma Selcan Ma\u{g}ara, Noah Dietrich, Ali Burak \"Unal, Mete Akg\"un</dc:creator>
    </item>
    <item>
      <title>Understanding and Reusing Test Suites Across Database Systems</title>
      <link>https://arxiv.org/abs/2410.21731</link>
      <description>arXiv:2410.21731v1 Announce Type: cross 
Abstract: Database Management System (DBMS) developers have implemented extensive test suites to test their DBMSs. For example, the SQLite test suites contain over 92 million lines of code. Despite these extensive efforts, test suites are not systematically reused across DBMSs, leading to wasted effort. Integration is challenging, as test suites use various test case formats and rely on unstandardized test runner features. We present a unified test suite, SQuaLity, in which we integrated test cases from three widely-used DBMSs, SQLite, PostgreSQL, and DuckDB. In addition, we present an empirical study to determine the potential of reusing these systems' test suites. Our results indicate that reusing test suites is challenging: First, test formats and test runner commands vary widely; for example, SQLite has 4 test runner commands, while MySQL has 112 commands with additional features, to, for example, execute file operations or interact with a shell. Second, while some test suites contain mostly standard-compliant statements (e.g., 99% in SQLite), other test suites mostly test non-standardized functionality (e.g., 31% of statements in the PostgreSQL test suite are nonstandardized). Third, test reuse is complicated by various explicit and implicit dependencies, such as the need to set variables and configurations, certain test cases requiring extensions not present by default, and query results depending on specific clients. Despite the above findings, we have identified 3 crashes, 3 hangs, and multiple compatibility issues across four different DBMSs by executing test suites across DBMSs, indicating the benefits of reuse. Overall, this work represents the first step towards test-case reuse in the context of DBMSs, and we hope that it will inspire follow-up work on this important topic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21731v1</guid>
      <category>cs.SE</category>
      <category>cs.DB</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3698829</arxiv:DOI>
      <dc:creator>Suyang Zhong, Manuel Rigger</dc:creator>
    </item>
    <item>
      <title>Pushing the Performance Envelope of DNN-based Recommendation Systems Inference on GPUs</title>
      <link>https://arxiv.org/abs/2410.22249</link>
      <description>arXiv:2410.22249v1 Announce Type: cross 
Abstract: Personalized recommendation is a ubiquitous application on the internet, with many industries and hyperscalers extensively leveraging Deep Learning Recommendation Models (DLRMs) for their personalization needs (like ad serving or movie suggestions). With growing model and dataset sizes pushing computation and memory requirements, GPUs are being increasingly preferred for executing DLRM inference. However, serving newer DLRMs, while meeting acceptable latencies, continues to remain challenging, making traditional deployments increasingly more GPU-hungry, resulting in higher inference serving costs. In this paper, we show that the embedding stage continues to be the primary bottleneck in the GPU inference pipeline, leading up to a 3.2x embedding-only performance slowdown.
  To thoroughly grasp the problem, we conduct a detailed microarchitecture characterization and highlight the presence of low occupancy in the standard embedding kernels. By leveraging direct compiler optimizations, we achieve optimal occupancy, pushing the performance by up to 53%. Yet, long memory latency stalls continue to exist. To tackle this challenge, we propose specialized plug-and-play-based software prefetching and L2 pinning techniques, which help in hiding and decreasing the latencies. Further, we propose combining them, as they complement each other. Experimental evaluations using A100 GPUs with large models and datasets show that our proposed techniques improve performance by up to 103% for the embedding stage, and up to 77% for the overall DLRM inference pipeline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22249v1</guid>
      <category>cs.AR</category>
      <category>cs.DB</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <category>cs.SE</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rishabh Jain, Vivek M. Bhasi, Adwait Jog, Anand Sivasubramaniam, Mahmut T. Kandemir, Chita R. Das</dc:creator>
    </item>
    <item>
      <title>ElasticNotebook: Enabling Live Migration for Computational Notebooks (Technical Report)</title>
      <link>https://arxiv.org/abs/2309.11083</link>
      <description>arXiv:2309.11083v5 Announce Type: replace 
Abstract: Computational notebooks (e.g., Jupyter, Google Colab) are widely used for interactive data science and machine learning. In those frameworks, users can start a session, then execute cells (i.e., a set of statements) to create variables, train models, visualize results, etc. Unfortunately, existing notebook systems do not offer live migration: when a notebook launches on a new machine, it loses its state, preventing users from continuing their tasks from where they had left off. This is because, unlike DBMS, the sessions directly rely on underlying kernels (e.g., Python/R interpreters) without an additional data management layer. Existing techniques for preserving states, such as copying all variables or OS-level checkpointing, are unreliable (often fail), inefficient, and platform-dependent. Also, re-running code from scratch can be highly time-consuming. In this paper, we introduce a new notebook system, ElasticNotebook, that offers live migration via checkpointing/restoration using a novel mechanism that is reliable, efficient, and platform-independent. Specifically, by observing all cell executions via transparent, lightweight monitoring, ElasticNotebook can find a reliable and efficient way (i.e., replication plan) for reconstructing the original session state, considering variable-cell dependencies, observed runtime, variable sizes, etc. To this end, our new graph-based optimization problem finds how to reconstruct all variables (efficiently) from a subset of variables that can be transferred across machines. We show that ElasticNotebook reduces end-to-end migration and restoration times by 85%-98% and 94%-99%, respectively, on a variety (i.e., Kaggle, JWST, and Tutorial) of notebooks with negligible runtime and memory overheads of &lt;2.5% and &lt;10%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.11083v5</guid>
      <category>cs.DB</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.14778/3626292.3626296</arxiv:DOI>
      <dc:creator>Zhaoheng Li, Pranav Gor, Rahul Prabhu, Hui Yu, Yuzhou Mao, Yongjoo Park</dc:creator>
    </item>
    <item>
      <title>PGB: Benchmarking Differentially Private Synthetic Graph Generation Algorithms</title>
      <link>https://arxiv.org/abs/2408.02928</link>
      <description>arXiv:2408.02928v3 Announce Type: replace 
Abstract: Differentially private graph analysis is a powerful tool for deriving insights from diverse graph data while protecting individual information. Designing private analytic algorithms for different graph queries often requires starting from scratch. In contrast, differentially private synthetic graph generation offers a general paradigm that supports one-time generation for multiple queries. Although a rich set of differentially private graph generation algorithms has been proposed, comparing them effectively remains challenging due to various factors, including differing privacy definitions, diverse graph datasets, varied privacy requirements, and multiple utility metrics.
  To this end, we propose PGB (Private Graph Benchmark), a comprehensive benchmark designed to enable researchers to compare differentially private graph generation algorithms fairly. We begin by identifying four essential elements of existing works as a 4-tuple: mechanisms, graph datasets, privacy requirements, and utility metrics. We discuss principles regarding these elements to ensure the comprehensiveness of a benchmark. Next, we present a benchmark instantiation that adheres to all principles, establishing a new method to evaluate existing and newly proposed graph generation algorithms. Through extensive theoretical and empirical analysis, we gain valuable insights into the strengths and weaknesses of prior algorithms. Our results indicate that there is no universal solution for all possible cases. Finally, we provide guidelines to help researchers select appropriate mechanisms for various scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02928v3</guid>
      <category>cs.DB</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shang Liu, Hao Du, Yang Cao, Bo Yan, Jinfei Liu, Masatoshi Yoshikawa</dc:creator>
    </item>
    <item>
      <title>Modeling and Monitoring of Indoor Populations using Sparse Positioning Data (Extension)</title>
      <link>https://arxiv.org/abs/2410.21142</link>
      <description>arXiv:2410.21142v2 Announce Type: replace 
Abstract: In large venues like shopping malls and airports, knowledge on the indoor populations fuels applications such as business analytics, venue management, and safety control. In this work, we provide means of modeling populations in partitions of indoor space offline and of monitoring indoor populations continuously, by using indoor positioning data. However, the low-sampling rates of indoor positioning render the data temporally and spatially sparse, which in turn renders the offline capture of indoor populations challenging. It is even more challenging to continuously monitor indoor populations, as positioning data may be missing or not ready yet at the current moment. To address these challenges, we first enable probabilistic modeling of populations in indoor space partitions as Normal distributions. Based on that, we propose two learning-based estimators for on-the-fly prediction of population distributions. Leveraging the prediction-based schemes, we provide a unified continuous query processing framework for a type of query that enables continuous monitoring of populated partitions. The framework encompasses caching and result validity mechanisms to reduce cost and maintain monitoring effectiveness. Extensive experiments on two real data sets show that the proposed estimators are able to outperform the state-of-the-art alternatives and that the query processing framework is effective and efficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21142v2</guid>
      <category>cs.DB</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiao Li, Huan Li, Hua Lu, Christian S. Jensen</dc:creator>
    </item>
  </channel>
</rss>
