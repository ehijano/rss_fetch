<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.FL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.FL</link>
    <description>cs.FL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.FL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 07 Nov 2024 05:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>One-dimensional cellular automata with a unique active transition</title>
      <link>https://arxiv.org/abs/2411.03601</link>
      <description>arXiv:2411.03601v1 Announce Type: cross 
Abstract: A one-dimensional cellular automaton $\tau : A^\mathbb{Z} \to A^\mathbb{Z}$ is a transformation of the full shift defined via a finite neighborhood $S \subset \mathbb{Z}$ and a local function $\mu : A^S \to A$. We study the family of cellular automata whose finite neighborhood $S$ is an interval containing $0$, and there exists a pattern $p \in A^S$ satisfying that $\mu(z) = z(0)$ if and only if $z \neq p$; this means that these cellular automata have a unique \emph{active transition}. Despite its simplicity, this family presents interesting and subtle problems, as the behavior of the cellular automaton completely depends on the structure of $p$. We show that every cellular automaton $\tau$ with a unique active transition $p \in A^S$ is either idempotent or strictly almost equicontinuous, and we completely characterize each one of these situations in terms of $p$. In essence, the idempotence of $\tau$ depends on the existence of a certain subpattern of $p$ with a translational symmetry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03601v1</guid>
      <category>nlin.CG</category>
      <category>cs.FL</category>
      <category>math.DS</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alonso Castillo-Ramirez, Maria G. Maga\~na-Chavez, Luguis de los Santos Ba\~nos</dc:creator>
    </item>
    <item>
      <title>Transformers as Transducers</title>
      <link>https://arxiv.org/abs/2404.02040</link>
      <description>arXiv:2404.02040v3 Announce Type: replace 
Abstract: We study the sequence-to-sequence mapping capacity of transformers by relating them to finite transducers, and find that they can express surprisingly large classes of transductions. We do so using variants of RASP, a programming language designed to help people "think like transformers," as an intermediate representation. We extend the existing Boolean variant B-RASP to sequence-to-sequence functions and show that it computes exactly the first-order rational functions (such as string rotation). Then, we introduce two new extensions. B-RASP[pos] enables calculations on positions (such as copying the first half of a string) and contains all first-order regular functions. S-RASP adds prefix sum, which enables additional arithmetic operations (such as squaring a string) and contains all first-order polyregular functions. Finally, we show that masked average-hard attention transformers can simulate S-RASP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02040v3</guid>
      <category>cs.FL</category>
      <category>cs.LG</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lena Strobl, Dana Angluin, David Chiang, Jonathan Rawski, Ashish Sabharwal</dc:creator>
    </item>
    <item>
      <title>SynCode: LLM Generation with Grammar Augmentation</title>
      <link>https://arxiv.org/abs/2403.01632</link>
      <description>arXiv:2403.01632v4 Announce Type: replace-cross 
Abstract: LLMs are widely used in complex AI applications. These applications underscore the need for LLM outputs to adhere to a specific format, for their integration with other components in the systems. Typically the format rules e.g., for data serialization formats such as JSON, YAML, or Code in Programming Language are expressed as context-free grammar (CFG). Due to the hallucinations and unreliability of LLMs, instructing LLMs to adhere to specified syntax becomes an increasingly important challenge.
  We present SynCode, a novel framework for efficient and general syntactical decoding with LLMs, to address this challenge. SynCode ensures soundness and completeness with respect to the CFG of a formal language, effectively retaining valid tokens while filtering out invalid ones. SynCode uses an offline-constructed, efficient lookup table, the DFA mask store, derived from the DFA of the language's grammar for efficient generation. SynCode seamlessly integrates with any language defined by CFG, as evidenced by experiments focusing on generating JSON, Python, and Go outputs. Our experiments evaluating the effectiveness of SynCode for JSON generation demonstrate that SynCode eliminates all syntax errors and significantly outperforms state-of-the-art baselines. Furthermore, our results underscore how SynCode significantly reduces 96.07% of syntax errors in generated Python and Go code, showcasing its substantial impact on enhancing syntactical precision in LLM generation. Our code is available at https://github.com/uiuc-focal-lab/syncode</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01632v4</guid>
      <category>cs.LG</category>
      <category>cs.FL</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shubham Ugare, Tarun Suresh, Hangoo Kang, Sasa Misailovic, Gagandeep Singh</dc:creator>
    </item>
    <item>
      <title>Reward Machines for Deep RL in Noisy and Uncertain Environments</title>
      <link>https://arxiv.org/abs/2406.00120</link>
      <description>arXiv:2406.00120v3 Announce Type: replace-cross 
Abstract: Reward Machines provide an automaton-inspired structure for specifying instructions, safety constraints, and other temporally extended reward-worthy behaviour. By exposing the underlying structure of a reward function, they enable the decomposition of an RL task, leading to impressive gains in sample efficiency. Although Reward Machines and similar formal specifications have a rich history of application towards sequential decision-making problems, they critically rely on a ground-truth interpretation of the domain-specific vocabulary that forms the building blocks of the reward function--such ground-truth interpretations are elusive in the real world due in part to partial observability and noisy sensing. In this work, we explore the use of Reward Machines for Deep RL in noisy and uncertain environments. We characterize this problem as a POMDP and propose a suite of RL algorithms that exploit task structure under uncertain interpretation of the domain-specific vocabulary. Through theory and experiments, we expose pitfalls in naive approaches to this problem while simultaneously demonstrating how task structure can be successfully leveraged under noisy interpretations of the vocabulary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00120v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.FL</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew C. Li, Zizhao Chen, Toryn Q. Klassen, Pashootan Vaezipoor, Rodrigo Toro Icarte, Sheila A. McIlraith</dc:creator>
    </item>
  </channel>
</rss>
