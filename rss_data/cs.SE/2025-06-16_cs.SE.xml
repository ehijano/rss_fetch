<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Jun 2025 02:28:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Application Modernization with LLMs: Addressing Core Challenges in Reliability, Security, and Quality</title>
      <link>https://arxiv.org/abs/2506.10984</link>
      <description>arXiv:2506.10984v1 Announce Type: new 
Abstract: AI-assisted code generation tools have revolutionized software development, offering unprecedented efficiency and scalability. However, multiple studies have consistently highlighted challenges such as security vulnerabilities, reliability issues, and inconsistencies in the generated code. Addressing these concerns is crucial to unlocking the full potential of this transformative technology. While advancements in foundational and code-specialized language models have made notable progress in mitigating some of these issues, significant gaps remain, particularly in ensuring high-quality, trustworthy outputs.
  This paper builds upon existing research on leveraging large language models (LLMs) for application modernization. It explores an opinionated approach that emphasizes two core capabilities of LLMs: code reasoning and code generation. The proposed framework integrates these capabilities with human expertise to tackle application modernization challenges effectively. It highlights the indispensable role of human involvement and guidance in ensuring the success of AI-assisted processes.
  To demonstrate the framework's utility, this paper presents a detailed case study, walking through its application in a real-world scenario. The analysis includes a step-by-step breakdown, assessing alternative approaches where applicable. This work aims to provide actionable insights and a robust foundation for future research in AI-driven application modernization. The reference implementation created for this paper is available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10984v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ahilan Ayyachamy Nadar Ponnusamy</dc:creator>
    </item>
    <item>
      <title>Collaboration Tools and their Role in Agile Software Projects</title>
      <link>https://arxiv.org/abs/2506.10985</link>
      <description>arXiv:2506.10985v1 Announce Type: new 
Abstract: The purpose of this review is to understand the importance of collaboration tools which are Slack, Microsoft Teams, Confluence in Agile and software projects. Agile methodologies rely on flexibility, using cycles and integration throughout various levels of developing cycles. However, it is still a great problem for many teams to collaborate and communicate even if staff members and teams are working remotely. In terms of collaboration, the applications and technologies mean better organization of work, increased mutually understandable openness and fast and efficient inter team and interpersonal interactions to enhance results of projects into productivity. This paper examines how these tools fit the Agile principles, how they facilitate iterative development, and encouraging effective initiation and tracking of tasks in small and large projects. The insights focus on how Slack, Microsoft Teams, and Confluence are essential for gaining better task coordination, supporting knowledge sharing, and adopting agile values across cross-functional contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10985v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raman Mohammed Hussein, Bryar A. Hassan</dc:creator>
    </item>
    <item>
      <title>CoMRAT: Commit Message Rationale Analysis Tool</title>
      <link>https://arxiv.org/abs/2506.10986</link>
      <description>arXiv:2506.10986v1 Announce Type: new 
Abstract: In collaborative open-source development, the rationale for code changes is often captured in commit messages, making them a rich source of valuable information. However, research on rationale in commit messages remains limited. In this paper, we present CoMRAT, a tool for analyzing decision and rationale sentences rationale in commit messages. CoMRAT enables a) researchers to produce metrics and analyses on rationale information in any Github module, and b) developers to check the amount of rationale in their commit messages. A preliminary evaluation suggests the tool's usefulness and usability in both these research and development contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10986v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Mouna Dhaouadi, Bentley James Oakes, Michalis Famelis</dc:creator>
    </item>
    <item>
      <title>Chain of Draft for Software Engineering: Challenges in Applying Concise Reasoning to Code Tasks</title>
      <link>https://arxiv.org/abs/2506.10987</link>
      <description>arXiv:2506.10987v1 Announce Type: new 
Abstract: Large language models (LLMs) have become vital tools for software development, but they often require verbose intermediate reasoning for complex code tasks, leading to high latency and costs. This research extends the Chain of Draft (CoD) method to software engineering, designing and evaluating multiple CoD variants tailored for code tasks. Through comprehensive experiments on all 300 samples from the SWE-bench benchmark, we found that all CoD variants used significantly fewer tokens than Chain of Thought (CoT), with Baseline CoD being most efficient at 55.4% of CoT's tokens. While this represents substantial efficiency gains - translating to approximately 45% reduction in processing time and API costs - it differs from the extreme 7.6% reported in the original CoD paper for mathematical reasoning. This difference stems from the inherent complexity and context-dependency of software tasks, which require more detailed reasoning to maintain solution quality. Our multi-dimensional quality assessment revealed that CoD variants maintain over 90% of CoT's code quality across key metrics including correctness, compatibility, and maintainability, making them practical alternatives for real-world development scenarios where efficiency matters. This research demonstrates how domain-specific characteristics influence prompting strategy effectiveness and provides a framework for balancing efficiency with solution quality in software engineering applications. Our findings offer practical guidance for optimizing LLM-based development workflows through appropriate prompting strategy selection based on project requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10987v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaoyi Yang</dc:creator>
    </item>
    <item>
      <title>You Only Train Once: A Flexible Training Framework for Code Vulnerability Detection Driven by Vul-Vector</title>
      <link>https://arxiv.org/abs/2506.10988</link>
      <description>arXiv:2506.10988v1 Announce Type: new 
Abstract: With the pervasive integration of computer applications across industries, the presence of vulnerabilities within code bases poses significant risks. The diversity of software ecosystems coupled with the intricate nature of modern software engineering has led to a shift from manual code vulnerability identification towards the adoption of automated tools. Among these, deep learning-based approaches have risen to prominence due to their superior accuracy; however, these methodologies encounter several obstacles. Primarily, they necessitate extensive labeled datasets and prolonged training periods, and given the rapid emergence of new vulnerabilities, the frequent retraining of models becomes a resource-intensive endeavor, thereby limiting their applicability in cutting-edge scenarios. To mitigate these challenges, this paper introduces the \underline{\textbf{YOTO}}--\underline{\textbf{Y}}ou \underline{\textbf{O}}nly \underline{\textbf{T}}rain \underline{\textbf{O}}nce framework. This innovative approach facilitates the integration of multiple types of vulnerability detection models via parameter fusion, eliminating the need for joint training. Consequently, YOTO enables swift adaptation to newly discovered vulnerabilities, significantly reducing both the time and computational resources required for model updates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10988v1</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bowen Tian, Zhengyang Xu, Mingqiang Wu, Songning Lai, Yutai Yue</dc:creator>
    </item>
    <item>
      <title>Prompt engineering and framework: implementation to increase code reliability based guideline for LLMs</title>
      <link>https://arxiv.org/abs/2506.10989</link>
      <description>arXiv:2506.10989v1 Announce Type: new 
Abstract: In this paper, we propose a novel prompting approach aimed at enhancing the ability of Large Language Models (LLMs) to generate accurate Python code. Specifically, we introduce a prompt template designed to improve the quality and correctness of generated code snippets, enabling them to pass tests and produce reliable results. Through experiments conducted on two state-of-the-art LLMs using the HumanEval dataset, we demonstrate that our approach outperforms widely studied zero-shot and Chain-of-Thought (CoT) methods in terms of the Pass@k metric. Furthermore, our method achieves these improvements with significantly reduced token usage compared to the CoT approach, making it both effective and resource-efficient, thereby lowering the computational demands and improving the eco-footprint of LLM capabilities. These findings highlight the potential of tailored prompting strategies to optimize code generation performance, paving the way for broader applications in AI-driven programming tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10989v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rogelio Cruz, Jonatan Contreras, Francisco Guerrero, Ezequiel Rodriguez, Carlos Valdez, Citlali Carrillo</dc:creator>
    </item>
    <item>
      <title>On the Effectiveness of the 'Follow-the-Sun' Strategy in Mitigating the Carbon Footprint of AI in Cloud Instances</title>
      <link>https://arxiv.org/abs/2506.10990</link>
      <description>arXiv:2506.10990v1 Announce Type: new 
Abstract: 'Follow-the-Sun' (FtS) is a theoretical computational model aimed at minimizing the carbon footprint of computer workloads. It involves dynamically moving workloads to regions with cleaner energy sources as demand increases and energy production relies more on fossil fuels. With the significant power consumption of Artificial Intelligence (AI) being a subject of extensive debate, FtS is proposed as a strategy to mitigate the carbon footprint of training AI models. However, the literature lacks scientific evidence on the advantages of FtS to mitigate the carbon footprint of AI workloads. In this paper, we present the results of an experiment conducted in a partial synthetic scenario to address this research gap. We benchmarked four AI algorithms in the anomaly detection domain and measured the differences in carbon emissions in four cases: no strategy, FtS, and two strategies previously introduced in the state of the art, namely Flexible Start and Pause and Resume. To conduct our experiment, we utilized historical carbon intensity data from the year 2021 for seven European cities. Our results demonstrate that the FtS strategy not only achieves average reductions of up to 14.6% in carbon emissions (with peaks of 16.3%) but also helps in preserving the time needed for training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10990v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.DC</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Vergallo, Lu\'is Cruz, Alessio Errico, Luca Mainetti</dc:creator>
    </item>
    <item>
      <title>What is Business Process Automation Anyway?</title>
      <link>https://arxiv.org/abs/2506.10991</link>
      <description>arXiv:2506.10991v1 Announce Type: new 
Abstract: Many organizations strive to increase the level of automation in their business processes. While automation historically was mainly concerned with automating physical labor, current automation efforts mostly focus on automation in a digital manner, thus targeting work that is related to the interaction between humans and computers. This type of automation, commonly referred to as business process automation, has many facets. Yet, academic literature mainly focuses on Robotic Process Automation, a specific automation capability. Recognizing that leading vendors offer automation capabilities going way beyond that, we use this paper to develop a detailed understanding of business process automation in industry. To this end, we conduct a structured market analysis of the 18 predominant vendors of business process automation solutions as identified by Gartner. As a result, we provide a comprehensive overview of the business process automation capabilities currently offered by industrial vendors. We show which types and facets of automation exist and which aspects represent promising directions for the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10991v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hoang Vu, Henrik Leopold, Han van der Aa</dc:creator>
    </item>
    <item>
      <title>Towards a Theory on Process Automation Effects</title>
      <link>https://arxiv.org/abs/2506.10992</link>
      <description>arXiv:2506.10992v1 Announce Type: new 
Abstract: Process automation is a crucial strategy for improving business processes, but little attention has been paid to the effects that automation has once it is operational. This paper addresses this research problem by reviewing the literature on human-automation interaction. Although many of the studies in this field have been conducted in different domains, they provide a foundation for developing propositions about process automation effects. Our analysis focuses on how humans perceive automation technology when working within a process, allowing us to propose an effective engagement model between technology, process participants, process managers, and software developers. This paper offers insights and recommendations that can help organizations optimize their use of process automation. We further derive novel research questions for a discourse within the process automation community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10992v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hoang Vu, Jennifer Haase, Henrik Leopold, Jan Mendling</dc:creator>
    </item>
    <item>
      <title>Contract-based Verification of Digital Twins</title>
      <link>https://arxiv.org/abs/2506.10993</link>
      <description>arXiv:2506.10993v1 Announce Type: new 
Abstract: Digital twins are becoming powerful tools in industrial applications, offering virtual representations of cyber-physical systems. However, verification of these models remains a significant challenge due to the potentially large datasets used by the digital twin. This paper introduces an innovative methodology for verifying neural network-based digital twin models, in a black-box fashion, by integrating model checking into the process. The latter relies on defining and applying system-level contracts that capture the system's requirements, to verify the behavior of digital twin models, implemented in Simulink. We develop an automated solution that simulates the digital twin model for certain inputs, and feeds the predicted outputs together with the inputs to the contract model described as a network of timed automata in the UPPAAL model checker. The latter verifies whether the predicted outputs fulfill the specified contracts. This approach allows us to identify scenarios where the digital twin's behavior fails to meet the contracts, without requiring the digital twin's design technicalities. We apply our method to a boiler system case study for which we identify prediction errors via contract verification. Our work demonstrates the effectiveness of integrating model checking with digital twin models for continuous improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10993v1</guid>
      <category>cs.SE</category>
      <category>cs.FL</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Naeem, Cristina Seceleanu</dc:creator>
    </item>
    <item>
      <title>Improving Software Team Communication Through Social Interventions in Project Management Tools</title>
      <link>https://arxiv.org/abs/2506.10994</link>
      <description>arXiv:2506.10994v1 Announce Type: new 
Abstract: Productive software engineering teams require effective communication and balanced contributions between team members. However, teams are often ineffective at these skills, which is detrimental to project success. Project-based university courses are an opportunity for students to practise these skills, but we have yet to establish how we can guide students towards improving their communication and coordination. We aim to develop project management tool features, informed by social network analysis, that nudge students in software engineering group projects towards beneficial behaviours. To do this, we will first evaluate the suitability of social network analysis techniques for identifying areas of improvement in teams' communication. Then, we will develop features in a project management tool that aid students in identifying and addressing these areas of improvement, and evaluate them in the context of a software engineering group project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10994v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>April Clarke</dc:creator>
    </item>
    <item>
      <title>Evaluating Small-Scale Code Models for Code Clone Detection</title>
      <link>https://arxiv.org/abs/2506.10995</link>
      <description>arXiv:2506.10995v1 Announce Type: new 
Abstract: Detecting code clones is relevant to software maintenance and code refactoring. This challenge still presents unresolved cases, mainly when structural similarity does not reflect functional equivalence, though recent code models show promise. Therefore, this research aims to systematically measure the performance of several newly introduced small code models in classifying code pairs as clones or non-clones. The evaluation is based on five datasets: BigCloneBench, CodeJam, Karnalim, POJ104, and PoolC, as well as six code models: CodeBERT, GraphCodeBERT, Salesforce T5, UniXCoder, PLBART, and Polycoder. Most models performed well across standard metrics, including accuracy, precision, recall, and F1-score. However, a marginal fraction of clones remains challenging to detect, especially when the code looks similar but performs different operations. The source code that illustrates our approach is available at: https://github.com/jorge-martinez-gil/small-code-models</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10995v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jorge Martinez-Gil</dc:creator>
    </item>
    <item>
      <title>Evaluating LLMs for Visualization Tasks</title>
      <link>https://arxiv.org/abs/2506.10996</link>
      <description>arXiv:2506.10996v1 Announce Type: new 
Abstract: Information Visualization has been utilized to gain insights from complex data. In recent times, Large Language Models (LLMs) have performed very well in many tasks. In this paper, we showcase the capabilities of different popular LLMs to generate code for visualization based on simple prompts. We also analyze the power of LLMs to understand some common visualizations by answering simple questions. Our study shows that LLMs could generate code for some visualizations as well as answer questions about them. However, LLMs also have several limitations. We believe that our insights can be used to improve both LLMs and Information Visualization systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10996v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.5220/0013079600003912</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 20th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISIGRAPP 2025)</arxiv:journal_reference>
      <dc:creator>Saadiq Rauf Khan, Vinit Chandak, Sougata Mukherjea</dc:creator>
    </item>
    <item>
      <title>A Theory-driven Interpretation and Elaboration of Verification and Validation</title>
      <link>https://arxiv.org/abs/2506.10997</link>
      <description>arXiv:2506.10997v1 Announce Type: new 
Abstract: This paper presents a formal theory of verification and validation (V&amp;V) within systems engineering, grounded in the axiom that V&amp;V are fundamentally knowledge-building activities. Using dynamic epistemic modal logic, we develop precise definitions of verification and validation, articulating their roles in confirming and contextualizing knowledge about systems. The theory formalizes the interplay between epistemic states, evidence, and reasoning processes, allowing for the derivation of theorems that clarify the conceptual underpinnings of V&amp;V. By providing a formal foundation, this work addresses ambiguities in traditional V&amp;V practices, offering a structured framework to enhance precision and consistency in systems engineering methodologies. The insights gained have implications for both academic research and practical applications, fostering a deeper understanding of V&amp;V as critical components of engineering knowledge generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10997v1</guid>
      <category>cs.SE</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanumanthrao Kannan, Alejandro Salado</dc:creator>
    </item>
    <item>
      <title>Towards Automated Formal Verification of Backend Systems with LLMs</title>
      <link>https://arxiv.org/abs/2506.10998</link>
      <description>arXiv:2506.10998v1 Announce Type: new 
Abstract: Software testing plays a critical role in ensuring that systems behave as intended. However, existing automated testing approaches struggle to match the capabilities of human engineers due to key limitations such as test locality, lack of general reliability, and business logic blindness. In this work, we propose a novel framework that leverages functional programming and type systems to translate Scala backend code into formal Lean representations. Our pipeline automatically generates theorems that specify the intended behavior of APIs and database operations, and uses LLM-based provers to verify them. When a theorem is proved, the corresponding logic is guaranteed to be correct and no further testing is needed. If the negation of a theorem is proved instead, it confirms a bug. In cases where neither can be proved, human intervention is required. We evaluate our method on realistic backend systems and find that it can formally verify over 50% of the test requirements, which suggests that half of a testing engineer's workload can be automated. Additionally, with an average cost of only $2.19 per API, LLM-based verification is significantly more cost-effective than manual testing and can be scaled easily through parallel execution. Our results indicate a promising direction for scalable, AI-powered software testing, with the potential to greatly improve engineering productivity as models continue to advance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10998v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kangping Xu, Yifan Luo, Yang Yuan, Andrew Chi-Chih Yao</dc:creator>
    </item>
    <item>
      <title>Automated Validation of COBOL to Java Transformation</title>
      <link>https://arxiv.org/abs/2506.10999</link>
      <description>arXiv:2506.10999v1 Announce Type: new 
Abstract: Recent advances in Large Language Model (LLM) based Generative AI techniques have made it feasible to translate enterpriselevel code from legacy languages such as COBOL to modern languages such as Java or Python. While the results of LLM-based automatic transformation are encouraging, the resulting code cannot be trusted to correctly translate the original code. We propose a framework and a tool to help validate the equivalence of COBOL and translated Java. The results can also help repair the code if there are some issues and provide feedback to the AI model to improve. We have developed a symbolic-execution-based test generation to automatically generate unit tests for the source COBOL programs which also mocks the external resource calls. We generate equivalent JUnit test cases with equivalent mocking as COBOL and run them to check semantic equivalence between original and translated programs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10999v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>ASE 2024</arxiv:journal_reference>
      <dc:creator>Atul Kumar, Diptikalyan Saha, Toshikai Yasue, Kohichi Ono, Saravanan Krishnan, Sandeep Hans, Fumiko Satoh, Gerald Mitchell, Sachin Kumar</dc:creator>
    </item>
    <item>
      <title>Ever-Improving Test Suite by Leveraging Large Language Models</title>
      <link>https://arxiv.org/abs/2506.11000</link>
      <description>arXiv:2506.11000v1 Announce Type: new 
Abstract: Augmenting test suites with test cases that reflect the actual usage of the software system is extremely important to sustain the quality of long lasting software systems. In this paper, we propose E-Test, an approach that incrementally augments a test suite with test cases that exercise behaviors that emerge in production and that are not been tested yet. E-Test leverages Large Language Models to identify already-tested, not-yet-tested, and error-prone unit execution scenarios, and augment the test suite accordingly. Our experimental evaluation shows that E-Test outperforms the main state-of-the-art approaches to identify inadequately tested behaviors and optimize test suites.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11000v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3696630.3728614</arxiv:DOI>
      <dc:creator>Ketai Qiu</dc:creator>
    </item>
    <item>
      <title>Rethinking Technological Readiness in the Era of AI Uncertainty</title>
      <link>https://arxiv.org/abs/2506.11001</link>
      <description>arXiv:2506.11001v1 Announce Type: new 
Abstract: Artificial intelligence (AI) is poised to revolutionize military combat systems, but ensuring these AI-enabled capabilities are truly mission-ready presents new challenges. We argue that current technology readiness assessments fail to capture critical AI-specific factors, leading to potential risks in deployment. We propose a new AI Readiness Framework to evaluate the maturity and trustworthiness of AI components in military systems. The central thesis is that a tailored framework - analogous to traditional Technology Readiness Levels (TRL) but expanded for AI - can better gauge an AI system's reliability, safety, and suitability for combat use. Using current data evaluation tools and testing practices, we demonstrate the framework's feasibility for near-term implementation. This structured approach provides military decision-makers with clearer insight into whether an AI-enabled system has met the necessary standards of performance, transparency, and human integration to be deployed with confidence, thus advancing the field of defense technology management and risk assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11001v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S. Tucker Browne, Mark M. Bailey</dc:creator>
    </item>
    <item>
      <title>Notes On Writing Effective Empirical Software Engineering Papers: An Opinionated Primer</title>
      <link>https://arxiv.org/abs/2506.11002</link>
      <description>arXiv:2506.11002v1 Announce Type: new 
Abstract: While mastered by some, good scientific writing practices within Empirical Software Engineering (ESE) research appear to be seldom discussed and documented. Despite this, these practices are implicit or even explicit evaluation criteria of typical software engineering conferences and journals. In this pragmatic, educational-first document, we want to provide guidance to those who may feel overwhelmed or confused by writing ESE papers, but also those more experienced who still might find an opinionated collection of writing advice useful. The primary audience we had in mind for this paper were our own BSc, MSc, and PhD students, but also students of others. Our documented advice therefore reflects a subjective and personal vision of writing ESE papers. By no means do we claim to be fully objective, generalizable, or representative of the whole discipline. With that being said, writing papers in this way has worked pretty well for us so far. We hope that this guide can at least partially do the same for others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11002v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roberto Verdecchia, Justus Bogner</dc:creator>
    </item>
    <item>
      <title>EmbedAgent: Benchmarking Large Language Models in Embedded System Development</title>
      <link>https://arxiv.org/abs/2506.11003</link>
      <description>arXiv:2506.11003v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown promise in various tasks, yet few benchmarks assess their capabilities in embedded system development.In this paper, we introduce EmbedAgent, a paradigm designed to simulate real-world roles in embedded system development, such as Embedded System Programmer, Architect, and Integrator. This paradigm enables LLMs to be tested in tasks that bridge the gap between digital and physical systems, allowing for a more comprehensive assessment of their capabilities. To evaluate LLMs on these tasks, we propose Embedbench, the first comprehensive benchmark for embedded system programming, circuit design, and cross-platform migration.Embedbench consists of 126 cases, covering 9 electronic components across 3 hardware platforms. Through extensive experiments on 10 mainstream LLMs, we uncover several key findings. Surprisingly, despite the simplicity of the cases, DeepSeek-R1 achieves only a 55.6% pass@1 rate when provided with schematic information, and 50.0% when tasked with generating the schematics itself. In the cross-platform migration tasks, LLMs show relatively strong performance with MicroPython on the Raspberry Pi Pico (with the top model achieving 73.8% pass@1), but perform poorly on ESP-IDF, where the best model reaches only 29.4% pass@1.Interestingly, we observe that general-purpose chat LLMs like DeepSeek-V3 often fail to utilize relevant pre-trained knowledge in this domain, while reasoning LLMs tend to overthink and overlook efficient knowledge during pretraining. Based on these insights, we propose two strategies: retrieval augmented generation and compiler feedback-to enhance LLM performance. These strategies result in significant improvements, with Deepseek-R1 reaching a 65.1% pass@1 with correct schematics, and 53.1% without. Additionally, the accuracy of the Arduino to ESP32 migration task improves from 21.4% to 27.8%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11003v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiyang Xu, Jialun Cao, Mingyuan Wu, Wenliang Zhong, Yaojie Lu, Ben He, Xianpei Han, Shing-Chi Cheung, Le Sun</dc:creator>
    </item>
    <item>
      <title>Automated Extraction and Analysis of Developer's Rationale in Open Source Software</title>
      <link>https://arxiv.org/abs/2506.11005</link>
      <description>arXiv:2506.11005v1 Announce Type: new 
Abstract: Contributors to open source software must deeply understand a project's history to make coherent decisions which do not conflict with past reasoning. However, inspecting all related changes to a proposed contribution requires intensive manual effort, and previous research has not yet produced an automated mechanism to expose and analyze these conflicts. In this article, we propose such an automated approach for rationale analyses, based on an instantiation of Kantara, an existing high-level rationale extraction and management architecture. Our implementation leverages pre-trained models and Large Language Models, and includes structure-based mechanisms to detect reasoning conflicts and problems which could cause design erosion in a project over time. We show the feasibility of our extraction and analysis approach using the OOM-Killer module of the Linux Kernel project, and investigate the approach's generalization to five other highly active open source projects. The results confirm that our automated approach can support rationale analyses with reasonable performance, by finding interesting relationships and to detect potential conflicts and reasoning problems. We also show the effectiveness of the automated extraction of decision and rationale sentences and the prospects for generalizing this to other open source projects. This automated approach could therefore be used by open source software developers to proactively address hidden issues and to ensure that new changes do not conflict with past decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11005v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Mouna Dhaouadi, Bentley Oakes, Michalis Famelis</dc:creator>
    </item>
    <item>
      <title>Test code generation at Ericsson using Program Analysis Augmented Fine Tuned LLMs</title>
      <link>https://arxiv.org/abs/2506.11006</link>
      <description>arXiv:2506.11006v1 Announce Type: new 
Abstract: We describe test code generation using Large Language Models (LLMs) in Ericsson. Our input is a test step in natural language (English) and our output is code (Java) which accomplishes the test step. We describe how straight forward prompting does not suffice and results in LLM assuming functions and signatures which are not present in the code repository. We then show how we alleviate the problem by a combination of Retrieval Augmented Generation (RAG) along with prompt engineering that expanded the simple prompt with additional contextual information using static program analysis. We then describe further improvements that we obtained by fine-tuning the underlying LLM. The fine tuning is done based on a custom designed prompt template which has pre-dependent classes, their public methods as well two exemplar outputs obtained from RAG. Our results establish that our fine tuned models help improve the correspondence or conformity with the original developer written test code as measured by the traditional metrics of F1-score based on the methods used in the generated code. Fine tuning of a 8x7b Mixture of Experts (MoE) model leads to an average improvement of 8\% over the base model and is comparable to the scores on a much larger 8x22b MoE model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11006v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sai Krishna, Balvinder Singh, Sujoy Roychowdhury, Giriprasad Sridhara, Sourav Mazumdar, Magnus Sandelin, Dimitris Rentas, Maciej Nalepa, Karol Sawicki, Jakub Gajda</dc:creator>
    </item>
    <item>
      <title>Impact of Comments on LLM Comprehension of Legacy Code</title>
      <link>https://arxiv.org/abs/2506.11007</link>
      <description>arXiv:2506.11007v1 Announce Type: new 
Abstract: Large language models (LLMs) have been increasingly integrated into software engineering and maintenance tasks due to their high performance with software engineering tasks and robust understanding of modern programming languages. However, the ability of LLMs to comprehend code written with legacy languages remains a research gap challenged by real-world legacy systems lacking or containing inaccurate documentation that may impact LLM comprehension. To assess LLM comprehension of legacy languages, there is a need for objective LLM evaluation. In order to objectively measure LLM comprehension of legacy languages, we need an efficient, quantitative evaluation method. We leverage multiple-choice question answering (MCQA), an emerging LLM evaluation methodology, to evaluate LLM comprehension of legacy code and the impact of comment prevalence and inaccurate comments. In this work, we present preliminary findings on the impact of documentation on LLM comprehension of legacy code and outline strategic objectives for future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11007v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rock Sabetto, Emily Escamilla, Devesh Agarwal, Sujay Kandwal, Justin F. Brunelle, Scott Rosen, Nitin Naik, Samruddhi Thaker, Eric O. Scott, Jacob Zimmer, Amit Madan, Arun Sridharan, Doug Wendt, Michael Doyle, Christopher Glasz, Jasper Phillips, William Macke, Colin Diggs, Michael Bartholf, Zachary Robin, Paul Ursino</dc:creator>
    </item>
    <item>
      <title>Encoding Software For Perpetuity: A Compact Representation Of Apollo 11 Guidance Code</title>
      <link>https://arxiv.org/abs/2506.11008</link>
      <description>arXiv:2506.11008v1 Announce Type: new 
Abstract: This brief note presents a novel method for encoding historic Apollo 11 Lunar Module guidance computer code into a single, compact Quick Response Code (QR code) format, creating an accessible digital artifact for transmission and archival purposes. By applying tokenization, selective content preservation, and minimal HTML/JavaScript techniques, we successfully compressed key components of the original Assembly Language Code (AGC) into a shareable, preservable, and scannable 3 kilobyte (KB) image. We evaluate multiple compression strategies and their tradeoffs in terms of size, readability, and historical significance. This method addresses the challenge of making historically significant software artifacts available through modern mobile devices without requiring specialized hardware or internet connectivity. While numerous digital preservation methods exist for historic software, this approach balances accessibility with historical significance, offering a complementary method to traditional archival techniques. This work contributes to the broader field of computing heritage preservation by demonstrating how landmark software can be made accessible instantly through contemporary mobile technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11008v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Noever</dc:creator>
    </item>
    <item>
      <title>Human-In-The-Loop Software Development Agents: Challenges and Future Directions</title>
      <link>https://arxiv.org/abs/2506.11009</link>
      <description>arXiv:2506.11009v1 Announce Type: new 
Abstract: Multi-agent LLM-driven systems for software development are rapidly gaining traction, offering new opportunities to enhance productivity. At Atlassian, we deployed Human-in-the-Loop Software Development Agents to resolve Jira work items and evaluated the generated code quality using functional correctness testing and GPT-based similarity scoring. This paper highlights two major challenges: the high computational costs of unit testing and the variability in LLM-based evaluations. We also propose future research directions to improve evaluation frameworks for Human-In-The-Loop software development tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11009v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jirat Pasuksmit, Wannita Takerngsaksiri, Patanamon Thongtanunam, Chakkrit Tantithamthavorn, Ruixiong Zhang, Shiyan Wang, Fan Jiang, Jing Li, Evan Cook, Kun Chen, Ming Wu</dc:creator>
    </item>
    <item>
      <title>Enhancing Inventory Management with Progressive Web Applications (PWAs): A Scalable Solution for Small and Large Enterprises</title>
      <link>https://arxiv.org/abs/2506.11011</link>
      <description>arXiv:2506.11011v1 Announce Type: new 
Abstract: Efficient inventory management is crucial for both small and large enterprises to optimize operational workflows and reduce overhead costs. This paper explores the development and implementation of a Progressive Web Application (PWA) designed to enhance the inventory management experience. The application integrates key functionalities such as barcode and QR code scanning, geolocation-based warehouse identification, and cross-device accessibility. By leveraging PWA technology, the solution ensures offline capabilities, responsive user experience, and seamless adaptability across various platforms. The study discusses the challenges and benefits of implementing PWA in inventory management systems, including its limitations in performance compared to native applications. Insights from the development process provide a roadmap for future developers looking to integrate PWA technology into enterprise applications. This research contributes to the growing domain of web-based inventory solutions, offering a scalable and cost-effective alternative to traditional inventory management software.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11011v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhi Desai</dc:creator>
    </item>
    <item>
      <title>Toward a Brazilian Research Agenda in Quantum Software Engineering: A Systematic Mapping Study</title>
      <link>https://arxiv.org/abs/2506.11013</link>
      <description>arXiv:2506.11013v1 Announce Type: new 
Abstract: Context: Quantum Software Engineering (QSE) has emerged as a key field to support the development of reliable, maintainable, and scalable quantum applications, bridging advances in quantum computing with established practices in software engineering. Problem: Despite its growth, the field still suffers from fragmented knowledge, with a lack of standardized methodologies, tools, and guidelines tailored to the unique features of the quantum paradigm. Additionally, countries like Brazil have had limited participation in the development of this emerging domain. Objective: This study aims to map the state of the art in QSE by identifying current research trends, recurring contributions, and existing gaps that can guide future investigations and strategic initiatives. Methodology: A systematic mapping study was conducted analyzing selected publications based on inclusion and exclusion criteria. Articles were categorized by study type, research type, and alignment with the SWEBOK knowledge areas. Results: Most of the reviewed studies are primary research articles written in English, with a strong focus on Software Engineering Models and Methods, Software Architecture, and Software Testing. Conceptual proposals and technical solutions predominate, while empirical validations remain limited. Conclusions: Findings confirm that QSE is a promising but still maturing field. The standardization of practices, expansion of empirical studies, and inclusion of researchers from developing countries are crucial for advancing the discipline. Additionally, Brazilian contributions are still scarce, highlighting the urgent need to establish a national research agenda. As a main contribution, this study proposes a Brazilian Research Agenda in QSE, outlining priority areas and opportunities to foster a local scientific community and accelerate progress in this emerging field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11013v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Filipe Fernandes, Cl\'audia Werner</dc:creator>
    </item>
    <item>
      <title>MultiMind: A Plug-in for the Implementation of Development Tasks Aided by AI Assistants</title>
      <link>https://arxiv.org/abs/2506.11014</link>
      <description>arXiv:2506.11014v1 Announce Type: new 
Abstract: The integration of AI assistants into software development workflows is rapidly evolving, shifting from automation-assisted tasks to collaborative interactions between developers and AI. Large Language Models (LLMs) have demonstrated their effectiveness in several development activities, including code completion, test case generation, and documentation production. However, embedding AI-assisted tasks within Integrated Development Environments (IDEs) presents significant challenges. It requires designing mechanisms to invoke AI assistants at the appropriate time, coordinate interactions with multiple assistants, process the generated outputs, and present feedback in a way that seamlessly integrates with the development workflow. To address these issues, we introduce MultiMind, a Visual Studio Code plug-in that streamlines the creation of AI-assisted development tasks. MultiMind provides a modular and extensible framework, enabling developers to cost-effectively implement and experiment with new AI-powered interactions without the need for complex IDE customizations. MultiMind has been tested in two use cases: one for the automatic generation of code comments and the other about the definition of AI-powered chat.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11014v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3696630.3730564</arxiv:DOI>
      <dc:creator>Benedetta Donato, Leonardo Mariani, Daniela Micucci, Oliviero Riganelli, Marco Somaschini</dc:creator>
    </item>
    <item>
      <title>ZjsComponent: A Pragmatic Approach to Modular, Reusable UI Fragments for Web Development</title>
      <link>https://arxiv.org/abs/2506.11016</link>
      <description>arXiv:2506.11016v1 Announce Type: new 
Abstract: In this paper, I present ZjsComponent, a lightweight and framework-agnostic web component designed for creating modular, reusable UI elements with minimal developer overhead. ZjsComponent is an example implementation of an approach to creating components and object instances that can be used purely from HTML. Unlike traditional approaches to components, the approach implemented by ZjsComponent does not require build-steps, transpiling, pre-compilation, any specific ecosystem or any other dependency. All that is required is that the browser can load and execute Javascript as needed by Web Components. ZjsComponent allows dynamic loading and isolation of HTML+JS fragments, offering developers a simple way to build reusable interfaces with ease. This approach is dependency-free, provides significant DOM and code isolation, and supports simple lifecycle hooks as well as traditional methods expected of an instance of a class.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11016v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lelanthran Manickum</dc:creator>
    </item>
    <item>
      <title>Formation of requirements traceability in the process of information systems design</title>
      <link>https://arxiv.org/abs/2506.11018</link>
      <description>arXiv:2506.11018v1 Announce Type: new 
Abstract: The traceability of requirements in the information system design process is considered an essential property of the project, one of its quality characteristics. The point here is that traceability provides the methods of validation and verification of software systems, and that the system model based on requirements traceability reduces the system's dependence on developers and, in general, makes it as straightforward as possible. One of the challenges of the traceability process, dubbed "The grand challenge of traceability" among traceability researchers, is its integration into the design process. In this paper, to achieve this goal, we propose the application of the Adaptive Clustering Method (ACM) of Information Systems developed by the author, which is based on the idea of a seamless system architecture that provides explicit interconnection of project artifacts of different levels of abstraction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11018v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Grigory Tsiperman</dc:creator>
    </item>
    <item>
      <title>Mind the Metrics: Patterns for Telemetry-Aware In-IDE AI Application Development using the Model Context Protocol (MCP)</title>
      <link>https://arxiv.org/abs/2506.11019</link>
      <description>arXiv:2506.11019v1 Announce Type: new 
Abstract: AI development environments are evolving into observability first platforms that integrate real time telemetry, prompt traces, and evaluation feedback into the developer workflow. This paper introduces telemetry aware integrated development environments (IDEs) enabled by the Model Context Protocol (MCP), a system that connects IDEs with prompt metrics, trace logs, and versioned control for real time refinement. We present design patterns for local prompt iteration, CI based optimization, and autonomous agents that adapt behavior using telemetry. Rather than focusing on a single algorithm, we describe an architecture that supports integration with frameworks like DSPy, PromptWizard, and Prompts as Programs. We demonstrate this through Opik, an open source MCP server for LLM telemetry, and position our approach within the emerging LLMOps ecosystem. This work lays a foundation for future research on prompt optimization, IDE agent tooling, and empirical benchmarking in telemetry rich AI development workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11019v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Koc, Jacques Verre, Douglas Blank, Abigail Morgan</dc:creator>
    </item>
    <item>
      <title>Extracting Knowledge Graphs from User Stories using LangChain</title>
      <link>https://arxiv.org/abs/2506.11020</link>
      <description>arXiv:2506.11020v1 Announce Type: new 
Abstract: This thesis introduces a novel methodology for the automated generation of knowledge graphs from user stories by leveraging the advanced capabilities of Large Language Models. Utilizing the LangChain framework as a basis, the User Story Graph Transformer module was developed to extract nodes and relationships from user stories using an LLM to construct accurate knowledge graphs.This innovative technique was implemented in a script to fully automate the knowledge graph extraction process. Additionally, the evaluation was automated through a dedicated evaluation script, utilizing an annotated dataset for assessment. By enhancing the visualization and understanding of user requirements and domain concepts, this method fosters better alignment between software functionalities and user expectations, ultimately contributing to more effective and user-centric software development processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11020v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Thayn\'a Camargo da Silva</dc:creator>
    </item>
    <item>
      <title>Eliminating Hallucination-Induced Errors in LLM Code Generation with Functional Clustering</title>
      <link>https://arxiv.org/abs/2506.11021</link>
      <description>arXiv:2506.11021v1 Announce Type: new 
Abstract: Modern code-generation LLMs can already solve a large fraction of programming problems, yet they still hallucinate subtle bugs that make their outputs unsafe for autonomous deployment. We present functional clustering, a black-box wrapper that eliminates nearly all hallucination-induced errors while providing a tunable confidence score. The wrapper samples many candidate programs, executes each on a self-generated test suite, and clusters candidates whose I/O behavior is identical; the empirical mass of the largest cluster serves as an exact confidence estimate. A single scalar threshold on this estimate lets users trade coverage for reliability with exponential guarantees. On LiveCodeBench our verifier preserves baseline pass@1 on solvable tasks yet slashes the error rate of returned answers from ~65% to 2%, and drives it to 0% at a conservative threshold while still answering 15.6% of prompts. Manual audits show that the few residual mistakes stem from prompt misinterpretation, not random generation noise, narrowing future work to specification clarity. Because the method requires only sampling and sandbox execution, it applies unchanged to closed-source APIs and future models, offering a practical path toward dependable, autonomous code generation. Our code is available on Github (https://github.com/20ChaituR/functional-clustering).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11021v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Chaitanya Ravuri, Saman Amarasinghe</dc:creator>
    </item>
    <item>
      <title>Security Degradation in Iterative AI Code Generation -- A Systematic Analysis of the Paradox</title>
      <link>https://arxiv.org/abs/2506.11022</link>
      <description>arXiv:2506.11022v1 Announce Type: new 
Abstract: The rapid adoption of Large Language Models(LLMs) for code generation has transformed software development, yet little attention has been given to how security vulnerabilities evolve through iterative LLM feedback. This paper analyzes security degradation in AI-generated code through a controlled experiment with 400 code samples across 40 rounds of "improvements" using four distinct prompting strategies. Our findings show a 37.6% increase in critical vulnerabilities after just five iterations, with distinct vulnerability patterns emerging across different prompting approaches. This evidence challenges the assumption that iterative LLM refinement improves code security and highlights the essential role of human expertise in the loop. We propose practical guidelines for developers to mitigate these risks, emphasizing the need for robust human validation between LLM iterations to prevent the paradoxical introduction of new security issues during supposedly beneficial code "improvements".</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11022v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shivani Shukla, Himanshu Joshi, Romilla Syed</dc:creator>
    </item>
    <item>
      <title>Software Security Mapping Framework: Operationalization of Security Requirements</title>
      <link>https://arxiv.org/abs/2506.11051</link>
      <description>arXiv:2506.11051v1 Announce Type: new 
Abstract: The escalating complexity of modern software development environments has heightened concerns around supply chain security. However, existing frameworks often fall short in translating abstract security principles into concrete, actionable practices. This paper introduces the Software Security Mapping Framework, a structured solution designed to operationalize security requirements across hierarchical levels -- from high-level regulatory standards (e.g., ISM, Australia cybersecurity standard published by the Australian Signals Directorate), through mid-level frameworks (e.g., NIST SSDF, the U.S. Secure Software Development Framework), to fine-grained technical activities (e.g., SLSA, a software supply chain security framework). Developed through collaborative research with academic experts and industry practitioners, the framework systematically maps 131 refined security requirements to over 400 actionable operational steps spanning the software development lifecycle. It is grounded in four core security goals: Secure Software Environment, Secure Software Development, Software Traceability, and Vulnerability Management. Our approach leverages the KAOS goal modeling methodology to establish traceable linkages between strategic goals and tactical operations, enhancing clarity, accountability, and practical implementation. To facilitate adoption, we provide a web-based navigation tool for interactive exploration of the framework. A real-world case study based on the Log4j vulnerability illustrates the framework's utility by generating a tailored checklist aligned with industry best practices. Additionally, we offer a structured, machine-readable OSCAL Catalog Model of the Software Security Mapping Framework, enabling organizations to automate implementation, streamline compliance processes, and respond effectively to evolving security risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11051v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sung Une Lee, Liming Dong, Zhenchang Xing, Muhammad Ejaz Ahmed, Stefan Avgoustakis</dc:creator>
    </item>
    <item>
      <title>Refactoring Codebases through Library Design</title>
      <link>https://arxiv.org/abs/2506.11058</link>
      <description>arXiv:2506.11058v1 Announce Type: new 
Abstract: Maintainable and general software allows developers to build robust applications efficiently, yet achieving these qualities often requires refactoring specialized solutions into reusable components. This challenge becomes particularly relevant as code agents become increasingly accurate at solving isolated programming problems. We investigate code agents' capacity to refactor code in ways supporting growth and reusability. We present both a method and a benchmark for refactoring: Librarian, a sample-and-rerank method for generating reusable libraries, and Minicode, a benchmark where code agents must minimize and refactor multiple independent solutions into a joint library. Compared to state-of-the-art code agents, Librarian achieves strong results on both compression and correctness on Minicode, obtaining compression rates 1.6-2x better than coding agents while also improving correctness. We open-source our code and benchmark at https://code-refactor.github.io/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11058v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziga Kovacic, Celine Lee, Justin Chiu, Wenting Zhao, Kevin Ellis</dc:creator>
    </item>
    <item>
      <title>CodeMirage: A Multi-Lingual Benchmark for Detecting AI-Generated and Paraphrased Source Code from Production-Level LLMs</title>
      <link>https://arxiv.org/abs/2506.11059</link>
      <description>arXiv:2506.11059v1 Announce Type: new 
Abstract: Large language models (LLMs) have become integral to modern software development, producing vast amounts of AI-generated source code. While these models boost programming productivity, their misuse introduces critical risks, including code plagiarism, license violations, and the propagation of insecure programs. As a result, robust detection of AI-generated code is essential. To support the development of such detectors, a comprehensive benchmark that reflects real-world conditions is crucial. However, existing benchmarks fall short -- most cover only a limited set of programming languages and rely on less capable generative models. In this paper, we present CodeMirage, a comprehensive benchmark that addresses these limitations through three major advancements: (1) it spans ten widely used programming languages, (2) includes both original and paraphrased code samples, and (3) incorporates outputs from ten state-of-the-art production-level LLMs, including both reasoning and non-reasoning models from six major providers. Using CodeMirage, we evaluate ten representative detectors across four methodological paradigms under four realistic evaluation configurations, reporting results using three complementary metrics. Our analysis reveals nine key findings that uncover the strengths and weaknesses of current detectors, and identify critical challenges for future work. We believe CodeMirage offers a rigorous and practical testbed to advance the development of robust and generalizable AI-generated code detectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11059v1</guid>
      <category>cs.SE</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hanxi Guo, Siyuan Cheng, Kaiyuan Zhang, Guangyu Shen, Xiangyu Zhang</dc:creator>
    </item>
    <item>
      <title>Code Researcher: Deep Research Agent for Large Systems Code and Commit History</title>
      <link>https://arxiv.org/abs/2506.11060</link>
      <description>arXiv:2506.11060v1 Announce Type: new 
Abstract: Large Language Model (LLM)-based coding agents have shown promising results on coding benchmarks, but their effectiveness on systems code remains underexplored. Due to the size and complexities of systems code, making changes to a systems codebase is a daunting task, even for humans. It requires researching about many pieces of context, derived from the large codebase and its massive commit history, before making changes. Inspired by the recent progress on deep research agents, we design the first deep research agent for code, called Code Researcher, and apply it to the problem of generating patches for mitigating crashes reported in systems code. Code Researcher performs multi-step reasoning about semantics, patterns, and commit history of code to gather sufficient context. The context is stored in a structured memory which is used for synthesizing a patch. We evaluate Code Researcher on kBenchSyz, a benchmark of Linux kernel crashes, and show that it significantly outperforms strong baselines, achieving a crash-resolution rate of 58%, compared to 37.5% by SWE-agent. On an average, Code Researcher explores 10 files in each trajectory whereas SWE-agent explores only 1.33 files, highlighting Code Researcher's ability to deeply explore the codebase. Through another experiment on an open-source multimedia software, we show the generalizability of Code Researcher. Our experiments highlight the importance of global context gathering and multi-faceted reasoning for large codebases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11060v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ramneet Singh, Sathvik Joel, Abhav Mehrotra, Nalin Wadhwa, Ramakrishna B Bairi, Aditya Kanade, Nagarajan Natarajan</dc:creator>
    </item>
    <item>
      <title>CoQuIR: A Comprehensive Benchmark for Code Quality-Aware Information Retrieval</title>
      <link>https://arxiv.org/abs/2506.11066</link>
      <description>arXiv:2506.11066v1 Announce Type: new 
Abstract: Code retrieval is essential in modern software development, as it boosts code reuse and accelerates debugging. However, current benchmarks primarily emphasize functional relevance while neglecting critical dimensions of software quality. Motivated by this gap, we introduce CoQuIR, the first large-scale, multilingual benchmark specifically designed to evaluate quality-aware code retrieval across four key dimensions: correctness, efficiency, security, and maintainability. CoQuIR provides fine-grained quality annotations for 42,725 queries and 134,907 code snippets in 11 programming languages, and is accompanied by two quality-centric evaluation metrics: Pairwise Preference Accuracy and Margin-based Ranking Score. Using CoQuIR, we benchmark 23 retrieval models, covering both open-source and proprietary systems, and find that even top-performing models frequently fail to distinguish buggy or insecure code from their more robust counterparts. Furthermore, we conduct preliminary investigations into training methods that explicitly encourage retrievers to recognize code quality. Using synthetic datasets, we demonstrate promising improvements in quality-aware metrics across various models, without sacrificing semantic relevance. Downstream code generation experiments further validate the effectiveness of our approach. Overall, our work highlights the importance of integrating quality signals into code retrieval systems, laying the groundwork for more trustworthy and robust software development tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11066v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiahui Geng, Fengyu Cai, Shaobo Cui, Qing Li, Liangwei Chen, Chenyang Lyu, Haonan Li, Derui Zhu, Walter Pretschner, Heinz Koeppl, Fakhri Karray</dc:creator>
    </item>
    <item>
      <title>DCE-LLM: Dead Code Elimination with Large Language Models</title>
      <link>https://arxiv.org/abs/2506.11076</link>
      <description>arXiv:2506.11076v1 Announce Type: new 
Abstract: Dead code introduces several challenges in software development, such as increased binary size and maintenance difficulties. It can also obscure logical errors and be exploited for obfuscation in malware. For LLM-based code-related tasks, dead code introduces vulnerabilities that can mislead these models, raising security concerns. Although modern compilers and IDEs offer dead code elimination, sophisticated patterns can bypass these tools. A universal approach that includes classification, location, explanation, and correction is needed, yet current tools often require significant manual effort. We present DCE-LLM, a framework for automated dead code elimination using a small CodeBERT model with an attribution-based line selector to efficiently locate suspect code. LLMs then generate judgments and explanations, fine-tuned on a large-scale, annotated dead code dataset to provide detailed explanations and patches. DCE-LLM outperforms existing tools, with advanced unreachability detection, automated correction, and support for multiple programming languages. Experimental results show DCE-LLM achieves over 94% F1 scores for unused and unreachable code, significantly surpassing GPT-4o by 30%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11076v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minyu Chen, Guoqiang Li, Ling-I Wu, Ruibang Liu</dc:creator>
    </item>
    <item>
      <title>Research and Analysis of Employers' Opinion on the Necessary Skills that Students in the Field of Web Programming Should Possess</title>
      <link>https://arxiv.org/abs/2506.11084</link>
      <description>arXiv:2506.11084v1 Announce Type: new 
Abstract: In the era of artificial intelligence (AI) and chatbots, based on large language models that can generate programming code in any language, write texts and summarize information, it is obvious that the requirements of employers for graduating students have already changed. The modern IT world offers significant automation of programming through software frameworks and a huge set of third-party libraries and application programming interfaces (APIs). All these tools provide most of the necessary functionality out of the box (already implemented), and quite naturally the question arises as to what is more useful for students - to teach how to use these ready-made tools or the basic principles of working and development of web applications from scratch. This paper analyzes the results of a survey conducted among IT employers, aimed to identify what, in their opinion, are the necessary technical skills that graduating students in the field of Web Programming should possess in order to join the company's work as quickly and effectively as possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11084v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the THIRD NATIONAL SCIENTIFIC-PRACTICAL CONFERENCE "DIGITAL TRANSFORMATION OF EDUCATION" - PROBLEMS AND SOLUTIONS 2025, pp.259-264, ISSN: 3033-0629, Academic Publishing House of the University of Ruse</arxiv:journal_reference>
      <dc:creator>Yordan Kalmukov</dc:creator>
    </item>
    <item>
      <title>LeanExplore: A search engine for Lean 4 declarations</title>
      <link>https://arxiv.org/abs/2506.11085</link>
      <description>arXiv:2506.11085v1 Announce Type: new 
Abstract: The expanding Lean 4 ecosystem poses challenges for navigating its vast libraries. This paper introduces LeanExplore, a search engine for Lean 4 declarations. LeanExplore enables users to semantically search for statements, both formally and informally, across select Lean 4 packages (including Batteries, Init, Lean, Mathlib, PhysLean, and Std). This search capability is powered by a hybrid ranking strategy, integrating scores from a multi-source semantic embedding model (capturing conceptual meaning from formal Lean code, docstrings, AI-generated informal translations, and declaration titles), BM25+ for keyword-based lexical relevance, and a PageRank-based score reflecting declaration importance and interconnectedness. The search engine is accessible via a dedicated website (https://www.leanexplore.com/) and a Python API (https://github.com/justincasher/lean-explore). Furthermore, the database can be downloaded, allowing users to self-host the service. LeanExplore integrates easily with LLMs via the model context protocol (MCP), enabling users to chat with an AI assistant about Lean declarations or utilize the search engine for building theorem-proving agents. This work details LeanExplore's architecture, data processing, functionalities, and its potential to enhance Lean 4 workflows and AI-driven mathematical research</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11085v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <category>cs.LO</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Justin Asher (Independent Researcher)</dc:creator>
    </item>
    <item>
      <title>Denoising Programming Knowledge Tracing with a Code Graph-based Tuning Adaptor</title>
      <link>https://arxiv.org/abs/2506.11107</link>
      <description>arXiv:2506.11107v1 Announce Type: new 
Abstract: Programming Knowledge Tracking (PKT) aims to dynamically diagnose learners' mastery levels of programming knowledge based on their coding activities, facilitating more effective and personalized programming education. However, current PKT studies primarily focus on the implicit relationship between code content and knowledge assessment, often overlooking two types of noise signals in long-term programming activities: unwanted signals from unrelated submissions and weak signals from minor modifications. This practical challenge significantly limits model performance and application. To address this issue, we propose Coda, a Code graph-based tuning adaptor designed to enhance existing PKT models by identifying and mitigating the impact of noise. Specifically, Coda first transforms the loose code sequences submitted by each learner into a compact code graph. By leveraging this code graph, unwanted signals can be identified from a semantic similarity perspective. We then apply a cluster-aware GCN to the code graph, which improves the discrimination of weak signals and enables their clustering for identification. Finally, a lightweight yet effective adaptor is incorporated into the PKT task through optimization with two noise feature-based constraints and a navigational regularization term, to correct knowledge states affected by noise. It is worth mentioning that the Coda framework is model-agnostic and can be adapted to most existing PKT solutions. Extensive experimental results on four real-world datasets demonstrate that Coda effectively performs the PKT task in the presence of noisy programming records, outperforming typical baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11107v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weibo Gao, Qi Liu, Rui Li, Yuze Zhao, Hao Wang, Linan Yre, Fangzhou Yao, Zheng Zhang</dc:creator>
    </item>
    <item>
      <title>From over-reliance to smart integration: using Large-Language Models as translators between specialized modeling and simulation tools</title>
      <link>https://arxiv.org/abs/2506.11141</link>
      <description>arXiv:2506.11141v1 Announce Type: new 
Abstract: Large Language Models (LLMs) offer transformative potential for Modeling &amp; Simulation (M&amp;S) through natural language interfaces that simplify workflows. However, over-reliance risks compromising quality due to ambiguities, logical shortcuts, and hallucinations. This paper advocates integrating LLMs as middleware or translators between specialized tools to mitigate complexity in M&amp;S tasks. Acting as translators, LLMs can enhance interoperability across multi-formalism, multi-semantics, and multi-paradigm systems. We address two key challenges: identifying appropriate languages and tools for modeling and simulation tasks, and developing efficient software architectures that integrate LLMs without performance bottlenecks. To this end, the paper explores LLM-mediated workflows, emphasizes structured tool integration, and recommends Low-Rank Adaptation-based architectures for efficient task-specific adaptations. This approach ensures LLMs complement rather than replace specialized tools, fostering high-quality, reliable M&amp;S processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11141v1</guid>
      <category>cs.SE</category>
      <category>cs.ET</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Philippe J. Giabbanelli, John Beverley, Istvan David, Andreas Tolk</dc:creator>
    </item>
    <item>
      <title>Mutual-Supervised Learning for Sequential-to-Parallel Code Translation</title>
      <link>https://arxiv.org/abs/2506.11153</link>
      <description>arXiv:2506.11153v1 Announce Type: new 
Abstract: The rise of GPU-based high-performance computing (HPC) has driven the widespread adoption of parallel programming models such as CUDA. Yet, the inherent complexity of parallel programming creates a demand for the automated sequential-to-parallel approaches. However, data scarcity poses a significant challenge for machine learning-based sequential-to-parallel code translation. Although recent back-translation methods show promise, they still fail to ensure functional equivalence in the translated code. In this paper, we propose a novel Mutual-Supervised Learning (MSL) framework for sequential-to-parallel code translation to address the functional equivalence issue. MSL consists of two models, a Translator and a Tester. Through an iterative loop consisting of Co-verify and Co-evolve steps, the Translator and the Tester mutually generate data for each other and improve collectively. The Tester generates unit tests to verify and filter functionally equivalent translated code, thereby evolving the Translator, while the Translator generates translated code as augmented input to evolve the Tester. Experimental results demonstrate that MuSL significantly enhances the performance of the base model: when applied to Qwen2.5-Coder, it not only improves Pass@1 by up to 28.91% and boosts Tester performance by 68.90%, but also outperforms the previous state-of-the-art method CodeRosetta by 1.56 and 6.92 in BLEU and CodeBLEU scores, while achieving performance comparable to DeepSeek-R1 and GPT-4.1. Our code is available at https://github.com/kcxain/musl.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11153v1</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Changxin Ke, Rui Zhang, Shuo Wang, Li Ding, Guangli Li, Yuanbo Wen, Shuoming Zhang, Ruiyuan Xu, Jin Qin, Jiaming Guo, Chenxi Wang, Ling Li, Qi Guo, Yunji Chen</dc:creator>
    </item>
    <item>
      <title>Model Discovery and Graph Simulation: A Lightweight Alternative to Chaos Engineering</title>
      <link>https://arxiv.org/abs/2506.11176</link>
      <description>arXiv:2506.11176v1 Announce Type: new 
Abstract: Microservice applications are prone to cascading failures because of dense inter-service dependencies. Ensuring resilience usually demands fault-injection experiments in production-like setups. We propose \textit{model discovery} -- an automated CI/CD step that extracts a live dependency graph from trace data -- and show that this lightweight representation is sufficient for accurate resilience prediction. Using the DeathStarBench Social Network, we build the graph, simulate failures via Monte-Carlo, and run matching chaos experiments on the real system. The graph model closely matches reality: with no replication, 16 trials yield an observed resilience of 0.186 versus a predicted 0.161; with replication, both observed and predicted values converge to 0.305 (mean absolute error \leq 0.0004). These results indicate that even a simple, automatically discovered graph can estimate microservice availability with high fidelity, offering rapid design-time insight without full-scale failure testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11176v1</guid>
      <category>cs.SE</category>
      <category>cs.DC</category>
      <category>cs.DM</category>
      <category>cs.ET</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Anatoly A. Krasnovsky, Alexander Zorkin</dc:creator>
    </item>
    <item>
      <title>Beyond Formal Semantics for Capabilities and Skills: Model Context Protocol in Manufacturing</title>
      <link>https://arxiv.org/abs/2506.11180</link>
      <description>arXiv:2506.11180v1 Announce Type: new 
Abstract: Explicit modeling of capabilities and skills -- whether based on ontologies, Asset Administration Shells, or other technologies -- requires considerable manual effort and often results in representations that are not easily accessible to Large Language Models (LLMs). In this work-in-progress paper, we present an alternative approach based on the recently introduced Model Context Protocol (MCP). MCP allows systems to expose functionality through a standardized interface that is directly consumable by LLM-based agents. We conduct a prototypical evaluation on a laboratory-scale manufacturing system, where resource functions are made available via MCP. A general-purpose LLM is then tasked with planning and executing a multi-step process, including constraint handling and the invocation of resource functions via MCP. The results indicate that such an approach can enable flexible industrial automation without relying on explicit semantic models. This work lays the basis for further exploration of external tool integration in LLM-driven production systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11180v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luis Miguel Vieira da Silva, Aljosha K\"ocher, Felix Gehlhoff</dc:creator>
    </item>
    <item>
      <title>LLM-as-a-Judge for Reference-less Automatic Code Validation and Refinement for Natural Language to Bash in IT Automation</title>
      <link>https://arxiv.org/abs/2506.11237</link>
      <description>arXiv:2506.11237v1 Announce Type: new 
Abstract: In an effort to automatically evaluate and select the best model and improve code quality for automatic incident remediation in IT Automation, it is crucial to verify if the generated code for remediation action is syntactically and semantically correct and whether it can be executed correctly as intended. There are three approaches: 1) conventional methods use surface form similarity metrics (token match, exact match, etc.) which have numerous limitations, 2) execution-based evaluation focuses more on code functionality based on pass/fail judgments for given test-cases, and 3) LLM-as-a-Judge employs LLMs for automated evaluation to judge if it is a correct answer for a given problem based on pre-defined metrics. In this work, we focused on enhancing LLM-as-a-Judge using bidirectional functionality matching and logic representation for reference-less automatic validation and refinement for Bash code generation to select the best model for automatic incident remediation in IT Automation. We used execution-based evaluation as ground-truth to evaluate our LLM-as-a-Judge metrics. Results show high accuracy and agreement with execution-based evaluation (and up to 8% over baseline). Finally, we built Reflection code agents to utilize judgments and feedback from our evaluation metrics which achieved significant improvement (up to 24% increase in accuracy) for automatic code refinement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11237v1</guid>
      <category>cs.SE</category>
      <category>cs.CL</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ngoc Phuoc An Vo, Brent Paulovicks, Vadim Sheinin</dc:creator>
    </item>
    <item>
      <title>Invocable APIs derived from NL2SQL datasets for LLM Tool-Calling Evaluation</title>
      <link>https://arxiv.org/abs/2506.11266</link>
      <description>arXiv:2506.11266v1 Announce Type: new 
Abstract: Large language models (LLMs) are routinely deployed as agentic systems, with access to tools that interact with live environments to accomplish tasks. In enterprise deployments these systems need to interact with API collections that can be extremely large and complex, often backed by databases. In order to create datasets with such characteristics, we explore how existing NL2SQL (Natural Language to SQL query) datasets can be used to automatically create NL2API datasets. Specifically, this work describes a novel data generation pipeline that exploits the syntax of SQL queries to construct a functionally equivalent sequence of API calls. We apply this pipeline to one of the largest NL2SQL datasets, BIRD-SQL to create a collection of over 2500 APIs that can be served as invocable tools or REST-endpoints. We pair natural language queries from BIRD-SQL to ground-truth API sequences based on this API pool. We use this collection to study the performance of 10 public LLMs and find that all models struggle to determine the right set of tools (consisting of tasks of intent detection, sequencing with nested function calls, and slot-filling). We find that models have extremely low task completion rates (7-47 percent - depending on the dataset) which marginally improves to 50 percent when models are employed as ReACT agents that interact with the live API environment. The best task completion rates are far below what may be required for effective general-use tool-calling agents, suggesting substantial scope for improvement in current state-of-the-art tool-calling LLMs. We also conduct detailed ablation studies, such as assessing the impact of the number of tools available as well as the impact of tool and slot-name obfuscation. We compare the performance of models on the original SQL generation tasks and find that current models are sometimes able to exploit SQL better than APIs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11266v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Benjamin Elder, Anupama Murthi, Jungkoo Kang, Ankita Rajaram Naik, Kiran Kate, Kinjal Basu, Danish Contractor</dc:creator>
    </item>
    <item>
      <title>A Tale of Two Systems: Characterizing Architectural Complexity on Machine Learning-Enabled Systems</title>
      <link>https://arxiv.org/abs/2506.11295</link>
      <description>arXiv:2506.11295v1 Announce Type: new 
Abstract: How can the complexity of ML-enabled systems be managed effectively? The goal of this research is to investigate how complexity affects ML-Enabled Systems (MLES). To address this question, this research aims to introduce a metrics-based architectural model to characterize the complexity of MLES. The goal is to support architectural decisions, providing a guideline for the inception and growth of these systems. This paper brings, side-by-side, the architecture representation of two systems that can be used as case studies for creating the metrics-based architectural model: the SPIRA and the Ocean Guard MLES.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11295v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renato Cordeiro Ferreira (University of S\~ao Paulo, Jheronimus Academy of Data Science, Technical University of Eindhoven, Tilburg University)</dc:creator>
    </item>
    <item>
      <title>A Step-by-Step Guide to Creating a Robust Autonomous Drone Testing Pipeline</title>
      <link>https://arxiv.org/abs/2506.11400</link>
      <description>arXiv:2506.11400v1 Announce Type: new 
Abstract: Autonomous drones are rapidly reshaping industries ranging from aerial delivery and infrastructure inspection to environmental monitoring and disaster response. Ensuring the safety, reliability, and efficiency of these systems is paramount as they transition from research prototypes to mission-critical platforms. This paper presents a step-by-step guide to establishing a robust autonomous drone testing pipeline, covering each critical stage: Software-in-the-Loop (SIL) Simulation Testing, Hardware-in-the-Loop (HIL) Testing, Controlled Real-World Testing, and In-Field Testing. Using practical examples, including the marker-based autonomous landing system, we demonstrate how to systematically verify drone system behaviors, identify integration issues, and optimize performance. Furthermore, we highlight emerging trends shaping the future of drone testing, including the integration of Neurosymbolic and LLMs, creating co-simulation environments, and Digital Twin-enabled simulation-based testing techniques. By following this pipeline, developers and researchers can achieve comprehensive validation, minimize deployment risks, and prepare autonomous drones for safe and reliable real-world operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11400v1</guid>
      <category>cs.SE</category>
      <category>cs.RO</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yupeng Jiang, Yao Deng, Sebastian Schroder, Linfeng Liang, Suhaas Gambhir, Alice James, Avishkar Seth, James Pirrie, Yihao Zhang, Xi Zheng</dc:creator>
    </item>
    <item>
      <title>ReVeal: Self-Evolving Code Agents via Iterative Generation-Verification</title>
      <link>https://arxiv.org/abs/2506.11442</link>
      <description>arXiv:2506.11442v1 Announce Type: new 
Abstract: Recent advances in reinforcement learning (RL) with verifiable outcome rewards have significantly improved the reasoning capabilities of large language models (LLMs), especially when combined with multi-turn tool interactions. However, existing methods lack both meaningful verification signals from realistic environments and explicit optimization for verification, leading to unreliable self-verification. To address these limitations, we propose ReVeal, a multi-turn reinforcement learning framework that interleaves code generation with explicit self-verification and tool-based evaluation. ReVeal enables LLMs to autonomously generate test cases, invoke external tools for precise feedback, and improves performance via a customized RL algorithm with dense, per-turn rewards. As a result, ReVeal fosters the co-evolution of a model's generation and verification capabilities through RL training, expanding the reasoning boundaries of the base model, demonstrated by significant gains in Pass@k on LiveCodeBench. It also enables test-time scaling into deeper inference regimes, with code consistently evolving as the number of turns increases during inference, ultimately surpassing DeepSeek-R1-Zero-Qwen-32B. These findings highlight the promise of ReVeal as a scalable and effective paradigm for building more robust and autonomous AI agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11442v1</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiyang Jin, Kunzhao Xu, Hang Li, Xueting Han, Yanmin Zhou, Cheng Li, Jing Bai</dc:creator>
    </item>
    <item>
      <title>Understanding the Issue Types in Open Source Blockchain-based Software Projects with the Transformer-based BERTopic</title>
      <link>https://arxiv.org/abs/2506.11451</link>
      <description>arXiv:2506.11451v1 Announce Type: new 
Abstract: Blockchain-based software systems are increasingly deployed across diverse domains, yet a systematic understanding of their development challenges remains limited. This paper presents a large-scale empirical study of 497,742 issues mined from 1,209 open-source blockchain projects hosted on GitHub. Employing BERTopic, a transformer-based topic modeling technique, we identify 49 distinct issue topics and organize them hierarchically into 11 major subcategories. Our analysis reveals that both general software development issues and blockchain-specific concerns are nearly equally represented, with Wallet Management and UI Enhancement emerging as the most prominent topics. We further examine the temporal evolution of issue categories and resolution times, finding that Wallet issues not only dominate in frequency but also exhibit the longest resolution time. Conversely, Mechanisms issues are resolved significantly faster. Issue frequency surged after 2016 with the rise of Ethereum and decentralized applications, but declined after 2022. These findings enhance our understanding of blockchain software maintenance, informing the development of specialized tools and practices to improve robustness and maintainability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11451v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Nahidul Islam Opu, Md Shahidul Islam, Sara Rouhani, Shaiful Chowdhury</dc:creator>
    </item>
    <item>
      <title>VulStamp: Vulnerability Assessment using Large Language Model</title>
      <link>https://arxiv.org/abs/2506.11484</link>
      <description>arXiv:2506.11484v1 Announce Type: new 
Abstract: Although modern vulnerability detection tools enable developers to efficiently identify numerous security flaws, indiscriminate remediation efforts often lead to superfluous development expenses. This is particularly true given that a substantial portion of detected vulnerabilities either possess low exploitability or would incur negligible impact in practical operational environments. Consequently, vulnerability severity assessment has emerged as a critical component in optimizing software development efficiency. Existing vulnerability assessment methods typically rely on manually crafted descriptions associated with source code artifacts. However, due to variability in description quality and subjectivity in intention interpretation, the performance of these methods is seriously limited. To address this issue, this paper introduces VulStamp, a novel intention-guided framework, to facilitate description-free vulnerability assessment. Specifically, VulStamp adopts static analysis together with Large Language Model (LLM) to extract the intention information of vulnerable code. Based on the intention information, VulStamp uses a prompt-tuned model for vulnerability assessment. Furthermore, to mitigate the problem of imbalanced data associated with vulnerability types, VulStamp integrates a Reinforcement Learning (RL)-based prompt-tuning method to train the assessment model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11484v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator> Haoshen, Ming Hu, Xiaofei Xie, Jiaye Li, Mingsong Chen</dc:creator>
    </item>
    <item>
      <title>A Procedural Framework for Assessing the Desirability of Process Deviations</title>
      <link>https://arxiv.org/abs/2506.11525</link>
      <description>arXiv:2506.11525v1 Announce Type: new 
Abstract: Conformance checking techniques help process analysts to identify where and how process executions deviate from a process model. However, they cannot determine the desirability of these deviations, i.e., whether they are problematic, acceptable or even beneficial for the process. Such desirability assessments are crucial to derive actions, but process analysts typically conduct them in a manual, ad-hoc way, which can be time-consuming, subjective, and irreplicable. To address this problem, this paper presents a procedural framework to guide process analysts in systematically assessing deviation desirability. It provides a step-by-step approach for identifying which input factors to consider in what order to categorize deviations into mutually exclusive desirability categories, each linked to action recommendations. The framework is based on a review and conceptualization of existing literature on deviation desirability, which is complemented by empirical insights from interviews with process analysis practitioners and researchers. We evaluate the framework through a desirability assessment task conducted with practitioners, indicating that the framework effectively enables them to streamline the assessment for a thorough yet concise evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11525v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Grohs, Nadine Cordes, Jana-Rebecca Rehse</dc:creator>
    </item>
    <item>
      <title>Augmenting the Generality and Performance of Large Language Models for Software Engineering</title>
      <link>https://arxiv.org/abs/2506.11548</link>
      <description>arXiv:2506.11548v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are revolutionizing software engineering (SE), with special emphasis on code generation and analysis. However, their applications to broader SE practices including conceptualization, design, and other non-code tasks, remain partially underexplored. This research aims to augment the generality and performance of LLMs for SE by (1) advancing the understanding of how LLMs with different characteristics perform on various non-code tasks, (2) evaluating them as sources of foundational knowledge in SE, and (3) effectively detecting hallucinations on SE statements. The expected contributions include a variety of LLMs trained and evaluated on domain-specific datasets, new benchmarks on foundational knowledge in SE, and methods for detecting hallucinations. Initial results in terms of performance improvements on various non-code tasks are promising.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11548v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>2025 IEEE/ACM 46th International Conference on Software Engineering (ICSE)</arxiv:journal_reference>
      <dc:creator>Fabian C. Pe\~na</dc:creator>
    </item>
    <item>
      <title>Leveraging GPT-4 for Vulnerability-Witnessing Unit Test Generation</title>
      <link>https://arxiv.org/abs/2506.11559</link>
      <description>arXiv:2506.11559v1 Announce Type: new 
Abstract: In the life-cycle of software development, testing plays a crucial role in quality assurance. Proper testing not only increases code coverage and prevents regressions but it can also ensure that any potential vulnerabilities in the software are identified and effectively fixed. However, creating such tests is a complex, resource-consuming manual process. To help developers and security experts, this paper explores the automatic unit test generation capability of one of the most widely used large language models, GPT-4, from the perspective of vulnerabilities. We examine a subset of the VUL4J dataset containing real vulnerabilities and their corresponding fixes to determine whether GPT-4 can generate syntactically and/or semantically correct unit tests based on the code before and after the fixes as evidence of vulnerability mitigation. We focus on the impact of code contexts, the effectiveness of GPT-4's self-correction ability, and the subjective usability of the generated test cases. Our results indicate that GPT-4 can generate syntactically correct test cases 66.5\% of the time without domain-specific pre-training. Although the semantic correctness of the fixes could be automatically validated in only 7. 5\% of the cases, our subjective evaluation shows that GPT-4 generally produces test templates that can be further developed into fully functional vulnerability-witnessing tests with relatively minimal manual effort.
  Therefore, despite the limited data, our initial findings suggest that GPT-4 can be effectively used in the generation of vulnerability-witnessing tests. It may not operate entirely autonomously, but it certainly plays a significant role in a partially automated process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11559v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>G\'abor Antal, D\'enes B\'an, Martin Isztin, Rudolf Ferenc, P\'eter Heged\H{u}s</dc:creator>
    </item>
    <item>
      <title>Identifying Helpful Context for LLM-based Vulnerability Repair: A Preliminary Study</title>
      <link>https://arxiv.org/abs/2506.11561</link>
      <description>arXiv:2506.11561v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have shown promise for automated vulnerability detection and repair in software systems. This paper investigates the performance of GPT-4o in repairing Java vulnerabilities from a widely used dataset (Vul4J), exploring how different contextual information affects automated vulnerability repair (AVR) capabilities. We compare the latest GPT-4o's performance against previous results with GPT-4 using identical prompts. We evaluated nine additional prompts crafted by us that contain various contextual information such as CWE or CVE information, and manually extracted code contexts. Each prompt was executed three times on 42 vulnerabilities, and the resulting fix candidates were validated using Vul4J's automated testing framework.
  Our results show that GPT-4o performed 11.9\% worse on average than GPT-4 with the same prompt, but was able to fix 10.5\% more distinct vulnerabilities in the three runs together. CVE information significantly improved repair rates, while the length of the task description had minimal impact. Combining CVE guidance with manually extracted code context resulted in the best performance. Using our \textsc{Top}-3 prompts together, GPT-4o repaired 26 (62\%) vulnerabilities at least once, outperforming both the original baseline (40\%) and its reproduction (45\%), suggesting that ensemble prompt strategies could improve vulnerability repair in zero-shot settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11561v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>G\'abor Antal, Bence Bogenf\"urst, Rudolf Ferenc, P\'eter Heged\H{u}s</dc:creator>
    </item>
    <item>
      <title>MBSR at Work: Perspectives from an Instructor and Software Developers</title>
      <link>https://arxiv.org/abs/2506.11588</link>
      <description>arXiv:2506.11588v1 Announce Type: new 
Abstract: In this paper, we present the preliminary findings from a qualitative study (i.e., semi-structured interviews) on how a Mindfulness-Based Stress Reduction (MBSR) program, carried out in the Software Development (SD) working context, is perceived by the software developers of a multinational company who participated in the MBSR program and by the instructor who led it. MBSR is a deeply personal and experiential practice in helping individuals manage stress, particularly in high-pressure environments such as workplaces, healthcare settings, education, and other demanding professional or personal situations. Although MBSR has been experimented in different working contexts; surprisingly, it has never been studied in the SD working context where there are several stress factors that developers experience (e.g., time pressure and uncertainty about the content of a particular task and its outcome). In this respect, qualitative research can generate valuable insights into the application of MBSR in the SD working context that cannot be captured by standardized quantitative measures. Being MBSR instructors and software developers the key stakeholders in delivering an MBSR program in the SD working context, understanding their first-hand experiences can provide a more detailed picture of the investigated phenomenon. The most important takeaway result of our research can be summarized as follows: despite initial skepticism, the developers recognized personal improvements due to the MBSR practice, though the integration of MBSR techniques in the working context remained challenging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11588v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simone Romano, Alberto Conforti, Gloria Guidetti, Sara Viotti, Rachele Ceschin, Giuseppe Scanniello</dc:creator>
    </item>
    <item>
      <title>Retrieval-Augmented Code Review Comment Generation</title>
      <link>https://arxiv.org/abs/2506.11591</link>
      <description>arXiv:2506.11591v1 Announce Type: new 
Abstract: Automated code review comment generation (RCG) aims to assist developers by automatically producing natural language feedback for code changes. Existing approaches are primarily either generation-based, using pretrained language models, or information retrieval-based (IR), reusing comments from similar past examples. While generation-based methods leverage code-specific pretraining on large code-natural language corpora to learn semantic relationships between code and natural language, they often struggle to generate low-frequency but semantically important tokens due to their probabilistic nature. In contrast, IR-based methods excel at recovering such rare tokens by copying from existing examples but lack flexibility in adapting to new code contexts-for example, when input code contains identifiers or structures not found in the retrieval database. To bridge the gap between generation-based and IR-based methods, this work proposes to leverage retrieval-augmented generation (RAG) for RCG by conditioning pretrained language models on retrieved code-review exemplars. By providing relevant examples that illustrate how similar code has been previously reviewed, the model is better guided to generate accurate review comments. Our evaluation on the Tufano et al. benchmark shows that RAG-based RCG outperforms both generation-based and IR-based RCG. It achieves up to +1.67% higher exact match and +4.25% higher BLEU scores compared to generation-based RCG. It also improves the generation of low-frequency ground-truth tokens by up to 24.01%. We additionally find that performance improves as the number of retrieved exemplars increases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11591v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hyunsun Hong, Jongmoon Baik</dc:creator>
    </item>
    <item>
      <title>Further Evidence on a Controversial Topic about Human-Based Experiments: Professionals vs. Students</title>
      <link>https://arxiv.org/abs/2506.11597</link>
      <description>arXiv:2506.11597v1 Announce Type: new 
Abstract: Most Software Engineering (SE) human-based controlled experiments rely on students as participants, raising concerns about their external validity. Specifically, the realism of results obtained from students and their applicability to the software industry remains in question. In this short paper, we bring further evidence on this controversial point. To do so, we compare 62 students and 42 software professionals on a bug-fixing task on the same Java program. The students were enrolled in a Bachelor's program in Computer Science, while the professionals were employed by two multinational companies (for one of them, the professionals were from two offices). Some variations in the experimental settings of the two groups (students and professionals) were present. For instance, the experimental environment of the experiment with professionals was more realistic; i.e., they faced some stress factors such as interruptions during the bug-fixing task. Considering the differences between the two groups of participants, the gathered data show that the students outperformed the professionals in fixing bugs. This diverges to some extent from past empirical evidence. Rather than presenting definitive conclusions, our results aim to catalyze the discussion on the use of students in experiments and pave the way for future investigations. Specifically, our results encourage us to examine the complex factors influencing SE tasks, making experiments as more realistic as possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11597v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simone Romano, Francesco Paolo Sferratore, Giuseppe Scanniello</dc:creator>
    </item>
    <item>
      <title>Understanding API Usage and Testing: An Empirical Study of C Libraries</title>
      <link>https://arxiv.org/abs/2506.11598</link>
      <description>arXiv:2506.11598v1 Announce Type: new 
Abstract: For library developers, understanding how their Application Programming Interfaces (APIs) are used in the field can be invaluable. Knowing how clients are using their APIs allows for data-driven decisions on prioritising bug reports, feature requests, and testing activities. For example, the priority of a bug report concerning an API can be partly determined by how widely that API is used.
  In this paper, we present an empirical study in which we analyse API usage across 21 popular open-source C libraries, such as OpenSSL and SQLite, with a combined total of 3,061 C/C++ clients. We compare API usage by clients with how well library test suites exercise the APIs to offer actionable insights for library developers. To our knowledge, this is the first study that compares API usage and API testing at scale for the C/C++ ecosystem. Our study shows that library developers do not prioritise their effort based on how clients use their API, with popular APIs often poorly tested. For example, in LMDB, a popular key-value store, 45% of the APIs are used by clients but not tested by the library test suite. We further show that client test suites can be leveraged to improve library testing e.g., improving coverage in LMDB by 14.7% with the important advantage that those tests are representative of how the APIs are used in the field.
  For our empirical study, we have developed LibProbe, a framework that can be used to analyse a large corpus of clients for a given library and produce various metrics useful to library developers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11598v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ahmed Zaki, Cristian Cadar</dc:creator>
    </item>
    <item>
      <title>Accelerating Delta Debugging through Probabilistic Monotonicity Assessment</title>
      <link>https://arxiv.org/abs/2506.11614</link>
      <description>arXiv:2506.11614v1 Announce Type: new 
Abstract: Delta debugging assumes search space monotonicity: if a program causes a failure, any supersets of that program will also induce the same failure, permitting the exclusion of subsets of non-failure-inducing programs. However, this assumption does not always hold in practice. This paper introduces Probabilistic Monotonicity Assessment (PMA), enhancing the efficiency of DDMIN-style algorithms without sacrificing effectiveness. PMA dynamically models and assesses the search space's monotonicity based on prior tests tried during the debugging process and uses a confidence function to quantify monotonicity, thereby enabling the probabilistic exclusion of subsets of non-failure-inducing programs. Our approach significantly reduces redundant tests that would otherwise be performed, without compromising the quality of the reduction.
  We evaluated PMA against two leading DDMIN-style tools, CHISEL and ProbDD. Our findings indicate that PMA cuts processing time by 59.2% compared to CHISEL, accelerates the reduction process (i.e., the number of tokens deleted per second) by 3.32x, and decreases the sizes of the final reduced programs by 6.7%. Against ProbDD, PMA reduces processing time by 22.0%, achieves a 1.34x speedup in the reduction process, and further decreases the sizes of the final reduced programs by 3.0%. These findings affirm PMA's role in significantly improving delta debugging's efficiency while maintaining or enhancing its effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11614v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yonggang Tao, Jingling Xue</dc:creator>
    </item>
    <item>
      <title>An Empirical study on LLM-based Log Retrieval for Software Engineering Metadata Management</title>
      <link>https://arxiv.org/abs/2506.11659</link>
      <description>arXiv:2506.11659v1 Announce Type: new 
Abstract: Developing autonomous driving systems (ADSs) involves generating and storing extensive log data from test drives, which is essential for verification, research, and simulation. However, these high-frequency logs, recorded over varying durations, pose challenges for developers attempting to locate specific driving scenarios. This difficulty arises due to the wide range of signals representing various vehicle components and driving conditions, as well as unfamiliarity of some developers' with the detailed meaning of these signals. Traditional SQL-based querying exacerbates this challenge by demanding both domain expertise and database knowledge, often yielding results that are difficult to verify for accuracy.
  This paper introduces a Large Language Model (LLM)-supported approach that combines signal log data with video recordings from test drives, enabling natural language based scenario searches while reducing the need for specialized knowledge. By leveraging scenario distance graphs and relative gap indicators, it provides quantifiable metrics to evaluate the reliability of query results. The method is implemented as an API for efficient database querying and retrieval of relevant records, paired with video frames for intuitive visualization. Evaluation on an open industrial dataset demonstrates improved efficiency and reliability in scenario retrieval, eliminating dependency on a single data source and conventional SQL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11659v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simin Sun, Yuchuan Jin, Miroslaw Staron</dc:creator>
    </item>
    <item>
      <title>SoK: Automated Vulnerability Repair: Methods, Tools, and Assessments</title>
      <link>https://arxiv.org/abs/2506.11697</link>
      <description>arXiv:2506.11697v1 Announce Type: new 
Abstract: The increasing complexity of software has led to the steady growth of vulnerabilities. Vulnerability repair investigates how to fix software vulnerabilities. Manual vulnerability repair is labor-intensive and time-consuming because it relies on human experts, highlighting the importance of Automated Vulnerability Repair (AVR). In this SoK, we present the systematization of AVR methods through the three steps of AVR workflow: vulnerability analysis, patch generation, and patch validation. We assess AVR tools for C/C++ and Java programs as they have been widely studied by the community. Since existing AVR tools for C/C++ programs are evaluated with different datasets, which often consist of a few vulnerabilities, we construct the first C/C++ vulnerability repair benchmark dataset, dubbed Vul4C, which contains 144 vulnerabilities as well as their exploits and patches. We use Vul4C to evaluate seven AVR tools for C/C++ programs and use the third-party Vul4J dataset to evaluate two AVR tools for Java programs. We also discuss future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11697v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiwei Hu, Zhen Li, Kedie Shu, Shenghua Guan, Deqing Zou, Shouhuai Xu, Bin Yuan, Hai Jin</dc:creator>
    </item>
    <item>
      <title>Classification of Quality Characteristics in Online User Feedback using Linguistic Analysis, Crowdsourcing and LLMs</title>
      <link>https://arxiv.org/abs/2506.11722</link>
      <description>arXiv:2506.11722v1 Announce Type: new 
Abstract: Software qualities such as usability or reliability are among the strongest determinants of mobile app user satisfaction and constitute a significant portion of online user feedback on software products, making it a valuable source of quality-related feedback to guide the development process. The abundance of online user feedback warrants the automated identification of quality characteristics, but the online user feedback's heterogeneity and the lack of appropriate training corpora limit the applicability of supervised machine learning. We therefore investigate the viability of three approaches that could be effective in low-data settings: language patterns (LPs) based on quality-related keywords, instructions for crowdsourced micro-tasks, and large language model (LLM) prompts. We determined the feasibility of each approach and then compared their accuracy. For the complex multiclass classification of quality characteristics, the LP-based approach achieved a varied precision (0.38-0.92) depending on the quality characteristic, and low recall; crowdsourcing achieved the best average accuracy in two consecutive phases (0.63, 0.72), which could be matched by the best-performing LLM condition (0.66) and a prediction based on the LLMs' majority vote (0.68). Our findings show that in this low-data setting, the two approaches that use crowdsourcing or LLMs instead of involving experts achieve accurate classifications, while the LP-based approach has only limited potential. The promise of crowdsourcing and LLMs in this context might even extend to building training corpora.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11722v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Eduard C. Groen, Fabiano Dalpiaz, Martijn van Vliet, Boris Winter, Joerg Doerr, Sjaak Brinkkemper</dc:creator>
    </item>
    <item>
      <title>A Short Survey on Formalising Software Requirements using Large Language Models</title>
      <link>https://arxiv.org/abs/2506.11874</link>
      <description>arXiv:2506.11874v1 Announce Type: new 
Abstract: This paper presents a focused literature survey on the use of large language models (LLM) to assist in writing formal specifications for software. A summary of thirty-five key papers is presented, including examples for specifying programs written in Dafny, C and Java. This paper arose from the project VERIFAI - Traceability and verification of natural language requirements that addresses the challenges in writing formal specifications from requirements that are expressed in natural language. Our methodology employed multiple academic databases to identify relevant research. The AI-assisted tool Elicit facilitated the initial paper selection, which were manually screened for final selection. The survey provides valuable insights and future directions for utilising LLMs while formalising software requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11874v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arshad Beg, Diarmuid O'Donoghue, Rosemary Monahan</dc:creator>
    </item>
    <item>
      <title>LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?</title>
      <link>https://arxiv.org/abs/2506.11928</link>
      <description>arXiv:2506.11928v1 Announce Type: new 
Abstract: Recent reports claim that large language models (LLMs) now outperform elite humans in competitive programming. Drawing on knowledge from a group of medalists in international algorithmic contests, we revisit this claim, examining how LLMs differ from human experts and where limitations still remain. We introduce LiveCodeBench Pro, a benchmark composed of problems from Codeforces, ICPC, and IOI that are continuously updated to reduce the likelihood of data contamination. A team of Olympiad medalists annotates every problem for algorithmic categories and conducts a line-by-line analysis of failed model-generated submissions. Using this new data and benchmark, we find that frontier models still have significant limitations: without external tools, the best model achieves only 53% pass@1 on medium-difficulty problems and 0% on hard problems, domains where expert humans still excel. We also find that LLMs succeed at implementation-heavy problems but struggle with nuanced algorithmic reasoning and complex case analysis, often generating confidently incorrect justifications. High performance appears largely driven by implementation precision and tool augmentation, not superior reasoning. LiveCodeBench Pro thus highlights the significant gap to human grandmaster levels, while offering fine-grained diagnostics to steer future improvements in code-centric LLM reasoning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11928v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihan Zheng, Zerui Cheng, Zeyu Shen, Shang Zhou, Kaiyuan Liu, Hansen He, Dongruixuan Li, Stanley Wei, Hangyi Hao, Jianzhu Yao, Peiyao Sheng, Zixuan Wang, Wenhao Chai, Aleksandra Korolova, Peter Henderson, Sanjeev Arora, Pramod Viswanath, Jingbo Shang, Saining Xie</dc:creator>
    </item>
    <item>
      <title>OntoGSN: An Ontology for Dynamic Management of Assurance Cases</title>
      <link>https://arxiv.org/abs/2506.11023</link>
      <description>arXiv:2506.11023v1 Announce Type: cross 
Abstract: Assurance cases (ACs) are a common artifact for building and maintaining confidence in system properties such as safety or robustness. Constructing an AC can be challenging, although existing tools provide support in static, document-centric applications and methods for dynamic contexts (e.g., autonomous driving) are emerging. Unfortunately, managing ACs remains a challenge, since maintaining the embedded knowledge in the face of changes requires substantial effort, in the process deterring developers - or worse, producing poorly managed cases that instill false confidence. To address this, we present OntoGSN: an ontology and supporting middleware for managing ACs in the Goal Structuring Notation (GSN) standard. OntoGSN offers a knowledge representation and a queryable graph that can be automatically populated, evaluated, and updated. Our contributions include: a 1:1 formalization of the GSN Community Standard v3 in an OWL ontology with SWRL rules; a helper ontology and parser for integration with a widely used AC tool; a repository and documentation of design decisions for OntoGSN maintenance; a SPARQL query library with automation patterns; and a prototypical interface. The ontology strictly adheres to the standard's text and has been evaluated according to FAIR principles, the OOPS framework, competency questions, and community feedback. The development of other middleware elements is guided by the community needs and subject to ongoing evaluations. To demonstrate the utility of our contributions, we illustrate dynamic AC management in an example involving assurance of adversarial robustness in large language models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11023v1</guid>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomas Bueno Momcilovic, Barbara Gallina, Ingmar Kessler, Dian Balta</dc:creator>
    </item>
    <item>
      <title>Technical Report for Argoverse2 Scenario Mining Challenges on Iterative Error Correction and Spatially-Aware Prompting</title>
      <link>https://arxiv.org/abs/2506.11124</link>
      <description>arXiv:2506.11124v1 Announce Type: cross 
Abstract: Scenario mining from extensive autonomous driving datasets, such as Argoverse 2, is crucial for the development and validation of self-driving systems. The RefAV framework represents a promising approach by employing Large Language Models (LLMs) to translate natural-language queries into executable code for identifying relevant scenarios. However, this method faces challenges, including runtime errors stemming from LLM-generated code and inaccuracies in interpreting parameters for functions that describe complex multi-object spatial relationships. This technical report introduces two key enhancements to address these limitations: (1) a fault-tolerant iterative code-generation mechanism that refines code by re-prompting the LLM with error feedback, and (2) specialized prompt engineering that improves the LLM's comprehension and correct application of spatial-relationship functions. Experiments on the Argoverse 2 validation set with diverse LLMs-Qwen2.5-VL-7B, Gemini 2.5 Flash, and Gemini 2.5 Pro-show consistent gains across multiple metrics; most notably, the proposed system achieves a HOTA-Temporal score of 52.37 on the official test set using Gemini 2.5 Pro. These results underline the efficacy of the proposed techniques for reliable, high-precision scenario mining.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11124v1</guid>
      <category>cs.CV</category>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifei Chen, Ross Greer</dc:creator>
    </item>
    <item>
      <title>KEENHash: Hashing Programs into Function-Aware Embeddings for Large-Scale Binary Code Similarity Analysis</title>
      <link>https://arxiv.org/abs/2506.11612</link>
      <description>arXiv:2506.11612v1 Announce Type: cross 
Abstract: Binary code similarity analysis (BCSA) is a crucial research area in many fields such as cybersecurity. Specifically, function-level diffing tools are the most widely used in BCSA: they perform function matching one by one for evaluating the similarity between binary programs. However, such methods need a high time complexity, making them unscalable in large-scale scenarios (e.g., 1/n-to-n search). Towards effective and efficient program-level BCSA, we propose KEENHash, a novel hashing approach that hashes binaries into program-level representations through large language model (LLM)-generated function embeddings. KEENHash condenses a binary into one compact and fixed-length program embedding using K-Means and Feature Hashing, allowing us to do effective and efficient large-scale program-level BCSA, surpassing the previous state-of-the-art methods. The experimental results show that KEENHash is at least 215 times faster than the state-of-the-art function matching tools while maintaining effectiveness. Furthermore, in a large-scale scenario with 5.3 billion similarity evaluations, KEENHash takes only 395.83 seconds while these tools will cost at least 56 days. We also evaluate KEENHash on the program clone search of large-scale BCSA across extensive datasets in 202,305 binaries. Compared with 4 state-of-the-art methods, KEENHash outperforms all of them by at least 23.16%, and displays remarkable superiority over them in the large-scale BCSA security scenario of malware detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11612v1</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhijie Liu, Qiyi Tang, Sen Nie, Shi Wu, Liang Feng Zhang, Yutian Tang</dc:creator>
    </item>
    <item>
      <title>GeoPandas-AI: A Smart Class Bringing LLM as Stateful AI Code Assistant</title>
      <link>https://arxiv.org/abs/2506.11781</link>
      <description>arXiv:2506.11781v1 Announce Type: cross 
Abstract: Geospatial data analysis plays a crucial role in tackling intricate societal challenges such as urban planning and climate modeling. However, employing tools like GeoPandas, a prominent Python library for geospatial data manipulation, necessitates expertise in complex domain-specific syntax and workflows. GeoPandas-AI addresses this gap by integrating LLMs directly into the GeoPandas workflow, transforming the GeoDataFrame class into an intelligent, stateful class for both data analysis and geospatial code development. This paper formalizes the design of such a smart class and provides an open-source implementation of GeoPandas-AI in PyPI package manager. Through its innovative combination of conversational interfaces and stateful exploitation of LLMs for code generation and data analysis, GeoPandas-AI introduces a new paradigm for code-copilots and instantiates it for geospatial development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11781v1</guid>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gaspard Merten, Gilles Dejaegere, Mahmoud Sakr</dc:creator>
    </item>
    <item>
      <title>code_transformed: The Influence of Large Language Models on Code</title>
      <link>https://arxiv.org/abs/2506.12014</link>
      <description>arXiv:2506.12014v1 Announce Type: cross 
Abstract: Coding remains one of the most fundamental modes of interaction between humans and machines. With the rapid advancement of Large Language Models (LLMs), code generation capabilities have begun to significantly reshape programming practices. This development prompts a central question: Have LLMs transformed code style, and how can such transformation be characterized? In this paper, we present a pioneering study that investigates the impact of LLMs on code style, with a focus on naming conventions, complexity, maintainability, and similarity. By analyzing code from over 19,000 GitHub repositories linked to arXiv papers published between 2020 and 2025, we identify measurable trends in the evolution of coding style that align with characteristics of LLM-generated code. For instance, the proportion of snake\_case variable names in Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we investigate how LLMs approach algorithmic problems by examining their reasoning processes. Given the diversity of LLMs and usage scenarios, among other factors, it is difficult or even impossible to precisely estimate the proportion of code generated or assisted by LLMs. Our experimental results provide the first large-scale empirical evidence that LLMs affect real-world programming style.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12014v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuliang Xu, Siming Huang, Mingmeng Geng, Yao Wan, Xuanhua Shi, Dongping Chen</dc:creator>
    </item>
    <item>
      <title>VulScribeR: Exploring RAG-based Vulnerability Augmentation with LLMs</title>
      <link>https://arxiv.org/abs/2408.04125</link>
      <description>arXiv:2408.04125v3 Announce Type: replace 
Abstract: Detecting vulnerabilities is vital for software security, yet deep learning-based vulnerability detectors (DLVD) face a data shortage, which limits their effectiveness. Data augmentation can potentially alleviate the data shortage, but augmenting vulnerable code is challenging and requires a generative solution that maintains vulnerability. Previous works have only focused on generating samples that contain single statements or specific types of vulnerabilities. Recently, large language models (LLMs) have been used to solve various code generation and comprehension tasks with inspiring results, especially when fused with retrieval augmented generation (RAG). Therefore, we propose VulScribeR, a novel LLM-based solution that leverages carefully curated prompt templates to augment vulnerable datasets. More specifically, we explore three strategies to augment both single and multi-statement vulnerabilities, with LLMs, namely Mutation, Injection, and Extension. Our extensive evaluation across four vulnerability datasets and DLVD models, using three LLMs, show that our approach beats two SOTA methods Vulgen and VGX, and Random Oversampling (ROS) by 27.48%, 27.93%, and 15.41% in f1-score with 5K generated vulnerable samples on average, and 53.84%, 54.10%, 69.90%, and 40.93% with 15K generated vulnerable samples. Our approach demonstrates its feasibility for large-scale data augmentation by generating 1K samples at as cheap as US$ 1.88.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04125v3</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seyed Shayan Daneshvar, Yu Nong, Xu Yang, Shaowei Wang, Haipeng Cai</dc:creator>
    </item>
    <item>
      <title>How Well Do Large Language Models Serve as End-to-End Secure Code Agents for Python?</title>
      <link>https://arxiv.org/abs/2408.10495</link>
      <description>arXiv:2408.10495v2 Announce Type: replace 
Abstract: The rapid advancement of large language models (LLMs) such as GPT-4 has revolutionized the landscape of software engineering, positioning these models at the core of modern development practices. As we anticipate these models to evolve into the primary and trustworthy tools used in software development, ensuring the security of the code they produce becomes paramount. How well can LLMs serve as end-to-end secure code producers? This paper presents a systematic investigation into LLMs' inherent potential to generate code with fewer vulnerabilities. Specifically, We studied GPT-3.5 and GPT-4's capability to identify and repair vulnerabilities in the code generated by four popular LLMs including themselves (GPT-3.5, GPT-4, Code Llama, and CodeGeeX2). By manually or automatically reviewing 4,900 pieces of code, our study reveals that: (1) large language models lack awareness of scenario-relevant security risks, which leads to the generation of over 75% vulnerable code on the SecurityEval benchmark; (2) LLMs such as GPT-3.5 and GPT-4 are unable to precisely identify vulnerabilities in the code they generated; (3) GPT-3.5 and GPT-4 can achieve 33.2%~59.6% success rates in repairing the insecure code produced by the 4 LLMs, but they both perform poorly when repairing self-produced code, indicating self-repair "blind spots". To address the limitation of a single round of repair, we developed a lightweight tool that prompts LLMs to construct safer source code through an iterative repair procedure based on the insights gained from our study. Experiments show that assisted by semantic analysis engines, our tool significantly improves the success rates of repair to 65.9%~85.5%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10495v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianian Gong, Nachuan Duan, Ziheng Tao, Zhaohui Gong, Yuan Yuan, Minlie Huang</dc:creator>
    </item>
    <item>
      <title>LLM-based Property-based Test Generation for Guardrailing Cyber-Physical Systems</title>
      <link>https://arxiv.org/abs/2505.23549</link>
      <description>arXiv:2505.23549v2 Announce Type: replace 
Abstract: Cyber-physical systems (CPSs) are complex systems that integrate physical, computational, and communication subsystems. The heterogeneous nature of these systems makes their safety assurance challenging. In this paper, we propose a novel automated approach for guardrailing cyber-physical systems using property-based tests (PBTs) generated by Large Language Models (LLMs). Our approach employs an LLM to extract properties from the code and documentation of CPSs. Next, we use the LLM to generate PBTs that verify the extracted properties on the CPS. The generated PBTs have two uses. First, they are used to test the CPS before it is deployed, i.e., at design time. Secondly, these PBTs can be used after deployment, i.e., at run time, to monitor the behavior of the system and guardrail it against unsafe states. We implement our approach in ChekProp and conduct preliminary experiments to evaluate the generated PBTs in terms of their relevance (how well they match manually crafted properties), executability (how many run with minimal manual modification), and effectiveness (coverage of the input space partitions). The results of our experiments and evaluation demonstrate a promising path forward for creating guardrails for CPSs using LLM-generated property-based tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23549v2</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Khashayar Etemadi, Marjan Sirjani, Mahshid Helali Moghadam, Per Strandberg, Paul Pettersson</dc:creator>
    </item>
    <item>
      <title>On Mutation-Guided Unit Test Generation</title>
      <link>https://arxiv.org/abs/2506.02954</link>
      <description>arXiv:2506.02954v3 Announce Type: replace 
Abstract: Unit tests play a vital role in uncovering potential faults in software. While tools like EvoSuite focus on maximizing code coverage, recent advances in large language models (LLMs) have shifted attention toward LLM-based test generation. However, code coverage metrics -- such as line and branch coverage -- remain overly emphasized in reported research, despite being weak indicators of a test suite's fault-detection capability. In contrast, mutation score offers a more reliable and stringent measure, as demonstrated in our findings where some test suites achieve 100% coverage but only 4% mutation score. Although a few studies consider mutation score, the effectiveness of LLMs in killing mutants remains underexplored. In this paper, we propose MUTGEN, a mutation-guided, LLM-based test generation approach that incorporates mutation feedback directly into the prompt. Evaluated on 204 subjects from two benchmarks, MUTGEN significantly outperforms both EvoSuite and vanilla prompt-based strategies in terms of mutation score. Furthermore, MUTGEN introduces an iterative generation mechanism that pushes the limits of LLMs in killing additional mutants. Our study also provide insights into the limitations of LLM-based generation, analyzing the reasons for live and uncovered mutants, and the impact of different mutation operators on generation effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02954v3</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guancheng Wang, Qinghua Xu, Lionel C. Briand, Kui Liu</dc:creator>
    </item>
    <item>
      <title>Black-Box Adversarial Attacks on LLM-Based Code Completion</title>
      <link>https://arxiv.org/abs/2408.02509</link>
      <description>arXiv:2408.02509v2 Announce Type: replace-cross 
Abstract: Modern code completion engines, powered by large language models (LLMs), assist millions of developers with their strong capabilities to generate functionally correct code. Due to this popularity, it is crucial to investigate the security implications of relying on LLM-based code completion. In this work, we demonstrate that state-of-the-art black-box LLM-based code completion engines can be stealthily biased by adversaries to significantly increase their rate of insecure code generation. We present the first attack, named INSEC, that achieves this goal. INSEC works by injecting an attack string as a short comment in the completion input. The attack string is crafted through a query-based optimization procedure starting from a set of carefully designed initialization schemes. We demonstrate INSEC's broad applicability and effectiveness by evaluating it on various state-of-the-art open-source models and black-box commercial services (e.g., OpenAI API and GitHub Copilot). On a diverse set of security-critical test cases, covering 16 CWEs across 5 programming languages, INSEC increases the rate of generated insecure code by more than 50%, while maintaining the functional correctness of generated code. We consider INSEC practical -- it requires low resources and costs less than 10 US dollars to develop on commodity hardware. Moreover, we showcase the attack's real-world deployability, by developing an IDE plug-in that stealthily injects INSEC into the GitHub Copilot extension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02509v2</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Slobodan Jenko, Niels M\"undler, Jingxuan He, Mark Vero, Martin Vechev</dc:creator>
    </item>
  </channel>
</rss>
