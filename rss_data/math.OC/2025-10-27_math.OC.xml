<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 Oct 2025 04:01:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Fixed Horizon Linear Quadratic Covariance Steering in Continuous Time with Hilbert-Schmidt Terminal Cost</title>
      <link>https://arxiv.org/abs/2510.21944</link>
      <description>arXiv:2510.21944v1 Announce Type: new 
Abstract: We formulate and solve the fixed horizon linear quadratic covariance steering problem in continuous time with a terminal cost measured in Hilbert-Schmidt (i.e., Frobenius) norm error between the desired and the controlled terminal covariances. For this problem, the necessary conditions of optimality become a coupled matrix ODE two-point boundary value problem. To solve this system of equations, we design a matricial recursive algorithm and prove its convergence. The proposed algorithm and its analysis make use of the linear fractional transforms parameterized by the state transition matrix of the associated Hamiltonian matrix. To illustrate the results, we provide two numerical examples: one with a two dimensional and another with a six dimensional state space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21944v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tushar Sial, Abhishek Halder</dc:creator>
    </item>
    <item>
      <title>Convex Bound of Nonlinear Dynamical Errors for Stochastic Optimal Control</title>
      <link>https://arxiv.org/abs/2510.21975</link>
      <description>arXiv:2510.21975v1 Announce Type: new 
Abstract: Applying linear controllers to nonlinear systems requires the dynamical linearization about a reference. In highly nonlinear environments such as cislunar space, the region of validity for these linearizations varies widely and can negatively affect controller performance if not carefully formulated. This paper presents a formulation that minimizes the nonlinear errors experienced by linear covariance controllers. The formulation involves upper-bounding the remainder term from the linearization process using higher-order terms in a Taylor series expansion, and resolving it into a convex function. This can serve as a cost function for controller gain optimization, and its convex nature allows for efficient solutions through convex optimization. This formulation is then demonstrated and compared with the current methods within a halo orbit stationkeeping scenario. The results show that the formulation proposed in this paper maintains the Gaussianity of the distribution in nonlinear simulations more effectively, thereby allowing the linear covariance controller to perform more as intended in nonlinear environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21975v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel C. Qi, Kenshiro Oguri</dc:creator>
    </item>
    <item>
      <title>Control of neural field equations with step-function inputs</title>
      <link>https://arxiv.org/abs/2510.22022</link>
      <description>arXiv:2510.22022v1 Announce Type: new 
Abstract: Wilson-Cowan and Amari-type models capture nonlinear neural population dynamics, providing a fundamental framework for modeling how sensory and other exogenous inputs shape activity in neural tissue. We study the controllability properties of Amari-type neural fields subject to piecewise/constant-in-time inputs. The model describes the time evolution of the polarization of neural tissue within a spatial continuum, with synaptic interactions represented by a convolution kernel. We study the synthesis of piecewise/constant-in-time inputs to achieve two-point boundary-type control objectives, namely, steering neural activity from an initial state to a prescribed target state. This approach is particularly relevant for predicting the emergence of paradoxical neural representations, such as discordant visual illusions that occur in response to overt sensory stimuli. We first present a control synthesis based on the Banach fixed-point theorem, which yields an iterative construction of a constant-in-time input under minimal regularity assumptions on the kernel and transfer function; however, it exhibits practical limitations, even in the linear case. To overcome these challenges, we then develop a generic synthesis framework based on the flow of neural dynamics drift, enabling explicit piecewise constant and constant-in-time inputs. Extensive numerical results in one and two spatial dimensions confirm the effectiveness of the proposed syntheses and demonstrate their superior performance compared to inputs derived from naive linearization at the initial or target states when these states are not equilibria of the drift dynamics. By providing a mathematically rigorous framework for controlling Amari-type neural fields, this work advances our understanding of nonlinear neural population control with potential applications in computational neuroscience, psychophysics, and neurostimulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22022v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.AP</category>
      <category>math.DS</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cyprien Tamekue, ShiNung Ching</dc:creator>
    </item>
    <item>
      <title>A Retraction-free Method for Nonsmooth Minimax Optimization over a Compact Manifold</title>
      <link>https://arxiv.org/abs/2510.22065</link>
      <description>arXiv:2510.22065v1 Announce Type: new 
Abstract: We study the minimax problem $\min_{x\in M} \max_y f_r(x,y):=f(x,y)-h(y)$, where $M$ is a compact submanifold, $f$ is continuously differentiable in $(x, y)$, $h$ is a closed, weakly-convex (possibly non-smooth) function and we assume that the regularized coupling function $-f_r(x,\cdot)$ is either $\mu$-PL for some $\mu&gt;0$ or concave ($\mu = 0$) for any fixed $x$ in the vicinity of $M$. To address the nonconvexity due to the manifold constraint, we use an exact penalty for the constraint $x \in M$, and enforcing a convex constraint $x\in X$ for some $X \supset M$, onto which projections can be computed efficiently. Building upon this new formulation for the manifold minimax problem in question, a single-loop smoothed manifold gradient descent-ascent (sm-MGDA) algorithm is proposed. Theoretically, any limit point of sm-MGDA sequence is a stationary point of the manifold minimax problem and sm-MGDA can generate an $O(\epsilon)$-stationary point of the original problem with $O(1/\epsilon^2)$ and $\tilde{O}(1/\epsilon^4)$ complexity for $\mu &gt; 0$ and $\mu = 0$ scenarios, respectively. Moreover, for the $\mu = 0$ setting, through adopting Tikhonov regularization of the dual, one can improve the complexity to $O(1/\epsilon^3)$ at the expense of asymptotic stationarity. The key component, common in the analysis of all cases, is to connect $\epsilon$-stationary points between the penalized problem and the original problem by showing that the constraint $x \in X$ becomes inactive and the penalty term tends to $0$ along any convergent subsequence. To our knowledge, sm-MGDA is the first retraction-free algorithm for minimax problems over compact submanifolds, and this is a very desirable algorithmic property since through avoiding retractions, one can get away with matrix orthogonalization subroutines required for computing retractions to manifolds arising in practice, which are not GPU friendly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22065v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Necdet Serhat Aybat, Jiang Hu, Zhanwang Deng</dc:creator>
    </item>
    <item>
      <title>Quasi-Self-Concordant Optimization with Lewis Weights</title>
      <link>https://arxiv.org/abs/2510.22088</link>
      <description>arXiv:2510.22088v1 Announce Type: new 
Abstract: In this paper, we study the problem $\min_{x\in \mathbb{R}^{d},Nx=v}\sum_{i=1}^{n}f((Ax-b)_{i})$ for a quasi-self-concordant function $f:\mathbb{R}\to\mathbb{R}$, where $A,N$ are $n\times d$ and $m\times d$ matrices, $b,v$ are vectors of length $n$ and $m$ with $n\ge d.$ We show an algorithm based on a trust-region method with an oracle that can be implemented using $\widetilde{O}(d^{1/3})$ linear system solves, improving the $\widetilde{O}(n^{1/3})$ oracle by {[}Adil-Bullins-Sachdeva, NeurIPS 2021{]}. Our implementation of the oracle relies on solving the overdetermined $\ell_{\infty}$-regression problem $\min_{x\in\mathbb{R}^{d},Nx=v}\|Ax-b\|_{\infty}$. We provide an algorithm that finds a $(1+\epsilon)$-approximate solution to this problem using $O((d^{1/3}/\epsilon+1/\epsilon^{2})\log(n/\epsilon))$ linear system solves. This algorithm leverages $\ell_{\infty}$ Lewis weight overestimates and achieves this iteration complexity via a simple lightweight IRLS approach, inspired by the work of {[}Ene-Vladu, ICML 2019{]}. Experimentally, we demonstrate that our algorithm significantly improves the runtime of the standard CVX solver.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22088v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alina Ene, Ta Duy Nguyen, Adrian Vladu</dc:creator>
    </item>
    <item>
      <title>From Time Series to Affine Systems</title>
      <link>https://arxiv.org/abs/2510.22089</link>
      <description>arXiv:2510.22089v1 Announce Type: new 
Abstract: The paper extends core results of behavioral systems theory from linear to affine time-invariant systems. We characterize the behavior of affine time-invariant systems via kernel, input-output, state-space, and finite-horizon data-driven representations, demonstrating a range of structural parallels with linear time-invariant systems. Building on these representations, we introduce a new persistence of excitation condition tailored to the model class of affine time-invariant systems. The condition yields a new fundamental lemma that parallels the classical result for linear systems while provably reducing data requirements. Our analysis highlights that excitation conditions must be adapted to the model class: overlooking structural differences may lead to unnecessarily conservative data requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22089v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. Padoan, J. Eising, I. Markovsky</dc:creator>
    </item>
    <item>
      <title>Accelerated Distance-adaptive Methods for H\"{o}lder Smooth and Convex Optimization</title>
      <link>https://arxiv.org/abs/2510.22135</link>
      <description>arXiv:2510.22135v1 Announce Type: new 
Abstract: This paper introduces new parameter-free first-order methods for convex optimization problems in which the objective function exhibits H\"{o}lder smoothness. Inspired by the recently proposed distance-over-gradient (DOG) technique, we propose an accelerated distance-adaptive method which achieves optimal anytime convergence rates for H\"{o}lder smooth problems without requiring prior knowledge of smoothness parameters or explicit parameter tuning. Importantly, our parameter-free approach removes the necessity of specifying target accuracy in advance, addressing a limitation found in the universal fast gradient methods (Nesterov, Yu. \textit{Mathematical Programming}, 2015). For convex stochastic optimization, we further present a parameter-free accelerated method that eliminates the need for line-search procedures. Preliminary experimental results highlight the effectiveness of our approach on convex nonsmooth problems and its advantages over existing parameter-free or accelerated methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22135v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yijin Ren, Haifeng Xu, Qi Deng</dc:creator>
    </item>
    <item>
      <title>A projection-free dynamics for nonsmooth composite optimization</title>
      <link>https://arxiv.org/abs/2510.22173</link>
      <description>arXiv:2510.22173v1 Announce Type: new 
Abstract: This paper proposes a projection-free primal-dual dynamics for the nonsmooth composite optimization problems with equality and inequality constraints. To deal with optimization constraints, this paper departs from the use of gradient projection method, but resorts to the idea of mirror descent to design a continuous-time smooth optimization dynamics which advantageously leads to easier convergence analysis and more efficient numerical simulation. Also, the strategy of proximal augmented Lagrangian (PAL$^{\dag}$) is extended to incorporate general convex equality-inequality constraints and the strong convexity-concavity of the primal-dual variables is achieved, ensuring exponential convergence of the resulting algorithm. Furthermore, the convergence result in this paper extends existing exponential convergence which either takes no account of constraints or considers only affine linear constraints, and it also enhances existing asymptotic convergence under convex constraints which unfortunately depends on the complex gradient projection scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22173v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Ni, Yangfan Qiu, Yanyan Xiao</dc:creator>
    </item>
    <item>
      <title>Partial Envelope for Optimization Problem with Nonconvex Constraints</title>
      <link>https://arxiv.org/abs/2510.22223</link>
      <description>arXiv:2510.22223v1 Announce Type: new 
Abstract: In this paper, we consider the nonlinear constrained optimization problem (NCP) with constraint set $\{x \in \mathcal{X}: c(x) = 0\}$, where $\mathcal{X}$ is a closed convex subset of $\mathbb{R}^n$. Building upon the forward-backward envelope framework for optimization over $\mathcal{X}$, we propose a forward-backward semi-envelope (FBSE) approach for solving (NCP). In the proposed semi-envelope approach, we eliminate the constraint $x \in \mathcal{X}$ through a specifically designed envelope scheme while preserving the constraint $x \in \mathcal{M} := \{x \in \mathbb{R}^n: c(x) = 0\}$. We establish that the forward-backward semi-envelope for (NCP) is well-defined and locally Lipschitz smooth over a neighborhood of $\mathcal{M}$. Furthermore, we prove that (NCP) and its corresponding forward-backward semi-envelope have the same first-order stationary points within a neighborhood of $\mathcal{X} \cap \mathcal{M}$. Consequently, our proposed forward-backward semi-envelope approach enables direct application of optimization methods over $\mathcal{M}$ while inheriting their convergence properties for (NCP). Additionally, we develop an inexact projected gradient descent method for minimizing the forward-backward semi-envelope over $\mathcal{M}$ and establish its global convergence. Preliminary numerical experiments demonstrate the practical efficiency and potential of our proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22223v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyin Hu, Xin Liu, Kim-Chuan Toh, Nachuan Xiao</dc:creator>
    </item>
    <item>
      <title>First-order majorization-minimization meets high-order majorant: Boosted inexact high-order forward-backward method</title>
      <link>https://arxiv.org/abs/2510.22231</link>
      <description>arXiv:2510.22231v1 Announce Type: new 
Abstract: This paper introduces a first-order majorization-minimization framework based on a high-order majorant for continuous functions, incorporating a non-quadratic regularization term of degree $p&gt;1$. Notably, it is shown to be valid if and only if the function is $p$-paraconcave, thus extending beyond Lipschitz and H\"{o}lder gradient continuity for $p \in (1,2]$, and implying concavity for $p&gt;2$. In the smooth setting, this majorant recovers a variant of the classical descent lemma with quadratic regularization. Building on this foundation, we develop a high-order inexact forward-backward algorithm (HiFBA) and its line-search-accelerated variant, named Boosted HiFBA. For convergence analysis, we introduce a high-order forward-backward envelope (HiFBE), which serves as a Lyapunov function. We establish subsequential convergence under suitable inexactness conditions, and we prove global convergence with linear rates for functions satisfying the Kurdyka-\L{}ojasiewicz inequality. Our preliminary experiments on linear inverse problems and regularized nonnegative matrix factorization highlight the efficiency of HiFBA and its boosted variant, demonstrating their potential for solving challenging nonconvex optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22231v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alireza Kabgani, Masoud Ahookhosh</dc:creator>
    </item>
    <item>
      <title>Actor-Critic Learning for Risk-Constrained Linear Quadratic Regulation</title>
      <link>https://arxiv.org/abs/2510.22267</link>
      <description>arXiv:2510.22267v1 Announce Type: new 
Abstract: In this paper, we investigate the infinite-horizon risk-constrained linear quadratic regulator problem (RC-QR), which augments the classical LQR formulation with a statistical constraint on the variability of the system state to incorporate risk awareness, a key requirement in safety-critical control applications. We propose an actor-critic learning algorithm that jointly performs policy evaluation and policy improvement in a model-free and online manner. The RC-QR problem is first reformulated as a max-min optimization problem, from which we develop a multi-time-scale stochastic approximation scheme. The critic employs temporal-difference learning to estimate the action-value function, the actor updates the policy parameters via a policy gradient step, and the dual variable is adapted through gradient ascent to enforce the risk constraint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22267v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Weijian Li, Andreas A. Malikopoulos</dc:creator>
    </item>
    <item>
      <title>Distributed Stochastic Proximal Algorithm on Riemannian Submanifolds for Weakly-convex Functions</title>
      <link>https://arxiv.org/abs/2510.22270</link>
      <description>arXiv:2510.22270v1 Announce Type: new 
Abstract: This paper aims to investigate the distributed stochastic optimization problems on compact embedded submanifolds (in the Euclidean space) for multi-agent network systems. To address the manifold structure, we propose a distributed Riemannian stochastic proximal algorithm framework by utilizing the retraction and Riemannian consensus protocol, and analyze three specific algorithms: the distributed Riemannian stochastic subgradient, proximal point, and prox-linear algorithms. When the local costs are weakly-convex and the initial points satisfy certain conditions, we show that the iterates generated by this framework converge to a nearly stationary point in expectation while achieving consensus. We further establish the convergence rate of the algorithm framework as $\mathcal{O}(\frac{1+\kappa_g}{\sqrt{k}})$ where $k$ denotes the number of iterations and $\kappa_g$ shows the impact of manifold geometry on the algorithm performance. Finally, numerical experiments are implemented to demonstrate the theoretical results and show the empirical performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22270v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jishu Zhao, Xi Wang, Jinlong Lei, Shixiang Chen</dc:creator>
    </item>
    <item>
      <title>An Interval Hessian-based line-search method for unconstrained nonconvex optimization</title>
      <link>https://arxiv.org/abs/2510.22342</link>
      <description>arXiv:2510.22342v1 Announce Type: new 
Abstract: Second-order Newton-type algorithms that leverage the exact Hessian or its approximation are central to solving nonlinear optimization problems. These algorithms have been proven to achieve a faster convergence rate than the first-order methods and can find second-order stationary points. However, their applications in solving large-scale nonconvex problems are hindered by three primary challenges: (1) the high computational cost associated with Hessian evaluations, (2) its inversion, and (3) ensuring descent direction at points where the Hessian becomes indefinite. We propose INTHOP, an interval Hessian-based optimization algorithm for nonconvex problems. Specifically, we propose a new search direction guaranteed to be descent and requiring Hessian evaluations and inversion only at specific iterations. The proposed search direction is based on approximating the original Hessian matrix by a positive-definite matrix. We prove that the difference between the approximate and exact Hessian is bounded within an interval. Accordingly, the approximate Hessian matrix is reused if the iterates are in the interval while computing the gradients at each iteration. We develop various algorithm variants based on the interval size updating methods and minimum eigenvalue computation methods. We apply the algorithm to an extensive set of test problems and compare its performance with steepest descent, quasi-Newton, and the Newton methods. We show empirically that our method solves more problems in fewer function and gradient evaluations than steepest descent and the quasi-Newton method. Compared to the Newton method, we illustrate that for nonconvex problems, we require substantially less O(n3) operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22342v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashutosh Sharma, Gauransh Dingwani, Nikhil Gupta, Vaishnavi Gupta, Ishan Bajaj</dc:creator>
    </item>
    <item>
      <title>Model-Free Optimization and Control of Rigid Body Dynamics: An Extremum Seeking for Vibrational Stabilization Approach</title>
      <link>https://arxiv.org/abs/2510.22402</link>
      <description>arXiv:2510.22402v1 Announce Type: new 
Abstract: In this paper, we introduce a model-free, real-time, dynamic optimization and control method for a class of rigid body dynamics. Our method is based on a recent extremum seeking control for vibrational stabilization (ESC-VS) approach that is applicable to a class of second-order mechanical systems. The new ESC-VS method is able to stabilize a rigid body dynamic system about the optimal state of an objective function that can be unknown expression-wise, but assessable through measurements; the ESC-VS is operable by using only one perturbation/vibrational signal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22402v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rohan Palanikumar, Ahmed A. Elgohary, Simone Martini, Sameh A. Eisa</dc:creator>
    </item>
    <item>
      <title>Extragradient Method for $(L_0, L_1)$-Lipschitz Root-finding Problems</title>
      <link>https://arxiv.org/abs/2510.22421</link>
      <description>arXiv:2510.22421v1 Announce Type: new 
Abstract: Introduced by Korpelevich in 1976, the extragradient method (EG) has become a cornerstone technique for solving min-max optimization, root-finding problems, and variational inequalities (VIs). Despite its longstanding presence and significant attention within the optimization community, most works focusing on understanding its convergence guarantees assume the strong L-Lipschitz condition. In this work, building on the proposed assumptions by Zhang et al. [2024b] for minimization and Vankov et al.[2024] for VIs, we focus on the more relaxed $\alpha$-symmetric $(L_0, L_1)$-Lipschitz condition. This condition generalizes the standard Lipschitz assumption by allowing the Lipschitz constant to scale with the operator norm, providing a more refined characterization of problem structures in modern machine learning. Under the $\alpha$-symmetric $(L_0, L_1)$-Lipschitz condition, we propose a novel step size strategy for EG to solve root-finding problems and establish sublinear convergence rates for monotone operators and linear convergence rates for strongly monotone operators. Additionally, we prove local convergence guarantees for weak Minty operators. We supplement our analysis with experiments validating our theory and demonstrating the effectiveness and robustness of the proposed step sizes for EG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22421v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Sayantan Choudhury, Nicolas Loizou</dc:creator>
    </item>
    <item>
      <title>Derivative-Free Sequential Quadratic Programming for Equality-Constrained Stochastic Optimization</title>
      <link>https://arxiv.org/abs/2510.22458</link>
      <description>arXiv:2510.22458v1 Announce Type: new 
Abstract: We consider solving nonlinear optimization problems with a stochastic objective and deterministic equality constraints, assuming that only zero-order information is available for both the objective and constraints, and that the objective is also subject to random sampling noise. Under this setting, we propose a Derivative-Free Stochastic Sequential Quadratic Programming (DF-SSQP) method. Due to the lack of derivative information, we adopt a simultaneous perturbation stochastic approximation (SPSA) technique to randomly estimate the gradients and Hessians of both the objective and constraints. This approach requires only a dimension-independent number of zero-order evaluations -- as few as eight -- at each iteration step. A key distinction between our derivative-free and existing derivative-based SSQP methods lies in the intricate random bias introduced into the gradient and Hessian estimates of the objective and constraints, brought by stochastic zero-order approximations. To address this issue, we introduce an online debiasing technique based on momentum-style estimators that properly aggregate past gradient and Hessian estimates to reduce stochastic noise, while avoiding excessive memory costs via a moving averaging scheme. Under standard assumptions, we establish the global almost-sure convergence of the proposed DF-SSQP method. Notably, we further complement the global analysis with local convergence guarantees by demonstrating that the rescaled iterates exhibit asymptotic normality, with a limiting covariance matrix resembling the minimax optimal covariance achieved by derivative-based methods, albeit larger due to the absence of derivative information. Our local analysis enables online statistical inference of model parameters leveraging DF-SSQP. Numerical experiments on benchmark nonlinear problems demonstrate both the global and local behavior of DF-SSQP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22458v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sen Na</dc:creator>
    </item>
    <item>
      <title>Tailoring Reproducing Kernels for Optimal Control via Policy Iteration</title>
      <link>https://arxiv.org/abs/2510.22494</link>
      <description>arXiv:2510.22494v1 Announce Type: new 
Abstract: This paper presents a novel approach to formulating the actor-critic method for optimal control by casting policy iteration in reproducing kernel Hilbert spaces (RKHSs -- also known as native spaces). By tailoring the reproducing kernel and RKHS to the dynamics of the nonlinear optimal control problem, we leverage recent advancements in characterizing error bounds from statistical and machine learning theory. These approximations define a general strategy to select the bases of the actor-critic networks, and we formally guarantee for the first time that this basis selection procedure leads to closed-form error bounds for the individual steps of policy iteration. These bounds often have a geometric and computable form, making them potentially useful for a priori or a posteriori evaluation of candidate collections of scattered bases. Numerical studies subsequently provide qualitative evidence of the practical performance achieved for the full recursion using the algorithms and theory developed for the single-step error bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22494v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shengyuan Niu, Ali Bouland, Haoran Wang, Filippos Fotiadis, Andrew Kurdila, Andrea L'Afflitto, Sai Tej Paruchuri, Kyriakos G. Vamvoudakis</dc:creator>
    </item>
    <item>
      <title>Cellular flow control design for mixing based on the least action principle</title>
      <link>https://arxiv.org/abs/2510.22703</link>
      <description>arXiv:2510.22703v1 Announce Type: new 
Abstract: We consider a novel approach for the enhancement of fluid mixing via pure stirring strategies building upon the Least Action Principle (LAP) for incompressible flows. The LAP is formally analogous to the Benamou--Brenier formulation of optimal transport, but imposes an incompressibility constraint. Our objective is to find a velocity field, generated by Hamiltonian flows, that minimizes the kinetic energy while ensuring that the initial scalar distribution reaches a prescribed degree of mixedness by a finite time. This formulation leads to a ``point to set" type of optimization problem which relaxes the requirement on controllability of the system compared to the classic LAP framework. In particular, we assume that the velocity field is induced by a finite set of cellular flows that can be controlled in time. We justify the feasibility of this constraint set and leverage Benamou--Brenier's results to establish the existence of a global optimal solution. Finally, we derive the corresponding optimality conditions for solving the optimal time control and conduct numerical experiments demonstrating the effectiveness of our control design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22703v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weiwei Hu, Ming-Jun Lai, Hao-Ning Wu</dc:creator>
    </item>
    <item>
      <title>The Iterates of Nesterov's Accelerated Algorithm Converge in The Critical Regimes</title>
      <link>https://arxiv.org/abs/2510.22715</link>
      <description>arXiv:2510.22715v1 Announce Type: new 
Abstract: In this paper, we prove that the iterates of the accelerated Nesterov's algorithm in the critical regime do converge in the weak topology to a global minimizer of an $L$-smooth function in a real Hilbert space, hence answering positively a conjecture posed by H. Attouch and co-authors a decade ago. This result is the algorithmic case of a very recent result on the continuous-time system posted by E. Ryu on X, with assistance from ChatGPT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22715v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Radu Ioan Bot, Jalal Fadili, Dang-Khoa Nguyen</dc:creator>
    </item>
    <item>
      <title>Feedback approximate controllability of blowup points for the heat equation with anti-interference blowup profile</title>
      <link>https://arxiv.org/abs/2510.22770</link>
      <description>arXiv:2510.22770v1 Announce Type: new 
Abstract: This paper is concerned with a feedback approximate controllability problem of blowup points for the heat equation. We show that the system is approximately controllable for blowup points with feedback controls and the feedback operator is bounded at any time before blowup. It is also proved that the blowup profile for feedback controllability of blowup points is stable with respect to initial data. That is, suppose that the initial data has a very small perturbation, the blowup profiles also have tiny changes. More precisely, it just undergoes a tiny translation in space and time. This means that our feedback strategy is anti-interference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22770v1</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ping Lin, Hatem Zaag</dc:creator>
    </item>
    <item>
      <title>Exploiting Electrolyzer Flexibility via Multiscale Model Predictive Control Cross Heterogeneous Energy Markets</title>
      <link>https://arxiv.org/abs/2510.22782</link>
      <description>arXiv:2510.22782v1 Announce Type: new 
Abstract: Green hydrogen production via electrolysis is crucial for decarbonization but faces significant economic hurdles primarily due to the high cost of the electricity. However, current electrolyzer-based hydrogen production processes predominantly rely on the single-scale Day-Ahead Market (DAM) for electricity procurement, failing to fully exploit the economic benefits offered by multi-scale electricity market that integrates both the DAM and the Real-Time Market (RTM), thereby eliminating the opportunity to reduce the overall cost. To mitigate this technical gap, this research investigates a dynamic operational strategy enabling electrolyzers to strategically navigate between the DAM and RTM to minimize net operation costs. Using a rolling horizon optimization framework to coordinate bidding and operation, we demonstrate a strategy where electrolyzers secure primary energy via exclusive DAM purchases, then actively engage the RTM to buy supplemental energy cheaply or, critically, sell procured DAM energy back at a profit during high RTM price periods. Our analysis reveals that this coordinated multi-scale electricity market participation strategy can dramatically reduce net electricity expenditures, achieving near-zero or even negative effective electricity costs for green hydrogen production under realistic market scenarios, effectively meaning the operation can profit from its electricity market interactions. By transforming electrolyzers from simple price-takers into active participants capable of arbitrage between market timescales, this approach unlocks a financially compelling pathway for green hydrogen, accelerating its deployment while simultaneously enhancing power grid flexibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22782v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhichao Chen (Department of Systems Engineering, City University of Hong Kong), Hongyuan Sheng (Department of Chemistry, Fudan University), Hao Wang (Department of Automation, Zhejiang University), Jiaze Ma (Department of Systems Engineering, City University of Hong Kong)</dc:creator>
    </item>
    <item>
      <title>Computing Binary Integer Programming via A New Exact Penalty Function</title>
      <link>https://arxiv.org/abs/2510.23209</link>
      <description>arXiv:2510.23209v1 Announce Type: new 
Abstract: Unconstrained binary integer programming (UBIP) poses significant computational challenges due to its discrete nature. We introduce a novel reformulation approach using a piecewise cubic function that transforms binary constraints into continuous equality constraints. Instead of solving the resulting constrained problem directly, we develop an exact penalty framework with a key theoretical advantage: the penalty parameter threshold ensuring exact equivalence is independent of the unknown solution set, unlike classical exact penalty theory. To facilitate the analysis of the penalty model, we introduce the concept of P-stationary points and systematically characterize their optimality properties and relationships with local and global minimizers. The P-stationary point enables the development of an efficient algorithm called APPA, which is guaranteed to converge to a P-stationary point within a finite number of iterations under a single mild assumption, namely, strong smoothness of the objective function over the unit box. Comprehensive numerical experiments demonstrate that APPA outperforms established solvers in both accuracy and efficiency across diverse problem instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23209v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuai Li, Shenglong Zhou</dc:creator>
    </item>
    <item>
      <title>Decoupled Solution for Composite Sparse-plus-Smooth Inverse Problems</title>
      <link>https://arxiv.org/abs/2510.23322</link>
      <description>arXiv:2510.23322v1 Announce Type: new 
Abstract: We consider composite linear inverse problems where the signal to recover is modeled as a sum of two functions. We study a variational framework formulated as an optimization problem over the pairs of components using two regularization terms, each of them acting on a different part of the solution. The specificity of our work is to study the case where one component is regularized with an atomic norm over a Banach space, which is known to promote sparse reconstruction, while the other is regularized with a quadratic norm over a Hilbert space, which promotes smooth solution.
  We show how this composite optimization problem can be reduced to an optimization problem over the Banach space component only up to a linear problem. This reveals a decoupling between the two components, allowing for a new composite representer theorem. It naturally induces a decoupled numerical procedure to solve the composite optimization problem.
  We exemplify our main result with a composite deconvolution problem of Dirac recovery over a smooth background. In this setting, we illustrate the relevance of a composite model and show a significant temporal gain on signal reconstruction, which results from our decoupled algorithmic approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23322v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Adrian Jarret, Julien Fageot</dc:creator>
    </item>
    <item>
      <title>Quadratic Truncated Random Return in Distributional LQR: Positive Definiteness, Density, and Log-Concavity</title>
      <link>https://arxiv.org/abs/2510.23332</link>
      <description>arXiv:2510.23332v1 Announce Type: new 
Abstract: Distributional linear quadratic regulator (LQR) is a new framework that integrates the distributional reinforcement learning and classical LQR, which offers a new way to study the random return instead of the expected cost. Unlike iterative approximation using dynamic programming in the DRL, a closed-form expression for the random return can be exactly characterized in the distributional LQR, which is defined over infinitely many random variables. In recent work [1, 2], it has been shown that this random return can be well approximated by a finite number of random variables, which we call truncated random return. In this paper, we study the truncated random return in the distributional LQR. We show that the truncated random return can be naturally expressed in the quadratic form. We develop a sufficient condition for the positive definiteness of the block symmetric matrix in the quadratic form and provide the lower and upper bounds on the eigenvalues of this matrix. We further show that in this case, the truncated random return follows a positively weighted non-central chi-square distribution if the random disturbances admits Gaussian, and its cumulative distribution function is log-concave if the probability density function of the random disturbances is log-concave.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23332v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruyi Teng, Dan Wang, Wei Chen, Yulong Gao</dc:creator>
    </item>
    <item>
      <title>Epsilon-Optimal Policies for Average-Cost Separable MDPs with Perturbations</title>
      <link>https://arxiv.org/abs/2510.23335</link>
      <description>arXiv:2510.23335v1 Announce Type: new 
Abstract: We study a class of infinite-horizon average-cost Markov Decision Processes (MDPs) whose reward and transition structures are nearly separable. For the totally separable baseline (that is, with no perturbation), we derive an explicit stationary decision rule that is exactly average-optimal. We then show that under an epsilon-perturbation of the separable structure, this policy remains epsilon-optimal, meaning that the loss in the average reward is of order O(epsilon).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23335v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dhairya Kantawala</dc:creator>
    </item>
    <item>
      <title>Flexibility aggregation via set projection for distribution grids with multiple interconnections</title>
      <link>https://arxiv.org/abs/2510.23352</link>
      <description>arXiv:2510.23352v1 Announce Type: new 
Abstract: With the increasing number of flexible energy devices in distribution grids, coordination between Transmission System Operators (TSOs) and Distribution System Operators (DSOs) becomes critical for optimal system operation. One form of coordination is to solve the overall system operation problem in a hierarchical way, computing Feasible Operational Regions (FORs) for the interconnection between TSO/DSO. Most methods for computing FORs rely on the assumption of only one interconnection point between TSO and DSOs, which is often violated in practice. In this work, we propose a method for computing FORs in distribution grids with multiple interconnection points to the transmission grid. We test our method in a grid with two interconnecting points and analyze the properties of the resulting high-dimensional FOR from a power systems perspective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23352v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ma\'isa Beraldo Bandeira, Alexander Engelmann, Timm Faulwasser</dc:creator>
    </item>
    <item>
      <title>A Sequential Planning Framework for the Operational Reality of Interacting Air Traffic Flow Regulations and Traffic Flow Programs</title>
      <link>https://arxiv.org/abs/2510.23402</link>
      <description>arXiv:2510.23402v1 Announce Type: new 
Abstract: Air Traffic Flow Management (ATFM) traffic regulations are being increasingly used as rising demand meets persistent workforce shortages. This operational strain has amplified a critical phenomenon that we call \emph{regulation cascading}: the compounding, non-linear interactions that occur when multiple regulations influence one another in unpredictable ways. As the number and complexity of regulations grow, cascading effects become more pronounced, undermining the network operator's ability to protect sectors reliably. To address this challenge, we introduce RegulationZero, a sequential planning framework that natively operates in the regulation space, optimizing over ordered sequences of flow-level regulations that remain fully compatible with existing slot-allocation systems such as CASA and RBS++. At its core, the method employs a hierarchical Monte Carlo Tree Search (MCTS) that first samples congestion hotspots and then selects candidate regulations synthesized by a local proposal engine. Each proposal is evaluated by a fast First-Planned-First-Served (FPFS) allocator to estimate its reward, with these feedbacks guiding the subsequent MCTS exploration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23402v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thinh Hoang, Daniel Delahaye</dc:creator>
    </item>
    <item>
      <title>A Newton-Kantorovich Inverse Function Theorem in Quasi-Metric Spaces</title>
      <link>https://arxiv.org/abs/2510.23431</link>
      <description>arXiv:2510.23431v1 Announce Type: new 
Abstract: The purpose of this work is to investigate root finding problems defined on (quasi-)metric spaces, and ranging in Euclidean spaces. The motivation for this line of inquiry stems from recent models in biology and phylogenetics, where problems of great practical significance are cast as optimization problems on (quasi-)metric spaces. We investigate a minimal algebraic setup that allows us to study a notion of differentiability suitable for Newton-type methods, called Newton differentiability. This notion of differentiability benefits from calculus rules and is sufficient to prove superlinear convergence of a Newton-type method. Finally, a Newton-Kantorovich-type theorem provides an inverse function result, applicable on (quasi-)metric spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23431v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Titus Pinta</dc:creator>
    </item>
    <item>
      <title>Individual Minima-Informed Multi-Objective Model Predictive Control for Fixed Point Stabilization</title>
      <link>https://arxiv.org/abs/2510.23454</link>
      <description>arXiv:2510.23454v1 Announce Type: new 
Abstract: Multi-objective model predictive control (MOMPC) for fixed point stabilization requires an automated a priori decision-making mechanism to translate a high-level preference into a single solution to be implemented. To this aim, we introduce an approach called individual minima-informed decision-making. This class of methods can be implemented through two sequential optimizations, regardless of the number of objectives, thereby improving the real-time capability of MOMPC. These methods operate on Pareto fronts and leverage the individual minima (IM), which are characteristic Pareto-optimal points. By this, we aim to produce a robust translation of a high-level preference to a suitable point on the Pareto front. However, guaranteeing the closed-loop stability of the resulting MOMPC scheme remains an open challenge.
  This paper addresses this gap by developing a novel MOMPC framework that integrates IM-informed decision-making while formally guaranteeing asymptotic stability. Our contribution is twofold. First, we propose and systematically analyze six variants of IM-informed decision-making methods -- including two novel methods -- designed to achieve the above-mentioned translation. Second, we embed these methods into a quasi-infinite horizon MOMPC framework and provide a rigorous proof of closed-loop asymptotic stability. The proof holds for any of the presented decision-making methods and relies on a descent condition that is less restrictive than those in prior literature. The practical applicability and effectiveness of the proposed framework are demonstrated in a numerical case study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23454v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Markus Herrmann-Wicklmayr, Kathrin Fla{\ss}kamp</dc:creator>
    </item>
    <item>
      <title>Point Convergence of Nesterov's Accelerated Gradient Method: An AI-Assisted Proof</title>
      <link>https://arxiv.org/abs/2510.23513</link>
      <description>arXiv:2510.23513v1 Announce Type: new 
Abstract: The Nesterov accelerated gradient method, introduced in 1983, has been a cornerstone of optimization theory and practice. Yet the question of its point convergence had remained open. In this work, we resolve this longstanding open problem in the affirmative. The discovery of the proof was heavily assisted by ChatGPT, a proprietary large language model, and we describe the process through which its assistance was</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23513v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Uijeong Jang, Ernest K. Ryu</dc:creator>
    </item>
    <item>
      <title>Approximately optimal distributed controls for high-dimensional stochastic systems with pairwise interaction through controls</title>
      <link>https://arxiv.org/abs/2510.23537</link>
      <description>arXiv:2510.23537v1 Announce Type: new 
Abstract: This paper investigates large-population stochastic control problems in which agents share their state information and cooperate to minimize a convex cost functional. The latter is decomposed into individual and coupling costs, with the distinctive feature that the coupling term is a pairwise interaction function between the controls. To address this setting, we follow closely (Jackson &amp; Lacker, 2025): we introduce a related problem where each agent observes only its own state. We then establish a quantitative bound on the difference between the value functions associated with these two problems. We obtain this result by reformulating the problems analytically as Hamilton-Jacobi type equations and comparing their associated Hamiltonians. The main difficulty of our approach lies in establishing a precise comparison between the distributions of the corresponding optimal controls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23537v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elise Devey</dc:creator>
    </item>
    <item>
      <title>From Zonal to Nodal Capacity Expansion Planning: Spatial Aggregation Impacts on a Realistic Test-Case</title>
      <link>https://arxiv.org/abs/2510.23586</link>
      <description>arXiv:2510.23586v1 Announce Type: new 
Abstract: Solving power system capacity expansion planning (CEP) problems at realistic spatial resolutions is computationally challenging. Thus, a common practice is to solve CEP over zonal models with low spatial resolution rather than over full-scale nodal power networks. Due to improvements in solving large-scale stochastic mixed integer programs, these computational limitations are becoming less relevant, and the assumption that zonal models are realistic and useful approximations of nodal CEP is worth revisiting. This work is the first to conduct a systematic computational study on the assumption that spatial aggregation can reasonably be used for ISO- and interconnect-scale CEP. By considering a realistic, large-scale test network based on the state of California with over 8,000 buses and 10,000 transmission lines, we demonstrate that well-designed small spatial aggregations can yield good approximations but that coarser zonal models result in large distortions of investment decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23586v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Elizabeth Glista, Bernard Knueven, Jean-Paul Watson</dc:creator>
    </item>
    <item>
      <title>MARS-M: When Variance Reduction Meets Matrices</title>
      <link>https://arxiv.org/abs/2510.21800</link>
      <description>arXiv:2510.21800v1 Announce Type: cross 
Abstract: Matrix-based preconditioned optimizers, such as Muon, have recently been shown to be more efficient than scalar-based optimizers for training large-scale neural networks, including large language models (LLMs). On the other hand, recent benchmarks on optimizers for LLM pre-training have demonstrated that variance-reduction techniques such as MARS can achieve substantial speedups over standard optimizers that do not employ variance reduction. In this paper, to achieve the best of both worlds, we introduce MARS-M, a new optimizer that integrates the variance reduction technique in MARS with Muon. Under standard regularity conditions, we prove that Muon-M converges to a first-order stationary point at a rate of $\tilde{\mathcal{O}}(T^{-1/3})$, which improves upon $\tilde{\mathcal{O}}(T^{-1/4})$ rate attained by Muon. Our empirical results on language modeling and computer vision tasks demonstrate that MARS-M consistently yields lower losses and improved performance across various downstream benchmarks. The implementation of MARS-M is available at https://github.com/AGI-Arena/MARS/MARS_M.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21800v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifeng Liu, Angela Yuan, Quanquan Gu</dc:creator>
    </item>
    <item>
      <title>An Introductory Guide to Koopman Learning</title>
      <link>https://arxiv.org/abs/2510.22002</link>
      <description>arXiv:2510.22002v1 Announce Type: cross 
Abstract: Koopman operators provide a linear framework for data-driven analyses of nonlinear dynamical systems, but their infinite-dimensional nature presents major computational challenges. In this article, we offer an introductory guide to Koopman learning, emphasizing rigorously convergent data-driven methods for forecasting and spectral analysis. We provide a unified account of error control via residuals in both finite- and infinite-dimensional settings, an elementary proof of convergence for generalized Laplace analysis -- a variant of filtered power iteration that works for operators with continuous spectra and no spectral gaps -- and review state-of-the-art approaches for computing continuous spectra and spectral measures. The goal is to provide both newcomers and experts with a clear, structured overview of reliable data-driven techniques for Koopman spectral analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22002v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <category>math.SP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew J. Colbrook, Zlatko Drma\v{c}, Andrew Horning</dc:creator>
    </item>
    <item>
      <title>Solving Continuous Mean Field Games: Deep Reinforcement Learning for Non-Stationary Dynamics</title>
      <link>https://arxiv.org/abs/2510.22158</link>
      <description>arXiv:2510.22158v1 Announce Type: cross 
Abstract: Mean field games (MFGs) have emerged as a powerful framework for modeling interactions in large-scale multi-agent systems. Despite recent advancements in reinforcement learning (RL) for MFGs, existing methods are typically limited to finite spaces or stationary models, hindering their applicability to real-world problems. This paper introduces a novel deep reinforcement learning (DRL) algorithm specifically designed for non-stationary continuous MFGs. The proposed approach builds upon a Fictitious Play (FP) methodology, leveraging DRL for best-response computation and supervised learning for average policy representation. Furthermore, it learns a representation of the time-dependent population distribution using a Conditional Normalizing Flow. To validate the effectiveness of our method, we evaluate it on three different examples of increasing complexity. By addressing critical limitations in scalability and density approximation, this work represents a significant advancement in applying DRL techniques to complex MFG problems, bringing the field closer to real-world multi-agent systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22158v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Magnino, Kai Shao, Zida Wu, Jiacheng Shen, Mathieu Lauri\`ere</dc:creator>
    </item>
    <item>
      <title>Graph-Coarsening Approach for the Capacitated Vehicle Routing Problem with Time Windows</title>
      <link>https://arxiv.org/abs/2510.22329</link>
      <description>arXiv:2510.22329v1 Announce Type: cross 
Abstract: The Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) is a fundamental NP-hard optimization problem in logistics. Solving large-scale instances remains computationally challenging for exact solvers. This work introduces a multilevel graph coarsening and refinement framework that aggregates customers into meta-nodes using a spatio-temporal distance metric. The reduced problem is solved with classical heuristics and subsequently expanded back into the original space with feasibility corrections. Preliminary experiments on Solomon benchmark instances show that the proposed method reduces computation time while preserving or improving solution quality, particularly with respect to capacity and time window constraints. The paper also explores the integration of quantum-inspired optimization techniques, highlighting their potential to further accelerate large-scale vehicle routing tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22329v1</guid>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mustafa Mert \"Ozy{\i}lmaz</dc:creator>
    </item>
    <item>
      <title>Genetic Optimization of a Software-Defined GNSS Receiver</title>
      <link>https://arxiv.org/abs/2510.22417</link>
      <description>arXiv:2510.22417v1 Announce Type: cross 
Abstract: Commercial off-the-shelf (COTS) Global Navigation Satellite System (GNSS) receivers face significant limitations under high-dynamic conditions, particularly in high-acceleration environments such as those experienced by launch vehicles. These performance degradations, often observed as discontinuities in the navigation solution, arise from the inability of traditional tracking loop bandwidths to cope with rapid variations in synchronization parameters. Software-Defined Radio (SDR) receivers overcome these constraints by enabling flexible reconfiguration of tracking loops; however, manual tuning involves a complex, multidimensional search and seldom ensures optimal performance. This work introduces a genetic algorithm-based optimization framework that autonomously explores the receiver configuration space to determine optimal loop parameters for phase, frequency, and delay tracking. The approach is validated within an SDR environment using realistically simulated GPS L1 signals for three representative dynamic regimes -guided rocket flight, Low Earth Orbit (LEO) satellite, and static receiver-processed with the open-source GNSS-SDR architecture. Results demonstrate that evolutionary optimization enables SDR receivers to maintain robust and accurate Position, Velocity, and Time (PVT) solutions across diverse dynamic conditions. The optimized configurations yielded maximum position and velocity errors of approximately 6 m and 0.08 m/s for the static case, 12 m and 2.5 m/s for the rocket case, and 5 m and 0.2 m/s for the LEO case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22417v1</guid>
      <category>eess.SP</category>
      <category>cs.NE</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Laura Train, Rodrigo Castellanos, Miguel G\'omez-L\'opez</dc:creator>
    </item>
    <item>
      <title>A Novel Discrete-time Model of Information Diffusion on Social Networks Considering Users Behavior</title>
      <link>https://arxiv.org/abs/2510.22501</link>
      <description>arXiv:2510.22501v1 Announce Type: cross 
Abstract: In this paper, we introduce the SDIR (Susceptible-Delayable-Infected-Recovered) model, an extension of the classical SIR epidemic framework, to provide a more explicit characterization of user behavior in online social networks. The newly merged state D (delayable) represents users who have received the information but delayed its spreading and may eventually choose not to share it at all. Based on the mean-field approximation method, we derive the dynamical equations of the model and investigate its convergence and stability conditions. Under these conditions, we further propose an approximation algorithm for the edge-deletion problem, aiming to minimize the influence of information diffusion by identifying approximate solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22501v1</guid>
      <category>cs.SI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tran Van Khanh, Do Xuan Cho, Hoang Phi Dung</dc:creator>
    </item>
    <item>
      <title>Transmission Neural Networks: Approximate Receding Horizon Control for Virus Spread on Networks</title>
      <link>https://arxiv.org/abs/2510.22871</link>
      <description>arXiv:2510.22871v1 Announce Type: cross 
Abstract: Transmission Neural Networks (TransNNs) pro- posed by Gao and Caines (2022) serve as both virus spread models over networks and neural network models with tuneable activation functions. This paper establishes that TransNNs provide upper bounds on the infection probability generated from the associated Markovian stochastic Susceptible-Infected- Susceptible (SIS) model with 2^n state configurations where n is the number of nodes in the network, and can be employed as an approximate model for the latter. Based on such an approximation, a TransNN-based receding horizon control approach for mitigating virus spread is proposed and we demonstrate that it allows significant computational savings compared to the dynamic programming solution to Markovian SIS model with 2^n state configurations, as well as providing less conservative control actions compared to the TransNN- based optimal control. Finally, numerical comparisons among (a) dynamic programming solutions for the Markovian SIS model, (b) TransNN-based optimal control and (c) the proposed TransNN-based receding horizon control are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22871v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuang Gao, Peter E. Caines</dc:creator>
    </item>
    <item>
      <title>An Error-Based Safety Buffer for Safe Adaptive Control (Extended Version)</title>
      <link>https://arxiv.org/abs/2510.23491</link>
      <description>arXiv:2510.23491v1 Announce Type: cross 
Abstract: We consider the problem of adaptive control of a class of feedback linearizable plants with matched parametric uncertainties whose states are accessible, subject to state constraints, which often arise due to safety considerations. In this paper, we combine adaptation and control barrier functions into a real-time control architecture that guarantees stability, ensures control performance, and remains safe even with the parametric uncertainties. Two problems are considered, differing in the nature of the parametric uncertainties. In both cases, the control barrier function is assumed to have an arbitrary relative degree. In addition to guaranteeing stability, it is proved that both the control objective and safety objective are met with near-zero conservatism. No excitation conditions are imposed on the command signal. Simulation results demonstrate the non-conservatism of all of the theoretical developments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23491v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter A. Fisher, Johannes Autenrieb, Anuradha M. Annaswamy</dc:creator>
    </item>
    <item>
      <title>Accelerated Gradient Methods for Nonconvex Optimization: Escape Trajectories From Strict Saddle Points and Convergence to Local Minima</title>
      <link>https://arxiv.org/abs/2307.07030</link>
      <description>arXiv:2307.07030v2 Announce Type: replace 
Abstract: This paper considers the problem of understanding the behavior of a general class of accelerated gradient methods on smooth nonconvex functions. Motivated by some recent works that have proposed effective algorithms, based on Polyak's heavy ball method and the Nesterov accelerated gradient method, to achieve convergence to a local minimum of nonconvex functions, this work proposes a broad class of Nesterov-type accelerated methods and puts forth a rigorous study of these methods encompassing the escape from saddle points and convergence to local minima through both an asymptotic and a non-asymptotic analysis. In the asymptotic regime, this paper answers an open question of whether Nesterov's accelerated gradient method (NAG) with variable momentum parameter avoids strict saddle points almost surely. This work also develops two metrics of asymptotic rates of convergence and divergence, and evaluates these two metrics for several popular standard accelerated methods such as the NAG and Nesterov's accelerated gradient with constant momentum (NCM) near strict saddle points. In the non-asymptotic regime, this work provides an analysis that leads to the "linear" exit time estimates from strict saddle neighborhoods for trajectories of these accelerated methods as well the necessary conditions for the existence of such trajectories. Finally, this work studies a sub-class of accelerated methods that can converge in convex neighborhoods of nonconvex functions with a near optimal rate to a local minimum and at the same time this sub-class offers superior saddle-escape behavior compared to that of NAG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.07030v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rishabh Dixit, Mert Gurbuzbalaban, Waheed U. Bajwa</dc:creator>
    </item>
    <item>
      <title>The Boosted Difference of Convex Functions Algorithm for Value-at-Risk Constrained Portfolio Optimization</title>
      <link>https://arxiv.org/abs/2402.09194</link>
      <description>arXiv:2402.09194v2 Announce Type: replace 
Abstract: A highly relevant problem of modern finance is the design of Value-at-Risk (VaR) optimal portfolios. Due to contemporary financial regulations, banks and other financial institutions are tied to use the risk measure to control their credit, market, and operational risks. Despite its practical relevance, the non-convexity induced by VaR constraints in portfolio optimization problems remains a major challenge. To address this complexity more effectively, this paper proposes the use of the Boosted Difference-of-Convex Functions Algorithm (BDCA) to approximately solve a Markowitz-style portfolio selection problem with a VaR constraint. As one of the key contributions, we derive a novel line search framework that allows the application of the algorithm to Difference-of-Convex functions (DC) programs where both components are non-smooth. Moreover, we prove that the BDCA linearly converges to a Karush-Kuhn-Tucker point for the problem at hand using the Kurdyka-Lojasiewicz property. We also outline that this result can be generalized to a broader class of piecewise-linear DC programs with linear equality and inequality constraints. In the practical part, extensive numerical experiments under consideration of best practices then demonstrate the robustness of the BDCA under challenging constraint settings and adverse initialization. In particular, the algorithm consistently identifies the highest number of feasible solutions even under the most challenging conditions, while other approaches from chance-constrained programming lead to a complete failure in these settings. Due to the open availability of all data sets and code, this paper further provides a practical guide for transparent and easily reproducible comparisons of VaR-constrained portfolio selection problems in Python.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09194v2</guid>
      <category>math.OC</category>
      <category>q-fin.PM</category>
      <category>q-fin.RM</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marah-Lisanne Thormann, Phan Tu Vuong, Alain B. Zemkoho</dc:creator>
    </item>
    <item>
      <title>Funnel Synthesis via LMI Copositivity Conditions for Nonlinear Systems</title>
      <link>https://arxiv.org/abs/2402.15629</link>
      <description>arXiv:2402.15629v3 Announce Type: replace 
Abstract: Funnel synthesis refers to a procedure for synthesizing a time-varying controlled invariant set and an associated control law around a nominal trajectory. The computation of the funnel involves solving a continuous-time differential equation or inequality, ensuring the invariance of the funnel. Previous approaches often compromise the invariance property of the funnel; for example, they may enforce the equation or the inequality only at discrete temporal nodes and do not have a formal guarantee of invariance at all times. This paper proposes a computational funnel synthesis method that can satisfy the invariance of the funnel without such compromises. We derive a finite number of linear matrix inequalities (LMIs) that imply the satifaction of a continuous-time differential linear matrix inequality guaranteeing the invariance of the funnel at all times from the initial to the final time. To this end, we utilize LMI conditions ensuring matrix copositivity, which then imply continuous-time invariance. The primary contribution of the paper is to prove that the resulting funnel is indeed invariant over a finite time horizon. We validate the proposed method via a three-dimensional trajectory planning and control problem with obstacle avoidance constraints, and a six-degree-of-freedom powered descent guidance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15629v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Taewan Kim, Beh\c{c}et A\c{c}{\i}kme\c{s}e</dc:creator>
    </item>
    <item>
      <title>Interpolating between Optimal Transport and KL regularized Optimal Transport using R\'enyi Divergences</title>
      <link>https://arxiv.org/abs/2404.18834</link>
      <description>arXiv:2404.18834v3 Announce Type: replace 
Abstract: Regularized optimal transport (OT) has received much attention in recent years starting from Cuturi's introduction of Kullback-Leibler (KL) divergence regularized OT. In this paper, we propose regularizing the OT problem using the family of $\alpha$-R\'enyi divergences for $\alpha \in (0, 1)$. R\'enyi divergences are neither $f$-divergences nor Bregman distances, but they recover the KL divergence in the limit $\alpha \nearrow 1$. The advantage of introducing the additional parameter $\alpha$ is that for $\alpha \searrow 0$ we obtain convergence to the unregularized OT problem. For the KL regularized OT problem, this was achieved by letting the regularization parameter $\varepsilon$ tend to zero, which causes numerical instabilities. We present two different ways to obtain premetrics on probability measures, namely by R\'enyi divergence constraints and by penalization. The latter premetric interpolates between the unregularized and the KL regularized OT problem with weak convergence of the unique minimizer, generalizing the interpolation property of KL regularized OT. We use a nested mirror descent algorithm to solve the primal formulation. Both on real and synthetic data sets R\'enyi regularized OT plans outperform their KL and Tsallis counterparts in terms of being closer to the unregularized transport plans and recovering the ground truth in inference tasks better.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18834v3</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.FA</category>
      <category>math.NA</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonas Bresch, Viktor Stein</dc:creator>
    </item>
    <item>
      <title>The Internal Model Principle of Time-Varying Optimization</title>
      <link>https://arxiv.org/abs/2407.08037</link>
      <description>arXiv:2407.08037v3 Announce Type: replace 
Abstract: Time-varying optimization problems are central to many engineering applications, where performance metrics and system constraints evolve dynamically with time. Several algorithms have been proposed to address these problems; a common characteristic among them is their implicit reliance on knowledge of the optimizers' temporal variability. In this paper, we provide a fundamental characterization of this property: we show that an algorithm can track time-varying optimizers if and only if it incorporates a model of the temporal variability of the optimization problem. We refer to this concept as the internal model principle of time-varying optimization. Our analysis relies on showing that time-varying optimization problems can be recast as output regulation problems and, by using tools from center manifold theory, we establish necessary and sufficient conditions for exact asymptotic tracking. As a result, these findings enable the design of new algorithms for time-varying optimization. We demonstrate the effectiveness of the approach through numerical experiments on both synthetic problems and the dynamic traffic assignment problem from traffic control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08037v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gianluca Bianchin, Bryan Van Scoy</dc:creator>
    </item>
    <item>
      <title>Efficient points in a sum of sets of alternatives</title>
      <link>https://arxiv.org/abs/2412.16331</link>
      <description>arXiv:2412.16331v2 Announce Type: replace 
Abstract: The concept of efficiency plays a prominent role in the formal solution of decision problems that involve incomparable alternatives. This paper develops necessary and sufficient conditions for the efficient points in a sum of sets of alternatives to be identical to the efficient points in one of the summands. Some of the conditions cover both finite and infinite sets; others are shown to hold only for finite sets. Examples are provided that illustrate these results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16331v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anas Mifrani</dc:creator>
    </item>
    <item>
      <title>A Fast and Convergent Algorithm for Unassigned Distance Geometry Problems</title>
      <link>https://arxiv.org/abs/2502.02280</link>
      <description>arXiv:2502.02280v3 Announce Type: replace 
Abstract: In this paper, we propose a fast and convergent algorithm to solve unassigned distance geometry problems (uDGP). Technically, we construct a novel quadratic measurement model by leveraging $\ell_0$-norm instead of $\ell_1$-norm in the literature. To solve the nonconvex model, we establish its optimality conditions and develop a fast iterative hard thresholding (IHT) algorithm. Theoretically, we rigorously prove that the whole generated sequence converges to the L-stationary point with the help of the Kurdyka-Lojasiewicz (KL) property. Numerical studies on the turnpike and beltway problems validate its superiority over existing $\ell_1$-norm-based method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02280v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jun Fan, Xiaoya Shan, Xianchao Xiu</dc:creator>
    </item>
    <item>
      <title>Extremum Seeking with High-Order Lie Bracket Approximations: Achieving Exponential Decay Rate</title>
      <link>https://arxiv.org/abs/2504.01136</link>
      <description>arXiv:2504.01136v3 Announce Type: replace 
Abstract: This paper focuses on the further development of the Lie bracket approximation approach for extremum seeking systems. Classical results in this area provide extremum seeking algorithms with exponential convergence rates for quadratic-like cost functions, and polynomial decay rates for cost functions of higher degrees. This paper proposes a novel control design approach that ensures the motion of the extremum seeking system along directions associated with higher-order Lie brackets, thereby ensuring exponential convergence for cost functions that are polynomial-like but with degree greater than two.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01136v3</guid>
      <category>math.OC</category>
      <category>math.DS</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victoria Grushkovskaya, Sameh A. Eisa</dc:creator>
    </item>
    <item>
      <title>Temporal Robustness in Discrete Time Linear Dynamical Systems</title>
      <link>https://arxiv.org/abs/2505.02347</link>
      <description>arXiv:2505.02347v3 Announce Type: replace 
Abstract: Discrete time linear dynamical systems, including Markov chains, have found many applications including in security settings such as in cybersecurity operations center (CSOC) management and in managing health risks. However, in these two scenarios, there is uncertainty about the time horizon for which the system runs. This creates uncertainty about the cost (or reward) incurred based on the state distribution when the system stops. Given past data samples of how long a system ran, we theoretically analyze the cost incurred at the stop of the system as a distributional robust cost estimation task in a Wasserstein ambiguity set. Towards this, we show an equivalence between a discrete time Markov Chain on a probability simplex and a global asymptotic stable (GAS) discrete time linear dynamical system, allowing us to base our study on a GAS system only. Then, we provide various polynomial time algorithms and hardness results for different cases in our theoretical study, including a novel proof of a fundamental result about Wassertein distance based polytope. We experiment with real world data in CSOC domain and prior data in health domain to reveal the benefits of our model and approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02347v3</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nilava Metya, Ankit Shah, Arunesh Sinha</dc:creator>
    </item>
    <item>
      <title>Strong and weak quantitative estimates in slow-fast diffusions using filtering techniques</title>
      <link>https://arxiv.org/abs/2505.07093</link>
      <description>arXiv:2505.07093v2 Announce Type: replace 
Abstract: The behavior of slow-fast diffusions as the separation of scale diverges is a well-studied problem in the literature. In this short paper, we revisit this problem and obtain a new proof of existing strong quantitative convergence estimates (in particular, $L^2$ estimates) and weak convergence estimates in terms of $n$ (the parameter associated with the separation of scales). In particular, we obtain the rate of $n^{-\frac{1}{2}}$ in the strong convergence estimates and the rate of $n^{-1}$ for weak convergence estimate which are already known to be optimal in the literature. We achieve this using nonlinear filtering theory where we represent the evolution of fast diffusion in terms of its conditional distribution given the slow diffusion. We then use the well-known Kushner-Stratanovich equation which gives the evolution of the conditional distribution of the fast diffusion given the slow diffusion and establish that this conditional distribution approaches the invariant measure of the ``frozen" diffusion (obtained by freezing the slow variable in the evolution equation of the fast diffusion). At the heart of the analysis lies a key estimate of a weighted Lipschitz distance like function between a generic one-parameter family of measures and the family of unique invariant measures (of the ``frozen" diffusion parametrized by a path). This estimate is in terms of the operator norm of the dual of the infinitesimal generator of the ``frozen" diffusion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07093v2</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sumith Reddy Anugu, Vivek S. Borkar</dc:creator>
    </item>
    <item>
      <title>Adaptive Acceleration Without Strong Convexity Priors Or Restarts</title>
      <link>https://arxiv.org/abs/2506.13033</link>
      <description>arXiv:2506.13033v3 Announce Type: replace 
Abstract: A longstanding challenge in optimization is achieving optimal performance when the strong convexity parameter m is unknown. In this paper, we propose NAG-free, a simple extension of Nesterov's accelerated gradient (NAG) which is the first method capable of estimating m directly, without priors or restarts. Our estimator is inexpensive: it requires no additional function or gradient evaluations, only the storage of one extra iterate and gradient already computed by NAG. We prove that, by estimating the smoothness parameter L via backtracking, NAG-free converges globally at least as fast as gradient descent. We also prove that, given an upper bound on L, NAG-free achieves accelerated convergence locally near the minimum under local smoothness of the Hessian and some mild additional assumptions. Finally, we present experiments with smooth and nonsmooth Hessians on both synthetic and real-world data which demonstrate that NAG-free is competitive with restart methods, and naturally adapts to favorable local curvature conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13033v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joao V. Cavalcanti, Laurent Lessard, Ashia C. Wilson</dc:creator>
    </item>
    <item>
      <title>Tikhonov regularized second-order dynamics with Hessian-driven damping for solving convex optimization problems</title>
      <link>https://arxiv.org/abs/2506.15968</link>
      <description>arXiv:2506.15968v2 Announce Type: replace 
Abstract: This paper deals with a Tikhonov regularized second-order dynamical system that incorporates time scaling, asymptotically vanishing damping and Hessian-driven damping for solving convex optimization problems. Under appropriate setting of the parameters, we first obtain fast convergence results of the function value along the trajectory generated by the dynamical system. Then, we show that the trajectory generated by the dynamical system converges weakly to a minimizer of the convex optimization problem. We also demonstrate that, by properly tuning these parameters, both fast convergence rates of the function value and strong convergence of the trajectory towards the minimum norm solution of the convex optimization problem can be achieved simultaneously. Furthermore, we study convergence properties of an inertial proximal gradient algorithm obtained by the temporal discretization of the dynamical system. Finally, we present numerical experiments to illustrate the obtained results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15968v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiangkai Sun, Guoxiang Tian, Huan Zhang</dc:creator>
    </item>
    <item>
      <title>Affine-Invariant Global Non-Asymptotic Convergence Analysis of BFGS under Self-Concordance</title>
      <link>https://arxiv.org/abs/2507.00361</link>
      <description>arXiv:2507.00361v2 Announce Type: replace 
Abstract: In this paper, we establish global non-asymptotic convergence guarantees for the BFGS quasi-Newton method without requiring strong convexity or the Lipschitz continuity of the gradient or Hessian. Instead, we consider the setting where the objective function is strictly convex and strongly self-concordant. For an arbitrary initial point and any arbitrary positive-definite initial Hessian approximation, we prove global linear and superlinear convergence guarantees for BFGS when the step size is determined using a line search scheme satisfying the weak Wolfe conditions. Moreover, all our global guarantees are affine-invariant, with the convergence rates depending solely on the initial error and the strongly self-concordant constant. Our results extend the global non-asymptotic convergence theory of BFGS beyond traditional assumptions and, for the first time, establish affine-invariant convergence guarantees aligning with the inherent affine invariance of the BFGS method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00361v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiujiang Jin, Aryan Mokhtari</dc:creator>
    </item>
    <item>
      <title>Quantum Alternating Direction Method of Multipliers for Semidefinite Programming</title>
      <link>https://arxiv.org/abs/2510.10056</link>
      <description>arXiv:2510.10056v2 Announce Type: replace 
Abstract: Semidefinite programming (SDP) is a fundamental convex optimization problem with wide-ranging applications. However, solving large-scale instances remains computationally challenging due to the high cost of solving linear systems and performing eigenvalue decompositions. In this paper, we present a quantum alternating direction method of multipliers (QADMM) for SDPs, building on recent advances in quantum computing. An inexact ADMM framework is developed, which tolerates errors in the iterates arising from block-encoding approximation and quantum measurement. Within this robust scheme, we design a polynomial proximal operator to address the semidefinite conic constraints and apply the quantum singular value transformation to accelerate the most costly projection updates. We prove that the scheme converges to an $\epsilon$-optimal solution of the SDP problem under the strong duality assumption. A detailed complexity analysis shows that the QADMM algorithm achieves favorable scaling with respect to dimension compared to the classical ADMM algorithm and quantum interior point methods, highlighting its potential for solving large-scale SDPs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10056v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hantao Nie, Dong An, Zaiwen Wen</dc:creator>
    </item>
    <item>
      <title>Scheduling multiple agile Earth observation satellites with multiple observations</title>
      <link>https://arxiv.org/abs/1812.00203</link>
      <description>arXiv:1812.00203v2 Announce Type: replace-cross 
Abstract: The Earth observation satellites (EOSs) are specially designed to collect images according to user requirements. The agile EOSs (AEOS), with stronger attitude maneuverability, greatly improve the observation capability, while increasing the complexity in scheduling. We address a multiple AEOSs scheduling with multiple observations for the first time}, where the objective function aims to maximize the entire observation profit over a fixed horizon. The profit attained by multiple observations for each target is nonlinear to the number of observations. We model the multiple AEOSs scheduling as a specific interval scheduling problem with each satellite orbit respected as machine. Then A column generation based framework is developed to solve this problem, in which we deal with the pricing problems with a label-setting algorithm. Extensive simulations are conducted on the basis of a China's AEOS constellation, and the results indicate the optimality gap is less than 3% on average, which validates the performance of the scheduling solution obtained by the proposed framework. We also compare the framework in the conventional EOS scheduling.</description>
      <guid isPermaLink="false">oai:arXiv.org:1812.00203v2</guid>
      <category>astro-ph.IM</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.asr.2025.10.042</arxiv:DOI>
      <dc:creator>Xinwei Wang, Chao Han, Roel Leus</dc:creator>
    </item>
    <item>
      <title>Faster Reinforcement Learning by Freezing Slow States</title>
      <link>https://arxiv.org/abs/2301.00922</link>
      <description>arXiv:2301.00922v4 Announce Type: replace-cross 
Abstract: We study infinite horizon Markov decision processes (MDPs) with "fast-slow" structure, where some state variables evolve rapidly ("fast states") while others change more gradually ("slow states"). This structure commonly arises in practice when decisions must be made at high frequencies over long horizons, and where slowly changing information still plays a critical role in determining optimal actions. Examples include inventory control under slowly changing demand indicators or dynamic pricing with gradually shifting consumer behavior. Modeling the problem at the natural decision frequency leads to MDPs with discount factors close to one, making them computationally challenging. We propose a novel approximation strategy that "freezes" slow states during phases of lower-level planning and subsequently applies value iteration to an auxiliary upper-level MDP that evolves on a slower timescale. Freezing states for short periods of time leads to easier-to-solve lower-level problems, while a slower upper-level timescale allows for a more favorable discount factor. On the theoretical side, we analyze the regret incurred by our frozen-state approach, which leads to simple insights on how to trade off regret versus computational cost. Empirically, we benchmark our new frozen-state methods on three domains, (i) inventory control with fixed order costs, (ii) a gridworld problem with spatial tasks, and (iii) dynamic pricing with reference-price effects. We demonstrate that the new methods produce high-quality policies with significantly less computation, and we show that simply omitting slow states is often a poor heuristic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.00922v4</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yijia Wang, Daniel R. Jiang</dc:creator>
    </item>
    <item>
      <title>Dynamic financial processes identification using sparse regressive reservoir computers</title>
      <link>https://arxiv.org/abs/2310.12144</link>
      <description>arXiv:2310.12144v2 Announce Type: replace-cross 
Abstract: In this document, we present key findings in structured matrix approximation theory, with applications to the regressive representation of dynamic financial processes. Initially, we explore a comprehensive approach involving generic nonlinear time delay embedding for time series data extracted from a financial or economic system under examination. Subsequently, we employ sparse least-squares and structured matrix approximation methods to discern approximate representations of the output coupling matrices. These representations play a pivotal role in establishing the regressive models corresponding to the recursive structures inherent in a given financial system. The document further introduces prototypical algorithms that leverage the aforementioned techniques. These algorithms are demonstrated through applications in approximate identification and predictive simulation of dynamic financial and economic processes, encompassing scenarios that may or may not exhibit chaotic behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.12144v2</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fredy Vides, Idelfonso B. R. Nogueira, Gabriela Lopez Gutierrez, Lendy Banegas, Evelyn Flores</dc:creator>
    </item>
    <item>
      <title>Non-local Hamilton-Jacobi-Bellman equations for the stochastic optimal control of path-dependent piecewise deterministic processes</title>
      <link>https://arxiv.org/abs/2408.02147</link>
      <description>arXiv:2408.02147v2 Announce Type: replace-cross 
Abstract: We study the optimal control of path-dependent piecewise deterministic processes. An appropriate dynamic programming principle is established. We prove that the associated value function is the unique minimax solution of the corresponding non-local path-dependent Hamilton-Jacobi-Bellman equation. This is the first well-posedness result for nonsmooth solutions of fully nonlinear non-local path-dependent partial differential equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02147v2</guid>
      <category>math.PR</category>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.spa.2025.104813</arxiv:DOI>
      <dc:creator>Elena Bandini, Christian Keller</dc:creator>
    </item>
    <item>
      <title>Painless Federated Learning: An Interplay of Line-Search and Extrapolation</title>
      <link>https://arxiv.org/abs/2408.17145</link>
      <description>arXiv:2408.17145v2 Announce Type: replace-cross 
Abstract: The classical line search for learning rate (LR) tuning in the stochastic gradient descent (SGD) algorithm can tame the convergence slowdown due to data-sampling noise. In a federated setting, wherein the client heterogeneity introduces a slowdown to the global convergence, line search can be relevantly adapted. In this work, we show that a stochastic variant of line search tames the heterogeneity in federated optimization in addition to that due to client-local gradient noise. To this end, we introduce Federated Stochastic Line Search (FedSLS) algorithm and show that it achieves deterministic rates in expectation. Specifically, FedSLS offers linear convergence for strongly convex objectives even with partial client participation. Recently, the extrapolation of the server's LR has shown promise for improved empirical performance for federated learning. To benefit from extrapolation, we extend FedSLS to Federated Extrapolated Stochastic Line Search (FedExpSLS) and prove its convergence. Our extensive empirical results show that the proposed methods perform at par or better than the popular federated learning algorithms across many convex and non-convex problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17145v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator> Geetika, Somya Tyagi, Bapi Chatterjee</dc:creator>
    </item>
    <item>
      <title>Optimal Kinematic Synthesis and Prototype Development of Knee Exoskeleton</title>
      <link>https://arxiv.org/abs/2409.02635</link>
      <description>arXiv:2409.02635v2 Announce Type: replace-cross 
Abstract: This study focuses on enhancing the design of an existing knee exoskeleton by addressing limitations in the range of motion (ROM) during Sit-to-Stand (STS) motions. While current knee exoskeletons emphasize toughness and rehabilitation, their closed-loop mechanisms hinder optimal ROM, which is crucial for effective rehabilitation. This research aims to optimize the exoskeleton design to achieve the necessary ROM, improving its functionality in rehabilitation. This can be achieved by utilizing kinematic modeling and formulation, the existing design was represented in the non-linear and non-convex mathematical functions. Optimization techniques, considering constraints based on human leg measurements, were applied to determine the best dimensions for the exoskeleton. This resulted in a significant increase in ROM compared to existing models. A MATLAB program was developed to compare the ROM of the optimized exoskeleton with the original design.
  To validate the practicality of the optimized design, analysis was conducted using a mannequin with average human dimensions, followed by constructing a cardboard dummy model to confirm simulation results. The STS motion of an average human was captured using a camera and TRACKER software, and the motion was compared with that of the dummy model to identify any misalignments between the human and exoskeleton knee joints. Furthermore, a prototype of the knee joint exoskeleton is being developed to further investigate misalignments and improve the design. Future work includes the use of EMG sensors for more detailed analysis and better results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02635v2</guid>
      <category>cs.RO</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Shashank Mani Gautam, Ekta Singla, Ashish Singla</dc:creator>
    </item>
    <item>
      <title>Efficient Adaptive Federated Optimization</title>
      <link>https://arxiv.org/abs/2410.18117</link>
      <description>arXiv:2410.18117v3 Announce Type: replace-cross 
Abstract: Adaptive optimization is critical in federated learning, where enabling adaptivity on both the server and client sides has proven essential for achieving optimal performance. However, the scalability of such jointly adaptive systems is often hindered by resource limitations in communication and memory. In this paper, we introduce a class of efficient adaptive algorithms, named $FedAda^2$ and its enhanced version $FedAda^2$++, designed specifically for large-scale, cross-device federated environments. $FedAda^2$ optimizes communication efficiency by avoiding the transfer of preconditioners between the server and clients. Additionally, $FedAda^2$++ extends this approach by incorporating memory-efficient adaptive optimizers on the client side, further reducing on-device memory usage. Theoretically, we demonstrate that $FedAda^2$ and $FedAda^2$++ achieve the same convergence rates for general, non-convex objectives as its more resource-intensive counterparts that directly integrate joint adaptivity. Extensive empirical evaluations on image and text datasets demonstrate both the advantages of joint adaptivity and the effectiveness of $FedAda^2$/$FedAda^2$++.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18117v3</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Su Hyeong Lee, Sidharth Sharma, Manzil Zaheer, Tian Li</dc:creator>
    </item>
    <item>
      <title>Generalized EXTRA stochastic gradient Langevin dynamics</title>
      <link>https://arxiv.org/abs/2412.01993</link>
      <description>arXiv:2412.01993v2 Announce Type: replace-cross 
Abstract: Langevin algorithms are popular Markov Chain Monte Carlo methods for Bayesian learning, particularly when the aim is to sample from the posterior distribution of a parametric model, given the input data and the prior distribution over the model parameters. Their stochastic versions such as stochastic gradient Langevin dynamics (SGLD) allow iterative learning based on randomly sampled mini-batches of large datasets and are scalable to large datasets. However, when data is decentralized across a network of agents subject to communication and privacy constraints, standard SGLD algorithms cannot be applied. Instead, we employ decentralized SGLD (DE-SGLD) algorithms, where Bayesian learning is performed collaboratively by a network of agents without sharing individual data. Nonetheless, existing DE-SGLD algorithms induce a bias at every agent that can negatively impact performance; this bias persists even when using full batches and is attributable to network effects. Motivated by the EXTRA algorithm and its generalizations for decentralized optimization, we propose the generalized EXTRA stochastic gradient Langevin dynamics, which eliminates this bias in the full-batch setting. Moreover, we show that, in the mini-batch setting, our algorithm provides performance bounds that significantly improve upon those of standard DE-SGLD algorithms in the literature. Our numerical results also demonstrate the efficiency of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01993v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mert Gurbuzbalaban, Mohammad Rafiqul Islam, Xiaoyu Wang, Lingjiong Zhu</dc:creator>
    </item>
    <item>
      <title>Identifiability of Autonomous and Controlled Open Quantum Systems</title>
      <link>https://arxiv.org/abs/2501.05270</link>
      <description>arXiv:2501.05270v3 Announce Type: replace-cross 
Abstract: Open quantum systems are a rich area of research in the intersection of quantum mechanics and stochastic analysis. By considering a variety of master equations, we unify multiple views of autonomous and controlled open quantum systems and, through considering their measurement dynamics, connect them to classical linear and bilinear system identification theory. This allows us to formulate corresponding notions of quantum state identifiability for these systems which, in particular, applies to quantum state tomography, providing conditions under which the probed quantum system is reconstructible. Interestingly, the dynamical representation of the system lends itself to considering two types of identifiability: the full master equation recovery and the recovery of the corresponding system matrices of the linear and bilinear systems. Both of these concepts are discussed in detail, and conditions under which reconstruction is possible are given. We set the groundwork for a number of constructive approaches to the identification of open quantum systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05270v3</guid>
      <category>quant-ph</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Waqas Parvaiz, Johannes Aspman, Ales Wodecki, Georgios Korpas, Jakub Marecek</dc:creator>
    </item>
    <item>
      <title>Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions</title>
      <link>https://arxiv.org/abs/2502.06309</link>
      <description>arXiv:2502.06309v4 Announce Type: replace-cross 
Abstract: As the economic and environmental costs of training and deploying large vision or language models increase dramatically, analog in-memory computing (AIMC) emerges as a promising energy-efficient solution. However, the training perspective, especially its training dynamic, is underexplored. In AIMC hardware, the trainable weights are represented by the conductance of resistive elements and updated using consecutive electrical pulses. While the conductance changes by a constant in response to each pulse, in reality, the change is scaled by asymmetric and non-linear response functions, leading to a non-ideal training dynamic. This paper provides a theoretical foundation for gradient-based training on AIMC hardware with non-ideal response functions. We demonstrate that asymmetric response functions negatively impact Analog SGD by imposing an implicit penalty on the objective. To overcome the issue, we propose Residual Learning algorithm, which provably converges exactly to a critical point by solving a bilevel optimization problem. We demonstrate that the proposed method can be extended to address other hardware imperfections, such as limited response granularity. As we know, it is the first paper to investigate the impact of a class of generic non-ideal response functions. The conclusion is supported by simulations validating our theoretical insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06309v4</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaoxian Wu, Quan Xiao, Tayfun Gokmen, Omobayode Fagbohungbe, Tianyi Chen</dc:creator>
    </item>
    <item>
      <title>On the Global Optimality of Fibonacci Lattices in the Torus</title>
      <link>https://arxiv.org/abs/2502.17082</link>
      <description>arXiv:2502.17082v2 Announce Type: replace-cross 
Abstract: We use linear programming bounds to analyze point sets in the torus with respect to their optimality for problems in discrepancy theory and quasi-Monte Carlo methods. These concepts will be unified by introducing tensor product energies.
  We show that the canonical $3$-point lattice in any dimension is globally optimal among all $3$-point sets in the torus with respect to a large class of such energies. This is a new instance of universal optimality, a special phenomenon that is only known for a small class of highly structured point sets.
  In the case of $d=2$ dimensions it is conjectured that so-called Fibonacci lattices should also be optimal with respect to a large class of potentials. To this end we show that the $5$-point Fibonacci lattice is globally optimal for a continuously parametrized class of potentials relevant to the analysis fo the quasi-Monte Carlo method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17082v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.CO</category>
      <category>math.MG</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Nagel</dc:creator>
    </item>
    <item>
      <title>Federated Structured Sparse PCA for Anomaly Detection in IoT Networks</title>
      <link>https://arxiv.org/abs/2503.23981</link>
      <description>arXiv:2503.23981v2 Announce Type: replace-cross 
Abstract: Although federated learning has gained prominence as a privacy-preserving framework tailored for distributed Internet of Things (IoT) environments, current federated principal component analysis (PCA) methods lack integration of sparsity, a critical feature for robust anomaly detection. To address this limitation, we propose a novel federated structured sparse PCA (FedSSP) approach for anomaly detection in IoT networks. The proposed model uniquely integrates double sparsity regularization: (1) row-wise sparsity governed by $\ell_{2,p}$-norm with $p\in[0,1)$ to eliminate redundant feature dimensions, and (2) element-wise sparsity via $\ell_{q}$-norm with $q\in[0,1)$ to suppress noise-sensitive components. To efficiently solve this non-convex optimization problem in a distributed setting, we devise a proximal alternating minimization (PAM) algorithm with rigorous theoretical proofs establishing its convergence guarantees. Experiments on real datasets validate that incorporating structured sparsity enhances both model interpretability and detection accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23981v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenyi Huang, Xinrong Li, Xianchao Xiu</dc:creator>
    </item>
    <item>
      <title>Modeling Cell Dynamics and Interactions with Unbalanced Mean Field Schr\"odinger Bridge</title>
      <link>https://arxiv.org/abs/2505.11197</link>
      <description>arXiv:2505.11197v3 Announce Type: replace-cross 
Abstract: Modeling the dynamics from sparsely time-resolved snapshot data is crucial for understanding complex cellular processes and behavior. Existing methods leverage optimal transport, Schr\"odinger bridge theory, or their variants to simultaneously infer stochastic, unbalanced dynamics from snapshot data. However, these approaches remain limited in their ability to account for cell-cell interactions. This integration is essential in real-world scenarios since intercellular communications are fundamental life processes and can influence cell state-transition dynamics. To address this challenge, we formulate the Unbalanced Mean-Field Schr\"odinger Bridge (UMFSB) framework to model unbalanced stochastic interaction dynamics from snapshot data. Inspired by this framework, we further propose CytoBridge, a deep learning algorithm designed to approximate the UMFSB problem. By explicitly modeling cellular transitions, proliferation, and interactions through neural networks, CytoBridge offers the flexibility to learn these processes directly from data. The effectiveness of our method has been extensively validated using both synthetic gene regulatory data and real scRNA-seq datasets. Compared to existing methods, CytoBridge identifies growth, transition, and interaction patterns, eliminates false transitions, and reconstructs the developmental landscape with greater accuracy. Code is available at: https://github.com/zhenyiizhang/CytoBridge-NeurIPS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11197v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhenyi Zhang, Zihan Wang, Yuhao Sun, Tiejun Li, Peijie Zhou</dc:creator>
    </item>
    <item>
      <title>Variational Regularized Unbalanced Optimal Transport: Single Network, Least Action</title>
      <link>https://arxiv.org/abs/2505.11823</link>
      <description>arXiv:2505.11823v2 Announce Type: replace-cross 
Abstract: Recovering the dynamics from a few snapshots of a high-dimensional system is a challenging task in statistical physics and machine learning, with important applications in computational biology. Many algorithms have been developed to tackle this problem, based on frameworks such as optimal transport and the Schr\"odinger bridge. A notable recent framework is Regularized Unbalanced Optimal Transport (RUOT), which integrates both stochastic dynamics and unnormalized distributions. However, since many existing methods do not explicitly enforce optimality conditions, their solutions often struggle to satisfy the principle of least action and meet challenges to converge in a stable and reliable way. To address these issues, we propose Variational RUOT (Var-RUOT), a new framework to solve the RUOT problem. By incorporating the optimal necessary conditions for the RUOT problem into both the parameterization of the search space and the loss function design, Var-RUOT only needs to learn a scalar field to solve the RUOT problem and can search for solutions with lower action. We also examined the challenge of selecting a growth penalty function in the widely used Wasserstein-Fisher-Rao metric and proposed a solution that better aligns with biological priors in Var-RUOT. We validated the effectiveness of Var-RUOT on both simulated data and real single-cell datasets. Compared with existing algorithms, Var-RUOT can find solutions with lower action while exhibiting faster convergence and improved training stability. Our code is available at https://github.com/ZerooVector/VarRUOT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11823v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuhao Sun, Zhenyi Zhang, Zihan Wang, Tiejun Li, Peijie Zhou</dc:creator>
    </item>
    <item>
      <title>Security of Gradient Tracking Algorithms Against Malicious Agents</title>
      <link>https://arxiv.org/abs/2505.14473</link>
      <description>arXiv:2505.14473v2 Announce Type: replace-cross 
Abstract: Consensus algorithms are fundamental to multi-agent distributed optimization, and their security under adversarial conditions is an active area of research. While prior works primarily establish conditions for successful global consensus under attack, little is known about system behavior when these conditions are violated. This paper addresses this gap by investigating the robustness of the Wang--Elia algorithm, which is a robust to noise version of gradient tracking algorithm, in the presence of malicious agents. We consider a network of agents collaboratively minimizing a global cost function, where a subset of agents may transmit faulty information to disrupt consensus. To quantify resilience, we formulate a security metric as an optimization problem, which is rooted in centralized attack detection literature. We provide a tractable reformulation of the optimization problem, and derive conditions under which the metric becomes unbounded, identifying undetectable attack signals that reveal inherent vulnerabilities. To facilitate design and analysis, we propose a well-posed variant of the metric and propose design methods to enhance network robustness against stealthy adversarial attacks. Numerical examples demonstrate the effectiveness of the proposed framework to enhance the resilience of multi-agent distributed optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14473v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sribalaji C. Anand, Alexander J Gallo, Nicola Bastianello</dc:creator>
    </item>
    <item>
      <title>SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating Memory-Efficient LLM Training</title>
      <link>https://arxiv.org/abs/2505.24749</link>
      <description>arXiv:2505.24749v2 Announce Type: replace-cross 
Abstract: Low-rank gradient-based optimization methods have significantly improved memory efficiency during the training of large language models (LLMs), enabling operations within constrained hardware without sacrificing performance. However, these methods primarily emphasize memory savings, often overlooking potential acceleration in convergence due to their reliance on standard isotropic steepest descent techniques, which can perform suboptimally in the highly anisotropic landscapes typical of deep networks, particularly LLMs. In this paper, we propose SUMO (Subspace-Aware Moment-Orthogonalization), an optimizer that employs exact singular value decomposition (SVD) for moment orthogonalization within a dynamically adapted low-dimensional subspace, enabling norm-inducing steepest descent optimization steps. By explicitly aligning optimization steps with the spectral characteristics of the loss landscape, SUMO effectively mitigates approximation errors associated with commonly used methods like Newton-Schulz orthogonalization approximation. We theoretically establish an upper bound on these approximation errors, proving their dependence on the condition numbers of moments, conditions we analytically demonstrate are encountered during LLM training. Furthermore, we both theoretically and empirically illustrate that exact orthogonalization via SVD substantially improves convergence rates while reducing overall complexity. Empirical evaluations confirm that SUMO accelerates convergence, enhances stability, improves performance, and reduces memory requirements by up to 20% compared to state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24749v2</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)</arxiv:journal_reference>
      <dc:creator>Yehonathan Refael, Guy Smorodinsky, Tom Tirer, Ofir Lindenbaum</dc:creator>
    </item>
    <item>
      <title>On the convergence of iterative regularization method assisted by the graph Laplacian with early stopping</title>
      <link>https://arxiv.org/abs/2506.23483</link>
      <description>arXiv:2506.23483v2 Announce Type: replace-cross 
Abstract: We present a data-assisted iterative regularization method for solving ill-posed inverse problems. The proposed approach, termed \texttt{IRMGL+\(\Psi\)}, integrates classical iterative techniques with a data-driven regularization term realized through an iteratively updated graph Laplacian. Our method commences by computing a preliminary solution using any suitable reconstruction method, which then serves as the basis for constructing the initial graph Laplacian. The solution is subsequently refined through an iterative process, where the graph Laplacian is simultaneously recalibrated at each step to effectively capture the evolving structure of the solution. A key innovation of this work lies in the formulation of this iterative scheme and the rigorous justification of the classical discrepancy principle as a reliable early stopping criterion specifically tailored to the proposed method. Under standard assumptions, we establish stability and convergence results for the scheme when the discrepancy principle is applied. Furthermore, we demonstrate the robustness and effectiveness of our method through numerical experiments utilizing four distinct initial reconstructors $\Psi$: the adjoint operator (Adj), filtered back projection (FBP), total variation (TV) denoising, and standard Tikhonov regularization (Tik). It is observed that \texttt{IRMGL+Adj} demonstrates a distinct advantage over the other initializers, producing a robust and stable approximate solution directly from a basic initial reconstruction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23483v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.FA</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harshit Bajpai, Gaurav Mittal, Ankik Kumar Giri</dc:creator>
    </item>
    <item>
      <title>Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training</title>
      <link>https://arxiv.org/abs/2507.09846</link>
      <description>arXiv:2507.09846v2 Announce Type: replace-cross 
Abstract: As both model and dataset sizes continue to scale rapidly, conventional pretraining strategies with fixed compute budgets-such as cosine learning rate schedules-are increasingly inadequate for large-scale training. Recent alternatives, including warmup-stable-decay (WSD) schedules and weight averaging, offer greater flexibility. However, WSD relies on explicit decay phases to track progress, while weight averaging addresses this limitation at the cost of additional memory. In search of a more principled and scalable alternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024], which has shown strong empirical performance across diverse settings. We show that SF-AdamW effectively navigates the "river" structure of the loss landscape without decay phases or auxiliary averaging, making it particularly suitable for continuously scaling training workloads. To understand this behavior, we conduct a theoretical and empirical analysis of SF dynamics, revealing that it implicitly performs weight averaging without memory overhead. Guided by this analysis, we propose a refined variant of SF that improves robustness to momentum and performs better under large batch sizes, addressing key limitations of the original method. Together, these results establish SF as a practical, scalable, and theoretically grounded approach for language model training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09846v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minhak Song, Beomhan Baek, Kwangjun Ahn, Chulhee Yun</dc:creator>
    </item>
    <item>
      <title>EVODiff: Entropy-aware Variance Optimized Diffusion Inference</title>
      <link>https://arxiv.org/abs/2509.26096</link>
      <description>arXiv:2509.26096v2 Announce Type: replace-cross 
Abstract: Diffusion models (DMs) excel in image generation, but suffer from slow inference and the training-inference discrepancies. Although gradient-based solvers like DPM-Solver accelerate the denoising inference, they lack theoretical foundations in information transmission efficiency. In this work, we introduce an information-theoretic perspective on the inference processes of DMs, revealing that successful denoising fundamentally reduces conditional entropy in reverse transitions. This principle leads to our key insights into the inference processes: (1) data prediction parameterization outperforms its noise counterpart, and (2) optimizing conditional variance offers a reference-free way to minimize both transition and reconstruction errors. Based on these insights, we propose an entropy-aware variance optimized method for the generative process of DMs, called EVODiff, which systematically reduces uncertainty by optimizing conditional entropy during denoising. Extensive experiments on DMs validate our insights and demonstrate that our method significantly and consistently outperforms state-of-the-art (SOTA) gradient-based solvers. For example, compared to the DPM-Solver++, EVODiff reduces the reconstruction error by up to 45.5\% (FID improves from 5.10 to 2.78) at 10 function evaluations (NFE) on CIFAR-10, cuts the NFE cost by 25\% (from 20 to 15 NFE) for high-quality samples on ImageNet-256, and improves text-to-image generation while reducing artifacts. Code is available at https://github.com/ShiguiLi/EVODiff.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26096v2</guid>
      <category>cs.CV</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shigui Li, Wei Chen, Delu Zeng</dc:creator>
    </item>
    <item>
      <title>Augmented Lagrangian Method based adjoint space framework for sparse reconstruction of acoustic source with boundary measurements</title>
      <link>https://arxiv.org/abs/2510.14805</link>
      <description>arXiv:2510.14805v2 Announce Type: replace-cross 
Abstract: We propose a semismooth Newton-based augmented Lagrangian method for reconstructing sparse sources in inverse acoustic scattering problems. The semismooth Newton method can be iterated in the space of measurements instead of the unknown source to be reconstructed. It is highly efficient, especially when the measurement data is much less than the acoustic source. The source can be calculated from Fenchel-Rockafellar duality theory. We can obtain lots of acceleration and leverage the computational cost. The numerical examples show the high efficiency of the proposed semismooth Newton-based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14805v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nirui Tan, Hongpeng Sun</dc:creator>
    </item>
  </channel>
</rss>
