<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Feb 2026 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Precedence-Constrained Decision Trees and Coverings</title>
      <link>https://arxiv.org/abs/2602.21312</link>
      <description>arXiv:2602.21312v1 Announce Type: new 
Abstract: This work considers a number of optimization problems and reductive relations between them. The two main problems we are interested in are the \emph{Optimal Decision Tree} and \emph{Set Cover}. We study these two fundamental tasks under precedence constraints, that is, if a test (or set) $X$ is a predecessor of $Y$, then in any feasible decision tree $X$ needs to be an ancestor of $Y$ (or respectively, if $Y$ is added to set cover, then so must be $X$). For the Optimal Decision Tree we consider two optimization criteria: worst case identification time (height of the tree) or the average identification time. Similarly, for the Set Cover we study two cost measures: the size of the cover or the average cover time.
  Our approach is to develop a number of algorithmic reductions, where an approximation algorithm for one problem provides an approximation for another via a black-box usage of a procedure for the former. En route we introduce other optimization problems either to complete the `reduction landscape' or because they hold the essence of combinatorial structure of our problems. The latter is brought by a problem of finding a maximum density precedence closed subfamily, where the density is defined as the ratio of the number of items the family covers to its size. By doing so we provide $\cO^*(\sqrt{m})$-approximation algorithms for all of the aforementioned problems. The picture is complemented by a number of hardness reductions that provide $o(m^{1/12-\epsilon})$-inapproximability results for the decision tree and covering problems. Besides giving a complete set of results for general precedence constraints, we also provide polylogarithmic approximation guarantees for two most typically studied and applicable precedence types, outforests and inforests. By providing corresponding hardness results, we show these results to be tight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21312v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Micha{\l} Szyfelbein, Dariusz Dereniowski</dc:creator>
    </item>
    <item>
      <title>DRESS and the WL Hierarchy: Climbing One Deletion at a Time</title>
      <link>https://arxiv.org/abs/2602.21557</link>
      <description>arXiv:2602.21557v1 Announce Type: new 
Abstract: The Cai--F\"urer--Immerman (CFI) construction provides the canonical family of hard instances for the Weisfeiler--Leman (WL) hierarchy: distinguishing the two non-isomorphic CFI graphs over a base graph $G$ requires $k$-WL where $k$ meets or exceeds the treewidth of $G$. In this paper, we introduce $\Delta^\ell$-DRESS, which applies $\ell$ levels of iterated node deletion to the DRESS continuous structural refinement framework. $\Delta^\ell$-DRESS runs Original-DRESS on all $\binom{n}{\ell}$ subgraphs obtained by removing $\ell$ nodes, and compares the resulting histograms. We show empirically on the canonical CFI benchmark family that Original-DRESS ($\Delta^0$) already distinguishes $\text{CFI}(K_3)$ (requiring 2-WL), and that each additional deletion level extends the range by one WL level: $\Delta^1$ reaches 3-WL, $\Delta^2$ reaches 4-WL, and $\Delta^3$ reaches 5-WL, distinguishing CFI pairs over $K_n$ for $n = 3, \ldots, 6$. Crucially, $\Delta^3$ fails on $\text{CFI}(K_7)$ (requiring 6-WL), confirming a sharp boundary at $(\ell+2)$-WL. The computational cost is $\mathcal{O}\bigl(\binom{n}{\ell} \cdot I \cdot m \cdot d_{\max}\bigr)$ -- polynomial in $n$ for fixed $\ell$. These results establish $\Delta^\ell$-DRESS as a practical framework for systematically climbing the WL hierarchy on the canonical CFI benchmark family.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21557v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eduar Castrillo Velilla</dc:creator>
    </item>
    <item>
      <title>Maximal Biclique Enumeration with Improved Worst-Case Time Complexity Guarantee: A Partition-Oriented Strategy</title>
      <link>https://arxiv.org/abs/2602.21700</link>
      <description>arXiv:2602.21700v1 Announce Type: new 
Abstract: The maximal biclique enumeration problem in bipartite graphs is fundamental and has numerous applications in E-commerce and transaction networks. Most existing studies adopt a branch-and-bound framework, which recursively expands a partial biclique with a vertex until no further vertices can be added. Equipped with a basic pivot selection strategy, all state-of-the-art methods have a worst-case time complexity no better than $O(m\cdot (\sqrt{2})^n)$}, where $m$ and $n$ are the number of edges and vertices in the graph, respectively. In this paper, we introduce a new branch-and-bound (BB) algorithm \texttt{IPS}. In \texttt{IPS}, we relax the strict stopping criterion of existing methods by allowing termination when all maximal bicliques within the current branch can be outputted in the time proportional to the number of maximal bicliques inside, reducing the total number of branches required. Second, to fully unleash the power of the new termination condition, we propose an improved pivot selection strategy, which well aligns with the new termination condition to achieve better theoretical and practical performance. Formally, \texttt{IPS} improves the worst-case time complexity to $O(m\cdot \alpha ^n + n\cdot \beta)$, where $\alpha (\approx 1.3954)$ is the largest positive root of $x^4-2x-1=0$ and $\beta$ represents the number of maximal bicliques in the graph, respectively. This result surpasses that of all existing algorithms given that $\alpha$ is strictly smaller than $\sqrt{2}$ and $\beta$ is at most $(\sqrt{2})^n-2$ theoretically. Furthermore, we apply an inclusion-exclusion-based framework to boost the performance of \texttt{IPS}, improving the worst-case time complexity to $O(n\cdot \gamma^2\cdot\alpha^\gamma + \gamma\cdot \beta)$ for large sparse graphs ($\gamma$ is a parameter satisfying $\gamma \ll n$ for sparse graphs).</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21700v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaixin Wang, Kaiqiang Yu, Cheng Long</dc:creator>
    </item>
    <item>
      <title>Delayed-Clairvoyant Flow Time Scheduling via a Borrow Graph Analysis</title>
      <link>https://arxiv.org/abs/2602.21827</link>
      <description>arXiv:2602.21827v1 Announce Type: new 
Abstract: We study the problem of preemptively scheduling jobs online over time on a single machine to minimize the total flow time.
  In the traditional clairvoyant scheduling model, the scheduler learns about the processing time of a job at its arrival, and scheduling at any time the job with the shortest remaining processing time (SRPT) is optimal. In contrast, the practically relevant non-clairvoyant model assumes that the processing time of a job is unknown at its arrival, and is only revealed when it completes. Non-clairvoyant flow time minimization does not admit algorithms with a constant competitive ratio. Consequently, the problem has been studied under speed augmentation (JACM'00) or with predicted processing times (STOC'21, SODA'22) to attain constant guarantees.
  In this paper, we consider $\alpha$-clairvoyant scheduling, where the scheduler learns the processing time of a job once it completes an $\alpha$-fraction of its processing time. This naturally interpolates between clairvoyant scheduling ($\alpha=0$) and non-clairvoyant scheduling ($\alpha=1$). By elegantly fusing two traditional algorithms, we propose a scheduling rule with a competitive ratio of $\mathcal{O}(\frac{1}{1-\alpha})$ whenever $0 \leq \alpha &lt; 1$. As $\alpha$ increases, our competitive guarantee transitions nicely (up to constants) between the previously established bounds for clairvoyant and non-clairvoyant flow time minimization. We complement this positive result with a tight randomized lower bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21827v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Lindermayr, Jens Schl\"oter</dc:creator>
    </item>
    <item>
      <title>Instance-optimal estimation of L2-norm</title>
      <link>https://arxiv.org/abs/2602.21937</link>
      <description>arXiv:2602.21937v1 Announce Type: new 
Abstract: The $L_2$-norm, or collision norm, is a core entity in the analysis of distributions and probabilistic algorithms. Batu and Canonne (FOCS 2017) presented an extensive analysis of algorithmic aspects of the $L_2$-norm and its connection to uniformity testing. However, when it comes to estimating the $L_2$-norm itself, their algorithm is not always optimal compared to the instance-specific second-moment bounds, $O(1/(\varepsilon\|\mu\|_2) + (\|\mu\|_3^3 - \|\mu\|_2^4) / (\varepsilon^2 \|\mu\|_2^4))$, as stated by Batu (WoLA 2025, open problem session).
  In this paper, we present an unbiased $L_2$-estimation algorithm whose sample complexity matches the instance-specific second-moment analysis. Additionally, we show that $\Omega(1/(\varepsilon \|\mu\|_2))$ is indeed a per-instance lower bound for estimating the norm of a distribution $\mu$ by sampling (even for non-unbiased estimators).</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21937v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomer Adar</dc:creator>
    </item>
    <item>
      <title>Tight Bounds for Online Scheduling in the One-Fast-Many-Slow Machines Setting</title>
      <link>https://arxiv.org/abs/2602.22108</link>
      <description>arXiv:2602.22108v1 Announce Type: new 
Abstract: In the One-Fast-Many-Slow decision problem, introduced by Sheffield and Westover (ITCS '25), a scheduler, with access to one fast machine and infinitely many slow machines, receives a series of tasks and must allocate the work among its machines. The goal is to minimize the overhead of an online algorithm over the optimal offline algorithm. Three versions of this setting were considered: Instantly-committing schedulers that must assign tasks to machines immediately and irrevocably, Eventually-committing schedulers whose assignments are irrevocable but can occur anytime after a task arrives, and Never-committing schedulers that can interrupt and restart a task on a different machine. In the Instantly-committing model, Sheffield and Westover showed that the optimal competitive ratio is equal to 2, while in the Eventually-committing model the competitive ratio lies in the interval [1.618, 1.678], and in the Never-committing model the competitive ratio lies in the interval [1.366, 1.5] (SPAA '24, ITCS '25). In the latter two models, the exact optimal competitive ratios were left as open problems, moreover Kuszmaul and Westover (SPAA '24) conjectured that the lower bound in the Eventually-committing model is tight.
  In this paper we resolve this problem by providing tight bounds for the competitive ratios in the Eventually-committing and Never-committing models. For Eventually-committing, we prove Kuszmaul and Westover's conjecture by giving an algorithm achieving a competitive ratio equal to the lower bound of $\frac{1+\sqrt{5}}{2}\approx 1.618$. For Never-committing, we provide an explicit Task Arrival Process (TAP) lower bounding the competitive ratio to the previous upper bound of 1.5.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22108v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John Jeang, Vladimir Podolskii</dc:creator>
    </item>
    <item>
      <title>Robust Permutation Flowshops Under Budgeted Uncertainty</title>
      <link>https://arxiv.org/abs/2602.22110</link>
      <description>arXiv:2602.22110v1 Announce Type: new 
Abstract: We consider the robust permutation flowshop problem under the budgeted uncertainty model, where at most a given number of job processing times may deviate on each machine. We show that solutions for this problem can be determined by solving polynomially many instances of the corresponding nominal problem. As a direct consequence, our result implies that this robust flowshop problem can be solved in polynomial time for two machines, and can be approximated in polynomial time for any fixed number of machines. The reduction that is our main result follows from an analysis similar to Bertsimas and Sim (2003) except that dualization is applied to the terms of a min-max objective rather than to a linear objective function. Our result may be surprising considering that heuristic and exact integer programming based methods have been developed in the literature for solving the two-machine flowshop problem. We conclude by showing a logarithmic factor improvement in the overall running time implied by a naive reduction to nominal problems in the case of two machines and three machines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22110v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.OC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noam Goldberg, Danny Hermelin, Dvir Shabtay</dc:creator>
    </item>
    <item>
      <title>The Instability of all Backoff Protocols</title>
      <link>https://arxiv.org/abs/2602.21315</link>
      <description>arXiv:2602.21315v1 Announce Type: cross 
Abstract: In this paper we prove Aldous's conjecture from 1987 that there is no backoff protocol that is stable for any positive arrival rate. The setting is a communication channel for coordinating requests for a shared resource. Each user who wants to access the resource makes a request by sending a message to the channel. The users don't have any way to communicate with each other, except by sending messages to the channel. The operation of the channel proceeds in discrete time steps. If exactly one message is sent to the channel during a time step then this message succeeds (and leaves the system). If multiple messages are sent during a time step then these messages collide. Each of the users that sent these messages therefore waits a random amount of time before re-sending. A backoff protocol is a randomised algorithm for determining how long to wait -- the waiting time is a function of how many collisions a message has had. Specifically, a backoff protocol is described by a send sequence $\overline{p} = (p_0,p_1,p_2,\ldots)$. If a message has had $k$ collisions before a time step then, with probability $p_k$, it sends during that time step, whereas with probability $1-p_k$ it is silent (waiting for later). The most famous backoff protocol is binary exponential backoff, where $p_k = 2^{-k}$. Under Kelly's model, in which the number of new messages that arrive in the system at each time step is given by a Poisson random variable with mean $\lambda$, Aldous proved that binary exponential backoff is unstable for any positive $\lambda$. He conjectured that the same is true for any backoff protocol. We prove this conjecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21315v1</guid>
      <category>math.PR</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leslie Ann Goldberg, John Lapinskas</dc:creator>
    </item>
    <item>
      <title>Steiner Forest for $H$-Subgraph-Free Graphs</title>
      <link>https://arxiv.org/abs/2602.21859</link>
      <description>arXiv:2602.21859v1 Announce Type: cross 
Abstract: Our main result is a full classification, for every connected graph $H$, of the computational complexity of Steiner Forest on $H$-subgraph-free graphs. To obtain this dichotomy, we establish the following new algorithmic, hardness, and combinatorial results:
  Algorithms: We identify two new classes of graph-theoretical structures that make it possible to solve Steiner Forest in polynomial time. Roughly speaking, our algorithms handle the following cases: (1) a set $X$ of vertices of bounded size that are pairwise connected by subgraphs of treewidth $2$ or bounded size, possibly together with an independent set of arbitrary size that is connected to $X$ in an arbitrary way; (2) a set $X$ of vertices of arbitrary size that are pairwise connected in a cyclic manner by subgraphs of treewidth $2$ or bounded size.
  Hardness results: We show that Steiner Forest remains NP-complete for graphs with 2-deletion set number $3$. (The $c$-deletion set number is the size of a smallest cutset $S$ such that every component of $G-S$ has at most $c$ vertices.)
  Combinatorial results: To establish the dichotomy, we perform a delicate graph-theoretic analysis showing that if $H$ is a path or a subdivided claw, then excluding $H$ as a subgraph either yields one of the two algorithmically favourable structures described above, or yields a graph class for which NP-completeness of Steiner Forest follows from either our new hardness result or a previously known one.
  Along the way to classifying the hardness for excluded subgraphs, we establish a dichotomy for graphs with $c$-deletion set number at most $k$. Specifically, our results together with pre-existing ones show that Steiner Forest is polynomial-time solvable if (1) $c=1$ and $k\geq 0$, or (2) $c=2$ and $k\leq 2$, or (3) $c\geq 3$ and $k=1$, and is NP-complete otherwise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21859v1</guid>
      <category>math.CO</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tala Eagling-Vose, David C. Kutner, Felicia Lucke, D\'aniel Marx, Barnaby Martin, Dani\"el Paulusma, Erik Jan van Leeuwen</dc:creator>
    </item>
    <item>
      <title>Optimal Trajectories in Discrete Space with Acceleration Constraints</title>
      <link>https://arxiv.org/abs/2602.21964</link>
      <description>arXiv:2602.21964v1 Announce Type: cross 
Abstract: In the racetrack acceleration model, proposed by Martin Gardner in 1973, each step consists of changing the position of the vehicle by a vector in $\mathbb{Z}^2$, with the constraints that two consecutive vectors differ by at most one unit in each dimension. We investigate three problems related to this model in arbitrary dimension in open space (no obstacles), where a configuration of the vehicle consists of its current position and the last-used vector. The three problems are the following. In Branching Cost (BC), given two configurations, the goal is to compute the minimum number of intermediate configurations (length of a trajectory) between the two configurations. Branching Trajectory (BT) has the same input and asks for a description of the corresponding trajectory. Multipoint Trajectory (MT) asks for an optimal trajectory that visits given points $p_1,\dots,p_n$ in a prescribed order, starting and ending with zero-speed configurations.\\ We revisit known approaches to solve BC in 2D, showing that this problem can be solved in constant time in any fixed number of dimensions $d$ (more generally, in $O(d \log d)$ time). We show that BT can also be solved in constant time for any fixed $d$, despite the fact that the length of the trajectory is not constant, by leveraging the fact that there always exists \emph{one} optimal trajectory compactly represented by $O(1)$ intermediate configurations. For MT, we collect theoretical and experimental evidence that the speed cannot be trivially bounded; local decisions may be impacted by points that are arbitrarily far in the visit order; and an optimal trajectory may require significant excursions out of the convex hull of the points. We still establish conservative speed bounds that a natural dynamic programming (DP) algorithm can exploit to solve reasonably large instances efficiently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21964v1</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arnaud Casteigts, Matteo De Francesco, Pierre Leone</dc:creator>
    </item>
    <item>
      <title>Sample Complexity Bounds for Robust Mean Estimation with Mean-Shift Contamination</title>
      <link>https://arxiv.org/abs/2602.22130</link>
      <description>arXiv:2602.22130v1 Announce Type: cross 
Abstract: We study the basic task of mean estimation in the presence of mean-shift contamination. In the mean-shift contamination model, an adversary is allowed to replace a small constant fraction of the clean samples by samples drawn from arbitrarily shifted versions of the base distribution. Prior work characterized the sample complexity of this task for the special cases of the Gaussian and Laplace distributions. Specifically, it was shown that consistent estimation is possible in these cases, a property that is provably impossible in Huber's contamination model. An open question posed in earlier work was to determine the sample complexity of mean estimation in the mean-shift contamination model for general base distributions. In this work, we study and essentially resolve this open question. Specifically, we show that, under mild spectral conditions on the characteristic function of the (potentially multivariate) base distribution, there exists a sample-efficient algorithm that estimates the target mean to any desired accuracy. We complement our upper bound with a qualitatively matching sample complexity lower bound. Our techniques make critical use of Fourier analysis, and in particular introduce the notion of a Fourier witness as an essential ingredient of our upper and lower bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22130v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilias Diakonikolas, Giannis Iakovidis, Daniel M. Kane, Sihan Liu</dc:creator>
    </item>
    <item>
      <title>Compressing Suffix Trees by Path Decompositions</title>
      <link>https://arxiv.org/abs/2506.14734</link>
      <description>arXiv:2506.14734v5 Announce Type: replace 
Abstract: The suffix tree is arguably the most fundamental data structure on strings: introduced by Weiner (SWAT 1973) and McCreight (JACM 1976), it allows solving a myriad of computational problems on strings in linear time. Motivated by its large space usage, subsequent research focused first on reducing its size by a constant factor via Suffix Arrays, and later on reaching space proportional to the size of the compressed string. Modern compressed indexes, such as the $r$-index (Gagie et al., SODA 2018), fit in space proportional to $r$, the number of runs in the Burrows-Wheeler transform (a strong and universal repetitiveness measure). These advances, however, came with a price: while modern compressed indexes boast optimal bounds in the RAM model, they are often orders of magnitude slower than uncompressed counterparts in practice due to catastrophic cache locality. This reality gap highlights that Big-O complexity in the RAM model has become a misleading predictor of real-world performance, leaving a critical question unanswered: can we design compressed indexes that are efficient in the I/O model of computation?
  We answer this in the affirmative by introducing a new Suffix Array sampling technique based on particular path decompositions of the suffix tree. We prove that sorting the suffix tree leaves by specific priority functions induces a decomposition where the number of distinct paths (each corresponding to a string suffix) is bounded by $r$. This allows us to solve indexed pattern matching efficiently in the I/O model using a Suffix Array sample of size at most $r$, strictly improving upon the (tight) $2r$ bound of Suffixient Arrays, another recent compressed Suffix Array sampling technique.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14734v5</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruben Becker, Davide Cenzato, Travis Gagie, Sung-Hwan Kim, Ragnar Groot Koerkamp, Giovanni Manzini, Nicola Prezza</dc:creator>
    </item>
    <item>
      <title>From Dynamic Programs to Greedy Algorithms</title>
      <link>https://arxiv.org/abs/2508.00776</link>
      <description>arXiv:2508.00776v2 Announce Type: replace 
Abstract: We show for several computational problems how classical greedy algorithms for special cases can be derived in a simple way from dynamic programs for the general case: interval scheduling (restricted to unit weights), knapsack (restricted to unit values), and shortest paths (restricted to nonnegative edge lengths). Conceptually, we repeatedly expand the Bellman equations underlying the dynamic program and use straightforward monotonicity properties to figure out which terms yield the optimal value under the respective restrictions. The approach offers an alternative for developing these greedy algorithms in undergraduate algorithms courses and/or for arguing their correctness. In the setting of interval scheduling, it elucidates the change in order from earliest start time first for the memoized dynamic program to earliest finish time first for the greedy algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00776v2</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dieter van Melkebeek</dc:creator>
    </item>
    <item>
      <title>Hardness of Maximum Likelihood Learning of DPPs</title>
      <link>https://arxiv.org/abs/2205.12377</link>
      <description>arXiv:2205.12377v3 Announce Type: replace-cross 
Abstract: Determinantal Point Processes (DPPs) are a widely used probabilistic model for negatively correlated sets. DPPs have been successfully employed in Machine Learning applications to select a diverse, yet representative subset of data. In these applications, a set of parameters that maximize the likelihood of the data is typically desirable. The algorithms used for this task to date either optimize over a limited family of DPPs, or use local improvement heuristics that do not provide theoretical guarantees of optimality.
  n seminal work on DPPs in Machine Learning, Kulesza conjectured in his PhD Thesis (2011) that the problem is NP-complete. The lack of a formal proof prompted Brunel et al. (COLT 2017) to suggest that, in opposition to Kulesza's conjecture, there might exist a polynomial-time algorithm for computing a maximum-likelihood DPP. They also presented some preliminary evidence supporting a conjecture that they suggested might lead to such an algorithm.
  In this work we prove Kulesza's conjecture. In fact, we prove the following stronger hardness of approximation result: even computing a $\left(1-O(\frac{1}{\log^9{N}})\right)$-approximation to the maximum log-likelihood of a DPP on a ground set of $N$ elements is NP-complete.
  From a technical perspective, we reduce the problem of approximating the maximum log-likelihood of a DPP to solving a gap instance of a $3$-Coloring problem on a hypergraph. This hypergraph is based on the bounded-degree construction of Bogdanov, Obata, and Trevisan (2002), which we enhance using the strong expanders of Alon and Capalbo (2007). We demonstrate that if a rank-$3$ DPP achieves near-optimal log-likelihood, its marginal kernel must encode an almost perfect ``vector-coloring" of the hypergraph. Finally, we show that these continuous vectors can be decoded into a proper $3$-coloring after removing a small fraction of ``noisy" edges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.12377v3</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Grigorescu, Brendan Juba, Karl Wimmer, Ning Xie</dc:creator>
    </item>
    <item>
      <title>Solving the Multiobjective Quasi-Clique Problem</title>
      <link>https://arxiv.org/abs/2403.10896</link>
      <description>arXiv:2403.10896v2 Announce Type: replace-cross 
Abstract: Given a simple undirected graph $G$, a quasi-clique is a subgraph of $G$ whose density is at least $\gamma$ $(0 &lt; \gamma \leq 1)$. Finding a maximum quasi-clique has been addressed from two different perspectives: $i)$ maximizing vertex cardinality for a given edge density; and $ii)$ maximizing edge density for a given vertex cardinality. However, when no a priori preference information about cardinality and density is available, a more natural approach is to consider the problem from a multiobjective perspective. We introduce the Multiobjective Quasi-clique Problem (MOQC), which aims to find a quasi-clique by simultaneously maximizing both vertex cardinality and edge density. To efficiently address this problem, we explore the relationship among MOQC, its single-objective counterpart problems, and a biobjective optimization problem, along with several properties of the MOQC problem and quasi-cliques. We propose a baseline approach using $\varepsilon$-constraint scalarization and introduce a Two-phase strategy, which applies a dichotomic search based on weighted sum scalarization in the first phase and an $\varepsilon$-constraint methodology in the second phase. Additionally, we present a Three-phase strategy that combines the dichotomic search used in Two-phase with a vertex-degree-based local search employing novel sufficient conditions to assess quasi-clique efficiency, followed by an $\varepsilon$-constraint in a final stage. Experimental results on real-world sparse graphs indicate that the integrated use of dichotomic search and local search, together with mechanisms to assess quasi-clique efficiency, makes the Three-phase strategy an effective approach for solving the MOQC problem in terms of running time and ability to produce new efficient quasi-cliques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10896v2</guid>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ejor.2024.12.018</arxiv:DOI>
      <arxiv:journal_reference>European Journal of Operational Research, 2025</arxiv:journal_reference>
      <dc:creator>Daniela Scherer dos Santos, Kathrin Klamroth, Pedro Martins, Lu\'is Paquete</dc:creator>
    </item>
    <item>
      <title>A Refreshment Stirred, Not Shaken: Invariant-Preserving Deployments of Differential Privacy for the U.S. Decennial Census</title>
      <link>https://arxiv.org/abs/2501.08449</link>
      <description>arXiv:2501.08449v2 Announce Type: replace-cross 
Abstract: Protecting an individual's privacy when releasing their data is inherently an exercise in relativity, regardless of how privacy is qualified or quantified. This is because we can only limit the gain in information about an individual relative to what could be derived from other sources. This framing is the essence of differential privacy (DP), through which this article examines two statistical disclosure control (SDC) methods for the United States Decennial Census: the Permutation Swapping Algorithm (PSA), which resembles the 2010 Census's disclosure avoidance system (DAS), and the TopDown Algorithm (TDA), which was used in the 2020 DAS. To varying degrees, both methods leave unaltered certain statistics of the confidential data (their invariants) and hence neither can be readily reconciled with DP, at least as originally conceived. Nevertheless, we show how invariants can naturally be integrated into DP and use this to establish that the PSA satisfies pure DP subject to the invariants it necessarily induces, thereby proving that this traditional SDC method can, in fact, be understood from the perspective of DP. By a similar modification to zero-concentrated DP, we also provide a DP specification for the TDA. Finally, as a point of comparison, we consider a counterfactual scenario in which the PSA was adopted for the 2020 Census, resulting in a reduction in the nominal protection loss budget but at the cost of releasing many more invariants. This highlights the pervasive danger of comparing budgets without accounting for the other dimensions on which DP formulations vary (such as the invariants they permit). Therefore, while our results articulate the mathematical guarantees of SDC provided by the PSA, the TDA, and the 2020 DAS in general, care must be taken in translating these guarantees into actual privacy protection$\unicode{x2014}$just as is the case for any DP deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08449v2</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.DS</category>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1162/99608f92.dab78690</arxiv:DOI>
      <arxiv:journal_reference>Harvard Data Science Review (2026), Special Issue 6</arxiv:journal_reference>
      <dc:creator>James Bailie, Ruobin Gong, Xiao-Li Meng</dc:creator>
    </item>
    <item>
      <title>Rethinking Flexible Graph Similarity Computation: One-step Alignment with Global Guidance</title>
      <link>https://arxiv.org/abs/2504.06533</link>
      <description>arXiv:2504.06533v3 Announce Type: replace-cross 
Abstract: Graph Edit Distance (GED) is a widely used measure of graph similarity, valued for its flexibility in encoding domain knowledge through operation costs. However, existing learning-based approximation methods follow a modeling paradigm that decouples local candidate match selection from both operation costs and global dependencies between matches. This decoupling undermines their ability to capture the intrinsic flexibility of GED and often forces them to rely on costly iterative refinement to obtain accurate alignments. In this work, we revisit the formulation of GED and revise the prevailing paradigm, and propose Graph Edit Network (GEN), an implementation of the revised formulation that tightly integrates cost-aware expense estimation with globally guided one-step alignment. Specifically, GEN incorporates operation costs into node matching expenses estimation, ensuring match decisions respect the specified cost setting. Furthermore, GEN models match dependencies within and across graphs, capturing each match's impact on the overall alignment. These designs enable accurate GED approximation without iterative refinement. Extensive experiments on real-world and synthetic benchmarks demonstrate that GEN achieves up to a 37.8% reduction in GED predictive errors, while increasing inference throughput by up to 414x. These results highlight GEN's practical efficiency and the effectiveness of the revision. Beyond this implementation, our revision provides a principled framework for advancing learning-based GED approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06533v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhouyang Liu, Ning Liu, Yixin Chen, Jiezhong He, Shuai Ma, Dongsheng Li</dc:creator>
    </item>
    <item>
      <title>Parallelizing the Approximate Minimum Degree Ordering Algorithm: Strategies and Evaluation</title>
      <link>https://arxiv.org/abs/2504.17097</link>
      <description>arXiv:2504.17097v2 Announce Type: replace-cross 
Abstract: The approximate minimum degree algorithm is widely used before numerical factorization to reduce fill-in for sparse matrices. While considerable attention has been given to the numerical factorization process, less focus has been placed on parallelizing the approximate minimum degree algorithm itself. In this paper, we explore different parallelization strategies, and introduce a novel parallel framework that leverages multiple elimination on distance-2 independent sets. Our evaluation shows that parallelism within individual elimination steps is limited due to low computational workload and significant memory contention. In contrast, our proposed framework overcomes these challenges by parallelizing the work across elimination steps. To the best of our knowledge, our implementation is the first scalable shared memory implementation of the approximate minimum degree algorithm. Experimental results show that we achieve up to a 7.29x speedup using 64 threads over the state-of-the-art sequential implementation in SuiteSparse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17097v2</guid>
      <category>cs.DC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1137/1.9781611979022.1</arxiv:DOI>
      <arxiv:journal_reference>Proc. 2026 SIAM Conf. on Parallel Processing for Scientific Computing (PP26), pp. 1-15 (2026)</arxiv:journal_reference>
      <dc:creator>Yen-Hsiang Chang, Ayd{\i}n Bulu\c{c}, James Demmel</dc:creator>
    </item>
    <item>
      <title>Benchmarking Filtered Approximate Nearest Neighbor Search Algorithms on Transformer-based Embedding Vectors</title>
      <link>https://arxiv.org/abs/2507.21989</link>
      <description>arXiv:2507.21989v2 Announce Type: replace-cross 
Abstract: Advances in embedding models for text, image, audio, and video drive progress across multiple domains, including retrieval-augmented generation, recommendation systems, and others. Many of these applications require an efficient method to retrieve items that are close to a given query in the embedding space while satisfying a filter condition based on the item's attributes, a problem known as filtered approximate nearest neighbor search (FANNS). By performing an in-depth literature analysis on FANNS, we identify a key gap in the research landscape: publicly available datasets with embedding vectors from state-of-the-art transformer-based text embedding models that contain abundant real-world attributes covering a broad spectrum of attribute types and value distributions. To fill this gap, we introduce the arxiv-for-fanns dataset of transformer-based embedding vectors for the abstracts of over 2.7 million arXiv papers, enriched with 11 real-world attributes such as authors and categories. We benchmark eleven different FANNS methods on our new dataset to evaluate their performance across different filter types, numbers of retrieved neighbors, dataset scales, and query selectivities. We distill our findings into eight key observations that guide users in selecting the most suitable FANNS method for their specific use cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21989v2</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <category>cs.IR</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick Iff, Paul Bruegger, Marcin Chrapek, David Kochergin, Maciej Besta, Torsten Hoefler</dc:creator>
    </item>
    <item>
      <title>Implicit Decision Diagrams</title>
      <link>https://arxiv.org/abs/2602.20793</link>
      <description>arXiv:2602.20793v2 Announce Type: replace-cross 
Abstract: Decision Diagrams (DDs) have emerged as a powerful tool for discrete optimization, with rapidly growing adoption. DDs are directed acyclic layered graphs; restricted DDs are a generalized greedy heuristic for finding feasible solutions, and relaxed DDs compute combinatorial relaxed bounds. There is substantial theory that leverages DD-based bounding, yet the complexity of constructing the DDs themselves has received little attention. Standard restricted DD construction requires $O(w \log(w))$ per layer; standard relaxed DD construction requires $O(w^2)$, where $w$ is the width of the DD. Increasing $w$ improves bound quality at the cost of more time and memory.
  We introduce implicit Decision Diagrams, storing arcs implicitly rather than explicitly, and reducing per-layer complexity to $O(w)$ for restricted and relaxed DDs. We prove this is optimal: any framework treating state-update and merge operations as black boxes cannot do better.
  Optimal complexity shifts the challenge from algorithmic overhead to low-level engineering. We show how implicit DDs can drive a MIP solver, and release ImplicitDDs.jl (https://https://github.com/IsaacRudich/ImplicitDDs.jl), an open-source Julia solver exploiting the implementation refinements our theory enables. Experiments demonstrate the solver outperforms Gurobi on Subset Sum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20793v2</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isaac Rudich, Louis-Martin Rousseau</dc:creator>
    </item>
  </channel>
</rss>
