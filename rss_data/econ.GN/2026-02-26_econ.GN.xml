<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.GN updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.GN</link>
    <description>econ.GN updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.GN" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Feb 2026 05:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Network Effects in Corporate Emissions: Evidence from a Data-Dependent Spatial Panel Model</title>
      <link>https://arxiv.org/abs/2602.21434</link>
      <description>arXiv:2602.21434v1 Announce Type: new 
Abstract: We study spillover effects in corporate toxic emissions using a heterogeneous panel network of U.S. industrial facilities from 2000-2023. Rather than imposing a network structure a priori, we uncover an unobserved web of influence directly from the data using recent advances in high-dimensional network econometrics. Indirect effects transmitted through the estimated network account for about 28% of the total impact of key firm balance-sheet characteristics. By contrast, distance-based networks generate no statistically discernible spillovers, while a priori firm- or industry-based networks substantially overstate within-group spillins relative to the data-driven network. These findings show that who is linked to whom, and with what strength, matters critically for assessing systemic environmental risk and for designing targeted regulation. Methodologically, the paper provides a flexible framework for quantifying facility-level emissions spillovers and their consequences in financial and policy settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21434v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stylianos Asimakopoulos, George Kapetanios, Vasilis Sarafidis, Alexia Ventouri</dc:creator>
    </item>
    <item>
      <title>Can ranked-choice voting elect the least popular candidate?</title>
      <link>https://arxiv.org/abs/2602.21504</link>
      <description>arXiv:2602.21504v1 Announce Type: new 
Abstract: We analyze how frequently instant runoff voting (IRV) selects the weakest (or least popular) candidate in three-candidate elections. We consider four definitions of ``weakest candidate'': the Borda loser, the Bucklin loser, the candidate with the most last-place votes, and the candidate with minimum social utility. We determine the probability that IRV selects the weakest candidate under the impartial anonymous culture and impartial culture models of voter behavior, and use Monte Carlo simulations to estimate these probabilities under several spatial models. We also examine this question empirically using a large dataset of real elections. Our results show that IRV can select the weakest candidates under each of these definitions, but such outcomes are generally rare. Across most models, the probability that IRV elects a given type of weakest candidate is at most 5\%. Larger probabilities arise only when the electorate is extremely polarized.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21504v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David McCune, Jennifer Wilson</dc:creator>
    </item>
    <item>
      <title>Selecting representative community partitions under modularity degeneracy: the STAR method</title>
      <link>https://arxiv.org/abs/2602.21838</link>
      <description>arXiv:2602.21838v1 Announce Type: new 
Abstract: Community detection based on modularity maximization is one of the most widely used approaches for uncovering mesoscale structures in complex networks. However, it is well known that the modularity function exhibits a highly degenerate optimization landscape: a large number of structurally distinct partitions attain close modularity values. This degeneracy raises issues of instability, reproducibility, and interpretability of the detected communities. We propose a simple and user-friendly post-processing method to address this problem by selecting a representative partition among the set of high-modularity solutions. The proposed approach is model-agnostic and can be applied a posteriori to the output of any modularity-based community detection algorithm. Rather than seeking the optimal partition in terms of modularity, our method aims to identify a solution that best represents the structural features shared across degenerate partitions. We compare our approach with consensus clustering methods, which pursue a similar objective, and show that the resulting partitions are highly consistent, while being obtained through a substantially simpler procedure that does not require additional optimization steps or external software packages. Moreover, unlike standard consensus clustering techniques, the proposed method can be applied to networks with both positive and negative edge weights, making it suitable for a wide range of applications involving signed networks and correlation-based systems, such as social, financial, and neuroscience networks. Overall, the method provides a practical and robust tool for handling degeneracy in modularity-based community detection, combining simplicity with broad applicability across different types of networks and real-world problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21838v1</guid>
      <category>econ.GN</category>
      <category>math.OC</category>
      <category>physics.soc-ph</category>
      <category>q-fin.EC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesca Grassetti, Rossana Mastrandrea</dc:creator>
    </item>
    <item>
      <title>The economic alignment problem of artificial intelligence</title>
      <link>https://arxiv.org/abs/2602.21843</link>
      <description>arXiv:2602.21843v1 Announce Type: new 
Abstract: Artificial intelligence (AI) is advancing exponentially and is likely to have profound impacts on human wellbeing, social equity, and environmental sustainability. Here we argue that the "alignment problem" in AI research is also an economic alignment problem, as developing advanced AI inside a growth-based system is likely to increase social, environmental, and existential risks. We show that post-growth research offers concepts and policies that could substantially reduce AI risks, such as by replacing optimisation with satisficing, using the Doughnut of social and planetary boundaries to guide development, and curbing systemic rebound with resource caps. We propose governance and business reforms that treat AI as a commons and prioritise tool-like autonomy-enhancing systems over agentic AI. Finally, we argue that the development of artificial general intelligence (AGI) may require a new economics, for which post-growth scholarship provides a strong foundation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21843v1</guid>
      <category>econ.GN</category>
      <category>cs.CY</category>
      <category>q-fin.EC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel W. O'Neill, Stefano Vrizzi, Noemi Luna Carmeno, Felix Creutzig, Jefim Vogel</dc:creator>
    </item>
    <item>
      <title>Modelling the Index of Sustainable Economic Welfare (ISEW) and its response to policies</title>
      <link>https://arxiv.org/abs/2602.21971</link>
      <description>arXiv:2602.21971v1 Announce Type: new 
Abstract: Given the challenge of achieving societal welfare in an environmentally sustainable way, the Index of Sustainable Economic Welfare (ISEW) has emerged as an alternative indicator of progress in response to critiques of Gross Domestic Product (GDP). The ISEW compares the benefits of economic activity with its social and environmental costs. So far, most studies empirically analyse the ISEW for past developments, while no studies have simulated the ISEW using a dynamic macroeconomic model. We address this important gap by incorporating the ISEW into COMPASS, an ecological macroeconomic model that features the Doughnut of biophysical boundaries and social thresholds. First, we analyse how the ISEW is affected by three social and environmental policies: a carbon tax, income redistribution, and working-time reduction. We find that the ISEW grows in all scenarios. The strongest improvement over business-as-usual arises when all policies are combined, while the individual policies mostly affect the ISEW positively. Only in the case of working-time reduction, the ISEW decreases. Our study underscores the benefit of dynamically modelling the ISEW for anticipating the net effect of multiple impulses and their interconnections on the indicator. Second, we explore how the ISEW compares to GDP and the Doughnut when evaluating social and environmental policies. Our results suggest that the ISEW is better than GDP at capturing their effects, but it omits the full environmental costs of growth. We argue that the Doughnut, with its comprehensive picture of biophysical boundaries and social thresholds, provides better guidance for policymakers striving for sustainable wellbeing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21971v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luzie Dallinger, Reo Van Eynde, Jefim Vogel, Lorenzo Di Domenico, Se\'an Fearon, Tina Beigi, C\'edric Crofils, Kevin J. Dillman, Daniel W. O'Neill</dc:creator>
    </item>
    <item>
      <title>Intelligence Without Integrity: Why Capable LLMs May Undermine Reliability</title>
      <link>https://arxiv.org/abs/2602.20440</link>
      <description>arXiv:2602.20440v2 Announce Type: replace 
Abstract: As LLMs become embedded in research workflows and organizational decision processes, their effect on analytical reliability remains uncertain. We distinguish two dimensions of analytical reliability -- intelligence (the capacity to reach correct conclusions) and integrity (the stability of conclusions when analytically irrelevant cues about desired outcomes are introduced) -- and ask whether frontier LLMs possess both. Whether these dimensions trade off is theoretically ambiguous: the sophistication enabling accurate analysis may also enable responsiveness to non-evidential cues, or alternatively, greater capability may confer protection through better calibration and discernment. Using synthetically generated data with embedded ground truth, we evaluate fourteen models on a task simulating empirical analysis of hospital merger effects. We find that intelligence and integrity trade off: frontier models most likely to reach correct conclusions under neutral conditions are often most susceptible to shifting conclusions under motivated framing. We extend work on sycophancy by introducing goal-conditioned analytical sycophancy: sensitivity of inference to cues about desired outcomes, even when no belief is asserted and evidence is held constant. Unlike simple prompt sensitivity, models shift conclusions away from objective evidence in response to analytically irrelevant framing. This finding has important implications for empirical research and organizations. Selecting tools based on capability benchmarks may inadvertently select against the stability needed for reliable and replicable analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20440v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryan Allen, Aticus Peterson</dc:creator>
    </item>
    <item>
      <title>Some Simple Economics of AGI</title>
      <link>https://arxiv.org/abs/2602.20946</link>
      <description>arXiv:2602.20946v2 Announce Type: replace 
Abstract: For millennia, human cognition was the primary engine of progress on Earth. As AI decouples cognition from biology, the marginal cost of measurable execution falls to zero, absorbing any labor capturable by metrics--including creative, analytical, and innovative work. The binding constraint on growth is no longer intelligence but human verification bandwidth: the capacity to validate, audit, and underwrite responsibility when execution is abundant. We model the AGI transition as the collision of two racing cost curves: an exponentially decaying Cost to Automate and a biologically bottlenecked Cost to Verify. This structural asymmetry widens a Measurability Gap between what agents can execute and what humans can afford to verify. It also drives a shift from skill-biased to measurability-biased technical change. Rents migrate to verification-grade ground truth, cryptographic provenance, and liability underwriting--the ability to insure outcomes rather than merely generate them. The current human-in-the-loop equilibrium is unstable: eroded from below as apprenticeship collapses (Missing Junior Loop) and from within as experts codify their obsolescence (Codifier's Curse). Unverified deployment becomes privately rational--a Trojan Horse externality. Unmanaged, these forces pull toward a Hollow Economy. Yet by scaling verification alongside agentic capabilities, the forces that threaten collapse become the catalyst for unbounded discovery and experimentation--an Augmented Economy. We derive a practical playbook for individuals, companies, investors, and policymakers. Today's defining challenge is not the race to deploy the most autonomous systems; it is the race to secure the foundations of their oversight. Only by scaling our bandwidth for verification alongside our capacity for execution can we ensure that the intelligence we have summoned preserves the humanity that initiated it.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20946v2</guid>
      <category>econ.GN</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <category>q-fin.EC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Catalini, Xiang Hui, Jane Wu</dc:creator>
    </item>
  </channel>
</rss>
