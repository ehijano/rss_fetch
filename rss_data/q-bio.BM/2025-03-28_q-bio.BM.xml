<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.BM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.BM</link>
    <description>q-bio.BM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.BM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Mar 2025 04:00:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Can geometric combinatorics improve RNA branching predictions?</title>
      <link>https://arxiv.org/abs/2503.20977</link>
      <description>arXiv:2503.20977v1 Announce Type: new 
Abstract: Prior results for tRNA and 5S rRNA demonstrated that secondary structure prediction accuracy can be significantly improved by modifying the parameters in the multibranch loop entropic penalty function. However, for reasons not well understood at the time, the scale of improvement possible across both families was well below the level for each family when considered separately. We resolve this dichotomy here by showing that each family has a characteristic target region geometry, which is distinct from the other and significantly different from their own dinucleotide shuffles. This required a much more efficient approach to computing the necessary information from the branching parameter space, and a new theoretical characterization of the region geometries. The insights gained point strongly to considering multiple possible secondary structures generated by varying the multiloop parameters. We provide proof-of-principle results that this significantly improves prediction accuracy across all 8 additional families in the Archive II benchmarking dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20977v1</guid>
      <category>q-bio.BM</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Svetlana Poznanovi\'c, Owen Cardwell, Christine Heitsch</dc:creator>
    </item>
    <item>
      <title>Two for the Price of One: Integrating Large Language Models to Learn Biophysical Interactions</title>
      <link>https://arxiv.org/abs/2503.21017</link>
      <description>arXiv:2503.21017v1 Announce Type: new 
Abstract: Deep learning models have become fundamental tools in drug design. In particular, large language models trained on biochemical sequences learn feature vectors that guide drug discovery through virtual screening. However, such models do not capture the molecular interactions important for binding affinity and specificity. Therefore, there is a need to 'compose' representations from distinct biological modalities to effectively represent molecular complexes. We present an overview of the methods to combine molecular representations and propose that future work should balance computational efficiency and expressiveness. Specifically, we argue that improvements in both speed and accuracy are possible by learning to merge the representations from internal layers of domain specific biological language models. We demonstrate that 'composing' biochemical language models performs similar or better than standard methods representing molecular interactions despite having significantly fewer features. Finally, we discuss recent methods for interpreting and democratizing large language models that could aid the development of interaction aware foundation models for biology, as well as their shortcomings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21017v1</guid>
      <category>q-bio.BM</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Joseph D. Clark, Tanner J. Dean, Diwakar Shukla</dc:creator>
    </item>
    <item>
      <title>Regulation of Dendritic Cell Function by Ermiaosan via the EP4-cAMP-CREB Signaling Pathway</title>
      <link>https://arxiv.org/abs/2503.21233</link>
      <description>arXiv:2503.21233v1 Announce Type: new 
Abstract: Ermiao San (EMS), a traditional Chinese medicine composed of Atractylodes macrocephala and Cortex Phellodendron, has demonstrated therapeutic efficacy in rheumatoid arthritis (RA). Studies suggest that EMS modulates dendritic cell (DC) maturation in adjuvant arthritis (AA) rats, though the precise mechanisms remain unclear. Prostaglandin receptor 4 (EP4) is critical in inflammation and DC function, while cyclic adenosine monophosphate (cAMP) regulates cellular signaling, potentially influencing RA pathogenesis via protein kinase A (PKA) and cAMP response element-binding protein (CREB) activation. EMS exerts protective effects in RA rats by suppressing DC functions, including reduced EP4 mRNA/protein expression, diminished cAMP levels, and impaired CREB phosphorylation. Additionally, serum from EMS-treated rats inhibited antigen uptake by bone marrow-derived DCs (BMDCs), downregulating CD40, CD80, and CD86 expression and altering pro-inflammatory cytokine secretion. Mechanistically, EMS-treated serum suppressed the EP4-cAMP pathway by decreasing EP4 protein expression and CREB activation, alongside reduced intracellular cAMP and PKA levels in BMDCs co-stimulated with PGE2 and TNF-a. These findings indicate that EMS alleviates RA by inhibiting the EP4-cAMP-CREB signaling axis in DCs, providing a scientific rationale for its clinical application in RA treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21233v1</guid>
      <category>q-bio.BM</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jie-Min Ding, Liu Min, Wang Jin, Si-Meng Cheng, Xiang-Wen Meng, Xiao-Yi Jia, Wang Ning</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Benchmark for RNA 3D Structure-Function Modeling</title>
      <link>https://arxiv.org/abs/2503.21681</link>
      <description>arXiv:2503.21681v1 Announce Type: new 
Abstract: The RNA structure-function relationship has recently garnered significant attention within the deep learning community, promising to grow in importance as nucleic acid structure models advance. However, the absence of standardized and accessible benchmarks for deep learning on RNA 3D structures has impeded the development of models for RNA functional characteristics.
  In this work, we introduce a set of seven benchmarking datasets for RNA structure-function prediction, designed to address this gap. Our library builds on the established Python library rnaglib, and offers easy data distribution and encoding, splitters and evaluation methods, providing a convenient all-in-one framework for comparing models. Datasets are implemented in a fully modular and reproducible manner, facilitating for community contributions and customization. Finally, we provide initial baseline results for all tasks using a graph neural network.
  Source code: https://github.com/cgoliver/rnaglib
  Documentation: https://rnaglib.org</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21681v1</guid>
      <category>q-bio.BM</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luis Wyss, Vincent Mallet, Wissam Karroucha, Karsten Borgwardt, Carlos Oliver</dc:creator>
    </item>
    <item>
      <title>CMADiff: Cross-Modal Aligned Diffusion for Controllable Protein Generation</title>
      <link>https://arxiv.org/abs/2503.21450</link>
      <description>arXiv:2503.21450v1 Announce Type: cross 
Abstract: AI-assisted protein design has emerged as a critical tool for advancing biotechnology, as deep generative models have demonstrated their reliability in this domain. However, most existing models primarily utilize protein sequence or structural data for training, neglecting the physicochemical properties of proteins.Moreover, they are deficient to control the generation of proteins in intuitive conditions. To address these limitations,we propose CMADiff here, a novel framework that enables controllable protein generation by aligning the physicochemical properties of protein sequences with text-based descriptions through a latent diffusion process. Specifically, CMADiff employs a Conditional Variational Autoencoder (CVAE) to integrate physicochemical features as conditional input, forming a robust latent space that captures biological traits. In this latent space, we apply a conditional diffusion process, which is guided by BioAligner, a contrastive learning-based module that aligns text descriptions with protein features, enabling text-driven control over protein sequence generation. Validated by a series of evaluations including AlphaFold3, the experimental results indicate that CMADiff outperforms protein sequence generation benchmarks and holds strong potential for future applications. The implementation and code are available at https://github.com/HPC-NEAU/PhysChemDiff.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21450v1</guid>
      <category>cs.CE</category>
      <category>q-bio.BM</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changjian Zhou, Yuexi Qiu, Tongtong Ling, Jiafeng Li, Shuanghe Liu, Xiangjing Wang, Jia Song, Wensheng Xiang</dc:creator>
    </item>
    <item>
      <title>Long-context Protein Language Modeling Using Bidirectional Mamba with Shared Projection Layers</title>
      <link>https://arxiv.org/abs/2411.08909</link>
      <description>arXiv:2411.08909v2 Announce Type: replace 
Abstract: Self-supervised training of language models (LMs) has seen great success for protein sequences in learning meaningful representations and for generative drug design. Most protein LMs are based on the Transformer architecture trained on individual proteins with short context lengths. Such protein LMs cannot extrapolate to longer proteins and protein complexes well. They also fail to account for the underlying biological mechanisms carried out by biomolecular interactions and dynamics i.e., proteins often interact with other proteins, molecules, and pathways in complex biological systems. In this work, we propose LC-PLM based on an alternative protein LM architecture, BiMamba-S, built upon selective structured state-space models, to learn high-quality universal protein representations at the amino acid token level using masked language modeling. We also introduce its graph-contextual variant, LC-PLM-G, which contextualizes protein-protein interaction (PPI) graphs for a second stage of training. LC-PLM demonstrates favorable neural scaling laws, better length extrapolation capability, and a 7% to 34% improvement on protein downstream tasks than Transformer-based ESM-2. LC-PLM-G further trained within the context of PPI graphs shows promising results on protein structure and function prediction tasks. Our study demonstrates the benefit of increasing the context size with computationally efficient LM architecture (e.g. structured state space models) in learning universal protein representations and incorporating molecular interaction context contained in biological graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08909v2</guid>
      <category>q-bio.BM</category>
      <category>cs.LG</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yingheng Wang, Zichen Wang, Gil Sadeh, Luca Zancato, Alessandro Achille, George Karypis, Huzefa Rangwala</dc:creator>
    </item>
    <item>
      <title>Inductive-Associative Meta-learning Pipeline with Human Cognitive Patterns for Unseen Drug-Target Interaction Prediction</title>
      <link>https://arxiv.org/abs/2501.16391</link>
      <description>arXiv:2501.16391v2 Announce Type: replace-cross 
Abstract: Significant differences in protein structures hinder the generalization of existing drug-target interaction (DTI) models, which often rely heavily on pre-learned binding principles or detailed annotations. In contrast, BioBridge designs an Inductive-Associative pipeline inspired by the workflow of scientists who base their accumulated expertise on drawing insights into novel drug-target pairs from weakly related references. BioBridge predicts novel drug-target interactions using limited sequence data, incorporating multi-level encoders with adversarial training to accumulate transferable binding principles. On these principles basis, BioBridge employs a dynamic prototype meta-learning framework to associate insights from weakly related annotations, enabling robust predictions for previously unseen drug-target pairs. Extensive experiments demonstrate that BioBridge surpasses existing models, especially for unseen proteins. Notably, when only homologous protein binding data is available, BioBridge proves effective for virtual screening of the epidermal growth factor receptor and adenosine receptor, underscoring its potential in drug discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16391v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.BM</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoqing Lian, Jie Zhu, Tianxu Lv, Shiyun Nie, Hang Fan, Guosheng Wu, Yunjun Ge, Lihua Li, Xiangxiang Zeng, Xiang Pan</dc:creator>
    </item>
  </channel>
</rss>
