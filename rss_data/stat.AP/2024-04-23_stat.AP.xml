<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Apr 2024 04:00:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Scalable Bayesian Image-on-Scalar Regression for Population-Scale Neuroimaging Data Analysis</title>
      <link>https://arxiv.org/abs/2404.13204</link>
      <description>arXiv:2404.13204v1 Announce Type: new 
Abstract: Bayesian Image-on-Scalar Regression (ISR) offers significant advantages for neuroimaging data analysis, including flexibility and the ability to quantify uncertainty. However, its application to large-scale imaging datasets, such as found in the UK Biobank, is hindered by the computational demands of traditional posterior computation methods, as well as the challenge of individual-specific brain masks that deviate from the common mask typically used in standard ISR approaches. To address these challenges, we introduce a novel Bayesian ISR model that is scalable and accommodates inconsistent brain masks across subjects in large scale imaging studies. Our model leverages Gaussian process priors and integrates salience area indicators to facilitate ISR. We develop a cutting-edge scalable posterior computation algorithm that employs stochastic gradient Langevin dynamics coupled with memory mapping techniques, ensuring that computation time scales linearly with subsample size and memory usage is constrained only by the batch size. Our approach uniquely enables direct spatial posterior inferences on brain activation regions. The efficacy of our method is demonstrated through simulations and analysis of the UK Biobank task fMRI data, encompassing 8411 subjects and over 120,000 voxels per image, showing that it can achieve a speed increase of 4 to 11 times and enhance statistical power by 8% to 18% compared to traditional Gibbs sampling with zero-imputation in various simulation scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13204v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuliang Xu, Timothy D. Johnson, Thomas E. Nichols, Jian Kang</dc:creator>
    </item>
    <item>
      <title>Accelerated System-Reliability-based Disaster Resilience Analysis for Structural Systems</title>
      <link>https://arxiv.org/abs/2404.13321</link>
      <description>arXiv:2404.13321v1 Announce Type: new 
Abstract: Resilience has emerged as a crucial concept for evaluating structural performance under disasters because of its ability to extend beyond traditional risk assessments, accounting for a system's ability to minimize disruptions and maintain functionality during recovery. To facilitate the holistic understanding of resilience performance in structural systems, a system-reliability-based disaster resilience analysis framework was developed. The framework describes resilience using three criteria: reliability, redundancy, and recoverability, and the system's internal resilience is evaluated by inspecting the characteristics of reliability and redundancy for different possible progressive failure modes. However, the practical application of this framework has been limited to complex structures with numerous sub-components, as it becomes intractable to evaluate the performances for all possible initial disruption scenarios. To bridge the gap between the theory and practical use, especially for evaluating reliability and redundancy, this study centers on the idea that the computational burden can be substantially alleviated by focusing on initial disruption scenarios that are practically significant. To achieve this research goal, we propose three methods to efficiently eliminate insignificant scenarios: the sequential search method, the n-ball sampling method, and the surrogate model-based adaptive sampling algorithm. Three numerical examples, including buildings and a bridge, are introduced to prove the applicability and efficiency of the proposed approaches. The findings of this study are expected to offer practical solutions to the challenges of assessing resilience performance in complex structural systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13321v1</guid>
      <category>stat.AP</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taeyong Kim, Sang-ri Yi</dc:creator>
    </item>
    <item>
      <title>A Bayesian Hybrid Design with Borrowing from Historical Study</title>
      <link>https://arxiv.org/abs/2404.13177</link>
      <description>arXiv:2404.13177v1 Announce Type: cross 
Abstract: In early phase drug development of combination therapy, the primary objective is to preliminarily assess whether there is additive activity when a novel agent combined with an established monotherapy. Due to potential feasibility issues with a large randomized study, uncontrolled single-arm trials have been the mainstream approach in cancer clinical trials. However, such trials often present significant challenges in deciding whether to proceed to the next phase of development. A hybrid design, leveraging data from a completed historical clinical study of the monotherapy, offers a valuable option to enhance study efficiency and improve informed decision-making. Compared to traditional single-arm designs, the hybrid design may significantly enhance power by borrowing external information, enabling a more robust assessment of activity. The primary challenge of hybrid design lies in handling information borrowing. We introduce a Bayesian dynamic power prior (DPP) framework with three components of controlling amount of dynamic borrowing. The framework offers flexible study design options with explicit interpretation of borrowing, allowing customization according to specific needs. Furthermore, the posterior distribution in the proposed framework has a closed form, offering significant advantages in computational efficiency. The proposed framework's utility is demonstrated through simulations and a case study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13177v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaohua Lu, John Toso, Girma Ayele, Philip He</dc:creator>
    </item>
    <item>
      <title>Difference-in-Differences under Bipartite Network Interference: A Framework for Quasi-Experimental Assessment of the Effects of Environmental Policies on Health</title>
      <link>https://arxiv.org/abs/2404.13442</link>
      <description>arXiv:2404.13442v1 Announce Type: cross 
Abstract: Pollution from coal-fired power plants has been linked to substantial health and mortality burdens in the US. In recent decades, federal regulatory policies have spurred efforts to curb emissions through various actions, such as the installation of emissions control technologies on power plants. However, assessing the health impacts of these measures, particularly over longer periods of time, is complicated by several factors. First, the units that potentially receive the intervention (power plants) are disjoint from those on which outcomes are measured (communities), and second, pollution emitted from power plants disperses and affects geographically far-reaching areas. This creates a methodological challenge known as bipartite network interference (BNI). To our knowledge, no methods have been developed for conducting quasi-experimental studies with panel data in the BNI setting. In this study, motivated by the need for robust estimates of the total health impacts of power plant emissions control technologies in recent decades, we introduce a novel causal inference framework for difference-in-differences analysis under BNI with staggered treatment adoption. We explain the unique methodological challenges that arise in this setting and propose a solution via a data reconfiguration and mapping strategy. The proposed approach is advantageous because analysis is conducted at the intervention unit level, avoiding the need to arbitrarily define treatment status at the outcome unit level, but it permits interpretation of results at the more policy-relevant outcome unit level. Using this interference-aware approach, we investigate the impacts of installation of flue gas desulfurization scrubbers on coal-fired power plants on coronary heart disease hospitalizations among older Americans over the period 2003-2014, finding an overall beneficial effect in mitigating such disease outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13442v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kevin L. Chen, Falco J. Bargagli-Stoffi, Raphael C. Kim, Lucas R. F. Henneman, Rachel C. Nethery</dc:creator>
    </item>
    <item>
      <title>Robust inference for the unification of confidence intervals in meta-analysis</title>
      <link>https://arxiv.org/abs/2404.13707</link>
      <description>arXiv:2404.13707v1 Announce Type: cross 
Abstract: Traditional meta-analysis assumes that the effect sizes estimated in individual studies follow a Gaussian distribution. However, this distributional assumption is not always satisfied in practice, leading to potentially biased results. In the situation when the number of studies, denoted as K, is large, the cumulative Gaussian approximation errors from each study could make the final estimation unreliable. In the situation when K is small, it is not realistic to assume the random-effect follows Gaussian distribution. In this paper, we present a novel empirical likelihood method for combining confidence intervals under the meta-analysis framework. This method is free of the Gaussian assumption in effect size estimates from individual studies and from the random-effects. We establish the large-sample properties of the non-parametric estimator, and introduce a criterion governing the relationship between the number of studies, K, and the sample size of each study, n_i. Our methodology supersedes conventional meta-analysis techniques in both theoretical robustness and computational efficiency. We assess the performance of our proposed methods using simulation studies, and apply our proposed methods to two examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13707v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Liang, Haicheng Huang, Hongsheng Dai, Yinghui Wei</dc:creator>
    </item>
    <item>
      <title>Inference for multiple change-points in generalized integer-valued autoregressive model</title>
      <link>https://arxiv.org/abs/2404.13834</link>
      <description>arXiv:2404.13834v1 Announce Type: cross 
Abstract: In this paper, we propose a computationally valid and theoretically justified methods, the likelihood ratio scan method (LRSM), for estimating multiple change-points in a piecewise stationary generalized conditional integer-valued autoregressive process. LRSM with the usual window parameter $h$ is more satisfied to be used in long-time series with few and even change-points vs. LRSM with the multiple window parameter $h_{mix}$ performs well in short-time series with large and dense change-points. The computational complexity of LRSM can be efficiently performed with order $O((\log n)^3 n)$. Moreover, two bootstrap procedures, namely parametric and block bootstrap, are developed for constructing confidence intervals (CIs) for each of the change-points. Simulation experiments and real data analysis show that the LRSM and bootstrap procedures have excellent performance and are consistent with the theoretical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13834v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Danshu Sheng, Dehui Wang</dc:creator>
    </item>
    <item>
      <title>Stochastic Volatility in Mean: Efficient Analysis by a Generalized Mixture Sampler</title>
      <link>https://arxiv.org/abs/2404.13986</link>
      <description>arXiv:2404.13986v1 Announce Type: cross 
Abstract: In this paper we consider the simulation-based Bayesian analysis of stochastic volatility in mean (SVM) models. Extending the highly efficient Markov chain Monte Carlo mixture sampler for the SV model proposed in Kim et al. (1998) and Omori et al. (2007), we develop an accurate approximation of the non-central chi-squared distribution as a mixture of thirty normal distributions. Under this mixture representation, we sample the parameters and latent volatilities in one block. We also detail a correction of the small approximation error by using additional Metropolis-Hastings steps. The proposed method is extended to the SVM model with leverage. The methodology and models are applied to excess holding yields in empirical studies, and the SVM model with leverage is shown to outperform competing volatility models based on marginal likelihoods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13986v1</guid>
      <category>econ.EM</category>
      <category>q-fin.MF</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daichi Hiraki, Siddhartha Chib, Yasuhiro Omori</dc:creator>
    </item>
    <item>
      <title>A Multimodal Feature Distillation with CNN-Transformer Network for Brain Tumor Segmentation with Incomplete Modalities</title>
      <link>https://arxiv.org/abs/2404.14019</link>
      <description>arXiv:2404.14019v1 Announce Type: cross 
Abstract: Existing brain tumor segmentation methods usually utilize multiple Magnetic Resonance Imaging (MRI) modalities in brain tumor images for segmentation, which can achieve better segmentation performance. However, in clinical applications, some modalities are missing due to resource constraints, leading to severe degradation in the performance of methods applying complete modality segmentation. In this paper, we propose a Multimodal feature distillation with Convolutional Neural Network (CNN)-Transformer hybrid network (MCTSeg) for accurate brain tumor segmentation with missing modalities. We first design a Multimodal Feature Distillation (MFD) module to distill feature-level multimodal knowledge into different unimodality to extract complete modality information. We further develop a Unimodal Feature Enhancement (UFE) module to model the relationship between global and local information semantically. Finally, we build a Cross-Modal Fusion (CMF) module to explicitly align the global correlations among different modalities even when some modalities are missing. Complementary features within and across different modalities are refined via the CNN-Transformer hybrid architectures in both the UFE and CMF modules, where local and global dependencies are both captured. Our ablation study demonstrates the importance of the proposed modules with CNN-Transformer networks and the convolutional blocks in Transformer for improving the performance of brain tumor segmentation with missing modalities. Extensive experiments on the BraTS2018 and BraTS2020 datasets show that the proposed MCTSeg framework outperforms the state-of-the-art methods in missing modalities cases. Our code is available at: https://github.com/mkang315/MCTSeg.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14019v1</guid>
      <category>cs.CV</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ming Kang, Fung Fung Ting, Rapha\"el C. -W. Phan, Zongyuan Ge, Chee-Ming Ting</dc:creator>
    </item>
    <item>
      <title>Competition and Collaboration in Crowdsourcing Communities: What happens when peers evaluate each other?</title>
      <link>https://arxiv.org/abs/2404.14141</link>
      <description>arXiv:2404.14141v1 Announce Type: cross 
Abstract: Crowdsourcing has evolved as an organizational approach to distributed problem solving and innovation. As contests are embedded in online communities and evaluation rights are assigned to the crowd, community members face a tension: they find themselves exposed to both competitive motives to win the contest prize and collaborative participation motives in the community. The competitive motive suggests they may evaluate rivals strategically according to their self-interest, the collaborative motive suggests they may evaluate their peers truthfully according to mutual interest. Using field data from Threadless on 38 million peer evaluations of more than 150,000 submissions across 75,000 individuals over 10 years and two natural experiments to rule out alternative explanations, we answer the question of how community members resolve this tension. We show that as their skill level increases, they become increasingly competitive and shift from using self-promotion to sabotaging their closest competitors. However, we also find signs of collaborative behavior when high-skilled members show leniency toward those community members who do not directly threaten their chance of winning. We explain how the individual-level use of strategic evaluations translates into important organizational-level outcomes by affecting the community structure through individuals' long-term participation. While low-skill targets of sabotage are less likely to participate in future contests, high-skill targets are more likely. This suggests a feedback loop between competitive evaluation behavior and future participation. These findings have important implications for the literature on crowdsourcing design, and the evolution and sustainability of crowdsourcing communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14141v1</guid>
      <category>econ.GN</category>
      <category>cs.GT</category>
      <category>cs.HC</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1287/orsc.2021.15163</arxiv:DOI>
      <arxiv:journal_reference>Organization Science, 2024</arxiv:journal_reference>
      <dc:creator>Christoph Riedl, Tom Grad, Christopher Lettl</dc:creator>
    </item>
    <item>
      <title>Maximally informative feature selection using Information Imbalance: Application to COVID-19 severity prediction</title>
      <link>https://arxiv.org/abs/2404.14275</link>
      <description>arXiv:2404.14275v1 Announce Type: cross 
Abstract: Clinical databases typically include, for each patient, many heterogeneous features, for example blood exams, the clinical history before the onset of the disease, the evolution of the symptoms, the results of imaging exams, and many others. We here propose to exploit a recently developed statistical approach, the Information Imbalance, to compare different subsets of patient features, and automatically select the set of features which is maximally informative for a given clinical purpose, especially in minority classes. We adapt the Information Imbalance approach to work in a clinical framework, where patient features are often categorical and are generally available only for a fraction of the patients. We apply this algorithm to a data set of ~ 1,300 patients treated for COVID-19 in Udine hospital before October 2021. Using this approach, we find combinations of features which, if used in combination, are maximally informative of the clinical fate and of the severity of the disease. The optimal number of features, which is determined automatically, turns out to be between 10 and 15. These features can be measured at admission. The approach can be used also if the features are available only for a fraction of the patients, does not require imputation and, importantly, is able to automatically select features with small inter-feature correlation. Clinical insights deriving from this study are also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14275v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Romina Wild, Emanuela Sozio, Riccardo G. Margiotta, Fabiana Dellai, Angela Acquasanta, Fabio Del Ben, Carlo Tascini, Francesco Curcio, Alessandro Laio</dc:creator>
    </item>
    <item>
      <title>Statistical Validation of Contagion Centrality in Financial Networks</title>
      <link>https://arxiv.org/abs/2404.14337</link>
      <description>arXiv:2404.14337v1 Announce Type: cross 
Abstract: In this paper, we introduce a novel centrality measure to evaluate shock propagation on financial networks capturing a notion of contagion and systemic risk contributions. In comparison to many popular centrality metrics (e.g., eigenvector centrality) which provide only a relative centrality between nodes, our proposed measure is in an absolute scale permitting comparisons of contagion risk over time. In addition, we provide a statistical validation method when the network is estimated from data, as is done in practice. This statistical test allows us to reliably assess the computed centrality values. We validate our methodology on simulated data and conduct empirical case studies using financial data. We find that our proposed centrality measure increases significantly during times of financial distress and is able to provide insights in to the (market implied) risk-levels of different firms and sectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14337v1</guid>
      <category>q-fin.MF</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Agathe Sadeghi, Zachary Feinstein</dc:creator>
    </item>
    <item>
      <title>Distributed ARIMA Models for Ultra-long Time Series</title>
      <link>https://arxiv.org/abs/2007.09577</link>
      <description>arXiv:2007.09577v4 Announce Type: replace 
Abstract: Providing forecasts for ultra-long time series plays a vital role in various activities, such as investment decisions, industrial production arrangements, and farm management. This paper develops a novel distributed forecasting framework to tackle challenges associated with forecasting ultra-long time series by using the industry-standard MapReduce framework. The proposed model combination approach facilitates distributed time series forecasting by combining the local estimators of time series models delivered from worker nodes and minimizing a global loss function. In this way, instead of unrealistically assuming the data generating process (DGP) of an ultra-long time series stays invariant, we make assumptions only on the DGP of subseries spanning shorter time periods. We investigate the performance of the proposed approach with AutoRegressive Integrated Moving Average (ARIMA) models using the real data application as well as numerical simulations. Compared to directly fitting the whole data with ARIMA models, our approach results in improved forecasting accuracy and computational efficiency both in point forecasts and prediction intervals, especially for longer forecast horizons. Moreover, we explore some potential factors that may affect the forecasting performance of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2007.09577v4</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ijforecast.2022.05.001</arxiv:DOI>
      <dc:creator>Xiaoqian Wang, Yanfei Kang, Rob J Hyndman, Feng Li</dc:creator>
    </item>
    <item>
      <title>Modelling non-stationarity in asymptotically independent extremes</title>
      <link>https://arxiv.org/abs/2203.05860</link>
      <description>arXiv:2203.05860v4 Announce Type: replace 
Abstract: In many practical applications, evaluating the joint impact of combinations of environmental variables is important for risk management and structural design analysis. When such variables are considered simultaneously, non-stationarity can exist within both the marginal distributions and dependence structure, resulting in complex data structures. In the context of extremes, few methods have been proposed for modelling trends in extremal dependence, even though capturing this feature is important for quantifying joint impact. Moreover, most proposed techniques are only applicable to data structures exhibiting asymptotic dependence. Motivated by observed dependence trends of data from the UK Climate Projections, we propose a novel semi-parametric modelling framework for bivariate extremal dependence structures. This framework allows us to capture a wide variety of dependence trends for data exhibiting asymptotic independence. When applied to the climate projection dataset, our model detects significant dependence trends in observations and, in combination with models for marginal non-stationarity, can be used to produce estimates of bivariate risk measures at future time points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.05860v4</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>C. J. R. Murphy-Barltrop, J. L. Wadsworth</dc:creator>
    </item>
    <item>
      <title>A Bayesian Basket Trial Design Using Local Power Prior</title>
      <link>https://arxiv.org/abs/2312.15352</link>
      <description>arXiv:2312.15352v2 Announce Type: replace 
Abstract: In recent years, basket trials, which enable the evaluation of an experimental therapy across multiple tumor types within a single protocol, have gained prominence in early-phase oncology development. Unlike traditional trials, where each tumor type is evaluated separately with limited sample size, basket trials offer the advantage of borrowing information across various tumor types. However, a key challenge in designing basket trials lies in dynamically determining the extent of information borrowing across tumor types to enhance statistical power while maintaining an acceptable type I error rate. In this paper, we propose a local power prior framework that includes a 3-component borrowing mechanism with explicit model interpretation. Unlike many existing Bayesian methods that require Markov Chain Monte Carlo (MCMC) sampling, the proposed framework offers a closed-form solution, eliminating the time-consuming nature of MCMC in large-scale simulations for evaluating operating characteristics. Extensive simulations have been conducted and demonstrated a good performance of the proposal method comparable to the other complex methods. The significantly shortened computation time further underscores the practical utility in the context of basket trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15352v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haiming Zhou, Rex Shen, Sutan Wu, Philip He</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Heterogeneous Treatment Effects Discovered by Generic Machine Learning in Randomized Experiments</title>
      <link>https://arxiv.org/abs/2203.14511</link>
      <description>arXiv:2203.14511v3 Announce Type: replace-cross 
Abstract: Researchers are increasingly turning to machine learning (ML) algorithms to investigate causal heterogeneity in randomized experiments. Despite their promise, ML algorithms may fail to accurately ascertain heterogeneous treatment effects under practical settings with many covariates and small sample size. In addition, the quantification of estimation uncertainty remains a challenge. We develop a general approach to statistical inference for heterogeneous treatment effects discovered by a generic ML algorithm. We apply the Neyman's repeated sampling framework to a common setting, in which researchers use an ML algorithm to estimate the conditional average treatment effect and then divide the sample into several groups based on the magnitude of the estimated effects. We show how to estimate the average treatment effect within each of these groups, and construct a valid confidence interval. In addition, we develop nonparametric tests of treatment effect homogeneity across groups, and rank-consistency of within-group average treatment effects. The validity of our methodology does not rely on the properties of ML algorithms because it is solely based on the randomization of treatment assignment and random sampling of units. Finally, we generalize our methodology to the cross-fitting procedure by accounting for the additional uncertainty induced by the random splitting of data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.14511v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kosuke Imai, Michael Lingzhi Li</dc:creator>
    </item>
    <item>
      <title>lmw: Linear Model Weights for Causal Inference</title>
      <link>https://arxiv.org/abs/2303.08790</link>
      <description>arXiv:2303.08790v2 Announce Type: replace-cross 
Abstract: The linear regression model is widely used in the biomedical and social sciences as well as in policy and business research to adjust for covariates and estimate the average effects of treatments. Behind every causal inference endeavor there is a hypothetical randomized experiment. However, in routine regression analyses in observational studies, it is unclear how well the adjustments made by regression approximate key features of randomized experiments, such as covariate balance, study representativeness, sample boundedness, and unweighted sampling. In this paper, we provide software to empirically address this question. We introduce the lmw package for R to compute the implied linear model weights and perform diagnostics for their evaluation. The weights are obtained as part of the design stage of the study; that is, without using outcome information. The implementation is general and applicable, for instance, in settings with instrumental variables and multi-valued treatments; in essence, in any situation where the linear model is the vehicle for adjustment and estimation of average treatment effects with discrete-valued interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.08790v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ambarish Chattopadhyay, Noah Greifer, Jose R. Zubizarreta</dc:creator>
    </item>
    <item>
      <title>Generalized Score Matching</title>
      <link>https://arxiv.org/abs/2303.08987</link>
      <description>arXiv:2303.08987v2 Announce Type: replace-cross 
Abstract: Score matching is an estimation procedure that has been developed for statistical models whose probability density function is known up to proportionality but whose normalizing constant is intractable, so that maximum likelihood is difficult or impossible to implement. To date, applications of score matching have focused more on continuous IID models. Motivated by various data modelling problems, this article proposes a unified asymptotic theory of generalized score matching developed under the independence assumption, covering both continuous and discrete response data, thereby giving a sound basis for score-matchingbased inference. Real data analyses and simulation studies provide convincing evidence of strong practical performance of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.08987v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiazhen Xu, Janice L. Scealy, Andrew T. A. Wood, Tao Zou</dc:creator>
    </item>
    <item>
      <title>Improving estimation for asymptotically independent bivariate extremes via global estimators for the angular dependence function</title>
      <link>https://arxiv.org/abs/2303.13237</link>
      <description>arXiv:2303.13237v3 Announce Type: replace-cross 
Abstract: Modelling the extremal dependence of bivariate variables is important in a wide variety of practical applications, including environmental planning, catastrophe modelling and hydrology. The majority of these approaches are based on the framework of bivariate regular variation, and a wide range of literature is available for estimating the dependence structure in this setting. However, such procedures are only applicable to variables exhibiting asymptotic dependence, even though asymptotic independence is often observed in practice. In this paper, we consider the so-called `angular dependence function'; this quantity summarises the extremal dependence structure for asymptotically independent variables. Until recently, only pointwise estimators of the angular dependence function have been available. We introduce a range of global estimators and compare them to another recently introduced technique for global estimation through a systematic simulation study, and a case study on river flow data from the north of England, UK.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.13237v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>C. J. R. Murphy-Barltrop, J. L. Wadsworth, E. F. Eastoe</dc:creator>
    </item>
    <item>
      <title>Masked Transformer for Electrocardiogram Classification</title>
      <link>https://arxiv.org/abs/2309.07136</link>
      <description>arXiv:2309.07136v2 Announce Type: replace-cross 
Abstract: Electrocardiogram (ECG) is one of the most important diagnostic tools in clinical applications. With the advent of advanced algorithms, various deep learning models have been adopted for ECG tasks. However, the potential of Transformers for ECG data is not yet realized, despite their widespread success in computer vision and natural language processing. In this work, we present a useful masked Transformer method for ECG classification referred to as MTECG, which expands the application of masked autoencoders to ECG time series. We construct a dataset comprising 220,251 ECG recordings with a broad range of diagnoses annoated by medical experts to explore the properties of MTECG. Under the proposed training strategies, a lightweight model with 5.7M parameters performs stably well on a broad range of masking ratios (5%-75%). The ablation studies highlight the importance of fluctuated reconstruction targets, training schedule length, layer-wise LR decay and DropPath rate. The experiments on both private and public ECG datasets demonstrate that MTECG-T significantly outperforms the recent state-of-the-art algorithms in ECG classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.07136v2</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ya Zhou, Xiaolin Diao, Yanni Huo, Yang Liu, Xiaohan Fan, Wei Zhao</dc:creator>
    </item>
  </channel>
</rss>
