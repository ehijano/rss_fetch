<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 13 Oct 2025 04:00:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Mozart: A Chiplet Ecosystem-Accelerator Codesign Framework for Composable Bespoke Application Specific Integrated Circuits</title>
      <link>https://arxiv.org/abs/2510.08873</link>
      <description>arXiv:2510.08873v1 Announce Type: new 
Abstract: Modern AI acceleration faces a fundamental challenge: conventional assumptions about memory requirements, batching effectiveness, and latency-throughput tradeoffs are systemwide generalizations that ignore the heterogeneous computational patterns of individual neural network operators. However, going towards network-level customization and operator-level heterogeneity incur substantial Non-Recurring Engineering (NRE) costs. While chiplet-based approaches have been proposed to amortize NRE costs, reuse opportunities remain limited without carefully identifying which chiplets are truly necessary. This paper introduces Mozart, a chiplet ecosystem and accelerator codesign framework that systematically constructs low cost bespoke application-specific integrated circuits (BASICs). BASICs leverage operator-level disaggregation to explore chiplet and memory heterogeneity, tensor fusion, and tensor parallelism, with place-and-route validation ensuring physical implementability. The framework also enables constraint-aware system-level optimization across deployment contexts ranging from datacenter inference serving to edge computing in autonomous vehicles. The evaluation confirms that with just 8 strategically selected chiplets, Mozart-generated composite BASICs achieve 43.5%, 25.4%, 67.7%, and 78.8% reductions in energy, energy-cost product, energy-delay product (EDP), and energy-delay-cost product compared to traditional homogeneous accelerators. For datacenter LLM serving, Mozart achieves 15-19% energy reduction and 35-39% energy-cost improvement. In speculative decoding, Mozart delivers throughput improvements of 24.6-58.6% while reducing energy consumption by 38.6-45.6%. For autonomous vehicle perception, Mozart reduces energy-cost by 25.54% and energy by 10.53% under real-time constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08873v1</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoran Jin, Jirong Yang, Yunpeng Liu, Barry Lyu, Kangqi Zhang, Nathaniel Bleier</dc:creator>
    </item>
    <item>
      <title>A High-Efficiency SoC for Next-Generation Mobile DNA Sequencing</title>
      <link>https://arxiv.org/abs/2510.08940</link>
      <description>arXiv:2510.08940v1 Announce Type: new 
Abstract: Hand-sized Deoxyribonucleic acid (DNA) sequencing machines are of growing importance in several life sciences fields as their small footprints enable a broader range of use cases than their larger, stationary counterparts. However, as currently designed, they lack sufficient embedded computing to process the large volume of measurements generated by their internal sensory system. As a consequence, they rely on external devices for additional processing capability. This dependence on external processing places a significant communication burden on the sequencer's embedded electronics. Moreover, it also prevents a truly mobile solution for sequencing in real-time. Anticipating next-generation machines that include suitably advanced processing, we present a System-on-Chip (SoC) fabricated in 22-nm complementary metal-oxide semiconductor (CMOS). Our design, based on a general-purpose reduced instruction set computing (RISC-V) core, also includes accelerators for DNA detection that allow our system to demonstrate a 13X performance improvement over commercial embedded multicore processors combined with a near 3000X boost in energy efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08940v1</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abel Beyene, Zhongpan Wu, Yunus Dawji, Karim Hammad, Ebrahim Ghafar-Zadeh, Sebastian Magierowski</dc:creator>
    </item>
    <item>
      <title>HERO: Hardware-Efficient RL-based Optimization Framework for NeRF Quantization</title>
      <link>https://arxiv.org/abs/2510.09010</link>
      <description>arXiv:2510.09010v1 Announce Type: new 
Abstract: Neural Radiance Field (NeRF) has emerged as a promising 3D reconstruction method, delivering high-quality results for AR/VR applications. While quantization methods and hardware accelerators have been proposed to enhance NeRF's computational efficiency, existing approaches face crucial limitations. Current quantization methods operate without considering hardware architecture, resulting in sub-optimal solutions within the vast design space encompassing accuracy, latency, and model size. Additionally, existing NeRF accelerators heavily rely on human experts to explore this design space, making the optimization process time-consuming, inefficient, and unlikely to discover optimal solutions. To address these challenges, we introduce HERO, a reinforcement learning framework performing hardware-aware quantization for NeRF. Our framework integrates a NeRF accelerator simulator to generate real-time hardware feedback, enabling fully automated adaptation to hardware constraints. Experimental results demonstrate that HERO achieves 1.31-1.33 $\times$ better latency, 1.29-1.33 $\times$ improved cost efficiency, and a more compact model size compared to CAQ, a previous state-of-the-art NeRF quantization framework. These results validate our framework's capability to effectively navigate the complex design space between hardware and algorithm requirements, discovering superior quantization policies for NeRF implementation. Code is available at https://github.com/ypzhng/HERO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09010v1</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yipu Zhang, Chaofang Ma, Jinming Ge, Lin Jiang, Jiang Xu, Wei Zhang</dc:creator>
    </item>
    <item>
      <title>Sequencing on Silicon: AI SoC Design for Mobile Genomics at the Edge</title>
      <link>https://arxiv.org/abs/2510.09339</link>
      <description>arXiv:2510.09339v1 Announce Type: new 
Abstract: Miniature DNA sequencing hardware has begun to succeed in mobile contexts, driving demand for efficient machine learning at the edge. This domain leverages deep learning techniques familiar from speech and time-series analysis for both low-level signal processing and high-level genomic interpretation. Unlike audio, however, nanopore sequencing presents raw data rates over 100X higher, requiring more aggressive compute and memory handling. In this paper, we present a CMOS system-on-chip (SoC) designed for mobile genetic analysis. Our approach combines a multi-core RISC-V processor with tightly coupled accelerators for deep learning and bioinformatics. A hardware/software co-design strategy enables energy-efficient operation across a heterogeneous compute fabric, targeting real-time, on-device genome analysis. This work exemplifies the integration of deep learning, edge computing, and domain-specific hardware to advance next-generation mobile genomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09339v1</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Magierowski, Zhongpan Wu, Abel Beyene, Karim Hammad</dc:creator>
    </item>
    <item>
      <title>LOTION: Smoothing the Optimization Landscape for Quantized Training</title>
      <link>https://arxiv.org/abs/2510.08757</link>
      <description>arXiv:2510.08757v1 Announce Type: cross 
Abstract: Optimizing neural networks for quantized objectives is fundamentally challenging because the quantizer is piece-wise constant, yielding zero gradients everywhere except at quantization thresholds where the derivative is undefined. Most existing methods deal with this issue by relaxing gradient computations with techniques like Straight Through Estimators (STE) and do not provide any guarantees of convergence. In this work, taking inspiration from Nesterov smoothing, we approximate the quantized loss surface with a continuous loss surface. In particular, we introduce LOTION, \textbf{L}ow-precision \textbf{O}ptimization via s\textbf{T}ochastic-no\textbf{I}se sm\textbf{O}othi\textbf{N}g, a principled smoothing framework that replaces the raw quantized loss with its expectation under unbiased randomized-rounding noise. In this framework, standard optimizers are guaranteed to converge to a local minimum of the loss surface. Moreover, when using noise derived from stochastic rounding, we show that the global minima of the original quantized loss are preserved. We empirically demonstrate that this method outperforms standard QAT on synthetic testbeds and on 150M- and 300M- parameter language models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08757v1</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mujin Kwun, Depen Morwani, Chloe Huangyuan Su, Stephanie Gil, Nikhil Anand, Sham Kakade</dc:creator>
    </item>
  </channel>
</rss>
