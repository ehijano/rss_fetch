<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 16 Oct 2025 05:30:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>High vs. Low AGI: Ontology and Conceptual Taxonomy for Geopolitical Coherence</title>
      <link>https://arxiv.org/abs/2510.12809</link>
      <description>arXiv:2510.12809v1 Announce Type: new 
Abstract: The rapid progression of Artificial General Intelligence (AGI) research demands conceptual tools capable of distinguishing between systems developed for open, commercial integration and those destined for sovereign, securitized deployments. Without such distinctions, risk assessments and regulatory debates collapse AGI into legacy dual-use frameworks that are ill-suited for these resources, capturing the possibility of civilian and military application but overlooking the distinct societal lineages yielded by corporate and state-grade architectures. This paper proposes a taxonomy distinguishing low-AGI and high-AGI, clarifying how commercial-economic and security-sovereign architectures can be distinguished not only by function, but by the social and political ecosystems that produce them. The taxonomy builds on international relations concepts of "high/low politics," viewed through the lens of construal-level theory, which allows it to even capture how cooperation and conflict may coexist in the context of AGI's emerging geopolitical stakes. By embedding AGI within power structures and securitization theory, this contribution extends dual-use discourse through an ontological taxonomy that enables more granular risk assessment and governance design--equipping policymakers and researchers to anticipate security dilemmas, institutional demands, and technical-political spillovers in the international system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12809v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonio Max</dc:creator>
    </item>
    <item>
      <title>Cyber Slavery Infrastructures: A Socio-Technical Study of Forced Criminality in Transnational Cybercrime</title>
      <link>https://arxiv.org/abs/2510.12814</link>
      <description>arXiv:2510.12814v1 Announce Type: new 
Abstract: The rise of ``cyber slavery," a technologically facilitated variant of forced criminality, signifies a concerning convergence of human trafficking and digital exploitation. In Southeast Asia, trafficked individuals are increasingly coerced into engaging in cybercrimes, including online fraud and financial phishing, frequently facilitated by international organized criminal networks. This study adopts a hybrid qualitative-computational methodology, combining a systematic narrative review with case-level metadata extracted from real-world cyber trafficking incidents through collaboration with Indian law enforcement agencies. We introduce a five-tier victimization framework that outlines the sequential state transitions of cyber-slavery victims, ranging from initial financial deception to physical exploitation, culminating in systemic prosecution through trace-based misattribution. Furthermore, our findings indicate that a significant socio-technical risk of cyber slavery is its capacity to evolve from forced to voluntary digital criminality, as victims, initially compelled to engage in cyber-enabled crimes, may choose to persist in their involvement due to financial incentives and the perceived security provided by digital anonymity. This legal-technological gap hampers victim identification processes, imposing excessive pressure on law enforcement systems dependent on binary legal categorizations, which ultimately hinders the implementation of victim-centered investigative methods and increases the likelihood of prosecutorial misclassification, thus reinforcing the structural obstacles to addressing cyber slavery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12814v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gargi Sarkar, Sandeep Kumar Shukla</dc:creator>
    </item>
    <item>
      <title>National Data Platform's Education Hub</title>
      <link>https://arxiv.org/abs/2510.12820</link>
      <description>arXiv:2510.12820v1 Announce Type: new 
Abstract: As demand for AI literacy and data science education grows, there is a critical need for infrastructure that bridges the gap between research data, computational resources, and educational experiences. To address this gap, we developed a first-of-its-kind Education Hub within the National Data Platform. This hub enables seamless connections between collaborative research workspaces, classroom environments, and data challenge settings. Early use cases demonstrate the effectiveness of the platform in supporting complex and resource-intensive educational activities. Ongoing efforts aim to enhance the user experience and expand adoption by educators and learners alike.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12820v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pedro Ramonetti, Melissa Floca, Kate O'Laughlin, Amarnath Gupta, Manish Parashar, Ilkay Altintas</dc:creator>
    </item>
    <item>
      <title>Evidence Without Injustice: A New Counterfactual Test for Fair Algorithms</title>
      <link>https://arxiv.org/abs/2510.12822</link>
      <description>arXiv:2510.12822v1 Announce Type: new 
Abstract: The growing philosophical literature on algorithmic fairness has examined statistical criteria such as equalized odds and calibration, causal and counterfactual approaches, and the role of structural and compounding injustices. Yet an important dimension has been overlooked: whether the evidential value of an algorithmic output itself depends on structural injustice. Our paradigmatic pair of examples contrasts a predictive policing algorithm, which relies on historical crime data, with a camera-based system that records ongoing offenses, both designed to guide police deployment. In evaluating the moral acceptability of acting on a piece of evidence, we must ask not only whether the evidence is probative in the actual world, but also whether it would remain probative in nearby worlds without the relevant injustices. The predictive policing algorithm fails this test, but the camera-based system passes it. When evidence fails the test, it is morally problematic to use it punitively, more so than evidence that passes the test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12822v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michele Loi, Marcello Di Bello, Nicol\`o Cangiotti</dc:creator>
    </item>
    <item>
      <title>Gobernanza y trazabilidad "a prueba de AI Act" para casos de uso legales: un marco t\'ecnico-jur\'idico, m\'etricas forenses y evidencias auditables</title>
      <link>https://arxiv.org/abs/2510.12830</link>
      <description>arXiv:2510.12830v1 Announce Type: new 
Abstract: This paper presents a comprehensive governance framework for AI systems in the legal sector, designed to ensure verifiable compliance with the EU AI Act. The framework integrates a normative mapping of the regulation to technical controls, a forensic architecture for RAG/LLM systems, and an evaluation system with metrics weighted by legal risk. As a primary contribution, we present rag-forense, an open-source implementation of the framework, accompanied by an experimental protocol to demonstrate compliance. -- Este art\'iculo presenta un marco integral de gobernanza para sistemas de IA en el sector legal, dise\~nado para garantizar el cumplimiento verificable del Reglamento de IA de la UE (AI Act). El marco integra una cartograf\'ia normativa de la ley a controles t\'ecnicos, una arquitectura forense para sistemas RAG/LLM y un sistema de evaluaci\'on con m\'etricas ponderadas por el riesgo jur\'idico. Como principal contribuci\'on, se presenta rag-forense, una implementaci\'on de c\'odigo abierto del marco, acompa\~nada de un protocolo experimental para demostrar la conformidad.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12830v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alex Dantart</dc:creator>
    </item>
    <item>
      <title>BanglaMATH : A Bangla benchmark dataset for testing LLM mathematical reasoning at grades 6, 7, and 8</title>
      <link>https://arxiv.org/abs/2510.12836</link>
      <description>arXiv:2510.12836v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have tremendous potential to play a key role in supporting mathematical reasoning, with growing use in education and AI research. However, most existing benchmarks are limited to English, creating a significant gap for low-resource languages. For example, Bangla is spoken by nearly 250 million people who would collectively benefit from LLMs capable of native fluency. To address this, we present BanglaMATH, a dataset of 1.7k Bangla math word problems across topics such as Arithmetic, Algebra, Geometry, and Logical Reasoning, sourced from Bangla elementary school workbooks and annotated with details like grade level and number of reasoning steps. We have designed BanglaMATH to evaluate the mathematical capabilities of both commercial and open-source LLMs in Bangla, and we find that Gemini 2.5 Flash and DeepSeek V3 are the only models to achieve strong performance, with $\ge$ 80\% accuracy across three elementary school grades. Furthermore, we assess the robustness and language bias of these top-performing LLMs by augmenting the original problems with distracting information, and translating the problems into English. We show that both LLMs fail to maintain robustness and exhibit significant performance bias in Bangla. Our study underlines current limitations of LLMs in handling arithmetic and mathematical reasoning in low-resource languages, and highlights the need for further research on multilingual and equitable mathematical understanding. Dataset link: \href{https://github.com/TabiaTanzin/BanglaMATH-A-Bangla-benchmark-dataset-for-testing-LLM-mathematical-reasoning-at-grades-6-7-and-8.git}{https://github.com/BanglaMATH}</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12836v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tabia Tanzin Prama, Christopher M. Danforth, Peter Sheridan Dodds</dc:creator>
    </item>
    <item>
      <title>Conceptualizing Smart City Applications: Requirements, Architecture, Security Issues and Emerging Trends</title>
      <link>https://arxiv.org/abs/2510.12841</link>
      <description>arXiv:2510.12841v1 Announce Type: new 
Abstract: The emergence of smart cities and sustainable development has become a globally accepted form of urbanization. The epitome of smart city development has become possible due to the latest innovative integration of information and communication technology. Citizens of smart cities can enjoy the benefits of a smart living environment, ubiquitous connectivity, seamless access to services, intelligent decision making through smart governance, and optimized resource management. The widespread acceptance of smart cities has raised data security issues, authentication, unauthorized access, device-level vulnerability, and sustainability. This paper focuses on the wholistic overview and conceptual development of smart city. Initially, the work discusses the smart city idea and fundamentals explored in various pieces of literature. Further various smart city applications, including notable implementations, are put forth to understand the quality of living standards. Finally, the paper depicts a solid understanding of different security and privacy issues, including some crucial future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12841v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1111/exsy.12753</arxiv:DOI>
      <arxiv:journal_reference>Haque, AKM Bahalul, Bharat Bhushan, and Gaurav Dhiman. "Conceptualizing smart city applications: Requirements, architecture, security issues, and emerging trends." Expert Systems 39.5 (2022): e12753</arxiv:journal_reference>
      <dc:creator>A. K. M. Bahalul Haque, Bharat Bhushan, Gaurav Dhiman</dc:creator>
    </item>
    <item>
      <title>AI Alignment vs. AI Ethical Treatment: 10 Challenges</title>
      <link>https://arxiv.org/abs/2510.12844</link>
      <description>arXiv:2510.12844v1 Announce Type: new 
Abstract: A morally acceptable course of AI development should avoid two dangers: creating unaligned AI systems that pose a threat to humanity and mistreating AI systems that merit moral consideration in their own right. This paper argues these two dangers interact and that if we create AI systems that merit moral consideration, simultaneously avoiding both of these dangers would be extremely challenging. While our argument is straightforward and supported by a wide range of pretheoretical moral judgments, it has far-reaching moral implications for AI development. Although the most obvious way to avoid the tension between alignment and ethical treatment would be to avoid creating AI systems that merit moral consideration, this option may be unrealistic and is perhaps fleeting. So, we conclude by offering some suggestions for other ways of mitigating mistreatment risks associated with alignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12844v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1111/phib.12380</arxiv:DOI>
      <dc:creator>Adam Bradley, Bradford Saad</dc:creator>
    </item>
    <item>
      <title>Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification</title>
      <link>https://arxiv.org/abs/2510.12850</link>
      <description>arXiv:2510.12850v1 Announce Type: new 
Abstract: Developing AI systems capable of nuanced ethical reasoning is critical as they increasingly influence human decisions, yet existing models often rely on superficial correlations rather than principled moral understanding. This paper introduces Ethic-BERT, a BERT-based model for ethical content classification across four domains: Commonsense, Justice, Virtue, and Deontology. Leveraging the ETHICS dataset, our approach integrates robust preprocessing to address vocabulary sparsity and contextual ambiguities, alongside advanced fine-tuning strategies like full model unfreezing, gradient accumulation, and adaptive learning rate scheduling. To evaluate robustness, we employ an adversarially filtered "Hard Test" split, isolating complex ethical dilemmas. Experimental results demonstrate Ethic-BERT's superiority over baseline models, achieving 82.32% average accuracy on the standard test, with notable improvements in Justice and Virtue. In addition, the proposed Ethic-BERT attains 15.28% average accuracy improvement in the HardTest. These findings contribute to performance improvement and reliable decision-making using bias-aware preprocessing and proposed enhanced AI model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12850v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahamodul Hasan Mahadi, Md. Nasif Safwan, Souhardo Rahman, Shahnaj Parvin, Aminun Nahar, Kamruddin Nur</dc:creator>
    </item>
    <item>
      <title>Adaptive Generation of Bias-Eliciting Questions for LLMs</title>
      <link>https://arxiv.org/abs/2510.12857</link>
      <description>arXiv:2510.12857v1 Announce Type: new 
Abstract: Large language models (LLMs) are now widely deployed in user-facing applications, reaching hundreds of millions worldwide. As they become integrated into everyday tasks, growing reliance on their outputs raises significant concerns. In particular, users may unknowingly be exposed to model-inherent biases that systematically disadvantage or stereotype certain groups. However, existing bias benchmarks continue to rely on templated prompts or restrictive multiple-choice questions that are suggestive, simplistic, and fail to capture the complexity of real-world user interactions. In this work, we address this gap by introducing a counterfactual bias evaluation framework that automatically generates realistic, open-ended questions over sensitive attributes such as sex, race, or religion. By iteratively mutating and selecting bias-inducing questions, our approach systematically explores areas where models are most susceptible to biased behavior. Beyond detecting harmful biases, we also capture distinct response dimensions that are increasingly relevant in user interactions, such as asymmetric refusals and explicit acknowledgment of bias. Leveraging our framework, we construct CAB, a human-verified benchmark spanning diverse topics, designed to enable cross-model comparisons. Using CAB, we analyze a range of LLMs across multiple bias dimensions, revealing nuanced insights into how different models manifest bias. For instance, while GPT-5 outperforms other models, it nonetheless exhibits persistent biases in specific scenarios. These findings underscore the need for continual improvements to ensure fair model behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12857v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robin Staab, Jasper Dekoninck, Maximilian Baader, Martin Vechev</dc:creator>
    </item>
    <item>
      <title>Three Lenses on the AI Revolution: Risk, Transformation, Continuity</title>
      <link>https://arxiv.org/abs/2510.12859</link>
      <description>arXiv:2510.12859v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) has emerged as both a continuation of historical technological revolutions and a potential rupture with them. This paper argues that AI must be viewed simultaneously through three lenses: \textit{risk}, where it resembles nuclear technology in its irreversible and global externalities; \textit{transformation}, where it parallels the Industrial Revolution as a general-purpose technology driving productivity and reorganization of labor; and \textit{continuity}, where it extends the fifty-year arc of computing revolutions from personal computing to the internet to mobile. Drawing on historical analogies, we emphasize that no past transition constituted a strict singularity: disruptive shifts eventually became governable through new norms and institutions.
  We examine recurring patterns across revolutions -- democratization at the usage layer, concentration at the production layer, falling costs, and deepening personalization -- and show how these dynamics are intensifying in the AI era. Sectoral analysis illustrates how accounting, law, education, translation, advertising, and software engineering are being reshaped as routine cognition is commoditized and human value shifts to judgment, trust, and ethical responsibility. At the frontier, the challenge of designing moral AI agents highlights the need for robust guardrails, mechanisms for moral generalization, and governance of emergent multi-agent dynamics.
  We conclude that AI is neither a singular break nor merely incremental progress. It is both evolutionary and revolutionary: predictable in its median effects yet carrying singularity-class tail risks. Good outcomes are not automatic; they require coupling pro-innovation strategies with safety governance, ensuring equitable access, and embedding AI within a human order of responsibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12859v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masoud Makrehchi</dc:creator>
    </item>
    <item>
      <title>Toward LLM-Supported Automated Assessment of Critical Thinking Subskills</title>
      <link>https://arxiv.org/abs/2510.12915</link>
      <description>arXiv:2510.12915v1 Announce Type: new 
Abstract: Critical thinking represents a fundamental competency in today's education landscape. Developing critical thinking skills through timely assessment and feedback is crucial; however, there has not been extensive work in the learning analytics community on defining, measuring, and supporting critical thinking. In this paper, we investigate the feasibility of measuring core "subskills" that underlie critical thinking. We ground our work in an authentic task where students operationalize critical thinking: student-written argumentative essays. We developed a coding rubric based on an established skills progression and completed human coding for a corpus of student essays. We then evaluated three distinct approaches to automated scoring: zero-shot prompting, few-shot prompting, and supervised fine-tuning, implemented across three large language models (GPT-5, GPT-5-mini, and ModernBERT). GPT-5 with few-shot prompting achieved the strongest results and demonstrated particular strength on subskills with separable, frequent categories, while lower performance was observed for subskills that required detection of subtle distinctions or rare categories. Our results underscore critical trade-offs in automated critical thinking assessment: proprietary models offer superior reliability at higher cost, while open-source alternatives provide practical accuracy with reduced sensitivity to minority categories. Our work represents an initial step toward scalable assessment of higher-order reasoning skills across authentic educational contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12915v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marisa C. Peczuh, Nischal Ashok Kumar, Ryan Baker, Blair Lehman, Danielle Eisenberg, Caitlin Mills, Keerthi Chebrolu, Sudhip Nashi, Cadence Young, Brayden Liu, Sherry Lachman, Andrew Lan</dc:creator>
    </item>
    <item>
      <title>Addressing the alignment problem in transportation policy making: an LLM approach</title>
      <link>https://arxiv.org/abs/2510.13139</link>
      <description>arXiv:2510.13139v1 Announce Type: new 
Abstract: A key challenge in transportation planning is that the collective preferences of heterogeneous travelers often diverge from the policies produced by model-driven decision tools. This misalignment frequently results in implementation delays or failures. Here, we investigate whether large language models (LLMs), noted for their capabilities in reasoning and simulating human decision-making, can help inform and address this alignment problem. We develop a multi-agent simulation in which LLMs, acting as agents representing residents from different communities in a city, participate in a referendum on a set of transit policy proposals. Using chain-of-thought reasoning, LLM agents provide ranked-choice or approval-based preferences, which are aggregated using instant-runoff voting (IRV) to model democratic consensus. We implement this simulation framework with both GPT-4o and Claude-3.5, and apply it for Chicago and Houston. Our findings suggest that LLM agents are capable of approximating plausible collective preferences and responding to local context, while also displaying model-specific behavioral biases and modest divergences from optimization-based benchmarks. These capabilities underscore both the promise and limitations of LLMs as tools for solving the alignment problem in transportation decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13139v1</guid>
      <category>cs.CY</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <category>cs.MA</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyu Yan (Marco), Tianxing Dai (Marco),  Yu (Marco),  Nie</dc:creator>
    </item>
    <item>
      <title>Searching for a Farang: Collective Security among Women in Pattaya, Thailand</title>
      <link>https://arxiv.org/abs/2510.13162</link>
      <description>arXiv:2510.13162v1 Announce Type: new 
Abstract: We report on two months of ethnographic fieldwork in a women's centre in Pattaya, and interviews with 76 participants. Our findings, as they relate to digital security, show how (i) women in Pattaya, often working in the sex and massage industries, perceived relationships with farang men as their best, and sometimes only, option to achieve security; (ii) the strategies used by the women to appeal to a farang involved presenting themselves online, mirroring how they were being advertised by bar owners to attract customers; (iii) appealing to what they considered `Western ideals', the women sought out `Western technologies' and appropriated them for their benefit; (iv) the women navigated a series of online security risks, such as scams and abuse, which shaped their search for a farang; (v) the women developed collective security through knowledge-sharing to protect themselves and each other in their search for a farang. We situate our work in emerging digital security scholarship within marginalised contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13162v1</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Taylor Robinson, Rikke Bjerg Jensen</dc:creator>
    </item>
    <item>
      <title>Discrimination, artificial intelligence, and algorithmic decision-making</title>
      <link>https://arxiv.org/abs/2510.13465</link>
      <description>arXiv:2510.13465v1 Announce Type: new 
Abstract: Artificial intelligence (AI) has a huge impact on our personal lives and also on our democratic society as a whole. While AI offers vast opportunities for the benefit of people, its potential to embed and perpetuate bias and discrimination remains one of the most pressing challenges deriving from its increasing use. This new study, which was prepared by Prof. Frederik Zuiderveen Borgesius for the Anti-discrimination Department of the Council of Europe, elaborates on the risks of discrimination caused by algorithmic decision-making and other types of artificial intelligence (AI).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13465v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Frederik Zuiderveen Borgesius</dc:creator>
    </item>
    <item>
      <title>The Perfect Match? A Closer Look at the Relationship between EU Consumer Law and Data Protection Law</title>
      <link>https://arxiv.org/abs/2510.13466</link>
      <description>arXiv:2510.13466v1 Announce Type: new 
Abstract: In modern markets, many companies offer so-called 'free' services and monetize consumer data they collect through those services. This paper argues that consumer law and data protection law can usefully complement each other. Data protection law can also inform the interpretation of consumer law. Using consumer rights, consumers should be able to challenge excessive collection of their personal data. Consumer organizations have used consumer law to tackle data protection infringements. The interplay of data protection law and consumer protection law provides exciting opportunities for a more integrated vision on 'data consumer law'.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13466v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.54648/cola2017118</arxiv:DOI>
      <arxiv:journal_reference>Common Market Law Review, Volume 54 (2017), Issue 5</arxiv:journal_reference>
      <dc:creator>Natali Helberger, Frederik Zuiderveen Borgesius, Agustin Reyna</dc:creator>
    </item>
    <item>
      <title>Privacy, freedom of expression, and the right to be forgotten in Europe</title>
      <link>https://arxiv.org/abs/2510.13468</link>
      <description>arXiv:2510.13468v1 Announce Type: new 
Abstract: In this chapter we discuss the relation between privacy and freedom of expression in Europe. In principle, the two rights have equal weight in Europe - which right prevails depends on the circumstances of a case. We use the Google Spain judgment of the Court of Justice of the European Union, sometimes called the 'right to be forgotten' judgment, to illustrate the difficulties when balancing the two rights. The court decided in Google Spain that people have, under certain conditions, the right to have search results for their name delisted. We discuss how Google and Data Protection Authorities deal with such delisting requests in practice. Delisting requests illustrate that balancing privacy and freedom of expression interests will always remain difficult.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13468v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1017/9781316831960.018</arxiv:DOI>
      <arxiv:journal_reference>In: J. Polonetsky, O. Tene, and E. Selinger, Cambridge Handbook of Consumer Privacy, pp. 301 - 320 (Cambridge University Press 2018)</arxiv:journal_reference>
      <dc:creator>Stefan Kulk, Frederik Zuiderveen Borgesius</dc:creator>
    </item>
    <item>
      <title>Subject Roles in the EU AI Act: Mapping and Regulatory Implications</title>
      <link>https://arxiv.org/abs/2510.13591</link>
      <description>arXiv:2510.13591v1 Announce Type: new 
Abstract: The European Union's Artificial Intelligence Act (Regulation (EU) 2024/1689) establishes the world's first comprehensive regulatory framework for AI systems through a sophisticated ecosystem of interconnected subjects defined in Article 3. This paper provides a structured examination of the six main categories of actors - providers, deployers, authorized representatives, importers, distributors, and product manufacturers - collectively referred to as "operators" within the regulation. Through examination of these Article 3 definitions and their elaboration across the regulation's 113 articles, 180 recitals, and 13 annexes, we map the complete governance structure and analyze how the AI Act regulates these subjects. Our analysis reveals critical transformation mechanisms whereby subjects can assume different roles under specific conditions, particularly through Article 25 provisions ensuring accountability follows control. We identify how obligations cascade through the supply chain via mandatory information flows and cooperation requirements, creating a distributed yet coordinated governance system. The findings demonstrate how the regulation balances innovation with the protection of fundamental rights through risk-based obligations that scale with the capabilities and deployment contexts of AI systems, providing essential guidance for stakeholders implementing the AI Act's requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13591v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nicola Fabiano</dc:creator>
    </item>
    <item>
      <title>The Role of Computing Resources in Publishing Foundation Model Research</title>
      <link>https://arxiv.org/abs/2510.13621</link>
      <description>arXiv:2510.13621v1 Announce Type: new 
Abstract: Cutting-edge research in Artificial Intelligence (AI) requires considerable resources, including Graphics Processing Units (GPUs), data, and human resources. In this paper, we evaluate of the relationship between these resources and the scientific advancement of foundation models (FM). We reviewed 6517 FM papers published between 2022 to 2024, and surveyed 229 first-authors to the impact of computing resources on scientific output. We find that increased computing is correlated with national funding allocations and citations, but our findings don't observe the strong correlations with research environment (academic or industrial), domain, or study methodology. We advise that individuals and institutions focus on creating shared and affordable computing opportunities to lower the entry barrier for under-resourced researchers. These steps can help expand participation in FM research, foster diversity of ideas and contributors, and sustain innovation and progress in AI. The data will be available at: https://mit-calc.csail.mit.edu/</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13621v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuexing Hao, Yue Huang, Haoran Zhang, Chenyang Zhao, Zhenwen Liang, Paul Pu Liang, Yue Zhao, Lichao Sun, Saleh Kalantari, Xiangliang Zhang, Marzyeh Ghassemi</dc:creator>
    </item>
    <item>
      <title>International AI Safety Report 2025: First Key Update: Capabilities and Risk Implications</title>
      <link>https://arxiv.org/abs/2510.13653</link>
      <description>arXiv:2510.13653v1 Announce Type: new 
Abstract: Since the publication of the first International AI Safety Report, AI capabilities have continued to improve across key domains. New training techniques that teach AI systems to reason step-by-step and inference-time enhancements have primarily driven these advances, rather than simply training larger models. As a result, general-purpose AI systems can solve more complex problems in a range of domains, from scientific research to software development. Their performance on benchmarks that measure performance in coding, mathematics, and answering expert-level science questions has continued to improve, though reliability challenges persist, with systems excelling on some tasks while failing completely on others. These capability improvements also have implications for multiple risks, including risks from biological weapons and cyber attacks. Finally, they pose new challenges for monitoring and controllability. This update examines how AI capabilities have improved since the first Report, then focuses on key risk areas where substantial new evidence warrants updated assessments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13653v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yoshua Bengio, Stephen Clare, Carina Prunkl, Shalaleh Rismani, Maksym Andriushchenko, Ben Bucknall, Philip Fox, Tiancheng Hu, Cameron Jones, Sam Manning, Nestor Maslej, Vasilios Mavroudis, Conor McGlynn, Malcolm Murray, Charlotte Stix, Lucia Velasco, Nicole Wheeler, Daniel Privitera, S\"oren Mindermann, Daron Acemoglu, Thomas G. Dietterich, Fredrik Heintz, Geoffrey Hinton, Nick Jennings, Susan Leavy, Teresa Ludermir, Vidushi Marda, Helen Margetts, John McDermid, Jane Munga, Arvind Narayanan, Alondra Nelson, Clara Neppel, Gopal Ramchurn, Stuart Russell, Marietje Schaake, Bernhard Sch\"olkopf, Alavaro Soto, Lee Tiedrich, Ga\"el Varoquaux, Andrew Yao, Ya-Qin Zhang, Leandro Aguirre, Olubunmi Ajala, Fahad Albalawi Noora AlMalek, Christian Busch, Andr\'e Carvalho, Jonathan Collas, Amandeep Gill, Ahmet Hatip, Juha Heikkil\"a, Chris Johnson, Gill Jolly, Ziv Katzir, Mary Kerema, Hiroaki Kitano, Antonio Kr\"uger, Aoife McLysaght, Oleksii Molchanovskyi, Andrea Monti, Kyoung Mu Lee, Mona Nemer, Nuria Oliver, Raquel Pezoa, Audrey Plonk, Jos\'e Portillo, Balaraman Ravindran, Hammam Riza, Crystal Rugege, Haroon Sheikh, Denise Wong, Yi Zeng, Liming Zhu</dc:creator>
    </item>
    <item>
      <title>We Can Hide More Bits: The Unused Watermarking Capacity in Theory and in Practice</title>
      <link>https://arxiv.org/abs/2510.12812</link>
      <description>arXiv:2510.12812v1 Announce Type: cross 
Abstract: Despite rapid progress in deep learning-based image watermarking, the capacity of current robust methods remains limited to the scale of only a few hundred bits. Such plateauing progress raises the question: How far are we from the fundamental limits of image watermarking? To this end, we present an analysis that establishes upper bounds on the message-carrying capacity of images under PSNR and linear robustness constraints. Our results indicate theoretical capacities are orders of magnitude larger than what current models achieve. Our experiments show this gap between theoretical and empirical performance persists, even in minimal, easily analysable setups. This suggests a fundamental problem. As proof that larger capacities are indeed possible, we train ChunkySeal, a scaled-up version of VideoSeal, which increases capacity 4 times to 1024 bits, all while preserving image quality and robustness. These findings demonstrate modern methods have not yet saturated watermarking capacity, and that significant opportunities for architectural innovation and training strategies remain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12812v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aleksandar Petrov, Pierre Fernandez, Tom\'a\v{s} Sou\v{c}ek, Hady Elsahar</dc:creator>
    </item>
    <item>
      <title>From Noise to Signal to Selbstzweck: Reframing Human Label Variation in the Era of Post-training in NLP</title>
      <link>https://arxiv.org/abs/2510.12817</link>
      <description>arXiv:2510.12817v1 Announce Type: cross 
Abstract: Human Label Variation (HLV) refers to legitimate disagreement in annotation that reflects the genuine diversity of human perspectives rather than mere error. For decades, HLV in NLP was dismissed as noise to be discarded, and only slowly over the last decade has it been reframed as a signal for improving model robustness. With the rise of large language models (LLMs), where post-training on human feedback has become central to model alignment, the role of HLV has become increasingly consequential. Yet current preference-learning datasets routinely aggregate multiple annotations into a single label, thereby flattening diverse perspectives into a false universal agreement and erasing precisely the pluralism of human values that alignment aims to preserve. In this position paper, we argue that preserving HLV as an embodiment of human pluralism must be treated as a Selbstzweck - a goal it self when designing AI systems. We call for proactively incorporating HLV into preference datasets and outline actionable steps towards it.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12817v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shanshan Xu, Santosh T. Y. S. S, Barbara Plank</dc:creator>
    </item>
    <item>
      <title>MEDEQUALQA: Evaluating Biases in LLMs with Counterfactual Reasoning</title>
      <link>https://arxiv.org/abs/2510.12818</link>
      <description>arXiv:2510.12818v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly deployed in clinical decision support, yet subtle demographic cues can influence their reasoning. Prior work has documented disparities in outputs across patient groups, but little is known about how internal reasoning shifts under controlled demographic changes. We introduce MEDEQUALQA, a counterfactual benchmark that perturbs only patient pronouns (he/him, she/her, they/them) while holding critical symptoms and conditions (CSCs) constant. Each clinical vignette is expanded into single-CSC ablations, producing three parallel datasets of approximately 23,000 items each (69,000 total). We evaluate a GPT-4.1 model and compute Semantic Textual Similarity (STS) between reasoning traces to measure stability across pronoun variants. Our results show overall high similarity (mean STS &gt;0.80), but reveal consistent localized divergences in cited risk factors, guideline anchors, and differential ordering, even when final diagnoses remain unchanged. Our error analysis highlights certain cases in which the reasoning shifts, underscoring clinically relevant bias loci that may cascade into inequitable care. MEDEQUALQA offers a controlled diagnostic setting for auditing reasoning stability in medical AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12818v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajarshi Ghosh, Abhay Gupta, Hudson McBride, Anurag Vaidya, Faisal Mahmood</dc:creator>
    </item>
    <item>
      <title>Semantic knowledge guides innovation and drives cultural evolution</title>
      <link>https://arxiv.org/abs/2510.12837</link>
      <description>arXiv:2510.12837v1 Announce Type: cross 
Abstract: Cumulative cultural evolution enables human societies to generate increasingly complex knowledge and technology over generations. While social learning transmits innovations between individuals and generations, the cognitive processes that generate these innovations remain poorly understood. Here, we demonstrate that semantic knowledge-structured associations between concepts and their functions-provides cognitive scaffolding for cumulative innovation by guiding exploration toward plausible and meaningful actions. We tested this hypothesis using a cultural evolutionary agent-based model and a large-scale behavioural experiment (N = 1,243), in which individuals performed a task requiring the combination of items into novel innovations. Across both approaches, semantic knowledge and social learning interact synergistically to enhance innovation. Behaviorally, participants without access to semantic knowledge performed no better than chance, even when social learning was available, and relied on shallow exploration strategies. These findings suggest that semantic knowledge is a key cognitive process enabling human cumulative culture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12837v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.NE</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anil Yaman, Shen Tian, Bj\"orn Lindstr\"om</dc:creator>
    </item>
    <item>
      <title>FaStFACT: Faster, Stronger Long-Form Factuality Evaluations in LLMs</title>
      <link>https://arxiv.org/abs/2510.12839</link>
      <description>arXiv:2510.12839v1 Announce Type: cross 
Abstract: Evaluating the factuality of long-form generations from Large Language Models (LLMs) remains challenging due to accuracy issues and costly human assessment. Prior efforts attempt this by decomposing text into claims, searching for evidence, and verifying claims, but suffer from critical drawbacks: (1) inefficiency due to complex pipeline components unsuitable for long LLM outputs, and (2) ineffectiveness stemming from inaccurate claim sets and insufficient evidence collection of one-line snippets.
  To address these limitations, we propose \name, a fast and strong evaluation framework that achieves the highest alignment with human evaluation and efficiency among existing baselines. \name first employs chunk-level claim extraction integrated with confidence-based pre-verification, significantly reducing the cost of web searching and inference calling while ensuring reliability. For searching and verification, it collects document-level evidence from crawled webpages and selectively retrieves it during verification, addressing the evidence insufficiency problem in previous pipelines.
  Extensive experiments based on an aggregated and manually annotated benchmark demonstrate the reliability of \name in both efficiently and effectively evaluating the factuality of long-form LLM generations. Code and benchmark data is available at https://github.com/Yingjia-Wan/FastFact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12839v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yingjia Wan, Haochen Tan, Xiao Zhu, Xinyu Zhou, Zhiwei Li, Qingsong Lv, Changxuan Sun, Jiaqi Zeng, Yi Xu, Jianqiao Lu, Yinhong Liu, Zhijiang Guo</dc:creator>
    </item>
    <item>
      <title>From misinformation to climate crisis: Navigating vulnerabilities in the cyber-physical-social systems</title>
      <link>https://arxiv.org/abs/2510.13058</link>
      <description>arXiv:2510.13058v1 Announce Type: cross 
Abstract: Within the cyber-physical-social-climate nexus, all systems are deeply interdependent: cyber infrastructure facilitates communication, data processing, and automation across physical systems (such as power grids and networks), while social infrastructure provides the human capital and societal norms necessary for the system's functionality. Any disruption within any of these components, whether due to human error or system mismanagement, can propagate throughout the network, amplifying vulnerabilities and creating a significantly scaled impact. This chapter explores the critical role of human vulnerabilities within the cyber-physical-social-climate nexus, focusing on the interdependencies across cyber, physical, and social systems and how these interdependencies can scale in a climate context. While cyber and physical vulnerabilities are readily apparent, social vulnerabilities (such as misinformation, resistance to policy change, and lack of public awareness) often go unaddressed despite their profound impact on resilience and climate adaptation. Social infrastructure, including human capital, societal norms, and policy frameworks, shapes community responses and underpins adaptive capacity, yet it is also a significant point of failure when overlooked. This chapter examines how human cognitive biases, risk misperception, and decision-making silos within interconnected systems can lead to resource misallocation and weakened policy effectiveness. These factors are analyzed to demonstrate how inadequate responses across cyber-physical-social layers can cascade, amplifying climate-related risks. By addressing these human factors and aligning decision-making frameworks, we aim to strengthen resilience and foster cohesive adaptation strategies that account for the intricate interrelations of cyber-physical-social-climate systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13058v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tooba Aamir, Marthie Grobler, Giovanni Russello</dc:creator>
    </item>
    <item>
      <title>A theory-based AI automation exposure index: Applying Moravec's Paradox to the US labor market</title>
      <link>https://arxiv.org/abs/2510.13369</link>
      <description>arXiv:2510.13369v1 Announce Type: cross 
Abstract: This paper develops a theory-driven automation exposure index based on Moravec's Paradox. Scoring 19,000 O*NET tasks on performance variance, tacit knowledge, data abundance, and algorithmic gaps reveals that management, STEM, and sciences occupations show the highest exposure. In contrast, maintenance, agriculture, and construction show the lowest. The positive relationship between wages and exposure challenges the notion of skill-biased technological change if AI substitutes for workers. At the same time, tacit knowledge exhibits a positive relationship with wages consistent with seniority-biased technological change. This index identifies fundamental automatability rather than current capabilities, while also validating the AI annotation method pioneered by Eloundou et al. (2024) with a correlation of 0.72. The non-positive relationship with pre-LLM indices suggests a paradigm shift in automation patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13369v1</guid>
      <category>econ.GN</category>
      <category>cs.CY</category>
      <category>q-fin.EC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacob Schaal</dc:creator>
    </item>
    <item>
      <title>Deciphering the Crypto-shopper: Knowledge and Preferences of Consumers Using Cryptocurrencies for Purchases</title>
      <link>https://arxiv.org/abs/2310.02911</link>
      <description>arXiv:2310.02911v5 Announce Type: replace 
Abstract: The fast-growing cryptocurrency sector presents both challenges and opportunities for businesses and consumers alike. This study investigates the knowledge, expertise, and buying habits of people who shop using cryptocurrencies. Our survey of 516 participants shows that knowledge levels vary from beginners to experts. Interestingly, a segment of respondents, nearly 30%, showed high purchase frequency despite their limited knowledge. Regression analyses indicated that while domain knowledge plays a role, it only accounts for 11.6% of the factors affecting purchasing frequency. A K-means cluster analysis further segmented the respondents into three distinct groups, each having unique knowledge levels and purchasing tendencies. These results challenge the conventional idea linking extensive knowledge to increased cryptocurrency usage, suggesting other factors at play. Understanding this varying crypto-shopper demographic is pivotal for businesses, emphasizing the need for tailored strategies and user-friendly experiences. This study offers insights into current crypto-shopping behaviors and discusses future research exploring the broader impacts and potential shifts in the crypto-consumer landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.02911v5</guid>
      <category>cs.CY</category>
      <category>cs.CE</category>
      <category>cs.ET</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.5937/sjm20-56002</arxiv:DOI>
      <arxiv:journal_reference>Serbian Journal of Management 20 (2) (2025) 429 - 453</arxiv:journal_reference>
      <dc:creator>Massimiliano Silenzi, Umut Can Cabuk, Enis Karaarslan, Omer Aydin</dc:creator>
    </item>
    <item>
      <title>Reviewing Uses of Regulatory Compliance Monitoring</title>
      <link>https://arxiv.org/abs/2501.10362</link>
      <description>arXiv:2501.10362v5 Announce Type: replace 
Abstract: Organizations need to manage numerous business processes for delivering their services and products to customers. One important consideration thereby lies in the adherence to regulations such as laws, guidelines, or industry standards. In order to monitor adherence of their business processes to regulations -- in other words, their regulatory compliance -- organizations make use of various techniques that draw on process execution data of IT systems that support these processes. Previous research has investigated conformance checking, an operation of process mining, for the domains in which it is applied, its operationalization of regulations, the techniques being used, and the presentation of results produced. However, other techniques for regulatory compliance monitoring, which we summarize as compliance checking techniques, have not yet been investigated regarding these aspects in a structural manner. To this end, this work presents a systematic literature review on uses of regulatory compliance monitoring of business processes, thereby offering insights into the various techniques being used, their application and the results they generate. We highlight commonalities and differences between the approaches and find that various steps are performed manually; we also provide further impulses for research on compliance monitoring and its use in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10362v5</guid>
      <category>cs.CY</category>
      <category>cs.DB</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Finn Klessascheck, Luise Pufahl</dc:creator>
    </item>
    <item>
      <title>Discretion in the Loop: Human Expertise in Algorithm-Assisted College Advising</title>
      <link>https://arxiv.org/abs/2505.13325</link>
      <description>arXiv:2505.13325v3 Announce Type: replace 
Abstract: In higher education, many institutions use algorithmic alerts to flag at-risk students and deliver advising at scale. While much research has focused on evaluating algorithmic predictions, relatively little is known about how discretionary interventions by human experts shape outcomes in algorithm-assisted settings. We study this question using rich quantitative and qualitative data from a randomized controlled trial of an algorithm-assisted advising program at Georgia State University. Taking a mixed-methods approach, we examine whether and how advisors use context unavailable to an algorithm to guide interventions and influence student success. We develop a causal graphical framework for human expertise in the interventional setting, extending prior work on discretion in purely predictive settings. We then test a necessary condition for discretionary expertise using structured advisor logs and student outcomes data, identifying several interventions that meet the criterion for statistical significance. Accordingly, we estimate that 2 out of 3 interventions taken by advisors in the treatment arm were plausibly ``expertly targeted'' to students using non-algorithmic context. Systematic qualitative analysis of advisor notes corroborates these findings, showing a pattern of advisors incorporating diverse forms of contextual information--such as personal circumstances, financial issues, and student engagement--into their decisions. Finally, we document heterogeneity in advising styles, finding that one style elicits more holistic information about students and is associated with improved graduation rates. Our results offer theoretical and practical insight into the real-world effectiveness of algorithm-supported college advising, and underscore the importance of accounting for human expertise in the design, evaluation, and implementation of algorithmic decision systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13325v3</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kara Schechtman, Benjamin Brandon, Jenise Stafford, Hannah Li, Lydia T. Liu</dc:creator>
    </item>
    <item>
      <title>Effects of Antivaccine Tweets on COVID-19 Vaccinations, Cases, and Deaths</title>
      <link>https://arxiv.org/abs/2406.09142</link>
      <description>arXiv:2406.09142v3 Announce Type: replace-cross 
Abstract: Despite the wide availability of COVID-19 vaccines in the United States and their effectiveness in reducing hospitalizations and mortality during the pandemic, a majority of Americans chose not to be vaccinated during 2021. Recent work shows that vaccine misinformation affects intentions in controlled settings, but does not link it to real-world vaccination rates. Here, we present observational evidence of a causal relationship between exposure to antivaccine content and vaccination rates, and estimate the size of this effect. We present a compartmental epidemic model that includes vaccination, vaccine hesitancy, and exposure to antivaccine content. We fit the model to data to determine that a geographical pattern of exposure to online antivaccine content across US counties explains reduced vaccine uptake in the same counties. We find observational evidence that exposure to antivaccine content on Twitter caused about 14,000 people to refuse vaccination between February and August 2021 in the US, resulting in at least 545 additional cases and 8 additional deaths. This work provides a methodology for linking online speech with offline epidemic outcomes. Our findings should inform social media moderation policy as well as public health interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09142v3</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John Bollenbacher, Filippo Menczer, John Bryden</dc:creator>
    </item>
    <item>
      <title>Metrics for Assessing Inclusivity and Empowerment of People for Supporting the Design of Inclusive Product Lifecycles</title>
      <link>https://arxiv.org/abs/2410.17287</link>
      <description>arXiv:2410.17287v2 Announce Type: replace-cross 
Abstract: The design of an inclusive product lifecycle is important for empowering stakeholders through their meaningful inclusion in lifecycle processes. To achieve this, the inclusion of stakeholders must be structured in a way that supports their empowerment. Inclusivity addresses the lifecycle context to improve how diverse stakeholders are included across phases, supporting their empowerment. This study aims to build on current inclusive design approaches, which often focus on empowering users through the use of products. It proposes inclusive lifecycle processes as a way to empower a wider range of stakeholders for sustainable development. We apply a novel framework to real-life case studies from the literature to identify the dimensions of empowerment and inclusivity. By analysing the relationships between these dimensions, we propose specific metrics that show strong causal connections. These metrics allow measurable outcomes to support the fair development of stakeholders. Measuring inclusivity and empowerment can serve as a foundation for supporting sustainability, dignity, well-being, and fair stakeholder development, which are explored further in Part 2 and Part 3 of this study. This study contributes to design science by expanding the understanding of inclusive design through stakeholder inclusion in lifecycle phases and by shifting the focus from product to process design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17287v2</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naz Yaldiz, Amaresh Chakrabarti</dc:creator>
    </item>
    <item>
      <title>Position: The Artificial Intelligence and Machine Learning Community Should Adopt a More Transparent and Regulated Peer Review Process</title>
      <link>https://arxiv.org/abs/2502.00874</link>
      <description>arXiv:2502.00874v3 Announce Type: replace-cross 
Abstract: The rapid growth of submissions to top-tier Artificial Intelligence (AI) and Machine Learning (ML) conferences has prompted many venues to transition from closed to open review platforms. Some have fully embraced open peer reviews, allowing public visibility throughout the process, while others adopt hybrid approaches, such as releasing reviews only after final decisions or keeping reviews private despite using open peer review systems. In this work, we analyze the strengths and limitations of these models, highlighting the growing community interest in transparent peer review. To support this discussion, we examine insights from Paper Copilot, a website launched two years ago to aggregate and analyze AI / ML conference data while engaging a global audience. The site has attracted over 200,000 early-career researchers, particularly those aged 18-34 from 177 countries, many of whom are actively engaged in the peer review process. Drawing on our findings, this position paper advocates for a more transparent, open, and well-regulated peer review aiming to foster greater community involvement and propel advancements in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00874v3</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jing Yang</dc:creator>
    </item>
    <item>
      <title>Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs</title>
      <link>https://arxiv.org/abs/2510.03567</link>
      <description>arXiv:2510.03567v2 Announce Type: replace-cross 
Abstract: With the increasing adoption of Large Language Models (LLMs), more customization is needed to ensure privacy-preserving and safe generation. We address this objective from two critical aspects: unlearning of sensitive information and robustness to jail-breaking attacks. We investigate various constrained optimization formulations that address both aspects in a \emph{unified manner}, by finding the smallest possible interventions on LLM weights that either make a given vocabulary set unreachable or embed the LLM with robustness to tailored attacks by shifting part of the weights to a \emph{safer} region. Beyond unifying two key properties, this approach contrasts with previous work in that it doesn't require an oracle classifier that is typically not available or represents a computational overhead. Surprisingly, we find that the simplest point-wise constraint-based intervention we propose leads to better performance than max-min interventions, while having a lower computational cost. Comparison against state-of-the-art defense methods demonstrates superior performance of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03567v2</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fatmazohra Rezkellah, Ramzi Dakhmouche</dc:creator>
    </item>
  </channel>
</rss>
