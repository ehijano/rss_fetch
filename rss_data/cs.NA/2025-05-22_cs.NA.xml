<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 May 2025 04:02:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Improving the Predictability of the Madden-Julian Oscillation at Subseasonal Scales with Gaussian Process Models</title>
      <link>https://arxiv.org/abs/2505.15934</link>
      <description>arXiv:2505.15934v1 Announce Type: new 
Abstract: The Madden--Julian Oscillation (MJO) is an influential climate phenomenon that plays a vital role in modulating global weather patterns. In spite of the improvement in MJO predictions made by machine learning algorithms, such as neural networks, most of them cannot provide the uncertainty levels in the MJO forecasts directly. To address this problem, we develop a nonparametric strategy based on Gaussian process (GP) models. We calibrate GPs using empirical correlations and we propose a posteriori covariance correction. Numerical experiments demonstrate that our model has better prediction skills than the ANN models for the first five lead days. Additionally, our posteriori covariance correction extends the probabilistic coverage by more than three weeks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15934v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.ao-ph</category>
      <category>stat.ML</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoyuan Chen, Emil Constantinescu, Vishwas Rao, Cristiana Stan</dc:creator>
    </item>
    <item>
      <title>Fast-wave slow-wave spectral deferred correction methods applied to the compressible Euler equations</title>
      <link>https://arxiv.org/abs/2505.15985</link>
      <description>arXiv:2505.15985v1 Announce Type: new 
Abstract: This paper investigates the application of a fast-wave slow-wave spectral deferred correction time-stepping method (FWSW-SDC) to the compressible Euler equations. The resulting model achieves arbitrary order accuracy in time, demonstrating robust performance in standard benchmark idealised test cases for dynamical cores used for numerical weather prediction. The model uses a compatible finite element spatial discretisation, achieving good linear wave dispersion properties without spurious computational modes. A convergence test confirms the model's high temporal accuracy. Arbitrarily high spatial-temporal convergence is demonstrated using a gravity wave test case. The model is further extended to include the parametrisation of a simple physics process by adding two phases of moisture and its validity is demonstrated for a rising thermal problem. Finally, a baroclinic wave in simulated in a Cartesian domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15985v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.ao-ph</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Brown, Joscha Fregin, Thomas Bendall, Thomas Melvin, Daniel Ruprecht, Jemma Shipton</dc:creator>
    </item>
    <item>
      <title>A broken-FEEC framework for structure-preserving discretizations of polar domains with tensor-product splines</title>
      <link>https://arxiv.org/abs/2505.15996</link>
      <description>arXiv:2505.15996v1 Announce Type: new 
Abstract: We propose a novel projection-based approach to derive structure-preserving Finite Element Exterior Calculus (FEEC) discretizations using standard tensor-product splines on domains with a polar singularity. This approach follows the main lines of broken-FEEC schemes which define stable and structure-preserving operators in non-conforming discretizations of the de Rham sequence. Here, we devise a polar broken-FEEC framework that enables the use of standard tensor-product spline spaces while ensuring stability and smoothness for the solutions, as well as the preservation of the de Rham structure: A benefit of this approach is the ability to reuse codes that implement standard splines on smooth parametric domains, and efficient solvers such as Kronecker-product spline interpolation. Our construction is based on two pillars: the first one is an explicit characterization of smooth polar spline spaces within the tensor-product splines ones, which are either discontinuous or non square-integrable as a result of the singular polar pushforward operators. The second pillar consists of local, explicit and matrix-free conforming projection operators that map general tensor-product splines onto smooth polar splines, and that commute with the differential operators of the de Rham sequence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15996v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yaman G\"u\c{c}l\"u, Francesco Patrizi, Martin Campos Pinto</dc:creator>
    </item>
    <item>
      <title>Locally Subspace-Informed Neural Operators for Efficient Multiscale PDE Solving</title>
      <link>https://arxiv.org/abs/2505.16030</link>
      <description>arXiv:2505.16030v1 Announce Type: new 
Abstract: Neural operators (NOs) struggle with high-contrast multiscale partial differential equations (PDEs), where fine-scale heterogeneities cause large errors. To address this, we use the Generalized Multiscale Finite Element Method (GMsFEM) that constructs localized spectral basis functions on coarse grids. This approach efficiently captures dominant multiscale features while solving heterogeneous PDEs accurately at reduced computational cost. However, computing these basis functions is computationally expensive. This gap motivates our core idea: to use a NO to learn the subspace itself - rather than individual basis functions - by employing a subspace-informed loss. On standard multiscale benchmarks - namely a linear elliptic diffusion problem and the nonlinear, steady-state Richards equation - our hybrid method cuts solution error by approximately $60\%$ compared with standalone NOs and reduces basis-construction time by about $60$ times relative to classical GMsFEM, while remaining independent of forcing terms and boundary conditions. The result fuses multiscale finite-element robustness with NO speed, yielding a practical solver for heterogeneous PDEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16030v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Rudikov, Vladimir Fanaskov, Sergei Stepanov, Buzheng Shan, Ekaterina Muravleva, Yalchin Efendiev, Ivan Oseledets</dc:creator>
    </item>
    <item>
      <title>CUR Matrix Approximation through Convex Optimization for Feature Selection</title>
      <link>https://arxiv.org/abs/2505.16032</link>
      <description>arXiv:2505.16032v1 Announce Type: new 
Abstract: The singular value decomposition (SVD) is commonly used in applications requiring a low rank matrix approximation. However, the singular vectors cannot be interpreted in terms of the original data. For applications requiring this type of interpretation, e.g., selection of important data matrix columns or rows, the approximate CUR matrix factorization can be used. Work on the CUR matrix approximation has generally focused on algorithm development, theoretical guarantees, and applications. In this work, we present a novel deterministic CUR formulation and algorithm with theoretical convergence guarantees. The algorithm utilizes convex optimization, finds important columns and rows separately, and allows the user to control the number of important columns and rows selected from the original data matrix. We present numerical results and demonstrate the effectiveness of our CUR algorithm as a feature selection method on gene expression data. These results are compared to those using the SVD and other CUR algorithms as the feature selection method. Lastly, we present a novel application of CUR as a feature selection method to determine discriminant proteins when clustering protein expression data in a self-organizing map (SOM), and compare the performance of multiple CUR algorithms in this application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16032v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kathryn Linehan, Radu Balan</dc:creator>
    </item>
    <item>
      <title>Regularizing Ill-Posed Inverse Problems: Deblurring Barcodes</title>
      <link>https://arxiv.org/abs/2505.16045</link>
      <description>arXiv:2505.16045v1 Announce Type: new 
Abstract: This manuscript is designed to introduce students in applied mathematics and data science to the concept of regularization for ill-posed inverse problems. Construct a mathematical model that describes how an image gets blurred. Convert a calculus problem into a linear algebra problem by discretization. Inverting the blurring process should sharpen up an image; this requires the solution of a system of linear algebraic equations. Solving this linear system of equations turns out to be delicate, as deblurring is an example of an ill-posed inverse problem. To address this challenge, recast the system as a regularized least squares problem (also known as ridge regression).</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16045v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mark Embree</dc:creator>
    </item>
    <item>
      <title>A novel splitting method for Vlasov-Ampere</title>
      <link>https://arxiv.org/abs/2505.16243</link>
      <description>arXiv:2505.16243v1 Announce Type: new 
Abstract: Vlasov equations model the dynamics of plasma in the collisionless regime. A standard approach for numerically solving the Vlasov equation is to operator split the spatial and velocity derivative terms, allowing simpler time-stepping schemes to be applied to each piece separately (known as the Cheng-Knorr method). One disadvantage of such an operator split method is that the order of accuracy of fluid moments (e.g., mass, momentum, and energy) is restricted by the order of the operator splitting (second-order accuracy in the Cheng-Knorr case). In this work, we develop a novel approach that first represents the particle density function on a velocity mesh with a local fluid approximation in each discrete velocity band and then introduces an operator splitting that splits the inter-velocity band coupling terms from the dynamics within the discrete velocity band. The advantage is that the inter-velocity band coupling terms are only needed to achieve consistency of the full distribution functions, but the local fluid models within each band are sufficient to achieve high-order accuracy on global moments such as mass, momentum, and energy. The resulting scheme is verified on several standard Vlasov-Poisson test cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16243v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.plasm-ph</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James A. Rossmanith, Christine Vaughan</dc:creator>
    </item>
    <item>
      <title>Neural Field Equations with random data</title>
      <link>https://arxiv.org/abs/2505.16343</link>
      <description>arXiv:2505.16343v1 Announce Type: new 
Abstract: We study neural field equations, which are prototypical models of large-scale cortical activity, subject to random data. We view this spatially-extended, nonlocal evolution equation as a Cauchy problem on abstract Banach spaces, with randomness in the synaptic kernel, firing rate function, external stimuli, and initial conditions. We determine conditions on the random data that guarantee existence, uniqueness, and measurability of the solution in an appropriate Banach space, and examine the regularity of the solution in relation to the regularity of the inputs. We present results for linear and nonlinear neural fields, and for the two most common functional setups in the numerical analysis of this problem. In addition to the continuous problem, we analyse in abstract form neural fields that have been spatially discretised, setting the foundations for analysing uncertainty quantification (UQ) schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16343v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.PR</category>
      <category>nlin.PS</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Avitabile, Francesca Cavallini, Svetlana Dubinkina, Gabriel J. Lord</dc:creator>
    </item>
    <item>
      <title>Convergence analysis of GMRES applied to Helmholtz problems near resonances</title>
      <link>https://arxiv.org/abs/2505.16345</link>
      <description>arXiv:2505.16345v1 Announce Type: new 
Abstract: In this work we study how the convergence rate of GMRES is influenced by the properties of linear systems arising from Helmholtz problems near resonances or quasi-resonances. We extend an existing convergence bound to demonstrate that the approximation of small eigenvalues by harmonic Ritz values plays a key role in convergence behavior. Next, we analyze the impact of deflation using carefully selected vectors and combine this with a Complex Shifted Laplacian preconditioner. Finally, we apply these tools to two numerical examples near (quasi-)resonant frequencies, using them to explain how the convergence rate evolves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16345v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victorita Dolean, Pierre Marchand, Axel Modave, Timoth\'ee Raynaud</dc:creator>
    </item>
    <item>
      <title>Stochastic collocation schemes for Neural Field Equations with random data</title>
      <link>https://arxiv.org/abs/2505.16443</link>
      <description>arXiv:2505.16443v1 Announce Type: new 
Abstract: We develop and analyse numerical schemes for uncertainty quantification in neural field equations subject to random parametric data in the synaptic kernel, firing rate, external stimulus, and initial conditions. The schemes combine a generic projection method for spatial discretisation to a stochastic collocation scheme for the random variables. We study the problem in operator form, and derive estimates for the total error of the schemes, in terms of the spatial projector. We give conditions on the projected random data which guarantee analyticity of the semi-discrete solution as a Banach-valued function. We illustrate how to verify hypotheses starting from analytic random data and a choice of spatial projection. We provide evidence that the predicted convergence rates are found in various numerical experiments for linear and nonlinear neural field problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16443v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>nlin.PS</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Avitabile, Francesca Cavallini, Svetlana Dubinkina, Gabriel J. Lord</dc:creator>
    </item>
    <item>
      <title>Local projection stabilization methods for $\boldsymbol{H}({\rm curl})$ and $\boldsymbol{H}({\rm div})$ advection problems</title>
      <link>https://arxiv.org/abs/2505.16468</link>
      <description>arXiv:2505.16468v1 Announce Type: new 
Abstract: We devise local projection stabilization (LPS) methods for advection problems in the $\boldsymbol{H}$(curl) and $\boldsymbol{H}$(div) spaces, employing conforming finite element spaces of arbitrary order within a unified framework. The key ingredient is a local inf-sup condition, enabled by enriching the approximation space with appropriate $\boldsymbol{H}$(d) bubble functions (with d = curl or div). This enrichment allows for the construction of modified interpolation operators, which are crucial for establishing optimal a priori error estimates in the energy norm. Numerical examples are presented to verify both the theoretical results and the stabilization properties of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16468v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yangfan Luo, Jindong Wang, Shuonan Wu</dc:creator>
    </item>
    <item>
      <title>Implicit Neural Shape Optimization for 3D High-Contrast Electrical Impedance Tomography</title>
      <link>https://arxiv.org/abs/2505.16487</link>
      <description>arXiv:2505.16487v1 Announce Type: new 
Abstract: We present a novel implicit neural shape optimization framework for 3D high-contrast Electrical Impedance Tomography (EIT), addressing scenarios where conductivity exhibits sharp discontinuities across material interfaces. These high-contrast cases, prevalent in metallic implant monitoring and industrial defect detection, challenge traditional reconstruction methods due to severe ill-posedness. Our approach synergizes shape optimization with implicit neural representations, introducing key innovations including a shape derivative-based optimization scheme that explicitly incorporates high-contrast interface conditions and an efficient latent space representation that reduces variable dimensionality. Through rigorous theoretical analysis of algorithm convergence and extensive numerical experiments, we demonstrate substantial performance improvements, establishing our framework as promising for practical applications in medical imaging with metallic implants and industrial non-destructive testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16487v1</guid>
      <category>math.NA</category>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junqing Chen, Haibo Liu</dc:creator>
    </item>
    <item>
      <title>A Riemannian Optimization Approach for Finding the Nearest Reversible Markov Chain</title>
      <link>https://arxiv.org/abs/2505.16762</link>
      <description>arXiv:2505.16762v1 Announce Type: new 
Abstract: We address the algorithmic problem of determining the reversible Markov chain $\tilde X$ that is closest to a given Markov chain $X$, with an identical stationary distribution. More specifically, $\tilde X$ is the reversible Markov chain with the closest transition matrix, in the Frobenius norm, to the transition matrix of $X$. To compute the transition matrix of $\tilde X$, we propose a novel approach based on Riemannian optimization. Our method introduces a modified multinomial manifold endowed with a prescribed stationary vector, while also satisfying the detailed balance conditions, all within the framework of the Fisher metric. We evaluate the performance of the proposed approach in comparison with an existing quadratic programming method and demonstrate its effectiveness through a series of synthetic experiments, as well as in the construction of a reversible Markov chain from transition count data obtained via direct estimation from a stochastic differential equation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16762v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabio Durastante, Miryam Gnazzo, Beatrice Meini</dc:creator>
    </item>
    <item>
      <title>Quasi-optimal hierarchically semi-separable matrix approximation</title>
      <link>https://arxiv.org/abs/2505.16937</link>
      <description>arXiv:2505.16937v1 Announce Type: new 
Abstract: We present a randomized algorithm for producing a quasi-optimal hierarchically semi-separable (HSS) approximation to an $N\times N$ matrix $A$ using only matrix-vector products with $A$ and $A^T$. We prove that, using $O(k \log(N/k))$ matrix-vector products and ${O}(N k^2 \log(N/k))$ additional runtime, the algorithm returns an HSS matrix $B$ with rank-$k$ blocks whose expected Frobenius norm error $\mathbb{E}[\|A - B\|_F^2]$ is at most $O(\log(N/k))$ times worse than the best possible approximation error by an HSS rank-$k$ matrix. In fact, the algorithm we analyze in a simple modification of an empirically effective method proposed by [Levitt &amp; Martinsson, SISC 2024]. As a stepping stone towards our main result, we prove two results that are of independent interest: a similar guarantee for a variant of the algorithm which accesses $A$'s entries directly, and explicit error bounds for near-optimal subspace approximation using projection-cost-preserving sketches. To the best of our knowledge, our analysis constitutes the first polynomial-time quasi-optimality result for HSS matrix approximation, both in the explicit access model and the matrix-vector product query model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16937v1</guid>
      <category>math.NA</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noah Amsel, Tyler Chen, Feyza Duman Keles, Diana Halikias, Cameron Musco, Christopher Musco, David Persson</dc:creator>
    </item>
    <item>
      <title>Lp boundedness, r-nuclearity and approximation of pseudo-differential operators on $\hbar\mathbb{Z}^n$</title>
      <link>https://arxiv.org/abs/2505.16812</link>
      <description>arXiv:2505.16812v1 Announce Type: cross 
Abstract: In this work sufficient conditions on the order of the symbol are developed to ensure boundedness, compactness and r-nuclearity of pseudo-differential operators in $\hbar\mathbb{Z}^n$. In addition, these conditions allow us to obtain growth estimates for the eigenvalues of some elliptic operators, in particular perturbed discrete Schr\"odinger operator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16812v1</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Pablo Lopez</dc:creator>
    </item>
    <item>
      <title>The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm</title>
      <link>https://arxiv.org/abs/2505.16932</link>
      <description>arXiv:2505.16932v1 Announce Type: cross 
Abstract: Computing the polar decomposition and the related matrix sign function, has been a well-studied problem in numerical analysis for decades. More recently, it has emerged as an important subroutine in deep learning, particularly within the Muon optimization framework. However, the requirements in this setting differ significantly from those of traditional numerical analysis. In deep learning, methods must be highly efficient and GPU-compatible, but high accuracy is often unnecessary. As a result, classical algorithms like Newton-Schulz (which suffers from slow initial convergence) and methods based on rational functions (which rely on QR decompositions or matrix inverses) are poorly suited to this context. In this work, we introduce Polar Express, a GPU-friendly algorithm for computing the polar decomposition. Like classical polynomial methods such as Newton-Schulz, our approach uses only matrix-matrix multiplications, making it GPU-compatible. Motivated by earlier work of Chen &amp; Chow and Nakatsukasa &amp; Freund, Polar Express adapts the polynomial update rule at each iteration by solving a minimax optimization problem, and we prove that it enjoys a strong worst-case optimality guarantee. This property ensures both rapid early convergence and fast asymptotic convergence. We also address finite-precision issues, making it stable in bfloat16 in practice. We apply Polar Express within the Muon optimization framework and show consistent improvements in validation loss on large-scale models such as GPT-2, outperforming recent alternatives across a range of learning rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16932v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noah Amsel, David Persson, Christopher Musco, Robert Gower</dc:creator>
    </item>
    <item>
      <title>Horospherically Convex Optimization on Hadamard Manifolds Part I: Analysis and Algorithms</title>
      <link>https://arxiv.org/abs/2505.16970</link>
      <description>arXiv:2505.16970v1 Announce Type: cross 
Abstract: Geodesic convexity (g-convexity) is a natural generalization of convexity to Riemannian manifolds. However, g-convexity lacks many desirable properties satisfied by Euclidean convexity. For instance, the natural notions of half-spaces and affine functions are themselves not g-convex. Moreover, recent studies have shown that the oracle complexity of geodesically convex optimization necessarily depends on the curvature of the manifold (Criscitiello and Boumal, 2022; Criscitiello and Boumal, 2023; Hamilton and Moitra, 2021), a computational bottleneck for several problems, e.g., tensor scaling. Recently, Lewis et al. (2024) addressed this challenge by proving curvature-independent convergence of subgradient descent, assuming horospherical convexity of the objective's sublevel sets. Using a similar idea, we introduce a generalization of convex functions to Hadamard manifolds, utilizing horoballs and Busemann functions as building blocks (as proxies for half-spaces and affine functions). We refer to this new notion as horospherical convexity (h-convexity). We provide algorithms for both nonsmooth and smooth h-convex optimization, which have curvature-independent guarantees exactly matching those from Euclidean space; this includes generalizations of subgradient descent and Nesterov's accelerated method. Motivated by applications, we extend these algorithms and their convergence rates to minimizing a sum of horospherically convex functions, assuming access to a weighted-Fr\'echet-mean oracle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16970v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.DG</category>
      <category>math.NA</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher Criscitiello, Jungbin Kim</dc:creator>
    </item>
    <item>
      <title>Guided Diffusion Sampling on Function Spaces with Applications to PDEs</title>
      <link>https://arxiv.org/abs/2505.17004</link>
      <description>arXiv:2505.17004v1 Announce Type: cross 
Abstract: We propose a general framework for conditional sampling in PDE-based inverse problems, targeting the recovery of whole solutions from extremely sparse or noisy measurements. This is accomplished by a function-space diffusion model and plug-and-play guidance for conditioning. Our method first trains an unconditional discretization-agnostic denoising model using neural operator architectures. At inference, we refine the samples to satisfy sparse observation data via a gradient-based guidance mechanism. Through rigorous mathematical analysis, we extend Tweedie's formula to infinite-dimensional Hilbert spaces, providing the theoretical foundation for our posterior sampling approach. Our method (FunDPS) accurately captures posterior distributions in function spaces under minimal supervision and severe data scarcity. Across five PDE tasks with only 3% observation, our method achieves an average 32% accuracy improvement over state-of-the-art fixed-resolution diffusion baselines while reducing sampling steps by 4x. Furthermore, multi-resolution fine-tuning ensures strong cross-resolution generalizability. To the best of our knowledge, this is the first diffusion-based framework to operate independently of discretization, offering a practical and flexible solution for forward and inverse problems in the context of PDEs. Code is available at https://github.com/neuraloperator/FunDPS</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17004v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiachen Yao, Abbas Mammadov, Julius Berner, Gavin Kerrigan, Jong Chul Ye, Kamyar Azizzadenesheli, Anima Anandkumar</dc:creator>
    </item>
    <item>
      <title>Differential approximation of the Gaussian by short cosine sums with exponential error decay</title>
      <link>https://arxiv.org/abs/2307.13587</link>
      <description>arXiv:2307.13587v2 Announce Type: replace 
Abstract: In this paper, we propose a method to approximate the Gaussian function on ${\mathbb R}$ by a short cosine sum. We generalise and extend the differential approximation method proposed in [4, 40] to approximate $\mathrm{e}^{-t^{2}/2\sigma}$ in the weighted space $L^{2}({\mathbb R}, \mathrm{e}^{-t^{2}/2\rho})$ where $\sigma, \, \rho &gt;0$. We prove that the optimal frequency parameters $\lambda_1, \ldots , \lambda_{N}$ for this method in the approximation problem $ \min\limits_{\lambda_{1},\ldots, \lambda_{N}, \gamma_{1}, \ldots, \gamma_{N}}\|\mathrm{e}^{-\cdot^{2}/2\sigma} - \sum_{j=1}^{N} \gamma_{j} \, {\mathrm e}^{\lambda_{j} \cdot}\|_{L^{2}({\mathbb R}, \mathrm{e}^{-t^{2}/2\rho})}$, are zeros of a scaled Hermite polynomial. This observation leads us to a numerically stable approximation method with low computational cost of ${\mathcal O}(N^{3})$ operations. We derive a direct algorithm to solve this approximation problem based on a matrix pencil method for a special structured matrix. The entries of this matrix are determined by hypergeometric functions. For the weighted $L^{2}$-norm, we prove that the approximation error decays exponentially with respect to the length $N$ of the sum. An exponentially decaying error in the (unweighted) $L^{2}$-norm is achieved using a truncated cosine sum. Our new convergence result for approximation of Gaussian functions by exponential sums of length $N$ shows that exponential error decay rates $e^{-cN}$ are not only achievable for complete monotone functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.13587v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nadiia Derevianko, Gerlind Plonka</dc:creator>
    </item>
    <item>
      <title>Projection Methods for Operator Learning and Universal Approximation</title>
      <link>https://arxiv.org/abs/2406.12264</link>
      <description>arXiv:2406.12264v2 Announce Type: replace 
Abstract: We obtain a new universal approximation theorem for continuous (possibly nonlinear) operators on arbitrary Banach spaces using the Leray-Schauder mapping. Moreover, we introduce and study a method for operator learning in Banach spaces $L^p$ of functions with multiple variables, based on orthogonal projections on polynomial bases. We derive a universal approximation result for operators where we learn a linear projection and a finite dimensional mapping under some additional assumptions. For the case of $p=2$, we give some sufficient conditions for the approximation results to hold. This article serves as the theoretical framework for a deep learning methodology in operator learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12264v2</guid>
      <category>math.NA</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emanuele Zappala</dc:creator>
    </item>
    <item>
      <title>The high resolution sampling methods for acoustic sources from multi-frequency far field patterns at sparse observation directions</title>
      <link>https://arxiv.org/abs/2408.10829</link>
      <description>arXiv:2408.10829v2 Announce Type: replace 
Abstract: This work is dedicated to novel uniqueness results and high resolution sampling methods for source support from multi-frequency sparse far field patterns. With a single pair of observation directions $\pm\hat{x}$, we prove that the lines $\{z\in\mathbb R^2|\, \hat{x}\cdot z = \hat{x}\cdot y, \,y\in A_{\hat{x}}\}$ can be determined by multi-frequency far field patterns at the directions $\pm\hat{x}$, where $A_{\hat{x}}$ denotes a set containing the corners of the boundary and points whose normal vector to the boundary is parallel to $\hat{x}$. Furthermore, if the source support is composed of polygons and annuluses, then we prove that the support can be determined by multi-frequency far field patterns at sparse directions. Precisely, the lowest number of the observation directions is given in terms of the number of the corners and the annuluses. Inspired by the uniqueness arguments, we introduce two novel indicators to determine the source support. Numerical examples in two dimensions are presented to show the validity and robustness of the two indicators for reconstructing the boundaries of the source support with a high resolution. The second indicator also shows its powerful ability to determine the unknown source function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10829v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.AP</category>
      <category>math.MP</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaodong Liu, Qingxiang Shi</dc:creator>
    </item>
    <item>
      <title>A New Fick-Jacobs Derivation with Applications to Computational Branched Diffusion Networks</title>
      <link>https://arxiv.org/abs/2501.08247</link>
      <description>arXiv:2501.08247v2 Announce Type: replace 
Abstract: The Fick-Jacobs equation is a classical model reduction of 3-dimensional diffusion in a tube of varying radius to a 1-dimensional problem with radially scaled derivatives. This model has been shown to be unstable when the radial gradient is too steep. In this work, we present a new derivation of the Fick-Jacobs equation that results in the addition of higher order spatial derivative terms that provide additional stability in a wide variety of cases and improved solution convergence. We also derive new numerical schemes for branched nodes within networks and provide stability conditions for these schemes. The computational accuracy, efficiency, and stability of our method is demonstrated through a variety of numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08247v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zachary M. Miksis, Gillian Queisser</dc:creator>
    </item>
    <item>
      <title>What is a Sketch-and-Precondition Derivation for Low-Rank Approximation? Inverse Power Error or Inverse Power Estimation?</title>
      <link>https://arxiv.org/abs/2502.07993</link>
      <description>arXiv:2502.07993v2 Announce Type: replace 
Abstract: Randomized sketching accelerates large-scale numerical linear algebra by reducing computational complexity. While the traditional sketch-and-solve approach reduces the problem size directly through sketching, the sketch-and-precondition method leverages sketching to construct a computational friendly preconditioner. This preconditioner improves the convergence speed of iterative solvers applied to the original problem, maintaining accuracy in the full space. Furthermore, the convergence rate of the solver improves at least linearly with the sketch size. Despite its potential, developing a sketch-and-precondition framework for randomized algorithms in low-rank matrix approximation remains an open challenge. We introduce the Error-Powered Sketched Inverse Iteration (EPSI) Method via run sketched Newton iteration for the Lagrange form as a sketch-and-precondition variant for randomized low-rank approximation. Our method achieves theoretical guarantees, including a convergence rate that improves at least linearly with the sketch size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07993v2</guid>
      <category>math.NA</category>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruihan Xu, Yiping Lu</dc:creator>
    </item>
    <item>
      <title>A Modified Hermite Radial Basis Function for Accurate Interpolation</title>
      <link>https://arxiv.org/abs/2503.05752</link>
      <description>arXiv:2503.05752v3 Announce Type: replace 
Abstract: Accurate interpolation of functions and derivatives is crucial in solving partial differential equations (PDEs). The Radial Basis Function (RBF) method has become an extremely popular and robust approach for interpolation on scattered data. Hermite Radial Basis Function (HRBF) methods are an extension of the RBF and improve the overall accuracy by incorporating both function and derivative information. Infinitely smooth kernels, such as the Gaussian, use a shape-parameter to describe the width of support and are widely used due to their excellent approximation accuracy and ability to capture fine-scale details. Unfortunately, the use of infinitely smooth kernels suffers from ill-conditioning at low to moderate shape parameters, which affects the accuracy. This work proposes a Modified HRBF (MHRBF) method that introduces an additional polynomial term to balance kernel behavior, improving accuracy while maintaining or lowering computational cost. Using standard double-precision mathematics, the results indicate that compared to the HRBF method, the MHRBF method achieves lower error for all values of the shape parameter and domain size. The MHRBF is also able to achieve low errors at a lower computational cost as compared to the standard HRBF method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05752v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amirhossein Fashamiha, David Salac</dc:creator>
    </item>
    <item>
      <title>Maximum bound principle for Q-tensor gradient flow with low regularity integrators</title>
      <link>https://arxiv.org/abs/2504.11676</link>
      <description>arXiv:2504.11676v3 Announce Type: replace 
Abstract: We investigate low-regularity integrator (LRI) methods for the Q-tensor model governing nematic liquid-crystalline semilinear parabolic equation. First- and second-order temporal discretizations are developed using Duhamel's formula, and we rigorously prove that both schemes preserve the maximum bound principle (MBP) and energy dissipation under minimal regularity requirements. Optimal convergence rates are established for the proposed methods. Numerical experiments validate the theoretical findings, demonstrating that the eigenvalues of Q remain strictly confined within the physical range (-1/3},2/3).</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11676v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenshuai Hu, Guanghua Ji</dc:creator>
    </item>
    <item>
      <title>Convergence Guarantees for Gradient-Based Training of Neural PDE Solvers: From Linear to Nonlinear PDEs</title>
      <link>https://arxiv.org/abs/2505.14002</link>
      <description>arXiv:2505.14002v2 Announce Type: replace 
Abstract: We present a unified convergence theory for gradient-based training of neural network methods for partial differential equations (PDEs), covering both physics-informed neural networks (PINNs) and the Deep Ritz method. For linear PDEs, we extend the neural tangent kernel (NTK) framework for PINNs to establish global convergence guarantees for a broad class of linear operators. For nonlinear PDEs, we prove convergence to critical points via the \L{}ojasiewicz inequality under the random feature model, eliminating the need for strong over-parameterization and encompassing both gradient flow and implicit gradient descent dynamics. Our results further reveal that the random feature model exhibits an implicit regularization effect, preventing parameter divergence to infinity. Theoretical findings are corroborated by numerical experiments, providing new insights into the training dynamics and robustness of neural network PDE solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14002v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Zhao, Tao Luo</dc:creator>
    </item>
    <item>
      <title>A deep solver for BSDEs with jumps</title>
      <link>https://arxiv.org/abs/2211.04349</link>
      <description>arXiv:2211.04349v3 Announce Type: replace-cross 
Abstract: The aim of this work is to propose an extension of the deep solver by Han, Jentzen, E (2018) to the case of forward backward stochastic differential equations (FBSDEs) with jumps. As in the aforementioned solver, starting from a discretized version of the FBSDE and parametrizing the (high dimensional) control processes by means of a family of artificial neural networks (ANNs), the FBSDE is viewed as a model-based reinforcement learning problem and the ANN parameters are fitted so as to minimize a prescribed loss function. We take into account both finite and infinite jump activity by introducing, in the latter case, an approximation with finitely many jumps of the forward process. We successfully apply our algorithm to option pricing problems in low and high dimension and discuss the applicability in the context of counterparty credit risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.04349v3</guid>
      <category>math.PR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>q-fin.CP</category>
      <category>q-fin.PR</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kristoffer Andersson, Alessandro Gnoatto, Marco Patacca, Athena Picarelli</dc:creator>
    </item>
    <item>
      <title>A Discrete Exterior Calculus of Bundle-valued Forms</title>
      <link>https://arxiv.org/abs/2406.05383</link>
      <description>arXiv:2406.05383v2 Announce Type: replace-cross 
Abstract: The discretization of Cartan's exterior calculus of differential forms has been fruitful in a variety of theoretical and practical endeavors: from computational electromagnetics to the development of Finite-Element Exterior Calculus, the development of structure-preserving numerical tools satisfying exact discrete equivalents to Stokes' theorem or the de Rham complex for the exterior derivative have found numerous applications in computational physics. However, there has been a dearth of effort in establishing a more general discrete calculus, this time for differential forms with values in vector bundles over a combinatorial manifold equipped with a connection. In this work, we propose a discretization of the exterior covariant derivative of bundle-valued differential forms. We demonstrate that our discrete operator mimics its continuous counterpart, satisfies the Bianchi identities on simplicial cells, and contrary to previous attempts at its discretization, ensures numerical convergence to its exact evaluation with mesh refinement under mild assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05383v2</guid>
      <category>math.DG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Theo Braune, Yiying Tong, Fran\c{c}ois Gay-Balmaz, Mathieu Desbrun</dc:creator>
    </item>
    <item>
      <title>Isotropic Q-fractional Brownian motion on the sphere: regularity and fast simulation</title>
      <link>https://arxiv.org/abs/2410.19649</link>
      <description>arXiv:2410.19649v2 Announce Type: replace-cross 
Abstract: As an extension of isotropic Gaussian random fields and Q-Wiener processes on d-dimensional spheres, isotropic Q-fractional Brownian motion is introduced and sample H\"older regularity in space-time is shown depending on the regularity of the spatial covariance operator Q and the Hurst parameter H. The processes are approximated by a spectral method in space for which strong and almost sure convergence are shown. The underlying sample paths of fractional Brownian motion are simulated by circulant embedding or conditionalized random midpoint displacement. Temporal accuracy and computational complexity are numerically tested, the latter matching the complexity of simulating a Q-Wiener process if allowing for a temporal error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19649v2</guid>
      <category>math.PR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1098/rsta.2024.0238</arxiv:DOI>
      <dc:creator>Annika Lang, Bj\"orn M\"uller</dc:creator>
    </item>
    <item>
      <title>Discrepancies are Virtue: Weak-to-Strong Generalization through Lens of Intrinsic Dimension</title>
      <link>https://arxiv.org/abs/2502.05075</link>
      <description>arXiv:2502.05075v3 Announce Type: replace-cross 
Abstract: Weak-to-strong (W2S) generalization is a type of finetuning (FT) where a strong (large) student model is trained on pseudo-labels generated by a weak teacher. Surprisingly, W2S FT often outperforms the weak teacher. We seek to understand this phenomenon through the observation that FT often occurs in intrinsically low-dimensional spaces. Leveraging the low intrinsic dimensionality of FT, we analyze W2S in the ridgeless regression setting from a variance reduction perspective. For a strong student-weak teacher pair with sufficiently expressive low-dimensional feature subspaces $\mathcal{V}_s, \mathcal{V}_w$, we provide an exact characterization of the variance that dominates the generalization error of W2S. This unveils a virtue of discrepancy between the strong and weak models in W2S: the variance of the weak teacher is inherited by the strong student in $\mathcal{V}_s \cap \mathcal{V}_w$, while reduced by a factor of $\dim(\mathcal{V}_s)/N$ in the subspace of discrepancy $\mathcal{V}_w \setminus \mathcal{V}_s$ with $N$ pseudo-labels for W2S. Our analysis further casts light on the sample complexities and the scaling of performance gap recovery in W2S. The analysis is supported by experiments on synthetic regression problems, as well as real vision and NLP tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05075v3</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yijun Dong, Yicheng Li, Yunai Li, Jason D. Lee, Qi Lei</dc:creator>
    </item>
    <item>
      <title>Interacting Twisted Bilayer Graphene with Systematic Modeling of Structural Relaxation</title>
      <link>https://arxiv.org/abs/2504.03479</link>
      <description>arXiv:2504.03479v2 Announce Type: replace-cross 
Abstract: Twisted bilayer graphene (TBG) has drawn significant interest due to recent experiments which show that TBG can exhibit strongly correlated behavior such as the superconducting and correlated insulator phases. Much of the theoretical work on TBG has been based on analysis of the Bistritzer-MacDonald model which includes a phenomenological parameter to account for lattice relaxation. In this work, we use a newly developed continuum model which systematically accounts for the effects of structural relaxation. In particular, we model structural relaxation by coupling linear elasticity to a stacking energy that penalizes disregistry. We compare the impact of the two relaxation models on the corresponding many-body model by defining an interacting model projected to the flat bands. We perform tests at charge neutrality at both the Hartree-Fock and Coupled Cluster Singles and Doubles (CCSD) level of theory and find the systematic relaxation model gives quantitative differences from the simplified relaxation model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03479v2</guid>
      <category>math-ph</category>
      <category>cond-mat.mes-hall</category>
      <category>cond-mat.str-el</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.MP</category>
      <category>math.NA</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyu Kong, Alexander B. Watson, Mitchell Luskin, Kevin D. Stubbs</dc:creator>
    </item>
    <item>
      <title>Partition-wise Graph Filtering: A Unified Perspective Through the Lens of Graph Coarsening</title>
      <link>https://arxiv.org/abs/2505.14033</link>
      <description>arXiv:2505.14033v2 Announce Type: replace-cross 
Abstract: Filtering-based graph neural networks (GNNs) constitute a distinct class of GNNs that employ graph filters to handle graph-structured data, achieving notable success in various graph-related tasks. Conventional methods adopt a graph-wise filtering paradigm, imposing a uniform filter across all nodes, yet recent findings suggest that this rigid paradigm struggles with heterophilic graphs. To overcome this, recent works have introduced node-wise filtering, which assigns distinct filters to individual nodes, offering enhanced adaptability. However, a fundamental gap remains: a comprehensive framework unifying these two strategies is still absent, limiting theoretical insights into the filtering paradigms. Moreover, through the lens of Contextual Stochastic Block Model, we reveal that a synthesis of graph-wise and node-wise filtering provides a sufficient solution for classification on graphs exhibiting both homophily and heterophily, suggesting the risk of excessive parameterization and potential overfitting with node-wise filtering. To address the limitations, this paper introduces Coarsening-guided Partition-wise Filtering (CPF). CPF innovates by performing filtering on node partitions. The method begins with structure-aware partition-wise filtering, which filters node partitions obtained via graph coarsening algorithms, and then performs feature-aware partition-wise filtering, refining node embeddings via filtering on clusters produced by $k$-means clustering over features. In-depth analysis is conducted for each phase of CPF, showing its superiority over other paradigms. Finally, benchmark node classification experiments, along with a real-world graph anomaly detection application, validate CPF's efficacy and practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14033v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>eess.SP</category>
      <category>math.NA</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3711896.3737075</arxiv:DOI>
      <dc:creator>Guoming Li, Jian Yang, Yifan Chen</dc:creator>
    </item>
  </channel>
</rss>
