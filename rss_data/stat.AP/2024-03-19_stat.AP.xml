<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 19 Mar 2024 04:01:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 19 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Process-Aware Analysis of Treatment Paths in Heart Failure Patients: A Case Study</title>
      <link>https://arxiv.org/abs/2403.10544</link>
      <description>arXiv:2403.10544v1 Announce Type: new 
Abstract: Process mining in healthcare presents a range of challenges when working with different types of data within the healthcare domain. There is high diversity considering the variety of data collected from healthcare processes: operational processes given by claims data, a collection of events during surgery, data related to pre-operative and post-operative care, and high-level data collections based on regular ambulant visits with no apparent events. In this case study, a data set from the last category is analyzed. We apply process-mining techniques on sparse patient heart failure data and investigate whether an information gain towards several research questions is achievable. Here, available data are transformed into an event log format, and process discovery and conformance checking are applied. Additionally, patients are split into different cohorts based on comorbidities, such as diabetes and chronic kidney disease, and multiple statistics are compared between the cohorts. Conclusively, we apply decision mining to determine whether a patient will have a cardiovascular outcome and whether a patient will die.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10544v1</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.5220/0012392600003657</arxiv:DOI>
      <dc:creator>Harry H. Beyel, Marlo Verket, Viki Peeva, Christian Rennert, Marco Pegoraro, Katharina Sch\"utt, Wil M. P. van der Aalst, Nikolaus Marx</dc:creator>
    </item>
    <item>
      <title>Bayesian Design for Sampling Anomalous Spatio-Temporal Data</title>
      <link>https://arxiv.org/abs/2403.10791</link>
      <description>arXiv:2403.10791v1 Announce Type: new 
Abstract: Data collected from arrays of sensors are essential for informed decision-making in various systems. However, the presence of anomalies can compromise the accuracy and reliability of insights drawn from the collected data or information obtained via statistical analysis. This study aims to develop a robust Bayesian optimal experimental design (BOED) framework with anomaly detection methods for high-quality data collection. We introduce a general framework that involves anomaly generation, detection and error scoring when searching for an optimal design. This method is demonstrated using two comprehensive simulated case studies: the first study uses a spatial dataset, and the second uses a spatio-temporal river network dataset. As a baseline approach, we employed a commonly used prediction-based utility function based on minimising errors. Results illustrate the trade-off between predictive accuracy and anomaly detection performance for our method under various design scenarios. An optimal design robust to anomalies ensures the collection and analysis of more trustworthy data, playing a crucial role in understanding the dynamics of complex systems such as the environment, therefore enabling informed decisions in monitoring, management, and response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10791v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katie Buchhorn, Kerrie Mengersen, Edgar Santos-Fernandez, James McGree</dc:creator>
    </item>
    <item>
      <title>Long-range Ising model for regional-scale seismic risk analysis</title>
      <link>https://arxiv.org/abs/2403.11429</link>
      <description>arXiv:2403.11429v1 Announce Type: new 
Abstract: This study introduces the long-range Ising model from statistical mechanics to the Performance-Based Earthquake Engineering (PBEE) framework for regional seismic damage analysis. The application of the PBEE framework at a regional scale entails estimating the damage states of numerous structures, typically performed using fragility function-based stochastic simulations. However, these simulations often assume independence or employ simplistic dependency models among capacities of structures, leading to significant misrepresentation of risk. The Ising model addresses this issue by converting the available information on binary damage states (safe or failure) into a joint probability mass function, leveraging the principle of maximum entropy. The Ising model offers two main benefits: (1) it requires only the first- and second-order cross-moments, enabling seamless integration with the existing PBEE framework, and (2) it provides meaningful physical interpretations of the model parameters, facilitating the uncovering of insights not apparent from data. To demonstrate the proposed method, we applied the Ising model to $156$ buildings in Antakya, Turkey, using post-hazard damage evaluation data, and to $182$ buildings in Pacific Heights, San Francisco, using simulated data from the Regional Resilience Determination (R2D) tool. In both instances, the Ising model accurately reproduces the provided information and generates meaningful insights into regional damages. The study also investigates the change in Ising model parameters under varying earthquake magnitudes, along with the mean-field approximation, further facilitating the applicability of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11429v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sebin Oh, Sang-ri Yi, Ziqi Wang</dc:creator>
    </item>
    <item>
      <title>Resilient by Design: Simulating Street Network Disruptions across Every Urban Area in the World</title>
      <link>https://arxiv.org/abs/2403.10636</link>
      <description>arXiv:2403.10636v1 Announce Type: cross 
Abstract: Street networks allow people and goods to move through cities, but they are vulnerable to disasters like floods, earthquakes, and terrorist attacks. Well-planned network design can make a city more resilient and robust to such disruptions, but we still know little about worldwide patterns of vulnerability, or worldwide empirical relationships between specific design characteristics and resilience. This study quantifies and measures the vulnerability of the street networks of every urban area in the world then models the relationships between vulnerability and street network design characteristics. To do so, we simulate over 2.4 billion trips across more than 8,000 urban areas in 178 countries, while also simulating network disruption events representing floods, earthquakes, and targeted attacks. We find that disrupting high-centrality nodes severely impacts network function. All else equal, networks with higher connectivity, fewer chokepoints, or less circuity are less vulnerable to disruption's impacts. This study thus contributes a new global understanding of network design and vulnerability to the literature. We argue that these design characteristics offer high leverage points for street network resilience and robustness that planners should emphasize when designing or retrofitting urban networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10636v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.SY</category>
      <category>econ.GN</category>
      <category>eess.SY</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.tra.2024.104016</arxiv:DOI>
      <arxiv:journal_reference>Transportation Research Part A: Policy and Practice, 2024</arxiv:journal_reference>
      <dc:creator>Geoff Boeing, Jaehyun Ha</dc:creator>
    </item>
    <item>
      <title>Spatio-temporal Occupancy Models with INLA</title>
      <link>https://arxiv.org/abs/2403.10680</link>
      <description>arXiv:2403.10680v1 Announce Type: cross 
Abstract: Modern methods for quantifying and predicting species distribution play a crucial part in biodiversity conservation. Occupancy models are a popular choice for analyzing species occurrence data as they allow to separate the observational error induced by imperfect detection, and the sources of bias affecting the occupancy process. However, the spatial and temporal variation in occupancy not accounted for by environmental covariates is often ignored or modelled through simple spatial structures as the computational costs of fitting explicit spatio-temporal models is too high. In this work, we demonstrate how INLA may be used to fit complex occupancy models and how the R-INLA package can provide a user-friendly interface to make such complex models available to users.
  We show how occupancy models, provided some simplification on the detection process, can be framed as latent Gaussian models and benefit from the powerful INLA machinery. A large selection of complex modelling features, and random effect modelshave already been implemented in R-INLA. These become available for occupancy models, providing the user with an efficient and flexible toolbox.
  We illustrate how INLA provides a computationally efficient framework for developing and fitting complex occupancy models using two case studies. Through these, we show how different spatio-temporal models that include spatial-varying trends, smooth terms, and spatio-temporal random effects can be fitted. At the cost of limiting the complexity of the detection model, INLA can incorporate a range of complex structures in the process.
  INLA-based occupancy models provide an alternative framework to fit complex spatiotemporal occupancy models. The need for new and more flexible computationally approaches to fit such models makes INLA an attractive option for addressing complex ecological problems, and a promising area of research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10680v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jafet Belmont, Sara Martino, Janine Illian, H{\aa}vard Rue</dc:creator>
    </item>
    <item>
      <title>Modelling co-evolution of resource feedback and social network dynamics in human-environmental systems</title>
      <link>https://arxiv.org/abs/2403.10938</link>
      <description>arXiv:2403.10938v1 Announce Type: cross 
Abstract: Games with environmental feedback have become a crucial area of study across various scientific domains, modelling the dynamic interplay between human decisions and environmental changes, and highlighting the consequences of our choices on natural resources and biodiversity. In this work, we propose a co-evolutionary model for human-environment systems that incorporates the effects of knowledge feedback and social interaction on the sustainability of common pool resources. The model represents consumers as agents who adjust their resource extraction based on the resource's state. These agents are connected through social networks, where links symbolize either affinity or aversion among them. The interplay between social dynamics and resource dynamics is explored, with the system's evolution analyzed across various network topologies and initial conditions. We find that knowledge feedback can independently sustain common pool resources. However, the impact of social interactions on sustainability is dual-faceted: it can either support or impede sustainability, influenced by the network's connectivity and heterogeneity. A notable finding is the identification of a critical network mean degree, beyond which a depletion/repletion transition parallels an absorbing/active state transition in social dynamics, i.e., individual agents and their connections are/are not prone to being frozen in their social states. Furthermore, the study examines the evolution of the social network, revealing the emergence of two polarized groups where agents within each community have the same affinity. Comparative analyses using Monte-Carlo simulations and rate equations are employed, along with analytical arguments, to reinforce the study's findings. The model successfully captures how information spread and social dynamics may impact the sustanebility of common pool resource.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10938v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>nlin.AO</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Meghdad Saeedian, Chengyi Tu, Fabio Menegazzo, Paolo D'Odorico, Sandro Azaele, Samir Suweis</dc:creator>
    </item>
    <item>
      <title>Zero-Inflated Stochastic Volatility Model for Disaggregated Inflation Data with Exact Zeros</title>
      <link>https://arxiv.org/abs/2403.10945</link>
      <description>arXiv:2403.10945v1 Announce Type: cross 
Abstract: The disaggregated time-series data for Consumer Price Index often exhibits frequent instances of exact zero price changes, stemming from measurement errors inherent in the data collection process. However, the currently prominent stochastic volatility model of trend inflation is designed for aggregate measures of price inflation, where exact zero price changes rarely occur. We propose a zero-inflated stochastic volatility model applicable to such nonstationary real-valued multivariate time-series data with exact zeros, by a Bayesian dynamic generalized linear model that jointly specifies the dynamic zero-generating process. We also provide an efficient custom Gibbs sampler that leverages the P\'olya-Gamma augmentation. Applying the model to disaggregated Japanese Consumer Price Index data, we find that the zero-inflated model provides more sensible and informative estimates of time-varying trend and volatility. Through an out-of-sample forecasting exercise, we find that the zero-inflated model provides improved point forecasts when zero-inflation is prominent, and better coverage of interval forecasts of the non-zero data by the non-zero distributional component.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10945v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geonhee Han, Kaoru Irie</dc:creator>
    </item>
    <item>
      <title>Effects of model misspecification on small area estimators</title>
      <link>https://arxiv.org/abs/2403.11276</link>
      <description>arXiv:2403.11276v1 Announce Type: cross 
Abstract: Nested error regression models are commonly used to incorporate observational unit specific auxiliary variables to improve small area estimates. When the mean structure of this model is misspecified, there is generally an increase in the mean square prediction error (MSPE) of Empirical Best Linear Unbiased Predictors (EBLUP). Observed Best Prediction (OBP) method has been proposed with the intent to improve on the MSPE over EBLUP. We conduct a Monte Carlo simulation experiment to understand the effect of mispsecification of mean structures on different small area estimators. Our simulation results lead to an unexpected result that OBP may perform very poorly when observational unit level auxiliary variables are used and that OBP can be improved significantly when population means of those auxiliary variables (area level auxiliary variables) are used in the nested error regression model or when a corresponding area level model is used. Our simulation also indicates that the MSPE of OBP in an increasing function of the difference between the sample and population means of the auxiliary variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11276v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuting Chen, Partha Lahiri, Nicola Salvati</dc:creator>
    </item>
    <item>
      <title>Bayesian multi-exposure image fusion for robust high dynamic range preprocessing in ptychography</title>
      <link>https://arxiv.org/abs/2403.11344</link>
      <description>arXiv:2403.11344v1 Announce Type: cross 
Abstract: Image information is restricted by the dynamic range of the detector, which can be addressed using multi-exposure image fusion (MEF). The conventional MEF approach employed in ptychography is often inadequate under conditions of low signal-to-noise ratio (SNR) or variations in illumination intensity. To address this, we developed a Bayesian approach for MEF based on a modified Poisson noise model that considers the background and saturation. Our method outperforms conventional MEF under challenging experimental conditions, as demonstrated by the synthetic and experimental data. Furthermore, this method is versatile and applicable to any imaging scheme requiring high dynamic range (HDR).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11344v1</guid>
      <category>eess.IV</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shantanu Kodgirwar, Lars Loetgering, Chang Liu, Aleena Joseph, Leona Licht, Daniel S. Penagos Molina, Wilhelm Eschen, Jan Rothhardt, Michael Habeck</dc:creator>
    </item>
    <item>
      <title>Risk Set Matched Difference-in-Differences for the Analysis of Effect Modification in an Observational Study on the Impact of Gun Violence on Health Outcomes</title>
      <link>https://arxiv.org/abs/2305.04143</link>
      <description>arXiv:2305.04143v4 Announce Type: replace 
Abstract: Gun violence is a major source of injury and death in the United States. However, relatively little is known about the effects of firearm injuries on survivors and their family members and how these effects vary across subpopulations. To study these questions and, more generally, to address a gap in the causal inference literature, we present a framework for the study of effect modification or heterogeneous treatment effects in difference-in-differences designs. We implement a new matching technique, which combines profile matching and risk set matching, to (i) preserve the time alignment of covariates, exposure, and outcomes, avoiding pitfalls of other common approaches for difference-in-differences, and (ii) explicitly control biases due to imbalances in observed covariates in subgroups discovered from the data. Our case study shows significant and persistent effects of nonfatal firearm injuries on several health outcomes for those injured and on the mental health of their family members. Sensitivity analyses reveal that these results are moderately robust to unmeasured confounding bias. Finally, while the effects for those injured vary largely by the severity of the injury and its documented intent, for families, effects are strongest for those whose relative's injury is documented as resulting from an assault, self-harm, or law enforcement intervention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.04143v4</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric R. Cohn, Zirui Song, Jose R. Zubizarreta</dc:creator>
    </item>
    <item>
      <title>A finite mixture approach for the analysis of digital skills in Finland, Italy and Bulgaria: the role of socio-economic factors</title>
      <link>https://arxiv.org/abs/2311.03801</link>
      <description>arXiv:2311.03801v3 Announce Type: replace 
Abstract: The digital divide is the gap among population sub-groups in accessing and/or using digital technologies. For instance, older people show a lower propensity to have a broadband connection, use the Internet, and adopt new technologies than the younger ones. Motivated by the analysis of the heterogeneity in the use of digital technologies, we build a bipartite network concerning the presence of various digital skills in individuals from three different European countries: Finland, Italy, and Bulgaria. Bipartite networks provide a useful structure for representing relationships between two disjoint sets of nodes, formally called sending and receiving nodes. The goal is to perform a clustering of individuals (sending nodes) based on their digital skills (receiving nodes) for each country. In this regard, we employ a Mixture of Latent Trait Analyzers (MLTA) accounting for concomitant variables, which allows us to (i) cluster individuals according to their individual profile; (ii) analyze how socio-economic and demographic characteristics, as well as intergenerational ties, influence individual digitalization. Results show that the type of digitalization substantially depends on age, income and level of education, while the presence of children in the household seems to play an important role in the digitalization process in Italy and Finland only.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.03801v3</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dalila Failli, Bruno Arpino, Maria Francesca Marino</dc:creator>
    </item>
    <item>
      <title>Passenger Network Ridership Model Through a BRT System, the case of TransMilenio in Bogot\'a</title>
      <link>https://arxiv.org/abs/2112.04009</link>
      <description>arXiv:2112.04009v2 Announce Type: replace-cross 
Abstract: We present a ridership model of individual trajectories of users within a public transport network for which there are several different routes between origin and destiny and for which the automatic fare collection data does not include information about the exit station; only card identification and time of entry into the system. This model is implemented in the case of the Troncal component of TransMilenio, the BRT system that is the backbone of public transport in Bogot\'a. The granularity of the data allows to identify the occupation of every bus within the system at any particular day. As a validation, we compare the average bus occupation of two particular days, 17 of May of 2020 and the 1 of December of 2020.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.04009v2</guid>
      <category>nlin.CG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gabriel Villalobos, Juan D. Garcia-Arteaga, Arturo Arg\"uelles</dc:creator>
    </item>
    <item>
      <title>Unit Commitment Predictor With a Performance Guarantee: A Support Vector Machine Classifier</title>
      <link>https://arxiv.org/abs/2310.08601</link>
      <description>arXiv:2310.08601v2 Announce Type: replace-cross 
Abstract: The system operators usually need to solve large-scale unit commitment problems within limited time frame for computation. This paper provides a pragmatic solution, showing how by learning and predicting the on/off commitment decisions of conventional units, there is a potential for system operators to warm start their solver and speed up their computation significantly. For the prediction, we train linear and kernelized support vector machine classifiers, providing an out-of-sample performance guarantee if properly regularized, converting to distributionally robust classifiers. For the unit commitment problem, we solve a mixed-integer second-order cone problem. Our results based on the IEEE 6- and 118-bus test systems show that the kernelized SVM with proper regularization outperforms other classifiers, reducing the computational time by a factor of 1.7. In addition, if there is a tight computational limit, while the unit commitment problem without warm start is far away from the optimal solution, its warmly-started version can be solved to (near) optimality within the time limit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.08601v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Farzaneh Pourahmadi, Jalal Kazempour</dc:creator>
    </item>
    <item>
      <title>Multinomial belief networks</title>
      <link>https://arxiv.org/abs/2311.16909</link>
      <description>arXiv:2311.16909v2 Announce Type: replace-cross 
Abstract: A Bayesian approach to machine learning is attractive when we need to quantify uncertainty, deal with missing observations, when samples are scarce, or when the data is sparse. All of these commonly apply when analysing healthcare data. To address these analytical requirements, we propose a deep generative model for multinomial count data where both the weights and hidden units of the network are Dirichlet distributed. A Gibbs sampling procedure is formulated that takes advantage of a series of augmentation relations, analogous to the Zhou--Cong--Chen model. We apply the model on small handwritten digits, and a large experimental dataset of DNA mutations in cancer, and we show how the model is able to extract biologically meaningful meta-signatures in a fully data-driven way.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.16909v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>H. C. Donker, D. Neijzen, J. de Jong, G. A. Lunter</dc:creator>
    </item>
    <item>
      <title>Limit Order Book Dynamics and Order Size Modelling Using Compound Hawkes Process</title>
      <link>https://arxiv.org/abs/2312.08927</link>
      <description>arXiv:2312.08927v3 Announce Type: replace-cross 
Abstract: Hawkes Process has been used to model Limit Order Book (LOB) dynamics in several ways in the literature however the focus has been limited to capturing the inter-event times while the order size is usually assumed to be constant. We propose a novel methodology of using Compound Hawkes Process for the LOB where each event has an order size sampled from a calibrated distribution. The process is formulated in a novel way such that the spread of the process always remains positive. Further, we condition the model parameters on time of day to support empirical observations. We make use of an enhanced non-parametric method to calibrate the Hawkes kernels and allow for inhibitory cross-excitation kernels. We showcase the results and quality of fits for an equity stock's LOB in the NASDAQ exchange and compare them against several baselines. Finally, we conduct a market impact study of the simulator and show the empirical observation of a concave market impact function is indeed replicated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.08927v3</guid>
      <category>q-fin.TR</category>
      <category>cs.CE</category>
      <category>q-fin.CP</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Konark Jain, Nick Firoozye, Jonathan Kochems, Philip Treleaven</dc:creator>
    </item>
  </channel>
</rss>
