<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Apr 2024 04:00:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 11 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Confidence Intervals on Multivariate Normal Quantiles for Environmental Specification Development in Multi-axis Shock and Vibration Testing</title>
      <link>https://arxiv.org/abs/2404.06565</link>
      <description>arXiv:2404.06565v1 Announce Type: new 
Abstract: This article describes two Monte Carlo methods for calculating confidence intervals on cumulative density function (CDF) based multivariate normal quantiles that allows for controlling the tail regions of a multivariate distribution where one is most concerned about extreme responses. The CDF based multivariate normal quantiles associated with bivariate distributions are represented as contours and for trivariate distributions represented as iso-surfaces. We first provide a novel methodology for an inverse problem, characterizing the uncertainty on the $\tau^{\mathrm{th}}$ multivariate quantile probability, when using concurrent univariate quantile probabilities. The uncertainty on the $\tau^{\mathrm{th}}$ multivariate quantile probability demonstrates inadequacy in univariate methods which neglect correlation between multiple variates. Limitations of traditional multivariate normal tolerance regions and simultaneous univariate tolerance methods are discussed thereby necessitating the need for confidence intervals on CDF based multivariate normal quantiles. Two Monte Carlo methods are discussed; the first calculates the CDF over a tessellated domain followed by taking a bootstrap confidence interval over the tessellated CDF. The CDF based multivariate quantiles are then estimated from the CDF confidence intervals. For the second method, only the point associated with highest probability density along the CDF based quantile is calculated, which greatly improves the computational speed compared to the first method. Monte Carlo simulation studies are used to assess the performance of the various methods. Finally, real data analysis is performed to illustrate a workflow for CDF based multivariate normal quantiles in the domain of mechanical shock and vibration to specify a minimum conservative test level for environmental specification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06565v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam Watts, Thomas Thompson, Dustin Harvey</dc:creator>
    </item>
    <item>
      <title>A General Identification Algorithm For Data Fusion Problems Under Systematic Selection</title>
      <link>https://arxiv.org/abs/2404.06602</link>
      <description>arXiv:2404.06602v1 Announce Type: new 
Abstract: Causal inference is made challenging by confounding, selection bias, and other complications. A common approach to addressing these difficulties is the inclusion of auxiliary data on the superpopulation of interest. Such data may measure a different set of variables, or be obtained under different experimental conditions than the primary dataset. Analysis based on multiple datasets must carefully account for similarities between datasets, while appropriately accounting for differences.
  In addition, selection of experimental units into different datasets may be systematic; similar difficulties are encountered in missing data problems. Existing methods for combining datasets either do not consider this issue, or assume simple selection mechanisms.
  In this paper, we provide a general approach, based on graphical causal models, for causal inference from data on the same superpopulation that is obtained under different experimental conditions. Our framework allows both arbitrary unobserved confounding, and arbitrary selection processes into different experimental regimes in our data.
  We describe how systematic selection processes may be organized into a hierarchy similar to censoring processes in missing data: selected completely at random (SCAR), selected at random (SAR), and selected not at random (SNAR). In addition, we provide a general identification algorithm for interventional distributions in this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06602v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaron J. R. Lee, AmirEmad Ghassami, Ilya Shpitser</dc:creator>
    </item>
    <item>
      <title>Bayesian Model Selection with Latent Group-Based Effects and Variances with the R Package slgf</title>
      <link>https://arxiv.org/abs/2404.06698</link>
      <description>arXiv:2404.06698v1 Announce Type: new 
Abstract: Linear modeling is ubiquitous, but performance can suffer when the model is misspecified. We have recently demonstrated that latent groupings in the levels of categorical predictors can complicate inference in a variety of fields including bioinformatics, agriculture, industry, engineering, and medicine. Here we present the R package slgf which enables the user to easily implement our recently-developed approach to detect group-based regression effects, latent interactions, and/or heteroscedastic error variance through Bayesian model selection. We focus on the scenario in which the levels of a categorical predictor exhibit two latent groups. We treat the detection of this grouping structure as an unsupervised learning problem by searching the space of possible groupings of factor levels. First we review the suspected latent grouping factor (SLGF) method. Next, using both observational and experimental data, we illustrate the usage of slgf in the context of several common linear model layouts: one-way analysis of variance (ANOVA), analysis of covariance (ANCOVA), a two-way replicated layout, and a two-way unreplicated layout. We have selected data that reveal the shortcomings of classical analyses to emphasize the advantage our method can provide when a latent grouping structure is present.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06698v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas A. Metzger, Christopher T. Franck</dc:creator>
    </item>
    <item>
      <title>Covariance Regression with High-Dimensional Predictors</title>
      <link>https://arxiv.org/abs/2404.06701</link>
      <description>arXiv:2404.06701v1 Announce Type: new 
Abstract: In the high-dimensional landscape, addressing the challenges of covariance regression with high-dimensional covariates has posed difficulties for conventional methodologies. This paper addresses these hurdles by presenting a novel approach for high-dimensional inference with covariance matrix outcomes. The proposed methodology is illustrated through its application in elucidating brain coactivation patterns observed in functional magnetic resonance imaging (fMRI) experiments and unraveling complex associations within anatomical connections between brain regions identified through diffusion tensor imaging (DTI). In the pursuit of dependable statistical inference, we introduce an integrative approach based on penalized estimation. This approach combines data splitting, variable selection, aggregation of low-dimensional estimators, and robust variance estimation. It enables the construction of reliable confidence intervals for covariate coefficients, supported by theoretical confidence levels under specified conditions, where asymptotic distributions are provided. Through various types of simulation studies, the proposed approach performs well for covariance regression in the presence of high-dimensional covariates. This innovative approach is applied to the Lifespan Human Connectome Project (HCP) Aging Study, which aims to uncover a typical aging trajectory and variations in the brain connectome among mature and older adults. The proposed approach effectively identifies brain networks and associated predictors of white matter integrity, aligning with established knowledge of the human brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06701v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuheng He, Changliang Zou, Yi Zhao</dc:creator>
    </item>
    <item>
      <title>A new way to evaluate G-Wishart normalising constants via Fourier analysis</title>
      <link>https://arxiv.org/abs/2404.06803</link>
      <description>arXiv:2404.06803v1 Announce Type: new 
Abstract: The G-Wishart distribution is an essential component for the Bayesian analysis of Gaussian graphical models as the conjugate prior for the precision matrix. Evaluating the marginal likelihood of such models usually requires computing high-dimensional integrals to determine the G-Wishart normalising constant. Closed-form results are known for decomposable or chordal graphs, while an explicit representation as a formal series expansion has been derived recently for general graphs. The nested infinite sums, however, do not lend themselves to computation, remaining of limited practical value. Borrowing techniques from random matrix theory and Fourier analysis, we provide novel exact results well suited to the numerical evaluation of the normalising constant for a large class of graphs beyond chordal graphs. Furthermore, they open new possibilities for developing more efficient sampling schemes for Bayesian inference of Gaussian graphical models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06803v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ching Wong, Giusi Moffa, Jack Kuipers</dc:creator>
    </item>
    <item>
      <title>Sensitivity analysis for publication bias in meta-analysis of sparse data based on exact likelihood</title>
      <link>https://arxiv.org/abs/2404.06837</link>
      <description>arXiv:2404.06837v1 Announce Type: new 
Abstract: Meta-analysis is a powerful tool to synthesize findings from multiple studies. The normal-normal random-effects model is widely used accounting for between-study heterogeneity. However, meta-analysis of sparse data, which may arise when the event rate is low for binary or count outcomes, poses a challenge to the normal-normal random-effects model in the accuracy and stability in inference since the normal approximation in the within-study likelihood may not be good. To reduce bias arising from data sparsity, the generalized linear mixed model can be used by replacing the approximate normal within-study likelihood with an exact likelihood. Publication bias is one of the most serious threats in meta-analysis. Several objective sensitivity analysis methods for evaluating potential impacts of selective publication are available for the normal-normal random-effects model. We propose a sensitivity analysis method by extending the likelihood-based sensitivity analysis with the $t$-statistic selection function of Copas to several generalized linear mixed-effects models. Through applications of our proposed method to several real-world meta-analyses and simulation studies, the proposed method was proven to outperform the likelihood-based sensitivity analysis based on the normal-normal model. The proposed method would give a useful guidance to address publication bias in meta-analysis of sparse data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06837v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taojun Hu, Yi Zhou, Sattoshi Hattori</dc:creator>
    </item>
    <item>
      <title>Multiple imputation for longitudinal data: A tutorial</title>
      <link>https://arxiv.org/abs/2404.06967</link>
      <description>arXiv:2404.06967v1 Announce Type: new 
Abstract: Longitudinal studies are frequently used in medical research and involve collecting repeated measures on individuals over time. Observations from the same individual are invariably correlated and thus an analytic approach that accounts for this clustering by individual is required. While almost all research suffers from missing data, this can be particularly problematic in longitudinal studies as participation often becomes harder to maintain over time. Multiple imputation (MI) is widely used to handle missing data in such studies. When using MI, it is important that the imputation model is compatible with the proposed analysis model. In a longitudinal analysis, this implies that the clustering considered in the analysis model should be reflected in the imputation process. Several MI approaches have been proposed to impute incomplete longitudinal data, such as treating repeated measurements of the same variable as distinct variables or using generalized linear mixed imputation models. However, the uptake of these methods has been limited, as they require additional data manipulation and use of advanced imputation procedures. In this tutorial, we review the available MI approaches that can be used for handling incomplete longitudinal data, including where individuals are clustered within higher-level clusters. We illustrate implementation with replicable R and Stata code using a case study from the Childhood to Adolescence Transition Study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06967v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Rushani Wijesuriya, Margarita Moreno-Betancur, John B Carlin, Ian R White, Matteo Quartagno, Katherine J Lee</dc:creator>
    </item>
    <item>
      <title>Adaptive Strategy of Testing Alphas in High Dimensional Linear Factor Pricing Models</title>
      <link>https://arxiv.org/abs/2404.06984</link>
      <description>arXiv:2404.06984v1 Announce Type: new 
Abstract: In recent years, there has been considerable research on testing alphas in high-dimensional linear factor pricing models. In our study, we introduce a novel max-type test procedure that performs well under sparse alternatives. Furthermore, we demonstrate that this new max-type test procedure is asymptotically independent from the sum-type test procedure proposed by Pesaran and Yamagata (2017). Building on this, we propose a Fisher combination test procedure that exhibits good performance for both dense and sparse alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06984v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenxi Zhao, Ping Zhao, Long Feng, Zhaojun Wang</dc:creator>
    </item>
    <item>
      <title>Model-free Change-point Detection Using Modern Classifiers</title>
      <link>https://arxiv.org/abs/2404.06995</link>
      <description>arXiv:2404.06995v1 Announce Type: new 
Abstract: In contemporary data analysis, it is increasingly common to work with non-stationary complex datasets. These datasets typically extend beyond the classical low-dimensional Euclidean space, making it challenging to detect shifts in their distribution without relying on strong structural assumptions. This paper introduces a novel offline change-point detection method that leverages modern classifiers developed in the machine-learning community. With suitable data splitting, the test statistic is constructed through sequential computation of the Area Under the Curve (AUC) of a classifier, which is trained on data segments on both ends of the sequence. It is shown that the resulting AUC process attains its maxima at the true change-point location, which facilitates the change-point estimation. The proposed method is characterized by its complete nonparametric nature, significant versatility, considerable flexibility, and absence of stringent assumptions pertaining to the underlying data or any distributional shifts. Theoretically, we derive the limiting pivotal distribution of the proposed test statistic under null, as well as the asymptotic behaviors under both local and fixed alternatives. The weak consistency of the change-point estimator is provided. Extensive simulation studies and the analysis of two real-world datasets illustrate the superior performance of our approach compared to existing model-free change-point detection methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06995v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rohit Kanrar, Feiyu Jiang, Zhanrui Cai</dc:creator>
    </item>
    <item>
      <title>To impute or not to? Testing multivariate normality on incomplete dataset: Revisiting the BHEP test</title>
      <link>https://arxiv.org/abs/2404.07136</link>
      <description>arXiv:2404.07136v1 Announce Type: new 
Abstract: In this paper, we focus on testing multivariate normality using the BHEP test with data that are missing completely at random. Our objective is twofold: first, to gain insight into the asymptotic behavior of BHEP test statistics under two widely used approaches for handling missing data, namely complete-case analysis and imputation, and second, to compare the power performance of test statistic under these approaches. It is observed that under the imputation approach, the affine invariance of test statistics is not preserved. To address this issue, we propose an appropriate bootstrap algorithm for approximating p-values. Extensive simulation studies demonstrate that both mean and median approaches exhibit greater power compared to testing with complete-case analysis, and open some questions for further research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07136v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Danijel Aleksi\'c, Bojana Milo\v{s}evi\'c</dc:creator>
    </item>
    <item>
      <title>High-dimensional copula-based Wasserstein dependence</title>
      <link>https://arxiv.org/abs/2404.07141</link>
      <description>arXiv:2404.07141v1 Announce Type: new 
Abstract: We generalize 2-Wasserstein dependence coefficients to measure dependence between a finite number of random vectors. This generalization includes theoretical properties, and in particular focuses on an interpretation of maximal dependence and an asymptotic normality result for a proposed semi-parametric estimator under a Gaussian copula assumption. In addition, we discuss general axioms for dependence measures between multiple random vectors, other plausible normalizations, and various examples. Afterwards, we look into plug-in estimators based on penalized empirical covariance matrices in order to deal with high dimensionality issues and take possible marginal independencies into account by inducing (block) sparsity. The latter ideas are investigated via a simulation study, considering other dependence coefficients as well. We illustrate the use of the developed methods in two real data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07141v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steven De Keyser, Irene Gijbels</dc:creator>
    </item>
    <item>
      <title>Causal Unit Selection using Tractable Arithmetic Circuits</title>
      <link>https://arxiv.org/abs/2404.06681</link>
      <description>arXiv:2404.06681v1 Announce Type: cross 
Abstract: The unit selection problem aims to find objects, called units, that optimize a causal objective function which describes the objects' behavior in a causal context (e.g., selecting customers who are about to churn but would most likely change their mind if encouraged). While early studies focused mainly on bounding a specific class of counterfactual objective functions using data, more recent work allows one to find optimal units exactly by reducing the causal objective to a classical objective on a meta-model, and then applying a variant of the classical Variable Elimination (VE) algorithm to the meta-model -- assuming a fully specified causal model is available. In practice, however, finding optimal units using this approach can be very expensive because the used VE algorithm must be exponential in the constrained treewidth of the meta-model, which is larger and denser than the original model. We address this computational challenge by introducing a new approach for unit selection that is not necessarily limited by the constrained treewidth. This is done through compiling the meta-model into a special class of tractable arithmetic circuits that allows the computation of optimal units in time linear in the circuit size. We finally present empirical results on random causal models that show order-of-magnitude speedups based on the proposed method for solving unit selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06681v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haiying Huang, Adnan Darwiche</dc:creator>
    </item>
    <item>
      <title>A Copula Graphical Model for Multi-Attribute Data using Optimal Transport</title>
      <link>https://arxiv.org/abs/2404.06735</link>
      <description>arXiv:2404.06735v1 Announce Type: cross 
Abstract: Motivated by modern data forms such as images and multi-view data, the multi-attribute graphical model aims to explore the conditional independence structure among vectors. Under the Gaussian assumption, the conditional independence between vectors is characterized by blockwise zeros in the precision matrix. To relax the restrictive Gaussian assumption, in this paper, we introduce a novel semiparametric multi-attribute graphical model based on a new copula named Cyclically Monotone Copula. This new copula treats the distribution of the node vectors as multivariate marginals and transforms them into Gaussian distributions based on the optimal transport theory. Since the model allows the node vectors to have arbitrary continuous distributions, it is more flexible than the classical Gaussian copula method that performs coordinatewise Gaussianization. We establish the concentration inequalities of the estimated covariance matrices and provide sufficient conditions for selection consistency of the group graphical lasso estimator. For the setting with high-dimensional attributes, a {Projected Cyclically Monotone Copula} model is proposed to address the curse of dimensionality issue that arises from solving high-dimensional optimal transport problems. Numerical results based on synthetic and real data show the efficiency and flexibility of our methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06735v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qi Zhang, Bing Li, Lingzhou Xue</dc:creator>
    </item>
    <item>
      <title>A New Statistic for Testing Covariance Equality in High-Dimensional Gaussian Low-Rank Models</title>
      <link>https://arxiv.org/abs/2404.07100</link>
      <description>arXiv:2404.07100v1 Announce Type: cross 
Abstract: In this paper, we consider the problem of testing equality of the covariance matrices of L complex Gaussian multivariate time series of dimension $M$ . We study the special case where each of the L covariance matrices is modeled as a rank K perturbation of the identity matrix, corresponding to a signal plus noise model. A new test statistic based on the estimates of the eigenvalues of the different covariance matrices is proposed. In particular, we show that this statistic is consistent and with controlled type I error in the high-dimensional asymptotic regime where the sample sizes $N_1,\ldots,N_L$ of each time series and the dimension $M$ both converge to infinity at the same rate, while $K$ and $L$ are kept fixed. We also provide some simulations on synthetic and real data (SAR images) which demonstrate significant improvements over some classical methods such as the GLRT, or other alternative methods relevant for the high-dimensional regime and the low-rank model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07100v1</guid>
      <category>math.ST</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TSP.2024.3382476</arxiv:DOI>
      <dc:creator>R\'emi Beisson, Pascal Vallet, Audrey Giremus, Guillaume Ginolhac</dc:creator>
    </item>
    <item>
      <title>Active Learning for a Recursive Non-Additive Emulator for Multi-Fidelity Computer Experiments</title>
      <link>https://arxiv.org/abs/2309.11772</link>
      <description>arXiv:2309.11772v2 Announce Type: replace 
Abstract: Computer simulations have become essential for analyzing complex systems, but high-fidelity simulations often come with significant computational costs. To tackle this challenge, multi-fidelity computer experiments have emerged as a promising approach that leverages both low-fidelity and high-fidelity simulations, enhancing both the accuracy and efficiency of the analysis. In this paper, we introduce a new and flexible statistical model, the Recursive Non-Additive (RNA) emulator, that integrates the data from multi-fidelity computer experiments. Unlike conventional multi-fidelity emulation approaches that rely on an additive auto-regressive structure, the proposed RNA emulator recursively captures the relationships between multi-fidelity data using Gaussian process priors without making the additive assumption, allowing the model to accommodate more complex data patterns. Importantly, we derive the posterior predictive mean and variance of the emulator, which can be efficiently computed in a closed-form manner, leading to significant improvements in computational efficiency. Additionally, based on this emulator, we introduce three active learning strategies that optimize the balance between accuracy and simulation costs to guide the selection of the fidelity level and input locations for the next simulation run. We demonstrate the effectiveness of the proposed approach in a suite of synthetic examples and a real-world problem. An R package RNAmf for the proposed methodology is provided on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.11772v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junoh Heo, Chih-Li Sung</dc:creator>
    </item>
    <item>
      <title>Decomposition with Monotone B-splines: Fitting and Testing</title>
      <link>https://arxiv.org/abs/2401.06383</link>
      <description>arXiv:2401.06383v2 Announce Type: replace 
Abstract: A univariate continuous function can always be decomposed as the sum of a non-increasing function and a non-decreasing one. Based on this property, we propose a non-parametric regression method that combines two spline-fitted monotone curves. We demonstrate by extensive simulations that, compared to standard spline-fitting methods, the proposed approach is particularly advantageous in high-noise scenarios. Several theoretical guarantees are established for the proposed approach. Additionally, we present statistics to test the monotonicity of a function based on monotone decomposition, which can better control Type I error and achieve comparable (if not always higher) power compared to existing methods. Finally, we apply the proposed fitting and testing approaches to analyze the single-cell pseudotime trajectory datasets, identifying significant biological insights for non-monotonically expressed genes through Gene Ontology enrichment analysis. The source code implementing the methodology and producing all results is accessible at https://github.com/szcf-weiya/MonotoneDecomposition.jl.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06383v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lijun Wang, Xiaodan Fan, Hongyu Zhao, Jun S. Liu</dc:creator>
    </item>
    <item>
      <title>Statistics in Survey Sampling</title>
      <link>https://arxiv.org/abs/2401.07625</link>
      <description>arXiv:2401.07625v2 Announce Type: replace 
Abstract: Survey sampling theory and methods are introduced. Sampling designs and estimation methods are carefully discussed as a textbook for survey sampling. Topics includes Horvitz-Thompson estimation, simple random sampling, stratified sampling, cluster sampling, ratio estimation, regression estimation, variance estimation, two-phase sampling, and nonresponse adjustment methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07625v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jae Kwang Kim</dc:creator>
    </item>
    <item>
      <title>A Bayesian Approach for Selecting Relevant External Data (BASE): Application to a study of Long-Term Outcomes in a Hemophilia Gene Therapy Trial</title>
      <link>https://arxiv.org/abs/2403.13260</link>
      <description>arXiv:2403.13260v2 Announce Type: replace 
Abstract: Gene therapies aim to address the root causes of diseases, particularly those stemming from rare genetic defects that can be life-threatening or severely debilitating. While there has been notable progress in the development of gene therapies in recent years, understanding their long-term effectiveness remains challenging due to a lack of data on long-term outcomes, especially during the early stages of their introduction to the market. To address the critical question of estimating long-term efficacy without waiting for the completion of lengthy clinical trials, we propose a novel Bayesian framework. This framework selects pertinent data from external sources, often early-phase clinical trials with more comprehensive longitudinal efficacy data that could lead to an improved inference of the long-term efficacy outcome. We apply this methodology to predict the long-term factor IX (FIX) levels of HEMGENIX (etranacogene dezaparvovec), the first FDA-approved gene therapy to treat adults with severe Hemophilia B, in a phase 3 study. Our application showcases the capability of the framework to estimate the 5-year FIX levels following HEMGENIX therapy, demonstrating sustained FIX levels induced by HEMGENIX infusion. Additionally, we provide theoretical insights into the methodology by establishing its posterior convergence properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13260v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyu Pan, Xiang Zhang, Weining Shen, Ting Ye</dc:creator>
    </item>
    <item>
      <title>A communication-efficient, online changepoint detection method for monitoring distributed sensor networks</title>
      <link>https://arxiv.org/abs/2403.18549</link>
      <description>arXiv:2403.18549v2 Announce Type: replace 
Abstract: We consider the challenge of efficiently detecting changes within a network of sensors, where we also need to minimise communication between sensors and the cloud. We propose an online, communication-efficient method to detect such changes. The procedure works by performing likelihood ratio tests at each time point, and two thresholds are chosen to filter unimportant test statistics and make decisions based on the aggregated test statistics respectively. We provide asymptotic theory concerning consistency and the asymptotic distribution if there are no changes. Simulation results suggest that our method can achieve similar performance to the idealised setting, where we have no constraints on communication between sensors, but substantially reduce the transmission costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18549v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyang Yang, Idris A. Eckley, Paul Fearnhead</dc:creator>
    </item>
    <item>
      <title>Estimation of Optimal Dynamic Treatment Assignment Rules under Policy Constraints</title>
      <link>https://arxiv.org/abs/2106.05031</link>
      <description>arXiv:2106.05031v4 Announce Type: replace-cross 
Abstract: This paper studies statistical decisions for dynamic treatment assignment problems. Many policies involve dynamics in their treatment assignments where treatments are sequentially assigned to individuals across multiple stages and the effect of treatment at each stage is usually heterogeneous with respect to the prior treatments, past outcomes, and observed covariates. We consider estimating an optimal dynamic treatment rule that guides the optimal treatment assignment for each individual at each stage based on the individual's history. This paper proposes an empirical welfare maximization approach in a dynamic framework. The approach estimates the optimal dynamic treatment rule using data from an experimental or quasi-experimental study. The paper proposes two estimation methods: one solves the treatment assignment problem at each stage through backward induction, and the other solves the whole dynamic treatment assignment problem simultaneously across all stages. We derive finite-sample upper bounds on worst-case average welfare regrets for the proposed methods and show $1/\sqrt{n}$-minimax convergence rates. We also modify the simultaneous estimation method to incorporate intertemporal budget/capacity constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.05031v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shosei Sakaguchi</dc:creator>
    </item>
    <item>
      <title>Trade-off between predictive performance and FDR control for high-dimensional Gaussian model selection</title>
      <link>https://arxiv.org/abs/2302.01831</link>
      <description>arXiv:2302.01831v3 Announce Type: replace-cross 
Abstract: In the context of the high-dimensional Gaussian linear regression for ordered variables, we study the variable selection procedure via the minimization of the penalized least-squares criterion. We focus on model selection where the penalty function depends on an unknown multiplicative constant commonly calibrated for prediction. We propose a new proper calibration of this hyperparameter to simultaneously control predictive risk and false discovery rate. We obtain non-asymptotic bounds on the False Discovery Rate with respect to the hyperparameter and we provide an algorithm to calibrate it. This algorithm is based on quantities that can typically be observed in real data applications. The algorithm is validated in an extensive simulation study and is compared with some existing variable selection procedures. Finally, we study an extension of our approach to the case in which an ordering of the variables is not available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.01831v3</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Perrine Lacroix, Marie-Laure Martin</dc:creator>
    </item>
    <item>
      <title>Local Causal Discovery for Estimating Causal Effects</title>
      <link>https://arxiv.org/abs/2302.08070</link>
      <description>arXiv:2302.08070v4 Announce Type: replace-cross 
Abstract: Even when the causal graph underlying our data is unknown, we can use observational data to narrow down the possible values that an average treatment effect (ATE) can take by (1) identifying the graph up to a Markov equivalence class; and (2) estimating that ATE for each graph in the class. While the PC algorithm can identify this class under strong faithfulness assumptions, it can be computationally prohibitive. Fortunately, only the local graph structure around the treatment is required to identify the set of possible ATE values, a fact exploited by local discovery algorithms to improve computational efficiency. In this paper, we introduce Local Discovery using Eager Collider Checks (LDECC), a new local causal discovery algorithm that leverages unshielded colliders to orient the treatment's parents differently from existing methods. We show that there exist graphs where LDECC exponentially outperforms existing local discovery algorithms and vice versa. Moreover, we show that LDECC and existing algorithms rely on different faithfulness assumptions, leveraging this insight to weaken the assumptions for identifying the set of possible ATE values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.08070v4</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shantanu Gupta, David Childers, Zachary C. Lipton</dc:creator>
    </item>
    <item>
      <title>Root-n consistent semiparametric learning with high-dimensional nuisance functions under minimal sparsity</title>
      <link>https://arxiv.org/abs/2305.04174</link>
      <description>arXiv:2305.04174v2 Announce Type: replace-cross 
Abstract: Treatment effect estimation under unconfoundedness is a fundamental task in causal inference. In response to the challenge of analyzing high-dimensional datasets collected in substantive fields such as epidemiology, genetics, economics, and social sciences, many methods for treatment effect estimation with high-dimensional nuisance parameters (the outcome regression and the propensity score) have been developed in recent years. However, it is still unclear what is the necessary and sufficient sparsity condition on the nuisance parameters for the treatment effect to be $\sqrt{n}$-estimable. In this paper, we propose a new Double-Calibration strategy that corrects the estimation bias of the nuisance parameter estimates computed by regularized high-dimensional techniques and demonstrate that the corresponding Doubly-Calibrated estimator achieves $1 / \sqrt{n}$-rate as long as one of the nuisance parameters is sparse with sparsity below $\sqrt{n} / \log p$, where $p$ denotes the ambient dimension of the covariates, whereas the other nuisance parameter can be arbitrarily complex and completely misspecified. The Double-Calibration strategy can also be applied to settings other than treatment effect estimation, e.g. regression coefficient estimation in the presence of diverging number of controls in a semiparametric partially linear model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.04174v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Liu, Yuhao Wang</dc:creator>
    </item>
    <item>
      <title>multilevLCA: An R Package for Single-Level and Multilevel Latent Class Analysis with Covariates</title>
      <link>https://arxiv.org/abs/2305.07276</link>
      <description>arXiv:2305.07276v2 Announce Type: replace-cross 
Abstract: This contribution presents a guide to the R package multilevLCA, which offers a complete and innovative set of technical tools for the latent class analysis of single-level and multilevel categorical data. We describe the available model specifications, mainly falling within the fixed-effect or random-effect approaches. Maximum likelihood estimation of the model parameters, enhanced by a refined initialization strategy, is implemented either simultaneously, i.e., in one-step, or by means of the more advantageous two-step estimator. The package features i) semi-automatic model selection when a priori information on the number of classes is lacking, ii) predictors of class membership, and iii) output visualization tools for any of the available model specifications. All functionalities are illustrated by means of a real application on citizenship norms data, which are available in the package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.07276v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johan Lyrvall, Roberto Di Mari, Zsuzsa Bakk, Jennifer Oser, Jouni Kuha</dc:creator>
    </item>
    <item>
      <title>A Federated Data Fusion-Based Prognostic Model for Applications with Multi-Stream Incomplete Signals</title>
      <link>https://arxiv.org/abs/2311.07474</link>
      <description>arXiv:2311.07474v2 Announce Type: replace-cross 
Abstract: Most prognostic methods require a decent amount of data for model training. In reality, however, the amount of historical data owned by a single organization might be small or not large enough to train a reliable prognostic model. To address this challenge, this article proposes a federated prognostic model that allows multiple users to jointly construct a failure time prediction model using their multi-stream, high-dimensional, and incomplete data while keeping each user's data local and confidential. The prognostic model first employs multivariate functional principal component analysis to fuse the multi-stream degradation signals. Then, the fused features coupled with the times-to-failure are utilized to build a (log)-location-scale regression model for failure prediction. To estimate parameters using distributed datasets and keep the data privacy of all participants, we propose a new federated algorithm for feature extraction. Numerical studies indicate that the performance of the proposed model is the same as that of classic non-federated prognostic models and is better than that of the models constructed by each user itself.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.07474v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Madi Arabi, Xiaolei Fang</dc:creator>
    </item>
    <item>
      <title>A compromise criterion for weighted least squares estimates</title>
      <link>https://arxiv.org/abs/2404.00753</link>
      <description>arXiv:2404.00753v2 Announce Type: replace-cross 
Abstract: When independent errors in a linear model have non-identity covariance, the ordinary least squares estimate of the model coefficients is less efficient than the weighted least squares estimate. However, the practical application of weighted least squares is challenging due to its reliance on the unknown error covariance matrix. Although feasible weighted least squares estimates, which use an approximation of this matrix, often outperform the ordinary least squares estimate in terms of efficiency, this is not always the case. In some situations, feasible weighted least squares can be less efficient than ordinary least squares. The comparison between these two estimates has significant implications for the application of regression analysis in varied fields, yet such a comparison remains an unresolved challenge despite its seemingly straightforward nature. In this study, we directly address this challenge by identifying the conditions under which feasible weighted least squares estimates using fixed weights demonstrate greater efficiency than the ordinary least squares estimate. These conditions provide guidance for the design of feasible estimates using random weights. They also shed light on how certain robust regression estimates behave with respect to the linear model with normal errors of unequal variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00753v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jordan Bryan, Haibo Zhou, Didong Li</dc:creator>
    </item>
    <item>
      <title>Least Squares-Based Permutation Tests in Time Series</title>
      <link>https://arxiv.org/abs/2404.06238</link>
      <description>arXiv:2404.06238v2 Announce Type: replace-cross 
Abstract: This paper studies permutation tests for regression parameters in a time series setting, where the time series is assumed stationary but may exhibit an arbitrary (but weak) dependence structure. In such a setting, it is perhaps surprising that permutation tests can offer any type of inference guarantees, since permuting of covariates can destroy its relationship with the response. Indeed, the fundamental assumption of exchangeability of errors required for the finite-sample exactness of permutation tests, can easily fail. However, we show that permutation tests may be constructed which are asymptotically valid for a wide class of stationary processes, but remain exact when exchangeability holds. We also consider the problem of testing for no monotone trend and we construct asymptotically valid permutation tests in this setting as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06238v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph P. Romano, Marius A. Tirlea</dc:creator>
    </item>
    <item>
      <title>Permutation Testing for Monotone Trend</title>
      <link>https://arxiv.org/abs/2404.06239</link>
      <description>arXiv:2404.06239v2 Announce Type: replace-cross 
Abstract: In this paper, we consider the fundamental problem of testing for monotone trend in a time series. While the term "trend" is commonly used and has an intuitive meaning, it is first crucial to specify its exact meaning in a hypothesis testing context. A commonly used well-known test is the Mann-Kendall test, which we show does not offer Type 1 error control even in large samples. On the other hand, by an appropriate studentization of the Mann-Kendall statistic, we construct permutation tests that offer asymptotic error control quite generally, but retain the exactness property of permutation tests for i.i.d. observations. We also introduce "local" Mann-Kendall statistics as a means of testing for local rather than global trend in a time series. Similar properties of permutation tests are obtained for these tests as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06239v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph P. Romano, Marius A. Tirlea</dc:creator>
    </item>
  </channel>
</rss>
