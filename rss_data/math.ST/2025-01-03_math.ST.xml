<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 03 Jan 2025 05:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Analyzing Scalogram Ridges in the Presence of Noise</title>
      <link>https://arxiv.org/abs/2501.00270</link>
      <description>arXiv:2501.00270v1 Announce Type: new 
Abstract: While ridges in the scalogram, determined by the squared modulus of analytic wavelet transform (AWT), have been widely applied in time series analysis and time-frequency analysis, their behavior in noisy environments remains underexplored. We fill in this gap in this paper. We define ridges as paths formed by local maximizers of the scalogram along the scale axis, and analyze their properties when the signal is contaminated by stationary Gaussian noise. In addition to establishing several key properties of the AWT for random processes, we investigate the probabilistic characteristics of the resulting random ridge points in the scalogram. Specifically, we establish the uniqueness property of the ridge point at individual time instances and prove the upper hemicontinuity of the set of ridge points. Furthermore, we derive bounds on the probability that the deviation between the ridge points of noisy and clean signals exceeds a specified threshold. These bounds are expressed in terms of constants related to the signal-to-noise ratio and apply when the signal satisfies the adaptive harmonic model and is corrupted by a stationary Gaussian process. To achieve these results, we derive maximal inequalities for the complex modulus of nonstationary Gaussian processes, leveraging classical tools such as the Borell-TIS inequality and Dudley's theorem, which might be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00270v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gi-Ren Liu, Yuan-Chung Sheu, Hau-Tieng Wu</dc:creator>
    </item>
    <item>
      <title>Tensor Topic Modeling Via HOSVD</title>
      <link>https://arxiv.org/abs/2501.00535</link>
      <description>arXiv:2501.00535v1 Announce Type: new 
Abstract: By representing documents as mixtures of topics, topic modeling has allowed the successful analysis of datasets across a wide spectrum of applications ranging from ecology to genetics. An important body of recent work has demonstrated the computational and statistical efficiency of probabilistic Latent Semantic Indexing (pLSI)-- a type of topic modeling -- in estimating both the topic matrix (corresponding to distributions over word frequencies), and the topic assignment matrix. However, these methods are not easily extendable to the incorporation of additional temporal, spatial, or document-specific information, thereby potentially neglecting useful information in the analysis of spatial or longitudinal datasets that can be represented as tensors. Consequently, in this paper, we propose using a modified higher-order singular value decomposition (HOSVD) to estimate topic models based on a Tucker decomposition, thus accommodating the complexity of tensor data. Our method exploits the strength of tensor decomposition in reducing data to lower-dimensional spaces and successfully recovers lower-rank topic and cluster structures, as well as a core tensor that highlights interactions among latent factors. We further characterize explicitly the convergence rate of our method in entry-wise $\ell_1$ norm. Experiments on synthetic data demonstrate the statistical efficiency of our method and its ability to better capture patterns across multiple dimensions. Additionally, our approach also performs well when applied to large datasets of research abstracts and in the analysis of vaginal microbiome data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00535v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yating Liu, Claire Donnat</dc:creator>
    </item>
    <item>
      <title>Theory and Applications of Kernel Stein Discrepancy on Riemannian Manifolds</title>
      <link>https://arxiv.org/abs/2501.00695</link>
      <description>arXiv:2501.00695v1 Announce Type: new 
Abstract: Distributional comparison is a fundamental problem in statistical data analysis with numerous applications in a variety of fields including but not limited to Sciences and Engineering. Numerous methods exist for distributional comparison but Kernel Stein Discrepancy (KSD) has gained significant popularity in recent times. In this paper, we present a novel mathematically rigorous and consistent generalization of KSD to Riemannian manifolds. We first generalize the Stein's operator to Riemannian manifolds and use it to establish the Stein's Lemma on Riemannian manifolds. Then we define a novel Stein class and use it to develop what we call qualified kernels that are used to define the KSD in closed form on Rimeannian manifolds. We present several examples of our theory applied to commonly encountered Riemannian manifolds in applications namely, the Riemannian homogeneous spaces, for example, the n-sphere, the Grassmanian, Stiefel, the manifold of symmetric positive definite matrices and others. On these aforementioned manifolds, we consider a variety of distributions with intractable normalization constants and derive closed form expressions for the KSD and the minimum KSD estimator (mKSDE). Several theoretical properties of mKSDE are established and we present results of comparison between mKSDE and MLE on a widely popular example, the sphere.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00695v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoda Qu, Baba C. Vemuri</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Markov-switching Ordinary Differential Processes</title>
      <link>https://arxiv.org/abs/2501.00087</link>
      <description>arXiv:2501.00087v1 Announce Type: cross 
Abstract: We investigate the parameter recovery of Markov-switching ordinary differential processes from discrete observations, where the differential equations are nonlinear additive models. This framework has been widely applied in biological systems, control systems, and other domains; however, limited research has been conducted on reconstructing the generating processes from observations. In contrast, many physical systems, such as human brains, cannot be directly experimented upon and rely on observations to infer the underlying systems. To address this gap, this manuscript presents a comprehensive study of the model, encompassing algorithm design, optimization guarantees, and quantification of statistical errors. Specifically, we develop a two-stage algorithm that first recovers the continuous sample path from discrete samples and then estimates the parameters of the processes. We provide novel theoretical insights into the statistical error and linear convergence guarantee when the processes are $\beta$-mixing. Our analysis is based on the truncation of the latent posterior processes and demonstrates that the truncated processes approximate the true processes under mixing conditions. We apply this model to investigate the differences in resting-state brain networks between the ADHD group and normal controls, revealing differences in the transition rate matrices of the two groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00087v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katherine Tsai, Mladen Kolar, Sanmi Koyejo</dc:creator>
    </item>
    <item>
      <title>Polynomial time sampling from log-smooth distributions in fixed dimension under semi-log-concavity of the forward diffusion with application to strongly dissipative distributions</title>
      <link>https://arxiv.org/abs/2501.00565</link>
      <description>arXiv:2501.00565v1 Announce Type: cross 
Abstract: In this article we provide a stochastic sampling algorithm with polynomial complexity in fixed dimension that leverages the recent advances on diffusion models where it is shown that under mild conditions, sampling can be achieved via an accurate estimation of intermediate scores across the marginals $(p_t)_{t\ge 0}$ of the standard Ornstein-Uhlenbeck process started at the density we wish to sample from. The heart of our method consists into approaching these scores via a computationally cheap estimator and relating the variance of this estimator to the smoothness properties of the forward process. Under the assumption that the density to sample from is $L$-log-smooth and that the forward process is semi-log-concave: $-\nabla^2 \log(p_t) \succeq -\beta I_d$ for some $\beta \geq 0$, we prove that our algorithm achieves an expected $\epsilon$ error in $\text{KL}$ divergence in $O(d^7L^{d+2}\epsilon^{-2(d+3)} (L+\beta)^2d^{2(d+1)})$ time. In particular, our result allows to fully transfer the problem of sampling from a log-smooth distribution into a regularity estimate problem. As an application, we derive an exponential complexity improvement for the problem of sampling from a $L$-log-smooth distribution that is $\alpha$-strongly log-concave distribution outside some ball of radius $R$: after proving that such distributions verify the semi-log-concavity assumption, a result which might be of independent interest, we recover a $poly(R,L,\alpha^{-1}, \epsilon^{-1})$ complexity in fixed dimension which exponentially improves upon the previously known $poly(e^{RL^2}, L,\alpha^{-1}, \log(\epsilon^{-1}))$ complexity in the low precision regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00565v1</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Adrien Vacher, Omar Chehab, Anna Korba</dc:creator>
    </item>
    <item>
      <title>Compositional Covariate Importance Testing via Partial Conjunction of Bivariate Hypotheses</title>
      <link>https://arxiv.org/abs/2501.00566</link>
      <description>arXiv:2501.00566v1 Announce Type: cross 
Abstract: Compositional data (i.e., data comprising random variables that sum up to a constant) arises in many applications including microbiome studies, chemical ecology, political science, and experimental designs. Yet when compositional data serve as covariates in a regression, the sum constraint renders every covariate automatically conditionally independent of the response given the other covariates, since each covariate is a deterministic function of the others. Since essentially all covariate importance tests and variable selection methods, including parametric ones, are at their core testing conditional independence, they are all completely powerless on regression problems with compositional covariates. In fact, compositionality causes ambiguity in the very notion of relevant covariates. To address this problem, we identify a natural way to translate the typical notion of relevant covariates to the setting with compositional covariates and establish that it is intuitive, well-defined, and unique. We then develop corresponding hypothesis tests and controlled variable selection procedures via a novel connection with \emph{bivariate} conditional independence testing and partial conjunction hypothesis testing. Finally, we provide theoretical guarantees of the validity of our methods, and through numerical experiments demonstrate that our methods are not only valid but also powerful across a range of data-generating scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00566v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ritwik Bhaduri, Siyuan Ma, Lucas Janson</dc:creator>
    </item>
    <item>
      <title>Fiducial inference for partially identified parameters with applications to instrumental variable models</title>
      <link>https://arxiv.org/abs/2501.00837</link>
      <description>arXiv:2501.00837v1 Announce Type: cross 
Abstract: In the past two decades, there has been a fast-growing literature on fiducial inference since it was first proposed by R. A. Fisher in the 1930s. However, most of the fiducial inference based methods and related approaches have been developed for point-identified models, i.e., statistical models where the parameters of interest are uniquely determined by the observed data and the model's assumptions. In this paper, we propose a novel fiducial approach for partially identified statistical models. As a leading example, we consider the instrumental variable model with a variety of causal assumptions and estimands. The proposed methods are illustrated through extensive simulations and a data analysis evaluating the effect of consuming Vitamin A supplementation on reducing mortality rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00837v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Cui, Jan Hannig</dc:creator>
    </item>
    <item>
      <title>A stronger Sylvester's criterion for positive semidefinite matrices</title>
      <link>https://arxiv.org/abs/2501.00894</link>
      <description>arXiv:2501.00894v1 Announce Type: cross 
Abstract: Sylvester's criterion characterizes positive definite (PD) and positive semidefinite (PSD) matrices without the need of eigendecomposition. It states that a symmetric matrix is PD if and only if all of its leading principal minors are positive, and a symmetric matrix is PSD if and only if all of its principal minors are nonnegative. For an $m\times m$ symmetric matrix, Sylvester's criterion requires computing $m$ and $2^m-1$ determinants to verify it is PD and PSD, respectively. Therefore, it is less useful for PSD matrices due to the exponential growth in the number of principal submatrices as the matrix dimension increases. We provide a stronger Sylvester's criterion for PSD matrices which only requires to verify the nonnegativity of $m(m+1)/2$ determinants. Based on the new criterion, we provide a method to derive elementwise criteria for PD and PSD matrices. We illustrate the applications of our results in PD or PSD matrix completion and highlight their statistics applications via nonlinear semidefinite program.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00894v1</guid>
      <category>math.RA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingrui Zhang, Peng Ding</dc:creator>
    </item>
    <item>
      <title>A Heavily Right Strategy for Integrating Dependent Studies in Any Dimension</title>
      <link>https://arxiv.org/abs/2501.01065</link>
      <description>arXiv:2501.01065v1 Announce Type: cross 
Abstract: Recently, there has been a surge of interest in hypothesis testing methods for combining dependent studies without explicitly assessing their dependence. Among these, the Cauchy combination test (CCT) stands out for its approximate validity and power, leveraging a heavy-tail approximation insensitive to dependence. However, CCT is highly sensitive to large $p$-values and inverting it to construct confidence regions can result in regions lacking compactness, convexity, or connectivity. This article proposes a "heavily right" strategy by excluding the left half of the Cauchy distribution in the combination rule, retaining CCT's resilience to dependence while resolving its sensitivity to large $p$-values. Moreover, the Half-Cauchy combination as well as the harmonic mean approach guarantees bounded and convex confidence regions, distinguishing them as the only known combination tests with all such desirable properties. Efficient and accurate algorithms are introduced for implementing both methods. Additionally, we develop a divide-and-combine strategy for constructing confidence regions for high-dimensional mean estimation using the Half-Cauchy method, and empirically illustrate its advantages over the Hotelling $T^2$ approach. To demonstrate the practical utility of our Half-Cauchy approach, we apply it to network meta-analysis, constructing simultaneous confidence intervals for treatment effect comparisons across multiple clinical trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01065v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianle Liu, Xiao-Li Meng, Natesh S. Pillai</dc:creator>
    </item>
    <item>
      <title>Learning Spectral Methods by Transformers</title>
      <link>https://arxiv.org/abs/2501.01312</link>
      <description>arXiv:2501.01312v1 Announce Type: cross 
Abstract: Transformers demonstrate significant advantages as the building block of modern LLMs. In this work, we study the capacities of Transformers in performing unsupervised learning. We show that multi-layered Transformers, given a sufficiently large set of pre-training instances, are able to learn the algorithms themselves and perform statistical estimation tasks given new instances. This learning paradigm is distinct from the in-context learning setup and is similar to the learning procedure of human brains where skills are learned through past experience. Theoretically, we prove that pre-trained Transformers can learn the spectral methods and use the classification of bi-class Gaussian mixture model as an example. Our proof is constructive using algorithmic design techniques. Our results are built upon the similarities of multi-layered Transformer architecture with the iterative recovery algorithms used in practice. Empirically, we verify the strong capacity of the multi-layered (pre-trained) Transformer on unsupervised learning through the lens of both the PCA and the Clustering tasks performed on the synthetic and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01312v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yihan He, Yuan Cao, Hong-Yu Chen, Dennis Wu, Jianqing Fan, Han Liu</dc:creator>
    </item>
    <item>
      <title>Fast data inversion for high-dimensional dynamical systems from noisy measurements</title>
      <link>https://arxiv.org/abs/2501.01324</link>
      <description>arXiv:2501.01324v1 Announce Type: cross 
Abstract: In this work, we develop a scalable approach for a flexible latent factor model for high-dimensional dynamical systems. Each latent factor process has its own correlation and variance parameters, and the orthogonal factor loading matrix can be either fixed or estimated. We utilize an orthogonal factor loading matrix that avoids computing the inversion of the posterior covariance matrix at each time of the Kalman filter, and derive closed-form expressions in an expectation-maximization algorithm for parameter estimation, which substantially reduces the computational complexity without approximation. Our study is motivated by inversely estimating slow slip events from geodetic data, such as continuous GPS measurements. Extensive simulated studies illustrate higher accuracy and scalability of our approach compared to alternatives. By applying our method to geodetic measurements in the Cascadia region, our estimated slip better agrees with independently measured seismic data of tremor events. The substantial acceleration from our method enables the use of massive noisy data for geological hazard quantification and other applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01324v1</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yizi Lin, Xubo Liu, Paul Segall, Mengyang Gu</dc:creator>
    </item>
    <item>
      <title>Derivatives and residual distribution of regularized M-estimators with application to adaptive tuning</title>
      <link>https://arxiv.org/abs/2107.05143</link>
      <description>arXiv:2107.05143v2 Announce Type: replace 
Abstract: This paper studies M-estimators with gradient-Lipschitz loss function regularized with convex penalty in linear models with Gaussian design matrix and arbitrary noise distribution. A practical example is the robust M-estimator constructed with the Huber loss and the Elastic-Net penalty and the noise distribution has heavy-tails. Our main contributions are three-fold. (i) We provide general formulae for the derivatives of regularized M-estimators $\hat\beta(y,X)$ where differentiation is taken with respect to both $y$ and $X$; this reveals a simple differentiability structure shared by all convex regularized M-estimators. (ii) Using these derivatives, we characterize the distribution of the residual $r_i = y_i-x_i^\top\hat\beta$ in the intermediate high-dimensional regime where dimension and sample size are of the same order. (iii) Motivated by the distribution of the residuals, we propose a novel adaptive criterion to select tuning parameters of regularized M-estimators. The criterion approximates the out-of-sample error up to an additive constant independent of the estimator, so that minimizing the criterion provides a proxy for minimizing the out-of-sample error. The proposed adaptive criterion does not require the knowledge of the noise distribution or of the covariance of the design. Simulated data confirms the theoretical findings, regarding both the distribution of the residuals and the success of the criterion as a proxy of the out-of-sample error. Finally our results reveal new relationships between the derivatives of $\hat\beta(y,X)$ and the effective degrees of freedom of the M-estimator, which are of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.05143v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre C Bellec, Yiwei Shen</dc:creator>
    </item>
    <item>
      <title>Unveiling low-dimensional patterns induced by convex non-differentiable regularizers</title>
      <link>https://arxiv.org/abs/2405.07677</link>
      <description>arXiv:2405.07677v2 Announce Type: replace 
Abstract: Popular regularizers with non-differentiable penalties, such as Lasso, Elastic Net, Generalized Lasso, or SLOPE, reduce the dimension of the parameter space by inducing sparsity or clustering in the estimators' coordinates. In this paper, we focus on linear regression and explore the asymptotic distributions of the resulting low-dimensional patterns when the number of regressors $p$ is fixed, the number of observations $n$ goes to infinity, and the penalty function increases at the rate of $\sqrt{n}$. While the asymptotic distribution of the rescaled estimation error can be derived by relatively standard arguments, convergence of patterns requires a separate proof, which is yet missing from the literature, even for the simplest case of Lasso. To fill this gap, we use the Hausdorff distance as a suitable mode of convergence for subdifferentials, resulting in the desired pattern convergence. Furthermore, we derive the exact limiting probability of recovering the true model pattern. This probability goes to 1 if and only if the penalty scaling constant diverges to infinity and the regularizer-specific asymptotic irrepresentability condition is satisfied. We then propose simple two-step procedures that asymptotically recover the model patterns, irrespective of whether the irrepresentability condition holds or not.
  Interestingly, our theory shows that Fused Lasso cannot reliably recover its own clustering pattern, even for independent regressors. It also demonstrates how this problem can be resolved by "concavifying" the Fused Lasso penalty coefficients. Additionally, sampling from the asymptotic error distribution facilitates comparisons between different regularizers. We provide short simulation studies showcasing an illustrative comparison between the asymptotic properties of Lasso, Fused Lasso, and SLOPE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07677v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ivan Hejn\'y, Jonas Wallin, Ma{\l}gorzata Bogdan, Micha{\l} Kos</dc:creator>
    </item>
    <item>
      <title>On the Frequentist Coverage of Bayes Posteriors in Nonlinear Inverse Problems</title>
      <link>https://arxiv.org/abs/2407.13970</link>
      <description>arXiv:2407.13970v2 Announce Type: replace 
Abstract: We study asymptotic frequentist coverage and approximately Gaussian properties of Bayes posterior credible sets in nonlinear inverse problems when a Gaussian prior is placed on the parameter of the PDE. The aim is to ensure valid frequentist coverage of Bayes credible intervals when estimating continuous linear functionals of the parameter. Our results show that Bayes credible intervals have conservative coverage under certain smoothness assumptions on the parameter and a compatibility condition between the likelihood and the prior, regardless of whether an efficient limit exists and/or Bernstein von-Mises theorem holds. In the latter case, our results yield a corollary with more relaxed sufficient conditions than previous works. We illustrate practical utility of the results through the example of estimating the conductivity coefficient of a second order elliptic PDE, where a near-$N^{-1/2}$ contraction rate and conservative coverage results are obtained for linear functionals that were shown not to be estimable efficiently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13970v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youngsoo Baek, Katerina Papagiannouli, Sayan Mukherjee</dc:creator>
    </item>
    <item>
      <title>Joint Probability Estimation of Many Binary Outcomes via Localized Adversarial Lasso</title>
      <link>https://arxiv.org/abs/2410.15166</link>
      <description>arXiv:2410.15166v3 Announce Type: replace 
Abstract: In this work we consider estimating the probability of many (possibly dependent) binary outcomes which is at the core of many applications, e.g., multi-level treatments in causal inference, demands for bundle of products, etc. Without further conditions, the probability distribution of an M dimensional binary vector is characterized by exponentially in M coefficients which can lead to a high-dimensional problem even without the presence of covariates. Understanding the (in)dependence structure allows us to substantially improve the estimation as it allows for an effective factorization of the probability distribution. In order to estimate the probability distribution of a M dimensional binary vector, we leverage a Bahadur representation that connects the sparsity of its coefficients with independence across the components. We propose to use regularized and adversarial regularized estimators to obtain an adaptive estimator with respect to the dependence structure which allows for rates of convergence to depend on this intrinsic (lower) dimension. These estimators are needed to handle several challenges within this setting, including estimating nuisance parameters, estimating covariates, and nonseparable moment conditions. Our main results consider the presence of (low dimensional) covariates for which we propose a locally penalized estimator. We provide pointwise rates of convergence addressing several issues in the theoretical analyses as we strive for making a computationally tractable formulation. We apply our results in the estimation of causal effects with multiple binary treatments and show how our estimators can improve the finite sample performance when compared with non-adaptive estimators that try to estimate all the probabilities directly. We also provide simulations that are consistent with our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15166v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandre Belloni, Yan Chen, Matthew Harding</dc:creator>
    </item>
    <item>
      <title>Nonparametric Regression in Dirichlet Spaces: A Random Obstacle Approach</title>
      <link>https://arxiv.org/abs/2412.14357</link>
      <description>arXiv:2412.14357v2 Announce Type: replace 
Abstract: In this paper, we consider nonparametric estimation over general Dirichlet metric measure spaces. Unlike the more commonly studied reproducing kernel Hilbert space, whose elements may be defined pointwise, a Dirichlet space typically only contain equivalence classes, i.e. its elements are only unique almost everywhere. This lack of pointwise definition presents significant challenges in the context of nonparametric estimation, for example the classical ridge regression problem is ill-posed. In this paper, we develop a new technique for renormalizing the ridge loss by replacing pointwise evaluations with certain \textit{local means} around the boundaries of obstacles centered at each data point. The resulting renormalized empirical risk functional is well-posed and even admits a representer theorem in terms of certain equilibrium potentials, which are truncated versions of the associated Green function, cut-off at a data-driven threshold. We study the global, out-of-sample consistency of the sample minimizer, and derive an adaptive upper bound on its convergence rate that highlights the interplay of the analytic, geometric, and probabilistic properties of the Dirichlet form. Our framework notably does not require the smoothness of the underlying space, and is applicable to both manifold and fractal settings. To the best of our knowledge, this is the first paper to obtain out-of-sample convergence guarantees in the framework of general metric measure Dirichlet spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14357v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prem Talwai, David Simchi-Levi</dc:creator>
    </item>
    <item>
      <title>Algorithms for ridge estimation with convergence guarantees</title>
      <link>https://arxiv.org/abs/2104.12314</link>
      <description>arXiv:2104.12314v2 Announce Type: replace-cross 
Abstract: The extraction of filamentary structure from a point cloud is discussed. The filaments are modeled as ridge lines or higher dimensional ridges of an underlying density. We propose two novel algorithms, and provide theoretical guarantees for their convergences, by which we mean that the algorithms can asymptotically recover the full ridge set. We consider the new algorithms as alternatives to the Subspace Constrained Mean Shift (SCMS) algorithm for which no such theoretical guarantees are known.</description>
      <guid isPermaLink="false">oai:arXiv.org:2104.12314v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wanli Qiao, Wolfgang Polonik</dc:creator>
    </item>
    <item>
      <title>Kernel methods for long term dose response curves</title>
      <link>https://arxiv.org/abs/2201.05139</link>
      <description>arXiv:2201.05139v2 Announce Type: replace-cross 
Abstract: A core challenge in causal inference is how to extrapolate long term effects, of possibly continuous actions, from short term experimental data. It arises in artificial intelligence: the long term consequences of continuous actions may be of interest, yet only short term rewards may be collected in exploration. For this estimand, called the long term dose response curve, we propose a simple nonparametric estimator based on kernel ridge regression. By embedding the distribution of the short term experimental data with kernels, we derive interpretable weights for extrapolating long term effects. Our method allows actions, short term rewards, and long term rewards to be continuous in general spaces. It also allows for nonlinearity and heterogeneity in the link between short term effects and long term effects. We prove uniform consistency, with nonasymptotic error bounds reflecting the effective dimension of the data. As an application, we estimate the long term dose response curve of Project STAR, a social program which randomly assigned students to various class sizes. We extend our results to long term counterfactual distributions, proving weak convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.05139v2</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rahul Singh, Hannah Zhou</dc:creator>
    </item>
    <item>
      <title>Adapting to Unknown Low-Dimensional Structures in Score-Based Diffusion Models</title>
      <link>https://arxiv.org/abs/2405.14861</link>
      <description>arXiv:2405.14861v2 Announce Type: replace-cross 
Abstract: This paper investigates score-based diffusion models when the underlying target distribution is concentrated on or near low-dimensional manifolds within the higher-dimensional space in which they formally reside, a common characteristic of natural image distributions. Despite previous efforts to understand the data generation process of diffusion models, existing theoretical support remains highly suboptimal in the presence of low-dimensional structure, which we strengthen in this paper. For the popular Denoising Diffusion Probabilistic Model (DDPM), we find that the dependency of the error incurred within each denoising step on the ambient dimension $d$ is in general unavoidable. We further identify a unique design of coefficients that yields a converges rate at the order of $O(k^{2}/\sqrt{T})$ (up to log factors), where $k$ is the intrinsic dimension of the target distribution and $T$ is the number of steps. This represents the first theoretical demonstration that the DDPM sampler can adapt to unknown low-dimensional structures in the target distribution, highlighting the critical importance of coefficient design. All of this is achieved by a novel set of analysis tools that characterize the algorithmic dynamics in a more deterministic manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14861v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gen Li, Yuling Yan</dc:creator>
    </item>
    <item>
      <title>Empirical Networks are Sparse: Enhancing Multi-Edge Models with Zero-Inflation</title>
      <link>https://arxiv.org/abs/2406.09169</link>
      <description>arXiv:2406.09169v2 Announce Type: replace-cross 
Abstract: Real-world networks are sparse. As we show in this article, even when a large number of interactions is observed, most node pairs remain disconnected. We demonstrate that classical multi-edge network models, such as the $G(N,p)$, configuration models, and stochastic block models, fail to accurately capture this phenomenon. To mitigate this issue, zero-inflation must be integrated into these traditional models. Through zero-inflation, we incorporate a mechanism that accounts for the excess number of zeroes (disconnected pairs) observed in empirical data. By performing an analysis on all the datasets from the Sociopatterns repository, we illustrate how zero-inflated models more accurately reflect the sparsity and heavy-tailed edge count distributions observed in empirical data. Our findings underscore that failing to account for these ubiquitous properties in real-world networks inadvertently leads to biased models that do not accurately represent complex systems and their dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09169v2</guid>
      <category>cs.SI</category>
      <category>math.ST</category>
      <category>physics.soc-ph</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giona Casiraghi, Georges Andres</dc:creator>
    </item>
    <item>
      <title>Tame sparse exponential random graphs</title>
      <link>https://arxiv.org/abs/2406.17390</link>
      <description>arXiv:2406.17390v2 Announce Type: replace-cross 
Abstract: In this paper, we obtain a precise estimate of the probability that the sparse binomial random graph contains a large number of vertices in a triangle. The estimate of log of this probability is correct up to second order, and enables us to propose an exponential random graph model based on the number of vertices in a triangle. Specifically, by tuning a single parameter, we can with high probability induce any given fraction of vertices in a triangle. Moreover, in the proposed exponential random graph model we derive the large deviation principle for the number of edges. As a byproduct, we propose a consistent estimator of the tuning parameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17390v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suman Chakraborty, Remco van der Hofstad, Frank den Hollander</dc:creator>
    </item>
    <item>
      <title>High-accuracy sampling from constrained spaces with the Metropolis-adjusted Preconditioned Langevin Algorithm</title>
      <link>https://arxiv.org/abs/2412.18701</link>
      <description>arXiv:2412.18701v2 Announce Type: replace-cross 
Abstract: In this work, we propose a first-order sampling method called the Metropolis-adjusted Preconditioned Langevin Algorithm for approximate sampling from a target distribution whose support is a proper convex subset of $\mathbb{R}^{d}$. Our proposed method is the result of applying a Metropolis-Hastings filter to the Markov chain formed by a single step of the preconditioned Langevin algorithm with a metric $\mathscr{G}$, and is motivated by the natural gradient descent algorithm for optimisation. We derive non-asymptotic upper bounds for the mixing time of this method for sampling from target distributions whose potentials are bounded relative to $\mathscr{G}$, and for exponential distributions restricted to the support. Our analysis suggests that if $\mathscr{G}$ satisfies stronger notions of self-concordance introduced in Kook and Vempala (2024), then these mixing time upper bounds have a strictly better dependence on the dimension than when is merely self-concordant. We also provide numerical experiments that demonstrates the practicality of our proposed method. Our method is a high-accuracy sampler due to the polylogarithmic dependence on the error tolerance in our mixing time upper bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18701v2</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vishwak Srinivasan, Andre Wibisono, Ashia Wilson</dc:creator>
    </item>
    <item>
      <title>Debiased Nonparametric Regression for Statistical Inference and Distributionally Robustness</title>
      <link>https://arxiv.org/abs/2412.20173</link>
      <description>arXiv:2412.20173v2 Announce Type: replace-cross 
Abstract: This study proposes a debiasing method for smooth nonparametric estimators. While machine learning techniques such as random forests and neural networks have demonstrated strong predictive performance, their theoretical properties remain relatively underexplored. Specifically, many modern algorithms lack assurances of pointwise asymptotic normality and uniform convergence, which are critical for statistical inference and robustness under covariate shift and have been well-established for classical methods like Nadaraya-Watson regression. To address this, we introduce a model-free debiasing method that guarantees these properties for smooth estimators derived from any nonparametric regression approach. By adding a correction term that estimates the conditional expected residual of the original estimator, or equivalently, its estimation error, we obtain a debiased estimator with proven pointwise asymptotic normality, and uniform convergence. These properties enable statistical inference and enhance robustness to covariate shift, making the method broadly applicable to a wide range of nonparametric regression problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20173v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
  </channel>
</rss>
