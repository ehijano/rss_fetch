<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 16 Dec 2024 05:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Semi-Supervised Community Detection via Quasi-Stationary Distributions</title>
      <link>https://arxiv.org/abs/2412.09793</link>
      <description>arXiv:2412.09793v1 Announce Type: new 
Abstract: Spectral clustering is a widely used method for community detection in networks. We focus on a semi-supervised community detection scenario in the Partially Labeled Stochastic Block Model (PL-SBM) with two balanced communities, where a fixed portion of labels is known. Our approach leverages random walks in which the revealed nodes in each community act as absorbing states. By analyzing the quasi-stationary distributions associated with these random walks, we construct a classifier that distinguishes the two communities by examining differences in the associated eigenvectors. We establish upper and lower bounds on the error rate for a broad class of quasi-stationary algorithms, encompassing both spectral and voting-based approaches. In particular, we prove that this class of algorithms can achieve the optimal error rate in the connected regime. We further demonstrate empirically that our quasi-stationary approach improves performance on both real-world and simulated datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09793v1</guid>
      <category>math.ST</category>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Fraiman, Michael Nisenzon</dc:creator>
    </item>
    <item>
      <title>Ordering results between two finite arithmetic mixture models with multiple-outlier location-scale distributed components</title>
      <link>https://arxiv.org/abs/2412.10071</link>
      <description>arXiv:2412.10071v1 Announce Type: new 
Abstract: In this article, we introduce finite mixture models (FMMs) renowned for capturing population heterogeneity. Our focus lies in establishing stochastic comparisons between two arithmetic (finite) mixture models, employing the vector majorization concept in the context of various univariate orders of magnitude, transform, and variability. These comparisons are conducted within the framework of multiple-outlier location-scale models. Specifically, we derive sufficient conditions for comparing two finite arithmetic mixture models with components distributed in a multiple-outlier location-scale model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10071v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raju Bhakta, Nuria Torrado, Sangita Das, Suchandan Kayal</dc:creator>
    </item>
    <item>
      <title>A Statistical Analysis for Supervised Deep Learning with Exponential Families for Intrinsically Low-dimensional Data</title>
      <link>https://arxiv.org/abs/2412.09779</link>
      <description>arXiv:2412.09779v1 Announce Type: cross 
Abstract: Recent advances have revealed that the rate of convergence of the expected test error in deep supervised learning decays as a function of the intrinsic dimension and not the dimension $d$ of the input space. Existing literature defines this intrinsic dimension as the Minkowski dimension or the manifold dimension of the support of the underlying probability measures, which often results in sub-optimal rates and unrealistic assumptions. In this paper, we consider supervised deep learning when the response given the explanatory variable is distributed according to an exponential family with a $\beta$-H\"older smooth mean function. We consider an entropic notion of the intrinsic data-dimension and demonstrate that with $n$ independent and identically distributed samples, the test error scales as $\tilde{\mathcal{O}}\left(n^{-\frac{2\beta}{2\beta + \bar{d}_{2\beta}(\lambda)}}\right)$, where $\bar{d}_{2\beta}(\lambda)$ is the $2\beta$-entropic dimension of $\lambda$, the distribution of the explanatory variables. This improves on the best-known rates. Furthermore, under the assumption of an upper-bounded density of the explanatory variables, we characterize the rate of convergence as $\tilde{\mathcal{O}}\left( d^{\frac{2\lfloor\beta\rfloor(\beta + d)}{2\beta + d}}n^{-\frac{2\beta}{2\beta + d}}\right)$, establishing that the dependence on $d$ is not exponential but at most polynomial. We also demonstrate that when the explanatory variable has a lower bounded density, this rate in terms of the number of data samples, is nearly optimal for learning the dependence structure for exponential families.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09779v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saptarshi Chakraborty, Peter L. Bartlett</dc:creator>
    </item>
    <item>
      <title>Flexible Bayesian Nonparametric Product Mixtures for Multi-scale Functional Clustering</title>
      <link>https://arxiv.org/abs/2412.09792</link>
      <description>arXiv:2412.09792v1 Announce Type: cross 
Abstract: There is a rich literature on clustering functional data with applications to time-series modeling, trajectory data, and even spatio-temporal applications. However, existing methods routinely perform global clustering that enforces identical atom values within the same cluster. Such grouping may be inadequate for high-dimensional functions, where the clustering patterns may change between the more dominant high-level features and the finer resolution local features. While there is some limited literature on local clustering approaches to deal with the above problems, these methods are typically not scalable to high-dimensional functions, and their theoretical properties are not well-investigated. Focusing on basis expansions for high-dimensional functions, we propose a flexible non-parametric Bayesian approach for multi-resolution clustering. The proposed method imposes independent Dirichlet process (DP) priors on different subsets of basis coefficients that ultimately results in a product of DP mixture priors inducing local clustering. We generalize the approach to incorporate spatially correlated error terms when modeling random spatial functions to provide improved model fitting. An efficient Markov chain Monte Carlo (MCMC) algorithm is developed for implementation. We show posterior consistency properties under the local clustering approach that asymptotically recovers the true density of random functions. Extensive simulations illustrate the improved clustering and function estimation under the proposed method compared to classical approaches. We apply the proposed approach to a spatial transcriptomics application where the goal is to infer clusters of genes with distinct spatial patterns of expressions. Our method makes an important contribution by expanding the limited literature on local clustering methods for high-dimensional functions with theoretical guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09792v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tsung-Hung Yao, Suprateek Kundu</dc:creator>
    </item>
    <item>
      <title>$L$-estimation of Claim Severity Models Weighted by Kumaraswamy Density</title>
      <link>https://arxiv.org/abs/2412.09830</link>
      <description>arXiv:2412.09830v1 Announce Type: cross 
Abstract: Statistical modeling of claim severity distributions is essential in insurance and risk management, where achieving a balance between robustness and efficiency in parameter estimation is critical against model contaminations. Two \( L \)-estimators, the method of trimmed moments (MTM) and the method of winsorized moments (MWM), are commonly used in the literature, but they are constrained by rigid weighting schemes that either discard or uniformly down-weight extreme observations, limiting their customized adaptability. This paper proposes a flexible robust \( L \)-estimation framework weighted by Kumaraswamy densities, offering smoothly varying observation-specific weights that preserve valuable information while improving robustness and efficiency. The framework is developed for parametric claim severity models, including Pareto, lognormal, and Fr{\'e}chet distributions, with theoretical justifications on asymptotic normality and variance-covariance structures. Through simulations and application to a U.S. indemnity loss dataset, the proposed method demonstrates superior performance over MTM, MWM, and MLE approaches, particularly in handling outliers and heavy-tailed distributions, making it a flexible and reliable alternative for loss severity modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09830v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chudamani Poudyal, Gokarna R. Aryal, Keshav Pokhrel</dc:creator>
    </item>
    <item>
      <title>The Stein-log-Sobolev inequality and the exponential rate of convergence for the continuous Stein variational gradient descent method</title>
      <link>https://arxiv.org/abs/2412.10295</link>
      <description>arXiv:2412.10295v1 Announce Type: cross 
Abstract: The Stein Variational Gradient Descent method is a variational inference method in statistics that has recently received a lot of attention. The method provides a deterministic approximation of the target distribution, by introducing a nonlocal interaction with a kernel. Despite the significant interest, the exponential rate of convergence for the continuous method has remained an open problem, due to the difficulty of establishing the related so-called Stein-log-Sobolev inequality. Here, we prove that the inequality is satisfied for each space dimension and every kernel whose Fourier transform has a quadratic decay at infinity and is locally bounded away from zero and infinity. Moreover, we construct weak solutions to the related PDE satisfying exponential rate of decay towards the equilibrium. The main novelty in our approach is to interpret the Stein-Fisher information, also called the squared Stein discrepancy, as a duality pairing between $H^{-1}(\mathbb{R}^d)$ and $H^{1}(\mathbb{R}^d)$, which allows us to employ the Fourier transform. We also provide several examples of kernels for which the Stein-log-Sobolev inequality fails, partially showing the necessity of our assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10295v1</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jos\'e A. Carrillo, Jakub Skrzeczkowski, Jethro Warnett</dc:creator>
    </item>
    <item>
      <title>Learning of deep convolutional network image classifiers via stochastic gradient descent and over-parametrization</title>
      <link>https://arxiv.org/abs/2404.07128</link>
      <description>arXiv:2404.07128v2 Announce Type: replace 
Abstract: Image classification from independent and identically distributed random variables is considered. Image classifiers are defined which are based on a linear combination of deep convolutional networks with max-pooling layer. Here all the weights are learned by stochastic gradient descent. A general result is presented which shows that the image classifiers are able to approximate the best possible deep convolutional network. In case that the a posteriori probability satisfies a suitable hierarchical composition model it is shown that the corresponding deep convolutional neural network image classifier achieves a rate of convergence which is independent of the dimension of the images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07128v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Kohler, Adam Krzyzak, Alisha S\"anger</dc:creator>
    </item>
    <item>
      <title>The distribution of Bayes' ratio</title>
      <link>https://arxiv.org/abs/2404.00744</link>
      <description>arXiv:2404.00744v3 Announce Type: replace-cross 
Abstract: The ratio of Bayesian evidences is a popular tool in cosmology to compare different models. There are however several issues with this method: Bayes' ratio depends on the prior even in the limit of non-informative priors, and Jeffrey's scale, used to assess the test, is arbitrary. Moreover, the standard use of Bayes' ratio is often criticized for being unable to reject models. In this paper, we address these shortcoming by promoting evidences and evidence ratios to frequentist statistics and deriving their sampling distributions. By comparing the evidence ratios to their sampling distributions, poor fitting models can now be rejected. Our method additionally does not depend on the prior in the limit of very weak priors, thereby safeguarding the experimenter against premature rejection of a theory with a uninformative prior, and replaces the arbitrary Jeffrey's scale by probability thresholds for rejection. We provide analytical solutions for some simplified cases (Gaussian data, linear parameters, and nested models), and we apply the method to cosmological supernovae Ia data. We dub our method the FB method, for Frequentist-Bayesian.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00744v3</guid>
      <category>astro-ph.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luca Amendola, Vrund Patel, Ziad Sakr, Elena Sellentin, Kevin Wolz</dc:creator>
    </item>
    <item>
      <title>Non-Markovian Reduced Models to Unravel Transitions in Non-equilibrium Systems</title>
      <link>https://arxiv.org/abs/2408.13428</link>
      <description>arXiv:2408.13428v2 Announce Type: replace-cross 
Abstract: This work proposes a general framework for capturing noise-driven transitions in spatially extended non-equilibrium systems and explains the emergence of coherent patterns beyond the instability onset. The framework relies on stochastic parameterizations to reduce the original equations' complexity while capturing the key effects of unresolved scales. It works for both Gaussian and Levy-type noise. Our parameterizations offer two key advantages. First, they approximate stochastic invariant manifolds when the latter exist. Second, even when such manifolds break down, our formulas can be adapted by a simple optimization of its constitutive parameters. This allows us to handle scenarios with weak time-scale separation where the system has undergone multiple transitions, resulting in large-amplitude solutions not captured by invariant manifold or other time-scale separation methods. The optimized stochastic parameterizations capture how small-scale noise impacts larger scales through the system's nonlinear interactions. This effect is achieved by the very fabric of our parameterizations incorporating non-Markovian coefficients into the reduced equation. Such coefficients account for the noise's past influence using a finite memory length, selected for optimal performance. The specific "memory" function, which determines how this past influence is weighted, depends on the noise's strength and how it interacts with the system's nonlinearities. Remarkably, training our theory-guided reduced models on a single noise path effectively learns the optimal memory length for out-of-sample predictions, including rare events. This success stems from our "hybrid" approach, which combines analytical understanding with data-driven learning. This combination avoids a key limitation of purely data-driven methods: their struggle to generalize to unseen scenarios, also known as the "extrapolation problem."</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13428v2</guid>
      <category>math.DS</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Micka\"el D. Chekroun, Honghu Liu, James C. McWilliams</dc:creator>
    </item>
  </channel>
</rss>
