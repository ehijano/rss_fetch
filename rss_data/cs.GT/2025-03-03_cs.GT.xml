<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 03 Mar 2025 05:00:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Semicoarse Correlated Equilibria and LP-Based Guarantees for Gradient Dynamics in Normal-Form Games</title>
      <link>https://arxiv.org/abs/2502.20466</link>
      <description>arXiv:2502.20466v1 Announce Type: new 
Abstract: Projected gradient ascent describes a form of no-regret learning algorithm that is known to converge to a coarse correlated equilibrium. Recent results showed that projected gradient ascent often finds the Nash equilibrium, even in situations where the set of coarse correlated equilibria is very large. We introduce semicoarse correlated equilibria, a solution concept that refines coarse correlated equilibria for the outcomes of gradient dynamics, while remaining computationally tractable through linear programming representations. Our theoretical analysis of the discretised Bertrand competition mirrors those recently established for mean-based learning in first-price auctions. With at least two firms of lowest marginal cost, Nash equilibria emerge as the only semicoarse equilibria under concavity conditions on firm profits. In first-price auctions, the granularity of the bid space affects semicoarse equilibria, but finer granularity for lower bids also induces convergence to Nash equilibria. Unlike previous work that aims to prove convergence to a Nash equilibrium that often relies on epoch based analysis and probability theoretic machinery, our LP-based duality approach enables a simple and tractable analysis of equilibrium selection under gradient-based learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20466v1</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mete \c{S}eref Ahunbay, Martin Bichler</dc:creator>
    </item>
    <item>
      <title>Learning to Steer Learners in Games</title>
      <link>https://arxiv.org/abs/2502.20770</link>
      <description>arXiv:2502.20770v1 Announce Type: new 
Abstract: We consider the problem of learning to exploit learning algorithms through repeated interactions in games. Specifically, we focus on the case of repeated two player, finite-action games, in which an optimizer aims to steer a no-regret learner to a Stackelberg equilibrium without knowledge of its payoffs. We first show that this is impossible if the optimizer only knows that the learner is using an algorithm from the general class of no-regret algorithms. This suggests that the optimizer requires more information about the learner's objectives or algorithm to successfully exploit them. Building on this intuition, we reduce the problem for the optimizer to that of recovering the learner's payoff structure. We demonstrate the effectiveness of this approach if the learner's algorithm is drawn from a smaller class by analyzing two examples: one where the learner uses an ascent algorithm, and another where the learner uses stochastic mirror ascent with known regularizer and step sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20770v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yizhou Zhang, Yi-An Ma, Eric Mazumdar</dc:creator>
    </item>
    <item>
      <title>Flattening Supply Chains: When do Technology Improvements lead to Disintermediation?</title>
      <link>https://arxiv.org/abs/2502.20783</link>
      <description>arXiv:2502.20783v1 Announce Type: new 
Abstract: In the digital economy, technological innovations make it cheaper to produce high-quality content. For example, generative AI tools reduce costs for creators who develop content to be distributed online, but can also reduce production costs for the users who consume that content. These innovations can thus lead to disintermediation, since consumers may choose to use these technologies directly, bypassing intermediaries. To investigate when technological improvements lead to disintermediation, we study a game with an intermediary, suppliers of a production technology, and consumers. First, we show disintermediation occurs whenever production costs are too high or too low. We then investigate the consequences of disintermediation for welfare and content quality at equilibrium. While the intermediary is welfare-improving, the intermediary extracts all gains to social welfare and its presence can raise or lower content quality. We further analyze how disintermediation is affected by the level of competition between suppliers and the intermediary's fee structure. More broadly, our results take a step towards assessing how production technology innovations affect the survival of intermediaries and impact the digital economy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20783v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S. Nageeb Ali, Nicole Immorlica, Meena Jagadeesan, Brendan Lucier</dc:creator>
    </item>
    <item>
      <title>Metric Distortion in Peer Selection</title>
      <link>https://arxiv.org/abs/2502.21084</link>
      <description>arXiv:2502.21084v1 Announce Type: new 
Abstract: In the metric distortion problem, a set of voters and candidates lie in a common metric space, and a committee of $k$ candidates is to be elected. The goal is to select a committee with a small social cost, defined as an increasing function of the distances between voters and selected candidates, but a voting rule only has access to voters' ordinal preferences. The distortion of a rule is then defined as the worst-case ratio between the social cost of the selected set and the optimal set, over all possible preferences and consistent distances.
  We initiate the study of metric distortion when voters and candidates coincide, which arises naturally in peer selection, and provide tight results for various social cost functions on the line metric. We consider both utilitarian and egalitarian social cost, given by the sum and maximum of the individual social costs, respectively. For utilitarian social cost, we show that the voting rule that selects the $k$ middle agents achieves a distortion that varies between $1$ and $2$ as $k$ varies from $1$ to $n$ when the cost of an individual is the sum of their distances to all selected candidates (additive aggregation). When the cost of an individual is their distance to their $q$th closest candidate ($q$-cost), we provide positive results for $q=k=2$ but mostly show that negative results for general elections carry over to our setting: No constant distortion is possible when $q\leq k/2$ and no distortion better than $3/2$ is possible for $q\geq k/2+1$. For egalitarian social cost, selecting extreme agents achieves the best-possible distortion of $2$ for additive cost and $q$-cost with $q&gt; k/3$, whereas no constant distortion is possible for $q\leq k/3$. Overall, having a common set of voters and candidates allows for better constants compared to the general setting, but cases in which no constant is possible in general remain hard in this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21084v1</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Javier Cembrano, Golnoosh Shahkarami</dc:creator>
    </item>
    <item>
      <title>Large Language Model Strategic Reasoning Evaluation through Behavioral Game Theory</title>
      <link>https://arxiv.org/abs/2502.20432</link>
      <description>arXiv:2502.20432v1 Announce Type: cross 
Abstract: Strategic decision-making involves interactive reasoning where agents adapt their choices in response to others, yet existing evaluations of large language models (LLMs) often emphasize Nash Equilibrium (NE) approximation, overlooking the mechanisms driving their strategic choices. To bridge this gap, we introduce an evaluation framework grounded in behavioral game theory, disentangling reasoning capability from contextual effects. Testing 22 state-of-the-art LLMs, we find that GPT-o3-mini, GPT-o1, and DeepSeek-R1 dominate most games yet also demonstrate that the model scale alone does not determine performance. In terms of prompting enhancement, Chain-of-Thought (CoT) prompting is not universally effective, as it increases strategic reasoning only for models at certain levels while providing limited gains elsewhere. Additionally, we investigate the impact of encoded demographic features on the models, observing that certain assignments impact the decision-making pattern. For instance, GPT-4o shows stronger strategic reasoning with female traits than males, while Gemma assigns higher reasoning levels to heterosexual identities compared to other sexual orientations, indicating inherent biases. These findings underscore the need for ethical standards and contextual alignment to balance improved reasoning with fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20432v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingru Jia, Zehua Yuan, Junhao Pan, Paul E. McNamara, Deming Chen</dc:creator>
    </item>
    <item>
      <title>Public Projects with Preferences and Predictions</title>
      <link>https://arxiv.org/abs/2403.01042</link>
      <description>arXiv:2403.01042v2 Announce Type: replace 
Abstract: When making a decision as a group, there are two primary paradigms: aggregating preferences (e.g. voting, mechanism design) and aggregating information (e.g. discussion, consulting, forecasting). Almost all formally-studied group decisionmaking mechanisms fall under one paradigm or the other, but not both. We consider a public projects problem with the objective of maximizing utilitarian social welfare. Decisionmakers have both preferences, modeled as utility functions over the alternatives; and information, modeled as Bayesian signals relevant to the alternatives' external welfare impact. Aligning incentives is highly challenging because, on the one hand, agents can provide bad information in order to manipulate the mechanism into satisfying their preferences; and on the other hand, they can misreport their preferences to favor selection of an alternative for which their information rewards are high.
  We propose a two-stage mechanism for this problem. The forecasting stage aggregates information using either a wagering mechanism or a prediction market (the mechanism is modular and compatible with both). The voting stage aggregates preferences, together with the forecasts from the previous stage, and selects an alternative by leveraging the recently-studied Quadratic Transfers Mechanism. We show that, when carefully combined, the entire two-stage mechanism is robust to manipulation of all forms, and under weak assumptions, satisfies Price of Anarchy guarantees. In the case of two alternatives, the Price of Anarchy tends to 1 as natural measures of the "size" of the population grow large. In most cases, the mechanisms achieve a balanced budget as well. We also give the first nonasymptotic Price of Anarchy guarantee for the Quadratic Transfers Mechanism, a result of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01042v2</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mary Monroe, Bo Waggoner</dc:creator>
    </item>
    <item>
      <title>Computing Game Symmetries and Equilibria That Respect Them</title>
      <link>https://arxiv.org/abs/2501.08905</link>
      <description>arXiv:2501.08905v2 Announce Type: replace 
Abstract: Strategic interactions can be represented more concisely, and analyzed and solved more efficiently, if we are aware of the symmetries within the multiagent system. Symmetries also have conceptual implications, for example for equilibrium selection. We study the computational complexity of identifying and using symmetries. Using the classical framework of normal-form games, we consider game symmetries that can be across some or all players and/or actions. We find a strong connection between game symmetries and graph automorphisms, yielding graph automorphism and graph isomorphism completeness results for characterizing the symmetries present in a game. On the other hand, we also show that the problem becomes polynomial-time solvable when we restrict the consideration of actions in one of two ways.
  Next, we investigate when exactly game symmetries can be successfully leveraged for Nash equilibrium computation. We show that finding a Nash equilibrium that respects a given set of symmetries is PPAD- and CLS-complete in general-sum and team games respectively -- that is, exactly as hard as Brouwer fixed point and gradient descent problems. Finally, we present polynomial-time methods for the special cases where we are aware of a vast number of symmetries, or where the game is two-player zero-sum and we do not even know the symmetries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08905v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>cs.MA</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emanuel Tewolde, Brian Hu Zhang, Caspar Oesterheld, Tuomas Sandholm, Vincent Conitzer</dc:creator>
    </item>
    <item>
      <title>Expected Variational Inequalities</title>
      <link>https://arxiv.org/abs/2502.18605</link>
      <description>arXiv:2502.18605v2 Announce Type: replace 
Abstract: Variational inequalities (VIs) encompass many fundamental problems in diverse areas ranging from engineering to economics and machine learning. However, their considerable expressivity comes at the cost of computational intractability. In this paper, we introduce and analyze a natural relaxation -- which we refer to as expected variational inequalities (EVIs) -- where the goal is to find a distribution that satisfies the VI constraint in expectation. By adapting recent techniques from game theory, we show that, unlike VIs, EVIs can be solved in polynomial time under general (nonmonotone) operators. EVIs capture the seminal notion of correlated equilibria, but enjoy a greater reach beyond games. We also employ our framework to capture and generalize several existing disparate results, including from settings such as smooth games, and games with coupled constraints or nonconcave utilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18605v2</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian Hu Zhang, Ioannis Anagnostides, Emanuel Tewolde, Ratip Emin Berker, Gabriele Farina, Vincent Conitzer, Tuomas Sandholm</dc:creator>
    </item>
    <item>
      <title>Bootstrapping Fisher Market Equilibrium and First-Price Pacing Equilibrium</title>
      <link>https://arxiv.org/abs/2402.02303</link>
      <description>arXiv:2402.02303v5 Announce Type: replace-cross 
Abstract: The linear Fisher market (LFM) is a basic equilibrium model from economics, which also has applications in fair and efficient resource allocation. First-price pacing equilibrium (FPPE) is a model capturing budget-management mechanisms in first-price auctions. In certain practical settings such as advertising auctions, there is an interest in performing statistical inference over these models. A popular methodology for general statistical inference is the bootstrap procedure. Yet, for LFM and FPPE there is no existing theory for the valid application of bootstrap procedures. In this paper, we introduce and devise several statistically valid bootstrap inference procedures for LFM and FPPE. The most challenging part is to bootstrap general FPPE, which reduces to bootstrapping constrained M-estimators, a largely unexplored problem. We devise a bootstrap procedure for FPPE under mild degeneracy conditions by using the powerful tool of epi-convergence theory. Experiments with synthetic and semi-real data verify our theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02303v5</guid>
      <category>math.ST</category>
      <category>cs.GT</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luofeng Liao, Christian Kroer</dc:creator>
    </item>
    <item>
      <title>Learning and Computation of $\Phi$-Equilibria at the Frontier of Tractability</title>
      <link>https://arxiv.org/abs/2502.18582</link>
      <description>arXiv:2502.18582v2 Announce Type: replace-cross 
Abstract: $\Phi$-equilibria -- and the associated notion of $\Phi$-regret -- are a powerful and flexible framework at the heart of online learning and game theory, whereby enriching the set of deviations $\Phi$ begets stronger notions of rationality. Recently, Daskalakis, Farina, Fishelson, Pipis, and Schneider (STOC '24) -- abbreviated as DFFPS -- settled the existence of efficient algorithms when $\Phi$ contains only linear maps under a general, $d$-dimensional convex constraint set $\mathcal{X}$. In this paper, we significantly extend their work by resolving the case where $\Phi$ is $k$-dimensional; degree-$\ell$ polynomials constitute a canonical such example with $k = d^{O(\ell)}$. In particular, positing only oracle access to $\mathcal{X}$, we obtain two main positive results: i) a $\text{poly}(n, d, k, \text{log}(1/\epsilon))$-time algorithm for computing $\epsilon$-approximate $\Phi$-equilibria in $n$-player multilinear games, and ii) an efficient online algorithm that incurs average $\Phi$-regret at most $\epsilon$ using $\text{poly}(d, k)/\epsilon^2$ rounds.
  We also show nearly matching lower bounds in the online learning setting, thereby obtaining for the first time a family of deviations that captures the learnability of $\Phi$-regret.
  From a technical standpoint, we extend the framework of DFFPS from linear maps to the more challenging case of maps with polynomial dimension. At the heart of our approach is a polynomial-time algorithm for computing an expected fixed point of any $\phi : \mathcal{X} \to \mathcal{X}$ based on the ellipsoid against hope (EAH) algorithm of Papadimitriou and Roughgarden (JACM '08). In particular, our algorithm for computing $\Phi$-equilibria is based on executing EAH in a nested fashion -- each step of EAH itself being implemented by invoking a separate call to EAH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18582v2</guid>
      <category>stat.ML</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian Hu Zhang, Ioannis Anagnostides, Emanuel Tewolde, Ratip Emin Berker, Gabriele Farina, Vincent Conitzer, Tuomas Sandholm</dc:creator>
    </item>
  </channel>
</rss>
