<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Mar 2025 02:15:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>How AI and Human Behaviors Shape Psychosocial Effects of Chatbot Use: A Longitudinal Randomized Controlled Study</title>
      <link>https://arxiv.org/abs/2503.17473</link>
      <description>arXiv:2503.17473v1 Announce Type: new 
Abstract: AI chatbots, especially those with voice capabilities, have become increasingly human-like, with more users seeking emotional support and companionship from them. Concerns are rising about how such interactions might impact users' loneliness and socialization with real people. We conducted a four-week randomized, controlled, IRB-approved experiment (n=981, &gt;300K messages) to investigate how AI chatbot interaction modes (text, neutral voice, and engaging voice) and conversation types (open-ended, non-personal, and personal) influence psychosocial outcomes such as loneliness, social interaction with real people, emotional dependence on AI and problematic AI usage. Results showed that while voice-based chatbots initially appeared beneficial in mitigating loneliness and dependence compared with text-based chatbots, these advantages diminished at high usage levels, especially with a neutral-voice chatbot. Conversation type also shaped outcomes: personal topics slightly increased loneliness but tended to lower emotional dependence compared with open-ended conversations, whereas non-personal topics were associated with greater dependence among heavy users. Overall, higher daily usage - across all modalities and conversation types - correlated with higher loneliness, dependence, and problematic use, and lower socialization. Exploratory analyses revealed that those with stronger emotional attachment tendencies and higher trust in the AI chatbot tended to experience greater loneliness and emotional dependence, respectively. These findings underscore the complex interplay between chatbot design choices (e.g., voice expressiveness) and user behaviors (e.g., conversation content, usage frequency). We highlight the need for further research on whether chatbots' ability to manage emotional content without fostering dependence or replacing human relationships benefits overall well-being.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17473v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cathy Mengying Fang, Auren R. Liu, Valdemar Danry, Eunhae Lee, Samantha W. T. Chan, Pat Pataranutaporn, Pattie Maes, Jason Phang, Michael Lampe, Lama Ahmad, Sandhini Agarwal</dc:creator>
    </item>
    <item>
      <title>Your voice is your voice: Supporting Self-expression through Speech Generation and LLMs in Augmented and Alternative Communication</title>
      <link>https://arxiv.org/abs/2503.17479</link>
      <description>arXiv:2503.17479v1 Announce Type: new 
Abstract: In this paper, we present Speak Ease: an augmentative and alternative communication (AAC) system to support users' expressivity by integrating multimodal input, including text, voice, and contextual cues (conversational partner and emotional tone), with large language models (LLMs). Speak Ease combines automatic speech recognition (ASR), context-aware LLM-based outputs, and personalized text-to-speech technologies to enable more personalized, natural-sounding, and expressive communication. Through an exploratory feasibility study and focus group evaluation with speech and language pathologists (SLPs), we assessed Speak Ease's potential to enable expressivity in AAC. The findings highlight the priorities and needs of AAC users and the system's ability to enhance user expressivity by supporting more personalized and contextually relevant communication. This work provides insights into the use of multimodal inputs and LLM-driven features to improve AAC systems and support expressivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17479v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yiwen Xu, Monideep Chakraborti, Tianyi Zhang, Katelyn Eng, Aanchan Mohan, Mirjana Prpa</dc:creator>
    </item>
    <item>
      <title>Reimagining Support: Exploring Autistic Individuals' Visions for AI in Coping with Negative Self-Talk</title>
      <link>https://arxiv.org/abs/2503.17504</link>
      <description>arXiv:2503.17504v1 Announce Type: new 
Abstract: Autistic individuals often experience negative self-talk (NST), leading to increased anxiety and depression. While therapy is recommended, it presents challenges for many autistic individuals. Meanwhile, a growing number are turning to large language models (LLMs) for mental health support. To understand how autistic individuals perceive AI's role in coping with NST, we surveyed 200 autistic adults and interviewed practitioners. We also analyzed LLM responses to participants' hypothetical prompts about their NST. Our findings show that participants view LLMs as useful for managing NST by identifying and reframing negative thoughts. Both participants and practitioners recognize AI's potential to support therapy and emotional expression. Participants also expressed concerns about LLMs' understanding of neurodivergent thought patterns, particularly due to the neurotypical bias of LLMs. Practitioners critiqued LLMs' responses as overly wordy, vague, and overwhelming. This study contributes to the growing research on AI-assisted mental health support, with specific insights for supporting the autistic community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17504v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Buse Carik, Victoria Izaac, Xiaohan Ding, Angela Scarpa, Eugenia Rho</dc:creator>
    </item>
    <item>
      <title>NAVIUS: Navigated Augmented Reality Visualization for Ureteroscopic Surgery</title>
      <link>https://arxiv.org/abs/2503.17511</link>
      <description>arXiv:2503.17511v1 Announce Type: new 
Abstract: Ureteroscopy is the standard of care for diagnosing and treating kidney stones and tumors. However, current ureteroscopes have a limited field of view, requiring significant experience to adequately navigate the renal collecting system. This is evidenced by the fact that inexperienced surgeons have higher rates of missed stones. One-third of patients with residual stones require re-operation within 20 months. In order to aid surgeons to fully explore the kidney, this study presents the Navigated Augmented Reality Visualization for Ureteroscopic Surgery (NAVIUS) system. NAVIUS assists surgeons by providing 3D maps of the target anatomy, real-time scope positions, and preoperative imaging overlays. To enable real-time navigation and visualization, we integrate an electromagnetic tracker-based navigation pipeline with augmented reality visualizations. NAVIUS connects to 3D Slicer and Unity with OpenIGTLink, and uses HoloLens 2 as a holographic interface. We evaluate NAVIUS through a user study where surgeons conducted ureteroscopy on kidney phantoms with and without visual guidance. With our proposed system, we observed that surgeons explored more areas within the collecting system with NAVIUS (average 23.73% increase), and NASA-TLX metrics were improved (up to 27.27%). NAVIUS acts as a step towards better surgical outcomes and surgeons' experience. The codebase for the system will be available at: https://github.com/vu-maple-lab/NAVIUS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17511v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ayberk Acar, Jumanh Atoum, Peter S. Connor, Clifford Pierre, Carisa N. Lynch, Nicholas L. Kavoussi, Jie Ying Wu</dc:creator>
    </item>
    <item>
      <title>Accessible Text Descriptions for UpSet Plots</title>
      <link>https://arxiv.org/abs/2503.17517</link>
      <description>arXiv:2503.17517v1 Announce Type: new 
Abstract: Data visualizations are typically not accessible to blind and low-vision (BLV) users. Automatically generating text descriptions offers an enticing mechanism for democratizing access to the information held in complex scientific charts, yet appropriate procedures for generating those texts remain elusive. Pursuing this issue, we study a single complex chart form: UpSet plots. UpSet Plots are a common way to analyze set data, an area largely unexplored by prior accessibility literature. By analyzing the patterns present in real-world examples, we develop a system for automatically captioning any UpSet plot. We evaluated the utility of our captions via semi-structured interviews with (N=11) BLV users and found that BLV users find them informative. In extensions, we find that sighted users can use our texts similarly to UpSet plots and that they are better than naive LLM usage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17517v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew McNutt, Maggie K McCracken, Ishrat Jahan Eliza, Daniel Hajas, Jake Wagoner, Nate Lanza, Jack Wilburn, Sarah Creem-Regehr, Alexander Lex</dc:creator>
    </item>
    <item>
      <title>A Case Study of Scalable Content Annotation Using Multi-LLM Consensus and Human Review</title>
      <link>https://arxiv.org/abs/2503.17620</link>
      <description>arXiv:2503.17620v1 Announce Type: new 
Abstract: Content annotation at scale remains challenging, requiring substantial human expertise and effort. This paper presents a case study in code documentation analysis, where we explore the balance between automation efficiency and annotation accuracy. We present MCHR (Multi-LLM Consensus with Human Review), a novel semi-automated framework that enhances annotation scalability through the systematic integration of multiple LLMs and targeted human review. Our framework introduces a structured consensus-building mechanism among LLMs and an adaptive review protocol that strategically engages human expertise. Through our case study, we demonstrate that MCHR reduces annotation time by 32% to 100% compared to manual annotation while maintaining high accuracy (85.5% to 98%) across different difficulty levels, from basic binary classification to challenging open-set scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17620v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingyue Yuan, Jieshan Chen, Zhenchang Xing, Aaron Quigley</dc:creator>
    </item>
    <item>
      <title>Kintsugi-Inspired Design: Communicatively Reconstructing Identities Online After Trauma</title>
      <link>https://arxiv.org/abs/2503.17639</link>
      <description>arXiv:2503.17639v1 Announce Type: new 
Abstract: Trauma can disrupt one's sense of self and mental well-being, leading survivors to reconstruct their identities in online communities. Drawing from 30 in-depth interviews, we present a sociotechnical process model that illustrates the mechanisms of online identity reconstruction and the pathways to integration. We introduce the concept of fractured identities, reflecting the enduring impact of trauma on one's self-concept.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17639v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Casey Randazzo, Tawfiq Ammari</dc:creator>
    </item>
    <item>
      <title>Do You "Trust" This Visualization? An Inventory to Measure Trust in Visualizations</title>
      <link>https://arxiv.org/abs/2503.17670</link>
      <description>arXiv:2503.17670v1 Announce Type: new 
Abstract: Trust plays a critical role in visual data communication and decision-making, yet existing visualization research employs varied trust measures, making it challenging to compare and synthesize findings across studies. In this work, we first took a bottom-up, data-driven approach to understand what visualization readers mean when they say they "trust" a visualization. We compiled and adapted a broad set of trust-related statements from existing inventories and collected responses on visualizations with varying degrees of trustworthiness. Through exploratory factor analysis, we derived an operational definition of trust in visualizations. Our findings indicate that people perceive a trustworthy visualization as one that presents credible information and is comprehensible and usable. Additionally, we found that general trust disposition influences how individuals assess visualization trustworthiness. Building on these insights, we developed a compact inventory consisting of statements that not only effectively represent each trust factor but also exhibit high item discrimination. We further validated our inventory through two trust games with real-world stakes, demonstrating that our measures reliably predict behavioral trust. Finally, we illustrate how this standardized inventory can be applied across diverse visualization research contexts. Utilizing our inventory, future research can examine how design choices, tasks, and domains influence trust, and how to foster appropriate trusting behavior in human-data interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17670v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Huichen Will Wang, Kylie Lin, Andrew Cohen, Ryan Kennedy, Zach Zwald, Carolina Nobre, Cindy Xiong Bearfield</dc:creator>
    </item>
    <item>
      <title>Human-AI Interaction and User Satisfaction: Empirical Evidence from Online Reviews of AI Products</title>
      <link>https://arxiv.org/abs/2503.17955</link>
      <description>arXiv:2503.17955v2 Announce Type: new 
Abstract: Human-AI Interaction (HAI) guidelines and design principles have become increasingly important in both industry and academia to guide the development of AI systems that align with user needs and expectations. However, large-scale empirical evidence on how HAI principles shape user satisfaction in practice remains limited. This study addresses that gap by analyzing over 100,000 user reviews of AI-related products from G2, a leading review platform for business software and services. Based on widely adopted industry guidelines, we identify seven core HAI dimensions and examine their coverage and sentiment within the reviews. We find that the sentiment on four HAI dimensions-adaptability, customization, error recovery, and security-is positively associated with overall user satisfaction. Moreover, we show that engagement with HAI dimensions varies by professional background: Users with technical job roles are more likely to discuss system-focused aspects, such as reliability, while non-technical users emphasize interaction-focused features like customization and feedback. Interestingly, the relationship between HAI sentiment and overall satisfaction is not moderated by job role, suggesting that once an HAI dimension has been identified by users, its effect on satisfaction is consistent across job roles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17955v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Stefan Pasch, Sun-Young Ha</dc:creator>
    </item>
    <item>
      <title>Generating Multimodal Textures with a Soft Hydro-Pneumatic Haptic Ring</title>
      <link>https://arxiv.org/abs/2503.17971</link>
      <description>arXiv:2503.17971v1 Announce Type: new 
Abstract: The growing adoption of extended reality, XR, has driven demand for wearable technologies that can replicate natural tactile sensations and allow users to interact freely with their surroundings using bare fingers. However, most existing wearable haptic technologies that support such free interactions can deliver sensations across limited tactile modalities. Here, we introduce a soft haptic ring and a data-driven rendering methodology to generate multimodal texture sensations. The device integrates pneumatic and hydraulic actuation to simulate roughness, thermal, and softness cues on the proximal phalanx, enabling users to explore surroundings naturally with their fingertips. The rendering methodology dynamically modulates those cues based on the user's exploratory actions. We validated our approach by conducting a user study with fifteen participants, who matched six virtual textures generated by the ring to their real counterparts and rated their perceived sensations. Participants achieved up to ninety percent accuracy in texture matching. The adjective ratings confirmed that the ring delivers distinct, perceptually rich stimuli across all rendered sensations. These findings highlight the ring's potential for immersive XR applications, offering diverse tactile feedback without restricting physical interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17971v1</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.2139/ssrn.5170637</arxiv:DOI>
      <dc:creator>Ana Sanz Cozcolluela, Yasemin Vardar</dc:creator>
    </item>
    <item>
      <title>Predicting Multitasking in Manual and Automated Driving with Optimal Supervisory Control</title>
      <link>https://arxiv.org/abs/2503.17993</link>
      <description>arXiv:2503.17993v1 Announce Type: new 
Abstract: Modern driving involves interactive technologies that can divert attention, increasing the risk of accidents. This paper presents a computational cognitive model that simulates human multitasking while driving. Based on optimal supervisory control theory, the model predicts how multitasking adapts to variations in driving demands, interactive tasks, and automation levels. Unlike previous models, it accounts for context-dependent multitasking across different degrees of driving automation. The model predicts longer in-car glances on straight roads and shorter glances during curves. It also anticipates increased glance durations with driver aids such as lane-centering assistance and their interaction with environmental demands. Validated against two empirical datasets, the model offers insights into driver multitasking amid evolving in-car technologies and automation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17993v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jussi Jokinen, Patrick Ebel, Tuomo Kujala</dc:creator>
    </item>
    <item>
      <title>A Robot-Led Intervention for Emotion Regulation: From Expression to Reappraisal</title>
      <link>https://arxiv.org/abs/2503.18243</link>
      <description>arXiv:2503.18243v1 Announce Type: new 
Abstract: Emotion regulation is a crucial skill for managing emotions in everyday life, yet finding a constructive and accessible method to support these processes remains challenging due to their cognitive demands. In this study, we explore how regular interactions with a social robot, conducted in a structured yet familiar environment within university halls and departments, can provide effective support for emotion regulation through cognitive reappraisal. Twenty-one students participated in a five-session study at a university hall or department, where the robot facilitated structured conversations, encouraging the students to reinterpret emotionally charged situations that they shared with the robot. Quantitative and qualitative results indicate significant improvements in emotion self-regulation, with participants reporting better understanding and control of their emotions. The intervention led to significant changes in constructive emotion regulation tendencies and positive effects on mood and sentiment after each session. The findings also demonstrate that repeated interactions with the robot encouraged greater emotional expressiveness, including longer speech disclosures, increased use of affective language, and heightened facial arousal. Notably, expressiveness followed structured patterns aligned with the reappraisal process, with expression peaking during key reappraisal moments, particularly when participants were prompted to reinterpret negative experiences. The qualitative feedback further highlighted how the robot fostered introspection and provided a supportive space for discussing emotions, enabling participants to confront long-avoided emotional challenges. These findings demonstrate the potential of robots to effectively assist in emotion regulation in familiar environments, offering both emotional support and cognitive guidance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18243v1</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guy Laban, Julie Wang, Hatice Gunes</dc:creator>
    </item>
    <item>
      <title>How to Capture and Study Conversations Between Research Participants and ChatGPT: GPT for Researchers (g4r.org)</title>
      <link>https://arxiv.org/abs/2503.18303</link>
      <description>arXiv:2503.18303v1 Announce Type: new 
Abstract: As large language models (LLMs) like ChatGPT become increasingly integrated into our everyday lives--from customer service and education to creative work and personal productivity--understanding how people interact with these AI systems has become a pressing issue. Despite the widespread use of LLMs, researchers lack standardized tools for systematically studying people's interactions with LLMs. To address this issue, we introduce GPT for Researchers (G4R), or g4r.org, a free website that researchers can use to easily create and integrate a GPT Interface into their studies. At g4r.org, researchers can (1) enable their study participants to interact with GPT (such as ChatGPT), (2) customize GPT Interfaces to guide participants' interactions with GPT (e.g., set constraints on topics or adjust GPT's tone or response style), and (3) capture participants' interactions with GPT by downloading data on messages exchanged between participants and GPT. By facilitating study participants' interactions with GPT and providing detailed data on these interactions, G4R can support research on topics such as consumer interactions with AI agents or LLMs, AI-assisted decision-making, and linguistic patterns in human-AI communication. With this goal in mind, we provide a step-by-step guide to using G4R at g4r.org.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18303v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jin Kim</dc:creator>
    </item>
    <item>
      <title>Generative AI in Knowledge Work: Design Implications for Data Navigation and Decision-Making</title>
      <link>https://arxiv.org/abs/2503.18419</link>
      <description>arXiv:2503.18419v1 Announce Type: new 
Abstract: Our study of 20 knowledge workers revealed a common challenge: the difficulty of synthesizing unstructured information scattered across multiple platforms to make informed decisions. Drawing on their vision of an ideal knowledge synthesis tool, we developed Yodeai, an AI-enabled system, to explore both the opportunities and limitations of AI in knowledge work. Through a user study with 16 product managers, we identified three key requirements for Generative AI in knowledge work: adaptable user control, transparent collaboration mechanisms, and the ability to integrate background knowledge with external information. However, we also found significant limitations, including overreliance on AI, user isolation, and contextual factors outside the AI's reach. As AI tools become increasingly prevalent in professional settings, we propose design principles that emphasize adaptability to diverse workflows, accountability in personal and collaborative contexts, and context-aware interoperability to guide the development of human-centered AI systems for product managers and knowledge workers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18419v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713337</arxiv:DOI>
      <dc:creator>Bhada Yun, Dana Feng, Ace S. Chen, Afshin Nikzad, Niloufar Salehi</dc:creator>
    </item>
    <item>
      <title>Safeguarding Mobile GUI Agent via Logic-based Action Verification</title>
      <link>https://arxiv.org/abs/2503.18492</link>
      <description>arXiv:2503.18492v1 Announce Type: new 
Abstract: Large Foundation Models (LFMs) have unlocked new possibilities in human-computer interaction, particularly with the rise of mobile Graphical User Interface (GUI) Agents capable of interpreting GUIs. These agents promise to revolutionize mobile computing by allowing users to automate complex mobile tasks through simple natural language instructions. However, the inherent probabilistic nature of LFMs, coupled with the ambiguity and context-dependence of mobile tasks, makes LFM-based automation unreliable and prone to errors. To address this critical challenge, we introduce VeriSafe Agent (VSA): a formal verification system that serves as a logically grounded safeguard for Mobile GUI Agents. VSA is designed to deterministically ensure that an agent's actions strictly align with user intent before conducting an action. At its core, VSA introduces a novel autoformalization technique that translates natural language user instructions into a formally verifiable specification, expressed in our domain-specific language (DSL). This enables runtime, rule-based verification, allowing VSA to detect and prevent erroneous actions executing an action, either by providing corrective feedback or halting unsafe behavior. To the best of our knowledge, VSA is the first attempt to bring the rigor of formal verification to GUI agent. effectively bridging the gap between LFM-driven automation and formal software verification. We implement VSA using off-the-shelf LLM services (GPT-4o) and evaluate its performance on 300 user instructions across 18 widely used mobile apps. The results demonstrate that VSA achieves 94.3%-98.33% accuracy in verifying agent actions, representing a significant 20.4%-25.6% improvement over existing LLM-based verification methods, and consequently increases the GUI agent's task completion rate by 90%-130%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18492v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jungjae Lee, Dongjae Lee, Chihun Choi, Youngmin Im, Jaeyoung Wi, Kihong Heo, Sangeun Oh, Sunjae Lee, Insik Shin</dc:creator>
    </item>
    <item>
      <title>"Becoming My Own Audience": How Dancers React to Avatars Unlike Themselves in Motion Capture-Supported Live Improvisational Performance</title>
      <link>https://arxiv.org/abs/2503.18606</link>
      <description>arXiv:2503.18606v1 Announce Type: new 
Abstract: The use of motion capture in live dance performances has created an emerging discipline enabling dancers to play different avatars on the digital stage. Unlike classical workflows, avatars enable performers to act as different characters in customized narratives, but research has yet to address how movement, improvisation, and perception change when dancers act as avatars. We created five avatars representing differing genders, shapes, and body limitations, and invited 15 dancers to improvise with each in practice and performance settings. Results show that dancers used avatars to distance themselves from their own habitual movements, exploring new ways of moving through differing physical constraints. Dancers explored using gender-stereotyped movements like powerful or feminine actions, experimenting with gender identity. However, focusing on avatars can coincide with a lack of continuity in improvisation. This work shows how emerging practices with performance technology enable dancers to improvise with new constraints, stepping outside the classical stage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18606v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713390</arxiv:DOI>
      <dc:creator>Fan Zhang, Molin Li, Xiaoyu Chang, Kexue Fu, Richard William Allen, RAY LC</dc:creator>
    </item>
    <item>
      <title>Reading Decisions from Gaze Direction during Graphics Turing Test of Gait Animation</title>
      <link>https://arxiv.org/abs/2503.18619</link>
      <description>arXiv:2503.18619v1 Announce Type: new 
Abstract: We investigated gaze direction during movement observation. The eye movement data were collected during an experiment, in which different models of movement production (based on movement primitives, MPs) were compared in a two alternatives forced choice task (2AFC).
  Participants observed side-by-side presentation of two naturalistic 3D-rendered human movement videos, where one video was based on motion captured gait sequence, the other one was generated by recombining the machine-learned MPs to approximate the same movement. The task was to discriminate between these movements while their eye movements were recorded. We are complementing previous binary decision data analyses with eye tracking data. Here, we are investigating the role of gaze direction during task execution. We computed the shared information between gaze features and decisions of the participants, and between gaze features and correct answers.
  We found that eye movements reflect the decision of participants during the 2AFC task, but not the correct answer. This result is important for future experiments, which should take advantage of eye tracking to complement binary decision data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18619v1</guid>
      <category>cs.HC</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Knopp, Daniel Auras, Alexander C. Sch\"utz, Dominik Endres</dc:creator>
    </item>
    <item>
      <title>Two Types of Data Privacy Controls</title>
      <link>https://arxiv.org/abs/2503.18729</link>
      <description>arXiv:2503.18729v1 Announce Type: new 
Abstract: Users share a vast amount of data while using web and mobile applications. Most service providers such as email and social media providers provide users with privacy controls, which aim to give users the means to control what, how, when, and with whom, users share data. Nevertheless, it is not uncommon to hear users say that they feel they have lost control over their data on the web.
  This article aims to shed light on the often overlooked difference between two main types of privacy from a control perspective: privacy between a user and other users, and privacy between a user and institutions. We argue why this difference is important and what we need to do from here.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18729v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eman Alashwali</dc:creator>
    </item>
    <item>
      <title>Group Decision-Making System with Sentiment Analysis of Discussion Chat and Fuzzy Consensus Modeling</title>
      <link>https://arxiv.org/abs/2503.18765</link>
      <description>arXiv:2503.18765v1 Announce Type: new 
Abstract: Group Decision-Making (GDM) plays a crucial role in various real-life scenarios where individuals express their opinions in natural language rather than structured numerical values. Traditional GDM approaches often overlook the subjectivity and ambiguity present in human discussions, making it challenging to achieve a fair and consensus-driven decision. This paper proposes a fuzzy consensus-based group decision-making system that integrates sentiment and emotion analysis to extract preference values from textual inputs. The proposed framework combines explicit voting preferences with sentiment scores derived from chat discussions, which are then processed using a Fuzzy Inference System (FIS) to compute a total preference score for each alternative and determine the top-ranked option. To ensure fairness in group decision-making, we introduce a fuzzy logic-based consensus measurement model that evaluates participants' agreement and confidence levels to assess overall feedback. To illustrate the effectiveness of our approach, we apply the methodology to a restaurant selection scenario, where a group of individuals must decide on a dining option based on brief chat discussions. The results demonstrate that the fuzzy consensus mechanism successfully aggregates individual preferences and ensures a balanced outcome that accurately reflects group sentiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18765v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adilet Yerkin, Pakizar Shamoi</dc:creator>
    </item>
    <item>
      <title>REALM: A Dataset of Real-World LLM Use Cases</title>
      <link>https://arxiv.org/abs/2503.18792</link>
      <description>arXiv:2503.18792v1 Announce Type: new 
Abstract: Large Language Models, such as the GPT series, have driven significant industrial applications, leading to economic and societal transformations. However, a comprehensive understanding of their real-world applications remains limited. To address this, we introduce REALM, a dataset of over 94,000 LLM use cases collected from Reddit and news articles. REALM captures two key dimensions: the diverse applications of LLMs and the demographics of their users. It categorizes LLM applications and explores how users' occupations relate to the types of applications they use. By integrating real-world data, REALM offers insights into LLM adoption across different domains, providing a foundation for future research on their evolving societal roles. A dedicated dashboard https://realm-e7682.web.app/ presents the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18792v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingwen Cheng, Kshitish Ghate, Wenyue Hua, William Yang Wang, Hong Shen, Fei Fang</dc:creator>
    </item>
    <item>
      <title>Artificial Intelligence Can Emulate Human Normative Judgments on Emotional Visual Scenes</title>
      <link>https://arxiv.org/abs/2503.18796</link>
      <description>arXiv:2503.18796v1 Announce Type: new 
Abstract: Affective reactions have deep biological foundations, however in humans the development of emotion concepts is also shaped by language and higher-order cognition. A recent breakthrough in AI has been the creation of multimodal language models that exhibit impressive intellectual capabilities, but their responses to affective stimuli have not been investigated. Here we study whether state-of-the-art multimodal systems can emulate human emotional ratings on a standardized set of images, in terms of affective dimensions and basic discrete emotions. The AI judgements correlate surprisingly well with the average human ratings: given that these systems were not explicitly trained to match human affective reactions, this suggests that the ability to visually judge emotional content can emerge from statistical learning over large-scale databases of images paired with linguistic descriptions. Besides showing that language can support the development of rich emotion concepts in AI, these findings have broad implications for sensitive use of multimodal AI technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18796v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zaira Romeo, Alberto Testolin</dc:creator>
    </item>
    <item>
      <title>Virtual Reality in Manufacturing Education: A Scoping Review Indicating State-of-the-Art, Benefits, and Challenges Across Domains, Levels, and Entities</title>
      <link>https://arxiv.org/abs/2503.18805</link>
      <description>arXiv:2503.18805v2 Announce Type: new 
Abstract: To address the shortage of a skilled workforce in the U.S. manufacturing industry, immersive Virtual Reality (VR)-based training solutions hold promising potential. To effectively utilize VR to meet workforce demands, it is important to understand the role of VR in manufacturing education. Therefore, we conduct a scoping review in the field. As a first step, we used a 5W1H (What, Where, Who, When, Why, How) formula as a problem-solving approach to define a comprehensive taxonomy that can consider the role of VR from all relevant possibilities. Our taxonomy categorizes VR applications across three key aspects: (1) Domains, (2) Levels, and (3) Entities. Using a systematic literature search and analysis, we reviewed 108 research articles to find the current state, benefits, challenges, and future opportunities of VR in the field. It was found that VR has been explored in a variety of areas and provides numerous benefits to learners. Despite these benefits, its adoption in manufacturing education is limited. This review discusses the identified barriers and provides actionable insights to address them. These insights can enable the widespread usage of immersive technology to nurture and develop a workforce equipped with the skills required to excel in the evolving landscape of manufacturing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18805v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ananya Ipsita, Ramesh Kaki, Ziyi Liu, Mayank Patel, Runlin Duan, Lakshmi Deshpande, Lin-Ping Yuan, Victoria Lowell, Ashok Maharaj, Kylie Peppler, Steven Feiner, Karthik Ramani</dc:creator>
    </item>
    <item>
      <title>Intanify AI Platform: Embedded AI for Automated IP Audit and Due Diligence</title>
      <link>https://arxiv.org/abs/2503.17374</link>
      <description>arXiv:2503.17374v1 Announce Type: cross 
Abstract: In this paper we introduce a Platform created in order to support SMEs' endeavor to extract value from their intangible assets effectively. To implement the Platform, we developed five knowledge bases using a knowledge-based ex-pert system shell that contain knowledge from intangible as-set consultants, patent attorneys and due diligence lawyers. In order to operationalize the knowledge bases, we developed a "Rosetta Stone", an interpreter unit for the knowledge bases outside the shell and embedded in the plat-form. Building on the initial knowledge bases we have created a system of red flags, risk scoring, and valuation with the involvement of the same experts; these additional systems work upon the initial knowledge bases and therefore they can be regarded as meta-knowledge-representations that take the form of second-order knowledge graphs. All this clever technology is dressed up in an easy-to-handle graphical user interface that we will showcase at the conference. The initial platform was finished mid-2024; therefore, it qualifies as an "emerging application of AI" and "deployable AI", while development continues. The two firms that provided experts for developing the knowledge bases obtained a white-label version of the product (i.e. it runs under their own brand "powered by Intanify"), and there are two completed cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17374v1</guid>
      <category>cs.CY</category>
      <category>cs.CE</category>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Dorfler, V., Dryden, D., &amp; Lee, V. (2025, 25 February - 4 March 2025). Intanify AI Platform: Embedded AI for Automated IP Audit and Due Diligence AAAI 2025: The 39th Annual AAAI Conference on Artificial Intelligence, Philadelphia, PA</arxiv:journal_reference>
      <dc:creator>Viktor Dorfler, Dylan Dryden, Viet Lee</dc:creator>
    </item>
    <item>
      <title>AEJIM: A Real-Time AI Framework for Crowdsourced, Transparent, and Ethical Environmental Hazard Detection and Reporting</title>
      <link>https://arxiv.org/abs/2503.17401</link>
      <description>arXiv:2503.17401v1 Announce Type: cross 
Abstract: Environmental journalism is vital for raising awareness of ecological crises and driving evidence-based policy, yet traditional methods falter under delays, inaccuracies, and scalability limits, especially in under-monitored regions critical to the United Nations Sustainable Development Goals. To bridge these gaps, this paper introduces the AI-Environmental Journalism Integration Model (AEJIM), an innovative framework combining real-time hazard detection, crowdsourced validation, and AI-driven reporting.
  Validated through a pilot study, AEJIM significantly improved the speed and accuracy of environmental hazard reporting, outperforming traditional methods. Furthermore, the model directly addresses key ethical, regulatory, and scalability challenges, ensuring AI accountability through Explainable AI (XAI), GDPR-compliant data governance, and active public participation. AEJIM provides a transparent and adaptable solution, setting a new benchmark for AI-enhanced environmental journalism and supporting informed global decision-making across diverse socio-political landscapes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17401v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Torsten Tiltack</dc:creator>
    </item>
    <item>
      <title>What's Producible May Not Be Reachable: Measuring the Steerability of Generative Models</title>
      <link>https://arxiv.org/abs/2503.17482</link>
      <description>arXiv:2503.17482v1 Announce Type: cross 
Abstract: How should we evaluate the quality of generative models? Many existing metrics focus on a model's producibility, i.e. the quality and breadth of outputs it can generate. However, the actual value from using a generative model stems not just from what it can produce but whether a user with a specific goal can produce an output that satisfies that goal. We refer to this property as steerability. In this paper, we first introduce a mathematical framework for evaluating steerability independently from producibility. Steerability is more challenging to evaluate than producibility because it requires knowing a user's goals. We address this issue by creating a benchmark task that relies on one key idea: sample an output from a generative model and ask users to reproduce it. We implement this benchmark in a large-scale user study of text-to-image models and large language models. Despite the ability of these models to produce high-quality outputs, they all perform poorly on steerabilty. This suggests that we need to focus on improving the steerability of generative models. We show such improvements are indeed possible: through reinforcement learning techniques, we create an alternative steering mechanism for image models that achieves more than 2x improvement on this benchmark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17482v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keyon Vafa, Sarah Bentley, Jon Kleinberg, Sendhil Mullainathan</dc:creator>
    </item>
    <item>
      <title>Autonomous Radiotherapy Treatment Planning Using DOLA: A Privacy-Preserving, LLM-Based Optimization Agent</title>
      <link>https://arxiv.org/abs/2503.17553</link>
      <description>arXiv:2503.17553v1 Announce Type: cross 
Abstract: Radiotherapy treatment planning is a complex and time-intensive process, often impacted by inter-planner variability and subjective decision-making. To address these challenges, we introduce Dose Optimization Language Agent (DOLA), an autonomous large language model (LLM)-based agent designed for optimizing radiotherapy treatment plans while rigorously protecting patient privacy. DOLA integrates the LLaMa3.1 LLM directly with a commercial treatment planning system, utilizing chain-of-thought prompting, retrieval-augmented generation (RAG), and reinforcement learning (RL). Operating entirely within secure local infrastructure, this agent eliminates external data sharing. We evaluated DOLA using a retrospective cohort of 18 prostate cancer patients prescribed 60 Gy in 20 fractions, comparing model sizes (8 billion vs. 70 billion parameters) and optimization strategies (No-RAG, RAG, and RAG+RL) over 10 planning iterations. The 70B model demonstrated significantly improved performance, achieving approximately 16.4% higher final scores than the 8B model. The RAG approach outperformed the No-RAG baseline by 19.8%, and incorporating RL accelerated convergence, highlighting the synergy of retrieval-based memory and reinforcement learning. Optimal temperature hyperparameter analysis identified 0.4 as providing the best balance between exploration and exploitation. This proof of concept study represents the first successful deployment of locally hosted LLM agents for autonomous optimization of treatment plans within a commercial radiotherapy planning system. By extending human-machine interaction through interpretable natural language reasoning, DOLA offers a scalable and privacy-conscious framework, with significant potential for clinical implementation and workflow improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17553v1</guid>
      <category>physics.med-ph</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Humza Nusrat (Department of Radiation Oncology, Henry Ford Health, Detroit, USA, College of Human Medicine, Michigan State University, East Lansing, USA), Bing Luo (Department of Radiation Oncology, Henry Ford Health, Detroit, USA), Ryan Hall (Department of Radiation Oncology, Henry Ford Health, Detroit, USA), Joshua Kim (Department of Radiation Oncology, Henry Ford Health, Detroit, USA), Hassan Bagher-Ebadian (Department of Radiation Oncology, Henry Ford Health, Detroit, USA, College of Human Medicine, Michigan State University, East Lansing, USA), Anthony Doemer (Department of Radiation Oncology, Henry Ford Health, Detroit, USA), Benjamin Movsas (Department of Radiation Oncology, Henry Ford Health, Detroit, USA, College of Human Medicine, Michigan State University, East Lansing, USA), Kundan Thind (Department of Radiation Oncology, Henry Ford Health, Detroit, USA, College of Human Medicine, Michigan State University, East Lansing, USA)</dc:creator>
    </item>
    <item>
      <title>AI-Based Screening for Depression and Social Anxiety Through Eye Tracking: An Exploratory Study</title>
      <link>https://arxiv.org/abs/2503.17625</link>
      <description>arXiv:2503.17625v1 Announce Type: cross 
Abstract: Well-being is a dynamic construct that evolves over time and fluctuates within individuals, presenting challenges for accurate quantification. Reduced well-being is often linked to depression or anxiety disorders, which are characterised by biases in visual attention towards specific stimuli, such as human faces. This paper introduces a novel approach to AI-assisted screening of affective disorders by analysing visual attention scan paths using convolutional neural networks (CNNs). Data were collected from two studies examining (1) attentional tendencies in individuals diagnosed with major depression and (2) social anxiety. These data were processed using residual CNNs through images generated from eye-gaze patterns. Experimental results, obtained with ResNet architectures, demonstrated an average accuracy of 48% for a three-class system and 62% for a two-class system. Based on these exploratory findings, we propose that this method could be employed in rapid, ecological, and effective mental health screening systems to assess well-being through eye-tracking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17625v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.54663/2182-9306.2024.SpecialIssueMBP.75-91</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Marketing, Communication and New Media, Special Issue on Marketing &amp; Business Perspectives: Fostering AI as a Tool for Wellbeing, December 2024, pp. 55-91</arxiv:journal_reference>
      <dc:creator>Karol Chlasta, Katarzyna Wisiecka, Krzysztof Krejtz, Izabela Krejtz</dc:creator>
    </item>
    <item>
      <title>DeepFund: Will LLM be Professional at Fund Investment? A Live Arena Perspective</title>
      <link>https://arxiv.org/abs/2503.18313</link>
      <description>arXiv:2503.18313v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities across various domains, but their effectiveness in financial decision making, particularly in fund investment, remains inadequately evaluated. Current benchmarks primarily assess LLMs understanding of financial documents rather than their ability to manage assets or analyze trading opportunities in dynamic market conditions. A critical limitation in existing evaluation methodologies is the backtesting approach, which suffers from information leakage when LLMs are evaluated on historical data they may have encountered during pretraining. This paper introduces DeepFund, a comprehensive platform for evaluating LLM based trading strategies in a simulated live environment. Our approach implements a multi agent framework where LLMs serve as both analysts and managers, creating a realistic simulation of investment decision making. The platform employs a forward testing methodology that mitigates information leakage by evaluating models on market data released after their training cutoff dates. We provide a web interface that visualizes model performance across different market conditions and investment parameters, enabling detailed comparative analysis. Through DeepFund, we aim to provide a more accurate and fair assessment of LLMs capabilities in fund investment, offering insights into their potential real world applications in financial markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18313v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changlun Li, Yao Shi, Yuyu Luo, Nan Tang</dc:creator>
    </item>
    <item>
      <title>Rise of the Community Champions: From Reviewer Crunch to Community Power</title>
      <link>https://arxiv.org/abs/2503.18336</link>
      <description>arXiv:2503.18336v1 Announce Type: cross 
Abstract: Academic publishing is facing a crisis driven by exponential growth in submissions and an overwhelmed peer review system, leading to inconsistent decisions and a severe reviewer shortage. This paper introduces Panvas, a platform that reimagines academic publishing as a continuous, community-driven process. Panvas addresses these systemic failures with a novel combination of economic incentives (paid reviews) and rich interaction mechanisms (multi-dimensional ratings, threaded discussions, and expert-led reviews). By moving beyond the traditional accept/reject paradigm and integrating paper hosting with code/data repositories and social networking, Panvas fosters a meritocratic environment for scholarly communication and presents a radical rethinking of how we evaluate and disseminate scientific knowledge. We present the system design, development roadmap, and a user study plan to evaluate its effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18336v1</guid>
      <category>cs.SI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changlun Li, Yao Shi, Yuyu Luo, Nan Tang</dc:creator>
    </item>
    <item>
      <title>Manipulation and the AI Act: Large Language Model Chatbots and the Danger of Mirrors</title>
      <link>https://arxiv.org/abs/2503.18387</link>
      <description>arXiv:2503.18387v1 Announce Type: cross 
Abstract: Large Language Model chatbots are increasingly taking the form and visage of human beings, adapting human faces, names, voices, personalities, and quirks, including those of celebrities and well-known political figures. Personifying AI chatbots could foreseeably increase their trust with users. However, it could also make them more capable of manipulation, by creating the illusion of a close and intimate relationship with an artificial entity. The European Commission has finalized the AI Act, with the EU Parliament making amendments banning manipulative and deceptive AI systems that cause significant harm to users. Although the AI Act covers harms that accumulate over time, it is unlikely to prevent harms associated with prolonged discussions with AI chatbots. Specifically, a chatbot could reinforce a person's negative emotional state over weeks, months, or years through negative feedback loops, prolonged conversations, or harmful recommendations, contributing to a user's deteriorating mental health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18387v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Krook</dc:creator>
    </item>
    <item>
      <title>Words as Bridges: Exploring Computational Support for Cross-Disciplinary Translation Work</title>
      <link>https://arxiv.org/abs/2503.18471</link>
      <description>arXiv:2503.18471v1 Announce Type: cross 
Abstract: Scholars often explore literature outside of their home community of study. This exploration process is frequently hampered by field-specific jargon. Past computational work often focuses on supporting translation work by removing jargon through simplification and summarization; here, we explore a different approach that preserves jargon as useful bridges to new conceptual spaces. Specifically, we cast different scholarly domains as different language-using communities, and explore how to adapt techniques from unsupervised cross-lingual alignment of word embeddings to explore conceptual alignments between domain-specific word embedding spaces.We developed a prototype cross-domain search engine that uses aligned domain-specific embeddings to support conceptual exploration, and tested this prototype in two case studies. We discuss qualitative insights into the promises and pitfalls of this approach to translation work, and suggest design insights for future interfaces that provide computational support for cross-domain information seeking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18471v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3708359.3712110</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 30th International Conference on Intelligent User Interfaces (2025) p.1598-1623</arxiv:journal_reference>
      <dc:creator>Calvin Bao, Yow-Ting Shiue, Marine Carpuat, Joel Chan</dc:creator>
    </item>
    <item>
      <title>The (Un)suitability of Passwords and Password Managers in Virtual Reality</title>
      <link>https://arxiv.org/abs/2503.18550</link>
      <description>arXiv:2503.18550v1 Announce Type: cross 
Abstract: As Virtual Reality (VR) expands into fields like healthcare and education, ensuring secure and user-friendly authentication becomes essential. Traditional password entry methods in VR are cumbersome and insecure, making password managers (PMs) a potential solution. To explore this field, we conducted a user study (n=126 VR users) where participants expressed a strong preference for simpler passwords and showed interest in biometric authentication and password managers. On these grounds, we provide the first in-depth evaluation of PMs in VR. We report findings from 91 cognitive walkthroughs, revealing that while PMs improve usability, they are not yet ready for prime time. Key features like cross-app autofill are missing, and user experiences highlight the need for better solutions. Based on consolidated user views and expert analysis, we make recommendations on how to move forward in improving VR authentication systems, ultimately creating more practical solutions for this growing field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18550v1</guid>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emiram Kablo, Yorick Last, Patricia Arias Cabarcos, Melanie Volkamer</dc:creator>
    </item>
    <item>
      <title>Self-Reported Confidence of Large Language Models in Gastroenterology: Analysis of Commercial, Open-Source, and Quantized Models</title>
      <link>https://arxiv.org/abs/2503.18562</link>
      <description>arXiv:2503.18562v1 Announce Type: cross 
Abstract: This study evaluated self-reported response certainty across several large language models (GPT, Claude, Llama, Phi, Mistral, Gemini, Gemma, and Qwen) using 300 gastroenterology board-style questions. The highest-performing models (GPT-o1 preview, GPT-4o, and Claude-3.5-Sonnet) achieved Brier scores of 0.15-0.2 and AUROC of 0.6. Although newer models demonstrated improved performance, all exhibited a consistent tendency towards overconfidence. Uncertainty estimation presents a significant challenge to the safe use of LLMs in healthcare. Keywords: Large Language Models; Confidence Elicitation; Artificial Intelligence; Gastroenterology; Uncertainty Quantification</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18562v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nariman Naderi, Seyed Amir Ahmad Safavi-Naini, Thomas Savage, Zahra Atf, Peter Lewis, Girish Nadkarni, Ali Soroush</dc:creator>
    </item>
    <item>
      <title>The case for delegated AI autonomy for Human AI teaming in healthcare</title>
      <link>https://arxiv.org/abs/2503.18778</link>
      <description>arXiv:2503.18778v1 Announce Type: cross 
Abstract: In this paper we propose an advanced approach to integrating artificial intelligence (AI) into healthcare: autonomous decision support. This approach allows the AI algorithm to act autonomously for a subset of patient cases whilst serving a supportive role in other subsets of patient cases based on defined delegation criteria. By leveraging the complementary strengths of both humans and AI, it aims to deliver greater overall performance than existing human-AI teaming models. It ensures safe handling of patient cases and potentially reduces clinician review time, whilst being mindful of AI tool limitations. After setting the approach within the context of current human-AI teaming models, we outline the delegation criteria and apply them to a specific AI-based tool used in histopathology. The potential impact of the approach and the regulatory requirements for its successful implementation are then discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18778v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yan Jia, Harriet Evans, Zoe Porter, Simon Graham, John McDermid, Tom Lawton, David Snead, Ibrahim Habli</dc:creator>
    </item>
    <item>
      <title>GhostWriter: Augmenting Collaborative Human-AI Writing Experiences Through Personalization and Agency</title>
      <link>https://arxiv.org/abs/2402.08855</link>
      <description>arXiv:2402.08855v2 Announce Type: replace 
Abstract: Writing is a well-established practice to support ideation and creativity. While Large Language Models (LLMs) have become ubiquitous in providing different kinds of writing assistance to different writers, LLM-powered writing systems often fall short in capturing the nuanced personalization and control necessary for effective support and creative exploration. To address these challenges, we introduce GhostWriter, an AI-enhanced writing design probe that enables users to exercise enhanced agency and personalization. GhostWriter leverages LLMs to implicitly learn the user's intended writing style for seamless personalization, while exposing explicit teaching moments for style refinement and reflection. We study 18 participants who use GhostWriter for editing and creative tasks, observing that it helps users craft personalized text and empowers them by providing multiple ways to steer system output. Based on this study, we present insights on people's relationships with AI-assisted writing and offer design recommendations to promote user agency in similar co-creative systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08855v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Catherine Yeh, Gonzalo Ramos, Rachel Ng, Andy Huntington, Richard Banks</dc:creator>
    </item>
    <item>
      <title>AutoTRIZ: Automating Engineering Innovation with TRIZ and Large Language Models</title>
      <link>https://arxiv.org/abs/2403.13002</link>
      <description>arXiv:2403.13002v4 Announce Type: replace 
Abstract: Various ideation methods, such as morphological analysis and design-by-analogy, have been developed to aid creative problem-solving and innovation. Among them, the Theory of Inventive Problem Solving (TRIZ) stands out as one of the best-known methods. However, the complexity of TRIZ and its reliance on users' knowledge, experience, and reasoning capabilities limit its practicality. To address this, we introduce AutoTRIZ, an artificial ideation system that integrates Large Language Models (LLMs) to automate and enhance the TRIZ methodology. By leveraging LLMs' vast pre-trained knowledge and advanced reasoning capabilities, AutoTRIZ offers a novel, generative, and interpretable approach to engineering innovation. AutoTRIZ takes a problem statement from the user as its initial input, automatically conduct the TRIZ reasoning process and generates a structured solution report. We demonstrate and evaluate the effectiveness of AutoTRIZ through comparative experiments with textbook cases and a real-world application in the design of a Battery Thermal Management System (BTMS). Moreover, the proposed LLM-based framework holds the potential for extension to automate other knowledge-based ideation methods, such as SCAMPER, Design Heuristics, and Design-by-Analogy, paving the way for a new era of AI-driven innovation tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13002v4</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shuo Jiang, Weifeng Li, Yuping Qian, Yangjun Zhang, Jianxi Luo</dc:creator>
    </item>
    <item>
      <title>From Interaction to Impact: Towards Safer AI Agents Through Understanding and Evaluating Mobile UI Operation Impacts</title>
      <link>https://arxiv.org/abs/2410.09006</link>
      <description>arXiv:2410.09006v2 Announce Type: replace 
Abstract: With advances in generative AI, there is increasing work towards creating autonomous agents that can manage daily tasks by operating user interfaces (UIs). While prior research has studied the mechanics of how AI agents might navigate UIs and understand UI structure, the effects of agents and their autonomous actions-particularly those that may be risky or irreversible-remain under-explored. In this work, we investigate the real-world impacts and consequences of mobile UI actions taken by AI agents. We began by developing a taxonomy of the impacts of mobile UI actions through a series of workshops with domain experts. Following this, we conducted a data synthesis study to gather realistic mobile UI screen traces and action data that users perceive as impactful. We then used our impact categories to annotate our collected data and data repurposed from existing mobile UI navigation datasets. Our quantitative evaluations of different large language models (LLMs) and variants demonstrate how well different LLMs can understand the impacts of mobile UI actions that might be taken by an agent. We show that our taxonomy enhances the reasoning capabilities of these LLMs for understanding the impacts of mobile UI actions, but our findings also reveal significant gaps in their ability to reliably classify more nuanced or complex categories of impact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09006v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3708359.3712153</arxiv:DOI>
      <dc:creator>Zhuohao Jerry Zhang, Eldon Schoop, Jeffrey Nichols, Anuj Mahajan, Amanda Swearngin</dc:creator>
    </item>
    <item>
      <title>Bridging the Early Science Gap with Artificial Intelligence: Evaluating Large Language Models as Tools for Early Childhood Science Education</title>
      <link>https://arxiv.org/abs/2501.01192</link>
      <description>arXiv:2501.01192v3 Announce Type: replace 
Abstract: Early childhood science education is crucial for developing scientific literacy, yet translating complex scientific concepts into age-appropriate content remains challenging for educators. Our study evaluates four leading Large Language Models (LLMs) - GPT-4, Claude, Gemini, and Llama - on their ability to generate preschool-appropriate scientific explanations across biology, chemistry, and physics. Through systematic evaluation by 30 nursery teachers using established pedagogical criteria, we identify significant differences in the models' capabilities to create engaging, accurate, and developmentally appropriate content. Unexpectedly, Claude outperformed other models, particularly in biological topics, while all LLMs struggled with abstract chemical concepts. Our findings provide practical insights for educators leveraging AI in early science education and offer guidance for developers working to enhance LLMs' educational applications. The results highlight the potential and current limitations of using LLMs to bridge the early childhood science literacy gap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01192v3</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706599.3721261</arxiv:DOI>
      <arxiv:journal_reference>Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, 2025, Yokohama, Japan. ACM, New York, NY, USA, 6 pages</arxiv:journal_reference>
      <dc:creator>Annika Bush, Amin Alibakhshi</dc:creator>
    </item>
    <item>
      <title>To Google or To ChatGPT? A Comparison of CS2 Students' Information Gathering Approaches and Outcomes</title>
      <link>https://arxiv.org/abs/2501.11935</link>
      <description>arXiv:2501.11935v3 Announce Type: replace 
Abstract: LLMs such as ChatGPT have been widely adopted by students in higher education as tools for learning programming and related concepts. However, it remains unclear how effective students are and what strategies students use while learning with LLMs. Since the majority of students' experiences in online self-learning have come through using search engines such as Google, evaluating AI tools in this context can help us address these gaps. In this mixed methods research, we conducted an exploratory within-subjects study to understand how CS2 students learn programming concepts using both LLMs as well as traditional online methods such as educational websites and videos to examine how students approach learning within and across both scenarios. We discovered that students found it easier to learn a more difficult concept using traditional methods than using ChatGPT. We also found that students ask fewer follow-ups and use more keyword-based queries for search engines while their prompts to LLMs tend to explicitly ask for information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11935v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aayush Kumar, Daniel Prol, Amin Alipour, Sruti Srinivasa Ragavan</dc:creator>
    </item>
    <item>
      <title>Eye Gaze as a Signal for Conveying User Attention in Contextual AI Systems</title>
      <link>https://arxiv.org/abs/2501.13878</link>
      <description>arXiv:2501.13878v2 Announce Type: replace 
Abstract: Advanced multimodal AI agents can now collaborate with users to solve challenges in the world. Yet, these emerging contextual AI systems rely on explicit communication channels between the user and system. We hypothesize that implicit communication of the user's interests and intent would reduce friction and improve user experience in contextual AI. In this work, we explore the potential of wearable eye tracking to convey user attention to the agents. We measure the eye tracking signal quality requirements to effectively map gaze traces to physical objects, then conduct experiments to provide visual scanpath history as additional context when querying multimodal agents. Our results show that eye tracking provides high value as a user attention signal and can convey information about the user's current task and interests to the agent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13878v2</guid>
      <category>cs.HC</category>
      <category>cs.CV</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ethan Wilson, Naveen Sendhilnathan, Charlie S. Burlingham, Yusuf Mansour, Robert Cavin, Sai Deep Tetali, Ajoy Savio Fernandes, Michael J. Proulx</dc:creator>
    </item>
    <item>
      <title>Robot Character Generation and Adaptive Human-Robot Interaction with Personality Shaping</title>
      <link>https://arxiv.org/abs/2503.15518</link>
      <description>arXiv:2503.15518v2 Announce Type: replace 
Abstract: We present a novel framework for designing emotionally agile robots with dynamic personalities and memory-based learning, with the aim of performing adaptive and non-deterministic interactions with humans while conforming to shared social understanding. While existing work has largely focused on emotion recognition and static response systems, many approaches rely on sentiment analysis and action mapping frameworks that are pre-defined with limited dimensionality and fixed configurations, lacking the flexibility of dynamic personality traits and memory-enabled adaptation. Other systems are often restricted to limited modes of expression and fail to develop a causal relationship between human behavior and the robot's proactive physical actions, resulting in constrained adaptability and reduced responsiveness in complex, dynamic interactions. Our methodology integrates the Big Five Personality Traits, Appraisal Theory, and abstracted memory layers through Large Language Models (LLMs). The LLM generates a parameterized robot personality based on the Big Five, processes human language and sentiments, evaluates human behavior using Appraisal Theory, and generates emotions and selects appropriate actions adapted by historical context over time. We validated the framework by testing three robots with distinct personalities in identical background contexts and found that personality, appraisal, and memory influence the adaptability of human-robot interactions. The impact of the individual components was further validated through ablation tests. We conclude that this system enables robots to engage in meaningful and personalized interactions with users, and holds significant potential for applications in domains such as pet robots, assistive robots, educational robots, and collaborative functional robots, where cultivating tailored relationships and enriching user experiences are essential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15518v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cheng Tang, Chao Tang, Steven Gong, Thomas M. Kwok, Yue Hu</dc:creator>
    </item>
    <item>
      <title>Human-AI Interaction Design Standards</title>
      <link>https://arxiv.org/abs/2503.16472</link>
      <description>arXiv:2503.16472v2 Announce Type: replace 
Abstract: The rapid development of artificial intelligence (AI) has significantly transformed human-computer interactions, making it essential to establish robust design standards to ensure effective, ethical, and human-centered AI (HCAI) solutions. Standards serve as the foundation for the adoption of new technologies, and human-AI interaction (HAII) standards are critical to supporting the industrialization of AI technology by following an HCAI approach. These design standards aim to provide clear principles, requirements, and guidelines for designing, developing, deploying, and using AI systems, enhancing the user experience and performance of AI systems. Despite their importance, the creation and adoption of HCAI-based interaction design standards face challenges, including the absence of universal frameworks, the inherent complexity of HAII, and the ethical dilemmas that arise in such systems. This chapter provides a comparative analysis of HAII versus traditional human-computer interaction (HCI) and outlines guiding principles for HCAI-based design. It explores international, regional, national, and industry standards related to HAII design from an HCAI perspective and reviews design guidelines released by leading companies such as Microsoft, Google, and Apple. Additionally, the chapter highlights tools available for implementing HAII standards and presents case studies of human-centered interaction design for AI systems in diverse fields, including healthcare, autonomous vehicles, and customer service. It further examines key challenges in developing HAII standards and suggests future directions for the field. Emphasizing the importance of ongoing collaboration between AI designers, developers, and experts in human factors and HCI, this chapter stresses the need to advance HCAI-based interaction design standards to ensure human-centered AI solutions across various domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16472v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chaoyi Zhao, Wei Xu</dc:creator>
    </item>
    <item>
      <title>Optimizing Generative AI's Accuracy and Transparency in Inductive Thematic Analysis: A Human-AI Comparison</title>
      <link>https://arxiv.org/abs/2503.16485</link>
      <description>arXiv:2503.16485v2 Announce Type: replace 
Abstract: This study highlights the transparency and accuracy of GenAI's inductive thematic analysis, particularly using GPT-4 Turbo API integrated within a stepwise prompt-based Python script. This approach ensured a traceable and systematic coding process, generating codes with supporting statements and page references, which enhanced validation and reproducibility. The results indicate that GenAI performs inductive coding in a manner closely resembling human coders, effectively categorizing themes at a level like the average human coder. However, in interpretation, GenAI extends beyond human coders by situating themes within a broader conceptual context, providing a more generalized and abstract perspective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16485v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Nyaaba, Min SungEun, Mary Abiswin Apam, Kwame Owoahene Acheampong, Emmanuel Dwamena</dc:creator>
    </item>
    <item>
      <title>Open Models, Closed Minds? On Agents Capabilities in Mimicking Human Personalities through Open Large Language Models</title>
      <link>https://arxiv.org/abs/2401.07115</link>
      <description>arXiv:2401.07115v3 Announce Type: replace-cross 
Abstract: The emergence of unveiling human-like behaviors in Large Language Models (LLMs) has led to a closer connection between NLP and human psychology. Scholars have been studying the inherent personalities exhibited by LLMs and attempting to incorporate human traits and behaviors into them. However, these efforts have primarily focused on commercially-licensed LLMs, neglecting the widespread use and notable advancements seen in Open LLMs. This work aims to address this gap by employing a set of 12 LLM Agents based on the most representative Open models and subject them to a series of assessments concerning the Myers-Briggs Type Indicator (MBTI) test and the Big Five Inventory (BFI) test. Our approach involves evaluating the intrinsic personality traits of Open LLM agents and determining the extent to which these agents can mimic human personalities when conditioned by specific personalities and roles. Our findings unveil that $(i)$ each Open LLM agent showcases distinct human personalities; $(ii)$ personality-conditioned prompting produces varying effects on the agents, with only few successfully mirroring the imposed personality, while most of them being ``closed-minded'' (i.e., they retain their intrinsic traits); and $(iii)$ combining role and personality conditioning can enhance the agents' ability to mimic human personalities. Our work represents a step up in understanding the dense relationship between NLP and human psychology through the lens of Open LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07115v3</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucio La Cava, Andrea Tagarelli</dc:creator>
    </item>
    <item>
      <title>Materiality and Risk in the Age of Pervasive AI Sensors</title>
      <link>https://arxiv.org/abs/2402.11183</link>
      <description>arXiv:2402.11183v2 Announce Type: replace-cross 
Abstract: Artificial intelligence (AI) systems connected to sensor-laden devices are becoming pervasive, which has notable implications for a range of AI risks, including to privacy, the environment, autonomy and more. There is therefore a growing need for increased accountability around the responsible development and deployment of these technologies. Here we highlight the dimensions of risk associated with AI systems that arise from the material affordances of sensors and their underlying calculative models. We propose a sensor-sensitive framework for diagnosing these risks, complementing existing approaches such as the US National Institute of Standards and Technology AI Risk Management Framework and the European Union AI Act, and discuss its implementation. We conclude by advocating for increased attention to the materiality of algorithmic systems, and of on-device AI sensors in particular, and highlight the need for development of a sensor design paradigm that empowers users and communities and leads to a future of increased fairness, accountability and transparency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11183v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1038/s42256-025-01017-7</arxiv:DOI>
      <arxiv:journal_reference>Nature Machine Intelligence (2025): 1-12</arxiv:journal_reference>
      <dc:creator>Mona Sloane, Emanuel Moss, Susan Kennedy, Matthew Stewart, Pete Warden, Brian Plancher, Vijay Janapa Reddi</dc:creator>
    </item>
    <item>
      <title>MMAC-Copilot: Multi-modal Agent Collaboration Operating Copilot</title>
      <link>https://arxiv.org/abs/2404.18074</link>
      <description>arXiv:2404.18074v3 Announce Type: replace-cross 
Abstract: Large language model agents that interact with PC applications often face limitations due to their singular mode of interaction with real-world environments, leading to restricted versatility and frequent hallucinations. To address this, we propose the Multi-Modal Agent Collaboration framework (MMAC-Copilot), a framework utilizes the collective expertise of diverse agents to enhance interaction ability with application. The framework introduces a team collaboration chain, enabling each participating agent to contribute insights based on their specific domain knowledge, effectively reducing the hallucination associated with knowledge domain gaps. We evaluate MMAC-Copilot using the GAIA benchmark and our newly introduced Visual Interaction Benchmark (VIBench). MMAC-Copilot achieved exceptional performance on GAIA, with an average improvement of 6.8\% over existing leading systems. VIBench focuses on non-API-interactable applications across various domains, including 3D gaming, recreation, and office scenarios. It also demonstrated remarkable capability on VIBench. We hope this work can inspire in this field and provide a more comprehensive assessment of Autonomous agents. The anonymous Github is available at \href{https://anonymous.4open.science/r/ComputerAgentWithVision-3C12}{Anonymous Github}</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18074v3</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zirui Song, Yaohang Li, Meng Fang, Yanda Li, Zhenhao Chen, Zecheng Shi, Yuan Huang, Xiuying Chen, Ling Chen</dc:creator>
    </item>
    <item>
      <title>Humans and Large Language Models in Clinical Decision Support: A Study with Medical Calculators</title>
      <link>https://arxiv.org/abs/2411.05897</link>
      <description>arXiv:2411.05897v2 Announce Type: replace-cross 
Abstract: Although large language models (LLMs) have been assessed for general medical knowledge using licensing exams, their ability to support clinical decision-making, such as selecting medical calculators, remains uncertain. We assessed nine LLMs, including open-source, proprietary, and domain-specific models, with 1,009 multiple-choice question-answer pairs across 35 clinical calculators and compared LLMs to humans on a subset of questions. While the highest-performing LLM, OpenAI o1, provided an answer accuracy of 66.0% (CI: 56.7-75.3%) on the subset of 100 questions, two human annotators nominally outperformed LLMs with an average answer accuracy of 79.5% (CI: 73.5-85.0%). Ultimately, we evaluated medical trainees and LLMs in recommending medical calculators across clinical scenarios like risk stratification and diagnosis. With error analysis showing that the highest-performing LLMs continue to make mistakes in comprehension (49.3% of errors) and calculator knowledge (7.1% of errors), our findings highlight that LLMs are not superior to humans in calculator recommendation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05897v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nicholas Wan, Qiao Jin, Joey Chan, Guangzhi Xiong, Serina Applebaum, Aidan Gilson, Reid McMurry, R. Andrew Taylor, Aidong Zhang, Qingyu Chen, Zhiyong Lu</dc:creator>
    </item>
    <item>
      <title>Passive Heart Rate Monitoring During Smartphone Use in Everyday Life</title>
      <link>https://arxiv.org/abs/2503.03783</link>
      <description>arXiv:2503.03783v3 Announce Type: replace-cross 
Abstract: Resting heart rate (RHR) is an important biomarker of cardiovascular health and mortality, but tracking it longitudinally generally requires a wearable device, limiting its availability. We present PHRM, a deep learning system for passive heart rate (HR) and RHR measurements during everyday smartphone use, using facial video-based photoplethysmography. Our system was developed using 225,773 videos from 495 participants and validated on 185,970 videos from 205 participants in laboratory and free-living conditions, representing the largest validation study of its kind. Compared to reference electrocardiogram, PHRM achieved a mean absolute percentage error (MAPE) &lt; 10% for HR measurements across three skin tone groups of light, medium and dark pigmentation; MAPE for each skin tone group was non-inferior versus the others. Daily RHR measured by PHRM had a mean absolute error &lt; 5 bpm compared to a wearable HR tracker, and was associated with known risk factors. These results highlight the potential of smartphones to enable passive and equitable heart health monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03783v3</guid>
      <category>q-bio.TO</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shun Liao, Paolo Di Achille, Jiang Wu, Silviu Borac, Jonathan Wang, Xin Liu, Eric Teasley, Lawrence Cai, Yuzhe Yang, Yun Liu, Daniel McDuff, Hao-Wei Su, Brent Winslow, Anupam Pathak, Shwetak Patel, James A. Taylor, Jameson K. Rogers, Ming-Zher Poh</dc:creator>
    </item>
    <item>
      <title>When neural implant meets multimodal LLM: A dual-loop system for neuromodulation and naturalistic neuralbehavioral research</title>
      <link>https://arxiv.org/abs/2503.12334</link>
      <description>arXiv:2503.12334v2 Announce Type: replace-cross 
Abstract: We propose a novel dual-loop system that synergistically combines responsive neurostimulation (RNS) implants with artificial intelligence-driven wearable devices for treating post-traumatic stress disorder (PTSD) and enabling naturalistic brain research. In PTSD Therapy Mode, an implanted closed-loop neural device monitors amygdala activity and provides on-demand stimulation upon detecting pathological theta oscillations, while an ensemble of wearables (smart glasses, smartwatches, smartphones) uses multimodal large language model (LLM) analysis of sensory data to detect environmental or physiological PTSD triggers and deliver timely audiovisual interventions. Logged events from both the neural and wearable loops are analyzed to personalize trigger detection and progressively transition patients to non-invasive interventions. In Neuroscience Research Mode, the same platform is adapted for real-world brain activity capture. Wearable-LLM systems recognize naturalistic events (social interactions, emotional situations, compulsive behaviors, decision making) and signal implanted RNS devices (via wireless triggers) to record synchronized intracranial data during these moments. This approach builds on recent advances in mobile intracranial EEG recording and closed-loop neuromodulation in humans (BRAIN Initiative, 2023) (Mobbs et al., 2021). We discuss how our interdisciplinary system could revolutionize PTSD therapy and cognitive neuroscience by enabling 24/7 monitoring, context-aware intervention, and rich data collection outside traditional labs. The vision is a future where AI-enhanced devices continuously collaborate with the human brain, offering therapeutic support and deep insights into neural function, with the resulting real-world context rich neural data, in turn, accelerating the development of more biologically-grounded and human-centric AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12334v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Edward Hong Wang, Cynthia Xin Wen</dc:creator>
    </item>
  </channel>
</rss>
