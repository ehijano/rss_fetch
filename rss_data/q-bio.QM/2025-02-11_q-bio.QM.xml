<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 12 Feb 2025 02:58:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>DiffNMR2: NMR Guided Sampling Acquisition Through Diffusion Model Uncertainty</title>
      <link>https://arxiv.org/abs/2502.05230</link>
      <description>arXiv:2502.05230v1 Announce Type: new 
Abstract: Nuclear Magnetic Resonance (NMR) spectrometry uses electro-frequency pulses to probe the resonance of a compound's nucleus, which is then analyzed to determine its structure. The acquisition time of high-resolution NMR spectra remains a significant bottleneck, especially for complex biological samples such as proteins. In this study, we propose a novel and efficient sub-sampling strategy based on a diffusion model trained on protein NMR data. Our method iteratively reconstructs under-sampled spectra while using model uncertainty to guide subsequent sampling, significantly reducing acquisition time. Compared to state-of-the-art strategies, our approach improves reconstruction accuracy by 52.9\%, reduces hallucinated peaks by 55.6%, and requires 60% less time in complex NMR experiments. This advancement holds promise for many applications, from drug discovery to materials science, where rapid and high-resolution spectral analysis is critical.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05230v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Etienne Goffinet, Sen Yan, Fabrizio Gabellieri, Laurence Jennings, Lydia Gkoura, Filippo Castiglione, Ryan Young, Idir Malki, Ankita Singh, Thomas Launey</dc:creator>
    </item>
    <item>
      <title>MultistageOT: Multistage optimal transport infers trajectories from a snapshot of single-cell data</title>
      <link>https://arxiv.org/abs/2502.05241</link>
      <description>arXiv:2502.05241v1 Announce Type: new 
Abstract: Single-cell RNA-sequencing captures a temporal slice, or a snapshot, of a cell differentiation process. A major bioinformatical challenge is the inference of differentiation trajectories from a single snapshot, and methods that account for outlier cells that are unrelated to the differentiation process have yet to be established. We present MultistageOT: a Multistage Optimal Transport-based framework for trajectory inference in a snapshot (https://github.com/dahlinlab/MultistageOT). Application of optimal transport has proven successful for many single-cell tasks, but classical bimarginal optimal transport for trajectory inference fails to model temporal progression in a snapshot. Representing a novel generalization of optimal transport, MultistageOT addresses this major limitation by introducing a temporal dimension, allowing for high resolution modeling of intermediate differentiation stages. We challenge MultistageOT with snapshot data of cell differentiation, demonstrating effectiveness in pseudotime ordering, detection of outliers, and significantly improved fate prediction accuracy over state-of-the-art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05241v1</guid>
      <category>q-bio.QM</category>
      <category>q-bio.CB</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Magnus Tronstad, Johan Karlsson, Joakim S. Dahlin</dc:creator>
    </item>
    <item>
      <title>Efficient Spatial Estimation of Perceptual Thresholds for Retinal Implants via Gaussian Process Regression</title>
      <link>https://arxiv.org/abs/2502.06672</link>
      <description>arXiv:2502.06672v1 Announce Type: new 
Abstract: Retinal prostheses restore vision by electrically stimulating surviving neurons, but calibrating perceptual thresholds - the minimum stimulus intensity required for perception - remains a time-intensive challenge, especially for high-electrode-count devices. Since neighboring electrodes exhibit spatial correlations, we propose a Gaussian Process Regression (GPR) framework to predict thresholds at unsampled locations while leveraging uncertainty estimates to guide adaptive sampling. Using perceptual threshold data from four Argus II users, we show that GPR with a Mat\'ern kernel provides more accurate threshold predictions than a Radial Basis Function (RBF) kernel (p &lt; .001, Wilcoxon signed-rank test). In addition, spatially optimized sampling yielded lower prediction error than uniform random sampling for Participants 1 and 3 (p &lt; .05). While adaptive sampling dynamically selects electrodes based on model uncertainty, its accuracy gains over spatial sampling were not statistically significant (p &gt; .05), though it approached significance for Participant 1 (p = .074). These findings establish GPR with spatial sampling as a scalable, efficient approach to retinal prosthesis calibration, minimizing patient burden while maintaining predictive accuracy. More broadly, this framework offers a generalizable solution for adaptive calibration in neuroprosthetic devices with spatially structured stimulation thresholds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06672v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roksana Sadeghi, Michael Beyeler</dc:creator>
    </item>
    <item>
      <title>SparseFocus: Learning-based One-shot Autofocus for Microscopy with Sparse Content</title>
      <link>https://arxiv.org/abs/2502.06452</link>
      <description>arXiv:2502.06452v1 Announce Type: cross 
Abstract: Autofocus is necessary for high-throughput and real-time scanning in microscopic imaging. Traditional methods rely on complex hardware or iterative hill-climbing algorithms. Recent learning-based approaches have demonstrated remarkable efficacy in a one-shot setting, avoiding hardware modifications or iterative mechanical lens adjustments. However, in this paper, we highlight a significant challenge that the richness of image content can significantly affect autofocus performance. When the image content is sparse, previous autofocus methods, whether traditional climbing-hill or learning-based, tend to fail. To tackle this, we propose a content-importance-based solution, named SparseFocus, featuring a novel two-stage pipeline. The first stage measures the importance of regions within the image, while the second stage calculates the defocus distance from selected important regions. To validate our approach and benefit the research community, we collect a large-scale dataset comprising millions of labelled defocused images, encompassing both dense, sparse and extremely sparse scenarios. Experimental results show that SparseFocus surpasses existing methods, effectively handling all levels of content sparsity. Moreover, we integrate SparseFocus into our Whole Slide Imaging (WSI) system that performs well in real-world applications. The code and dataset will be made available upon the publication of this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06452v1</guid>
      <category>cs.CV</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongping Zhai, Xiaoxi Fu, Qiang Su, Jia Hu, Yake Zhang, Yunfeng Zhou, Chaofan Zhang, Xiao Li, Wenxin Wang, Dongdong Wu, Shen Yan</dc:creator>
    </item>
    <item>
      <title>FRAP analysis Measuring biophysical kinetic parameters using image analysis</title>
      <link>https://arxiv.org/abs/2402.16615</link>
      <description>arXiv:2402.16615v3 Announce Type: replace 
Abstract: Understanding transcription factor dynamics is crucial for unraveling the regulatory mechanisms of gene expression that underpin cellular function and development. Measurements of transcription factor subcellular movements are essential for developing predictive models of gene expression. However, obtaining these quantitative measurements poses significant challenges due to the inherent variability of biological data and the need for high precision in tracking the movement and interaction of molecules. Our computational pipeline provides a solution to these challenges, offering a comprehensive approach to the quantitative analysis of transcription factor dynamics. Our pipeline integrates advanced image segmentation to accurately delineate individual nuclei, precise nucleus tracking to monitor changes over time, and detailed intensity extraction to measure fluorescence as a proxy for transcription factor activity. Combining our pipeline with techniques such as fluorescence recovery after photobleaching enables the estimation of vital biophysical parameters, such as transcription factor import and export rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16615v3</guid>
      <category>q-bio.QM</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sharva V. Hiremath, Etika Goyal, Gregory T. Reeves, Cranos M. Williams</dc:creator>
    </item>
    <item>
      <title>Automated Cell Structure Extraction for 3D Electron Microscopy by Deep Learning</title>
      <link>https://arxiv.org/abs/2405.06303</link>
      <description>arXiv:2405.06303v3 Announce Type: replace 
Abstract: Modeling the 3D structures of cells and tissues is crucial in biology. Sequential cross-sectional images from electron microscopy provide high-resolution intracellular structure information. The segmentation of complex cell structures remains a laborious manual task for experts, demanding time and effort. This bottleneck in analyzing biological images requires efficient and automated solutions. In this study, the deep learning-based automated segmentation of biological images was explored to enable accurate reconstruction of the 3D structures of cells and organelles. An analysis system for the cell images of Cyanidioschyzon merolae, a primitive unicellular red algae, was constructed. This system utilizes sequential cross-sectional images captured by a focused ion beam scanning electron microscope (FIB-SEM). A U-Net was adopted and training was performed to identify and segment cell organelles from single-cell images. In addition, the segment anything model (SAM) and 3D watershed algorithm were employed to extract individual 3D images of each cell from large-scale microscope images containing numerous cells. Finally, the trained U-Net was applied to segment each structure within these 3D images. Through this procedure, the creation of 3D cell models could be fully automated. The adoption of other deep learning techniques and combinations of image processing methods will also be explored to enhance the segmentation accuracy further.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06303v3</guid>
      <category>q-bio.QM</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin Kousaka, Atsuko H. Iwane, Yuichi Togashi</dc:creator>
    </item>
    <item>
      <title>Deep Learning for Protein-Ligand Docking: Are We There Yet?</title>
      <link>https://arxiv.org/abs/2405.14108</link>
      <description>arXiv:2405.14108v5 Announce Type: replace-cross 
Abstract: The effects of ligand binding on protein structures and their in vivo functions carry numerous implications for modern biomedical research and biotechnology development efforts such as drug discovery. Although several deep learning (DL) methods and benchmarks designed for protein-ligand docking have recently been introduced, to date no prior works have systematically studied the behavior of the latest docking and structure prediction methods within the broadly applicable context of (1) using predicted (apo) protein structures for docking (e.g., for applicability to new proteins); (2) binding multiple (cofactor) ligands concurrently to a given target protein (e.g., for enzyme design); and (3) having no prior knowledge of binding pockets (e.g., for generalization to unknown pockets). To enable a deeper understanding of docking methods' real-world utility, we introduce PoseBench, the first comprehensive benchmark for broadly applicable protein-ligand docking. PoseBench enables researchers to rigorously and systematically evaluate DL methods for apo-to-holo protein-ligand docking and protein-ligand structure prediction using both primary ligand and multi-ligand benchmark datasets, the latter of which we introduce for the first time to the DL community. Empirically, using PoseBench, we find that (1) DL co-folding methods generally outperform comparable conventional and DL docking baselines, yet popular methods such as AlphaFold 3 are still challenged by prediction targets with novel protein sequences; (2) certain DL co-folding methods are highly sensitive to their input multiple sequence alignments, while others are not; and (3) DL methods struggle to strike a balance between structural accuracy and chemical specificity when predicting novel or multi-ligand protein targets. Code, data, tutorials, and benchmark results are available at https://github.com/BioinfoMachineLearning/PoseBench.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14108v5</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.BM</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Morehead, Nabin Giri, Jian Liu, Pawan Neupane, Jianlin Cheng</dc:creator>
    </item>
    <item>
      <title>CISCA and CytoDArk0: a Cell Instance Segmentation and Classification method for histo(patho)logical image Analyses and a new, open, Nissl-stained dataset for brain cytoarchitecture studies</title>
      <link>https://arxiv.org/abs/2409.04175</link>
      <description>arXiv:2409.04175v2 Announce Type: replace-cross 
Abstract: Delineating and classifying individual cells in microscopy tissue images is inherently challenging yet remains essential for advancements in medical and neuroscientific research. In this work, we propose a new deep learning framework, CISCA, for automatic cell instance segmentation and classification in histological slices. At the core of CISCA is a network architecture featuring a lightweight U-Net with three heads in the decoder. The first head classifies pixels into boundaries between neighboring cells, cell bodies, and background, while the second head regresses four distance maps along four directions. The outputs from the first and second heads are integrated through a tailored post-processing step, which ultimately produces the segmentation of individual cells. The third head enables the simultaneous classification of cells into relevant classes, if required. We demonstrate the effectiveness of our method using four datasets, including CoNIC, PanNuke, and MoNuSeg, which are publicly available H&amp;Estained datasets that cover diverse tissue types and magnifications. In addition, we introduce CytoDArk0, the first annotated dataset of Nissl-stained histological images of the mammalian brain, containing nearly 40k annotated neurons and glia cells, aimed at facilitating advancements in digital neuropathology and brain cytoarchitecture studies. We evaluate CISCA against other state-of-the-art methods, demonstrating its versatility, robustness, and accuracy in segmenting and classifying cells across diverse tissue types, magnifications, and staining techniques. This makes CISCA well-suited for detailed analyses of cell morphology and efficient cell counting in both digital pathology workflows and brain cytoarchitecture research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04175v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Valentina Vadori, Jean-Marie Gra\"ic, Antonella Peruffo, Giulia Vadori, Livio Finos, Enrico Grisan</dc:creator>
    </item>
    <item>
      <title>Identifying perturbation targets through causal differential networks</title>
      <link>https://arxiv.org/abs/2410.03380</link>
      <description>arXiv:2410.03380v2 Announce Type: replace-cross 
Abstract: Identifying variables responsible for changes to a biological system enables applications in drug target discovery and cell engineering. Given a pair of observational and interventional datasets, the goal is to isolate the subset of observed variables that were the targets of the intervention. Directly applying causal discovery algorithms is challenging: the data may contain thousands of variables with as few as tens of samples per intervention, and biological systems do not adhere to classical causality assumptions. We propose a causality-inspired approach to address this practical setting. First, we infer noisy causal graphs from the observational and interventional data. Then, we learn to map the differences between these graphs, along with additional statistical features, to sets of variables that were intervened upon. Both modules are jointly trained in a supervised framework, on simulated and real data that reflect the nature of biological interventions. This approach consistently outperforms baselines for perturbation modeling on seven single-cell transcriptomics datasets. We also demonstrate significant improvements over current causal discovery methods for predicting soft and hard intervention targets across a variety of synthetic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03380v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Menghua Wu, Umesh Padia, Sean H. Murphy, Regina Barzilay, Tommi Jaakkola</dc:creator>
    </item>
  </channel>
</rss>
