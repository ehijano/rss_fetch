<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Apr 2025 04:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>When to Truncate the Archive? On the Effect of the Truncation Frequency in Multi-Objective Optimisation</title>
      <link>https://arxiv.org/abs/2504.01332</link>
      <description>arXiv:2504.01332v1 Announce Type: new 
Abstract: Using an archive to store nondominated solutions found during the search of a multi-objective evolutionary algorithm (MOEA) is a useful practice. However, as nondominated solutions of a multi-objective optimisation problem can be enormous or infinitely many, it is desirable to provide the decision-maker with only a small, representative portion of all the nondominated solutions in the archive, thus entailing a truncation operation. Then, an important issue is when to truncate the archive. This can be done once a new solution generated, a batch of new solutions generated, or even using an unbounded archive to keep all nondominated solutions generated and truncate it later. Intuitively, the last approach may lead to a better result since we have all the information in hand before performing the truncation. In this paper, we study this issue and investigate the effect of the timing of truncating the archive. We apply well-established truncation criteria that are commonly used in the population maintenance procedure of MOEAs (e.g., crowding distance, hypervolume indicator, and decomposition). We show that, interestingly, truncating the archive once a new solution generated tends to be the best, whereas considering an unbounded archive is often the worst. We analyse and discuss this phenomenon. Our results highlight the importance of developing effective subset selection techniques (rather than employing the population maintenance methods in MOEAs) when using a large archive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01332v1</guid>
      <category>cs.NE</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiji Cui, Zimin Liang, Lie Meng Pang, Hisao Ishibuchi, Miqing Li</dc:creator>
    </item>
    <item>
      <title>Overcoming Deceptiveness in Fitness Optimization with Unsupervised Quality-Diversity</title>
      <link>https://arxiv.org/abs/2504.01915</link>
      <description>arXiv:2504.01915v1 Announce Type: new 
Abstract: Policy optimization seeks the best solution to a control problem according to an objective or fitness function, serving as a fundamental field of engineering and research with applications in robotics. Traditional optimization methods like reinforcement learning and evolutionary algorithms struggle with deceptive fitness landscapes, where following immediate improvements leads to suboptimal solutions. Quality-diversity (QD) algorithms offer a promising approach by maintaining diverse intermediate solutions as stepping stones for escaping local optima. However, QD algorithms require domain expertise to define hand-crafted features, limiting their applicability where characterizing solution diversity remains unclear. In this paper, we show that unsupervised QD algorithms - specifically the AURORA framework, which learns features from sensory data - efficiently solve deceptive optimization problems without domain expertise. By enhancing AURORA with contrastive learning and periodic extinction events, we propose AURORA-XCon, which outperforms all traditional optimization baselines and matches, in some cases even improving by up to 34%, the best QD baseline with domain-specific hand-crafted features. This work establishes a novel application of unsupervised QD algorithms, shifting their focus from discovering novel solutions toward traditional optimization and expanding their potential to domains where defining feature spaces poses challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01915v1</guid>
      <category>cs.NE</category>
      <category>cs.RO</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3712256.3726314</arxiv:DOI>
      <dc:creator>Lisa Coiffard, Paul Templier, Antoine Cully</dc:creator>
    </item>
    <item>
      <title>An Explainable Reconfiguration-Based Optimization Algorithm for Industrial and Reliability-Redundancy Allocation Problems</title>
      <link>https://arxiv.org/abs/2504.01331</link>
      <description>arXiv:2504.01331v1 Announce Type: cross 
Abstract: Industrial and reliability optimization problems often involve complex constraints and require efficient, interpretable solutions. This paper presents AI-AEFA, an advanced parameter reconfiguration-based metaheuristic algorithm designed to address large-scale industrial and reliability-redundancy allocation problems. AI-AEFA enhances search space exploration and convergence efficiency through a novel log-sigmoid-based parameter adaptation and chaotic mapping mechanism. The algorithm is validated across twenty-eight IEEE CEC 2017 constrained benchmark problems, fifteen large-scale industrial optimization problems, and seven reliability-redundancy allocation problems, consistently outperforming state-of-the-art optimization techniques in terms of feasibility, computational efficiency, and convergence speed. The additional key contribution of this work is the integration of SHAP (Shapley Additive Explanations) to enhance the interpretability of AI-AEFA, providing insights into the impact of key parameters such as Coulomb's constant, charge, acceleration, and electrostatic force. This explainability feature enables a deeper understanding of decision-making within the AI-AEFA framework during the optimization processes. The findings confirm AI-AEFA as a robust, scalable, and interpretable optimization tool with significant real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01331v1</guid>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dikshit Chauhan, Nitin Gupta, Anupam Yadav</dc:creator>
    </item>
    <item>
      <title>Learning Graph Quantized Tokenizers</title>
      <link>https://arxiv.org/abs/2410.13798</link>
      <description>arXiv:2410.13798v2 Announce Type: replace 
Abstract: Transformers serve as the backbone architectures of Foundational Models, where domain-specific tokenizers allow them to adapt to various domains. Graph Transformers (GTs) have recently emerged as leading models in geometric deep learning, outperforming Graph Neural Networks (GNNs) in various graph learning tasks. However, the development of tokenizers for graphs has lagged behind other modalities. To address this, we introduce GQT (\textbf{G}raph \textbf{Q}uantized \textbf{T}okenizer), which decouples tokenizer training from Transformer training by leveraging multi-task graph self-supervised learning, yielding robust and generalizable graph tokens. Furthermore, the GQT utilizes Residual Vector Quantization (RVQ) to learn hierarchical discrete tokens, resulting in significantly reduced memory requirements and improved generalization capabilities. By combining the GQT with token modulation, a Transformer encoder achieves state-of-the-art performance on 20 out of 22 benchmarks, including large-scale homophilic and heterophilic datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13798v2</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Limei Wang, Kaveh Hassani, Si Zhang, Dongqi Fu, Baichuan Yuan, Weilin Cong, Zhigang Hua, Hao Wu, Ning Yao, Bo Long</dc:creator>
    </item>
    <item>
      <title>Resolving the Exploration-Exploitation Dilemma in Evolutionary Algorithms: A Novel Human-Centered Framework</title>
      <link>https://arxiv.org/abs/2501.02153</link>
      <description>arXiv:2501.02153v2 Announce Type: replace 
Abstract: Evolutionary Algorithms (EAs) are widely employed tools for complex search and optimization tasks; however, the absence of an overarching operational framework that permits a systematic regulation of the exploration-exploitation tradeoff--critical for efficient convergence--restricts the full actualization of their potential, leading to the so-called exploration-exploitation dilemma in algorithm design. A systematic resolution to this dilemma requires: (1) an independent yet coordinated control over exploration and exploitation, and (2) an explicit, computationally feasible, adaptive regulation mechanism. The current, almost decentralized, traditional parameter tuning-centeric approach--lacks the foundation to satisfy these requirements under encoding-imposed structural constraints.
  We propose a Human-Centered Two-Phase Search (HCTPS) framework, in which the actualization of (1) and (2) is enabled through an external configuration variable--the Search Space Control Parameter (SSCP). As the sole control knob of HCTPS, the SSCP centralizes exploration adjustments, sparing users from micromanaging traditional parameters with unintelligible interdependencies. To this construct, the human user serves as a meta-parameter, adaptively steering the regulatory process via SSCP adjustments. We prove that the HCTPS strictly surpasses the current approach in terms of search space coverage without disrupting the EAs' inherent convergence mechanisms, demonstrate a concrete instantiation of it--using the Genetic Algorithm as the underlying heuristic on a suite of global benchmark unconstrained optimization problems, provide a through assessment of the proposed framework, and envision future research directions.
  Any search algorithm prone to this dilemma can be applied in light of the proposed framework, being algorithm-agnostic by design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02153v2</guid>
      <category>cs.NE</category>
      <category>math.OC</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ehsan Shams</dc:creator>
    </item>
    <item>
      <title>Stochastic Reservoir Computers</title>
      <link>https://arxiv.org/abs/2405.12382</link>
      <description>arXiv:2405.12382v2 Announce Type: replace-cross 
Abstract: Reservoir computing is a form of machine learning that utilizes nonlinear dynamical systems to perform complex tasks in a cost-effective manner when compared to typical neural networks. Many recent advancements in reservoir computing, in particular quantum reservoir computing, make use of reservoirs that are inherently stochastic. However, the theoretical justification for using these systems has not yet been well established. In this paper, we investigate the universality of stochastic reservoir computers, in which we use a stochastic system for reservoir computing using the probabilities of each reservoir state as the readout instead of the states themselves. In stochastic reservoir computing, the number of distinct states of the entire reservoir computer can potentially scale exponentially with the size of the reservoir hardware, offering the advantage of compact device size. We prove that classes of stochastic echo state networks, and therefore the class of all stochastic reservoir computers, are universal approximating classes. We also investigate the performance of two practical examples of stochastic reservoir computers in classification and chaotic time series prediction. While shot noise is a limiting factor in the performance of stochastic reservoir computing, we show significantly improved performance compared to a deterministic reservoir computer with similar hardware in cases where the effects of noise are small.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12382v2</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>nlin.AO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1038/s41467-025-58349-6</arxiv:DOI>
      <arxiv:journal_reference>Nature Communications 16, 3070 (2025)</arxiv:journal_reference>
      <dc:creator>Peter J. Ehlers, Hendra I. Nurdin, Daniel Soh</dc:creator>
    </item>
    <item>
      <title>EVOS: Efficient Implicit Neural Training via EVOlutionary Selector</title>
      <link>https://arxiv.org/abs/2412.10153</link>
      <description>arXiv:2412.10153v2 Announce Type: replace-cross 
Abstract: We propose EVOlutionary Selector (EVOS), an efficient training paradigm for accelerating Implicit Neural Representation (INR). Unlike conventional INR training that feeds all samples through the neural network in each iteration, our approach restricts training to strategically selected points, reducing computational overhead by eliminating redundant forward passes. Specifically, we treat each sample as an individual in an evolutionary process, where only those fittest ones survive and merit inclusion in training, adaptively evolving with the neural network dynamics. While this is conceptually similar to Evolutionary Algorithms, their distinct objectives (selection for acceleration vs. iterative solution optimization) require a fundamental redefinition of evolutionary mechanisms for our context. In response, we design sparse fitness evaluation, frequency-guided crossover, and augmented unbiased mutation to comprise EVOS. These components respectively guide sample selection with reduced computational cost, enhance performance through frequency-domain balance, and mitigate selection bias from cached evaluation. Extensive experiments demonstrate that our method achieves approximately 48%-66% reduction in training time while ensuring superior convergence without additional cost, establishing state-of-the-art acceleration among recent sampling-based strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10153v2</guid>
      <category>cs.CV</category>
      <category>cs.MM</category>
      <category>cs.NE</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weixiang Zhang, Shuzhao Xie, Chengwei Ren, Siyi Xie, Chen Tang, Shijia Ge, Mingzi Wang, Zhi Wang</dc:creator>
    </item>
  </channel>
</rss>
