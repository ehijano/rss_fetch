<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 03 Oct 2025 04:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>ENLighten: Lighten the Transformer, Enable Efficient Optical Acceleration</title>
      <link>https://arxiv.org/abs/2510.01673</link>
      <description>arXiv:2510.01673v1 Announce Type: new 
Abstract: Photonic computing has emerged as a promising substrate for accelerating the dense linear-algebra operations at the heart of AI, yet adoption for large Transformer models remains in its infancy. We identify two bottlenecks: (1) costly electro--optic conversions and data-movement overheads that erode energy efficiency as model sizes scale; (2) a mismatch between limited on-chip photonic resources and Transformer scale, which forces frequent reuse of photonic tensor cores and dilutes throughput gains. To address these challenges, we introduce a hardware--software co-design framework. First, we propose \texttt{Lighten}, a PTC-aware compression flow that post-hoc decomposes each Transformer weight matrix into a low-rank component plus a structured-sparse component aligned to photonic tensor-core granularity, without lengthy retraining. Second, we present \texttt{ENLighten}, a reconfigurable photonic accelerator with dynamically adaptive tensor cores, driven by broadband light redistribution, enabling fine-grained sparsity support and full power gating of inactive parts. On ImageNet, \texttt{Lighten} prunes a Base-scale Vision Transformer by 50\% with $\approx$1\% accuracy drop after only 3 epochs (about 1 hour) of fine-tuning. Deployed on \texttt{ENLighten}, it achieves a $2.5\times$ improvement in energy--delay product over the state-of-the-art photonic Transformer accelerator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01673v1</guid>
      <category>cs.ET</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanqing Zhu, Zhican Zhou, Shupeng Ning, Xuhao Wu, Ray Chen, Yating Wan, David Pan</dc:creator>
    </item>
    <item>
      <title>Integrated Security Mechanisms for Weight Protection in Memristive Crossbar Arrays</title>
      <link>https://arxiv.org/abs/2510.01350</link>
      <description>arXiv:2510.01350v1 Announce Type: cross 
Abstract: Memristive crossbar arrays enable in-memory computing by performing parallel analog computations directly within memory, making them well-suited for machine learning, neural networks, and neuromorphic systems. However, despite their advantages, non-volatile memristors are vulnerable to security threats (such as adversarial extraction of stored weights when the hardware is compromised. Protecting these weights is essential since they represent valuable intellectual property resulting from lengthy and costly training processes using large, often proprietary, datasets. As a solution we propose two security mechanisms: Keyed Permutor and Watermark Protection Columns; where both safeguard critical weights and establish verifiable ownership (even in cases of data leakage). Our approach integrates efficiently with existing memristive crossbar architectures without significant design modifications. Simulations across 45nm, 22nm, and 7nm CMOS nodes, using a realistic interconnect model and a large RF dataset, show that both mechanisms offer robust protection with under 10% overhead in area, delay and power. We also present initial experiments employing the widely known MNIST dataset; further highlighting the feasibility of securing memristive in-memory computing systems with minimal performance trade-offs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01350v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <category>cs.NE</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ASAP65064.2025.00043</arxiv:DOI>
      <dc:creator>Muhammad Faheemur Rahman, Wayne Burleson</dc:creator>
    </item>
    <item>
      <title>Zero-shot reasoning for simulating scholarly peer-review</title>
      <link>https://arxiv.org/abs/2510.02027</link>
      <description>arXiv:2510.02027v1 Announce Type: cross 
Abstract: The scholarly publishing ecosystem faces a dual crisis of unmanageable submission volumes and unregulated AI, creating an urgent need for new governance models to safeguard scientific integrity. The traditional human-only peer review regime lacks a scalable, objective benchmark, making editorial processes opaque and difficult to audit. Here we investigate a deterministic simulation framework that provides the first stable, evidence-based standard for evaluating AI-generated peer review reports. Analyzing 352 peer-review simulation reports, we identify consistent system state indicators that demonstrate its reliability. First, the system is able to simulate calibrated editorial judgment, with 'Revise' decisions consistently forming the majority outcome (&gt;50%) across all disciplines, while 'Reject' rates dynamically adapt to field-specific norms, rising to 45% in Health Sciences. Second, it maintains unwavering procedural integrity, enforcing a stable 29% evidence-anchoring compliance rate that remains invariant across diverse review tasks and scientific domains. These findings demonstrate a system that is predictably rule-bound, mitigating the stochasticity of generative AI. For the scientific community, this provides a transparent tool to ensure fairness; for publishing strategists, it offers a scalable instrument for auditing workflows, managing integrity risks, and implementing evidence-based governance. The framework repositions AI as an essential component of institutional accountability, providing the critical infrastructure to maintain trust in scholarly communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02027v1</guid>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Khalid M. Saqr</dc:creator>
    </item>
    <item>
      <title>A spiking photonic neural network of 40.000 neurons, trained with rank-order coding for leveraging sparsity</title>
      <link>https://arxiv.org/abs/2411.19209</link>
      <description>arXiv:2411.19209v3 Announce Type: replace 
Abstract: Spiking neural networks are neuromorphic systems that emulate certain aspects of biological neurons, offering potential advantages in energy efficiency and speed by for example leveraging sparsity. While CMOS-based electronic SNN hardware has shown promise, scalability and parallelism challenges remain. Photonics provides a promising platform for SNNs due to the speed of excitable photonic devices standing in as neurons and the parallelism and low-latency of optical signal conduction. Here, we present a photonic SNN comprising 40,000 neurons using off-the-shelf components, including a spatial light modulator and a CMOS camera, enabling scalable and cost-effective implementations for photonic SNN proof of concept studies. The system is governed by a modified Ikeda map, were adding additional inhibitory feedback forcing introduces excitability akin to biological dynamics. Using latency encoding and sparsity, the network achieves 83.5% accuracy on MNIST using 22% of neurons, and 77.5% with 8.5% neuron utilization. Training is performed via liquid state machine concepts combined with the hardware-compatible SPSA algorithm, marking its first use in photonic neural networks. This demonstration integrates photonic nonlinearity, excitability, and sparse computation, paving the way for efficient large-scale photonic neuromorphic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19209v3</guid>
      <category>cs.ET</category>
      <category>cs.NE</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1088/2634-4386/addee7</arxiv:DOI>
      <arxiv:journal_reference>Neuromorph. Comput. Eng. 5 034003 (2025)</arxiv:journal_reference>
      <dc:creator>Ria Talukder, Anas Skalli, Xavier Porte, Simon Thorpe, Daniel Brunner</dc:creator>
    </item>
    <item>
      <title>QMill: Representative Quantum Data Generation for Quantum Machine Learning Utility</title>
      <link>https://arxiv.org/abs/2509.21622</link>
      <description>arXiv:2509.21622v2 Announce Type: replace 
Abstract: Quantum machine learning (QML) promises significant speedups, particularly when operating on quantum datasets. However, its progress is hindered by the scarcity of suitable training data. Existing synthetic data generation methods fall short in capturing essential entanglement properties, limiting their utility for QML. To address this, we introduce QMill, a low-depth quantum data generation framework that produces entangled, high-quality samples emulating diverse classical and quantum distributions, enabling more effective development and evaluation of QML models in representative-data settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21622v2</guid>
      <category>cs.ET</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason Ludmir, Ian Martin, Nicholas S. DiBrita, Daniel Leeds, Tirthak Patel</dc:creator>
    </item>
    <item>
      <title>Revisiting Noise-adaptive Transpilation in Quantum Computing: How Much Impact Does it Have?</title>
      <link>https://arxiv.org/abs/2507.01195</link>
      <description>arXiv:2507.01195v2 Announce Type: replace-cross 
Abstract: Transpilation, particularly noise-aware optimization, is widely regarded as essential for maximizing the performance of quantum circuits on superconducting quantum computers. The common wisdom is that each circuit should be transpiled using up-to-date noise calibration data to optimize fidelity. In this work, we revisit the necessity of frequent noise-adaptive transpilation, conducting an in-depth empirical study across five IBM 127-qubit quantum computers and 16 diverse quantum algorithms. Our findings reveal novel and interesting insights: (1) noise-aware transpilation leads to a heavy concentration of workloads on a small subset of qubits, which increases output error variability; (2) using random mapping can mitigate this effect while maintaining comparable average fidelity; and (3) circuits compiled once with calibration data can be reliably reused across multiple calibration cycles and time periods without significant loss in fidelity. These results suggest that the classical overhead associated with daily, per-circuit noise-aware transpilation may not be justified. We propose lightweight alternatives that reduce this overhead without sacrificing fidelity -- offering a path to more efficient and scalable quantum workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01195v2</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuqian Huo, Jinbiao Wei, Christopher Kverne, Mayur Akewar, Janki Bhimani, Tirthak Patel</dc:creator>
    </item>
  </channel>
</rss>
