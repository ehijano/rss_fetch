<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 12 Jun 2024 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 12 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Federated Nonparametric Hypothesis Testing with Differential Privacy Constraints: Optimal Rates and Adaptive Tests</title>
      <link>https://arxiv.org/abs/2406.06749</link>
      <description>arXiv:2406.06749v1 Announce Type: new 
Abstract: Federated learning has attracted significant recent attention due to its applicability across a wide range of settings where data is collected and analyzed across disparate locations. In this paper, we study federated nonparametric goodness-of-fit testing in the white-noise-with-drift model under distributed differential privacy (DP) constraints.
  We first establish matching lower and upper bounds, up to a logarithmic factor, on the minimax separation rate. This optimal rate serves as a benchmark for the difficulty of the testing problem, factoring in model characteristics such as the number of observations, noise level, and regularity of the signal class, along with the strictness of the $(\epsilon,\delta)$-DP requirement. The results demonstrate interesting and novel phase transition phenomena. Furthermore, the results reveal an interesting phenomenon that distributed one-shot protocols with access to shared randomness outperform those without access to shared randomness. We also construct a data-driven testing procedure that possesses the ability to adapt to an unknown regularity parameter over a large collection of function classes with minimal additional cost, all while maintaining adherence to the same set of DP constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06749v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>T. Tony Cai, Abhinav Chakraborty, Lasse Vuursteen</dc:creator>
    </item>
    <item>
      <title>Optimal Federated Learning for Nonparametric Regression with Heterogeneous Distributed Differential Privacy Constraints</title>
      <link>https://arxiv.org/abs/2406.06755</link>
      <description>arXiv:2406.06755v1 Announce Type: new 
Abstract: This paper studies federated learning for nonparametric regression in the context of distributed samples across different servers, each adhering to distinct differential privacy constraints. The setting we consider is heterogeneous, encompassing both varying sample sizes and differential privacy constraints across servers. Within this framework, both global and pointwise estimation are considered, and optimal rates of convergence over the Besov spaces are established.
  Distributed privacy-preserving estimators are proposed and their risk properties are investigated. Matching minimax lower bounds, up to a logarithmic factor, are established for both global and pointwise estimation. Together, these findings shed light on the tradeoff between statistical accuracy and privacy preservation. In particular, we characterize the compromise not only in terms of the privacy budget but also concerning the loss incurred by distributing data within the privacy framework as a whole. This insight captures the folklore wisdom that it is easier to retain privacy in larger samples, and explores the differences between pointwise and global estimation under distributed privacy constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06755v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>T. Tony Cai, Abhinav Chakraborty, Lasse Vuursteen</dc:creator>
    </item>
    <item>
      <title>Causality for Complex Continuous-time Functional Longitudinal Studies with Dynamic Treatment Regimes</title>
      <link>https://arxiv.org/abs/2406.06868</link>
      <description>arXiv:2406.06868v1 Announce Type: new 
Abstract: Causal inference in longitudinal studies is often hampered by treatment-confounder feedback. Existing methods typically assume discrete time steps or step-like data changes, which we term ``regular and irregular functional studies,'' limiting their applicability to studies with continuous monitoring data, like intensive care units or continuous glucose monitoring. These studies, which we formally term ``functional longitudinal studies,'' require new approaches. Moreover, existing methods tailored for ``functional longitudinal studies'' can only investigate static treatment regimes, which are independent of historical covariates or treatments, leading to either stringent parametric assumptions or strong positivity assumptions. This restriction has limited the range of causal questions these methods can answer and their practicality. We address these limitations by developing a nonparametric framework for functional longitudinal data, accommodating dynamic treatment regimes that depend on historical covariates or treatments, and may or may not depend on the actual treatment administered. To build intuition and explain our approach, we provide a comprehensive review of existing methods for regular and irregular longitudinal studies. We then formally define the potential outcomes and causal effects of interest, develop identification assumptions, and derive g-computation and inverse probability weighting formulas through novel applications of stochastic process and measure theory. Additionally, we compute the efficient influence curve using semiparametric theory. Our framework generalizes existing literature, and achieves double robustness under specific conditions. Finally, to aid interpretation, we provide sufficient and intuitive conditions for our identification assumptions, enhancing the applicability of our methodology to real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06868v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Ying</dc:creator>
    </item>
    <item>
      <title>Inferring the dependence graph density of binary graphical models in high dimension</title>
      <link>https://arxiv.org/abs/2406.07066</link>
      <description>arXiv:2406.07066v1 Announce Type: new 
Abstract: We consider a system of binary interacting chains describing the dynamics of a group of $N$ components that, at each time unit, either send some signal to the others or remain silent otherwise. The interactions among the chains are encoded by a directed Erd\"os-R\'enyi random graph with unknown parameter $ p \in (0, 1) .$ Moreover, the system is structured within two populations (excitatory chains versus inhibitory ones) which are coupled via a mean field interaction on the underlying Erd\"os-R\'enyi graph. In this paper, we address the question of inferring the connectivity parameter $p$ based only on the observation of the interacting chains over $T$ time units. In our main result, we show that the connectivity parameter $p$ can be estimated with rate $N^{-1/2}+N^{1/2}/T+(\log(T)/T)^{1/2}$ through an easy-to-compute estimator. Our analysis relies on a precise study of the spatio-temporal decay of correlations of the interacting chains. This is done through the study of coalescing random walks defining a backward regeneration representation of the system. Interestingly, we also show that this backward regeneration representation allows us to perfectly sample the system of interacting chains (conditionally on each realization of the underlying Erd\"os-R\'enyi graph) from its stationary distribution. These probabilistic results have an interest in its own.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07066v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julien Chevallier, Eva L\"ocherbach, Guilherme Ost</dc:creator>
    </item>
    <item>
      <title>Power Analysis for Experiments with Clustered Data, Ratio Metrics, and Regression for Covariate Adjustment</title>
      <link>https://arxiv.org/abs/2406.06834</link>
      <description>arXiv:2406.06834v1 Announce Type: cross 
Abstract: We describe how to calculate standard errors for A/B tests that include clustered data, ratio metrics, and/or covariate adjustment. We may do this for power analysis/sample size calculations prior to running an experiment using historical data, or after an experiment for hypothesis testing and confidence intervals. The different applications have a common framework, using the sample variance of certain residuals. The framework is compatible with modular software, can be plugged into standard tools, doesn't require computing covariance matrices, and is numerically stable. Using this approach we estimate that covariate adjustment gives a median 66% variance reduction for a key metric, reducing experiment run time by 66%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06834v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tim Hesterberg (Instacart), Ben Knight (Instacart)</dc:creator>
    </item>
    <item>
      <title>A Dynamic Likelihood Approach to Filtering for advection-diffusion1 Dynamics</title>
      <link>https://arxiv.org/abs/2406.06837</link>
      <description>arXiv:2406.06837v1 Announce Type: cross 
Abstract: A Bayesian data assimilation scheme is formulated for advection-dominated advective and diffusive evolutionary problems, based upon the Dynamic Likelihood (DLF) approach to filtering. The DLF was developed specifically for hyperbolic problems -waves-, and in this paper, it is extended via a split step formulation, to handle advection-diffusion problems. In the dynamic likelihood approach, observations and their statistics are used to propagate probabilities along characteristics, evolving the likelihood in time. The estimate posterior thus inherits phase information. For advection-diffusion the advective part of the time evolution is handled on the basis of observations alone, while the diffusive part is informed through the model as well as observations. We expect, and indeed show here, that in advection-dominated problems, the DLF approach produces better estimates than other assimilation approaches, particularly when the observations are sparse and have low uncertainty. The added computational expense of the method is cubic in the total number of observations over time, which is on the same order of magnitude as a standard Kalman filter and can be mitigated by bounding the number of forward propagated observations, discarding the least informative data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06837v1</guid>
      <category>math.DS</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Johannes Krotz, Juan M. Restrepo, Jorge Ramirez</dc:creator>
    </item>
    <item>
      <title>On the Limitation of Kernel Dependence Maximization for Feature Selection</title>
      <link>https://arxiv.org/abs/2406.06903</link>
      <description>arXiv:2406.06903v1 Announce Type: cross 
Abstract: A simple and intuitive method for feature selection consists of choosing the feature subset that maximizes a nonparametric measure of dependence between the response and the features. A popular proposal from the literature uses the Hilbert-Schmidt Independence Criterion (HSIC) as the nonparametric dependence measure. The rationale behind this approach to feature selection is that important features will exhibit a high dependence with the response and their inclusion in the set of selected features will increase the HSIC. Through counterexamples, we demonstrate that this rationale is flawed and that feature selection via HSIC maximization can miss critical features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06903v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keli Liu, Feng Ruan</dc:creator>
    </item>
    <item>
      <title>Efficient combination of observational and experimental datasets under general restrictions on outcome mean functions</title>
      <link>https://arxiv.org/abs/2406.06941</link>
      <description>arXiv:2406.06941v1 Announce Type: cross 
Abstract: A researcher collecting data from a randomized controlled trial (RCT) often has access to an auxiliary observational dataset that may be confounded or otherwise biased for estimating causal effects. Common modeling assumptions impose restrictions on the outcome mean function - the conditional expectation of the outcome of interest given observed covariates - in the two datasets. Running examples from the literature include settings where the observational dataset is subject to outcome-mediated selection bias or to confounding bias taking an assumed parametric form. We propose a succinct framework to derive the efficient influence function for any identifiable pathwise differentiable estimand under a general class of restrictions on the outcome mean function. This uncovers surprising results that with homoskedastic outcomes and a constant propensity score in the RCT, even strong parametric assumptions cannot improve the semiparametric lower bound for estimating various average treatment effects. We then leverage double machine learning to construct a one-step estimator that achieves the semiparametric efficiency bound even in cases when the outcome mean function and other nuisance parameters are estimated nonparametrically. The goal is to empower a researcher with custom, previously unstudied modeling restrictions on the outcome mean function to systematically construct causal estimators that maximially leverage their assumptions for variance reduction. We demonstrate the finite sample precision gains of our estimator over existing approaches in extensions of various numerical studies and data examples from the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06941v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison H. Li</dc:creator>
    </item>
    <item>
      <title>New density/likelihood representations for Gibbs models based on generating functionals of point processes</title>
      <link>https://arxiv.org/abs/2406.07075</link>
      <description>arXiv:2406.07075v1 Announce Type: cross 
Abstract: Deriving exact density functions for Gibbs point processes has been challenging due to their general intractability, stemming from the intractability of their normalising constants/partition functions. This paper offers a solution to this open problem by exploiting a recent alternative representation of point process densities. Here, for a finite point process, the density is expressed as the void probability multiplied by a higher-order Papangelou conditional intensity function. By leveraging recent results on dependent thinnings, exact expressions for generating functionals and void probabilities of locally stable point processes are derived. Consequently, exact expressions for density/likelihood functions, partition functions and posterior densities are also obtained. The paper finally extends the results to locally stable Gibbsian random fields on lattices by representing them as point processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07075v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ottmar Cronie</dc:creator>
    </item>
    <item>
      <title>Convergence rate of random scan Coordinate Ascent Variational Inference under log-concavity</title>
      <link>https://arxiv.org/abs/2406.07292</link>
      <description>arXiv:2406.07292v1 Announce Type: cross 
Abstract: The Coordinate Ascent Variational Inference scheme is a popular algorithm used to compute the mean-field approximation of a probability distribution of interest. We analyze its random scan version, under log-concavity assumptions on the target density. Our approach builds on the recent work of M. Arnese and D. Lacker, \emph{Convergence of coordinate ascent variational inference for log-concave measures via optimal transport} [arXiv:2404.08792] which studies the deterministic scan version of the algorithm, phrasing it as a block-coordinate descent algorithm in the space of probability distributions endowed with the geometry of optimal transport. We obtain tight rates for the random scan version, which imply that the total number of factor updates required to converge scales linearly with the condition number and the number of blocks of the target distribution. By contrast, available bounds for the deterministic scan case scale quadratically in the same quantities, which is analogue to what happens for optimization of convex functions in Euclidean spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07292v1</guid>
      <category>stat.ML</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hugo Lavenant, Giacomo Zanella</dc:creator>
    </item>
    <item>
      <title>Probabilistic models and statistics for electronic financial markets in the digital age</title>
      <link>https://arxiv.org/abs/2406.07388</link>
      <description>arXiv:2406.07388v1 Announce Type: cross 
Abstract: The scope of this manuscript is to review some recent developments in statistics for discretely observed semimartingales which are motivated by applications for financial markets. Our journey through this area stops to take closer looks at a few selected topics discussing recent literature. We moreover highlight and explain the important role played by some classical concepts of probability and statistics. We focus on three main aspects: Testing for jumps; rough fractional stochastic volatility; and limit order microstructure noise. We review jump tests based on extreme value theory and complement the literature proposing new statistical methods. They are based on asymptotic theory of order statistics and the R\'{e}nyi representation. The second stage of our journey visits a recent strand of research showing that volatility is rough. We further investigate this and establish a minimax lower bound exploring frontiers to what extent the regularity of latent volatility can be recovered in a more general framework. Finally, we discuss a stochastic boundary model with one-sided microstructure noise for high-frequency limit order prices and its probabilistic and statistical foundation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07388v1</guid>
      <category>q-fin.ST</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Markus Bibinger</dc:creator>
    </item>
    <item>
      <title>Point estimation through Stein's method</title>
      <link>https://arxiv.org/abs/2305.19031</link>
      <description>arXiv:2305.19031v3 Announce Type: replace 
Abstract: Stein operators allow to characterise probability distributions via differential operators. We use these characterisations to obtain a new class of point estimators for marginal parameters of strictly stationary and ergodic processes. These so-called Stein estimators satisfy the desirable classical properties such as consistency and asymptotic normality. As a consequence of the usually simple form of the operator, we obtain explicit estimators in cases where standard methods such as (pseudo-) maximum likelihood estimation require a numerical procedure to calculate the estimate. In addition, with our approach, one can choose from a large class of test functions which allows to improve significantly on the moment estimator. For several probability laws, we can determine an estimator that shows an asymptotic behaviour close to efficiency in the i.i.d.\ case. Moreover, for i.i.d. observations, we retrieve data-dependent functions that result in asymptotically efficient estimators and give a sequence of explicit Stein estimators that converge to the maximum likelihood estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.19031v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bruno Ebner, Adrian Fischer, Robert E. Gaunt, Babette Picker, Yvik Swan</dc:creator>
    </item>
    <item>
      <title>Spectral Estimators for Structured Generalized Linear Models via Approximate Message Passing</title>
      <link>https://arxiv.org/abs/2308.14507</link>
      <description>arXiv:2308.14507v2 Announce Type: replace 
Abstract: We consider the problem of parameter estimation in a high-dimensional generalized linear model. Spectral methods obtained via the principal eigenvector of a suitable data-dependent matrix provide a simple yet surprisingly effective solution. However, despite their wide use, a rigorous performance characterization, as well as a principled way to preprocess the data, are available only for unstructured (i.i.d.\ Gaussian and Haar orthogonal) designs. In contrast, real-world data matrices are highly structured and exhibit non-trivial correlations. To address the problem, we consider correlated Gaussian designs capturing the anisotropic nature of the features via a covariance matrix $\Sigma$. Our main result is a precise asymptotic characterization of the performance of spectral estimators. This allows us to identify the optimal preprocessing that minimizes the number of samples needed for parameter estimation. Surprisingly, such preprocessing is universal across a broad set of statistical models, which partly addresses a conjecture on optimal spectral estimators for rotationally invariant designs. Our principled approach vastly improves upon previous heuristic methods, including for designs common in computational imaging and genetics. The proposed methodology, based on approximate message passing, is broadly applicable and opens the way to the precise characterization of spiked matrices and of the corresponding spectral methods in a variety of settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.14507v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yihan Zhang, Hong Chang Ji, Ramji Venkataramanan, Marco Mondelli</dc:creator>
    </item>
    <item>
      <title>Batched Nonparametric Contextual Bandits</title>
      <link>https://arxiv.org/abs/2402.17732</link>
      <description>arXiv:2402.17732v2 Announce Type: replace 
Abstract: We study nonparametric contextual bandits under batch constraints, where the expected reward for each action is modeled as a smooth function of covariates, and the policy updates are made at the end of each batch of observations. We establish a minimax regret lower bound for this setting and propose a novel batch learning algorithm that achieves the optimal regret (up to logarithmic factors). In essence, our procedure dynamically splits the covariate space into smaller bins, carefully aligning their widths with the batch size. Our theoretical results suggest that for nonparametric contextual bandits, a nearly constant number of policy updates can attain optimal regret in the fully online setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17732v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rong Jiang, Cong Ma</dc:creator>
    </item>
    <item>
      <title>Estimating Gaussian mixtures using sparse polynomial moment systems</title>
      <link>https://arxiv.org/abs/2106.15675</link>
      <description>arXiv:2106.15675v3 Announce Type: replace-cross 
Abstract: The method of moments is a classical statistical technique for density estimation that solves a system of moment equations to estimate the parameters of an unknown distribution. A fundamental question critical to understanding identifiability asks how many moment equations are needed to get finitely many solutions and how many solutions there are. We answer this question for classes of Gaussian mixture models using the tools of polyhedral geometry. In addition, we show that a generic Gaussian $k$-mixture model is identifiable from its first $3k+2$ moments. Using these results, we present a homotopy algorithm that performs parameter recovery for high dimensional Gaussian mixture models where the number of paths tracked scales linearly in the dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.15675v3</guid>
      <category>stat.ME</category>
      <category>math.AG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julia Lindberg, Carlos Am\'endola, Jose Israel Rodriguez</dc:creator>
    </item>
    <item>
      <title>Binary De Bruijn Processes</title>
      <link>https://arxiv.org/abs/2211.16921</link>
      <description>arXiv:2211.16921v2 Announce Type: replace-cross 
Abstract: Binary time series data are very common in many applications, and are typically modelled independently via a Bernoulli process with a single probability of success. However, the probability of a success can be dependent on the outcome successes of past events. Presented here is a novel approach for modelling binary time series data called a binary de Bruijn process which takes into account temporal correlation. The structure is derived from de Bruijn Graphs - a directed graph, where given a set of symbols, V, and a 'word' length, m, the nodes of the graph consist of all possible sequences of V of length m. De Bruijn Graphs are equivalent to mth order Markov chains, where the 'word' length controls the number of states that each individual state is dependent on. This increases correlation over a wider area. To quantify how clustered a sequence generated from a de Bruijn process is, the run lengths of letters are observed along with run length properties. Inference is also presented along with two application examples: precipitation data and the Oxford and Cambridge boat race.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.16921v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Louise Kimpton, Peter Challenor, Henry Wynn</dc:creator>
    </item>
    <item>
      <title>Spatial autoregressive model with measurement error in covariates</title>
      <link>https://arxiv.org/abs/2402.04593</link>
      <description>arXiv:2402.04593v2 Announce Type: replace-cross 
Abstract: The Spatial AutoRegressive model (SAR) is commonly used in studies involving spatial and network data to estimate the spatial or network peer influence and the effects of covariates on the response, taking into account the spatial or network dependence. While the model can be efficiently estimated with a Quasi maximum likelihood approach (QMLE), the detrimental effect of covariate measurement error on the QMLE and how to remedy it is currently unknown. If covariates are measured with error, then the QMLE may not have the $\sqrt{n}$ convergence and may even be inconsistent even when a node is influenced by only a limited number of other nodes or spatial units. We develop a measurement error-corrected ML estimator (ME-QMLE) for the parameters of the SAR model when covariates are measured with error. The ME-QMLE possesses statistical consistency and asymptotic normality properties. We consider two types of applications. The first is when the true covariate cannot be measured directly, and a proxy is observed instead. The second one involves including latent homophily factors estimated with error from the network for estimating peer influence. Our numerical results verify the bias correction property of the estimator and the accuracy of the standard error estimates in finite samples. We illustrate the method on a real dataset related to county-level death rates from the COVID-19 pandemic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04593v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subhadeep Paul, Shanjukta Nath</dc:creator>
    </item>
  </channel>
</rss>
