<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 10 Nov 2025 05:01:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Estimating Inhomogeneous Spatio-Temporal Background Intensity Functions using Graphical Dirichlet Processes</title>
      <link>https://arxiv.org/abs/2511.04974</link>
      <description>arXiv:2511.04974v1 Announce Type: new 
Abstract: An enhancement in seismic measuring instrumentation has been proven to have implications in the quantity of observed earthquakes, since denser networks usually allow recording more events. However, phenomena such as strong earthquakes or even aseismic transients, as slow slip earthquakes, may alter the occurrence of earthquakes. In the field of seismology, it is a standard practice to model background seismicity as a Poisson process. Based on this idea, this work proposes a model that can incorporate the evolving spatial intensity of Poisson processes over time (i.e., we include temporal changes in the background seismicity when modeling). In recent years, novel methodologies have been developed for quantifying the uncertainty in the estimation of the background seismicity in homogeneous cases using Bayesian non-parametric techniques. This work proposes a novel methodology based on graphical Dirichlet processes for incorporating spatial and temporal inhomogeneities in background seismicity. The proposed model in this work is applied to study the seismicity in the southern Mexico, using recorded data from 2000 to 2015.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04974v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isa\'ias Ba\~nales, Tomoaki Nishikawa, Yoshihiro Ito, Manuel J. Aguilar-Vel\'azquez</dc:creator>
    </item>
    <item>
      <title>On the Estimation of Climate Normals and Anomalies</title>
      <link>https://arxiv.org/abs/2511.05071</link>
      <description>arXiv:2511.05071v1 Announce Type: new 
Abstract: The quantification of the interannual component of variability in climatological time series is essential for the assessment and prediction of the El Ni\~{n}o - Southern Oscillation phenomenon. This is achieved by estimating the deviation of a climate variable (e.g., temperature, pressure, precipitation, or wind strength) from its normal conditions, defined by its baseline level and seasonal patterns. Climate normals are currently estimated by simple arithmetic averages calculated over the most recent 30-year period ending in a year divisible by 10. The suitability of the standard methodology has been questioned in the context of a changing climate, characterized by nonstationary conditions. The literature has focused on the choice of the bandwidth and the ability to account for trends induced by climate change. The paper contributes to the literature by proposing a regularized real time filter based on local trigonometric regression, optimizing the estimation bias-variance trade-off in the presence of climate change, and by introducing a class of seasonal kernels enhancing the localization of the estimates of climate normals. Application to sea surface temperature series in the \nino 3.4 region and zonal and trade winds strength in the equatorial and tropical Pacific region, illustrates the relevance of our proposal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05071v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tommaso Proietti, Alessandro Giovannelli</dc:creator>
    </item>
    <item>
      <title>Exponential Spatiotemporal GARCH Model with Asymmetric Volatility Spillovers</title>
      <link>https://arxiv.org/abs/2511.05126</link>
      <description>arXiv:2511.05126v1 Announce Type: new 
Abstract: This paper introduces a spatiotemporal exponential generalised autoregressive conditional heteroscedasticity (spatiotemporal E-GARCH) model, extending traditional spatiotemporal GARCH models by incorporating asymmetric volatility spillovers, while also generalising the time-series E-GARCH model to a spatiotemporal setting with instantaneous, potentially asymmetric volatility spillovers across space. The model allows for both temporal and spatial dependencies in volatility dynamics, capturing how financial shocks propagate across time, space, and network structures. We establish the theoretical properties of the model, deriving stationarity conditions and moment existence results. For estimation, we propose a quasi-maximum likelihood (QML) estimator and assess its finite-sample performance through Monte Carlo simulations. Empirically, we apply the model to financial networks, specifically analysing volatility spillovers in stock markets. We compare different network structures and analyse asymmetric effects in instantaneous volatility interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05126v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ariane Nidelle Meli Chrisko, Philipp Otto, Wolfgang Schmid</dc:creator>
    </item>
    <item>
      <title>Inference for the Extended Functional Cox Model: A UK Biobank Case Study</title>
      <link>https://arxiv.org/abs/2511.04852</link>
      <description>arXiv:2511.04852v1 Announce Type: cross 
Abstract: Multiple studies have shown that scalar summaries of objectively measured physical activity (PA) using accelerometers are the strongest predictors of mortality, outperforming all traditional risk factors, including age, sex, body mass index (BMI), and smoking. Here we show that diurnal patterns of PA and their day-to-day variability provide additional information about mortality. To do that, we introduce a class of extended functional Cox models and corresponding inferential tools designed to quantify the association between multiple functional and scalar predictors with time-to-event outcomes in large-scale (large $n$) high-dimensional (large $p$) datasets. Methods are applied to the UK Biobank study, which collected PA at every minute of the day for up to seven days, as well as time to mortality ($93{,}370$ participants with good quality accelerometry data and $931$ events). Simulation studies show that methods perform well in realistic scenarios and scale up to studies an order of magnitude larger than the UK Biobank accelerometry study. Establishing the feasibility and scalability of these methods for such complex and large data sets is a major milestone in applied Functional Data Analysis (FDA).</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04852v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erjia Cui, Angela Zhao, Ciprian M. Crainiceanu</dc:creator>
    </item>
    <item>
      <title>Clinical-ComBAT: a diffusion-weighted MRI harmonization method for clinical applications</title>
      <link>https://arxiv.org/abs/2511.04871</link>
      <description>arXiv:2511.04871v1 Announce Type: cross 
Abstract: Diffusion-weighted magnetic resonance imaging (DW-MRI) derived scalar maps are effective for assessing neurodegenerative diseases and microstructural properties of white matter in large number of brain conditions. However, DW-MRI inherently limits the combination of data from multiple acquisition sites without harmonization to mitigate scanner-specific biases. While the widely used ComBAT method reduces site effects in research, its reliance on linear covariate relationships, homogeneous populations, fixed site numbers, and well populated sites constrains its clinical use. To overcome these limitations, we propose Clinical-ComBAT, a method designed for real-world clinical scenarios. Clinical-ComBAT harmonizes each site independently, enabling flexibility as new data and clinics are introduced. It incorporates a non-linear polynomial data model, site-specific harmonization referenced to a normative site, and variance priors adaptable to small cohorts. It further includes hyperparameter tuning and a goodness-of-fit metric for harmonization assessment. We demonstrate its effectiveness on simulated and real data, showing improved alignment of diffusion metrics and enhanced applicability for normative modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04871v1</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gabriel Girard, Manon Edde, F\'elix Dumais, Yoan David, Matthieu Dumont, Guillaume Theaud, Jean-Christophe Houde, Arnaud Bor\'e, Maxime Descoteaux, Pierre-Marc Jodoin</dc:creator>
    </item>
    <item>
      <title>Uncertainty quantification and parameter optimization of plasma etching process using heteroscedastic Gaussian process</title>
      <link>https://arxiv.org/abs/2511.04990</link>
      <description>arXiv:2511.04990v1 Announce Type: cross 
Abstract: This study presents a comprehensive framework for uncertainty quantification (UQ) and design optimization of plasma etching in semiconductor manufacturing. The framework is demonstrated using experimental measurements of etched depth collected at nine wafer locations under various plasma conditions. A heteroscedastic Gaussian process (hetGP) surrogate model is employed to capture the complex uncertainty structure in the data, enabling distinct quantification of (a) spatial variability across the wafer and (b) process-related uncertainty arising from variations in chamber pressure, gas flow rate, and RF power. Epistemic uncertainty due to sparse data is further quantified and incorporated into a reliability-based design optimization (RBDO) scheme. The proposed method identifies optimal process parameters that minimize spatial variability of etch depth while maintaining reliability under both aleatory and epistemic uncertainties. The results demonstrate that this framework effectively integrates data-driven surrogate modeling with robust optimization, enhancing predictive accuracy and process reliability. Moreover, the proposed approach is generalizable to other semiconductor processes, such as photolithography, where performance is highly sensitive to multifaceted uncertainties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04990v1</guid>
      <category>physics.pop-ph</category>
      <category>stat.AP</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongsu Jung, Minji Kang, Muyoung Kim, Min Sup Choi, Hyeong-U Kim, Jaekwang Kim</dc:creator>
    </item>
    <item>
      <title>Function on Scalar Regression with Complex Survey Designs</title>
      <link>https://arxiv.org/abs/2511.05487</link>
      <description>arXiv:2511.05487v1 Announce Type: cross 
Abstract: Large health surveys increasingly collect high-dimensional functional data from wearable devices, and function on scalar regression (FoSR) is often used to quantify the relationship between these functional outcomes and scalar covariates such as age and sex. However, existing methods for FoSR fail to account for complex survey design. We introduce inferential methods for FoSR for studies with complex survey designs. The method combines fast univariate inference (FUI) developed for functional data outcomes and survey sampling inferential methods developed for scalar outcomes. Our approach consists of three steps: (1) fit survey weighted GLMs at each point along the functional domain, (2) smooth coefficients along the functional domain, and (3) use balanced repeated replication (BRR) or the Rao-Wu-Yue-Beaumont (RWYB) bootstrap to obtain pointwise and joint confidence bands for the functional coefficients. The method is motivated by association studies between continuous physical activity data and covariates collected in the National Health and Nutrition Examination Survey (NHANES). A first-of-its-kind analytical simulation study and empirical simulation using the NHANES data demonstrates that our method performs better than existing methods that do not account for the survey structure. Finally, application of the method in NHANES shows the practical implications of accounting for survey structure. The method is implemented in the R package svyfosr.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05487v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lily Koffman, Sunan Gao, Xinkai Zhou, Andrew Leroux, Ciprian Crainiceanu, John Muschelli III</dc:creator>
    </item>
    <item>
      <title>A semiparametric generalized exponential regression model with a principled distance-based prior</title>
      <link>https://arxiv.org/abs/2309.03165</link>
      <description>arXiv:2309.03165v2 Announce Type: replace 
Abstract: The generalized exponential distribution is a well-known probability model in lifetime data analysis and several other research areas, including precipitation modeling. Despite having broad applications for independently and identically distributed observations, its uses as a generalized linear model for non-identically distributed data are limited. This paper introduces a semiparametric Bayesian generalized exponential (GE) regression model. Our proposed approach involves modeling the GE rate parameter within a generalized additive model framework. An important feature is the integration of a principled distance-based prior for the GE shape parameter; this allows the model to shrink to an exponential regression model that retains the advantages of the exponential family. We draw inferences using the Markov chain Monte Carlo algorithm and discuss some theoretical results pertaining to Bayesian asymptotics. Extensive simulations demonstrate that the proposed model outperforms simpler alternatives. The Western Ghats mountain range holds critical importance in regulating monsoon rainfall across Southern India, profoundly impacting regional agriculture. Here, we analyze daily wet-day rainfall data for the monsoon months between 1901--2022 for the Northern, Middle, and Southern Western Ghats regions. Applying the proposed model to analyze the rainfall data over 122 years provides insights into model parameters, short-term temporal patterns, and the impact of climate change. We observe a significant decreasing trend in wet-day rainfall for the Southern Western Ghats region.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.03165v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arijit Dey, Arnab Hazra</dc:creator>
    </item>
    <item>
      <title>A General Simulation-Based Optimisation Framework for Multipoint Constant-Stress Accelerated Life Tests</title>
      <link>https://arxiv.org/abs/2507.00722</link>
      <description>arXiv:2507.00722v2 Announce Type: replace 
Abstract: Accelerated life testing (ALT) is a method of reducing the lifetime of components through exposure to extreme stress. This method of obtaining lifetime information involves the design of a testing experiment, i.e., an accelerated test plan. In this work, we adopt a simulation-based approach to obtaining optimal test plans for constant-stress accelerated life tests with multiple design points. Within this simulation framework we can easily assess a variety of test plans by modifying the number of test stresses (and their levels) and evaluating the allocation of test units. We obtain optimal test plans by utilising the differential evolution (DE) optimisation algorithm, where the inputs to the objective function are the test plan parameters, and the output is the RMSE (root mean squared error) of out-of-sample (extrapolated) model predictions. When the life-stress distribution is correctly specified, we show that the optimal number of stress levels is related to the number of model parameters. In terms of test unit allocation, we show that the proportion of test units is inversely related to the stress level. Our general simulation framework provides an alternative approach to theoretical optimisation, and is particularly favourable for large/complex multipoint test plans where analytical optimisation could prove intractable. Our procedure can be applied to a broad range of experimental scenarios, and serves as a useful tool to aid practitioners seeking to maximise component lifetime information through accelerated life testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00722v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Owen McGrath, Kevin Burke</dc:creator>
    </item>
    <item>
      <title>Parsimonious Gaussian mixture models with piecewise-constant eigenvalue profiles</title>
      <link>https://arxiv.org/abs/2507.01542</link>
      <description>arXiv:2507.01542v2 Announce Type: replace-cross 
Abstract: Gaussian mixture models (GMMs) are ubiquitous in statistical learning, particularly for unsupervised problems. While full GMMs suffer from the overparameterization of their covariance matrices in high-dimensional spaces, spherical GMMs (with isotropic covariance matrices) certainly lack flexibility to fit certain anisotropic distributions. Connecting these two extremes, we introduce a new family of parsimonious GMMs with piecewise-constant covariance eigenvalue profiles. These extend several low-rank models like the celebrated mixtures of probabilistic principal component analyzers (MPPCA), by enabling any possible sequence of eigenvalue multiplicities. If the latter are prespecified, then we can naturally derive an expectation-maximization (EM) algorithm to learn the mixture parameters. Otherwise, to address the notoriously-challenging issue of jointly learning the mixture parameters and hyperparameters, we propose a componentwise penalized EM algorithm, whose monotonicity is proven. We show the superior likelihood-parsimony tradeoffs achieved by our models on a variety of unsupervised experiments: density fitting, clustering and single-image denoising.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01542v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom Szwagier, Pierre-Alexandre Mattei, Charles Bouveyron, Xavier Pennec</dc:creator>
    </item>
    <item>
      <title>A Mega-Study of Digital Twins Reveals Strengths, Weaknesses and Opportunities for Further Improvement</title>
      <link>https://arxiv.org/abs/2509.19088</link>
      <description>arXiv:2509.19088v3 Announce Type: replace-cross 
Abstract: Digital representations of individuals ("digital twins") promise to transform social science and decision-making. Yet it remains unclear whether such twins truly mirror the people they emulate. We conducted 19 preregistered studies with a representative U.S. panel and their digital twins, each constructed from rich individual-level data, enabling direct comparisons between human and twin behavior across a wide range of domains and stimuli (including never-seen-before ones). Twins reproduced individual responses with 75% accuracy and seemingly low correlation with human answers (approximately 0.2). However, this apparently high accuracy was no higher than that achieved by generic personas based on demographics only. In contrast, correlation improved when twins incorporated detailed personal information, even outperforming traditional machine learning benchmarks that require additional data. Twins exhibited systematic strengths and weaknesses - performing better in social and personality domains, but worse in political ones - and were more accurate for participants with higher education, higher income, and moderate political views and religious attendance. Together, these findings delineate both the promise and the current limits of digital twins: they capture some relative differences among individuals but not yet the unique judgments of specific people. All data and code are publicly available to support the further development and evaluation of digital twin pipelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19088v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyi Peng, George Gui, Daniel J. Merlau, Grace Jiarui Fan, Malek Ben Sliman, Melanie Brucks, Eric J. Johnson, Vicki Morwitz, Abdullah Althenayyan, Silvia Bellezza, Dante Donati, Hortense Fong, Elizabeth Friedman, Ariana Guevara, Mohamed Hussein, Kinshuk Jerath, Bruce Kogut, Akshit Kumar, Kristen Lane, Hannah Li, Patryk Perkowski, Oded Netzer, Olivier Toubia</dc:creator>
    </item>
    <item>
      <title>Dynamical Systems Models for Market Evolution: A Mechanistic Alternative to Autoregressive Methods</title>
      <link>https://arxiv.org/abs/2510.06778</link>
      <description>arXiv:2510.06778v2 Announce Type: replace-cross 
Abstract: We present a novel approach to modeling market dynamics using ordinary differential equations that explicitly incorporates product competitiveness and consumer behavior. Our framework treats market segments as interacting populations in a dynamical system analogous to predator-prey models, where competitive advantages drive market share transitions through mechanistic modeling of market flows including new product adoption, refresh cycles, and obsolescence dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06778v2</guid>
      <category>math.DS</category>
      <category>stat.AP</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aparna Komarla, Max Hill</dc:creator>
    </item>
  </channel>
</rss>
