<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 20 May 2025 04:00:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A New Bayesian Bootstrap for Quantitative Trade and Spatial Models</title>
      <link>https://arxiv.org/abs/2505.11967</link>
      <description>arXiv:2505.11967v1 Announce Type: new 
Abstract: Economists use quantitative trade and spatial models to make counterfactual predictions. Because such predictions often inform policy decisions, it is important to communicate the uncertainty surrounding them. Three key challenges arise in this setting: the data are dyadic and exhibit complex dependence; the number of interacting units is typically small; and counterfactual predictions depend on the data in two distinct ways-through the estimation of structural parameters and through their role as inputs into the model's counterfactual equilibrium. I address these challenges by proposing a new Bayesian bootstrap procedure tailored to this context. The method is simple to implement and provides both finite-sample Bayesian and asymptotic frequentist guarantees. Revisiting the results in Waugh (2010), Caliendo and Parro (2015), and Artu\c{c} et al. (2010) illustrates the practical advantages of the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11967v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bas Sanders</dc:creator>
    </item>
    <item>
      <title>(Visualizing) Plausible Treatment Effect Paths</title>
      <link>https://arxiv.org/abs/2505.12014</link>
      <description>arXiv:2505.12014v1 Announce Type: new 
Abstract: We consider point estimation and inference for the treatment effect path of a policy. Examples include dynamic treatment effects in microeconomics, impulse response functions in macroeconomics, and event study paths in finance. We present two sets of plausible bounds to quantify and visualize the uncertainty associated with this object. Both plausible bounds are often substantially tighter than traditional confidence intervals, and can provide useful insights even when traditional (uniform) confidence bands appear uninformative. Our bounds can also lead to markedly different conclusions when there is significant correlation in the estimates, reflecting the fact that traditional confidence bands can be ineffective at visualizing the impact of such correlation. Our first set of bounds covers the average (or overall) effect rather than the entire treatment path. Our second set of bounds imposes data-driven smoothness restrictions on the treatment path. Post-selection Inference (Berk et al. [2013]) provides formal coverage guarantees for these bounds. The chosen restrictions also imply novel point estimates that perform well across our simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12014v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Freyaldenhoven, Christian Hansen</dc:creator>
    </item>
    <item>
      <title>Multivariate Affine GARCH with Heavy Tails: A Unified Framework for Portfolio Optimization and Option Valuation</title>
      <link>https://arxiv.org/abs/2505.12198</link>
      <description>arXiv:2505.12198v1 Announce Type: new 
Abstract: This paper develops and estimates a multivariate affine GARCH(1,1) model with Normal Inverse Gaussian innovations that captures time-varying volatility, heavy tails, and dynamic correlation across asset returns. We generalize the Heston-Nandi framework to a multivariate setting and apply it to 30 Dow Jones Industrial Average stocks. The model jointly supports three core financial applications: dynamic portfolio optimization, wealth path simulation, and option pricing. Closed-form solutions are derived for a Constant Relative Risk Aversion (CRRA) investor's intertemporal asset allocation, and we implement a forward-looking risk-adjusted performance comparison against Merton-style constant strategies. Using the model's conditional volatilities, we also construct implied volatility surfaces for European options, capturing skew and smile features. Empirically, we document substantial wealth-equivalent utility losses from ignoring time-varying correlation and tail risk. These findings underscore the value of a unified econometric framework for analyzing joint asset dynamics and for managing portfolio and derivative exposures under non-Gaussian risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12198v1</guid>
      <category>econ.EM</category>
      <category>q-fin.GN</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ayush Jha, Abootaleb Shirvani, Ali Jaffri, Svetlozar T. Rachev, Frank J. Fabozzi</dc:creator>
    </item>
    <item>
      <title>Opening the Black Box of Local Projections</title>
      <link>https://arxiv.org/abs/2505.12422</link>
      <description>arXiv:2505.12422v1 Announce Type: new 
Abstract: Local projections (LPs) are widely used in empirical macroeconomics to estimate impulse responses to policy interventions. Yet, in many ways, they are black boxes. It is often unclear what mechanism or historical episodes drive a particular estimate. We introduce a new decomposition of LP estimates into the sum of contributions of historical events, which is the product, for each time stamp, of a weight and the realization of the response variable. In the least squares case, we show that these weights admit two interpretations. First, they represent purified and standardized shocks. Second, they serve as proximity scores between the projected policy intervention and past interventions in the sample. Notably, this second interpretation extends naturally to machine learning methods, many of which yield impulse responses that, while nonlinear in predictors, still aggregate past outcomes linearly via proximity-based weights. Applying this framework to shocks in monetary and fiscal policy, global temperature, and the excess bond premium, we find that easily identifiable events-such as Nixon's interference with the Fed, stagflation, World War II, and the Mount Agung volcanic eruption-emerge as dominant drivers of often heavily concentrated impulse response estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12422v1</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Philippe Goulet Coulombe, Karin Klieber</dc:creator>
    </item>
    <item>
      <title>Machine learning the first stage in 2SLS: Practical guidance from bias decomposition and simulation</title>
      <link>https://arxiv.org/abs/2505.13422</link>
      <description>arXiv:2505.13422v1 Announce Type: new 
Abstract: Machine learning (ML) primarily evolved to solve "prediction problems." The first stage of two-stage least squares (2SLS) is a prediction problem, suggesting potential gains from ML first-stage assistance. However, little guidance exists on when ML helps 2SLS$\unicode{x2014}$or when it hurts. We investigate the implications of inserting ML into 2SLS, decomposing the bias into three informative components. Mechanically, ML-in-2SLS procedures face issues common to prediction and causal-inference settings$\unicode{x2014}$and their interaction. Through simulation, we show linear ML methods (e.g., post-Lasso) work well, while nonlinear methods (e.g., random forests, neural nets) generate substantial bias in second-stage estimates$\unicode{x2014}$potentially exceeding the bias of endogenous OLS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13422v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Connor Lennon, Edward Rubin, Glen Waddell</dc:creator>
    </item>
    <item>
      <title>From What Ifs to Insights: Counterfactuals in Causal Inference vs. Explainable AI</title>
      <link>https://arxiv.org/abs/2505.13324</link>
      <description>arXiv:2505.13324v1 Announce Type: cross 
Abstract: Counterfactuals play a pivotal role in the two distinct data science fields of causal inference (CI) and explainable artificial intelligence (XAI). While the core idea behind counterfactuals remains the same in both fields--the examination of what would have happened under different circumstances--there are key differences in how they are used and interpreted. We introduce a formal definition that encompasses the multi-faceted concept of the counterfactual in CI and XAI. We then discuss how counterfactuals are used, evaluated, generated, and operationalized in CI vs. XAI, highlighting conceptual and practical differences. By comparing and contrasting the two, we hope to identify opportunities for cross-fertilization across CI and XAI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13324v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Galit Shmueli, David Martens, Jaewon Yoo, Travis Greene</dc:creator>
    </item>
    <item>
      <title>Quasi-Bayesian Estimation and Inference with Control Functions</title>
      <link>https://arxiv.org/abs/2402.17374</link>
      <description>arXiv:2402.17374v2 Announce Type: replace 
Abstract: This paper introduces a quasi-Bayesian method that integrates frequentist nonparametric estimation with Bayesian inference in a two-stage process. Applied to an endogenous discrete choice model, the approach first uses kernel or sieve estimators to estimate the control function nonparametrically, followed by Bayesian methods to estimate the structural parameters. This combination leverages the advantages of both frequentist tractability for nonparametric estimation and Bayesian computational efficiency for complicated structural models. We analyze the asymptotic properties of the resulting quasi-posterior distribution, finding that its mean provides a consistent estimator for the parameters of interest, although its quantiles do not yield valid confidence intervals. However, bootstrapping the quasi-posterior mean accounts for the estimation uncertainty from the first stage, thereby producing asymptotically valid confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17374v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ruixuan Liu, Zhengfei Yu</dc:creator>
    </item>
    <item>
      <title>Quantifying the Internal Validity of Weighted Estimands</title>
      <link>https://arxiv.org/abs/2404.14603</link>
      <description>arXiv:2404.14603v3 Announce Type: replace 
Abstract: In this paper we study a class of weighted estimands, which we define as parameters that can be expressed as weighted averages of the underlying heterogeneous treatment effects. The popular ordinary least squares (OLS), two-stage least squares (2SLS), and two-way fixed effects (TWFE) estimands are all special cases within our framework. Our focus is on answering two questions concerning weighted estimands. First, under what conditions can they be interpreted as the average treatment effect for some (possibly latent) subpopulation? Second, when these conditions are satisfied, what is the upper bound on the size of that subpopulation, either in absolute terms or relative to a target population of interest? We argue that this upper bound provides a valuable diagnostic for empirical research. When a given weighted estimand corresponds to the average treatment effect for a small subset of the population of interest, we say its internal validity is low. Our paper develops practical tools to quantify the internal validity of weighted estimands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14603v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexandre Poirier, Tymon S{\l}oczy\'nski</dc:creator>
    </item>
    <item>
      <title>Pre-Training Estimators for Structural Models: Application to Consumer Search</title>
      <link>https://arxiv.org/abs/2505.00526</link>
      <description>arXiv:2505.00526v3 Announce Type: replace 
Abstract: We explore pretraining estimators for structural econometric models. The estimator is "pretrained" in the sense that the bulk of the computational cost and researcher effort occur during the construction of the estimator. Subsequent applications of the estimator to different datasets require little computational cost or researcher effort. The estimation leverages a neural net to recognize the structural model's parameter from data patterns. As an initial trial, this paper builds a pretrained estimator for a sequential search model that is known to be difficult to estimate. We evaluate the pretrained estimator on 12 real datasets. The estimation takes seconds to run and shows high accuracy. We provide the estimator at pnnehome.github.io. More generally, pretrained, off-the-shelf estimators can make structural models more accessible to researchers and practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00526v3</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanhao 'Max' Wei, Zhenling Jiang</dc:creator>
    </item>
    <item>
      <title>Calibration Strategies for Robust Causal Estimation: Theoretical and Empirical Insights on Propensity Score-Based Estimators</title>
      <link>https://arxiv.org/abs/2503.17290</link>
      <description>arXiv:2503.17290v3 Announce Type: replace-cross 
Abstract: The partitioning of data for estimation and calibration critically impacts the performance of propensity score based estimators like inverse probability weighting (IPW) and double/debiased machine learning (DML) frameworks. We extend recent advances in calibration techniques for propensity score estimation, improving the robustness of propensity scores in challenging settings such as limited overlap, small sample sizes, or unbalanced data. Our contributions are twofold: First, we provide a theoretical analysis of the properties of calibrated estimators in the context of DML. To this end, we refine existing calibration frameworks for propensity score models, with a particular emphasis on the role of sample-splitting schemes in ensuring valid causal inference. Second, through extensive simulations, we show that calibration reduces variance of inverse-based propensity score estimators while also mitigating bias in IPW, even in small-sample regimes. Notably, calibration improves stability for flexible learners (e.g., gradient boosting) while preserving the doubly robust properties of DML. A key insight is that, even when methods perform well without calibration, incorporating a calibration step does not degrade performance, provided that an appropriate sample-splitting approach is chosen.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17290v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sven Klaassen, Jan Rabenseifner, Jannis Kueck, Philipp Bach</dc:creator>
    </item>
  </channel>
</rss>
