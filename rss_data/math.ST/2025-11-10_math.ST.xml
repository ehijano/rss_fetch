<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Nov 2025 03:49:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Insights into Tail-Based and Order Statistics</title>
      <link>https://arxiv.org/abs/2511.04784</link>
      <description>arXiv:2511.04784v1 Announce Type: new 
Abstract: Heavy-tailed phenomena appear across diverse domains --from wealth and firm sizes in economics to network traffic, biological systems, and physical processes-- characterized by the disproportionate influence of extreme values. These distributions challenge classical statistical models, as their tails decay too slowly for conventional approximations to hold. Among their key descriptive measures are quantile contributions, which quantify the proportion of a total quantity (such as income, energy, or risk) attributed to observations above a given quantile threshold. This paper presents a theoretical study of the quantile contribution statistic and its relationship with order statistics. We derive a closed-form expression for the joint cumulative distribution function (CDF) of order statistics and, based on it, obtain an explicit CDF for quantile contributions applicable to small samples. We then investigate the asymptotic behavior of these contributions as the sample size increases, establishing the asymptotic normality of the numerator and characterizing the limiting distribution of the quantile contribution. Finally, simulation studies illustrate the convergence properties and empirical accuracy of the theoretical results, providing a foundation for applying quantile contributions in the analysis of heavy-tailed data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04784v1</guid>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hamidreza Maleki Almani</dc:creator>
    </item>
    <item>
      <title>Extending Characterizations of Multivariate Laws via Distance Distributions</title>
      <link>https://arxiv.org/abs/2511.04870</link>
      <description>arXiv:2511.04870v1 Announce Type: new 
Abstract: We extend a theorem of Maa, Pearl, and Bartoszynski, which links equality of interpoint distance distributions to equality of underlying multivariate distributions, beyond the restrictive class of homogeneous, translation-invariant distance functions. Our approach replaces geometric assumptions on the distance with analytic conditions: volume-regularity of distance-induced balls, Lebesgue differentiability with respect to the distance, and bounded centered oscillations of densities. Under these conditions, equality of interpoint distance distributions continues to imply equality of the generating laws. The result persists under monotone continuous transformations of homogeneous, translation-invariant distances, recovering the original statement, and it extends to compact Riemannian manifolds equipped with the geodesic metric. We further develop a quantitative version of the theorem, i.e., inequalities that connect discrepancies of interpoint distance distributions to the $L^2$-distance between densities, and obtain explicit rates under Ahlfors $\alpha$-regularity of the distance function and $\beta$-H\"older continuity of densities, capturing dependence on dimensionality. Several representative examples illustrate the applicability of the generalization to domain-specific distances used in modern statistics. The examples include non-homogeneous non-translation invariant distances such as Canberra, entropic distances, and the Bray--Curtis dissimilarity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04870v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annika Betken, Aljosa Marjanovic, Katharina Proksch</dc:creator>
    </item>
    <item>
      <title>Robust Forecasting of Sequences with Periodically Stationary Long Memory Multiplicative Seasonal Increments Observed with Noise and Cointegrated Sequences</title>
      <link>https://arxiv.org/abs/2511.04905</link>
      <description>arXiv:2511.04905v1 Announce Type: new 
Abstract: The problem of optimal estimation of linear functionals constructed from unobserved values of stochastic sequence with periodically stationary increments based on observations of the sequence with a periodically stationary noise is considered. For sequences with known spectral densities, we obtain formulas for calculating values of the mean square errors and the spectral characteristics of the optimal estimates of the functionals. Formulas that determine the least favorable spectral densities and minimax (robust) spectral characteristics of the optimal linear estimates of functionals are proposed in the case where spectral densities of the sequence are not exactly known while some sets of admissible spectral densities are given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04905v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.19139/soic-2310-5070-1408</arxiv:DOI>
      <arxiv:journal_reference>Statistics Opt. Inform. Comput., Vol. 10, March 2022, pp 295-338</arxiv:journal_reference>
      <dc:creator>Maksym Luz, Mykhailo Moklyachuk</dc:creator>
    </item>
    <item>
      <title>Nonparametric Inference on Unlabeled Histograms</title>
      <link>https://arxiv.org/abs/2511.05077</link>
      <description>arXiv:2511.05077v1 Announce Type: new 
Abstract: Statistical inference on histograms and frequency counts plays a central role in categorical data analysis. Moving beyond classical methods that directly analyze labeled frequencies, we introduce a framework that models the multiset of unlabeled histograms via a mixture distribution to better capture unseen domain elements in large-alphabet regime. We study the nonparametric maximum likelihood estimator (NPMLE) under this framework, and establish its optimal convergence rate under the Poisson setting. The NPMLE also immediately yields flexible and efficient plug-in estimators for functional estimation problems, where a localized variant further achieves the optimal sample complexity for a wide range of symmetric functionals. Extensive experiments on synthetic, real-world datasets, and large language models highlight the practical benefits of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05077v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yun Ma, Pengkun Yang</dc:creator>
    </item>
    <item>
      <title>Multivariate MM-estimators with auxiliary Scale for Linear Models with Structured Covariance Matrices</title>
      <link>https://arxiv.org/abs/2511.05134</link>
      <description>arXiv:2511.05134v1 Announce Type: new 
Abstract: We provide a unified approach to MM-estimation with auxiliary scale for balanced linear models with structured covariance matrices. This approach leads to estimators that are highly robust against outliers and highly efficient for normal data. These properties not only hold for estimators of the regression parameter, but also for estimators of scale invariant transformations of the variance parameters. Of main interest are MM-estimators for linear mixed effects models, but our approach also includes MM-estimators in several other standard multivariate models. We provide sufficient conditions for the existence of MM-functionals and MM-estimators, establish asymptotic properties such as consistency and asymptotic normality, and derive their robustness properties in terms of breakdown point and influence function. All the results are obtained for general identifiable covariance structures and are established under mild conditions on the distribution of the observations, which goes far beyond models with elliptically contoured densities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05134v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hendrik Paul Lopuhaa</dc:creator>
    </item>
    <item>
      <title>A Latent-Variable Formulation of the Poisson Canonical Polyadic Tensor Model: Maximum Likelihood Estimation and Fisher Information</title>
      <link>https://arxiv.org/abs/2511.05352</link>
      <description>arXiv:2511.05352v1 Announce Type: new 
Abstract: We establish parameter inference for the Poisson canonical polyadic (PCP) tensor model through a latent-variable formulation. Our approach exploits the observation that any random PCP tensor can be derived by marginalizing an unobservable random tensor of one dimension larger. The loglikelihood of this larger dimensional tensor, referred to as the "complete" loglikelihood, is comprised of multiple rank one PCP loglikelihoods. Using this methodology, we first derive non-iterative maximum likelihood estimators for the PCP model and demonstrate that several existing algorithms for fitting non-negative matrix and tensor factorizations are Expectation-Maximization algorithms. Next, we derive the observed and expected Fisher information matrices for the PCP model. The Fisher information provides us crucial insights into the well-posedness of the tensor model, such as the role that tensor rank plays in identifiability and indeterminacy. For the special case of rank one PCP models, we demonstrate that these results are greatly simplified.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05352v1</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlos Llosa-Vite, Daniel M. Dunlavy, Richard B. Lehoucq, Oscar L\'opez, Arvind Prasadan</dc:creator>
    </item>
    <item>
      <title>Near-Efficient and Non-Asymptotic Multiway Inference</title>
      <link>https://arxiv.org/abs/2511.05368</link>
      <description>arXiv:2511.05368v1 Announce Type: new 
Abstract: We establish non-asymptotic efficiency guarantees for tensor decomposition-based inference in count data models. Under a Poisson framework, we consider two related goals: (i) parametric inference, the estimation of the full distributional parameter tensor, and (ii) multiway analysis, the recovery of its canonical polyadic (CP) decomposition factors. Our main result shows that in the rank-one setting, a rank-constrained maximum-likelihood estimator achieves multiway analysis with variance matching the Cram\'{e}r-Rao Lower Bound (CRLB) up to absolute constants and logarithmic factors. This provides a general framework for studying "near-efficient" multiway estimators in finite-sample settings. For higher ranks, we illustrate that our multiway estimator may not attain the CRLB; nevertheless, CP-based parametric inference remains nearly minimax optimal, with error bounds that improve on prior work by offering more favorable dependence on the CP rank. Numerical experiments corroborate near-efficiency in the rank-one case and highlight the efficiency gap in higher-rank scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05368v1</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oscar L\'opez, Arvind Prasadan, Carlos Llosa-Vite, Richard B. Lehoucq, Daniel M. Dunlavy</dc:creator>
    </item>
    <item>
      <title>Sublinear iterations can suffice even for DDPMs</title>
      <link>https://arxiv.org/abs/2511.04844</link>
      <description>arXiv:2511.04844v1 Announce Type: cross 
Abstract: SDE-based methods such as denoising diffusion probabilistic models (DDPMs) have shown remarkable success in real-world sample generation tasks. Prior analyses of DDPMs have been focused on the exponential Euler discretization, showing guarantees that generally depend at least linearly on the dimension or initial Fisher information. Inspired by works in log-concave sampling (Shen and Lee, 2019), we analyze an integrator -- the denoising diffusion randomized midpoint method (DDRaM) -- that leverages an additional randomized midpoint to better approximate the SDE. Using a recently-developed analytic framework called the "shifted composition rule", we show that this algorithm enjoys favorable discretization properties under appropriate smoothness assumptions, with sublinear $\widetilde{O}(\sqrt{d})$ score evaluations needed to ensure convergence. This is the first sublinear complexity bound for pure DDPM sampling -- prior works which obtained such bounds worked instead with ODE-based sampling and had to make modifications to the sampler which deviate from how they are used in practice. We also provide experimental validation of the advantages of our method, showing that it performs well in practice with pre-trained image synthesis models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04844v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew S. Zhang, Stephen Huan, Jerry Huang, Nicholas M. Boffi, Sitan Chen, Sinho Chewi</dc:creator>
    </item>
    <item>
      <title>Training and Testing with Multiple Splits: A Central Limit Theorem for Split-Sample Estimators</title>
      <link>https://arxiv.org/abs/2511.04957</link>
      <description>arXiv:2511.04957v1 Announce Type: cross 
Abstract: As predictive algorithms grow in popularity, using the same dataset to both train and test a new model has become routine across research, policy, and industry. Sample-splitting attains valid inference on model properties by using separate subsamples to estimate the model and to evaluate it. However, this approach has two drawbacks, since each task uses only part of the data, and different splits can lead to widely different estimates. Averaging across multiple splits, I develop an inference approach that uses more data for training, uses the entire sample for testing, and improves reproducibility. I address the statistical dependence from reusing observations across splits by proving a new central limit theorem for a large class of split-sample estimators under arguably mild and general conditions. Importantly, I make no restrictions on model complexity or convergence rates. I show that confidence intervals based on the normal approximation are valid for many applications, but may undercover in important cases of interest, such as comparing the performance between two models. I develop a new inference approach for such cases, explicitly accounting for the dependence across splits. Moreover, I provide a measure of reproducibility for p-values obtained from split-sample estimators. Finally, I apply my results to two important problems in development and public economics: predicting poverty and learning heterogeneous treatment effects in randomized experiments. I show that my inference approach with repeated cross-fitting achieves better power than previous alternatives, often enough to find statistical significance that would otherwise be missed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04957v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bruno Fava</dc:creator>
    </item>
    <item>
      <title>Sensitivity analysis from a single input/output sample</title>
      <link>https://arxiv.org/abs/2303.17832</link>
      <description>arXiv:2303.17832v2 Announce Type: replace 
Abstract: The main objective of this paper is to estimate optimally Sobol' indices at any order when a unique input/output i.i.d.\ sample is available. Our approach stands on three main ingredients: semi-parametric estimation theory, high-order kernel estimation (inspired by the paper of Doksum in 1995), and mirror-type transformations as introduced in Bertin 2020 and Pujol 2022. We propose two different estimators. We prove that these estimators are asymptotically normal and efficient. Furthermore, we illustrate their numerical properties on standard examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.17832v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S\'ebastien Da Veiga (ENSAI), Fabrice Gamboa (IMT), Thierry Klein (IMT), Agn\`es Lagnoux (IMT), Cl\'ementine Prieur (AIRSEA)</dc:creator>
    </item>
    <item>
      <title>Inference with Mondrian Random Forests</title>
      <link>https://arxiv.org/abs/2310.09702</link>
      <description>arXiv:2310.09702v4 Announce Type: replace 
Abstract: Random forests are popular methods for regression and classification analysis, and many different variants have been proposed in recent years. One interesting example is the Mondrian random forest, in which the underlying constituent trees are constructed via a Mondrian process. We give precise bias and variance characterizations, along with a Berry-Esseen-type central limit theorem, for the Mondrian random forest regression estimator. By combining these results with a carefully crafted debiasing approach and an accurate variance estimator, we present valid statistical inference methods for the unknown regression function. These methods come with explicitly characterized error bounds in terms of the sample size, tree complexity parameter, and number of trees in the forest, and include coverage error rates for feasible confidence interval estimators. Our debiasing procedure for the Mondrian random forest also allows it to achieve the minimax-optimal point estimation convergence rate in mean squared error for multivariate $\beta$-H\"older regression functions, for all $\beta &gt; 0$, provided that the underlying tuning parameters are chosen appropriately. Efficient and implementable algorithms are devised for both batch and online learning settings, and we study the computational complexity of different Mondrian random forest implementations. Finally, simulations with synthetic data validate our theory and methodology, demonstrating their excellent finite-sample properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.09702v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Jason M. Klusowski, William G. Underwood</dc:creator>
    </item>
    <item>
      <title>Colored Gaussian directed acyclic graphical models</title>
      <link>https://arxiv.org/abs/2404.04024</link>
      <description>arXiv:2404.04024v3 Announce Type: replace 
Abstract: We study submodels of Gaussian DAG models defined by partial homogeneity constraints imposed on the model error variances and structural coefficients. We represent these models with colored DAGs and investigate their properties for use in statistical and causal inference. Local and global Markov properties are provided and shown to characterize the colored DAG model. Additional properties relevant to causal discovery are studied, including the existence and non-existence of faithful distributions and structural identifiability. Extending prior work of Peters and B\"uhlmann and Wu and Drton, we prove structural identifiability under the assumption of homogeneous structural coefficients, as well as for a family of models with partially homogeneous structural coefficients. The latter models, termed BPEC-DAGs, capture additional causal insights by clustering the direct causes of each node into communities according to their effect on their common target. An analogue of the GES algorithm for learning BPEC-DAGs is given and evaluated on real and synthetic data. Regarding model geometry, we provide a proof of a conjecture of Sullivant which generalizes to colored DAG models, colored undirected graphical models and directed ancestral graph models. The proof yields a tool for identification of Markov properties for any rationally parameterized model with globally, rationally identifiable parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04024v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tobias Boege, Kaie Kubjas, Pratik Misra, Liam Solus</dc:creator>
    </item>
    <item>
      <title>On the number of modes of Gaussian kernel density estimators</title>
      <link>https://arxiv.org/abs/2412.09080</link>
      <description>arXiv:2412.09080v3 Announce Type: replace 
Abstract: We consider the Gaussian kernel density estimator with bandwidth $\beta^{-\frac12}$ of $n$ iid Gaussian samples. Using the Kac-Rice formula and an Edgeworth expansion, we prove that the expected number of modes on the real line scales as $\Theta(\sqrt{\beta\log\beta})$ as $\beta,n\to\infty$ provided $n^c\lesssim \beta\lesssim n^{2-c}$ for some constant $c&gt;0$. An impetus behind this investigation is to determine the number of clusters to which Transformers are drawn in a metastable state.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09080v3</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Borjan Geshkovski, Philippe Rigollet, Yihang Sun</dc:creator>
    </item>
    <item>
      <title>Pseudo-Maximum Likelihood Theory for High-Dimensional Rank One Inference</title>
      <link>https://arxiv.org/abs/2503.01708</link>
      <description>arXiv:2503.01708v2 Announce Type: replace 
Abstract: We develop a pseudo-likelihood theory for rank one matrix estimation problems in the high dimensional limit. We prove a variational principle for the limiting pseudo-maximum likelihood which also characterizes the performance of the corresponding pseudo-maximum likelihood estimator. We show that this variational principle is universal and depends only on four parameters determined by the corresponding null model. Through this universality, we introduce a notion of equivalence for estimation problems of this type and, in particular, show that a broad class of estimation tasks, including community detection, sparse submatrix detection, and non-linear spiked matrix models, are equivalent to spiked matrix models. As an application, we obtain a complete description of the performance of the least-squares (or ``best rank one'') estimator for any rank one matrix estimation problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01708v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Curtis Grant, Aukosh Jagannath, Justin Ko</dc:creator>
    </item>
    <item>
      <title>Convergence and Optimality of the EM Algorithm Under Multi-Component Gaussian Mixture Models</title>
      <link>https://arxiv.org/abs/2509.08237</link>
      <description>arXiv:2509.08237v2 Announce Type: replace 
Abstract: Gaussian mixture models (GMMs) are fundamental statistical tools for modeling heterogeneous data. Due to the nonconcavity of the likelihood function, the Expectation-Maximization (EM) algorithm is widely used for parameter estimation of each Gaussian component. Existing analyses of the EM algorithm's convergence to the true parameter focus on either the two-component case or multi-component settings with known mixing probabilities and isotropic covariance matrices.
  In this work, we study the convergence of the EM algorithm for multi-component GMMs in full generality. The population-level EM is shown to converge to the true parameter when the smallest separation among all pairs of Gaussian components exceeds a logarithmic factor of the largest separation and the reciprocal of the minimal mixing probabilities. At the sample level, the EM algorithm is shown to be minimax rate-optimal, up to a logarithmic factor. We develop two distinct novel analytical approaches, each tailored to a different regime of separation, reflecting two complementary perspectives on the use of EM. As a byproduct of our analysis, we show that the EM algorithm, when used for community detection, also achieves the minimax optimal rate of misclustering error under milder separation conditions than spectral clustering and Lloyd's algorithm, an interesting result in its own right. Our analysis allows the number of components, the minimal mixing probabilities, the separation between Gaussian components and the dimension to grow with the sample size. Simulation studies corroborate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08237v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Bing, Dehan Kong, Bingqing Li</dc:creator>
    </item>
    <item>
      <title>Control variates for variance-reduced ratio of means estimators</title>
      <link>https://arxiv.org/abs/2510.13504</link>
      <description>arXiv:2510.13504v2 Announce Type: replace 
Abstract: The control variates method is a classical variance reduction technique for Monte Carlo estimators that exploits correlated auxiliary variables without introducing bias. In many applications, the quantity of interest can be expressed as a ratio of expectations. We propose a variance-reduced estimator for such ratios, which applies control variates to both the numerator and the denominator. The control variate coefficients are optimized jointly to minimize the variance of the resulting estimator. This approach theoretically guarantees variance reduction and naturally extends to approximate control variates. Simulation studies show significant variance reduction, particularly when correlations between variables and control variates are strong. The practical value of the method is illustrated on multi-fidelity applications: estimating a proportion in an aircraft design use case and a conditional value-at-risk in an electromagnetic dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13504v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Louison Bocquet-Nouaille, J\'er\^ome Morio, Benjamin Bobbia</dc:creator>
    </item>
    <item>
      <title>In-and-Out: Algorithmic Diffusion for Sampling Convex Bodies</title>
      <link>https://arxiv.org/abs/2405.01425</link>
      <description>arXiv:2405.01425v3 Announce Type: replace-cross 
Abstract: We present a new random walk for uniformly sampling high-dimensional convex bodies. It achieves state-of-the-art runtime complexity with stronger guarantees on the output than previously known, namely in R\'enyi divergence (which implies TV, $\mathcal{W}_2$, KL, $\chi^2$). The proof departs from known approaches for polytime algorithms for the problem -- we utilize a stochastic diffusion perspective to show contraction to the target distribution with the rate of convergence determined by functional isoperimetric constants of the target distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01425v3</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunbum Kook, Santosh S. Vempala, Matthew S. Zhang</dc:creator>
    </item>
    <item>
      <title>Identifying Drift, Diffusion, and Causal Structure from Temporal Snapshots</title>
      <link>https://arxiv.org/abs/2410.22729</link>
      <description>arXiv:2410.22729v3 Announce Type: replace-cross 
Abstract: Stochastic differential equations (SDEs) are a fundamental tool for modelling dynamic processes, including gene regulatory networks (GRNs), contaminant transport, financial markets, and image generation. However, learning the underlying SDE from data is a challenging task, especially if individual trajectories are not observable. Motivated by burgeoning research in single-cell datasets, we present the first comprehensive approach for jointly identifying the drift and diffusion of an SDE from its temporal marginals. Assuming linear drift and additive diffusion, we prove that these parameters are identifiable from marginals if and only if the initial distribution lacks any generalized rotational symmetries. We further prove that the causal graph of any SDE with additive diffusion can be recovered from the SDE parameters. To complement this theory, we adapt entropy-regularized optimal transport to handle anisotropic diffusion, and introduce APPEX (Alternating Projection Parameter Estimation from $X_0$), an iterative algorithm designed to estimate the drift, diffusion, and causal graph of an additive noise SDE, solely from temporal marginals. We show that APPEX iteratively decreases Kullback-Leibler divergence to the true solution, and demonstrate its effectiveness on simulated data from linear additive noise SDEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22729v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Guan, Joseph Janssen, Hossein Rahmani, Andrew Warren, Stephen Zhang, Elina Robeva, Geoffrey Schiebinger</dc:creator>
    </item>
    <item>
      <title>Covariate Adjustment in Randomized Experiments Motivated by Higher-Order Influence Functions</title>
      <link>https://arxiv.org/abs/2411.08491</link>
      <description>arXiv:2411.08491v3 Announce Type: replace-cross 
Abstract: Higher-Order Influence Functions (HOIF), developed in a series of papers over the past twenty years, are a fundamental theoretical device for constructing rate-optimal causal-effect estimators from observational studies. However, the value of HOIF for analyzing well-conducted randomized controlled trials (RCT) has not been explicitly explored. In the recent U.S. Food and Drug Administration and European Medicines Agency guidelines on the practice of covariate adjustment in analyzing RCT, in addition to the simple, unadjusted difference-in-mean estimator, it was also recommended to report the estimator adjusting for baseline covariates via a simple parametric working model, such as a linear model. However, when the number of baseline covariates $p$ is large, the recommendation is somewhat murky. In this paper, we show that HOIF-motivated estimators for the treatment-specific mean have significantly improved statistical properties compared to popular adjusted estimators in practice when $p$ is relatively large relative to the sample size $n$. We also characterize the conditions under which the HOIF-motivated estimator improves upon the unadjusted one. More importantly, we demonstrate that several state-of-the-art adjusted estimators proposed recently can be interpreted as particular HOIF-motivated estimators, thereby placing these estimators in a more unified framework. Numerical and empirical studies are conducted to corroborate our theoretical findings. An accompanying R package can be found on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08491v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sihui Zhao, Xinbo Wang, Lin Liu, Xin Zhang</dc:creator>
    </item>
    <item>
      <title>It's Hard to Be Normal: The Impact of Noise on Structure-agnostic Estimation</title>
      <link>https://arxiv.org/abs/2507.02275</link>
      <description>arXiv:2507.02275v3 Announce Type: replace-cross 
Abstract: Structure-agnostic causal inference studies how well one can estimate a treatment effect given black-box machine learning estimates of nuisance functions (like the impact of confounders on treatment and outcomes). Here, we find that the answer depends in a surprising way on the distribution of the treatment noise. Focusing on the partially linear model of \citet{robinson1988root}, we first show that the widely adopted double machine learning (DML) estimator is minimax rate-optimal for Gaussian treatment noise, resolving an open problem of \citet{mackey2018orthogonal}. Meanwhile, for independent non-Gaussian treatment noise, we show that DML is always suboptimal by constructing new practical procedures with higher-order robustness to nuisance errors. These \emph{ACE} procedures use structure-agnostic cumulant estimators to achieve $r$-th order insensitivity to nuisance errors whenever the $(r+1)$-st treatment cumulant is non-zero. We complement these core results with novel minimax guarantees for binary treatments in the partially linear model. Finally, using synthetic demand estimation experiments, we demonstrate the practical benefits of our higher-order robust estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02275v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jikai Jin, Lester Mackey, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>Phase Transition for Stochastic Block Model with more than $\sqrt{n}$ Communities</title>
      <link>https://arxiv.org/abs/2509.15822</link>
      <description>arXiv:2509.15822v2 Announce Type: replace-cross 
Abstract: Predictions from statistical physics postulate that recovery of the communities in Stochastic Block Model (SBM) is possible in polynomial time above, and only above, the Kesten-Stigum (KS) threshold. This conjecture has given rise to a rich literature, proving that non-trivial community recovery is indeed possible in SBM above the KS threshold, as long as the number $K$ of communities remains smaller than $\sqrt{n}$, where $n$ is the number of nodes in the observed graph. Failure of low-degree polynomials below the KS threshold was also proven when $K=o(\sqrt{n})$.
  When $K\geq \sqrt{n}$, Chin et al.(2025) recently prove that, in a sparse regime, community recovery in polynomial time is possible below the KS threshold by counting non-backtracking paths. This breakthrough result lead them to postulate a new threshold for the many communities regime $K\geq \sqrt{n}$. In this work, we provide evidences that confirm their conjecture for $K\geq \sqrt{n}$:
  1- We prove that, for any density of the graph, low-degree polynomials fail to recover communities below the threshold postulated by Chin et al.(2025);
  2- We prove that community recovery is possible in polynomial time above the postulated threshold, not only in the sparse regime of~Chin et al., but also in some (but not all) moderately sparse regimes by essentially by counting occurrences of cliques or self-avoiding paths of suitable size in the observed graph.
  In addition, we propose a detailed conjecture regarding the structure of motifs that are optimal in sparsity regimes not covered by cliques or self-avoiding paths counting. In particular, counting self-avoiding paths of length $\log(n)$--which is closely related to spectral algorithms based on the Non-Backtracking operator--is optimal only in the sparse regime. Other motif counts--unrelated to spectral properties--should be considered in denser regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15822v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandra Carpentier, Christophe Giraud, Nicolas Verzelen</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Extended Target Detection in mmWave Automotive Radar</title>
      <link>https://arxiv.org/abs/2509.26573</link>
      <description>arXiv:2509.26573v2 Announce Type: replace-cross 
Abstract: Millimeter-wave (mmWave) automotive radar systems offer high range resolution due to their wide bandwidth, enabling the detection of multiple spatially distributed scatterers from a single extended target, such as a vehicle. Traditional CFAR-based detection methods often treat these scatterers as independent point targets, thereby neglecting the inherent spatial structure of extended objects. To address this limitation, we propose a novel Range-Doppler (RD) segment-based statistical inference framework that captures the characteristic scattering profile of extended automotive targets. The framework employs Maximum Likelihood Estimation (MLE) for statistical parameter extraction and utilizes Gibbs sampling within a Markov Chain Monte Carlo (MCMC) scheme to model the posterior distribution of the segment features. A skewness-based test statistic, derived from the estimated distribution, is introduced for binary hypothesis testing to distinguish extended targets. Furthermore, we develop a detection pipeline incorporating Intersection over Union (IoU) metrics and peak-centric segment alignment, optimized for single-dwell radar operations. Comprehensive evaluations on both simulated and real-world datasets demonstrate the effectiveness of the proposed method, achieving enhanced detection accuracy and robustness in automotive radar scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26573v2</guid>
      <category>eess.SP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vinay Kulkarni, V. V. Reddy</dc:creator>
    </item>
    <item>
      <title>Correcting the Coverage Bias of Quantile Regression</title>
      <link>https://arxiv.org/abs/2511.00820</link>
      <description>arXiv:2511.00820v2 Announce Type: replace-cross 
Abstract: We develop a collection of methods for adjusting the predictions of quantile regression to ensure coverage. Our methods are model agnostic and can be used to correct for high-dimensional overfitting bias with only minimal assumptions. Theoretical results show that the estimates we develop are consistent and facilitate accurate calibration in the proportional asymptotic regime where the ratio of the dimension of the data and the sample size converges to a constant. This is further confirmed by experiments on both simulated and real data. A key component of our work is a new connection between the leave-one-out coverage and the fitted values of variables appearing in a dual formulation of the quantile regression problem. This facilitates the use of cross-validation in a variety of settings at significantly reduced computational costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00820v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Isaac Gibbs, John J. Cherian, Emmanuel J. Cand\`es</dc:creator>
    </item>
  </channel>
</rss>
