<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 21 May 2024 04:00:52 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 21 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Temporal and spatial downscaling for solar radiation</title>
      <link>https://arxiv.org/abs/2405.11046</link>
      <description>arXiv:2405.11046v1 Announce Type: new 
Abstract: Global and regional climate model projections are useful for gauging future patterns of climate variables, including solar radiation, but data from these models is often too coarse to assess local impacts. Within the context of solar radiation, the changing climate may have an effect on photovoltaic (PV) production, especially as the PV industry moves to extend plant lifetimes to 50 years. Predicting PV production while taking into account a changing climate requires data at a resolution that is useful for building PV plants. Although temporal and spatial downscaling of solar radiation data is widely studied, we present a novel method to downscale solar radiation data from daily averages to hourly profiles, while maintaining spatial correlation of parameters characterizing the diurnal profile of solar radiation. The method focuses on the use of a diurnal template which can be shifted and scaled according to the time or year and location and the use of thin plate splines for spatial downscaling. This analysis is applied to data from the National Solar Radiation Database housed at the National Renewable Energy Lab and a case study of the mentioned methods over several sub-regions of continental United States is presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11046v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maggie Bailey, Doug Nychka, Manajit Sengupta, Jaemo Yang, Soutir Bandyopadhyay</dc:creator>
    </item>
    <item>
      <title>Real Time Monitoring and Forecasting of COVID 19 Cases using an Adjusted Holt based Hybrid Model embedded with Wavelet based ANN</title>
      <link>https://arxiv.org/abs/2405.11213</link>
      <description>arXiv:2405.11213v1 Announce Type: new 
Abstract: Since the inception of the SARS - CoV - 2 (COVID - 19) novel coronavirus, a lot of time and effort is being allocated to estimate the trajectory and possibly, forecast with a reasonable degree of accuracy, the number of cases, recoveries, and deaths due to the same. The model proposed in this paper is a mindful step in the same direction. The primary model in question is a Hybrid Holt's Model embedded with a Wavelet-based ANN. To test its forecasting ability, we have compared three separate models, the first, being a simple ARIMA model, the second, also an ARIMA model with a wavelet-based function, and the third, being the proposed model. We have also compared the forecast accuracy of this model with that of a modern day Vanilla LSTM recurrent neural network model. We have tested the proposed model on the number of confirmed cases (daily) for the entire country as well as 6 hotspot states. We have also proposed a simple adjustment algorithm in addition to the hybrid model so that daily and/or weekly forecasts can be meted out, with respect to the entirety of the country, as well as a moving window performance metric based on out-of-sample forecasts. In order to have a more rounded approach to the analysis of COVID-19 dynamics, focus has also been given to the estimation of the Basic Reproduction Number, $R_0$ using a compartmental epidemiological model (SIR). Lastly, we have also given substantial attention to estimating the shelf-life of the proposed model. It is obvious yet noteworthy how an accurate model, in this regard, can ensure better allocation of healthcare resources, as well as, enable the government to take necessary measures ahead of time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11213v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agniva Das, Kunnummal Muralidharan</dc:creator>
    </item>
    <item>
      <title>Uncover mortality patterns and hospital effects in COVID-19 heart failure patients: a novel Multilevel logistic cluster-weighted modeling approach</title>
      <link>https://arxiv.org/abs/2405.11239</link>
      <description>arXiv:2405.11239v1 Announce Type: new 
Abstract: Evaluating hospitals' performance and its relation to patients' characteristics is of utmost importance to ensure timely, effective, and optimal treatment. Such a matter is particularly relevant in areas and situations where the healthcare system must contend with an unexpected surge in hospitalizations, such as for heart failure patients in the Lombardy region of Italy during the COVID-19 pandemic. Motivated by this issue, the paper introduces a novel Multilevel Logistic Cluster-Weighted Model (ML-CWMd) for predicting 45-day mortality following hospitalization due to COVID-19. The methodology flexibly accommodates dependence patterns among continuous, categorical, and dichotomous variables; effectively accounting for hospital-specific effects in distinct patient subgroups showing different attributes. A tailored Expectation-Maximization algorithm is developed for parameter estimation, and extensive simulation studies are conducted to evaluate its performance against competing models. The novel approach is applied to administrative data from the Lombardy Region, aiming to profile heart failure patients hospitalized for COVID-19 and investigate the hospital-level impact on their overall mortality. A scenario analysis demonstrates the model's efficacy in managing multiple sources of heterogeneity, thereby yielding promising results in aiding healthcare providers and policy-makers in the identification of patient-specific treatment pathways.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11239v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luca Caldera, Chiara Masci, Andrea Cappozzo, Marco Forlani, Barbara Antonelli, Olivia Leoni, Francesca Ieva</dc:creator>
    </item>
    <item>
      <title>Delivery strategies to improve piglets exposure to oral antibiotics</title>
      <link>https://arxiv.org/abs/2405.11004</link>
      <description>arXiv:2405.11004v1 Announce Type: cross 
Abstract: The widespread practice of delivering antibiotics through drinking water to livestock leads to considerable variability in exposure levels among animals, raising concerns regarding disease outbreaks and the emergence of antibiotic resistance. This variability is primarily driven by three pivotal factors: fluctuations in drug concentration within water pipes, variances in drinking behavior among animals, and differences in individual pharmacokinetic parameters. This article introduces an approach aimed at improving medication distribution by customizing it according to the drinking patterns of pigs, without escalating the medication dose. As examples, we demonstrate that incorporating the drinking behavior into the delivery of amoxicillin results in an increase in the percentage of piglets reaching an AUC/MIC ratio greater than 25h. Specifically, with Pasteurella multocida, the percentage rises from 30% to at least 60%, while with Actinobacillus pleuropneumoniae, it increases from 20% to more than 70%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11004v1</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noslen Hern\'andez (INRAE, InTheRes), B\'eatrice B. Roques (INRAE, InTheRes), Marl\`ene Z. Lacroix (INRAE, InTheRes), Didier Concordet (ENVT, INRAE, InTheRes)</dc:creator>
    </item>
    <item>
      <title>Adaptive Online Experimental Design for Causal Discovery</title>
      <link>https://arxiv.org/abs/2405.11548</link>
      <description>arXiv:2405.11548v1 Announce Type: cross 
Abstract: Causal discovery aims to uncover cause-and-effect relationships encoded in causal graphs by leveraging observational, interventional data, or their combination. The majority of existing causal discovery methods are developed assuming infinite interventional data. We focus on data interventional efficiency and formalize causal discovery from the perspective of online learning, inspired by pure exploration in bandit problems. A graph separating system, consisting of interventions that cut every edge of the graph at least once, is sufficient for learning causal graphs when infinite interventional data is available, even in the worst case. We propose a track-and-stop causal discovery algorithm that adaptively selects interventions from the graph separating system via allocation matching and learns the causal graph based on sampling history. Given any desired confidence value, the algorithm determines a termination condition and runs until it is met. We analyze the algorithm to establish a problem-dependent upper bound on the expected number of required interventional samples. Our proposed algorithm outperforms existing methods in simulations across various randomly generated causal graphs. It achieves higher accuracy, measured by the structural hamming distance (SHD) between the learned causal graph and the ground truth, with significantly fewer samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11548v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Qasim Elahi, Lai Wei, Murat Kocaoglu, Mahsa Ghasemi</dc:creator>
    </item>
    <item>
      <title>Morphological Prototyping for Unsupervised Slide Representation Learning in Computational Pathology</title>
      <link>https://arxiv.org/abs/2405.11643</link>
      <description>arXiv:2405.11643v1 Announce Type: cross 
Abstract: Representation learning of pathology whole-slide images (WSIs) has been has primarily relied on weak supervision with Multiple Instance Learning (MIL). However, the slide representations resulting from this approach are highly tailored to specific clinical tasks, which limits their expressivity and generalization, particularly in scenarios with limited data. Instead, we hypothesize that morphological redundancy in tissue can be leveraged to build a task-agnostic slide representation in an unsupervised fashion. To this end, we introduce PANTHER, a prototype-based approach rooted in the Gaussian mixture model that summarizes the set of WSI patches into a much smaller set of morphological prototypes. Specifically, each patch is assumed to have been generated from a mixture distribution, where each mixture component represents a morphological exemplar. Utilizing the estimated mixture parameters, we then construct a compact slide representation that can be readily used for a wide range of downstream tasks. By performing an extensive evaluation of PANTHER on subtyping and survival tasks using 13 datasets, we show that 1) PANTHER outperforms or is on par with supervised MIL baselines and 2) the analysis of morphological prototypes brings new qualitative and quantitative insights into model interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11643v1</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew H. Song, Richard J. Chen, Tong Ding, Drew F. K. Williamson, Guillaume Jaume, Faisal Mahmood</dc:creator>
    </item>
    <item>
      <title>Estimating optimal tailored active surveillance strategy under interval censoring</title>
      <link>https://arxiv.org/abs/2405.11720</link>
      <description>arXiv:2405.11720v1 Announce Type: cross 
Abstract: Active surveillance (AS) using repeated biopsies to monitor disease progression has been a popular alternative to immediate surgical intervention in cancer care. However, a biopsy procedure is invasive and sometimes leads to severe side effects of infection and bleeding. To reduce the burden of repeated surveillance biopsies, biomarker-assistant decision rules are sought to replace the fix-for-all regimen with tailored biopsy intensity for individual patients. Constructing or evaluating such decision rules is challenging. The key AS outcome is often ascertained subject to interval censoring. Furthermore, patients will discontinue their participation in the AS study once they receive a positive surveillance biopsy. Thus, patient dropout is affected by the outcomes of these biopsies. In this work, we propose a nonparametric kernel-based method to estimate the true positive rates (TPRs) and true negative rates (TNRs) of a tailored AS strategy, accounting for interval censoring and immediate dropouts. Based on these estimates, we develop a weighted classification framework to estimate the optimal tailored AS strategy and further incorporate the cost-benefit ratio for cost-effectiveness in medical decision-making. Theoretically, we provide a uniform generalization error bound of the derived AS strategy accommodating all possible trade-offs between TPRs and TNRs. Simulation and application to a prostate cancer surveillance study show the superiority of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11720v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Muxuan Liang, Yingqi Zhao, Daniel W. Lin, Matthew Cooperberg, Yingye Zheng</dc:creator>
    </item>
    <item>
      <title>Comparing predictive ability in presence of instability over a very short time</title>
      <link>https://arxiv.org/abs/2405.11954</link>
      <description>arXiv:2405.11954v1 Announce Type: cross 
Abstract: We consider forecast comparison in the presence of instability when this affects only a short period of time. We demonstrate that global tests do not perform well in this case, as they were not designed to capture very short-lived instabilities, and their power vanishes altogether when the magnitude of the shock is very large. We then discuss and propose approaches that are more suitable to detect such situations, such as nonparametric methods (S test or MAX procedure). We illustrate these results in different Monte Carlo exercises and in evaluating the nowcast of the quarterly US nominal GDP from the Survey of Professional Forecasters (SPF) against a naive benchmark of no growth, over the period that includes the GDP instability brought by the Covid-19 crisis. We recommend that the forecaster should not pool the sample, but exclude the short periods of high local instability from the evaluation exercise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11954v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabrizio Iacone, Luca Rossini, Andrea Viselli</dc:creator>
    </item>
    <item>
      <title>Estimations of the Local Conditional Tail Average Treatment Effect</title>
      <link>https://arxiv.org/abs/2109.08793</link>
      <description>arXiv:2109.08793v3 Announce Type: replace 
Abstract: The conditional tail average treatment effect (CTATE) is defined as a difference between the conditional tail expectations of potential outcomes, which can capture heterogeneity and deliver aggregated local information on treatment effects over different quantile levels and is closely related to the notion of second-order stochastic dominance and the Lorenz curve. These properties render it a valuable tool for policy evaluation. In this paper, we study estimation of the CTATE locally for a group of compliers (local CTATE or LCTATE) under the two-sided noncompliance framework. We consider a semiparametric treatment effect framework under endogeneity for the LCTATE estimation using a newly introduced class of consistent loss functions jointly for the conditional tail expectation and quantile. We establish the asymptotic theory of our proposed LCTATE estimator and provide an efficient algorithm for its implementation. We then apply the method to evaluate the effects of participating in programs under the Job Training Partnership Act in the US.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.08793v3</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Le-Yu Chen, Yu-Min Yen</dc:creator>
    </item>
  </channel>
</rss>
