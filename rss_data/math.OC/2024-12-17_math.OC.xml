<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Dec 2024 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Uncertainty propagation of stochastic hybrid systems: a case study for types of jump</title>
      <link>https://arxiv.org/abs/2412.10517</link>
      <description>arXiv:2412.10517v1 Announce Type: new 
Abstract: Stochastic hybrid systems are dynamic systems that undergo both random continuous-time flows and random discrete jumps. Depending on how randomness is introduced into the continuous dynamics, discrete transitions, or both, stochastic hybrid systems exhibit distinct characteristics. This paper investigates the role of uncertainties in the interplay between continuous flows and discrete jumps by studying probability density propagation. Specifically, we formulate stochastic Koopman/Frobenius-Perron operators for three types of one-dimensional stochastic hybrid systems to uncover their unique dynamic characteristics and verify them using Monte Carlo simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10517v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tejaswi K. C., William Clark, Taeyoung Lee</dc:creator>
    </item>
    <item>
      <title>Mean Field Game and Control for Switching Hybrid Systems</title>
      <link>https://arxiv.org/abs/2412.10522</link>
      <description>arXiv:2412.10522v1 Announce Type: new 
Abstract: Mean field games and controls involve guiding the behavior of large populations of interacting agents, where each individual's influence on the group is negligible but collectively impacts overall dynamics. Hybrid systems integrate continuous dynamics with discrete transitions, effectively modeling the complex interplay between continuous flows and instantaneous jumps in a unified framework. This paper formulates mean field game and control strategies for switching hybrid systems and proposes computational methods to solve the resulting integro-partial differential equation. This approach enables scalable, decentralized decision-making in large-scale switching systems, which is illustrated through numerical examples in an emergency evacuation scenario with sudden changes in the surrounding environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10522v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tejaswi K. C., Taeyoung Lee</dc:creator>
    </item>
    <item>
      <title>Certainty-Equivalence Model Predictive Control: Stability, Performance, and Beyond</title>
      <link>https://arxiv.org/abs/2412.10625</link>
      <description>arXiv:2412.10625v1 Announce Type: new 
Abstract: Handling model mismatch is a common challenge in model-based controller design, particularly in model predictive control (MPC). While robust MPC is effective in managing uncertainties, its inherent conservatism often makes it less desirable in practice. Certainty-equivalence MPC (CE-MPC), which relies on a nominal model, offers an appealing alternative due to its design simplicity and reduced online computational requirements. Contrary to the existing analysis where MPC has access to the true model, this paper investigates certainty-equivalence MPC for uncertain nonlinear systems with input constraints; here the uncertainty stems from parametric discrepancies between the true model and the nominal model. The primary contributions of the paper are twofold. First, a novel perturbation analysis of the MPC value function is provided, which avoids the common assumption of Lipschitz continuity for cost functions and has broader applicability to topics such as value function approximation, online model adaptation in MPC, and performance-driven MPC design. Besides, a sufficient stability condition and performance analysis of CE-MPC are derived, where the latter quantifies the suboptimality of CE-MPC relative to the infinite-horizon optimal controller with perfect model knowledge. The results provide valuable insights into how the prediction horizon and model mismatch jointly affect stability and performance. Furthermore, the general results are specialized to linear quadratic control, and the competitive ratio is derived, serving as the first ratio-like performance bound for predictive control of uncertain linear systems with constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10625v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Changrui Liu, Shengling Shi, Bart De Schutter</dc:creator>
    </item>
    <item>
      <title>A General Solution to Bellman's Lost-in-a-forest Problem</title>
      <link>https://arxiv.org/abs/2412.10686</link>
      <description>arXiv:2412.10686v1 Announce Type: new 
Abstract: We present a general solution to Bellman's lost-in-a-forest problem. The forest boundary is known and may take any shape. The starting point and the orientation are unspecified. We convert the problem into translation and rotation of the forest boundary. This transformation allows us to formulate this problem as a constrained minimization problem. Upon discretization, the problem becomes a variation of the traveling salesman problem or the Hamiltonian path problem. We derive several nontrivial results consistent with those from previous papers. This method is general, and we also extend the approach to solve related problems, including Moser's worm problem and the Opaque set problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10686v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhipeng Deng</dc:creator>
    </item>
    <item>
      <title>Destruction of the Resident Enterprise in the Special Economic Zone with Sanctions</title>
      <link>https://arxiv.org/abs/2412.10811</link>
      <description>arXiv:2412.10811v1 Announce Type: new 
Abstract: The activity of a special economic zone is defined by a dynamic equation, taking into account the individual strategies of residents. At a given point in time, in respect to the resident enterprise of a special economic zone, a regime is introduced that limits the flow of resources by 80% (sanctions), forming an integral indicator for a comprehensive assessment of the impact of sanctions on the enterprise. On the basis of the dynamic equation, an estimate of the economic damage for the potential SEZ of the Krasnoyarsk Territory is given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10811v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/MLSD.2019.8910997</arxiv:DOI>
      <arxiv:journal_reference>2019 Twelfth International Conference "Management of large-scale system development" (MLSD), Moscow, Russia, 2019, pp. 1-5</arxiv:journal_reference>
      <dc:creator>Sergei Masaev</dc:creator>
    </item>
    <item>
      <title>Depth of planning the state of a dynamic discrete system by autocorrelation function</title>
      <link>https://arxiv.org/abs/2412.10813</link>
      <description>arXiv:2412.10813v1 Announce Type: new 
Abstract: The production system (multidimensional object) is considered as a dynamic system with discrete time. Formalized: space (state of the object, control actions, goals, observed values, analytical estimates). Analytical estimates of the state of a dynamic system are formed through the autocorrelation function. The autocorrelation function is calculated with the regulator setting the length of the analyzed time series (analysis depth). A digital copy of the production system is created, characterized by 1.2 million parameters. Modeling the activities of the production system is performed in the author's complex of programs. In total, twenty-eight controller states are calculated to analyze the effect of repeating parameters affecting the activity of the production system. The simulation shows the cyclical dynamics of changes in the autocorrelation function. Formalization of the production system is carried out, which allows you to move on to other methods of analysis of the production system: Kalman filter, neural network forecast, recurrence equation, balances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10813v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/RusAutoCon49822.2020.9208187</arxiv:DOI>
      <arxiv:journal_reference>2020 International Russian Automation Conference (RusAutoCon), Sochi, Russia, 2020, pp. 989-993</arxiv:journal_reference>
      <dc:creator>Sergei Masaev</dc:creator>
    </item>
    <item>
      <title>A Stability Condition for Online Feedback Optimization without Timescale Separation</title>
      <link>https://arxiv.org/abs/2412.10964</link>
      <description>arXiv:2412.10964v1 Announce Type: new 
Abstract: Online Feedback Optimization (OFO) is a control approach to drive a dynamical plant to an optimal steady state. By interconnecting optimization algorithms with real-time plant measurements, OFO provides all the benefits of feedback control, yet without requiring exact knowledge of plant dynamics for computing a setpoint. On the downside, existing stability guarantees for OFO require the controller to evolve on a sufficiently slower timescale than the plant, possibly affecting transient performance and responsiveness to disturbances. In this paper, we prove that, under suitable conditions, OFO ensures stability without any timescale separation. In particular, the condition we propose is independent of the time constant of the plant, hence it is scaling-invariant. Our analysis leverages a composite Lyapunov function, which is the $\max$ of plant-related and controller-related components. We corroborate our theoretical results with numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10964v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.DS</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mattia Bianchi, Florian D\"orfler</dc:creator>
    </item>
    <item>
      <title>Stochastic Approximation and Brownian Repulsion based Evolutionary Search</title>
      <link>https://arxiv.org/abs/2412.11036</link>
      <description>arXiv:2412.11036v1 Announce Type: new 
Abstract: Many global optimization algorithms of the memetic variety rely on some form of stochastic search, and yet they often lack a sound probabilistic basis. Without a recourse to the powerful tools of stochastic calculus, treading the fine balance between exploration and exploitation could be tricky. In this work, we propose an evolutionary algorithm (EA) comprising two types of additive updates. The primary update utilizes stochastic approximation to guide a population of randomly initialized particles towards an optimum. We incorporate derivative-free Robbins-Monro type gains in the first update so as to provide a directional guidance to the candidate solutions. The secondary update leverages stochastic conditioning to apply random perturbations for a controlled yet efficient exploration. Specifically, conceptualized on a change of measures, the perturbation strategy discourages two or more trajectories exploring the same region of the search space. Our optimization algorithm, dubbed as SABRES (Stochastic Approximation and Brownian Repulsion based Evolutionary Search), is applied to CEC-2022 benchmark functions on global optimization. Numerical results are indicative of the potentialities of SABRES in solving a variety of challenging multi-modal, non-separable, and asymmetrical benchmark functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11036v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rajdeep Dutta, T Venkatesh Varma, Saikat Sarkar, Mariya Mamajiwala, Noor Awad, Senthilnath Jayavelu, Debasish Roy</dc:creator>
    </item>
    <item>
      <title>A Note on Valid Inequalities for PageRank Optimization with Edge Selection Constraints</title>
      <link>https://arxiv.org/abs/2412.11071</link>
      <description>arXiv:2412.11071v1 Announce Type: new 
Abstract: Cs\'{a}ji, Jungers, and Blondel prove that while a PageRank optimization problem with edge selection constraints is NP-hard, it can be solved optimally in polynomial time for the unconstrained case. This theoretical result is accompanied by several observations, which we leverage to develop valid inequalities in polynomial time for this class of NP-hard problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11071v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shang-Ru Yang, Yung-Han Liao, Chih-Ching Chien, Hao-Hsiang Wu</dc:creator>
    </item>
    <item>
      <title>Reliably Learn to Trim Multiparametric Quadratic Programs via Constraint Removal</title>
      <link>https://arxiv.org/abs/2412.11098</link>
      <description>arXiv:2412.11098v1 Announce Type: new 
Abstract: In a wide range of applications, we are required to rapidly solve a sequence of convex multiparametric quadratic programs (mp-QPs) on resource-limited hardwares. This is a nontrivial task and has been an active topic for decades in control and optimization communities. Observe that the main computational cost of existing solution algorithms lies in addressing many linear inequality constraints, though their majority are redundant and removing them will not change the optimal solution. This work learns from the results of previously solved mp-QP(s), based on which we propose novel methods to reliably trim (unsolved) mp-QPs via constraint removal, and the trimmed mp-QPs can be much cheaper to solve. Then, we extend to trim mp-QPs of model predictive control (MPC) whose parameter vectors are sampled from linear systems. Importantly, both online and offline solved mp-QPs can be utilized to adaptively trim mp-QPs in the closed-loop system. We show that the number of linear inequalities in the trimmed mp-QP of MPC decreases to zero in a finite timestep, which also can be reduced by increasing offline computation. Finally, simulations are performed to demonstrate the efficiency of our trimming method in removing redundant constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11098v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhinan Hou, Keyou You</dc:creator>
    </item>
    <item>
      <title>Distributed Bilevel Optimization via Adaptive Penalization with Time-Scale Separation</title>
      <link>https://arxiv.org/abs/2412.11218</link>
      <description>arXiv:2412.11218v1 Announce Type: new 
Abstract: This paper studies a class of distributed bilevel optimization (DBO) problems with a coupled inner-level subproblem. Existing approaches typically rely on hypergradient estimations involving computationally expensive Hessian information. To address this, we propose an equivalent constrained reformulation by treating the inner-level subproblem as an inequality constraint, and introduce an adaptive penalty function to properly penalize both inequality and consensus constraints based on subproblem properties. Moreover, we propose a loopless distributed algorithm, \ALGNAME, that employs multiple-timescale updates to solve each subproblem asymptotically without requiring Hessian information. Theoretically, we establish convergence rates of $\mathcal{O}(\frac{\kappa^4}{(1-\rho)^2 K^{1/3}})$ for nonconvex-strongly-convex cases and $\mathcal{O}(\frac{\kappa^2}{(1-\rho)^2 K^{2/3}})$ for distributed min-max problems. Our analysis shows the clear dependence of convergence performance on bilevel heterogeneity, the adaptive penalty parameter, and network connectivity, with a weaker assumption on heterogeneity requiring only bounded first-order heterogeneity at the optimum. Numerical experiments validate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11218v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Youcheng Niu, Jinming Xu, Ying Sun, Li Chai, Jiming Chen</dc:creator>
    </item>
    <item>
      <title>A preconditioned inexact infeasible quantum interior point method for linear optimization</title>
      <link>https://arxiv.org/abs/2412.11307</link>
      <description>arXiv:2412.11307v1 Announce Type: new 
Abstract: Quantum Interior Point Methods (QIPMs) have been attracting significant interests recently due to their potential of solving optimization problems substantially faster than state-of-the-art conventional algorithms. In general, QIPMs use Quantum Linear System Algorithms (QLSAs) to substitute classical linear system solvers. However, the performance of QLSAs depends on the condition numbers of the linear systems, which are typically proportional to the square of the reciprocal of the duality gap in QIPMs. To improve conditioning, a preconditioned inexact infeasible QIPM (II-QIPM) based on optimal partition estimation is developed in this work. We improve the condition number of the linear systems in II-QIPMs from quadratic dependence on the reciprocal of the duality gap to linear, and obtain better dependence with respect to the accuracy when compared to other II-QIPMs. Our method also attains better dependence with respect to the dimension when compared to other inexact infeasible Interior Point Methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11307v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zeguan Wu, Xiu Yang, Tam\'as Terlaky</dc:creator>
    </item>
    <item>
      <title>Stable Recovery of Regularized Linear Inverse Problems</title>
      <link>https://arxiv.org/abs/2412.11313</link>
      <description>arXiv:2412.11313v1 Announce Type: new 
Abstract: Recovering a low-complexity signal from its noisy observations by regularization methods is a cornerstone of inverse problems and compressed sensing. Stable recovery ensures that the original signal can be approximated linearly by optimal solutions of the corresponding Morozov or Tikhonov regularized optimization problems. In this paper, we propose new characterizations for stable recovery in finite-dimensional spaces, uncovering the role of nonsmooth second-order information. These insights enable a deeper understanding of stable recovery and their practical implications. As a consequence, we apply our theory to derive new sufficient conditions for stable recovery of the analysis group sparsity problems, including the group sparsity and isotropic total variation problems. Numerical experiments on these two problems give favorable results about using our conditions to test stable recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11313v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tran T. A. Nghia, Huy N. Pham, Nghia V. Vo</dc:creator>
    </item>
    <item>
      <title>First-Order Sweeping Processes and Extended Projected Dynamical Systems: Equivalence, Time-Discretization and Numerical Optimal Control</title>
      <link>https://arxiv.org/abs/2412.11320</link>
      <description>arXiv:2412.11320v1 Announce Type: new 
Abstract: Constrained dynamical systems are systems such that, by some means, the state stays within a given set. Two such systems are the (perturbed) Moreau sweeping process and the recently proposed extended Projected Dynamical System (ePDS). We show that under certain conditions solutions to the ePDS correspond to the solutions of a dynamic complementarity system, similar to the one equivalent to ordinary PDS. We then show that the perturbed sweeping process with time varying set can, under similar conditions, be reformulated as an ePDS. In this paper, we leverage these equivalences to develop an accurate discretization method for perturbed first-order Moreau sweeping processes via the finite elements with switch detection method. This allows the efficient optimal control of systems governed by ePDS and perturbed first-order sweeping processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11320v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anton Pozharskiy, Armin Nurkanovi\'c, Moritz Diehl</dc:creator>
    </item>
    <item>
      <title>Exact Verification of First-Order Methods via Mixed-Integer Linear Programming</title>
      <link>https://arxiv.org/abs/2412.11330</link>
      <description>arXiv:2412.11330v1 Announce Type: new 
Abstract: We present exact mixed-integer programming linear formulations for verifying the performance of first-order methods for parametric quadratic optimization. We formulate the verification problem as a mixed-integer linear program where the objective is to maximize the infinity norm of the fixed-point residual after a given number of iterations. Our approach captures a wide range of gradient, projection, proximal iterations through affine or piecewise affine constraints. We derive tight polyhedral convex hull formulations of the constraints representing the algorithm iterations. To improve the scalability, we develop a custom bound tightening technique combining interval propagation, operator theory, and optimization-based bound tightening. Numerical examples, including linear and quadratic programs from network optimization and sparse coding using Lasso, show that our method provides several orders of magnitude reductions in the worst-case fixed-point residuals, closely matching the true worst-case performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11330v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vinit Ranjan, Stefano Gualandi, Andrea Lodi, Bartolomeo Stellato</dc:creator>
    </item>
    <item>
      <title>Stochastic optimal self-path-dependent control: A new type of variational inequality and its viscosity solution</title>
      <link>https://arxiv.org/abs/2412.11383</link>
      <description>arXiv:2412.11383v1 Announce Type: new 
Abstract: In this paper, we explore a new class of stochastic control problems characterized by specific control constraints. Specifically, the admissible controls are subject to the ratcheting constraint, meaning they must be non-decreasing over time and are thus self-path-dependent. This type of problems is common in various practical applications, such as optimal consumption problems in financial engineering and optimal dividend payout problems in actuarial science. Traditional stochastic control theory does not readily apply to these problems due to their unique self-path-dependent control feature. To tackle this challenge, we introduce a new class of Hamilton-Jacobi-Bellman (HJB) equations, which are variational inequalities concerning the derivative of a new spatial argument that represents the historical maximum control value. Under the standard Lipschitz continuity condition, we demonstrate that the value functions for these self-path-dependent control problems are the unique solutions to their corresponding HJB equations in the viscosity sense.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11383v1</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <category>q-fin.MF</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingxin Guo, Zuo Quan Xu</dc:creator>
    </item>
    <item>
      <title>Relaxation methods for pessimistic bilevel optimization</title>
      <link>https://arxiv.org/abs/2412.11416</link>
      <description>arXiv:2412.11416v1 Announce Type: new 
Abstract: We consider a smooth pessimistic bilevel optimization problem, where the lower-level problem is convex and satisfies the Slater constraint qualification. These assumptions ensure that the Karush-Kuhn-Tucker (KKT) reformulation of our problem is well-defined. We then introduce and study the (i) Scholtes, (ii) Lin and Fukushima, (iii) Kadrani, Dussault and Benchakroun, (iv) Steffensen and Ulbrich, and (v) Kanzow and Schwartz relaxation methods for the KKT reformulation of our pessimistic bilevel program. These relaxations have been extensively studied and compared for mathematical programs with complementatrity constraints (MPCCs). To the best of our knowledge, such a study has not been conducted for the pessimistic bilevel optimization problem, which is completely different from an MPCC, as the complemetatrity conditions are part of the objective function, and not in the feasible set of the problem. After introducing these relaxations, we provide convergence results for global and local optimal solutions, as well as suitable versions of the C- and M-stationarity points of our pessimistic bilevel optimization problem. Numerical results are also provided to illustrate the practical implementation of these relaxation algorithms, as well as some preliminary comparisons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11416v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Imane Benchouk, Lateef Jolaoso, Khadra Nachi, Alain Zemkoho</dc:creator>
    </item>
    <item>
      <title>Mean field games of major-minor agents with recursive functionals</title>
      <link>https://arxiv.org/abs/2412.11433</link>
      <description>arXiv:2412.11433v1 Announce Type: new 
Abstract: This paper investigates a novel class of mean field games involving a major agent and numerous minor agents, where the agents' functionals are recursive with nonlinear backward stochastic differential equation (BSDE) representations. We term these games "recursive major-minor" (RMM) problems. Our RMM modeling is quite general, as it employs empirical (state, control) averages to define the weak couplings in both the functionals and dynamics of all agents, regardless of their status as major or minor. We construct an auxiliary limiting problem of the RMM by a novel unified structural scheme combining a bilateral perturbation with a mixed hierarchical recomposition. This scheme has its own merits as it can be applied to analyze more complex coupling structures than those in the current RMM. Subsequently, we derive the corresponding consistency condition and explore asymptotic RMM equilibria. Additionally, we examine the RMM problem in specific linear-quadratic settings for illustrative purposes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11433v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jianhui Huang, Wenqiang Li, Harry Zheng</dc:creator>
    </item>
    <item>
      <title>Inexact Proximal Point Algorithms for Zeroth-Order Global Optimization</title>
      <link>https://arxiv.org/abs/2412.11485</link>
      <description>arXiv:2412.11485v1 Announce Type: new 
Abstract: This work concerns the zeroth-order global minimization of continuous nonconvex functions with a unique global minimizer and possibly multiple local minimizers. We formulate a theoretical framework for inexact proximal point (IPP) methods for global optimization, establishing convergence guarantees under mild assumptions when either deterministic or stochastic estimates of proximal operators are used.The quadratic regularization in the proximal operator and the scaling effect of a parameter $\delta&gt;0$ create a concentrated landscape of an associated Gibbs measure that is practically effective for sampling. The convergence of the expectation under the Gibbs measure as $\delta\to 0^+$ is established, and the convergence rate of $\bigO(\delta)$ is derived under additional assumptions. These results provide a theoretical foundation for evaluating proximal operators inexactly using sampling-based methods such as Monte Carlo (MC) integration. In addition, we propose a new approach based on tensor train (TT) approximation. This approach employs a randomized TT cross algorithm to efficiently construct a low-rank TT approximation of a discretized function using a small number of function evaluations, and we provide an error analysis for the TT-based estimation. We then propose two practical IPP algorithms, TT-IPP and MC-IPP. The TT-IPP algorithm leverages TT estimates of the proximal operators, while the MC-IPP algorithm employs MC integration to estimate the proximal operators. Both algorithms are designed to adaptively balance efficiency and accuracy in inexact evaluations of proximal operators. The effectiveness of the two algorithms is demonstrated through experiments on diverse benchmark functions and various applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11485v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minxin Zhang, Fuqun Han, Yat Tin Chow, Stanley Osher, Hayden Schaeffer</dc:creator>
    </item>
    <item>
      <title>Fast Reflected Forward-Backward algorithm: achieving fast convergence rates for convex optimization with linear cone constraints</title>
      <link>https://arxiv.org/abs/2412.11505</link>
      <description>arXiv:2412.11505v1 Announce Type: new 
Abstract: In this paper, we derive a Fast Reflected Forward-Backward (Fast RFB) algorithm to solve the problem of finding a zero of the sum of a maximally monotone operator and a monotone and Lipschitz continuous operator in a real Hilbert space. Our approach extends the class of reflected forward-backward methods by introducing a Nesterov momentum term and a correction term, resulting in enhanced convergence performance. The iterative sequence of the proposed algorithm is proven to converge weakly, and the Fast RFB algorithm demonstrates impressive convergence rates, achieving $o\left( \frac{1}{k} \right)$ as $k \to +\infty$ for both the discrete velocity and the tangent residual at the \emph{last-iterate}. When applied to minimax problems with a smooth coupling term and nonsmooth convex regularizers, the resulting algorithm demonstrates significantly improved convergence properties compared to the current state of the art in the literature. For convex optimization problems with linear cone constraints, our approach yields a fully splitting primal-dual algorithm that ensures not only the convergence of iterates to a primal-dual solution, but also a \emph{last-iterate} convergence rate of $o\left( \frac{1}{k} \right)$ as $k \to +\infty$ for the objective function value, feasibility measure, and complementarity condition. This represents the most competitive theoretical result currently known for algorithms addressing this class of optimization problems. Numerical experiments are performed to illustrate the convergence behavior of Fast RFB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11505v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Radu Ioan Bot, Dang-Khoa Nguyen, Chunxiang Zong</dc:creator>
    </item>
    <item>
      <title>Strong Formulations for Hybrid System Control</title>
      <link>https://arxiv.org/abs/2412.11541</link>
      <description>arXiv:2412.11541v1 Announce Type: new 
Abstract: We study the mixed-integer quadratic programming formulation of an $n$-period hybrid control problem with a convex quadratic cost function and linear dynamics. We first give the convex hull description of the single-period, two-mode problem in the original variable space through two new classes of valid cuts. These cuts are then generalized to the single-period, multi-mode, multi-dimensional case and applied to solve the general $n$-period hybrid control problem. Computational experiments demonstrate the effectiveness of the proposed strong formulations derived through the cut generation process in the original variable space. These formulations yield a substantial reduction in computational effort for synthetic test instances and instances from the energy management problem of a power-split hybrid electric vehicle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11541v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jisun Lee, Hyungki Im, Alper Atamt\"urk</dc:creator>
    </item>
    <item>
      <title>A multilevel stochastic regularized first-order method with application to training</title>
      <link>https://arxiv.org/abs/2412.11630</link>
      <description>arXiv:2412.11630v1 Announce Type: new 
Abstract: In this paper, we propose a new multilevel stochastic framework for the solution of optimization problems. The proposed approach uses random regularized first-order models that exploit an available hierarchical description of the problem, being either in the classical variable space or in the function space, meaning that different levels of accuracy for the objective function are available. The converge analysis of the method is conducted and its numerical behavior is tested on the solution of finite-sum minimization problems. Indeed, the multilevel framework is tailored to the solution of such problems resulting in fact in a nontrivial variance reduction technique with adaptive step-size that outperforms standard approaches when solving nonconvex problems. Differently from classical deterministic multilevel methods, our stochastic method does not require the finest approximation to coincide with the original objective function. This allows to avoid the evaluation of the full sum in finite-sum minimization problems, opening at the solution of classification problems with large data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11630v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Filippo Marini, Margherita Porcelli, Elisa Riccietti</dc:creator>
    </item>
    <item>
      <title>On the SDP Relaxation of Direct Torque Finite Control Set Model Predictive Control</title>
      <link>https://arxiv.org/abs/2412.11666</link>
      <description>arXiv:2412.11666v1 Announce Type: new 
Abstract: This paper formulates a semidefinite programming relaxation for a long horizon direct-torque finite-control-set model predictive control problem. In parallel with this relaxation, a conventional branch-and-bound algorithm tailored for the original problem, but with an iteration limit to restrict its computational burden, is also solved. An input sequence candidate is extracted from the solution of the semidefinite program in the lifted space. This sequence is then compared with the so-called early-stopping branch-and-bound solution, and the best of the two is applied in a receding horizon fashion. In simulated case studies, the proposed approach exhibits significant improvements in torque transients, as the branch-and-bound alone struggles to find a meaningful solution due to the imposed limit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11666v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luca M. Hartmann, Orcun Karaca, Tinus Dorfling, Tobias Geyer, Adam Kurpisz</dc:creator>
    </item>
    <item>
      <title>A particle system approach towards the global well-posedness of master equations for potential mean field games of control</title>
      <link>https://arxiv.org/abs/2412.11742</link>
      <description>arXiv:2412.11742v1 Announce Type: new 
Abstract: This paper studies the $N$-particle systems as well as the HJB/master equations for a class of generalized mean field control (MFC) problems and the corresponding potential mean field games of control (MFGC). A local in time classical solution for the HJB equation is generated via a probabilistic approach based on the mean field maximum principle. Given an extension of the so called displacement convexity condition, we obtain the uniform estimates on the HJB equation for the $N$-particle system. Such estimates imply the displacement convexity/semi-concavity and thus the prior estimates on the solution to the HJB equation for generalized MFC problems. The global well-posedness of HJB/master equation for generalized MFC/potential MFGC is then proved thanks to the local well-posedness and the prior estimates. In view of the nature of the displacement convexity condition, such well-posedness is also true for the degenerated case. Our analysis on the $N$-particle system also induces an Lipschitz approximator to the optimal feedback function in generalized MFC/potential MFGC where an algebraic convergence rate is obtained. Furthermore, an alternative approximate Nash equilibrium is proposed based on the $N$-particle system, where the approximation error is quantified thanks to the aforementioned uniform estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11742v1</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <category>math.PR</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huafu Liao, Chenchen Mou</dc:creator>
    </item>
    <item>
      <title>Toward a Unified Theory of Gradient Descent under Generalized Smoothness</title>
      <link>https://arxiv.org/abs/2412.11773</link>
      <description>arXiv:2412.11773v1 Announce Type: new 
Abstract: We study the classical optimization problem $\min_{x \in \mathbb{R}^d} f(x)$ and analyze the gradient descent (GD) method in both nonconvex and convex settings. It is well-known that, under the $L$-smoothness assumption ($\|\nabla^2 f(x)\| \leq L$), the optimal point minimizing the quadratic upper bound $f(x_k) + \langle\nabla f(x_k), x_{k+1} - x_k\rangle + \frac{L}{2} \|x_{k+1} - x_k\|^2$ is $x_{k+1} = x_k - \gamma_k \nabla f(x_k)$ with step size $\gamma_k = \frac{1}{L}$. Surprisingly, a similar result can be derived under the $\ell$-generalized smoothness assumption ($\|\nabla^2 f(x)\| \leq \ell(\|\nabla f(x)\|)$). In this case, we derive the step size $$\gamma_k = \int_{0}^{1} \frac{d v}{\ell(\|\nabla f(x_k)\| + \|\nabla f(x_k)\| v)}.$$ Using this step size rule, we improve upon existing theoretical convergence rates and obtain new results in several previously unexplored setups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11773v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Tyurin</dc:creator>
    </item>
    <item>
      <title>Bivariate rational approximations of the general temperature integral</title>
      <link>https://arxiv.org/abs/2412.11781</link>
      <description>arXiv:2412.11781v1 Announce Type: new 
Abstract: The non-isothermal analysis of materials with the application of the Arrhenius equation involves temperature integration. If the frequency factor in the Arrhenius equation depends on temperature with a power-law relationship, the integral is known as the general temperature integral. This integral which has no analytical solution is estimated by the approximation functions with different accuracies. In this article, the rational approximations of the integral were obtained based on the minimization of the maximal deviation of bivariate functions. Mathematically, these problems belong to the class of quasiconvex optimization and can be solved using the bisection method. The approximations obtained in this study are more accurate than all approximates available in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11781v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alireza Aghili, Nadezda Sukhorukova, Julien Ugon</dc:creator>
    </item>
    <item>
      <title>Eckstein-Ferris-Pennanen-Robinson duality revisited: paramonotonicity, total Fenchel-Rockallar duality, and the Chambolle-Pock operator</title>
      <link>https://arxiv.org/abs/2412.11880</link>
      <description>arXiv:2412.11880v1 Announce Type: new 
Abstract: Finding zeros of the sum of two maximally monotoneoperators involving a continuous linear operator is a central problem in optimization and monotone operator theory. We revisit the duality framework proposed by Eckstein, Ferris, Pennanen, and Robinson from a quarter of a century ago. Paramonotonicity is identified as a broad condition ensuring that saddle points coincide with the closed convex rectangle formed by the primal and dual solutions. Additionally, we characterize total duality in the subdifferential setting and derive projection formulas for sets that arise in the analysis of the Chambolle-Pock algorithm within the recent framework developed by Bredies, Chenchene, Lorenz, and Naldi.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11880v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heinz H. Bauschke, Walaa M. Moursi, Shambhavi Singh</dc:creator>
    </item>
    <item>
      <title>Convergence of trust-region algorithms in compact metric spaces</title>
      <link>https://arxiv.org/abs/2412.11991</link>
      <description>arXiv:2412.11991v1 Announce Type: new 
Abstract: Trust-region algorithms can be applied to very abstract optimization problems because they do not require a specific direction of descent or anti-gradient. This has lead to recent interest in them, in particular in the area of integer optimal control problems, where the infinite-dimensional problem formulations do not assume vector space structure.
  We analyze a trust-region algorithm in the abstract setting of a compact metric space, a setting in which integer optimal control problems with total variation regularization can be formulated in. Our analysis allows to avoid a reset of the trust-region radius upon acceptance of the iterates when proving convergence to stationary points. This reset has been present in previous analyses of trust-region algorithms for integer optimal control problems. Our computational benchmark shows that the runtime can be considerably improved when avoiding this reset, which is now theoretically justified.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11991v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Manns</dc:creator>
    </item>
    <item>
      <title>On Differential Stability of a Class of Convex Optimization Problems</title>
      <link>https://arxiv.org/abs/2412.11996</link>
      <description>arXiv:2412.11996v1 Announce Type: new 
Abstract: The recent results of An, Luan, and Yen [Differential stability in convex optimization via generalized polyhedrality. Vietnam J. Math. https://-doi.org/10.1007/s10013-024-00721-y] on differential stability of parametric optimization problems described by proper generalized polyhedral convex functions and generalized polyhedral convex set-valued maps are analyzed, developed, and sharpened in this paper. Namely, keeping the Hausdorff locally convex topological vector spaces setting, we clarify the relationships between the upper estimates and lower estimates for the subdifferential and the singular subdifferential of the optimal value function. As shown by an example, the lower estimates can be strict. But, surprisingly, each upper estimate is an equality. Thus, exact formulas for the subdifferential and the singular subdifferential under consideration are obtained. In addition, it is proved that each subdifferential upper estimate coincides with the corresponding lower estimate if either the objective function or the constraint set-valued map is polyhedral convex.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11996v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nguyen Dong Yen, Duong Thi Viet An, Vu Thi Huong, Nguyen Ngoc Luan</dc:creator>
    </item>
    <item>
      <title>Bilevel Learning with Inexact Stochastic Gradients</title>
      <link>https://arxiv.org/abs/2412.12049</link>
      <description>arXiv:2412.12049v1 Announce Type: new 
Abstract: Bilevel learning has gained prominence in machine learning, inverse problems, and imaging applications, including hyperparameter optimization, learning data-adaptive regularizers, and optimizing forward operators. The large-scale nature of these problems has led to the development of inexact and computationally efficient methods. Existing adaptive methods predominantly rely on deterministic formulations, while stochastic approaches often adopt a doubly-stochastic framework with impractical variance assumptions, enforces a fixed number of lower-level iterations, and requires extensive tuning. In this work, we focus on bilevel learning with strongly convex lower-level problems and a nonconvex sum-of-functions in the upper-level. Stochasticity arises from data sampling in the upper-level which leads to inexact stochastic hypergradients. We establish their connection to state-of-the-art stochastic optimization theory for nonconvex objectives. Furthermore, we prove the convergence of inexact stochastic bilevel optimization under mild assumptions. Our empirical results highlight significant speed-ups and improved generalization in imaging tasks such as image denoising and deblurring in comparison with adaptive deterministic bilevel methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12049v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Sadegh Salehi, Subhadip Mukherjee, Lindon Roberts, Matthias J. Ehrhardt</dc:creator>
    </item>
    <item>
      <title>Invariant Kalman Filter for Relative Dynamics</title>
      <link>https://arxiv.org/abs/2412.10519</link>
      <description>arXiv:2412.10519v1 Announce Type: cross 
Abstract: This paper presents an invariant Kalman filter for estimating the relative trajectories between two dynamic systems. Invariant Kalman filters formulate the estimation error in terms of the group operation, ensuring that the error state does not depend on the current state estimate - a property referred to as state trajectory independence. This is particularly advantageous in extended Kalman filters, as it makes the propagation of the error covariance robust to large estimation errors. In this work, we construct invariant Kalman filters to the trajectory of one system relative to another. Specifically, we show that if the relative dynamics can be described solely by relative state variables, they automatically satisfy state trajectory independence, allowing for the development of an invariant Kalman filter. The corresponding relative invariant Kalman filter is formulated in an abstract fashion and is demonstrated numerically for the attitude dynamics of a rigid body.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10519v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tejaswi K. C., Maneesha Wickramasuriya, Taeyoung Lee</dc:creator>
    </item>
    <item>
      <title>Memory-Efficient 4-bit Preconditioned Stochastic Optimization</title>
      <link>https://arxiv.org/abs/2412.10663</link>
      <description>arXiv:2412.10663v1 Announce Type: cross 
Abstract: Preconditioned stochastic optimization algorithms, exemplified by Shampoo, have demonstrated superior performance over first-order optimizers, providing both theoretical advantages in convergence rates and practical improvements in large-scale neural network training. However, they incur substantial memory overhead due to the storage demands of non-diagonal preconditioning matrices. To address this, we introduce 4-bit quantization for Shampoo's preconditioners. We introduced two key methods: First, we apply Cholesky decomposition followed by quantization of the Cholesky factors, reducing memory usage by leveraging their lower triangular structure while preserving symmetry and positive definiteness to minimize information loss. To our knowledge, this is the first quantization approach applied to Cholesky factors of preconditioners. Second, we incorporate error feedback in the quantization process, efficiently storing Cholesky factors and error states in the lower and upper triangular parts of the same matrix. Through extensive experiments, we demonstrate that combining Cholesky quantization with error feedback enhances memory efficiency and algorithm performance in large-scale deep-learning tasks. Theoretically, we also provide convergence proofs for quantized Shampoo under both smooth and non-smooth stochastic optimization settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10663v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingyang Li, Kuangyu Ding, Kim-Chuan Toh, Pan Zhou</dc:creator>
    </item>
    <item>
      <title>Structured Sampling for Robust Euclidean Distance Geometry</title>
      <link>https://arxiv.org/abs/2412.10664</link>
      <description>arXiv:2412.10664v1 Announce Type: cross 
Abstract: This paper addresses the problem of estimating the positions of points from distance measurements corrupted by sparse outliers. Specifically, we consider a setting with two types of nodes: anchor nodes, for which exact distances to each other are known, and target nodes, for which complete but corrupted distance measurements to the anchors are available. To tackle this problem, we propose a novel algorithm powered by Nystr\"om method and robust principal component analysis. Our method is computationally efficient as it processes only a localized subset of the distance matrix and does not require distance measurements between target nodes. Empirical evaluations on synthetic datasets, designed to mimic sensor localization, and on molecular experiments, demonstrate that our algorithm achieves accurate recovery with a modest number of anchors, even in the presence of high levels of sparse outliers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10664v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chandra Kundu, Abiy Tasissa, HanQin Cai</dc:creator>
    </item>
    <item>
      <title>Optimal control of a kinetic equation</title>
      <link>https://arxiv.org/abs/2412.10747</link>
      <description>arXiv:2412.10747v1 Announce Type: cross 
Abstract: This work addresses an optimal control problem constrained by a degenerate kinetic equation of parabolic-hyperbolic type. Using a hypocoercivity framework we establish the well-posedness of the problem and demonstrate that the optimal solutions exhibit a hypocoercive decay property, ensuring stability and robustness. Building on this framework, we develop a finite element discretisation that preserves the stability properties of the continuous system. The effectiveness and accuracy of the proposed method are validated through a series of numerical experiments, showcasing its ability to handle challenging PDE-constrained optimal control problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10747v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaron Pim, Tristan Pryer, Alex Trenam</dc:creator>
    </item>
    <item>
      <title>ANaGRAM: A Natural Gradient Relative to Adapted Model for efficient PINNs learning</title>
      <link>https://arxiv.org/abs/2412.10782</link>
      <description>arXiv:2412.10782v1 Announce Type: cross 
Abstract: In the recent years, Physics Informed Neural Networks (PINNs) have received strong interest as a method to solve PDE driven systems, in particular for data assimilation purpose. This method is still in its infancy, with many shortcomings and failures that remain not properly understood. In this paper we propose a natural gradient approach to PINNs which contributes to speed-up and improve the accuracy of the training. Based on an in depth analysis of the differential geometric structures of the problem, we come up with two distinct contributions: (i) a new natural gradient algorithm that scales as $\min(P^2S, S^2P)$, where $P$ is the number of parameters, and $S$ the batch size; (ii) a mathematically principled reformulation of the PINNs problem that allows the extension of natural gradient to it, with proved connections to Green's function theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10782v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nilo Schwencke, Cyril Furtlehner</dc:creator>
    </item>
    <item>
      <title>Distributed Shape Learning of Complex Objects Using Gaussian Kernel</title>
      <link>https://arxiv.org/abs/2412.10916</link>
      <description>arXiv:2412.10916v1 Announce Type: cross 
Abstract: This paper addresses distributed learning of a complex object for multiple networked robots based on distributed optimization and kernel-based support vector machine. In order to overcome a fundamental limitation of polynomial kernels assumed in our antecessor, we employ Gaussian kernel as a kernel function for classification. The Gaussian kernel prohibits the robots to share the function through a finite number of equality constraints due to its infinite dimensionality of the function space. We thus reformulate the optimization problem assuming that the target function space is identified with the space spanned by the bases associated with not the data but a finite number of grid points. The above relaxation is shown to allow the robots to share the function by a finite number of equality constraints. We finally demonstrate the present approach through numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10916v1</guid>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Toshiyuki Oshima, Junya Yamauchi, Tatsuya Ibuki, Michio Seto, Takeshi Hatanaka</dc:creator>
    </item>
    <item>
      <title>Optimal Rates for Robust Stochastic Convex Optimization</title>
      <link>https://arxiv.org/abs/2412.11003</link>
      <description>arXiv:2412.11003v1 Announce Type: cross 
Abstract: The sensitivity of machine learning algorithms to outliers, particularly in high-dimensional spaces, necessitates the development of robust methods. Within the framework of $\epsilon$-contamination model, where the adversary can inspect and replace up to $\epsilon$ fraction of the samples, a fundamental open question is determining the optimal rates for robust stochastic convex optimization (robust SCO), provided the samples under $\epsilon$-contamination.
  We develop novel algorithms that achieve minimax-optimal excess risk (up to logarithmic factors) under the $\epsilon$-contamination model. Our approach advances beyonds existing algorithms, which are not only suboptimal but also constrained by stringent requirements, including Lipschitzness and smoothness conditions on sample functions.Our algorithms achieve optimal rates while removing these restrictive assumptions, and notably, remain effective for nonsmooth but Lipschitz population risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11003v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changyu Gao, Andrew Lowy, Xingyu Zhou, Stephen J. Wright</dc:creator>
    </item>
    <item>
      <title>Distributed Facility Location Games with Candidate Locations</title>
      <link>https://arxiv.org/abs/2412.11049</link>
      <description>arXiv:2412.11049v1 Announce Type: cross 
Abstract: We study the distributed facility location games with candidate locations, where agents on a line are partitioned into groups. Both desirable and obnoxious facility location settings are discussed. In distributed location problems, distortion can serve as a standard for quantifying performance, measuring the degree of difference between the actual location plan and the ideal location plan. For the desirable setting, under the max of sum cost objective, we give a strategyproof distributed mechanism with $5$-distortion, and prove that no strategyproof mechanism can have a distortion better than $\sqrt{2}+1$. Under the sum of max cost objective, we give a strategyproof distributed mechanism with $5$-distortion, and prove that no strategyproof mechanism can have a distortion better than $\frac{\sqrt{5}+1}{2}$. Under the max of max cost, we get a strategyproof distributed mechanism with $3$-distortion, and prove that no strategyproof mechanism can have a distortion better than $\frac{\sqrt{5}+1}{2}$. For the obnoxious setting, under three social objectives, we present that there is no strategyproof mechanism with bounded distortion in the case of discrete candidate locations, and no group strategyproof mechanism with bounded distortion in the case of continuous candidate locations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11049v1</guid>
      <category>cs.GT</category>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Feiyue Sun</dc:creator>
    </item>
    <item>
      <title>DisCo-DSO: Coupling Discrete and Continuous Optimization for Efficient Generative Design in Hybrid Spaces</title>
      <link>https://arxiv.org/abs/2412.11051</link>
      <description>arXiv:2412.11051v1 Announce Type: cross 
Abstract: We consider the challenge of black-box optimization within hybrid discrete-continuous and variable-length spaces, a problem that arises in various applications, such as decision tree learning and symbolic regression. We propose DisCo-DSO (Discrete-Continuous Deep Symbolic Optimization), a novel approach that uses a generative model to learn a joint distribution over discrete and continuous design variables to sample new hybrid designs. In contrast to standard decoupled approaches, in which the discrete and continuous variables are optimized separately, our joint optimization approach uses fewer objective function evaluations, is robust against non-differentiable objectives, and learns from prior samples to guide the search, leading to significant improvement in performance and sample efficiency. Our experiments on a diverse set of optimization tasks demonstrate that the advantages of DisCo-DSO become increasingly evident as the complexity of the problem increases. In particular, we illustrate DisCo-DSO's superiority over the state-of-the-art methods for interpretable reinforcement learning with decision trees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11051v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jacob F. Pettit, Chak Shing Lee, Jiachen Yang, Alex Ho, Daniel Faissol, Brenden Petersen, Mikel Landajuela</dc:creator>
    </item>
    <item>
      <title>New Approximation Guarantees for The Economic Warehouse Lot Scheduling Problem</title>
      <link>https://arxiv.org/abs/2412.11184</link>
      <description>arXiv:2412.11184v1 Announce Type: cross 
Abstract: In this paper, we present long-awaited algorithmic advances toward the efficient construction of near-optimal replenishment policies for a true inventory management classic, the economic warehouse lot scheduling problem. While this paradigm has accumulated a massive body of surrounding literature since its inception in the late '50s, we are still very much in the dark as far as basic computational questions are concerned, perhaps due to the evasive nature of dynamic policies in this context. The latter feature forced earlier attempts to either study highly-structured classes of policies or to forgo provably-good performance guarantees altogether; to this day, rigorously analyzable results have been few and far between.
  The current paper develops novel analytical foundations for directly competing against dynamic policies. Combined with further algorithmic progress and newly-gained insights, these ideas culminate to a polynomial-time approximation scheme for constantly-many commodities as well as to a proof-of-concept $(2-\frac{17}{5000} + \epsilon)$-approximation for general problem instances. In this regard, the efficient design of $\epsilon$-optimal dynamic policies appeared to have been out of reach, since beyond algorithmic challenges by themselves, even the polynomial-space representation of such policies has been a fundamental open question. On the other front, our sub-$2$-approximation constitutes the first improvement over the performance guarantees achievable via ``stationary order sizes and stationary intervals'' (SOSI) policies, which have been state-of-the-art since the mid-'90s.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11184v1</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Danny Segev</dc:creator>
    </item>
    <item>
      <title>Bayesian inference of mean velocity fields and turbulence models from flow MRI</title>
      <link>https://arxiv.org/abs/2412.11266</link>
      <description>arXiv:2412.11266v1 Announce Type: cross 
Abstract: We solve a Bayesian inverse Reynolds-averaged Navier-Stokes (RANS) problem that assimilates mean flow data by jointly reconstructing the mean flow field and learning its unknown RANS parameters. We devise an algorithm that learns the most likely parameters of an algebraic effective viscosity model, and estimates their uncertainties, from mean flow data of a turbulent flow. We conduct a flow MRI experiment to obtain mean flow data of a confined turbulent jet in an idealized medical device known as the FDA (Food and Drug Administration) nozzle. The algorithm successfully reconstructs the mean flow field and learns the most likely turbulence model parameters without overfitting. The methodology accepts any turbulence model, be it algebraic (explicit) or multi-equation (implicit), as long as the model is differentiable, and naturally extends to unsteady turbulent flows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11266v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. Kontogiannis, P. Nair, M. Loecher, D. B. Ennis, A. Marsden, M. P. Juniper</dc:creator>
    </item>
    <item>
      <title>Coupling-based Convergence Diagnostic and Stepsize Scheme for Stochastic Gradient Descent</title>
      <link>https://arxiv.org/abs/2412.11341</link>
      <description>arXiv:2412.11341v1 Announce Type: cross 
Abstract: The convergence behavior of Stochastic Gradient Descent (SGD) crucially depends on the stepsize configuration. When using a constant stepsize, the SGD iterates form a Markov chain, enjoying fast convergence during the initial transient phase. However, when reaching stationarity, the iterates oscillate around the optimum without making further progress. In this paper, we study the convergence diagnostics for SGD with constant stepsize, aiming to develop an effective dynamic stepsize scheme. We propose a novel coupling-based convergence diagnostic procedure, which monitors the distance of two coupled SGD iterates for stationarity detection. Our diagnostic statistic is simple and is shown to track the transition from transience stationarity theoretically. We conduct extensive numerical experiments and compare our method against various existing approaches. Our proposed coupling-based stepsize scheme is observed to achieve superior performance across a diverse set of convex and non-convex problems. Moreover, our results demonstrate the robustness of our approach to a wide range of hyperparameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11341v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiang Li, Qiaomin Xie</dc:creator>
    </item>
    <item>
      <title>Formulations and scalability of neural network surrogates in nonlinear optimization problems</title>
      <link>https://arxiv.org/abs/2412.11403</link>
      <description>arXiv:2412.11403v1 Announce Type: cross 
Abstract: We compare full-space, reduced-space, and gray-box formulations for representing trained neural networks in nonlinear constrained optimization problems. We test these formulations on a transient stability-constrained, security-constrained alternating current optimal power flow (SCOPF) problem where the transient stability criteria are represented by a trained neural network surrogate. Optimization problems are implemented in JuMP and trained neural networks are embedded using a new Julia package: MathOptAI.jl. To study the bottlenecks of the three formulations, we use neural networks with up to 590 million trained parameters. The full-space formulation is bottlenecked by the linear solver used by the optimization algorithm, while the reduced-space formulation is bottlenecked by the algebraic modeling environment and derivative computations. The gray-box formulation is the most scalable and is capable of solving with the largest neural networks tested. It is bottlenecked by evaluation of the neural network's outputs and their derivatives, which may be accelerated with a graphics processing unit (GPU). Leveraging the gray-box formulation and GPU acceleration, we solve our test problem with our largest neural network surrogate in 2.5$\times$ the time required for a simpler SCOPF problem without the stability constraint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11403v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert B. Parker, Oscar Dowson, Nicole LoGiudice, Manuel Garcia, Russell Bent</dc:creator>
    </item>
    <item>
      <title>A monotone block coordinate descent method for solving absolute value equations</title>
      <link>https://arxiv.org/abs/2412.11833</link>
      <description>arXiv:2412.11833v1 Announce Type: cross 
Abstract: In this paper, we proposed a monotone block coordinate descent method for solving absolute value equation (AVE). Under appropriate conditions, we analyzed the global convergence of the algorithm and conduct numerical experiments to demonstrate its feasibility and effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11833v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tingting Luo, Jiayu Liu, Cairong Chen, Qun Wang</dc:creator>
    </item>
    <item>
      <title>Capacitary measures in fractional order Sobolev spaces: Compactness and applications to minimization problems</title>
      <link>https://arxiv.org/abs/2412.11876</link>
      <description>arXiv:2412.11876v1 Announce Type: cross 
Abstract: Capacitary measures form a class of measures that vanish on sets of capacity zero. These measures are compact with respect to so-called $\gamma$-convergence, which relates a sequence of measures to the sequence of solutions of relaxed Dirichlet problems. This compactness result is already known for the classical $H^1(\Omega)$-capacity. This paper extends it to the fractional capacity defined for fractional order Sobolev spaces $H^s(\Omega)$ for $s\in (0,1)$. The compactness result is applied to obtain a finer optimality condition for a class of minimization problems in $H^s(\Omega)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11876v1</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Lentz</dc:creator>
    </item>
    <item>
      <title>Memory-Reduced Meta-Learning with Guaranteed Convergence</title>
      <link>https://arxiv.org/abs/2412.12030</link>
      <description>arXiv:2412.12030v1 Announce Type: cross 
Abstract: The optimization-based meta-learning approach is gaining increased traction because of its unique ability to quickly adapt to a new task using only small amounts of data. However, existing optimization-based meta-learning approaches, such as MAML, ANIL and their variants, generally employ backpropagation for upper-level gradient estimation, which requires using historical lower-level parameters/gradients and thus increases computational and memory overhead in each iteration. In this paper, we propose a meta-learning algorithm that can avoid using historical parameters/gradients and significantly reduce memory costs in each iteration compared to existing optimization-based meta-learning approaches. In addition to memory reduction, we prove that our proposed algorithm converges sublinearly with the iteration number of upper-level optimization, and the convergence error decays sublinearly with the batch size of sampled tasks. In the specific case in terms of deterministic meta-learning, we also prove that our proposed algorithm converges to an exact solution. Moreover, we quantify that the computational complexity of the algorithm is on the order of $\mathcal{O}(\epsilon^{-1})$, which matches existing convergence results on meta-learning even without using any historical parameters/gradients. Experimental results on meta-learning benchmarks confirm the efficacy of our proposed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12030v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Honglin Yang, Ji Ma, Xiao Yu</dc:creator>
    </item>
    <item>
      <title>On Control Networks Over Finite Lattices</title>
      <link>https://arxiv.org/abs/2208.03716</link>
      <description>arXiv:2208.03716v2 Announce Type: replace 
Abstract: The modeling and control of networks over finite lattices are studied via the algebraic state space approach. Using the semi-tensor product of matrices, we obtain the algebraic state space representation of the dynamics of (control) networks over finite lattices. Basic properties concerning networks over sublattices and product lattices are investigated, which shows the application of the analysis of lattice structure in the model reduction and control design of networks. Then algorithms are developed to recover the lattice structure from the structural matrix of a network over a lattice, and to construct comparability graphs over a finite set to verify whether a multiple-valued logical network is defined over a lattice. Finally, numerical examples are presented to illustrate the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.03716v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhengping Ji, Daizhan Cheng</dc:creator>
    </item>
    <item>
      <title>Optimal investment with insider information using Skorokhod &amp; Russo-Vallois integration</title>
      <link>https://arxiv.org/abs/2211.07471</link>
      <description>arXiv:2211.07471v2 Announce Type: replace 
Abstract: We study the maximization of the logarithmic utility for an insider with different anticipating techniques. Our aim is to compare the utilization of Russo-Vallois forward and Skorokhod integrals in this context. Theoretical analysis and illustrative numerical examples showcase that the Skorokhod insider outperforms the forward insider. This remarkable observation stands in contrast to the scenario involving risk-neutral traders. Furthermore, an ordinary trader could surpass both insiders if a significant negative fluctuation in the driving stochastic process leads to a sufficiently negative final value. These findings underline the intricate interplay between anticipating stochastic calculus and nonlinear utilities, which may yield non-intuitive results from the financial viewpoint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.07471v2</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>q-fin.MF</category>
      <category>q-fin.PM</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mauricio Elizalde, Carlos Escudero, Tomoyuki Ichiba</dc:creator>
    </item>
    <item>
      <title>A sparse optimization approach to infinite infimal convolution regularization</title>
      <link>https://arxiv.org/abs/2304.08628</link>
      <description>arXiv:2304.08628v2 Announce Type: replace 
Abstract: In this paper we introduce the class of infinite infimal convolution functionals and apply these functionals to the regularization of ill-posed inverse problems. The proposed regularization involves an infimal convolution of a continuously parametrized family of convex, positively one-homogeneous functionals defined on a common Banach space $X$. We show that, under mild assumptions, this functional admits an equivalent convex lifting in the space of measures with values in $X$. This reformulation allows us to prove well-posedness of a Tikhonov regularized inverse problem and opens the door to a sparse analysis of the solutions. In the case of finite-dimensional measurements we prove a representer theorem, showing that there exists a solution of the inverse problem that is sparse, in the sense that it can be represented as a linear combination of the extremal points of the ball of the lifted infinite infimal convolution functional. Then, we design a generalized conditional gradient method for computing solutions of the inverse problem without relying on an a priori discretization of the parameter space and of the Banach space $X$. The iterates are constructed as linear combinations of the extremal points of the lifted infinite infimal convolution functional. We prove a sublinear rate of convergence for our algorithm and apply it to denoising of signals and images using, as regularizer, infinite infimal convolutions of fractional-Laplacian-type operators with adaptive orders of smoothness and anisotropies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.08628v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kristian Bredies, Marcello Carioni, Martin Holler, Yury Korolev, Carola-Bibiane Sch\"onlieb</dc:creator>
    </item>
    <item>
      <title>Strong Convexity of Sets in Riemannian Manifolds</title>
      <link>https://arxiv.org/abs/2312.03583</link>
      <description>arXiv:2312.03583v3 Announce Type: replace 
Abstract: Curvature properties of convex objects, such as strong convexity, are important in designing and analyzing convex optimization algorithms in the Hilbertian or Riemannian settings. In the case of the Hilbertian setting, strongly convex sets are well studied. Herein, we propose various definitions of strong convexity for uniquely geodesic sets in a Riemannian manifold. We study their relationship, propose tools to determine the geodesic strongly convex nature of sets, and analyze the convergence of optimization algorithms over those sets. In particular, we demonstrate that the Riemannian Frank-Wolfe algorithm enjoys a global linear convergence rate when the Riemannian scaling inequalities hold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.03583v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Damien Scieur, David Mart\'inez-Rubio, Thomas Kerdreux, Alexandre d'Aspremont, Sebastian Pokutta</dc:creator>
    </item>
    <item>
      <title>Subdifferentially polynomially bounded functions and Gaussian smoothing-based zeroth-order optimization</title>
      <link>https://arxiv.org/abs/2405.04150</link>
      <description>arXiv:2405.04150v2 Announce Type: replace 
Abstract: We study the class of subdifferentially polynomially bounded (SPB) functions, which is a rich class of locally Lipschitz functions that encompasses all Lipschitz functions, all gradient- or Hessian-Lipschitz functions, and even some non-smooth locally Lipschitz functions. We show that SPB functions are compatible with Gaussian smoothing (GS), in the sense that the GS of any SPB function is well-defined and satisfies a descent lemma akin to gradient-Lipschitz functions, with the Lipschitz constant replaced by a polynomial function. Leveraging this descent lemma, we propose GS-based zeroth-order optimization algorithms with an adaptive stepsize strategy for minimizing SPB functions, and analyze their convergence rates with respect to both relative and absolute stationarity measures. Finally, we also establish the iteration complexity for achieving a $(\delta, \epsilon)$-approximate stationary point, based on a novel quantification of Goldstein stationarity via the GS gradient that could be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04150v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ming Lei, Ting Kei Pong, Shuqin Sun, Man-Chung Yue</dc:creator>
    </item>
    <item>
      <title>Inference in higher-order undirected graphical models and binary polynomial optimization</title>
      <link>https://arxiv.org/abs/2405.09727</link>
      <description>arXiv:2405.09727v2 Announce Type: replace 
Abstract: We consider the problem of inference in higher-order undirected graphical models with binary labels. We formulate this problem as a binary polynomial optimization problem and propose several linear programming relaxations for it. We compare the strength of the proposed linear programming relaxations theoretically. Finally, we demonstrate the effectiveness of these relaxations by performing a computational study for two important applications, namely, image restoration and decoding error-correcting codes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09727v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aida Khajavirad, Yakun Wang</dc:creator>
    </item>
    <item>
      <title>Adaptive Ensemble Control for Stochastic Systems with Mixed Asymmetric Laplace Noises</title>
      <link>https://arxiv.org/abs/2405.09973</link>
      <description>arXiv:2405.09973v3 Announce Type: replace 
Abstract: This paper presents an adaptive ensemble control for stochastic systems subject to asymmetric noises and outliers. Asymmetric noises skew system observations, and outliers with large amplitude deteriorate the observations even further. Such disturbances induce poor system estimation and degraded stochastic system control. In this work, we model the asymmetric noises and outliers by mixed asymmetric Laplace distributions (ALDs), and propose an optimal control for stochastic systems with mixed ALD noises. Particularly, we segregate the system disturbed by mixed ALD noises into subsystems, each of which is subject to a specific ALD noise. For each subsystem, we design an iterative quantile filter (IQF) to estimate the system parameters using system observations. With the estimated parameters by IQF, we derive the certainty equivalence (CE) control law for each subsystem. Then we use the Bayesian approach to ensemble the subsystem CE controllers, with each of the controllers weighted by their posterior probability. We finalize our control law as the weighted sum of the control signals by the sub-system CE controllers. To demonstrate our approach, we conduct numerical simulations and Monte Carlo analyses. The results show improved tracking performance by our approach for skew noises and its robustness to outliers, compared with single ALD based and RLS-based control policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09973v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yajie Yu, Xuehui Ma, Shiliang Zhang, Zhuzhu Wang, Xubing Shi, Yushuai Li, Tingwen Huang</dc:creator>
    </item>
    <item>
      <title>Port-Hamiltonian structures in infinite-dimensional optimal control: Primal-Dual gradient method and control-by-interconnection</title>
      <link>https://arxiv.org/abs/2406.01087</link>
      <description>arXiv:2406.01087v2 Announce Type: replace 
Abstract: In this note, we consider port-Hamiltonian structures in numerical optimal control of ordinary differential equations. By introducing a novel class of nonlinear monotone port-Hamiltonian (pH) systems, we show that the primal-dual gradient method may be viewed as an infinite-dimensional nonlinear pH system. The monotonicity and the particular block structure arising in the optimality system is used to prove exponential stability of the dynamics towards its equilibrium, which is a critical point of the first-order optimality conditions. Leveraging the port-based modeling, we propose an optimization-based controller in a suboptimal receding horizon control fashion. To this end, the primal-dual gradient based optimizer-dynamics is coupled to a pH plant dynamics in a power-preserving manner. We show that the resulting model is again monotone pH system and prove that the closed-loop exhibits local exponential convergence towards the equilibrium.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01087v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hannes Gernandt, Manuel Schaller</dc:creator>
    </item>
    <item>
      <title>Shape Optimization of Supercapacitor Electrode to Maximize Charge Storage</title>
      <link>https://arxiv.org/abs/2406.09616</link>
      <description>arXiv:2406.09616v2 Announce Type: replace 
Abstract: We build a new mathematical model of shape optimization for maximizing ionic concentration governed by the multi-physical coupling steady-state Poisson-Nernst-Planck system. Shape sensitivity analysis is performed to obtain the Eulerian derivative of the cost functional. The Gummel fixed-point method with inverse harmonic averaging technique on exponential coefficient is used to solve efficiently the steady-state Poisson-Nernst-Planck system. Various numerical results using a shape gradient algorithm in 2d and 3d are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09616v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Jiajie Li, Shenggao Zhou, Shengfeng Zhu</dc:creator>
    </item>
    <item>
      <title>Entropic Semi-Martingale Optimal Transport</title>
      <link>https://arxiv.org/abs/2408.09361</link>
      <description>arXiv:2408.09361v3 Announce Type: replace 
Abstract: Entropic Optimal Transport (EOT), also referred to as the Schr\"odinger problem, seeks to find a random processes with prescribed initial/final marginals and with minimal relative entropy with respect to a reference measure. The relative entropy forces the two measures to share the same support and only the drift of the controlled process can be adjusted, the diffusion being imposed by the reference measure. Therefore, at first sight, Semi-Martingale Optimal Transport (SMOT) problems (see [1]) seem out of the scope of applications of Entropic regularization techniques, which are otherwise very attractive from a computational point of view. However, when the process is observed only at discrete times, and become therefore a Markov chain, its relative entropy can remain finite even with variable diffusion coefficients, and discrete semi-martingales can be obtained as solutions of (multi-marginal) EOT problems.Given a (smooth) semi-martingale, the limit of the relative entropy of its time discretizations, scaled by the time step converges to the so-called ``specific relative entropy'', a convex functional of its variance process, similar to those used in SMOT.In this paper we use this observation to build an entropic time discretization of continuous SMOT problems. This allows to compute discrete approximations of solutions to continuous SMOT problems by a multi-marginal Sinkhorn algorithm, without the need of solving the non-linear Hamilton-Jacobi-Bellman pde's associated to the dual problem, as done for example in [1, 2]. We prove a convergence result of the time discrete entropic problem to the continuous time problem, we propose an implementation and provide numerical experiments supporting the theoretical convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09361v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jean-David Benamou, Guillaume Chazareix, Marc Hoffmann, Gr\'egoire Loeper, Fran\c{c}ois-Xavier Vialard</dc:creator>
    </item>
    <item>
      <title>Martingale deep learning for very high dimensional quasi-linear partial differential equations and stochastic optimal controls</title>
      <link>https://arxiv.org/abs/2408.14395</link>
      <description>arXiv:2408.14395v3 Announce Type: replace 
Abstract: High-dimensional parabolic partial differential equations (PDEs) often involve large-scale Hessian matrices, which are computationally expensive for deep learning methods relying on automatic differentiation to compute derivatives. This work aims to address this issue. In the proposed method, the PDE is reformulated into a martingale formulation, which allows the computation of loss functions to be derivative-free and parallelized in time-space domain. Then, the martingale formulation is enforced using a Galerkin method via adversarial learning techniques, which eliminate the need of computing conditional expectations in the margtingale property. This method is further extended to solve Hamilton-Jacobi-Bellman (HJB) equations and the associated Stochastic optimal control problems, enabling the simultaneous solution of the value function and optimal feedback control in a derivative-free manner. Numerical results demonstrate the effectiveness and efficiency of the proposed method, capable of solving HJB equations accurately with dimensionality up to 10,000.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14395v3</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Cai, Shuixin Fang, Wenzhong Zhang, Tao Zhou</dc:creator>
    </item>
    <item>
      <title>Problem-dependent convergence bounds for randomized linear gradient compression</title>
      <link>https://arxiv.org/abs/2411.12898</link>
      <description>arXiv:2411.12898v2 Announce Type: replace 
Abstract: In distributed optimization, the communication of model updates can be a performance bottleneck. Consequently, gradient compression has been proposed as a means of increasing optimization throughput. In general, due to information loss, compression introduces a penalty on the number of iterations needed to reach a solution. In this work, we investigate how the iteration penalty depends on the interaction between compression and problem structure, in the context of non-convex stochastic optimization. We focus on linear compression schemes, where compression and decompression can be modeled as multiplication with a random matrix. We consider several distributions of matrices, among them random orthogonal matrices and matrices with random Gaussian entries. We find that in each case, the impact of compression on convergence can be quantified in terms of the norm of the Hessian of the objective, using a norm defined by the compression scheme. The analysis reveals that in certain cases, compression performance is related to low-rank structure or other spectral properties of the problem. In these cases, our bounds predict that the penalty introduced by compression is significantly reduced compared to worst-case bounds that only consider the compression level, ignoring problem data. We verify the theoretical findings on several optimization problems, including fine-tuning an image classification model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12898v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Flynn, Patrick Johnstone, Shinjae Yoo</dc:creator>
    </item>
    <item>
      <title>Block Coordinate Descent Methods for Structured Nonconvex Optimization with Nonseparable Constraints: Optimality Conditions and Global Convergence</title>
      <link>https://arxiv.org/abs/2412.05918</link>
      <description>arXiv:2412.05918v2 Announce Type: replace 
Abstract: Coordinate descent algorithms are widely used in machine learning and large-scale data analysis due to their strong optimality guarantees and impressive empirical performance in solving non-convex problems. In this work, we introduce Block Coordinate Descent (BCD) method for structured nonconvex optimization with nonseparable constraints. Unlike traditional large-scale Coordinate Descent (CD) approaches, we do not assume the constraints are separable. Instead, we account for the possibility of nonlinear coupling among them. By leveraging the inherent problem structure, we propose new CD methods to tackle this specific challenge. Under the relatively mild condition of locally bounded non-convexity, we demonstrate that achieving coordinate-wise stationary points offer a stronger optimality criterion compared to standard critical points. Furthermore, under the Luo-Tseng error bound conditions, our BCD methods exhibit Q-linear convergence to coordinate-wise stationary points or critical points. To demonstrate the practical utility of our methods, we apply them to various machine learning and signal processing models. We also provide the geometry analysis for the models. Experiments on real-world data consistently demonstrate the superior objective values of our approaches compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05918v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhijie Yuan, Ganzhao Yuan, Lei Sun</dc:creator>
    </item>
    <item>
      <title>Quantum open system identification via global optimization: Optimally accurate Markovian models of open systems from time-series data</title>
      <link>https://arxiv.org/abs/2203.17164</link>
      <description>arXiv:2203.17164v2 Announce Type: replace-cross 
Abstract: Accurate models of the dynamics of quantum circuits are essential for optimizing and advancing quantum devices. Since first-principles models of environmental noise and dissipation in real quantum systems are often unavailable, deriving accurate models from measured time-series data is critical. However, identifying open quantum systems poses significant challenges: powerful methods from systems engineering can perform poorly beyond weak damping (as we show) because they fail to incorporate essential constraints required for quantum evolution (e.g., positivity). Common methods that can include these constraints are typically multi-step, fitting linear models to physically grounded master equations, often resulting in non-convex functions in which local optimization algorithms get stuck in local extrema (as we show). In this work, we solve these problems by formulating quantum system identification directly from data as a polynomial optimization problem, enabling the use of recently developed global optimization methods. These methods are essentially guaranteed to reach global optima, allowing us for the first time to efficiently obtain the most accurate Markovian model for a given system. In addition to its practical importance, this allows us to take the error of these Markovian models as an alternative (operational) measure of the non-Markovianity of a system. We test our method with the spin-boson model -- a two-level system coupled to a bath of harmonic oscillators -- for which we obtain the exact evolution using matrix-product-state techniques. We show that polynomial optimization using moment/sum-of-squares approaches significantly outperforms traditional optimization algorithms, and we show that even for strong damping Lindblad-form master equations can provide accurate models of the spin-boson system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.17164v2</guid>
      <category>quant-ph</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zakhar Popovych, Kurt Jacobs, Georgios Korpas, Jakub Marecek, Denys I. Bondar</dc:creator>
    </item>
    <item>
      <title>Geometrical Study of the Cone of Sums of Squares plus Sums of Nonnegative Circuits</title>
      <link>https://arxiv.org/abs/2305.14848</link>
      <description>arXiv:2305.14848v2 Announce Type: replace-cross 
Abstract: In this article, we combine sums of squares (SOS) and sums of nonnegative circuit (SONC) forms, two independent nonnegativity certificates for real homogeneous polynomials. We consider the convex cone SOS+SONC of forms that decompose into a sum of an SOS and a SONC form and study it from a geometric point of view. We show that the SOS+SONC cone is proper and neither closed under multiplications nor under linear transformation of variables. Moreover, we present an alternative proof of an analog of Hilbert's 1888 Theorem for the SOS+SONC cone and prove that in the non-Hilbert cases it provides a proper superset of both the SOS and the SONC cone. This follows by exploiting a new necessary condition for membership in the SONC cone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.14848v2</guid>
      <category>math.AG</category>
      <category>math.CO</category>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mareike Dressler, Salma Kuhlmann, Moritz Schick</dc:creator>
    </item>
    <item>
      <title>Parallel variational quantum algorithms with gradient-informed restart to speed up optimisation in the presence of barren plateaus</title>
      <link>https://arxiv.org/abs/2311.18090</link>
      <description>arXiv:2311.18090v3 Announce Type: replace-cross 
Abstract: Inspired by the Fleming-Viot stochastic process, we propose a parallel implementation of variational quantum algorithms with the aim of reducing the time spent by the algorithm in barren plateaus, where optimization direction is unclear. In the Fleming-Viot tradition, parallel searches are called particles. In the proposed approach, the search by a Fleming-Viot particle is stopped when it encounters a region where the gradient is too small or noisy, suggesting a barren plateau area. The stopped particle continues the search after being regenerated at another location of the parameter space, potentially taking the exploration away from barren plateaus. We first analyze the behavior of the Fleming-Viot particles from a theoretical standpoint. We show that, when simulated annealing optimizers are used as particles, the Fleming-Viot system is expected to find the global optimum faster than a single simulated annealing optimizer, with a relative efficiency that increases proportionally to the percentage of barren plateaus in the domain. This result is corroborated by numerical experiments carried out on synthetic problems as well as on instances of the Max-Cut problem, which show that our method performs better than plain simulated annealing when large barren plateaus are present in the domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18090v3</guid>
      <category>quant-ph</category>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Mastropietro (CNRS-IRIT, Universit\'e de Toulouse INP, Toulouse, France), Georgios Korpas (HSBC Lab, Innovation &amp; Ventures, HSBC, London, United Kingdom, Department of Computer Science, Czech Technical University in Prague, Czech Republic), Vyacheslav Kungurtsev (Department of Computer Science, Czech Technical University in Prague, Czech Republic), Jakub Marecek (Department of Computer Science, Czech Technical University in Prague, Czech Republic)</dc:creator>
    </item>
    <item>
      <title>Data Collaboration Analysis with Orthogonal Basis Alignment</title>
      <link>https://arxiv.org/abs/2403.02780</link>
      <description>arXiv:2403.02780v2 Announce Type: replace-cross 
Abstract: The Data Collaboration (DC) framework provides a privacy-preserving solution for multi-source data fusion, enabling the joint analysis of data from multiple sources to achieve enhanced insights. It utilizes linear transformations with secretly selected bases to ensure privacy guarantees through non-iterative communication. Despite its strengths, the DC framework often encounters performance instability due to theoretical challenges in aligning the bases used for mapping raw data. This study addresses these challenges by establishing a rigorous theoretical foundation for basis alignment within the DC framework, formulating it as an optimization problem over orthogonal matrices. Under specific assumptions, we demonstrate that this problem can be reduced to the Orthogonal Procrustes Problem, which has a well-known analytical solution. Extensive empirical evaluations across diverse datasets reveal that the proposed alignment method significantly enhances model performance and computational efficiency, outperforming existing approaches. Additionally, it demonstrates robustness across varying levels of differential privacy, thus enabling practical and reliable implementations of the DC framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02780v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keiyu Nosaka, Yuichi Takano, Akiko Yoshise</dc:creator>
    </item>
    <item>
      <title>Beyond adaptive gradient: Fast-Controlled Minibatch Algorithm for large-scale optimization</title>
      <link>https://arxiv.org/abs/2411.15795</link>
      <description>arXiv:2411.15795v3 Announce Type: replace-cross 
Abstract: Adaptive gradient methods have been increasingly adopted by deep learning community due to their fast convergence and reduced sensitivity to hyper-parameters. However, these methods come with limitations, such as increased memory requirements for elements like moving averages and a poorly understood convergence theory. To overcome these challenges, we introduce F-CMA, a Fast-Controlled Mini-batch Algorithm with a random reshuffling method featuring a sufficient decrease condition and a line-search procedure to ensure loss reduction per epoch, along with its deterministic proof of global convergence to a stationary point. To evaluate the F-CMA, we integrate it into conventional training protocols for classification tasks involving both convolutional neural networks and vision transformer models, allowing for a direct comparison with popular optimizers. Computational tests show significant improvements, including a decrease in the overall training time by up to 68%, an increase in per-epoch efficiency by up to 20%, and in model accuracy by up to 5%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15795v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Corrado Coppola, Lorenzo Papa, Irene Amerini, Laura Palagi</dc:creator>
    </item>
  </channel>
</rss>
