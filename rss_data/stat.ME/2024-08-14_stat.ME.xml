<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 15 Aug 2024 01:34:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 14 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Causal Graph Aided Causal Discovery in an Observational Aneurysmal Subarachnoid Hemorrhage Study</title>
      <link>https://arxiv.org/abs/2408.06464</link>
      <description>arXiv:2408.06464v1 Announce Type: new 
Abstract: Causal inference methods for observational data are increasingly recognized as a valuable complement to randomized clinical trials (RCTs). They can, under strong assumptions, emulate RCTs or help refine their focus. Our approach to causal inference uses causal directed acyclic graphs (DAGs). We are motivated by a concern that many observational studies in medicine begin without a clear definition of their objectives, without awareness of the scientific potential, and without tools to identify the necessary in itinere adjustments. We present and illustrate methods that provide "midway insights" during study's course, identify meaningful causal questions within the study's reach and point to the necessary data base enhancements for these questions to be meaningfully tackled. The method hinges on concepts of identification and positivity. Concepts are illustrated through an analysis of data generated by patients with aneurysmal Subarachnoid Hemorrhage (aSAH) halfway through a study, focusing in particular on the consequences of external ventricular drain (EVD) in strata of the aSAH population. In addition, we propose a method for multicenter studies, to monitor the impact of changes in practice at an individual center's level, by leveraging principles of instrumental variable (IV) inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06464v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlo Berzuini, Davide Luciani, Hiren C. Patel</dc:creator>
    </item>
    <item>
      <title>Generative Bayesian Modeling with Implicit Priors</title>
      <link>https://arxiv.org/abs/2408.06504</link>
      <description>arXiv:2408.06504v1 Announce Type: new 
Abstract: Generative models are a cornerstone of Bayesian data analysis, enabling predictive simulations and model validation. However, in practice, manually specified priors often lead to unreasonable simulation outcomes, a common obstacle for full Bayesian simulations. As a remedy, we propose to add small portions of real or simulated data, which creates implicit priors that improve the stability of Bayesian simulations. We formalize this approach, providing a detailed process for constructing implicit priors and relating them to existing research in this area. We also integrate the implicit priors into simulation-based calibration, a prominent Bayesian simulation task. Through two case studies, we demonstrate that implicit priors quickly improve simulation stability and model calibration. Our findings provide practical guidelines for implementing implicit priors in various Bayesian modeling scenarios, showcasing their ability to generate more reliable simulation outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06504v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luna Fazio, Maximilian Scholz, Paul-Christian B\"urkner</dc:creator>
    </item>
    <item>
      <title>Post-selection inference for high-dimensional mediation analysis with survival outcomes</title>
      <link>https://arxiv.org/abs/2408.06517</link>
      <description>arXiv:2408.06517v1 Announce Type: new 
Abstract: It is of substantial scientific interest to detect mediators that lie in the causal pathway from an exposure to a survival outcome. However, with high-dimensional mediators, as often encountered in modern genomic data settings, there is a lack of powerful methods that can provide valid post-selection inference for the identified marginal mediation effect. To resolve this challenge, we develop a post-selection inference procedure for the maximally selected natural indirect effect using a semiparametric efficient influence function approach. To this end, we establish the asymptotic normality of a stabilized one-step estimator that takes the selection of the mediator into account. Simulation studies show that our proposed method has good empirical performance. We further apply our proposed approach to a lung cancer dataset and find multiple DNA methylation CpG sites that might mediate the effect of cigarette smoking on lung cancer survival.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06517v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tzu-Jung Huang, Zhonghua Liu, Ian W. McKeague</dc:creator>
    </item>
    <item>
      <title>Conformal predictive intervals in survival analysis: a re-sampling approach</title>
      <link>https://arxiv.org/abs/2408.06539</link>
      <description>arXiv:2408.06539v1 Announce Type: new 
Abstract: The distribution-free method of conformal prediction (Vovk et al, 2005) has gained considerable attention in computer science, machine learning, and statistics. Candes et al. (2023) extended this method to right-censored survival data, addressing right-censoring complexity by creating a covariate shift setting, extracting a subcohort of subjects with censoring times exceeding a fixed threshold. Their approach only estimates the lower prediction bound for type I censoring, where all subjects have available censoring times regardless of their failure status. In medical applications, we often encounter more general right-censored data, observing only the minimum of failure time and censoring time. Subjects with observed failure times have unavailable censoring times. To address this, we propose a bootstrap method to construct one -- as well as two-sided conformal predictive intervals for general right-censored survival data under different working regression models. Through simulations, our method demonstrates excellent average coverage for the lower bound and good coverage for the two-sided predictive interval, regardless of working model is correctly specified or not, particularly under moderate censoring. We further extend the proposed method to several directions in medical applications. We apply this method to predict breast cancer patients' future survival times based on tumour characteristics and treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06539v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jing Qin, Jin Piao, Jing Ning, Yu Shen</dc:creator>
    </item>
    <item>
      <title>Double Robust high dimensional alpha test for linear factor pricing model</title>
      <link>https://arxiv.org/abs/2408.06612</link>
      <description>arXiv:2408.06612v1 Announce Type: new 
Abstract: In this paper, we investigate alpha testing for high-dimensional linear factor pricing models. We propose a spatial sign-based max-type test to handle sparse alternative cases. Additionally, we prove that this test is asymptotically independent of the spatial-sign-based sum-type test proposed by Liu et al. (2023). Based on this result, we introduce a Cauchy Combination test procedure that combines both the max-type and sum-type tests. Simulation studies and real data applications demonstrate that the new proposed test procedure is robust not only for heavy-tailed distributions but also for the sparsity of the alternative hypothesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06612v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ping Zhao, Long Feng, Hongfei Wang, Zhaojun Wang</dc:creator>
    </item>
    <item>
      <title>Considerations for missing data, outliers and transformations in permutation testing for ANOVA, ASCA(+) and related factorizations</title>
      <link>https://arxiv.org/abs/2408.06739</link>
      <description>arXiv:2408.06739v1 Announce Type: new 
Abstract: Multifactorial experimental designs allow us to assess the contribution of several factors, and potentially their interactions, to one or several responses of interests. Following the principles of the partition of the variance advocated by Sir R.A. Fisher, the experimental responses are factored into the quantitative contribution of main factors and interactions. A popular approach to perform this factorization in both ANOVA and ASCA(+) is through General Linear Models. Subsequently, different inferential approaches can be used to identify whether the contributions are statistically significant or not. Unfortunately, the performance of inferential approaches in terms of Type I and Type II errors can be heavily affected by missing data, outliers and/or the departure from normality of the distribution of the responses, which are commonplace problems in modern analytical experiments. In this paper, we study these problem and suggest good practices of application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06739v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oliver Polushkina Merchanskaya, Michael D. Sorochan Armstrong, Carolina G\'omez Llorente, Patricia Ferrer, Sergi Fernandez-Gonzalez, Miriam Perez-Cruz, Mar\'ia Dolores G\'omez-Roig, Jos\'e Camacho</dc:creator>
    </item>
    <item>
      <title>Stratification in Randomised Clinical Trials and Analysis of Covariance: Some Simple Theory and Recommendations</title>
      <link>https://arxiv.org/abs/2408.06760</link>
      <description>arXiv:2408.06760v1 Announce Type: new 
Abstract: A simple device for balancing for a continuous covariate in clinical trials is to stratify by whether the covariate is above or below some target value, typically the predicted median. This raises an issue as to which model should be used for modelling the effect of treatment on the outcome variable, $Y$. Should one fit, the stratum indicator, $S$, the continuous covariate, $X$, both or neither?
  This question has been investigated in the literature using simulations targetting the overall effect on inferences about treatment . However, when a covariate is added to a model there are three consequences for inference: 1) The mean square error effect, 2) The variance inflation factor and 3) second order precision. We consider that it is valuable to consider these three factors separately even if, ultimately, it is their joint effect that matters.
  We present some simple theory, concentrating in particular on the variance inflation factor, that may be used to guide trialists in their choice of model. We also consider the case where the precise form of the relationship between the outcome and the covariate is not known. We conclude by recommending that the continuous coovariate should always be in the model but that, depending on circumstances, there may be some justification in fitting the stratum indicator also.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06760v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Stephen Senn, Franz K\"onig, Martin Posch</dc:creator>
    </item>
    <item>
      <title>Joint model for interval-censored semi-competing events and longitudinal data with subject-specific within and between visits variabilities</title>
      <link>https://arxiv.org/abs/2408.06769</link>
      <description>arXiv:2408.06769v1 Announce Type: new 
Abstract: Dementia currently affects about 50 million people worldwide, and this number is rising. Since there is still no cure, the primary focus remains on preventing modifiable risk factors such as cardiovascular factors. It is now recognized that high blood pressure is a risk factor for dementia. An increasing number of studies suggest that blood pressure variability may also be a risk factor for dementia. However, these studies have significant methodological weaknesses and fail to distinguish between long-term and short-term variability. The aim of this work was to propose a new joint model that combines a mixed-effects model, which handles the residual variance distinguishing inter-visit variability from intra-visit variability, and an illness-death model that allows for interval censoring and semi-competing risks. A subject-specific random effect is included in the model for both variances. Risks can simultaneously depend on the current value and slope of the marker, as well as on each of the two components of the residual variance. The model estimation is performed by maximizing the likelihood function using the Marquardt-Levenberg algorithm. A simulation study validates the estimation procedure, which is implemented in an R package. The model was estimated using data from the Three-City (3C) cohort to study the impact of intra- and inter-visits blood pressure variability on the risk of dementia and death.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06769v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'eonie Courcoul, Catherine Helmer, Antoine Barbieri, H\'el\`ene Jacqmin-Gadda</dc:creator>
    </item>
    <item>
      <title>Optimal Network Pairwise Comparison</title>
      <link>https://arxiv.org/abs/2408.06987</link>
      <description>arXiv:2408.06987v1 Announce Type: new 
Abstract: We are interested in the problem of two-sample network hypothesis testing: given two networks with the same set of nodes, we wish to test whether the underlying Bernoulli probability matrices of the two networks are the same or not. We propose Interlacing Balance Measure (IBM) as a new two-sample testing approach. We consider the {\it Degree-Corrected Mixed-Membership (DCMM)} model for undirected networks, where we allow severe degree heterogeneity, mixed-memberships, flexible sparsity levels, and weak signals. In such a broad setting, how to find a test that has a tractable limiting null and optimal testing performances is a challenging problem. We show that IBM is such a test: in a broad DCMM setting with only mild regularity conditions, IBM has $N(0,1)$ as the limiting null and achieves the optimal phase transition.
  While the above is for undirected networks, IBM is a unified approach and is directly implementable for directed networks. For a broad directed-DCMM (extension of DCMM for directed networks) setting, we show that IBM has $N(0, 1/2)$ as the limiting null and continues to achieve the optimal phase transition. We have also applied IBM to the Enron email network and a gene co-expression network, with interesting results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06987v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiashun Jin, Zheng Tracy Ke, Shengming Luo, Yucong Ma</dc:creator>
    </item>
    <item>
      <title>The Complexities of Differential Privacy for Survey Data</title>
      <link>https://arxiv.org/abs/2408.07006</link>
      <description>arXiv:2408.07006v1 Announce Type: new 
Abstract: The concept of differential privacy (DP) has gained substantial attention in recent years, most notably since the U.S. Census Bureau announced the adoption of the concept for its 2020 Decennial Census. However, despite its attractive theoretical properties, implementing DP in practice remains challenging, especially when it comes to survey data. In this paper we present some results from an ongoing project funded by the U.S. Census Bureau that is exploring the possibilities and limitations of DP for survey data. Specifically, we identify five aspects that need to be considered when adopting DP in the survey context: the multi-staged nature of data production; the limited privacy amplification from complex sampling designs; the implications of survey-weighted estimates; the weighting adjustments for nonresponse and other data deficiencies, and the imputation of missing values. We summarize the project's key findings with respect to each of these aspects and also discuss some of the challenges that still need to be addressed before DP could become the new data protection standard at statistical agencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07006v1</guid>
      <category>stat.ME</category>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>J\"org Drechsler, James Bailie</dc:creator>
    </item>
    <item>
      <title>GARCH copulas and GARCH-mimicking copulas</title>
      <link>https://arxiv.org/abs/2408.07025</link>
      <description>arXiv:2408.07025v1 Announce Type: new 
Abstract: The bivariate copulas that describe the dependencies and partial dependencies of lagged variables in strictly stationary, first-order GARCH-type processes are investigated. It is shown that the copulas of symmetric GARCH processes are jointly symmetric but non-exchangeable, while the copulas of processes with symmetric innovation distributions and asymmetric leverage effects have weaker h-symmetry; copulas with asymmetric innovation distributions have neither form of symmetry. Since the actual copulas are typically inaccessible, due to the unknown functional forms of the marginal distributions of GARCH processes, methods of mimicking them are proposed. These rely on constructions that combine standard bivariate copulas for positive dependence with two uniformity-preserving transformations known as v-transforms. A variety of new copulas are introduced and the ones providing the best fit to simulated data from GARCH-type processes are identified. A method of constructing tractable simplified d-vines using linear v-transforms is described and shown to coincide with the vt-d-vine model when the two v-transforms are identical.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07025v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexandra Dias, Jialing Han, Alexander J. McNeil</dc:creator>
    </item>
    <item>
      <title>Conformal prediction after efficiency-oriented model selection</title>
      <link>https://arxiv.org/abs/2408.07066</link>
      <description>arXiv:2408.07066v1 Announce Type: new 
Abstract: Given a family of pretrained models and a hold-out set, how can we construct a valid conformal prediction set while selecting a model that minimizes the width of the set? If we use the same hold-out data set both to select a model (the model that yields the smallest conformal prediction sets) and then to construct a conformal prediction set based on that selected model, we suffer a loss of coverage due to selection bias. Alternatively, we could further splitting the data to perform selection and calibration separately, but this comes at a steep cost if the size of the dataset is limited. In this paper, we address the challenge of constructing a valid prediction set after efficiency-oriented model selection. Our novel methods can be implemented efficiently and admit finite-sample validity guarantees without invoking additional sample-splitting. We show that our methods yield prediction sets with asymptotically optimal size under certain notion of continuity for the model class. The improved efficiency of the prediction sets constructed by our methods are further demonstrated through applications to synthetic datasets in various settings and a real data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07066v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiting Liang, Wanrong Zhu, Rina Foygel Barber</dc:creator>
    </item>
    <item>
      <title>True and false discoveries with independent and sequential e-values</title>
      <link>https://arxiv.org/abs/2003.00593</link>
      <description>arXiv:2003.00593v2 Announce Type: replace 
Abstract: In this paper we use e-values in the context of multiple hypothesis testing assuming that the base tests produce independent, or sequential, e-values. Our simulation and empirical studies and theoretical considerations suggest that, under this assumption, our new algorithms are superior to the known algorithms using independent p-values and to our recent algorithms designed for e-values without the assumption of independence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2003.00593v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladimir Vovk, Ruodu Wang</dc:creator>
    </item>
    <item>
      <title>flexBART: Flexible Bayesian regression trees with categorical predictors</title>
      <link>https://arxiv.org/abs/2211.04459</link>
      <description>arXiv:2211.04459v3 Announce Type: replace 
Abstract: Most implementations of Bayesian additive regression trees (BART) one-hot encode categorical predictors, replacing each one with several binary indicators, one for every level or category. Regression trees built with these indicators partition the discrete set of categorical levels by repeatedly removing one level at a time. Unfortunately, the vast majority of partitions cannot be built with this strategy, severely limiting BART's ability to partially pool data across groups of levels. Motivated by analyses of baseball data and neighborhood-level crime dynamics, we overcame this limitation by re-implementing BART with regression trees that can assign multiple levels to both branches of a decision tree node. To model spatial data aggregated into small regions, we further proposed a new decision rule prior that creates spatially contiguous regions by deleting a random edge from a random spanning tree of a suitably defined network. Our re-implementation, which is available in the flexBART package, often yields improved out-of-sample predictive performance and scales better to larger datasets than existing implementations of BART.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.04459v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sameer K. Deshpande</dc:creator>
    </item>
    <item>
      <title>Stable Distillation and High-Dimensional Hypothesis Testing</title>
      <link>https://arxiv.org/abs/2212.12539</link>
      <description>arXiv:2212.12539v3 Announce Type: replace 
Abstract: While powerful methods have been developed for high-dimensional hypothesis testing assuming orthogonal parameters, current approaches struggle to generalize to the more common non-orthogonal case. We propose Stable Distillation (SD), a simple paradigm for iteratively extracting independent pieces of information from observed data, assuming a parametric model. When applied to hypothesis testing for large regression models, SD orthogonalizes the effect estimates of non-orthogonal predictors by judiciously introducing noise into the observed outcomes vector, yielding mutually independent p-values across predictors. Generic regression and gene-testing simulations show that SD yields a scalable approach for non-orthogonal designs that exceeds or matches the power of existing methods against sparse alternatives. While we only present explicit SD algorithms for hypothesis testing in ordinary least squares and logistic regression, we provide general guidance for deriving and improving the power of SD procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.12539v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryan Christ, Ira Hall, David Steinsaltz</dc:creator>
    </item>
    <item>
      <title>Assessing variable importance in survival analysis using machine learning</title>
      <link>https://arxiv.org/abs/2311.12726</link>
      <description>arXiv:2311.12726v3 Announce Type: replace 
Abstract: Given a collection of features available for inclusion in a predictive model, it may be of interest to quantify the relative importance of a subset of features for the prediction task at hand. For example, in HIV vaccine trials, participant baseline characteristics are used to predict the probability of HIV acquisition over the intended follow-up period, and investigators may wish to understand how much certain types of predictors, such as behavioral factors, contribute toward overall predictiveness. Time-to-event outcomes such as time to HIV acquisition are often subject to right censoring, and existing methods for assessing variable importance are typically not intended to be used in this setting. We describe a broad class of algorithm-agnostic variable importance measures for prediction in the context of survival data. We propose a nonparametric efficient estimation procedure that incorporates flexible learning of nuisance parameters, yields asymptotically valid inference, and enjoys double-robustness. We assess the performance of our proposed procedure via numerical simulations and analyze data from the HVTN 702 vaccine trial to inform enrollment strategies for future HIV vaccine trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.12726v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charles J. Wolock, Peter B. Gilbert, Noah Simon, Marco Carone</dc:creator>
    </item>
    <item>
      <title>Copula Approximate Bayesian Computation Using Distribution Random Forests</title>
      <link>https://arxiv.org/abs/2402.18450</link>
      <description>arXiv:2402.18450v2 Announce Type: replace 
Abstract: This paper introduces and provides an extensive simulation study of a new Approximate Bayesian Computation (ABC) framework for estimating the posterior distribution and the maximum likelihood estimate (MLE) of the parameters of models defined by intractable likelihood functions, which unifies ad extends previous ABC methods. This framework can describe the possibly skewed and high dimensional posterior distribution by a novel multivariate copula-based meta-\textit{t} distribution, based on univariate marginal posterior distributions which can account for skewness and accurately estimated by Distribution Random Forests (\texttt{drf}) while performing automatic summary statistics (covariates) selection, and on robustly-estimated copula dependence parameters. Also, the framework provides a novel multivariate mode estimator to perform for MLE and posterior mode estimation, and an optional step to perform model selection from a given set of models with posterior probabilities estimated by \texttt{drf}. The posterior distribution estimation accuracy of the ABC framework is illustrated and compared with previous standard ABC methods, through simulation studies involving low- and high-dimensional models with computable posterior distributions which are either unimodal, skewed, and multimodal; and exponential random graph and mechanistic network models which are each defined by an intractable likelihood from which it is costly to simulate large network datasets. This paper also introduces and studies a new new solution to simulation cost problem in ABC. Finally, the new framework is illustrated through analyses of large real-life networks of sizes ranging between 28,000 to 65.6 million nodes (between 3 million to 1.8 billion edges), including a large multilayer network with weighted directed edges.
  Keywords: Bayesian analysis, Maximum Likelihood, Intractable likelihood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18450v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Karabatsos</dc:creator>
    </item>
    <item>
      <title>Combining BART and Principal Stratification to estimate the effect of intermediate on primary outcomes with application to estimating the effect of family planning on employment in sub-Saharan Africa</title>
      <link>https://arxiv.org/abs/2408.03777</link>
      <description>arXiv:2408.03777v2 Announce Type: replace 
Abstract: There is interest in learning about the causal effect of family planning (FP) on empowerment related outcomes. Experimental data related to this question are available from trials in which FP programs increase access to FP. While program assignment is unconfounded, FP uptake and subsequent empowerment may share common causes. We use principal stratification to estimate the causal effect of an intermediate FP outcome on a primary outcome of interest, among women affected by a FP program. Within strata defined by the potential reaction to the program, FP uptake is unconfounded. To minimize the need for parametric assumptions, we propose to use Bayesian Additive Regression Trees (BART) for modeling stratum membership and outcomes of interest. We refer to the combined approach as Prince BART. We evaluate Prince BART through a simulation study and use it to assess the causal effect of modern contraceptive use on employment in six cities in Nigeria, based on quasi-experimental data from a FP program trial during the first half of the 2010s. We show that findings differ between Prince BART and alternative modeling approaches based on parametric assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03777v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Godoy Garraza, Ilene Speizer, Leontine Alkema</dc:creator>
    </item>
    <item>
      <title>Nuisance Function Tuning and Sample Splitting for Optimal Doubly Robust Estimation</title>
      <link>https://arxiv.org/abs/2212.14857</link>
      <description>arXiv:2212.14857v3 Announce Type: replace-cross 
Abstract: Estimators of doubly robust functionals typically rely on estimating two complex nuisance functions, such as the propensity score and conditional outcome mean for the average treatment effect functional. We consider the problem of how to estimate nuisance functions to obtain optimal rates of convergence for a doubly robust nonparametric functional that has witnessed applications across the causal inference and conditional independence testing literature. For several plug-in estimators and a first-order bias-corrected estimator, we illustrate the interplay between different tuning parameter choices for the nuisance function estimators and sample splitting strategies on the optimal rate of estimating the functional of interest. For each of these estimators and each sample splitting strategy, we show the necessity to either undersmooth or oversmooth the nuisance function estimators under low regularity conditions to obtain optimal rates of convergence for the functional of interest. Unlike the existing literature, we show that plug-in and first-order biased-corrected estimators can achieve minimax rates of convergence across all H\"older smoothness classes of the nuisance functions by careful combinations of sample splitting and nuisance function tuning strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.14857v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sean McGrath, Rajarshi Mukherjee</dc:creator>
    </item>
  </channel>
</rss>
