<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 Jan 2025 02:38:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>SoMa: Identifying, Exploring, and Understanding the DRAM Communication Scheduling Space for DNN Accelerators</title>
      <link>https://arxiv.org/abs/2501.12634</link>
      <description>arXiv:2501.12634v1 Announce Type: new 
Abstract: Modern Deep Neural Network (DNN) accelerators are equipped with increasingly larger on-chip buffers to provide more opportunities to alleviate the increasingly severe DRAM bandwidth pressure. However, most existing research on buffer utilization still primarily focuses on single-layer dataflow scheduling optimization. As buffers grow large enough to accommodate most single-layer weights in most networks, the impact of single-layer dataflow optimization on DRAM communication diminishes significantly. Therefore, developing new paradigms that fuse multiple layers to fully leverage the increasingly abundant on-chip buffer resources to reduce DRAM accesses has become particularly important, yet remains an open challenge. To address this challenge, we first identify the optimization opportunities in DRAM communication scheduling by analyzing the drawbacks of existing works on the layer fusion paradigm and recognizing the vast optimization potential in scheduling the timing of data prefetching from and storing to DRAM. To fully exploit these optimization opportunities, we develop a Tensor-centric Notation and its corresponding parsing method to represent different DRAM communication scheduling schemes and depict the overall space of DRAM communication scheduling. Then, to thoroughly and efficiently explore the space of DRAM communication scheduling for diverse accelerators and workloads, we develop an end-to-end scheduling framework, SoMa, which has already been developed into a compiler for our commercial accelerator product. Compared with the state-of-the-art (SOTA) Cocco framework, SoMa achieves, on average, a 2.11x performance improvement and a 37.3% reduction in energy cost simultaneously. Then, we leverage SoMa to study optimizations for LLM, perform design space exploration (DSE), and analyze the DRAM communication scheduling space through a practical example, yielding some..(more)</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12634v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingwei Cai (Tsinghua University), Xuan Wang (Xi'an Jiaotong University, IIISCT), Mingyu Gao (Tsinghua University, Shanghai Qi Zhi Institute), Sen Peng (Xi'an Jiaotong University, IIISCT), Zijian Zhu (Tsinghua University), Yuchen Wei (Tsinghua University), Zuotong Wu (Xi'an Jiaotong University, IIISCT), Kaisheng Ma (Tsinghua University)</dc:creator>
    </item>
    <item>
      <title>HEPPO: Hardware-Efficient Proximal Policy Optimization -- A Universal Pipelined Architecture for Generalized Advantage Estimation</title>
      <link>https://arxiv.org/abs/2501.12703</link>
      <description>arXiv:2501.12703v1 Announce Type: new 
Abstract: This paper introduces HEPPO, an FPGA-based accelerator designed to optimize the Generalized Advantage Estimation (GAE) stage in Proximal Policy Optimization (PPO). Unlike previous approaches that focused on trajectory collection and actor-critic updates, HEPPO addresses GAE's computational demands with a parallel, pipelined architecture implemented on a single System-on-Chip (SoC). This design allows for the adaptation of various hardware accelerators tailored for different PPO phases. A key innovation is our strategic standardization technique, which combines dynamic reward standardization and block standardization for values, followed by 8-bit uniform quantization. This method stabilizes learning, enhances performance, and manages memory bottlenecks, achieving a 4x reduction in memory usage and a 1.5x increase in cumulative rewards. We propose a solution on a single SoC device with programmable logic and embedded processors, delivering throughput orders of magnitude higher than traditional CPU-GPU systems. Our single-chip solution minimizes communication latency and throughput bottlenecks, significantly boosting PPO training efficiency. Experimental results show a 30% increase in PPO speed and a substantial reduction in memory access time, underscoring HEPPO's potential for broad applicability in hardware-efficient reinforcement learning algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12703v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hazem Taha, Ameer M. S. Abdelhadi</dc:creator>
    </item>
    <item>
      <title>Late Breaking Result: FPGA-Based Emulation and Fault Injection for CNN Inference Accelerators</title>
      <link>https://arxiv.org/abs/2501.12818</link>
      <description>arXiv:2501.12818v1 Announce Type: new 
Abstract: A new field programmable gate array (FPGA)-based emulation platform is proposed to accelerate fault tolerance analysis of inference accelerators of convolutional neural networks (CNN). For a given CNN model, hardware accelerator architecture, and FT analysis target, an FPGA-based CNN implementation is generated (with the help of the Tengine framework), and fault injection logic is added. In our first case study, we report how the classification accuracy drop depends on the faults injected into multipliers used in Multiply-and-Accumulate Units of NVDLA inference accelerator executing ResNet-18 CNN. The FT analysis emulated on Zynq UltraScale+ SoC is an order of magnitude faster than software emulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12818v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Filip Masar, Vojtech Mrazek, Lukas Sekanina</dc:creator>
    </item>
    <item>
      <title>Current Opinions on Memristor-Accelerated Machine Learning Hardware</title>
      <link>https://arxiv.org/abs/2501.12644</link>
      <description>arXiv:2501.12644v1 Announce Type: cross 
Abstract: The unprecedented advancement of artificial intelligence has placed immense demands on computing hardware, but traditional silicon-based semiconductor technologies are approaching their physical and economic limit, prompting the exploration of novel computing paradigms. Memristor offers a promising solution, enabling in-memory analog computation and massive parallelism, which leads to low latency and power consumption. This manuscript reviews the current status of memristor-based machine learning accelerators, highlighting the milestones achieved in developing prototype chips, that not only accelerate neural networks inference but also tackle other machine learning tasks. More importantly, it discusses our opinion on current key challenges that remain in this field, such as device variation, the need for efficient peripheral circuitry, and systematic co-design and optimization. We also share our perspective on potential future directions, some of which address existing challenges while others explore untouched territories. By addressing these challenges through interdisciplinary efforts spanning device engineering, circuit design, and systems architecture, memristor-based accelerators could significantly advance the capabilities of AI hardware, particularly for edge applications where power efficiency is paramount.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12644v1</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>physics.app-ph</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingrui Jiang, Yichun Xu, Zefan Li, Can Li</dc:creator>
    </item>
    <item>
      <title>Analyzing and Exploiting Branch Mispredictions in Microcode</title>
      <link>https://arxiv.org/abs/2501.12890</link>
      <description>arXiv:2501.12890v1 Announce Type: cross 
Abstract: We present uSpectre, a new class of transient execution attacks that exploit microcode branch mispredictions to transiently leak sensitive data. We find that many long-known and recently-discovered transient execution attacks, which were previously categorized as Spectre or Meltdown variants, are actually instances of uSpectre on some Intel microarchitectures. Based on our observations, we discover multiple new uSpectre attacks and present a defense against uSpectre vulnerabilities, called uSLH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12890v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicholas Mosier, Hamed Nemati, John C. Mitchell, Caroline Trippel</dc:creator>
    </item>
  </channel>
</rss>
