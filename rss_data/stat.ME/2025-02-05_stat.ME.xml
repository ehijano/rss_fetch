<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Feb 2025 05:00:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Improving exponential-family random graph models for bipartite networks</title>
      <link>https://arxiv.org/abs/2502.01892</link>
      <description>arXiv:2502.01892v1 Announce Type: new 
Abstract: Bipartite graphs, representing two-mode networks, arise in many research fields. These networks have two disjoint node sets representing distinct entity types, for example persons and groups, with edges representing associations between the two entity types. In bipartite graphs, the smallest possible cycle is a cycle of length four, and hence four-cycles are the smallest structure to model closure in such networks. Exponential-family random graph models (ERGMs) are a widely used model for social, and other, networks, including specifically bipartite networks. Existing ERGM terms to model four-cycles in bipartite networks, however, are relatively rarely used. In this work we demonstrate some problems with these existing terms to model four-cycles, and define new ERGM terms to help overcome these problems. The position of the new terms in the ERGM dependence hierarchy, and their interpretation, is discussed. The new terms are demonstrated in simulation experiments, and their application illustrated with ERGM models of empirical networks ranging in size from hundreds of nodes to hundreds of thousands of nodes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01892v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex Stivala, Peng Wang, Alessandro Lomi</dc:creator>
    </item>
    <item>
      <title>Detection and estimation of vertex-wise latent position shifts across networks</title>
      <link>https://arxiv.org/abs/2502.01947</link>
      <description>arXiv:2502.01947v1 Announce Type: new 
Abstract: Pairwise network comparison is essential for various applications, including neuroscience, disease research, and dynamic network analysis. While existing literature primarily focuses on comparing entire network structures, we address a vertex-wise comparison problem where two random networks share the same set of vertices but allow for structural variations in some vertices, enabling a more detailed and flexible analysis of network differences. In our framework, some vertices retain their latent positions between networks, while others undergo shifts. To identify the shifted and unshifted vertices and estimate their latent position shifts, we propose a method that first derives vertex embeddings in a low-rank Euclidean space for each network, then aligns these estimated vertex latent positions into a common space to resolve potential non-identifiability, and finally tests whether each vertex is shifted or not and estimates the vertex shifts. Our theoretical results establish the test statistic for the algorithms, guide parameter selection, and provide performance guarantees. Simulation studies and real data applications, including a case-control study in disease research and dynamic network analysis, demonstrate that the proposed algorithms are both computationally efficient and effective in extracting meaningful insights from network comparisons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01947v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Runbing Zheng</dc:creator>
    </item>
    <item>
      <title>Heteroscedastic Double Bayesian Elastic Net</title>
      <link>https://arxiv.org/abs/2502.02032</link>
      <description>arXiv:2502.02032v1 Announce Type: new 
Abstract: In many practical applications, regression models are employed to uncover relationships between predictors and a response variable, yet the common assumption of constant error variance is frequently violated. This issue is further compounded in high-dimensional settings where the number of predictors exceeds the sample size, necessitating regularization for effective estimation and variable selection. To address this problem, we propose the Heteroscedastic Double Bayesian Elastic Net (HDBEN), a novel framework that jointly models the mean and log-variance using hierarchical Bayesian priors incorporating both $\ell_1$ and $\ell_2$ penalties. Our approach simultaneously induces sparsity and grouping in the regression coefficients and variance parameters, capturing complex variance structures in the data. Theoretical results demonstrate that proposed HDBEN achieves posterior concentration, variable selection consistency, and asymptotic normality under mild conditions which justifying its behavior. Simulation studies further illustrate that HDBEN outperforms existing methods, particularly in scenarios characterized by heteroscedasticity and high dimensionality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02032v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>stat.ML</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masanari Kimura</dc:creator>
    </item>
    <item>
      <title>Multi-Study Causal Forest (MCF): A flexible framework for data borrowing in the presence of varying treatment effect heterogeneity</title>
      <link>https://arxiv.org/abs/2502.02110</link>
      <description>arXiv:2502.02110v1 Announce Type: new 
Abstract: Tailoring treatment assignment to specific individuals can improve the health outcomes, but a single study may offer inadequate information for this purpose. The ability to leverage information from an auxiliary data source deemed to be `most similar' to a primary data source has been shown to improve estimates of treatment effects. In this paper, we introduce a framework, the Multi-Study Causal Forest (MCF), to borrow individual patient-level data from an auxiliary data source in the presence of `varying sources' of treatment effect heterogeneity. We utilise a simulation study to demonstrate the superiority of the MCF in the presence of varying treatment allocation models (between-study heterogeneity) in addition to being able to account for the presence of within-study heterogeneity. This approach can combine data from randomised controlled trials, observational studies or a combination of both. We illustrate using Breast cancer data that the MCF performs favourably compared to an existing methodology in the presence of varying sources of (both between and within) heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02110v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashwini Venkatasubramaniam, Julian Wolfson</dc:creator>
    </item>
    <item>
      <title>Can linear algebra create perfect knockoffs?</title>
      <link>https://arxiv.org/abs/2502.02148</link>
      <description>arXiv:2502.02148v1 Announce Type: new 
Abstract: As new Model-X knockoff construction techniques are developed, primarily concerned with determining the correct conditional distribution from which to sample, we focus less on deriving the correct multivariate distribution and instead ask if ``perfect'' knockoffs can be constructed using linear algebra. Using mean absolute correlation between knockoffs and features as a measure of quality, we do produce knockoffs that are pseudo-perfect, however, the optimization algorithm is computationally very expensive. We outline a series of methods to significantly reduce the computation time of the algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02148v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-74491-4_81</arxiv:DOI>
      <arxiv:journal_reference>Big Data and Internet of Things. BDIoT 2024. Lecture Notes in Networks and Systems, vol 887. Springer, Cham</arxiv:journal_reference>
      <dc:creator>Christopher Hemmens, Stephan Robert-Nicoud</dc:creator>
    </item>
    <item>
      <title>On The Performance of a Two-Sided Shewhart Chart for Continuous Proportions with Estimated Parameters</title>
      <link>https://arxiv.org/abs/2502.02296</link>
      <description>arXiv:2502.02296v1 Announce Type: new 
Abstract: During the recent years there was an increased interest in studying the performance of different types of control charts, under various distributional models for continuous proportions, such as percentages and rates. In this work we consider the Kumaraswamy distribution, a popular and flexible distributional model for data in the unit interval (0,1) and investigate further the properties of a two-sided chart for individual observations for monitoring these types of processes, when the process parameters are unknown. Specifically, using Monte Carlo simulation, we evaluate the performance of the chart under a conditional perspective and provide empirical rules on how to select the appropriate size for the Phase I sample. In addition, we explore possible adjustments on the control limits of the chart, which take into account the available Phase I sample. The performance of the chart is also investigated for several out-of-control situations. The results show that for small and moderate size Phase I samples, practitioners have to choose whether they prefer a guaranteed in-control performance or an improved out-of-control performance. The implementation of the considered methods in practice is discussed via two numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02296v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Athanasios C. Rakitzis</dc:creator>
    </item>
    <item>
      <title>Variational inference for approximate reference priors using neural networks</title>
      <link>https://arxiv.org/abs/2502.02364</link>
      <description>arXiv:2502.02364v1 Announce Type: new 
Abstract: In Bayesian statistics, the choice of the prior can have an important influence on the posterior and the parameter estimation, especially when few data samples are available. To limit the added subjectivity from a priori information, one can use the framework of reference priors. However, computing such priors is a difficult task in general. We develop in this paper a flexible algorithm based on variational inference which computes approximations of reference priors from a set of parametric distributions using neural networks. We also show that our algorithm can retrieve reference priors when constraints are specified in the optimization problem to ensure the solution is proper. We propose a simple method to recover a relevant approximation of the parametric posterior distribution using Markov Chain Monte Carlo (MCMC) methods even if the density function of the parametric prior is not known in general. Numerical experiments on several statistical models of increasing complexity are presented. We show the usefulness of this approach by recovering the target distribution. The performance of the algorithm is evaluated on the prior distributions as well as the posterior distributions, jointly using variational inference and MCMC sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02364v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nils Baillie, Antoine Van Biesbroeck, Cl\'ement Gauchy</dc:creator>
    </item>
    <item>
      <title>Synthetic Random Environmental Time Series Generation with Similarity Control, Preserving Original Signal's Statistical Characteristics</title>
      <link>https://arxiv.org/abs/2502.02392</link>
      <description>arXiv:2502.02392v1 Announce Type: new 
Abstract: Synthetic datasets are widely used in many applications, such as missing data imputation, examining non-stationary scenarios, in simulations, training data-driven models, and analyzing system robustness. Typically, synthetic data are based on historical data obtained from the observed system. The data needs to represent a specific behavior of the system, yet be new and diverse enough so that the system is challenged with a broad range of inputs. This paper presents a method, based on discrete Fourier transform, for generating synthetic time series with similar statistical moments for any given signal. The suggested method makes it possible to control the level of similarity between the given signal and the generated synthetic signals. Proof shows analytically that this method preserves the first two statistical moments of the input signal, and its autocorrelation function. The method is compared to known methods, ARMA, GAN, and CoSMoS. A large variety of environmental datasets with different temporal resolutions, and from different domains are used, testing the generality and flexibility of the method. A Python library implementing this method is made available as open-source software.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02392v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.envsoft.2024.106283</arxiv:DOI>
      <arxiv:journal_reference>Environmental Modelling &amp; Software, Volume 185, February 2025, 106283</arxiv:journal_reference>
      <dc:creator>Ofek Aloni, Gal Perelman, Barak Fishbain</dc:creator>
    </item>
    <item>
      <title>Is this normal? A new projection pursuit index to assess a sample against a multivariate null distribution</title>
      <link>https://arxiv.org/abs/2502.02397</link>
      <description>arXiv:2502.02397v1 Announce Type: new 
Abstract: Many data problems contain some reference or normal conditions, upon which to compare newly collected data. This scenario occurs in data collected as part of clinical trials to detect adverse events, or for measuring climate change against historical norms. The data is typically multivariate, and often the normal ranges are specified by a multivariate normal distribution. The work presented in this paper develops methods to compare the new sample against the reference distribution with high-dimensional visualisation. It uses a projection pursuit guided tour to produce a sequence of low-dimensional projections steered towards those where the new sample is most different from the reference. A new projection pursuit index is defined for this purpose. The tour visualisation also includes drawing of the projected ellipse, which is computed analytically, corresponding to the reference distribution. The methods are implemented in the R package, tourr.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02397v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annalisa Calvi, Ursula Laa, Dianne Cook</dc:creator>
    </item>
    <item>
      <title>Nonlinear Covariance Shrinkage for Hotelling's $T^2$ in High Dimension</title>
      <link>https://arxiv.org/abs/2502.02006</link>
      <description>arXiv:2502.02006v1 Announce Type: cross 
Abstract: In this paper we study the problem of comparing the means of a single observation and a reference sample in the presence of a common data covariance matrix, where the data dimension $p$ grows linearly with the number of samples $n$ and $p/n$ converges to a number between 0 and 1. The approach we take is to replace the sample covariance matrix with a nonlinear shrinkage estimator -- i.e., a matrix with the same eigenvectors -- in Hotelling's $T^2$ test. Current approaches of this sort typically assume that the data covariance matrix has a condition number or spiked rank that increases slowly with dimension. However, this assumption is ill-suited to data sets containing many strongly correlated background covariates, as often found in finance, genetics, and remote sensing. To address this problem we construct, using variational methods and new local random-matrix laws, a nonlinear covariance shrinkage method tailored to optimize detection performance across a broad range of spiked ranks and condition numbers. We then demonstrate, via both simulated and real-world data, that our method outperforms existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02006v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin D. Robinson, Van Latimer</dc:creator>
    </item>
    <item>
      <title>Causal bandits with backdoor adjustment on unknown Gaussian DAGs</title>
      <link>https://arxiv.org/abs/2502.02020</link>
      <description>arXiv:2502.02020v1 Announce Type: cross 
Abstract: The causal bandit problem aims to sequentially learn the intervention that maximizes the expectation of a reward variable within a system governed by a causal graph. Most existing approaches assume prior knowledge of the graph structure, or impose unrealistically restrictive conditions on the graph. In this paper, we assume a Gaussian linear directed acyclic graph (DAG) over arms and the reward variable, and study the causal bandit problem when the graph structure is unknown. We identify backdoor adjustment sets for each arm using sequentially generated experimental and observational data during the decision process, which allows us to estimate causal effects and construct upper confidence bounds. By integrating estimates from both data sources, we develop a novel bandit algorithm, based on modified upper confidence bounds, to sequentially determine the optimal intervention. We establish both case-dependent and case-independent upper bounds on the cumulative regret for our algorithm, which improve upon the bounds of the standard multi-armed bandit algorithms. Our empirical study demonstrates its advantage over existing methods with respect to cumulative regret and computation time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02020v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yijia Zhao, Qing Zhou</dc:creator>
    </item>
    <item>
      <title>Target-aware Bayesian inference via generalized thermodynamic integration</title>
      <link>https://arxiv.org/abs/2502.02206</link>
      <description>arXiv:2502.02206v1 Announce Type: cross 
Abstract: In Bayesian inference, we are usually interested in the numerical approximation of integrals that are posterior expectations or marginal likelihoods (a.k.a., Bayesian evidence). In this paper, we focus on the computation of the posterior expectation of a function $f(\x)$. We consider a \emph{target-aware} scenario where $f(\x)$ is known in advance and can be exploited in order to improve the estimation of the posterior expectation. In this scenario, this task can be reduced to perform several independent marginal likelihood estimation tasks. The idea of using a path of tempered posterior distributions has been widely applied in the literature for the computation of marginal likelihoods. Thermodynamic integration, path sampling and annealing importance sampling are well-known examples of algorithms belonging to this family of methods. In this work, we introduce a generalized thermodynamic integration (GTI) scheme which is able to perform a target-aware Bayesian inference, i.e., GTI can approximate the posterior expectation of a given function. Several scenarios of application of GTI are discussed and different numerical simulations are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02206v1</guid>
      <category>stat.CO</category>
      <category>cs.CE</category>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s00180-023-01358-0</arxiv:DOI>
      <arxiv:journal_reference>Computational Statistics, Volume 38, Pages 2097-2119, year 2023</arxiv:journal_reference>
      <dc:creator>F. Llorente, L. Martino, D. Delgado</dc:creator>
    </item>
    <item>
      <title>Robust and Conjugate Spatio-Temporal Gaussian Processes</title>
      <link>https://arxiv.org/abs/2502.02450</link>
      <description>arXiv:2502.02450v1 Announce Type: cross 
Abstract: State-space formulations allow for Gaussian process (GP) regression with linear-in-time computational cost in spatio-temporal settings, but performance typically suffers in the presence of outliers. In this paper, we adapt and specialise the robust and conjugate GP (RCGP) framework of Altamirano et al. (2024) to the spatio-temporal setting. In doing so, we obtain an outlier-robust spatio-temporal GP with a computational cost comparable to classical spatio-temporal GPs. We also overcome the three main drawbacks of RCGPs: their unreliable performance when the prior mean is chosen poorly, their lack of reliable uncertainty quantification, and the need to carefully select a hyperparameter by hand. We study our method extensively in finance and weather forecasting applications, demonstrating that it provides a reliable approach to spatio-temporal modelling in the presence of outliers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02450v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Laplante, Matias Altamirano, Andrew Duncan, Jeremias Knoblauch, Fran\c{c}ois-Xavier Briol</dc:creator>
    </item>
    <item>
      <title>Hierarchical Sparse Bayesian Multitask Model with Scalable Inference for Microbiome Analysis</title>
      <link>https://arxiv.org/abs/2502.02552</link>
      <description>arXiv:2502.02552v1 Announce Type: cross 
Abstract: This paper proposes a hierarchical Bayesian multitask learning model that is applicable to the general multi-task binary classification learning problem where the model assumes a shared sparsity structure across different tasks. We derive a computationally efficient inference algorithm based on variational inference to approximate the posterior distribution. We demonstrate the potential of the new approach on various synthetic datasets and for predicting human health status based on microbiome profile. Our analysis incorporates data pooled from multiple microbiome studies, along with a comprehensive comparison with other benchmark methods. Results in synthetic datasets show that the proposed approach has superior support recovery property when the underlying regression coefficients share a common sparsity structure across different tasks. Our experiments on microbiome classification demonstrate the utility of the method in extracting informative taxa while providing well-calibrated predictions with uncertainty quantification and achieving competitive performance in terms of prediction metrics. Notably, despite the heterogeneity of the pooled datasets (e.g., different experimental objectives, laboratory setups, sequencing equipment, patient demographics), our method delivers robust results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02552v1</guid>
      <category>cs.LG</category>
      <category>q-bio.BM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haonan Zhu, Andre R. Goncalves, Camilo Valdes, Hiranmayi Ranganathan, Boya Zhang, Jose Manuel Mart\'i, Car Reen Kok, Monica K. Borucki, Nisha J. Mulakken, James B. Thissen, Crystal Jaing, Alfred Hero, Nicholas A. Be</dc:creator>
    </item>
    <item>
      <title>Minimax-Optimal Covariance Projected Spectral Clustering for High-Dimensional Nonspherical Mixtures</title>
      <link>https://arxiv.org/abs/2502.02580</link>
      <description>arXiv:2502.02580v1 Announce Type: cross 
Abstract: In mixture models, nonspherical (anisotropic) noise within each cluster is widely present in real-world data. We study both the minimax rate and optimal statistical procedure for clustering under high-dimensional nonspherical mixture models. In high-dimensional settings, we first establish the information-theoretic limits for clustering under Gaussian mixtures. The minimax lower bound unveils an intriguing informational dimension-reduction phenomenon: there exists a substantial gap between the minimax rate and the oracle clustering risk, with the former determined solely by the projected centers and projected covariance matrices in a low-dimensional space. Motivated by the lower bound, we propose a novel computationally efficient clustering method: Covariance Projected Spectral Clustering (COPO). Its key step is to project the high-dimensional data onto the low-dimensional space spanned by the cluster centers and then use the projected covariance matrices in this space to enhance clustering. We establish tight algorithmic upper bounds for COPO, both for Gaussian noise with flexible covariance and general noise with local dependence. Our theory indicates the minimax-optimality of COPO in the Gaussian case and highlights its adaptivity to a broad spectrum of dependent noise. Extensive simulation studies under various noise structures and real data analysis demonstrate our method's superior performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02580v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengzhu Huang, Yuqi Gu</dc:creator>
    </item>
    <item>
      <title>Visual analysis of bivariate dependence between continuous random variables</title>
      <link>https://arxiv.org/abs/2404.00820</link>
      <description>arXiv:2404.00820v2 Announce Type: replace 
Abstract: Scatter plots are widely recognized as fundamental tools for illustrating the relationship between two numerical variables. Despite this, based on solid theoretical foundations, scatter plots generated from pairs of continuous random variables may not serve as reliable tools for assessing dependence. Sklar's Theorem implies that scatter plots created from ranked data are preferable for such analysis as they exclusively convey information pertinent to dependence. This is in stark contrast to conventional scatter plots, which also encapsulate information about the variables' marginal distributions. Such additional information is extraneous to dependence analysis and can obscure the visual interpretation of the variables' relationship. In this article, we delve into the theoretical underpinnings of these ranked data scatter plots, hereafter referred to as rank plots. We offer insights into interpreting the information they reveal and examine their connections with various association measures, including Pearson's and Spearman's correlation coefficients, as well as Schweizer-Wolff's measure of dependence. Furthermore, we introduce a novel graphical combination for dependence analysis, termed a dplot, and demonstrate its efficacy through real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00820v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arturo Erdely, Manuel Rubio-Sanchez</dc:creator>
    </item>
    <item>
      <title>Exact Sampling of Spanning Trees via Fast-forwarded Random Walks</title>
      <link>https://arxiv.org/abs/2405.03096</link>
      <description>arXiv:2405.03096v2 Announce Type: replace 
Abstract: Tree graphs are routinely used in statistics. When estimating a Bayesian model with a tree component, sampling the posterior remains a core difficulty. Existing Markov chain Monte Carlo methods tend to rely on local moves, often leading to poor mixing. A promising approach is to instead directly sample spanning trees on an auxiliary graph. Current spanning tree samplers, such as the celebrated Aldous--Broder algorithm, predominantly rely on simulating random walks that are required to visit all the nodes of the graph. Such algorithms are prone to getting stuck in certain sub-graphs. We formalize this phenomenon using the bottlenecks in the random walk's transition probability matrix. We then propose a novel fast-forwarded cover algorithm that can break free from bottlenecks. The core idea is a marginalization argument that leads to a closed-form expression which allows for fast-forwarding to the event of visiting a new node. Unlike many existing approximation algorithms, our algorithm yields exact samples. We demonstrate the enhanced efficiency of the fast-forwarded cover algorithm, and illustrate its application in fitting a Bayesian dendrogram model on a Massachusetts crimes and communities dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03096v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edric Tam, David B. Dunson, Leo L. Duan</dc:creator>
    </item>
    <item>
      <title>A Deterministic Information Bottleneck Method for Clustering Mixed-Type Data</title>
      <link>https://arxiv.org/abs/2407.03389</link>
      <description>arXiv:2407.03389v3 Announce Type: replace 
Abstract: In this paper, we present an information-theoretic method for clustering mixed-type data, that is, data consisting of both continuous and categorical variables. The proposed approach is built on the deterministic variant of the Information Bottleneck algorithm, designed to optimally compress data while preserving its relevant structural information. We evaluate the performance of our method against four well-established clustering techniques for mixed-type data -- KAMILA, K-Prototypes, Factor Analysis for Mixed Data with K-Means, and Partitioning Around Medoids using Gower's dissimilarity -- using both simulated and real-world datasets. The results highlight that the proposed approach offers a competitive alternative to traditional clustering techniques, particularly under specific conditions where heterogeneity in data poses significant challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03389v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Efthymios Costa, Ioanna Papatsouma, Angelos Markos</dc:creator>
    </item>
    <item>
      <title>A novel framework for quantifying nominal outlyingness</title>
      <link>https://arxiv.org/abs/2408.07463</link>
      <description>arXiv:2408.07463v2 Announce Type: replace 
Abstract: Outlier detection is an important data mining tool that becomes particularly challenging when dealing with nominal data. First and foremost, flagging observations as outlying requires a well-defined notion of nominal outlyingness. This paper presents a definition of nominal outlyingness and introduces a general framework for quantifying outlyingness of nominal data. The proposed framework makes use of ideas from the association rule mining literature and can be used for calculating scores that indicate how outlying a nominal observation is. Methods for determining the involved hyperparameter values are presented and the concepts of variable contributions and outlyingness depth are introduced, in an attempt to enhance interpretability of the results. The proposed framework is evaluated on both synthetic and real-world data sets, demonstrating comparable performance to state-of-the-art frequent pattern mining algorithms and even outperforming them in certain cases. The ideas presented can serve as a tool for assessing the degree to which an observation differs from the rest of the data, under the assumption of sequences of nominal levels having been generated from a Multinomial distribution with varying event probabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07463v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Efthymios Costa, Ioanna Papatsouma</dc:creator>
    </item>
    <item>
      <title>COADVISE: Covariate Adjustment with Variable Selection and Missing Data Imputation in Randomized Controlled Trials</title>
      <link>https://arxiv.org/abs/2501.08945</link>
      <description>arXiv:2501.08945v2 Announce Type: replace 
Abstract: Adjusting for covariates in randomized controlled trials can enhance the credibility and efficiency of treatment effect estimation. However, handling numerous covariates and their non-linear transformations is challenging, particularly when outcomes and covariates have missing data. In this tutorial, we propose a principled Covariate Adjustment with Variable Selection and Missing Data Imputation (COADVISE) framework that enables (i) variable selection for covariates most relevant to the outcome, (ii) nonlinear adjustments, and (iii) robust imputation of missing data for both outcomes and covariates. This framework ensures consistent estimates with improved efficiency over unadjusted estimators and provides robust variance estimation, even under outcome model misspecification. We demonstrate efficiency gains through theoretical analysis and conduct extensive simulations to compare alternative variable selection strategies, offering cautionary recommendations. We showcase the practical utility of COADVISE by applying it to the Best Apnea Interventions for Research trial data from the National Sleep Research Resource. A user-friendly R package, Coadvise, facilitates implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08945v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Liu, Ke Zhu, Larry Han, Shu Yang</dc:creator>
    </item>
    <item>
      <title>Applying non-negative matrix factorization with covariates to multivariate time series data as a vector autoregression model</title>
      <link>https://arxiv.org/abs/2501.17446</link>
      <description>arXiv:2501.17446v2 Announce Type: replace 
Abstract: Non-negative matrix factorization (NMF) is a powerful technique for dimensionality reduction, but its application to time series data remains limited. This paper proposes a novel framework that integrates NMF with a vector autoregression (VAR) model to capture both latent structure and temporal dependencies in multivariate time series data. By representing the NMF coefficient matrix as a VAR model, the framework leverages the interpretability of NMF while incorporating the dynamic characteristics of time series data. This approach allows for the extraction of meaningful features and accurate predictions in time series data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17446v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kenichi Satoh</dc:creator>
    </item>
    <item>
      <title>Randomization Inference: Theory and Applications</title>
      <link>https://arxiv.org/abs/2406.09521</link>
      <description>arXiv:2406.09521v2 Announce Type: replace-cross 
Abstract: We review approaches to statistical inference based on randomization. Permutation tests are treated as an important special case. Under a certain group invariance property, referred to as the ``randomization hypothesis,'' randomization tests achieve exact control of the Type I error rate in finite samples. Although this unequivocal precision is very appealing, the range of problems that satisfy the randomization hypothesis is somewhat limited. We show that randomization tests are often asymptotically, or approximately, valid and efficient in settings that deviate from the conditions required for finite-sample error control. When randomization tests fail to offer even asymptotic Type 1 error control, their asymptotic validity may be restored by constructing an asymptotically pivotal test statistic. Randomization tests can then provide exact error control for tests of highly structured hypotheses with good performance in a wider class of problems. We give a detailed overview of several prominent applications of randomization tests, including two-sample permutation tests, regression, and conformal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09521v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David M. Ritzwoller, Joseph P. Romano, Azeem M. Shaikh</dc:creator>
    </item>
    <item>
      <title>Modeling Home Range and Intra-Specific Spatial Interaction in Wild Animal Populations</title>
      <link>https://arxiv.org/abs/2411.01694</link>
      <description>arXiv:2411.01694v2 Announce Type: replace-cross 
Abstract: Interactions among individuals from the same-species of wild animals are an important component of population dynamics. An interaction can be either static (based on overlap of space use) or dynamic (based on movement). The goal of this work is to determine the level of static interactions between individuals from the same-species of wild animals using 95\% and 50\% home ranges, as well as to model their movement interactions, which could include attraction, avoidance (or repulsion), or lack of interaction, in order to gain new insights and improve our understanding of ecological processes. Home range estimation methods (minimum convex polygon, kernel density estimator, and autocorrelated kernel density estimator), inhomogeneous multitype (or cross-type) summary statistics, and envelope testing methods (pointwise and global envelope tests) were proposed to study the nature of the same-species wild-animal spatial interactions. This study provides comprehensive, self-contained methodological details for investigating spatial interactions between individuals of the same species in wildlife populations. Using GPS collar data, we applied the methods to quantify both static and dynamic interactions between black bears in southern Alabama, USA. In general, our findings suggest that the black bears in our dataset showed no significant preference to live together or apart, i.e., there was no significant deviation from independence toward association or avoidance (i.e., segregation) between the bears. This can be loosely interpreted to mean that a black bear is generally indifferent to the presence of other black bears living or wandering nearby.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01694v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fekadu L. Bayisa, Christopher L. Seals, Hannah J. Leeper, Todd D. Steury, Elvan Ceyhan</dc:creator>
    </item>
  </channel>
</rss>
