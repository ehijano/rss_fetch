<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 15 Jul 2025 02:16:56 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Propensity score with factor loadings: the effect of the Paris Agreement</title>
      <link>https://arxiv.org/abs/2507.08764</link>
      <description>arXiv:2507.08764v1 Announce Type: new 
Abstract: Factor models for longitudinal data, where policy adoption is unconfounded with respect to a low-dimensional set of latent factor loadings, have become increasingly popular for causal inference. Most existing approaches, however, rely on a causal finite-sample approach or computationally intensive methods, limiting their applicability and external validity. In this paper, we propose a novel causal inference method for panel data based on inverse propensity score weighting where the propensity score is a function of latent factor loadings within a framework of causal inference from super-population. The approach relaxes the traditional restrictive assumptions of causal panel methods, while offering advantages in terms of causal interpretability, policy relevance, and computational efficiency. Under standard assumptions, we outline a three-step estimation procedure for the ATT and derive its large-sample properties using Mestimation theory. We apply the method to assess the causal effect of the Paris Agreement, a policy aimed at fostering the transition to a low-carbon economy, on European stock returns. Our empirical results suggest a statistically significant and negative short-run effect on the stock returns of firms that issued green bonds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08764v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Angelo Forino, Andrea Mercatanti, Giacomo Morelli</dc:creator>
    </item>
    <item>
      <title>Nonparametric Treatment Effect Identification in School Choice</title>
      <link>https://arxiv.org/abs/2112.03872</link>
      <description>arXiv:2112.03872v4 Announce Type: replace 
Abstract: This paper studies nonparametric identification and estimation of causal effects in centralized school assignment. In many centralized assignment algorithms, students are subjected to both lottery-driven variation and regression discontinuity (RD) driven variation. We characterize the full set of identified atomic treatment effects (aTEs), defined as the conditional average treatment effect between a pair of schools, given student characteristics. Atomic treatment effects are the building blocks of more aggregated notions of treatment contrasts, and common approaches to estimating aggregations of aTEs can mask important heterogeneity. In particular, many aggregations of aTEs put zero weight on aTEs driven by RD variation, and estimators of such aggregations put asymptotically vanishing weight on the RD-driven aTEs. We provide a diagnostic and recommend new aggregation schemes. Lastly, we provide estimators and accompanying asymptotic results for inference for those aggregations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.03872v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiafeng Chen</dc:creator>
    </item>
    <item>
      <title>Potential weights and implicit causal designs in linear regression</title>
      <link>https://arxiv.org/abs/2407.21119</link>
      <description>arXiv:2407.21119v3 Announce Type: replace 
Abstract: When we interpret linear regression as estimating causal effects justified by quasi-experimental treatment variation, what do we mean? This paper characterizes the necessary implications when linear regressions are interpreted causally. A minimal requirement for causal interpretation is that the regression estimates some contrast of individual potential outcomes under the true treatment assignment process. This requirement implies linear restrictions on the true distribution of treatment. Solving these linear restrictions leads to a set of implicit designs. Implicit designs are plausible candidates for the true design if the regression were to be causal. The implicit designs serve as a framework that unifies and extends existing theoretical results across starkly distinct settings (including multiple treatment, panel, and instrumental variables). They lead to new theoretical insights for widely used but less understood specifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21119v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiafeng Chen</dc:creator>
    </item>
    <item>
      <title>Reinterpreting demand estimation</title>
      <link>https://arxiv.org/abs/2503.23524</link>
      <description>arXiv:2503.23524v2 Announce Type: replace 
Abstract: This paper bridges the demand estimation and causal inference literatures by interpreting nonparametric structural assumptions as restrictions on counterfactual outcomes. It offers nontrivial and equivalent restatements of key demand estimation assumptions in the Neyman-Rubin potential outcomes model, for both settings with market-level data (Berry and Haile, 2014) and settings with demographic-specific market shares (Berry and Haile, 2024). The reformulation highlights a latent homogeneity assumption underlying structural demand models: The relationship between counterfactual outcomes is assumed to be identical across markets. This assumption is strong, but necessary for identification of market-level counterfactuals. Viewing structural demand models as misspecified but approximately correct reveals a tradeoff between specification flexibility and robustness to latent homogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23524v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiafeng Chen</dc:creator>
    </item>
    <item>
      <title>A step towards the integration of machine learning and classic model-based survey methods</title>
      <link>https://arxiv.org/abs/2402.07521</link>
      <description>arXiv:2402.07521v2 Announce Type: replace-cross 
Abstract: The usage of machine learning methods in traditional surveys including official statistics, is still very limited. Therefore, we propose a predictor supported by these algorithms, which can be used to predict any population or subpopulation characteristics. Machine learning methods have already been shown to be very powerful in identifying and modelling complex and nonlinear relationships between the variables, which means they have very good properties in case of strong departures from the classic assumptions. Therefore, we analyse the performance of our proposal under a different set-up, which, in our opinion, is of greater importance in real-life surveys. We study only small departures from the assumed model to show that our proposal is a good alternative, even in comparison with optimal methods under the model. Moreover, we propose the method of the ex ante accuracy estimation of machine learning predictors, giving the possibility of the accuracy comparison with classic methods. The solution to this problem is indicated in the literature as one of the key issues in integrating these approaches. The simulation studies are based on a real, longitudinal dataset, where the prediction of subpopulation characteristics is considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07521v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s13042-025-02718-6</arxiv:DOI>
      <dc:creator>Tomasz \.Z\k{a}d{\l}o, Adam Chwila</dc:creator>
    </item>
    <item>
      <title>Galerkin-ARIMA: A Two-Stage Polynomial Regression Framework for Fast Rolling One-Step-Ahead Forecasting</title>
      <link>https://arxiv.org/abs/2507.07469</link>
      <description>arXiv:2507.07469v2 Announce Type: replace-cross 
Abstract: We introduce Galerkin-ARIMA, a novel time-series forecasting framework that integrates Galerkin projection techniques with the classical ARIMA model to capture potentially nonlinear dependencies in lagged observations. By replacing the fixed linear autoregressive component with a spline-based basis expansion, Galerkin-ARIMA flexibly approximates the underlying relationship among past values via ordinary least squares, while retaining the moving-average structure and Gaussian innovation assumptions of ARIMA. We derive closed-form solutions for both the AR and MA components using two-stage Galerkin projections, establish conditions for asymptotic unbiasedness and consistency, and analyze the bias-variance trade-off under basis-size growth. Complexity analysis reveals that, for moderate basis dimensions, our approach can substantially reduce computational cost compared to maximum-likelihood ARIMA estimation. Through extensive simulations on four synthetic processes-including noisy ARMA, seasonal, trend-AR, and nonlinear recursion series-we demonstrate that Galerkin-ARIMA matches or closely approximates ARIMA's forecasting accuracy while achieving orders-of-magnitude speedups in rolling forecasting tasks. These results suggest that Galerkin-ARIMA offers a powerful, efficient alternative for modeling complex time series dynamics in high-volume or real-time applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07469v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haojie Liu, Zihan Lin</dc:creator>
    </item>
  </channel>
</rss>
