<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 12 Aug 2025 02:44:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On the Choice of Subspace for the Quasi-minimal Residual Method for Linear Inverse Problems</title>
      <link>https://arxiv.org/abs/2508.05793</link>
      <description>arXiv:2508.05793v1 Announce Type: new 
Abstract: Inverse problems arise in various scientific and engineering applications, necessitating robust numerical methods for their solution. In this work, we consider the effectiveness of Krylov subspace iterative methods, including GMRES, QMR, and their range restricted variants for solving linear discrete ill-posed problems. We analyze the impact of subspace selection on solution quality. Our findings indicate that range restricted QMR can outperform standard QMR, and confirm the previously observed behavior that range restricted GMRES can be superior to conventional GMRES in terms of approximation efficacy. Notably, range restricted QMR demonstrates a key advantage over GMRES with respect to range restricted QMR's singular spectrum which can make the method less sensitive to errors that are naturally present making it particularly effective when the noise level in the problem is uncertain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05793v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moshen Hu, Lucas Onisk</dc:creator>
    </item>
    <item>
      <title>A Minimal Perturbation Approach For The Rectangular Multiparameter Eigenvalue Problem</title>
      <link>https://arxiv.org/abs/2508.05948</link>
      <description>arXiv:2508.05948v1 Announce Type: new 
Abstract: The rectangular multiparameter eigenvalue problem (RMEP) involves rectangular coefficient matrices (usually with more rows than columns) and may potentially have no solution in its original form. A minimal perturbation framework is proposed to defines approximate solutions. Computationally, two particular scenarios are considered: computing one approximate eigen-tuple or a complete set of approximate eigen-tuples. For computing one approximate eigen-tuple, an alternating iterative scheme with proven convergence is devised, while for a complete set of approximate eigen-tuples, the framework leads to a standard MEP (RMEP with square coefficient matrices) for numerical solutions. The proposed approach is validated on RMEPs from discretizing the multiparameter Sturm-Liouville equation and the Helmholtz equations by the least-squares spectral method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05948v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shanheng Han, Lei-Hong Zhang, Ren-Cang Li</dc:creator>
    </item>
    <item>
      <title>Hierarchical Tucker Low-Rank Matrices: Construction and Matrix-Vector Multiplication</title>
      <link>https://arxiv.org/abs/2508.05958</link>
      <description>arXiv:2508.05958v1 Announce Type: new 
Abstract: In this paper, a hierarchical Tucker low-rank (HTLR) matrix is proposed to approximate non-oscillatory kernel functions in linear complexity. The HTLR matrix is based on the hierarchical matrix, with the low-rank blocks replaced by Tucker low-rank blocks. Using high-dimensional interpolation as well as tensor contractions, algorithms for the construction and matrix-vector multiplication of HTLR matrices are proposed admitting linear and quasi-linear complexities respectively. Numerical experiments demonstrate that the HTLR matrix performs well in both memory and runtime. Furthermore, the HTLR matrix can also be applied on quasi-uniform grids in addition to uniform grids, enhancing its versatility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05958v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yingzhou Li, Jingyu Liu</dc:creator>
    </item>
    <item>
      <title>$k\ell$-refinement: An adaptive mesh refinement scheme for hiearchical hybrid grids</title>
      <link>https://arxiv.org/abs/2508.06049</link>
      <description>arXiv:2508.06049v1 Announce Type: new 
Abstract: This work introduces an adaptive mesh refinement technique for hierarchical hybrid grids with the goal to reach scalability and maintain excellent performance on massively parallel computer systems. On the block structured hierarchical hybrid grids, this is accomplished by using classical, unstructured refinement only on the coarsest level of the hierarchy, while keeping the number of structured refinement levels constant on the whole domain. This leads to a compromise where the excellent performance characteristics of hierarchical hybrid grids can be maintained at the price that the flexibility of generating locally refined meshes is constrained. Furthermore, mesh adaptivity often relies on a posteriori error estimators or error indicators that tend to become computationally expensive. Again with the goal of preserving scalability and performance, a method is proposed that leverages the grid hierarchy and the full multigrid scheme that generates a natural sequence of approximations on the nested hierarchy of grids. This permits to compute a cheap error estimator that is well-suited for large-scale parallel computing. We present the theoretical foundations for both global and local error estimates and present a rigorous analysis of their effectivity. The proposed method, including error estimator and the adaptive coarse grid refinement, is implemented in the finite element framework HyTeG. Extensive numerical experiments are conducted to validate the effectiveness, as well as performance and scalability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06049v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Mann, Ulrich R\"ude</dc:creator>
    </item>
    <item>
      <title>A Preliminary Study on the Dimensional Stability Classification of Polynomial Spline Spaces over T-meshes</title>
      <link>https://arxiv.org/abs/2508.06217</link>
      <description>arXiv:2508.06217v1 Announce Type: new 
Abstract: This paper introduces the concept of dimensional stability for spline spaces over T-meshes, providing the first mathematical definition and a preliminary classification framework. We define dimensional stability as an invariant within the structurally isomorphic class, contingent on the rank stability of the conformality matrix. Absolute stability is proposed via structurally similar maps to address topological and order structures. Through the $k$-partition decomposition of T-connected components and analysis of the CNDC, we establish a correspondence between conformality vector spaces and rank stability. For diagonalizable T-meshes, decomposition into independent one-dimensional T $l$-edges facilitates basis function construction, while arbitrary T-meshes are partitioned into one- and two-dimensional components. These findings lay the groundwork for understanding dimensional stability and developing spline space basis functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06217v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bingru Huang, Falai Chen</dc:creator>
    </item>
    <item>
      <title>Fully discrete error analysis of finite element discretizations of time-dependent Stokes equations in a stream-function formulation</title>
      <link>https://arxiv.org/abs/2508.06235</link>
      <description>arXiv:2508.06235v1 Announce Type: new 
Abstract: In this paper we establish best approximation type error estimates for the fully discrete Galerkin solutions of the time-dependent Stokes problem using the stream-function formulation. For the time discretization we use the discontinuous Galerkin method of arbitrary degree, whereas we present the space discretization in a general framework. This makes our result applicable for a wide variety of space discretization methods, provided some Galerkin orthogonality conditions are satisfied. As an example, conformal $C^1$ and $C^0$ interior penalty methods are covered by our analysis. The results do not require any additional regularity assumptions beyond the natural regularity given by the domain and data and can be used for optimal control problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06235v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dmitriy Leykekhman, Boris Vexler, Jakob Wagner</dc:creator>
    </item>
    <item>
      <title>Numerical Considerations in Weighted Model Counting</title>
      <link>https://arxiv.org/abs/2508.06264</link>
      <description>arXiv:2508.06264v1 Announce Type: new 
Abstract: Weighted model counting computes the sum of the rational-valued weights associated with the satisfying assignments for a Boolean formula, where the weight of an assignment is given by the product of the weights assigned to the positive and negated variables comprising the assignment. Weighted model counting finds applications across a variety of domains including probabilistic reasoning and quantitative risk assessment.
  Most weighted model counting programs operate by (explicitly or implicitly) converting the input formula into a form that enables arithmetic evaluation, using multiplication for conjunctions and addition for disjunctions. Performing this evaluation using floating-point arithmetic can yield inaccurate results, and it cannot quantify the level of precision achieved. Computing with rational arithmetic gives exact results, but it is costly in both time and space.
  This paper describes how to combine multiple numeric representations to efficiently compute weighted model counts that are guaranteed to achieve a user-specified precision. When all weights are nonnegative, we prove that the precision loss of arithmetic evaluation using floating-point arithmetic can be tightly bounded. We show that supplementing a standard IEEE double-precision representation with a separate 64-bit exponent, a format we call extended-range double (ERD), avoids the underflow and overflow issues commonly encountered in weighted model counting. For problems with mixed negative and positive weights, we show that a combination of interval floating-point arithmetic and rational arithmetic can achieve the twin goals of efficiency and guaranteed precision. For our evaluations, we have devised especially challenging formulas and weight assignments, demonstrating the robustness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06264v1</guid>
      <category>math.NA</category>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Randal E. Bryant</dc:creator>
    </item>
    <item>
      <title>A Fully Discrete Truly Multidimensional Active Flux Method For The Two-Dimensional Euler Equations</title>
      <link>https://arxiv.org/abs/2508.06273</link>
      <description>arXiv:2508.06273v1 Announce Type: new 
Abstract: The Active Flux method is a finite volume method for hyperbolic conservation laws that uses both cell averages and point values as degrees of freedom. Several versions of such methods are currently under development. We focus on third order accurate, fully discrete Active Flux methods with compact stencil in space and time. These methods require exact or approximate evolution operators for the update of the point value degrees of freedom which are provided by the method of bicharacteristics. Here we propose new limiting strategies that guarantee positivity of pressure and density and furthermore discuss the implementation of reflecting boundary conditions. Numerical results show that the method leads to accurate approximates on coarse grids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06273v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erik Chudzik, Christiane Helzel, Amelie Porfetye</dc:creator>
    </item>
    <item>
      <title>Parallelized computation of quasi-periodic solutions for finite element problems: A Fourier series expansion-based shooting method</title>
      <link>https://arxiv.org/abs/2508.06302</link>
      <description>arXiv:2508.06302v1 Announce Type: new 
Abstract: High-dimensional nonlinear mechanical systems admit quasi-periodic solutions that are essential for the understanding of the dynamical systems. These quasi-periodic solutions stay on some invariant tori governed by complex PDEs in hyper-time. Here, we propose a Fourier series expansion-based shooting method (FSE-Shooting) for the parallelized computation of quasi-periodic solution with $d$ base frequencies ($d \ge 2$). We represent the associated $d$-torus as a collection of trajectories initialized at a ($d-1$)-torus. We drive a set of ODEs that hold for any of these trajectories. We also derive a set of boundary conditions that couple the initial and terminal states of these trajectories and then formulate a set of nonlinear algebraic equations via the coupling conditions. We use Fourier series expansion to parameterize the ($d-1$)-torus and shooting method to iterate the Fourier coefficients associated with initial torus such that the coupling conditions are satisfied. In particular, the terminal points of these trajectories are parallelized computed via Newmark integration, where the time points and Fourier coefficients are transformed to each other by alternating Frequency-Time method. A straightforward phase condition is devised to track the quasi-periodic solutions with priori unknown base frequencies. Additionally, the by-product of the FSE-Shooting can be also directly used to compute the Lyapunov exponents to assess the stabilities of quasi-periodic solutions. The results of three finite element systems show the efficiency and versatility of FSE-Shooting in high-dimensional nonlinear dynamical systems, including a three-dimensional shell structure with $1872$ DOFs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06302v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junqing Wu, Ling Hong, Mingwu Li, Jun Jiang</dc:creator>
    </item>
    <item>
      <title>A Tensor Train Approach for Deterministic Arithmetic Operations on Discrete Representations of Probability Distributions</title>
      <link>https://arxiv.org/abs/2508.06303</link>
      <description>arXiv:2508.06303v1 Announce Type: new 
Abstract: Computing with discrete representations of high-dimensional probability distributions is fundamental to uncertainty quantification, Bayesian inference, and stochastic modeling. However, storing and manipulating such distributions suffers from the curse of dimensionality, as memory and computational costs grow exponentially with dimension. Monte Carlo methods require thousands to billions of samples, incurring high computational costs and producing inconsistent results due to stochasticity. We present an efficient tensor train method for performing exact arithmetic operations on discretizations of continuous probability distributions while avoiding exponential growth. Our approach leverages low-rank tensor train decomposition to represent latent random variables compactly using Dirac deltas, enabling deterministic addition, subtraction and multiplication operations directly in the compressed format. We develop an efficient implementation using sparse matrices and specialized data structures that further enhances performance. Theoretical analysis demonstrates polynomial scaling of memory and computational complexity under rank assumptions, and shows how statistics of latent variables can be computed with polynomial complexity. Numerical experiments spanning randomized linear algebra to stochastic differential equations demonstrate orders-of-magnitude improvements in memory usage and computational time compared to conventional approaches, enabling tractable deterministic computations on discretized random variables in previously intractable dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06303v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gerhard Kirsten, Bilgesu Bilgin, Janith Petangoda, Phillip Stanley-Marbell</dc:creator>
    </item>
    <item>
      <title>Higher Order Regularization using Harmonic Eigenfunctions for Model-Based Reconstruction in Magnetic Particle Imaging</title>
      <link>https://arxiv.org/abs/2508.06306</link>
      <description>arXiv:2508.06306v1 Announce Type: new 
Abstract: Magnetic Particle Imaging (MPI) is a recent imaging modality where superparamagnetic nanoparticles are employed as tracers. The reconstruction task is to obtain the spatial particle distribution from a voltage signal induced by the particles. Generally, in computational imaging variational reconstruction techniques are common and rely on a mathematical model to describe the underlying physics. For the MPI reconstruction task we propose a model-based variational reconstruction technique which incorporates a higher order regularizer, where the regularizer is diagonalized by harmonic eigenfunctions. The proposed image reconstruction algorithm features two major stages: in the first stage, the core stage, the components of the MPI core response are reconstructed. This is the MPI-specific data approximation task which we formulate as a variational problem incorporating the higher order regularizer. The relationship between the particle distribution, the MPI core response and the measured data is given by a mathematical model which was introduced in our earlier research. According to this model the MPI core response is tied to the particle distribution by convolution. Therefore the outcome of the core stage yields the data for the second stage, the deconvolution stage, in which the final reconstructed image is produced by solving an ill-posed deconvolution problem in a robust way relying on earlier research. Interestingly, the quality of the final image depends significantly on the quality of the result of the core stage. A contribution is thus the enhancement of the core stage via higher order regularization. We provide a theoretical foundation for our approach and demonstrate its benefit with numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06306v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Thomas M\"arz, Vladyslav Gapyak, Andreas Weinmann</dc:creator>
    </item>
    <item>
      <title>Rational minimax approximation of matrix-valued functions</title>
      <link>https://arxiv.org/abs/2508.06378</link>
      <description>arXiv:2508.06378v1 Announce Type: new 
Abstract: In this paper, we present a rigorous framework for rational minimax approximation of matrix-valued functions that generalizes classical scalar approximation theory. Given sampled data $\{(x_\ell, {F}(x_\ell))\}_{\ell=1}^m$ where ${F}:\mathbb{C} \to \mathbb{C}^{s \times t}$ is a matrix-valued function, we study the problem of finding a matrix-valued rational approximant ${R}(x) = {P}(x)/q(x)$ (with ${P}:\mathbb{C} \to \mathbb{C}^{s \times t}$ a matrix-valued polynomial and $q(x)$ a nonzero scalar polynomial of prescribed degrees) that minimizes the worst-case Frobenius norm error over the given nodes: $$ \inf_{{R}(x) = {P}(x)/q(x)} \max_{1 \leq \ell \leq m} \|{F}(x_\ell) - {R}(x_\ell)\|_{\rm F}. $$ By reformulating this min-max optimization problem through Lagrangian duality, we derive a maximization dual problem over the probability simplex. We analyze weak and strong duality properties and establish a sufficient condition ensuring that the solution of the dual problem yields the minimax approximant $R(x)$. For numerical implementation, we propose an efficient method (\textsf{m-d-Lawson}) to solve the dual problem, generalizing Lawson's iteration to matrix-valued functions. Numerical experiments are conducted and compared to state-of-the-art approaches, demonstrating its efficiency as a novel computational framework for matrix-valued rational approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06378v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lei-Hong Zhang, Ya-Nan Zhang, Chenkun Zhang, Shanheng Han</dc:creator>
    </item>
    <item>
      <title>Heterogeneous optimized Schwarz Methods for heat conduction in composites with thermal contact resistance</title>
      <link>https://arxiv.org/abs/2508.06408</link>
      <description>arXiv:2508.06408v1 Announce Type: new 
Abstract: Heat transfer in composites is critical in engineering, where imperfect layer contact causes thermal contact resistance (TCR), leading to interfacial temperature discontinuity. We propose solving this numerically using the optimized Schwarz method (OSM), which decouples the heterogeneous problem into homogeneous subproblems. This avoids ill-conditioned systems from monolithic solving due to high contrast and interface jumps. Both energy estimate and Fourier analysis are used to prove the convergence of this algorithm when the standard Robin condition is applied to transmit information between subdomains. To achieve fast convergence, instead of the standard Robin, the scaled Robin transmission condition is proposed, and the involved free parameter is rigorously optimized. The results reveal several new findings due to the presence of TCR: first, the larger the TCR, the faster the OSM converges; second, mesh-independent convergence is achieved in the asymptotic sense, in contrast to the mesh-dependent results without TCR; and last, the heterogeneity contrast benefits the convergence, with a larger contrast leading to faster convergence. Interestingly, different from the case without TCR, the thermal conductivity also benefits the convergence, similar to the effect of heterogeneity. Numerical experiments confirm the theoretical findings and demonstrate the method's potential for nonlinear problems on irregular domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06408v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huan Zhang, Hui Zhang, Yan Wang, Yingxiang Xu</dc:creator>
    </item>
    <item>
      <title>Weak approximation of stochastic differential equations with sticky boundary conditions</title>
      <link>https://arxiv.org/abs/2508.06487</link>
      <description>arXiv:2508.06487v1 Announce Type: new 
Abstract: Sticky diffusion models a Markovian particle experiencing reflection and temporary adhesion phenomena at the boundary. Numerous numerical schemes exist for approximating stopped or reflected stochastic differential equations (SDEs), but this is not the case for sticky SDEs. In this paper, we construct and analyze half-order and first-order numerical schemes for the weak approximation of stochastic differential equations with sticky boundary conditions. We present the algorithms in general setting such that they can be used to solve general linear parabolic partial differential equations with second-order sticky boundary condition via the probabilistic representations of their solutions. Since the sticky diffusion spends non-zero amount of time on boundary, it poses extra challenge in designing the schemes and obtaining their order of convergence. We support the theoretical results with numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06487v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akash Sharma</dc:creator>
    </item>
    <item>
      <title>Stochastic Trace Optimization of Parameter Dependent Matrices Based on Statistical Learning Theory</title>
      <link>https://arxiv.org/abs/2508.05764</link>
      <description>arXiv:2508.05764v1 Announce Type: cross 
Abstract: We consider matrices $\boldsymbol{A}(\boldsymbol\theta)\in\mathbb{R}^{m\times m}$ that depend, possibly nonlinearly, on a parameter $\boldsymbol\theta$ from a compact parameter space $\Theta$. We present a Monte Carlo estimator for minimizing $\text{trace}(\boldsymbol{A}(\boldsymbol\theta))$ over all $\boldsymbol\theta\in\Theta$, and determine the sampling amount so that the backward error of the estimator is bounded with high probability. We derive two types of bounds, based on epsilon nets and on generic chaining. Both types predict a small sampling amount for matrices $\boldsymbol{A}(\boldsymbol\theta)$ with small offdiagonal mass, and parameter spaces $\Theta$ of small ``size.'' Dependence on the matrix dimension~$m$ is only weak or not explicit. The bounds based on epsilon nets are easier to evaluate and come with fully specified constants. In contrast, the bounds based on chaining depend on the Talagrand functionals which are difficult to evaluate, except in very special cases. Comparisons between the two types of bounds are difficult, although the literature suggests that chaining bounds can be superior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05764v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arvind K. Saibaba, Ilse C. F. Ipsen</dc:creator>
    </item>
    <item>
      <title>Machine Learning-Based Nonlinear Nudging for Chaotic Dynamical Systems</title>
      <link>https://arxiv.org/abs/2508.05778</link>
      <description>arXiv:2508.05778v1 Announce Type: cross 
Abstract: Nudging is an empirical data assimilation technique that incorporates an observation-driven control term into the model dynamics. The trajectory of the nudged system approaches the true system trajectory over time, even when the initial conditions differ. For linear state space models, such control terms can be derived under mild assumptions. However, designing effective nudging terms becomes significantly more challenging in the nonlinear setting. In this work, we propose neural network nudging, a data-driven method for learning nudging terms in nonlinear state space models. We establish a theoretical existence result based on the Kazantzis--Kravaris--Luenberger observer theory. The proposed approach is evaluated on three benchmark problems that exhibit chaotic behavior: the Lorenz 96 model, the Kuramoto--Sivashinsky equation, and the Kolmogorov flow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05778v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaemin Oh, Jinsil Lee, Youngjoon Hong</dc:creator>
    </item>
    <item>
      <title>Optimal Linear Baseline Models for Scientific Machine Learning</title>
      <link>https://arxiv.org/abs/2508.05831</link>
      <description>arXiv:2508.05831v1 Announce Type: cross 
Abstract: Across scientific domains, a fundamental challenge is to characterize and compute the mappings from underlying physical processes to observed signals and measurements. While nonlinear neural networks have achieved considerable success, they remain theoretically opaque, which hinders adoption in contexts where interpretability is paramount. In contrast, linear neural networks serve as a simple yet effective foundation for gaining insight into these complex relationships. In this work, we develop a unified theoretical framework for analyzing linear encoder-decoder architectures through the lens of Bayes risk minimization for solving data-driven scientific machine learning problems. We derive closed-form, rank-constrained linear and affine linear optimal mappings for forward modeling and inverse recovery tasks. Our results generalize existing formulations by accommodating rank-deficiencies in data, forward operators, and measurement processes. We validate our theoretical results by conducting numerical experiments on datasets from simple biomedical imaging, financial factor analysis, and simulations involving nonlinear fluid dynamics via the shallow water equations. This work provides a robust baseline for understanding and benchmarking learned neural network models for scientific machine learning problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05831v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander DeLise, Kyle Loh, Krish Patel, Meredith Teague, Andrea Arnold, Matthias Chung</dc:creator>
    </item>
    <item>
      <title>Debiasing Polynomial and Fourier Regression</title>
      <link>https://arxiv.org/abs/2508.05920</link>
      <description>arXiv:2508.05920v1 Announce Type: cross 
Abstract: We study the problem of approximating an unknown function $f:\mathbb{R}\to\mathbb{R}$ by a degree-$d$ polynomial using as few function evaluations as possible, where error is measured with respect to a probability distribution $\mu$. Existing randomized algorithms achieve near-optimal sample complexities to recover a $ (1+\varepsilon) $-optimal polynomial but produce biased estimates of the best polynomial approximation, which is undesirable.
  We propose a simple debiasing method based on a connection between polynomial regression and random matrix theory. Our method involves evaluating $f(\lambda_1),\ldots,f(\lambda_{d+1})$ where $\lambda_1,\ldots,\lambda_{d+1}$ are the eigenvalues of a suitably designed random complex matrix tailored to the distribution $\mu$. Our estimator is unbiased, has near-optimal sample complexity, and experimentally outperforms iid leverage score sampling.
  Additionally, our techniques enable us to debias existing methods for approximating a periodic function with a truncated Fourier series with near-optimal sample complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05920v1</guid>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chris Cama\~no, Raphael A. Meyer, Kevin Shu</dc:creator>
    </item>
    <item>
      <title>Kahan's Automatic Step-Size Control for Unconstrained Optimization</title>
      <link>https://arxiv.org/abs/2508.06002</link>
      <description>arXiv:2508.06002v1 Announce Type: cross 
Abstract: The Barzilai and Borwein (BB) gradient method is one of the most widely-used line-search gradient methods. It computes the step-size for the current iterate by using the information carried in the previous iteration. Recently, William Kahan [Kahan, Automatic Step-Size Control for Minimization Iterations, Technical report, University of California, Berkeley CA, USA, 2019] proposed new Gradient Descent (KGD) step-size strategies which iterate the step-size itself by effectively utilizing the information in the previous iteration. In the quadratic model, such a new step-size is shown to be mathematically equivalent to the long BB step, but no rigorous mathematical proof of its efficiency and effectiveness for the general unconstrained minimization is available. In this paper, by this equivalence with the long BB step, we first derive a short version of KGD step-size and show that, for the strongly convex quadratic model with a Hessian matrix $H$, both the long and short KGD step-size (and hence BB step-sizes) gradient methods converge at least R-linearly with a rate $1-\frac{1}{{\rm cond}(H)}$. For the general unconstrained minimization, we further propose an adaptive framework to effectively use the KGD step-sizes; global convergence and local R-linear convergence rate are proved. Numerical experiments are conducted on the CUTEst collection as well as the practical logistic regression problems, and we compare the performance of the proposed methods with various BB step-size approaches and other recently proposed adaptive gradient methods to demonstrate the efficiency and robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06002v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifeng Meng, Chungen Shen, Linuo Xue, Lei-Hong Zhang</dc:creator>
    </item>
    <item>
      <title>Transfinite Iteration of Operator Transforms and Spectral Projections in Hilbert and Banach Spaces</title>
      <link>https://arxiv.org/abs/2508.06025</link>
      <description>arXiv:2508.06025v1 Announce Type: cross 
Abstract: We study ordinal-indexed, multi-layer iterations of bounded operator transforms and prove convergence to spectral/ergodic projections under functional-calculus hypotheses. For normal operators on Hilbert space and polynomial or holomorphic layers that are contractive on the spectrum and fix the peripheral spectrum only at fixed points, the iterates converge in the strong operator topology by a countable stage to the spectral projection onto the joint peripheral fixed set. We describe spectral mapping at finite stages and identify the spectrum of the limit via the essential range. In reflexive Banach spaces, for Ritt or sectorial operators with a bounded H-infinity functional calculus, the composite layer is power-bounded and its mean-ergodic projection yields an idempotent commuting with the original operator; under a peripheral-separation condition the powers converge strongly to this projection. We provide explicit two-layer Schur filters, a concise Schur/Nevanlinna-Pick lemma, a Fejer-type monotonicity bound implying stabilization by the first countable limit (omega), examples that attain exactly the omega stage, and counterexamples outside the hypotheses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06025v1</guid>
      <category>math.FA</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.SP</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Faruk Alpay, Taylan Alpay, Hamdi Alakkad</dc:creator>
    </item>
    <item>
      <title>The Beauty of Anisotropic Mesh Refinement: Omnitrees for Efficient Dyadic Discretizations</title>
      <link>https://arxiv.org/abs/2508.06316</link>
      <description>arXiv:2508.06316v1 Announce Type: cross 
Abstract: Structured adaptive mesh refinement (AMR), commonly implemented via quadtrees and octrees, underpins a wide range of applications including databases, computer graphics, physics simulations, and machine learning. However, octrees enforce isotropic refinement in regions of interest, which can be especially inefficient for problems that are intrinsically anisotropic--much resolution is spent where little information is gained. This paper presents omnitrees as an anisotropic generalization of octrees and related data structures. Omnitrees allow to refine only the locally most important dimensions, providing tree structures that are less deep than bintrees and less wide than octrees. As a result, the convergence of the AMR schemes can be increased by up to a factor of the dimensionality d for very anisotropic problems, quickly offsetting their modest increase in storage overhead. We validate this finding on the problem of binary shape representation across 4,166 three-dimensional objects: Omnitrees increase the mean convergence rate by 1.5x, require less storage to achieve equivalent error bounds, and maximize the information density of the stored function faster than octrees. These advantages are projected to be even stronger for higher-dimensional problems. We provide a first validation by introducing a time-dependent rotation to create four-dimensional representations, and discuss the properties of their 4-d octree and omnitree approximations. Overall, omnitree discretizations can make existing AMR approaches more efficient, and open up new possibilities for high-dimensional applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06316v1</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <category>cs.GR</category>
      <category>cs.IT</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Theresa Pollinger, Masado Ishii, Jens Domke</dc:creator>
    </item>
    <item>
      <title>Does block size matter in randomized block Krylov low-rank approximation?</title>
      <link>https://arxiv.org/abs/2508.06486</link>
      <description>arXiv:2508.06486v1 Announce Type: cross 
Abstract: We study the problem of computing a rank-$k$ approximation of a matrix using randomized block Krylov iteration. Prior work has shown that, for block size $b = 1$ or $b = k$, a $(1 + \varepsilon)$-factor approximation to the best rank-$k$ approximation can be obtained after $\tilde O(k/\sqrt{\varepsilon})$ matrix-vector products with the target matrix. On the other hand, when $b$ is between $1$ and $k$, the best known bound on the number of matrix-vector products scales with $b(k-b)$, which could be as large as $O(k^2)$. Nevertheless, in practice, the performance of block Krylov methods is often optimized by choosing a block size $1 \ll b \ll k$. We resolve this theory-practice gap by proving that randomized block Krylov iteration produces a $(1 + \varepsilon)$-factor approximate rank-$k$ approximation using $\tilde O(k/\sqrt{\varepsilon})$ matrix-vector products for any block size $1\le b\le k$. Our analysis relies on new bounds for the minimum singular value of a random block Krylov matrix, which may be of independent interest. Similar bounds are central to recent breakthroughs on faster algorithms for sparse linear systems [Peng &amp; Vempala, SODA 2021; Nie, STOC 2022].</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06486v1</guid>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tyler Chen, Ethan N. Epperly, Raphael A. Meyer, Christopher Musco, Akash Rao</dc:creator>
    </item>
    <item>
      <title>Estimates of the numerical density for stochastic differential equations with multiplicative noise</title>
      <link>https://arxiv.org/abs/2409.04991</link>
      <description>arXiv:2409.04991v4 Announce Type: replace 
Abstract: We investigate the estimates of the density for the traditional Euler-Maruyama discretization of stochastic differential equations (SDEs) with multiplicative noise. Our estimates focus on two key aspects: (1) the $L^p$-upper bounds for derivatives of the logarithmic numerical density, (2) the sharp error order of the Euler scheme under the relative entropy (or Kullback-Leibler divergence). For the first aspect, we present estimates for the first-order and second-order derivatives of the logarithmic numerical density. The key technique is to adopt the Malliavin calculus to derive expressions of the derivatives of the logarithmic Green's function and to obtain an estimate for the inverse Malliavin matrix. Moreover, for the relative entropy error, we obtain a bound that is second order in time step, which then naturally leads to first-order error bounds under the total variation distance and Wasserstein distances. Compared with the usual weak error estimate for SDEs, such estimate can give an error bound for the worst case of a family of test functions instead of one test function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04991v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lei Li, Mengchao Wang, Yuliang Wang</dc:creator>
    </item>
    <item>
      <title>An Extension of the Euler-Maclaurin Summation Formula to Nearly Singular Functions</title>
      <link>https://arxiv.org/abs/2409.19192</link>
      <description>arXiv:2409.19192v2 Announce Type: replace 
Abstract: A extension of the Euler-Maclaurin (E-M) formula to near-singular functions is presented. This extension is derived based on earlier generalized E-M formulas for singular functions. The new E-M formulas consists of two components: a ``singular'' component that is a continuous extension of the earlier singular E-M formulas, and a ``jump'' component associated with the discontinuity of the integral with respect to a parameter that controls near singularity. The singular component of the new E-M formulas is an asymptotic series whose coefficients depend on the Hurwitz zeta function or the digamma function. Numerical examples of near-singular quadrature based on the extended E-M formula are presented, where accuracies of machine precision are achieved insensitive to the strength of the near singularity and with a very small number of quadrature nodes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19192v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bowei Wu</dc:creator>
    </item>
    <item>
      <title>Improving sampling by modifying the effective diffusion</title>
      <link>https://arxiv.org/abs/2410.00525</link>
      <description>arXiv:2410.00525v4 Announce Type: replace 
Abstract: Markov chain Monte Carlo samplers based on discretizations of (overdamped) Langevin dynamics are commonly used in the Bayesian inference and computational statistical physics literature to estimate high-dimensional integrals. One can introduce a non-constant diffusion matrix to precondition these dynamics, and recent works have optimized it in order to improve the rate of convergence to stationarity by overcoming entropic and energy barriers. However, the introduced methodologies to compute these optimal diffusions are generally not suited to high-dimensional settings, as they rely on costly optimization procedures. In this work, we propose to optimize over a class of diffusion matrices, based on one-dimensional collective variables (CVs), to help the dynamics explore the latent space defined by the CV. The form of the diffusion matrix is chosen in order to obtain an efficient effective diffusion in the latent space. We describe how this class of diffusion matrices can be constructed and learned during the simulation. We provide implementations of the Metropolis--Adjusted Langevin Algorithm and Riemann Manifold (Generalized) Hamiltonian Monte Carlo algorithms, and discuss numerical optimizations in the case when the CV depends only on a few degrees of freedom of the system. We illustrate the efficiency gains by computing mean transition durations between two metastable states of a dimer in a solvent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00525v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tony Leli\`evre, R\'egis Santet, Gabriel Stoltz</dc:creator>
    </item>
    <item>
      <title>Monolithic Multi-level Overlapping Schwarz Solvers for Fluid Problems</title>
      <link>https://arxiv.org/abs/2508.04356</link>
      <description>arXiv:2508.04356v2 Announce Type: replace 
Abstract: Additive overlapping Schwarz Methods are iterative methods of the domain decomposition type for the solution of partial differential equations. Numerical and parallel scalability of these methods can be achieved by adding coarse levels. A successful coarse space, inspired by iterative substructuring, is the generalized Dryja-Smith-Widlund (GDSW) space. In https://doi.org/10.1137/18M1184047, based on the GDSW approach, two-level monolithic overlapping Schwarz preconditioners for saddle point problems were introduced. We present parallel results up to 32768 MPI ranks for the solution of incompressible fluid problems for a Poiseuille flow example on the unit cube and a complex extrusion die geometry using a two- and a three-level monolithic overlapping Schwarz preconditioner. These results are achieved through the combination of the additive overlapping Schwarz solvers implemented in the Fast and Robust Overlapping Schwarz (FROSch) library https://doi.org/10.1007/978-3-030-56750-7_19, which is part of the Trilinos package ShyLU https://doi.org/10.1109/IPDPS.2012.64, and the FEATFLOW library http://www.featflow.de using a scalable interface for the efficient coupling of the two libraries. This work is part of the project StroemungsRaum - Novel Exascale-Architectures with Heterogeneous Hardware Components for Computational Fluid Dynamics Simulations, funded by the German Bundesministerium fur Forschung, Technologie und Raumfahrt BMFTR (formerly BMBF) as part of the program on New Methods and Technologies for Exascale Computing (SCALEXA).</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04356v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephan K\"ohler, Oliver Rheinbach</dc:creator>
    </item>
    <item>
      <title>Optimal sampling for least-squares approximation</title>
      <link>https://arxiv.org/abs/2409.02342</link>
      <description>arXiv:2409.02342v2 Announce Type: replace-cross 
Abstract: Least-squares approximation is one of the most important methods for recovering an unknown function from data. While in many applications the data is fixed, in many others there is substantial freedom to choose where to sample. In this paper, we review recent progress on near-optimal random sampling strategies for (weighted) least-squares approximation in arbitrary linear spaces. We introduce the Christoffel function as a key quantity in the analysis of (weighted) least-squares approximation from random samples, then show how it can be used to construct a random sampling strategy, termed Christoffel sampling, that possesses near-optimal sample complexity: namely, the number of samples scales log-linearly in the dimension of the approximation space $n$. We discuss a series of variations, extensions and further topics, and throughout highlight connections to approximation theory, machine learning, information-based complexity and numerical linear algebra. Finally, motivated by various contemporary applications, we consider a generalization of the classical setting where the samples need not be pointwise samples of a scalar-valued function, and the approximation space need not be linear. We show that, even in this significantly more general setting, suitable generalizations of Christoffel function still determine the sample complexity. Consequently, these can be used to design enhanced, Christoffel sampling strategies in a unified way for general recovery problems. This article is largely self-contained, and intended to be accessible to nonspecialists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02342v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ben Adcock</dc:creator>
    </item>
    <item>
      <title>Adaptive Collocation Point Strategies For Physics Informed Neural Networks via the QR Discrete Empirical Interpolation Method</title>
      <link>https://arxiv.org/abs/2501.07700</link>
      <description>arXiv:2501.07700v4 Announce Type: replace-cross 
Abstract: Physics-informed neural networks (PINNs) have gained significant attention for solving forward and inverse problems related to partial differential equations (PDEs). While advancements in loss functions and network architectures have improved PINN accuracy, the impact of collocation point sampling on their performance remains underexplored. Fixed sampling methods, such as uniform random sampling and equispaced grids, can fail to capture critical regions with high solution gradients, limiting their effectiveness for complex PDEs. Adaptive methods, inspired by adaptive mesh refinement from traditional numerical methods, address this by dynamically updating collocation points during training but may overlook residual dynamics between updates, potentially losing valuable information. To overcome this limitation, we propose two adaptive collocation point selection strategies utilizing the QR Discrete Empirical Interpolation Method (QR-DEIM), a reduced-order modeling technique for efficiently approximating nonlinear functions. Our results on benchmark PDEs demonstrate that our QR-DEIM-based approaches improve PINN accuracy compared to existing methods, offering a promising direction for adaptive collocation point strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07700v4</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrian Celaya, David Fuentes, Beatrice Riviere</dc:creator>
    </item>
  </channel>
</rss>
