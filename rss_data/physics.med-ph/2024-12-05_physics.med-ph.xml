<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.med-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.med-ph</link>
    <description>physics.med-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.med-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Dec 2024 05:00:58 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Precision in the Face of Noise -- Lessons from Kahneman, Siboney, and Sunstein for Radiation Oncology</title>
      <link>https://arxiv.org/abs/2412.02724</link>
      <description>arXiv:2412.02724v1 Announce Type: new 
Abstract: In this manuscript, we draw on the insights from Kahneman, Sibony, and Sunsteins influential nonfiction book Noise: A Flaw in Human Judgment to explore the concept of unwanted variability in judgment (i.e., noise). We introduce key terms and connect these insights to the field of radiation oncology by illustrating how noise contributes to errors in clinically relevant areas such as contouring. Additionally, we propose practical strategies to reduce noise in radiation oncology, such as through judgment aggregation and the use of artificial intelligence tools, building on the principles outlined in the book.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02724v1</guid>
      <category>physics.med-ph</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kareem A. Wahid, Clifton D. Fuller, David Fuentes</dc:creator>
    </item>
    <item>
      <title>Noninvasive In vivo Estimation of HbA1c Based on Beer Lambert Model from Photoplethysmogram Using Only Two Wavelengths</title>
      <link>https://arxiv.org/abs/2412.03053</link>
      <description>arXiv:2412.03053v1 Announce Type: new 
Abstract: Glycated hemoglobin (HbA1c) is the most important factor in diabetes control. Since HbA1c reflects the average blood glucose level over the preceding three months, it is unaffected by the patient's activity level or diet before the test. Noninvasive HbA1c measurement reduces both the pain and complications associated with fingertip piercing to collect blood. Photoplethysmography is helpful for measuring HbA1c without blood samples. Herein, only two wavelengths (615 and 525 nm) were used to estimate HbA1c noninvasively, where two different ratio calibrations were applied and performances were compared to a work that uses three wavelengths. For the fingertip type, the Pearson r values for HbA1c estimates are 0.896 and 0.905 considering ratio calibrations for blood vessel and whole finger models, respectively. Using another value (HbA1c) calibration in addition to ratio calibrations, we can improve this performance, such that the Pearson r values of HbA1c levels are 0.929 and 0.930 for blood vessel and whole finger models, respectively. In the previous study using three wavelengths, the Pearson r values were 0.916 and 0.959 for the blood-vessel and whole-finger models, respectively. Here, the RCF of SpO2 estimation is 0.986 when SpO2 ratio calibration is applied, while in the previous study, the RCF values of SpO2 estimation were 0.983 and 0.986 for the blood-vessel and whole finger models, respectively. Thus, we show that HbA1c estimation using only two wavelengths has comparable performance to previous studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03053v1</guid>
      <category>physics.med-ph</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/app13063626</arxiv:DOI>
      <arxiv:journal_reference>Applied Sciences 13, no. 6 (2023): 3626</arxiv:journal_reference>
      <dc:creator>Mrinmoy Sarker Turja, Tae Ho Kwon, Hyoungkeun Kim, Ki Doo Kim</dc:creator>
    </item>
    <item>
      <title>Assessing the performance of CT image denoisers using Laguerre-Gauss Channelized Hotelling Observer for lesion detection</title>
      <link>https://arxiv.org/abs/2412.02920</link>
      <description>arXiv:2412.02920v1 Announce Type: cross 
Abstract: The remarkable success of deep learning methods in solving computer vision problems, such as image classification, object detection, scene understanding, image segmentation, etc., has paved the way for their application in biomedical imaging. One such application is in the field of CT image denoising, whereby deep learning methods are proposed to recover denoised images from noisy images acquired at low radiation. Outputs derived from applying deep learning denoising algorithms may appear clean and visually pleasing; however, the underlying diagnostic image quality may not be on par with their normal-dose CT counterparts. In this work, we assessed the image quality of deep learning denoising algorithms by making use of visual perception- and data fidelity-based task-agnostic metrics (like the PSNR and the SSIM) - commonly used in the computer vision - and a task-based detectability assessment (the LCD) - extensively used in the CT imaging. When compared against normal-dose CT images, the deep learning denoisers outperformed low-dose CT based on metrics like the PSNR (by 2.4 to 3.8 dB) and SSIM (by 0.05 to 0.11). However, based on the LCD performance, the detectability using quarter-dose denoised outputs was inferior to that obtained using normal-dose CT scans.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02920v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>physics.med-ph</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/NSS/MIC/RTSD57108.2024.10658147</arxiv:DOI>
      <arxiv:journal_reference>2024 IEEE NSS MIC RTSD, Tampa, FL, USA, 2024, pp. 1-2</arxiv:journal_reference>
      <dc:creator>Prabhat Kc, Rongping Zeng</dc:creator>
    </item>
    <item>
      <title>Domain-Agnostic Stroke Lesion Segmentation Using Physics-Constrained Synthetic Data</title>
      <link>https://arxiv.org/abs/2412.03318</link>
      <description>arXiv:2412.03318v1 Announce Type: cross 
Abstract: Segmenting stroke lesions in Magnetic Resonance Imaging (MRI) is challenging due to diverse clinical imaging domains, with existing models struggling to generalise across different MRI acquisition parameters and sequences. In this work, we propose two novel physics-constrained approaches using synthetic quantitative MRI (qMRI) images to enhance the robustness and generalisability of segmentation models. We trained a qMRI estimation model to predict qMRI maps from MPRAGE images, which were used to simulate diverse MRI sequences for segmentation training. A second approach built upon prior work in synthetic data for stroke lesion segmentation, generating qMRI maps from a dataset of tissue labels. The proposed approaches improved over the baseline nnUNet on a variety of out-of-distribution datasets, with the second approach outperforming the prior synthetic data method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03318v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>physics.med-ph</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liam Chalcroft, Jenny Crinion, Cathy J. Price, John Ashburner</dc:creator>
    </item>
    <item>
      <title>Diffusion MRI with double diffusion encoding and variable mixing times disentangles water exchange from intrinsic kurtosis</title>
      <link>https://arxiv.org/abs/2306.03661</link>
      <description>arXiv:2306.03661v4 Announce Type: replace 
Abstract: Double diffusion encoding (DDE) makes diffusion MRI sensitive to a wide range of microstructural features, and the acquired data can be analysed using different approaches. Correlation tensor imaging (CTI) uses DDE to resolve three components of the diffusional kurtosis: isotropic, anisotropic, and microscopic. The microscopic kurtosis is estimated from the contrast between single diffusion encoding (SDE) and parallel DDE signals at the same b-value. Another approach is multi-Gaussian exchange (MGE), which employs DDE to measure exchange. Sensitivity to exchange is obtained by contrasting SDE and DDE signals at the same b-value. CTI and MGE exploit the same signal contrast to quantify microscopic kurtosis and exchange, and this study investigates the interplay between these two quantities. We perform Monte-Carlo simulations in different geometries with varying levels of exchange and study the behaviour of the parameters from CTI and MGE. We conclude that microscopic kurtosis from CTI is sensitive to the exchange rate and that intercompartmental exchange and the intrinsic kurtosis of individual compartments are distinct sources of microscopic kurtosis. In an attempt to disentangle these two sources, we propose a heuristic signal representation referred to as $\mu$MGE (MGE incorporating intrinsic kurtosis) that accounts for both effects, by exploiting the distinct signatures of exchange and intrinsic kurtosis with varying mixing time: exchange causes a slow dependence of the signal on mixing time while intrinsic kurtosis arguably has a much faster dependence. We find that applying $\mu$MGE to data acquired with multiple mixing times for both parallel and orthogonal DDE may allow estimation of the exchange rate as well as the isotropic, anisotropic, and intrinsic kurtosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.03661v4</guid>
      <category>physics.med-ph</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arthur Chakwizira, Filip Szczepankiewicz, Markus Nilsson</dc:creator>
    </item>
    <item>
      <title>Segmentation-Free Outcome Prediction from Head and Neck Cancer PET/CT Images: Deep Learning-Based Feature Extraction from Multi-Angle Maximum Intensity Projections (MA-MIPs)</title>
      <link>https://arxiv.org/abs/2405.01756</link>
      <description>arXiv:2405.01756v3 Announce Type: replace 
Abstract: We introduce an innovative, simple, effective segmentation-free approach for outcome prediction in head \&amp; neck cancer (HNC) patients. By harnessing deep learning-based feature extraction techniques and multi-angle maximum intensity projections (MA-MIPs) applied to Fluorodeoxyglucose Positron Emission Tomography (FDG-PET) volumes, our proposed method eliminates the need for manual segmentations of regions-of-interest (ROIs) such as primary tumors and involved lymph nodes. Instead, a state-of-the-art object detection model is trained to perform automatic cropping of the head and neck region on the PET volumes. A pre-trained deep convolutional neural network backbone is then utilized to extract deep features from MA-MIPs obtained from 72 multi-angel axial rotations of the cropped PET volumes. These deep features extracted from multiple projection views of the PET volumes are then aggregated and fused, and employed to perform recurrence-free survival analysis on a cohort of 489 HNC patients. The proposed approach outperforms the best performing method on the target dataset for the task of recurrence-free survival analysis. By circumventing the manual delineation of the malignancies on the FDG PET-CT images, our approach eliminates the dependency on subjective interpretations and highly enhances the reproducibility of the proposed survival analysis method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01756v3</guid>
      <category>physics.med-ph</category>
      <category>cs.AI</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/cancers16142538</arxiv:DOI>
      <arxiv:journal_reference>Cancers. 2024; 16(14):2538</arxiv:journal_reference>
      <dc:creator>Amirhosein Toosi, Isaac Shiri, Habib Zaidi, Arman Rahmim</dc:creator>
    </item>
    <item>
      <title>How to Segment in 3D Using 2D Models: Automated 3D Segmentation of Prostate Cancer Metastatic Lesions on PET Volumes Using Multi-angle Maximum Intensity Projections and Diffusion Models</title>
      <link>https://arxiv.org/abs/2407.18555</link>
      <description>arXiv:2407.18555v3 Announce Type: replace 
Abstract: Prostate specific membrane antigen (PSMA) positron emission tomography/computed tomography (PET/CT) imaging provides a tremendously exciting frontier in visualization of prostate cancer (PCa) metastatic lesions. However, accurate segmentation of metastatic lesions is challenging due to low signal-to-noise ratios and variable sizes, shapes, and locations of the lesions. This study proposes a novel approach for automated segmentation of metastatic lesions in PSMA PET/CT 3D volumetric images using 2D denoising diffusion probabilistic models (DDPMs). Instead of 2D trans-axial slices or 3D volumes, the proposed approach segments the lesions on generated multi-angle maximum intensity projections (MA-MIPs) of the PSMA PET images, then obtains the final 3D segmentation masks from 3D ordered subset expectation maximization (OSEM) reconstruction of 2D MA-MIPs segmentations. Our proposed method achieved superior performance compared to state-of-the-art 3D segmentation approaches in terms of accuracy and robustness in detecting and segmenting small metastatic PCa lesions. The proposed method has significant potential as a tool for quantitative analysis of metastatic burden in PCa patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18555v3</guid>
      <category>physics.med-ph</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-72744-3_21</arxiv:DOI>
      <arxiv:journal_reference>Deep Generative Models. DGM4MICCAI 2024. Lecture Notes in Computer Science, vol 15224. Springer, Cham</arxiv:journal_reference>
      <dc:creator>Amirhosein Toosi, Sara Harsini, Fran\c{c}ois B\'enard, Carlos Uribe, Arman Rahmim</dc:creator>
    </item>
    <item>
      <title>Photon-Counting CT in Cancer Radiotherapy: Technological Advances and Clinical Benefits</title>
      <link>https://arxiv.org/abs/2410.20236</link>
      <description>arXiv:2410.20236v3 Announce Type: replace 
Abstract: Photon-counting computed tomography (PCCT) marks a significant advancement over conventional energy-integrating detector (EID) CT systems. This review highlights PCCT's superior spatial and contrast resolution, reduced radiation dose, and multi-energy imaging capabilities, which address key challenges in radiotherapy, such as accurate tumor delineation, precise dose calculation, and treatment response monitoring. PCCT's improved anatomical clarity enhances tumor targeting while minimizing damage to surrounding healthy tissues. Additionally, metal artifact reduction (MAR) and quantitative imaging capabilities optimize workflows, enabling adaptive radiotherapy and radiomics-driven personalized treatment. Emerging clinical applications in brachytherapy and radiopharmaceutical therapy (RPT) show promising outcomes, although challenges like high costs and limited software integration remain. With advancements in artificial intelligence (AI) and dedicated radiotherapy packages, PCCT is poised to transform precision, safety, and efficacy in cancer radiotherapy, marking it as a pivotal technology for future clinical practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20236v3</guid>
      <category>physics.med-ph</category>
      <category>eess.IV</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keyur D. Shah, Jun Zhou, Justin Roper, Anees Dhabaan, Hania Al-Hallaq, Amir Pourmorteza, Xiaofeng Yang</dc:creator>
    </item>
  </channel>
</rss>
