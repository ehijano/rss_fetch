<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Feb 2025 05:00:56 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Minkowski tensors for voxelized data: robust, asymptotically unbiased estimators</title>
      <link>https://arxiv.org/abs/2502.00092</link>
      <description>arXiv:2502.00092v1 Announce Type: new 
Abstract: Minkowski tensors, also known as tensor valuations, provide robust $n$-point information for a wide range of random spatial structures. Local estimators for voxelized data, however, are unavoidably biased even in the limit of infinitely high resolution. Here, we substantially improve a recently proposed, asymptotically unbiased algorithm to estimate Minkowski tensors for voxelized data. Our improved algorithm is more robust and efficient. Moreover we generalize the theoretical foundations for an asymptotically bias-free estimation of the interfacial tensors to the case of finite unions of compact sets with positive reach, which is relevant for many applications like rough surfaces or composite materials. As a realistic test case, we consider, among others, random (beta) polytopes. We first derive explicit expressions of the expected Minkowski tensors, which we then compare to our simulation results. We obtain precise estimates with relative errors of a few percent for practically relevant resolutions. Finally, we apply our methods to real data of metallic grains and nanorough surfaces, and we provide an open-source python package, which works in any dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00092v1</guid>
      <category>math.ST</category>
      <category>cond-mat.dis-nn</category>
      <category>math.MG</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Hug, Michael A. Klatt, Dominik Pabst</dc:creator>
    </item>
    <item>
      <title>Right-censored models on massive data</title>
      <link>https://arxiv.org/abs/2502.00178</link>
      <description>arXiv:2502.00178v1 Announce Type: new 
Abstract: This article considers the automatic selection problem of the relevant explanatory variables in a right-censored model on a massive database. We propose and study four aggregated censored adaptive LASSO estimators constructed by dividing the observations in such a way as to keep the consistency of the estimator of the survival curve. We show that these estimators have the same theoretical oracle properties as the one built on the full database. Moreover, by Monte Carlo simulations we obtain that their calculation time is smaller than that of the full database. The simulations confirm also the theoretical properties. For optimal tuning parameter selection, we propose a BIC-type criterion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00178v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriela Ciuperca</dc:creator>
    </item>
    <item>
      <title>Fractional Cumulative Residual Entropy in the Quantile Framework and its Applications in the Financial Data</title>
      <link>https://arxiv.org/abs/2502.00349</link>
      <description>arXiv:2502.00349v1 Announce Type: new 
Abstract: Fractional cumulative residual entropy (FCRE) is a powerful tool for the analysis of complex systems. Most of the theoretical results and applications related to the FCRE of the lifetime random variable are based on the distribution function approach. However, there are situations in which the distribution function may not be available in explicit form but has a closed-form quantile function (QF), an alternative method of representing a probability distribution. Motivated by this, in the present study we introduce a quantile-based FCRE, its dynamic version and their various properties and examine their usefulness in different applied fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00349v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Iona Ann Sebastian, S. M. Sunoj</dc:creator>
    </item>
    <item>
      <title>Direct sampling from conditional distributions by sequential maximum likelihood estimations</title>
      <link>https://arxiv.org/abs/2502.00812</link>
      <description>arXiv:2502.00812v1 Announce Type: new 
Abstract: We can directly sample from the conditional distribution of any log-affine model. The algorithm is a Markov chain on a bounded integer lattice, and its transition probability is the ratio of the UMVUE (uniformly minimum variance unbiased estimator) of the expected counts to the total number of counts. The computation of the UMVUE accounts for most of the computational cost, which makes the implementation challenging. Here, we investigated an approximate algorithm that replaces the UMVUE with the MLE (maximum likelihood estimator). Although it is generally not exact, it is efficient and easy to implement; no prior study is required, such as about the connection matrices of the holonomic ideal in the original algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00812v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuhei Mano</dc:creator>
    </item>
    <item>
      <title>Minimax Optimality of Classical Scaling Under General Noise Conditions</title>
      <link>https://arxiv.org/abs/2502.00947</link>
      <description>arXiv:2502.00947v1 Announce Type: new 
Abstract: We establish the consistency of classical scaling under a broad class of noise models, encompassing many commonly studied cases in literature. Our approach requires only finite fourth moments of the noise, significantly weakening standard assumptions. We derive convergence rates for classical scaling and establish matching minimax lower bounds, demonstrating that classical scaling achieves minimax optimality in recovering the true configuration even when the input dissimilarities are corrupted by noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00947v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siddharth Vishwanath, Ery Arias-Castro</dc:creator>
    </item>
    <item>
      <title>On the Private Estimation of Smooth Transport Maps</title>
      <link>https://arxiv.org/abs/2502.01168</link>
      <description>arXiv:2502.01168v1 Announce Type: new 
Abstract: Estimating optimal transport maps between two distributions from respective samples is an important element for many machine learning methods. To do so, rather than extending discrete transport maps, it has been shown that estimating the Brenier potential of the transport problem and obtaining a transport map through its gradient is near minimax optimal for smooth problems. In this paper, we investigate the private estimation of such potentials and transport maps with respect to the distribution samples.We propose a differentially private transport map estimator achieving an $L^2$ error of at most $n^{-1} \vee n^{-\frac{2 \alpha}{2 \alpha - 2 + d}} \vee (n\epsilon)^{-\frac{2 \alpha}{2 \alpha + d}} $ up to poly-logarithmic terms where $n$ is the sample size, $\epsilon$ is the desired level of privacy, $\alpha$ is the smoothness of the true transport map, and $d$ is the dimension of the feature space. We also provide a lower bound for the problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01168v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cl\'ement Lalanne (IMT, ANITI), Franck Iutzeler (IMT, ANITI), Jean-Michel Loubes (IMT, ANITI), Julien Chhor (TSE-R)</dc:creator>
    </item>
    <item>
      <title>Uniform mean estimation for monotonic processes</title>
      <link>https://arxiv.org/abs/2502.01244</link>
      <description>arXiv:2502.01244v1 Announce Type: new 
Abstract: We consider the problem of deriving uniform confidence bands for the mean of a monotonic stochastic process, such as the cumulative distribution function (CDF) of a random variable, based on a sequence of i.i.d.~observations. Our approach leverages the coin-betting framework, and inherits several favourable characteristics of coin-betting methods. In particular, for each point in the domain of the mean function, we obtain anytime-valid confidence intervals that are numerically tight and adapt to the variance of the observations. To derive uniform confidence bands, we employ a continuous union bound that crucially leverages monotonicity. In the case of CDF estimation, we also exploit the fact that the empirical CDF is piece-wise constant to obtain simple confidence bands that can be easily computed. In simulations, we find that our confidence bands for the CDF achieve state-of-the-art performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01244v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eugenio Clerico, Hamish E Flynn, Patrick Rebeschini</dc:creator>
    </item>
    <item>
      <title>Necessary and sufficient conditions for convergence in distribution of quantile and P-P processes in $L^1(0,1)$</title>
      <link>https://arxiv.org/abs/2502.01254</link>
      <description>arXiv:2502.01254v1 Announce Type: new 
Abstract: We establish a necessary and sufficient condition for the quantile process based on iid sampling to converge in distribution in $L^1(0,1)$. The condition is that the quantile function is locally absolutely continuous on the open unit interval and satisfies a slight strengthening of square integrability. We further establish a necessary and sufficient condition for the P-P process based on iid sampling from two populations to converge in distribution in $L^1(0,1)$. The condition is that the P-P curve is locally absolutely continuous on the open unit interval. If either process converges in distribution then it may be approximated using the bootstrap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01254v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brendan K. Beare, Tetsuya Kaji</dc:creator>
    </item>
    <item>
      <title>On tail dependence parameters for non-continuous and autocorrelated margins</title>
      <link>https://arxiv.org/abs/2502.01271</link>
      <description>arXiv:2502.01271v1 Announce Type: new 
Abstract: Tail dependence plays an essential role in the characterization of joint extreme events in multivariate data. However, most standard tail dependence parameters assume continuous margins. This note presents a form of tail dependence suitable for non-continuous and discrete margins. We derive a representation of tail dependence based on the volume of a copula and prove its properties. We utilize a bivariate regular variation to show that our new metric is consistent with the standard tail dependence parameters on continuous margins. We further define tail dependence on autocorrelated margins where the tail dependence parameter examine lagged correlation on the sample.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01271v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victory Idowu</dc:creator>
    </item>
    <item>
      <title>Targeted Data Fusion for Causal Survival Analysis Under Distribution Shift</title>
      <link>https://arxiv.org/abs/2501.18798</link>
      <description>arXiv:2501.18798v1 Announce Type: cross 
Abstract: Causal inference across multiple data sources has the potential to improve the generalizability, transportability, and replicability of scientific findings. However, data integration methods for time-to-event outcomes -- common in medical contexts such as clinical trials -- remain underdeveloped. Existing data fusion methods focus on binary or continuous outcomes, neglecting the distinct challenges of survival analysis, including right-censoring and the unification of discrete and continuous time frameworks. To address these gaps, we propose two novel approaches for multi-source causal survival analysis. First, considering a target site-specific causal effect, we introduce a semiparametric efficient estimator for scenarios where data-sharing is feasible. Second, we develop a federated learning framework tailored to privacy-constrained environments. This framework dynamically adjusts source site-specific contributions, downweighting biased sources and upweighting less biased ones relative to the target population. Both approaches incorporate nonparametric machine learning models to enhance robustness and efficiency, with theoretical guarantees applicable to both continuous and discrete time-to-event outcomes. We demonstrate the practical utility of our methods through extensive simulations and an application to two randomized trials of a monoclonal neutralizing antibody for HIV-1 prevention: HVTN 704/HPTN 085 (cisgender men and transgender persons in the Americas and Switzerland) and HVTN 703/HPTN 081 (women in sub-Saharan Africa). The results highlight the potential of our approaches to efficiently estimate causal effects while addressing heterogeneity across data sources and adhering to privacy and robustness constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18798v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Liu, Alexander W. Levis, Ke Zhu, Shu Yang, Peter B. Gilbert, Larry Han</dc:creator>
    </item>
    <item>
      <title>A Bayesian decision-theoretic approach to sparse estimation</title>
      <link>https://arxiv.org/abs/2502.00126</link>
      <description>arXiv:2502.00126v1 Announce Type: cross 
Abstract: We extend the work of Hahn and Carvalho (2015) and develop a doubly-regularized sparse regression estimator by synthesizing Bayesian regularization with penalized least squares within a decision-theoretic framework. In contrast to existing Bayesian decision-theoretic formulation chiefly reliant upon the symmetric 0-1 loss, the new method -- which we call Bayesian Decoupling -- employs a family of penalized loss functions indexed by a sparsity-tuning parameter. We propose a class of reweighted l1 penalties, with two specific instances that achieve simultaneous bias reduction and convexity. The design of the penalties incorporates considerations of signal sizes, as enabled by the Bayesian paradigm. The tuning parameter is selected using a posterior benchmarking criterion, which quantifies the drop in predictive power relative to the posterior mean which is the optimal Bayes estimator under the squared error loss. Additionally, in contrast to the widely used median probability model technique which selects variables by thresholding posterior inclusion probabilities at the fixed threshold of 1/2, Bayesian Decoupling enables the use of a data-driven threshold which automatically adapts to estimated signal sizes and offers far better performance in high-dimensional settings with highly correlated predictors. Our numerical results in such settings show that certain combinations of priors and loss functions significantly improve the solution path compared to existing methods, prioritizing true signals early along the path before false signals are selected. Consequently, Bayesian Decoupling produces estimates with better prediction and selection performance. Finally, a real data application illustrates the practical advantages of our approaches which select sparser models with larger coefficient estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00126v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aihua Li, Surya T. Tokdar, Jason Xu</dc:creator>
    </item>
    <item>
      <title>Supervised Quadratic Feature Analysis: An Information Geometry Approach to Dimensionality Reduction</title>
      <link>https://arxiv.org/abs/2502.00168</link>
      <description>arXiv:2502.00168v1 Announce Type: cross 
Abstract: Supervised dimensionality reduction aims to map labeled data to a low-dimensional feature space while maximizing class discriminability. Despite the availability of methods for learning complex non-linear features (e.g. Deep Learning), there is an enduring demand for dimensionality reduction methods that learn linear features due to their interpretability, low computational cost, and broad applicability. However, there is a gap between methods that optimize linear separability (e.g. LDA), and more flexible but computationally expensive methods that optimize over arbitrary class boundaries (e.g. metric-learning methods). Here, we present Supervised Quadratic Feature Analysis (SQFA), a dimensionality reduction method for learning linear features that maximize the differences between class-conditional first- and second-order statistics, which allow for quadratic discrimination. SQFA exploits the information geometry of second-order statistics in the symmetric positive definite manifold. We show that SQFA features support quadratic discriminability in real-world problems. We also provide a theoretical link, based on information geometry, between SQFA and the Quadratic Discriminant Analysis (QDA) classifier.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00168v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.DG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Herrera-Esposito, Johannes Burge</dc:creator>
    </item>
    <item>
      <title>Score-Preserving Targeted Maximum Likelihood Estimation</title>
      <link>https://arxiv.org/abs/2502.00200</link>
      <description>arXiv:2502.00200v1 Announce Type: cross 
Abstract: Targeted maximum likelihood estimators (TMLEs) are asymptotically optimal among regular, asymptotically linear estimators. In small samples, however, we may be far from "asymptopia" and not reap the benefits of optimality. Here we propose a variant (score-preserving TMLE; SP-TMLE) that leverages an initial estimator defined as the solution of a large number of possibly data-dependent score equations. Instead of targeting only the efficient influence function in the TMLE update to knock out the plug-in bias, we also target the already-solved scores. Solving additional scores reduces the remainder term in the von-Mises expansion of our estimator because these scores may come close to spanning higher-order influence functions. The result is an estimator with better finite-sample performance. We demonstrate our approach in simulation studies leveraging the (relaxed) highly adaptive lasso (HAL) as our initial estimator. These simulations show that in small samples SP-TMLE has reduced bias relative to plug-in HAL and reduced variance relative to vanilla TMLE, blending the advantages of the two approaches. We also observe improved estimation of standard errors in small samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00200v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noel Pimentel, Alejandro Schuler, Mark van der Laan</dc:creator>
    </item>
    <item>
      <title>Learning to Fuse Temporal Proximity Networks: A Case Study in Chimpanzee Social Interactions</title>
      <link>https://arxiv.org/abs/2502.00302</link>
      <description>arXiv:2502.00302v1 Announce Type: cross 
Abstract: How can we identify groups of primate individuals which could be conjectured to drive social structure? To address this question, one of us has collected a time series of data for social interactions between chimpanzees. Here we use a network representation, leading to the task of combining these data into a time series of a single weighted network per time stamp, where different proximities should be given different weights reflecting their relative importance. We optimize these proximity-type weights in a principled way, using an innovative loss function which rewards structural consistency across time. The approach is empirically validated by carefully designed synthetic data. Using statistical tests, we provide a way of identifying groups of individuals that stay related for a significant length of time. Applying the approach to the chimpanzee data set, we detect cliques in the animal social network time series, which can be validated by real-world intuition from prior research and qualitative observations by chimpanzee experts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00302v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yixuan He, Aaron Sandel, David Wipf, Mihai Cucuringu, John Mitani, Gesine Reinert</dc:creator>
    </item>
    <item>
      <title>Confidence intervals for intentionally biased estimators</title>
      <link>https://arxiv.org/abs/2502.00450</link>
      <description>arXiv:2502.00450v1 Announce Type: cross 
Abstract: We propose and study three confidence intervals (CIs) centered at an estimator that is intentionally biased to reduce mean squared error. The first CI simply uses an unbiased estimator's standard error; compared to centering at the unbiased estimator, this CI has higher coverage probability for confidence levels above 91.7%, even if the biased and unbiased estimators have equal mean squared error. The second CI trades some of this "excess" coverage for shorter length. The third CI is centered at a convex combination of the two estimators to further reduce length. Practically, these CIs apply broadly and are simple to compute.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00450v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/07474938.2024.2312288</arxiv:DOI>
      <arxiv:journal_reference>Econometric Reviews 43 (2024) 197-214</arxiv:journal_reference>
      <dc:creator>David M. Kaplan, Xin Liu</dc:creator>
    </item>
    <item>
      <title>A Proof of The Changepoint Detection Threshold Conjecture in Preferential Attachment Models</title>
      <link>https://arxiv.org/abs/2502.00514</link>
      <description>arXiv:2502.00514v1 Announce Type: cross 
Abstract: We investigate the problem of detecting and estimating a changepoint in the attachment function of a network evolving according to a preferential attachment model on $n$ vertices, using only a single final snapshot of the network. Bet et al.~\cite{bet2023detecting} show that a simple test based on thresholding the number of vertices with minimum degrees can detect the changepoint when the change occurs at time $n-\Omega(\sqrt{n})$. They further make the striking conjecture that detection becomes impossible for any test if the change occurs at time $n-o(\sqrt{n}).$ Kaddouri et al.~\cite{kaddouri2024impossibility} make a step forward by proving the detection is impossible if the change occurs at time $n-o(n^{1/3}).$ In this paper, we resolve the conjecture affirmatively, proving that detection is indeed impossible if the change occurs at time $n-o(\sqrt{n}).$ Furthermore, we establish that estimating the changepoint with an error smaller than $o(\sqrt{n})$ is also impossible, thereby confirming that the estimator proposed in Bhamidi et al.~\cite{bhamidi2018change} is order-optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00514v1</guid>
      <category>math.PR</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hang Du, Shuyang Gong, Jiaming Xu</dc:creator>
    </item>
    <item>
      <title>Inference of Half Logistic Geometric Distribution Based on Generalized Order Statistics</title>
      <link>https://arxiv.org/abs/2502.01255</link>
      <description>arXiv:2502.01255v1 Announce Type: cross 
Abstract: As the unification of various models of ordered quantities, generalized order statistics act as a simplistic approach introduced in \cite{kamps1995concept}. In this present study, results pertaining to the expressions of marginal and joint moment generating functions from half logistic geometric distribution are presented based on generalized order statistics framework. We also consider the estimation problem of $\theta$ and provides a Bayesian framework. The two widely and popular methods called Markov chain Monte Carlo and Lindley approximations are used for obtaining the Bayes estimators.The results are derived under symmetric and asymmetric loss functions. Analysis of the special cases of generalized order statistics, \textit{i.e.,} order statistics is also presented. To have an insight into the practical applicability of the proposed results, two real data sets, one from the field of Demography and, other from reliability have been taken for analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01255v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Neetu Gupta, S. K. Neogy, Qazi J. Azhad, Bhagwati Devi</dc:creator>
    </item>
    <item>
      <title>Wrapped Gaussian on the manifold of Symmetric Positive Definite Matrices</title>
      <link>https://arxiv.org/abs/2502.01512</link>
      <description>arXiv:2502.01512v1 Announce Type: cross 
Abstract: Circular and non-flat data distributions are prevalent across diverse domains of data science, yet their specific geometric structures often remain underutilized in machine learning frameworks. A principled approach to accounting for the underlying geometry of such data is pivotal, particularly when extending statistical models, like the pervasive Gaussian distribution. In this work, we tackle those issue by focusing on the manifold of symmetric positive definite matrices, a key focus in information geometry. We introduced a non-isotropic wrapped Gaussian by leveraging the exponential map, we derive theoretical properties of this distribution and propose a maximum likelihood framework for parameter estimation. Furthermore, we reinterpret established classifiers on SPD through a probabilistic lens and introduce new classifiers based on the wrapped Gaussian model. Experiments on synthetic and real-world datasets demonstrate the robustness and flexibility of this geometry-aware distribution, underscoring its potential to advance manifold-based data analysis. This work lays the groundwork for extending classical machine learning and statistical methods to more complex and structured data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01512v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thibault de Surrel, Fabien Lotte, Sylvain Chevallier, Florian Yger</dc:creator>
    </item>
    <item>
      <title>Spectral Estimators for Multi-Index Models: Precise Asymptotics and Optimal Weak Recovery</title>
      <link>https://arxiv.org/abs/2502.01583</link>
      <description>arXiv:2502.01583v1 Announce Type: cross 
Abstract: Multi-index models provide a popular framework to investigate the learnability of functions with low-dimensional structure and, also due to their connections with neural networks, they have been object of recent intensive study. In this paper, we focus on recovering the subspace spanned by the signals via spectral estimators -- a family of methods that are routinely used in practice, often as a warm-start for iterative algorithms. Our main technical contribution is a precise asymptotic characterization of the performance of spectral methods, when sample size and input dimension grow proportionally and the dimension $p$ of the space to recover is fixed. Specifically, we locate the top-$p$ eigenvalues of the spectral matrix and establish the overlaps between the corresponding eigenvectors (which give the spectral estimators) and a basis of the signal subspace. Our analysis unveils a phase transition phenomenon in which, as the sample complexity grows, eigenvalues escape from the bulk of the spectrum and, when that happens, eigenvectors recover directions of the desired subspace. The precise characterization we put forward enables the optimization of the data preprocessing, thus allowing to identify the spectral estimator that requires the minimal sample size for weak recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01583v1</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Filip Kova\v{c}evi\'c, Yihan Zhang, Marco Mondelli</dc:creator>
    </item>
    <item>
      <title>Re-examining Double Descent and Scaling Laws under Norm-based Capacity via Deterministic Equivalence</title>
      <link>https://arxiv.org/abs/2502.01585</link>
      <description>arXiv:2502.01585v1 Announce Type: cross 
Abstract: We investigate double descent and scaling laws in terms of weights rather than the number of parameters. Specifically, we analyze linear and random features models using the deterministic equivalence approach from random matrix theory. We precisely characterize how the weights norm concentrate around deterministic quantities and elucidate the relationship between the expected test error and the norm-based capacity (complexity). Our results rigorously answer whether double descent exists under norm-based capacity and reshape the corresponding scaling laws. Moreover, they prompt a rethinking of the data-parameter paradigm - from under-parameterized to over-parameterized regimes - by shifting the focus to norms (weights) rather than parameter count.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01585v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yichen Wang, Yudong Chen, Lorenzo Rosasco, Fanghui Liu</dc:creator>
    </item>
    <item>
      <title>Statistical inference for high-dimensional spectral density matrix</title>
      <link>https://arxiv.org/abs/2212.13686</link>
      <description>arXiv:2212.13686v3 Announce Type: replace 
Abstract: The spectral density matrix is a fundamental object of interest in time series analysis, and it encodes both contemporary and dynamic linear relationships between component processes of the multivariate system. In this paper we develop novel inference procedures for the spectral density matrix in the high-dimensional setting. Specifically, we introduce a new global testing procedure to test the nullity of the cross-spectral density for a given set of frequencies and across pairs of component indices. For the first time, both Gaussian approximation and parametric bootstrap methodologies are employed to conduct inference for a high-dimensional parameter formulated in the frequency domain, and new technical tools are developed to provide asymptotic guarantees of the size accuracy and power for global testing. We further propose a multiple testing procedure for simultaneously testing the nullity of the cross-spectral density at a given set of frequencies. The method is shown to control the false discovery rate. Both numerical simulations and a real data illustration demonstrate the usefulness of the proposed testing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.13686v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Qing Jiang, Tucker S. McElroy, Xiaofeng Shao</dc:creator>
    </item>
    <item>
      <title>Online Estimation with Rolling Validation: Adaptive Nonparametric Estimation with Streaming Data</title>
      <link>https://arxiv.org/abs/2310.12140</link>
      <description>arXiv:2310.12140v3 Announce Type: replace 
Abstract: Online nonparametric estimators are gaining popularity due to their efficient computation and competitive generalization abilities. An important example includes variants of stochastic gradient descent. These algorithms often take one sample point at a time and incrementally update the parameter estimate of interest. In this work, we consider model selection/hyperparameter tuning for such online algorithms. We propose a weighted rolling validation procedure, an online variant of leave-one-out cross-validation, that costs minimal extra computation for many typical stochastic gradient descent estimators and maintains their online nature. Similar to batch cross-validation, it can boost base estimators to achieve better heuristic performance and adaptive convergence rate. Our analysis is straightforward, relying mainly on some general statistical stability assumptions. The simulation study underscores the significance of diverging weights in practice and demonstrates its favorable sensitivity even when there is only a slim difference between candidate estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.12140v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianyu Zhang, Jing Lei</dc:creator>
    </item>
    <item>
      <title>The numeraire e-variable and reverse information projection</title>
      <link>https://arxiv.org/abs/2402.18810</link>
      <description>arXiv:2402.18810v4 Announce Type: replace 
Abstract: We consider testing a composite null hypothesis $\mathcal{P}$ against a point alternative $\mathsf{Q}$ using e-variables, which are nonnegative random variables $X$ such that $\mathbb{E}_\mathsf{P}[X] \leq 1$ for every $\mathsf{P} \in \mathcal{P}$. This paper establishes a fundamental result: under no conditions whatsoever on $\mathcal{P}$ or $\mathsf{Q}$, there exists a special e-variable $X^*$ that we call the numeraire, which is strictly positive and satisfies $\mathbb{E}_\mathsf{Q}[X/X^*] \leq 1$ for every other e-variable $X$. In particular, $X^*$ is log-optimal in the sense that $\mathbb{E}_\mathsf{Q}[\log(X/X^*)] \leq 0$. Moreover, $X^*$ identifies a particular sub-probability measure $\mathsf{P}^*$ via the density $d \mathsf{P}^*/d \mathsf{Q} = 1/X^*$. As a result, $X^*$ can be seen as a generalized likelihood ratio of $\mathsf{Q}$ against $\mathcal{P}$. We show that $\mathsf{P}^*$ coincides with the reverse information projection (RIPr) when additional assumptions are made that are required for the latter to exist. Thus $\mathsf{P}^*$ is a natural definition of the RIPr in the absence of any assumptions on $\mathcal{P}$ or $\mathsf{Q}$. In addition to the abstract theory, we provide several tools for finding the numeraire and RIPr in concrete cases. We discuss several nonparametric examples where we can indeed identify the numeraire and RIPr, despite not having a reference measure. Our results have interpretations outside of testing in that they yield the optimal Kelly bet against $\mathcal{P}$ if we believe reality follows $\mathsf{Q}$. We end with a more general optimality theory that goes beyond the ubiquitous logarithmic utility. We focus on certain power utilities, leading to reverse R\'enyi projections in place of the RIPr, which also always exist.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18810v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Larsson, Aaditya Ramdas, Johannes Ruf</dc:creator>
    </item>
    <item>
      <title>Deep Horseshoe Gaussian Processes</title>
      <link>https://arxiv.org/abs/2403.01737</link>
      <description>arXiv:2403.01737v2 Announce Type: replace 
Abstract: Deep Gaussian processes have recently been proposed as natural objects to fit, similarly to deep neural networks, possibly complex features present in modern data samples, such as compositional structures. Adopting a Bayesian nonparametric approach, it is natural to use deep Gaussian processes as prior distributions, and use the corresponding posterior distributions for statistical inference. We introduce the deep Horseshoe Gaussian process Deep-HGP, a new simple prior based on deep Gaussian processes with a squared-exponential kernel, that in particular enables data-driven choices of the key lengthscale parameters. For nonparametric regression with random design, we show that the associated posterior distribution recovers the unknown true regression curve optimally in terms of quadratic loss, up to a logarithmic factor, in an adaptive way. The convergence rates are simultaneously adaptive to both the smoothness of the regression function and to its structure in terms of compositions. The dependence of the rates in terms of dimension are explicit, allowing in particular for input spaces of dimension increasing with the number of observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01737v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isma\"el Castillo, Thibault Randrianarisoa</dc:creator>
    </item>
    <item>
      <title>Likelihood distortion and Bayesian local robustness</title>
      <link>https://arxiv.org/abs/2405.15141</link>
      <description>arXiv:2405.15141v2 Announce Type: replace 
Abstract: Robust Bayesian analysis has been mainly devoted to detecting and measuring robustness w.r.t. the prior distribution. Many contributions in the literature aim to define suitable classes of priors which allow the computation of variations of quantities of interest while the prior changes within those classes. The literature has devoted much less attention to the robustness of Bayesian methods w.r.t. the likelihood function due to mathematical and computational complexity, and because it is often arguably considered a more objective choice compared to the prior. In this contribution, we propose a new approach to Bayesian local robustness, mainly focusing on robustness w.r.t. the likelihood function. Successively, we extend it to account for robustness w.r.t. the prior, as well as the prior and the likelihood jointly. This approach is based on the notion of distortion function introduced in the literature on risk theory. The novel robustness measure is a local sensitivity measure that turns out to be very tractable and easy to compute for several classes of distortion functions. Asymptotic properties are derived, and numerical experiments illustrate the theory and its applicability for modelling purposes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15141v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antonio Di Noia, Fabrizio Ruggeri, Antonietta Mira</dc:creator>
    </item>
    <item>
      <title>Marchenko-Pastur law for Daniell smoothed periodograms without simultaneous diagonalizability</title>
      <link>https://arxiv.org/abs/2408.14618</link>
      <description>arXiv:2408.14618v3 Announce Type: replace 
Abstract: The eigenvectors of a spectral density matrix $F(\theta)$ to a stationary Gaussian process $(X_t)_{t \in \mathbb{Z}}$ depend explicitly on the frequency $\theta \in [0,2\pi]$. The most commonly used estimator of the spectral density matrix $F(\theta)$ is the smoothed periodogram, which takes the form $ZZ^*$ for random matrices $Z$ with non-zero covariance between rows and columns. When the covariance matrices of the columns are not simultaneously diagonalizable, this covariance structure is non-separable and such matrices $ZZ^*$ are out of reach for the current state of random matrix theory. In this paper, we derive a Marchenko-Pastur law in this non-simultaneously diagonalizable case. The Marchenko-Pastur law emerges when the dimension $d$ of the process and the smoothing span $m$ of the smoothed periodogram grow at the same rate, which is slower than the number of observations $n$.
  On the technical level we prove a trace moment bound for matrices $YY^T$, where $Y$ is a matrix with correlated Gaussian entries. This allows for sub-polynomial error bounds in settings where the error $Y$ has correlations between different points in time as well as between features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14618v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ben Deitmar</dc:creator>
    </item>
    <item>
      <title>Multiple testing in multi-stream sequential change detection</title>
      <link>https://arxiv.org/abs/2501.04130</link>
      <description>arXiv:2501.04130v4 Announce Type: replace 
Abstract: Multi-stream sequential change detection involves simultaneously monitoring many streams of data and trying to detect when their distributions change, if at all. Here, we theoretically study multiple testing issues that arise from detecting changes in many streams. We point out that any algorithm with finite average run length (ARL) must have a trivial worst-case false detection rate (FDR), family-wise error rate (FWER), per-family error rate (PFER), and global error rate (GER); thus, any attempt to control these Type I error metrics is fundamentally in conflict with the desire for a finite ARL (which is typically necessary in order to have a small detection delay). One of our contributions is to define a new class of metrics which can be controlled, called error over patience (EOP). We propose algorithms that combine the recent e-detector framework (which generalizes the Shiryaev-Roberts and CUSUM methods) with the recent e-Benjamini-Hochberg procedure and e-Bonferroni procedures. We prove that these algorithms control the EOP at any desired level under very general dependence structures on the data within and across the streams. In fact, we prove a more general error control that holds uniformly over all stopping times and provides a smooth trade-off between the conflicting metrics. Additionally, if finiteness of the ARL is forfeited, we show that our algorithms control the worst-case Type I error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04130v4</guid>
      <category>math.ST</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanjit Dandapanthula, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>High dimensional analysis reveals conservative sharpening and a stochastic edge of stability</title>
      <link>https://arxiv.org/abs/2404.19261</link>
      <description>arXiv:2404.19261v2 Announce Type: replace-cross 
Abstract: Recent empirical and theoretical work has shown that the dynamics of the large eigenvalues of the training loss Hessian have some remarkably robust features across models and datasets in the full batch regime. There is often an early period of progressive sharpening where the large eigenvalues increase, followed by stabilization at a predictable value known as the edge of stability. Previous work showed that in the stochastic setting, the eigenvalues increase more slowly - a phenomenon we call conservative sharpening. We provide a theoretical analysis of a simple high-dimensional model which shows the origin of this slowdown. We also show that there is an alternative stochastic edge of stability which arises at small batch size that is sensitive to the trace of the Neural Tangent Kernel rather than the large Hessian eigenvalues. We conduct an experimental study which highlights the qualitative differences from the full batch phenomenology, and suggests that controlling the stochastic edge of stability can help optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19261v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>physics.data-an</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Atish Agarwala, Jeffrey Pennington</dc:creator>
    </item>
    <item>
      <title>Gaussian Approximation and Multiplier Bootstrap for Polyak-Ruppert Averaged Linear Stochastic Approximation with Applications to TD Learning</title>
      <link>https://arxiv.org/abs/2405.16644</link>
      <description>arXiv:2405.16644v2 Announce Type: replace-cross 
Abstract: In this paper, we obtain the Berry-Esseen bound for multivariate normal approximation for the Polyak-Ruppert averaged iterates of the linear stochastic approximation (LSA) algorithm with decreasing step size. Moreover, we prove the non-asymptotic validity of the confidence intervals for parameter estimation with LSA based on multiplier bootstrap. This procedure updates the LSA estimate together with a set of randomly perturbed LSA estimates upon the arrival of subsequent observations. We illustrate our findings in the setting of temporal difference learning with linear function approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16644v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sergey Samsonov, Eric Moulines, Qi-Man Shao, Zhuo-Song Zhang, Alexey Naumov</dc:creator>
    </item>
    <item>
      <title>On the Impossibility of Equating the Youden Index with Tjur's $R^2$-like metrics in $2\times 2$ Tables</title>
      <link>https://arxiv.org/abs/2411.05391</link>
      <description>arXiv:2411.05391v3 Announce Type: replace-cross 
Abstract: In 2017, Hughes claimed an equivalence between Tjurs $R^2$ coefficient of discrimination and the Youden index for assessing diagnostic test performance on $2\times 2$ contingency tables. We prove an impossibility result when averaging over binary outcomes (0s and 1s) under any continuous real-valued scoring rule. Our finding clarifies the limitations of such a possible equivalence and highlights the distinct roles these metrics play in diagnostic test assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05391v3</guid>
      <category>stat.OT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linard Hoessly</dc:creator>
    </item>
    <item>
      <title>Tropical combinatorics of max-linear Bayesian networks</title>
      <link>https://arxiv.org/abs/2411.10394</link>
      <description>arXiv:2411.10394v2 Announce Type: replace-cross 
Abstract: A polytrope is a tropical polyhedron that is also classically convex. We study the tropical combinatorial types of polytropes associated to weighted directed acyclic graphs (DAGs). This family of polytropes arises in algebraic statistics when describing the model class of max-linear Bayesian networks. We show how the edge weights of a network directly relate to the facet structure of the corresponding polytrope. We also give a classification of polytropes from weighted DAGs at different levels of equivalence. These results give insight on the statistical problem of identifiability for a max-linear Bayesian network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10394v2</guid>
      <category>math.CO</category>
      <category>math.AG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlos Am\'endola, Kamillo Ferry</dc:creator>
    </item>
    <item>
      <title>Block Coordinate DC Programming</title>
      <link>https://arxiv.org/abs/2411.11664</link>
      <description>arXiv:2411.11664v2 Announce Type: replace-cross 
Abstract: We introduce an extension of the Difference of Convex Algorithm (DCA) in the form of a block coordinate approach for problems with separable structure. For $n$ coordinate-blocks and $k$ iterations, our main result proves a non-asymptotic convergence rate of $O(n/k)$ for the proposed method. Furthermore, leveraging the connection between DCA and Expectation Maximization (EM), we propose a block coordinate EM algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11664v2</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hoomaan Maskan, Paniz Halvachi, Suvrit Sra, Alp Yurtsever</dc:creator>
    </item>
    <item>
      <title>Online Clustering with Bandit Information</title>
      <link>https://arxiv.org/abs/2501.11421</link>
      <description>arXiv:2501.11421v2 Announce Type: replace-cross 
Abstract: We study the problem of online clustering within the multi-armed bandit framework under the fixed confidence setting. In this multi-armed bandit problem, we have $M$ arms, each providing i.i.d. samples that follow a multivariate Gaussian distribution with an {\em unknown} mean and a known unit covariance. The arms are grouped into $K$ clusters based on the distance between their means using the Single Linkage (SLINK) clustering algorithm on the means of the arms. Since the true means are unknown, the objective is to obtain the above clustering of the arms with the minimum number of samples drawn from the arms, subject to an upper bound on the error probability. We introduce a novel algorithm, Average Tracking Bandit Online Clustering (ATBOC), and prove that this algorithm is order optimal, meaning that the upper bound on its expected sample complexity for given error probability $\delta$ is within a factor of 2 of an instance-dependent lower bound as $\delta \rightarrow 0$. Furthermore, we propose a computationally more efficient algorithm, Lower and Upper Confidence Bound-based Bandit Online Clustering (LUCBBOC), inspired by the LUCB algorithm for best arm identification. Simulation results demonstrate that the performance of LUCBBOC is comparable to that of ATBOC. We numerically assess the effectiveness of the proposed algorithms through numerical experiments on both synthetic datasets and the real-world MovieLens dataset. To the best of our knowledge, this is the first work on bandit online clustering that allows arms with different means in a cluster and $K$ greater than 2.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11421v2</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>G Dhinesh Chandran, Srinivas Reddy Kota, Srikrishna Bhashyam</dc:creator>
    </item>
    <item>
      <title>What is causal about causal models and representations?</title>
      <link>https://arxiv.org/abs/2501.19335</link>
      <description>arXiv:2501.19335v2 Announce Type: replace-cross 
Abstract: Causal Bayesian networks are 'causal' models since they make predictions about interventional distributions. To connect such causal model predictions to real-world outcomes, we must determine which actions in the world correspond to which interventions in the model. For example, to interpret an action as an intervention on a treatment variable, the action will presumably have to a) change the distribution of treatment in a way that corresponds to the intervention, and b) not change other aspects, such as how the outcome depends on the treatment; while the marginal distributions of some variables may change as an effect. We introduce a formal framework to make such requirements for different interpretations of actions as interventions precise. We prove that the seemingly natural interpretation of actions as interventions is circular: Under this interpretation, every causal Bayesian network that correctly models the observational distribution is trivially also interventionally valid, and no action yields empirical data that could possibly falsify such a model. We prove an impossibility result: No interpretation exists that is non-circular and simultaneously satisfies a set of natural desiderata. Instead, we examine non-circular interpretations that may violate some desiderata and show how this may in turn enable the falsification of causal models. By rigorously examining how a causal Bayesian network could be a 'causal' model of the world instead of merely a mathematical object, our formal framework contributes to the conceptual foundations of causal representation learning, causal discovery, and causal abstraction, while also highlighting some limitations of existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19335v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Frederik Hytting J{\o}rgensen, Luigi Gresele, Sebastian Weichwald</dc:creator>
    </item>
  </channel>
</rss>
