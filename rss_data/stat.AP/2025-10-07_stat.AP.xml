<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Oct 2025 04:00:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Robust and efficient estimation for the Generalized Extreme-Value distribution with application to flood frequency analysis in the UK</title>
      <link>https://arxiv.org/abs/2510.03338</link>
      <description>arXiv:2510.03338v1 Announce Type: new 
Abstract: A common approach for modeling extremes, such as peak flow or high temperatures, is the three-parameter Generalized Extreme-Value distribution. This is typically fit to extreme observations, here defined as maxima over disjoint blocks. This results in limited sample sizes and consequently, the use of classic estimators, such as the maximum likelihood estimator, may be inappropriate, as they are highly sensitive to outliers. To address these limitations, we propose a novel robust estimator based on the minimization of the density power divergence, controlled by a tuning parameter $\alpha$ that balances robustness and efficiency. When $\alpha = 0$, our estimator coincides with the maximum likelihood estimator; when $\alpha = 1$, it corresponds to the $L^2$ estimator, known for its robustness. We establish convenient theoretical properties of the proposed estimator, including its asymptotic normality and the boundedness of its influence function for $\alpha &gt; 0$. The practical efficiency of the method is demonstrated through empirical comparisons with the maximum likelihood estimator and other robust alternatives. Finally, we illustrate its relevance in a case study on flood frequency analysis in the UK and provide some general conclusions in Section 6.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03338v1</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Huet, Ilaria Prosdocimi</dc:creator>
    </item>
    <item>
      <title>Bayesian Variable Selection for Censored Spatial Responses with Application to PFAS Concentrations in California</title>
      <link>https://arxiv.org/abs/2510.03681</link>
      <description>arXiv:2510.03681v1 Announce Type: new 
Abstract: Per- and polyfluoroalkyl substances (PFAS) are persistent environmental pollutants of major public health concern due to their resistance to degradation, widespread presence, and potential health risks. Analyzing PFAS in groundwater is challenging due to left-censoring and strong spatial dependence. Although PFAS levels are influenced by sociodemographic, industrial, and environmental factors, the relative importance of these drivers remains unclear, highlighting the need for robust statistical tools to identify key predictors from a large candidate set. We present a Bayesian hierarchical framework that integrates censoring into a spatial process model via approximate Gaussian processes and employs a global-local shrinkage prior for high-dimensional variable selection. We evaluate three post-selection strategies, namely, credible interval rules, shrinkage weight thresholds, and clustering-based inclusion and compare their performance in terms of predictive accuracy, censoring robustness, and variable selection stability through cross-validation. Applied to PFOS concentrations in California groundwater, the model identifies a concise, interpretable set of predictors, including demographic composition, industrial facility counts, proximity to airports, traffic density, and environmental features such as herbaceous cover and elevation. These findings demonstrate that the proposed approach delivers stable, interpretable inference in censored, spatial, high-dimensional contexts, thereby offering actionable insights into the environmental and industrial factors affecting PFAS concentrations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03681v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suman Majumder, Indranil Sahoo</dc:creator>
    </item>
    <item>
      <title>Statistical Crime Linkage: Evaluating approaches within the Covenant for Using AI in Policing</title>
      <link>https://arxiv.org/abs/2510.03730</link>
      <description>arXiv:2510.03730v1 Announce Type: new 
Abstract: Linking crimes by modus operandi has long been employed as an effective tool for crime investigation. The standard statistical method that underpins statistical crime linkage has been logistic regression. The simplicity and interpretability of this approach has been seen as an advantage for law enforcement agencies using statistical crime linkage. In 2023, the National Police Chiefs' Council published the Covenant for Using Artificial Intelligence in Policing designed to guide the development of novel methods for use within policing. In this article, we investigate more statistical and machine learning methods that could underpin crime linkage models. We investigate a range of methods including regression-, sampling-, and machine learning-based techniques and evaluate them against the principles of Explainability and Transparency from the Covenant. We investigate our methods on a new data set on romance fraud in the UK, where 361 victims of fraud reported the behaviours and characteristics of the suspects involved in their case. We propose a sensitive, Explainable, and Transparent machine learning model for crime linkage and demonstrate how this method could support crime linkage efforts by law enforcement agencies using a dataset of romance fraud with unknown linkage status.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03730v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan A. Judd, Amy V. Tansell, Benjamin Costello, Liam Leonard, Jessica Woodhams, Rowland G. Seymour</dc:creator>
    </item>
    <item>
      <title>Machine Learning Workflows in Climate Modeling: Design Patterns and Insights from Case Studies</title>
      <link>https://arxiv.org/abs/2510.03305</link>
      <description>arXiv:2510.03305v1 Announce Type: cross 
Abstract: Machine learning has been increasingly applied in climate modeling on system emulation acceleration, data-driven parameter inference, forecasting, and knowledge discovery, addressing challenges such as physical consistency, multi-scale coupling, data sparsity, robust generalization, and integration with scientific workflows. This paper analyzes a series of case studies from applied machine learning research in climate modeling, with a focus on design choices and workflow structure. Rather than reviewing technical details, we aim to synthesize workflow design patterns across diverse projects in ML-enabled climate modeling: from surrogate modeling, ML parameterization, probabilistic programming, to simulation-based inference, and physics-informed transfer learning. We unpack how these workflows are grounded in physical knowledge, informed by simulation data, and designed to integrate observations. We aim to offer a framework for ensuring rigor in scientific machine learning through more transparent model development, critical evaluation, informed adaptation, and reproducibility, and to contribute to lowering the barrier for interdisciplinary collaboration at the interface of data science and climate modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03305v1</guid>
      <category>cs.LG</category>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tian Zheng, Subashree Venkatasubramanian, Shuolin Li, Amy Braverman, Xinyi Ke, Zhewen Hou, Peter Jin, Samarth Sanjay Agrawal</dc:creator>
    </item>
    <item>
      <title>How does course recommendation impact student outcomes? Examining directed self-placement with regression discontinuity analysis</title>
      <link>https://arxiv.org/abs/2510.03350</link>
      <description>arXiv:2510.03350v1 Announce Type: cross 
Abstract: For many students, placement into developmental education becomes a self-fulfilling prophecy. Placing college students into developmental education significantly negatively impacts student attainment, student probability of passing, and college credits earned. To combat these negative effects, many universities are investigating alternative placement mechanisms. Could directed self-placement be an effective alternative mechanism? Do students who self-place suffer the same negative impacts from placement recommendations as their traditionally placed counterparts? This paper uses longitudinal data with causal inference methods to examine whether directed self-placement has similar negative impacts on student grades and pass rates as mandatory placement schema. We begin with an analysis of over 20,000 student placement records into one of two different placement tracks for first-year writing. Longitudinal and institutional data allow us to control for characteristic variables such as student race, family income, and sex. The results of our regression discontinuity design show that directed self-placement does not negatively impact student grades or pass rate. This may be an improvement for students who place at or near the threshold for developmental/remedial education; However, class, race, and gender-based statistical differences persist in the program at-large, demonstrating that placement technique plays only one part in building a more equitable program.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03350v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jason Godfrey</dc:creator>
    </item>
    <item>
      <title>Estimating link level traffic emissions: enhancing MOVES with open-source data</title>
      <link>https://arxiv.org/abs/2510.03362</link>
      <description>arXiv:2510.03362v1 Announce Type: cross 
Abstract: Open-source data offers a scalable and transparent foundation for estimating vehicle activity and emissions in urban regions. In this study, we propose a data-driven framework that integrates MOVES and open-source GPS trajectory data, OpenStreetMap (OSM) road networks, regional traffic datasets and satellite imagery-derived feature vectors to estimate the link level operating mode distribution and traffic emissions. A neural network model is trained to predict the distribution of MOVES-defined operating modes using only features derived from readily available data. The proposed methodology was applied using open-source data related to 45 municipalities in the Boston Metropolitan area. The "ground truth" operating mode distribution was established using OSM open-source GPS trajectories. Compared to the MOVES baseline, the proposed model reduces RMSE by over 50% for regional scale traffic emissions of key pollutants including CO, NOx, CO2, and PM2.5. This study demonstrates the feasibility of low-cost, replicable, and data-driven emissions estimation using fully open data sources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03362v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lijiao Wang, Muhammad Usama, Haris N. Koutsopoulos, Zhengbing He</dc:creator>
    </item>
    <item>
      <title>Multi-task neural diffusion processes for uncertainty-quantified wind power prediction</title>
      <link>https://arxiv.org/abs/2510.03419</link>
      <description>arXiv:2510.03419v1 Announce Type: cross 
Abstract: Uncertainty-aware wind power prediction is essential for grid integration and reliable wind farm operation. We apply neural diffusion processes (NDPs)-a recent class of models that learn distributions over functions-and extend them to a multi-task NDP (MT-NDP) framework for wind power prediction. We provide the first empirical evaluation of NDPs in real supervisory control and data acquisition (SCADA) data. We introduce a task encoder within MT-NDPs to capture cross-turbine correlations and enable few-shot adaptation to unseen turbines. The proposed MT-NDP framework outperforms single-task NDPs and GPs in terms of point accuracy and calibration, particularly for wind turbines whose behaviour deviates from the fleet average. In general, NDP-based models deliver calibrated and scalable predictions suitable for operational deployment, offering sharper, yet trustworthy, predictive intervals that can support dispatch and maintenance decisions in modern wind farms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03419v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph Rawson, Domniki Ladopoulou, Petros Dellaportas</dc:creator>
    </item>
    <item>
      <title>Bayesian Transfer Learning for High-Dimensional Linear Regression via Adaptive Shrinkage</title>
      <link>https://arxiv.org/abs/2510.03449</link>
      <description>arXiv:2510.03449v1 Announce Type: cross 
Abstract: We introduce BLAST, Bayesian Linear regression with Adaptive Shrinkage for Transfer, a Bayesian multi-source transfer learning framework for high-dimensional linear regression. The proposed analytical framework leverages global-local shrinkage priors together with Bayesian source selection to balance information sharing and regularization. We show how Bayesian source selection allows for the extraction of the most useful data sources, while discounting biasing information that may lead to negative transfer. In this framework, both source selection and sparse regression are jointly accounted for in prediction and inference via Bayesian model averaging. The structure of our model admits efficient posterior simulation via a Gibbs sampling algorithm allowing full posterior inference for the target regression coefficients, making BLAST both computationally practical and inferentially straightforward. Our method achieves more accurate posterior inference for the target than regularization approaches based on target data alone, while offering competitive predictive performance and superior uncertainty quantification compared to current state-of-the-art transfer learning methods. We validate its effectiveness through extensive simulation studies and illustrate its analytical properties when applied to a case study on the estimation of tumor mutational burden from gene expression, using data from The Cancer Genome Atlas (TCGA).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03449v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parsa Jamshidian, Donatello Telesca</dc:creator>
    </item>
    <item>
      <title>Making high-order asymptotics practical: correcting goodness-of-fit test for astronomical count data</title>
      <link>https://arxiv.org/abs/2510.03466</link>
      <description>arXiv:2510.03466v1 Announce Type: cross 
Abstract: The C statistic is a widely used likelihood-ratio statistic for model fitting and goodness-of-fit assessments with Poisson data in high-energy physics and astrophysics. Although it enjoys convenient asymptotic properties, the statistic is routinely applied in cases where its nominal null distribution relies on unwarranted assumptions. Because researchers do not typically carry out robustness checks, their scientific findings are left vulnerable to misleading significance calculations. With an emphasis on low-count scenarios, we present a comprehensive study of the theoretical properties of C statistics and related goodness-of-fit algorithms. We focus on common ``plug-in'' algorithms where moments of C are obtained by assuming the true parameter equals its estimate. To correct such methods, we provide a suite of new principled user-friendly algorithms and well-calibrated p-values that are ready for immediate deployment in the (astro)physics data-analysis pipeline. Using both theoretical and numerical results, we show (a) standard $\chi^2$-based goodness-of-fit assessments are invalid in low-count settings, (b) naive methods (e.g., vanilla bootstrap) result in biased null distributions, and (c) the corrected Z-test based on conditioning and high-order asymptotics gives the best precision with low computational cost. We illustrate our methods via a suite of simulations and applied astrophysical analyses. An open-source Python package is provided in a GitHub repository.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03466v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiaoli Li, Yang Chen, Xiao-Li Meng, David van Dyk, Massimiliano Bonamente, Vinay Kashyap</dc:creator>
    </item>
    <item>
      <title>On Using Large Language Models to Enhance Clinically-Driven Missing Data Recovery Algorithms in Electronic Health Records</title>
      <link>https://arxiv.org/abs/2510.03844</link>
      <description>arXiv:2510.03844v1 Announce Type: cross 
Abstract: Objective: Electronic health records (EHR) data are prone to missingness and errors. Previously, we devised an "enriched" chart review protocol where a "roadmap" of auxiliary diagnoses (anchors) was used to recover missing values in EHR data (e.g., a diagnosis of impaired glycemic control might imply that a missing hemoglobin A1c value would be considered unhealthy). Still, chart reviews are expensive and time-intensive, which limits the number of patients whose data can be reviewed. Now, we investigate the accuracy and scalability of a roadmap-driven algorithm, based on ICD-10 codes (International Classification of Diseases, 10th revision), to mimic expert chart reviews and recover missing values. Materials and Methods: In addition to the clinicians' original roadmap from our previous work, we consider new versions that were iteratively refined using large language models (LLM) in conjunction with clinical expertise to expand the list of auxiliary diagnoses. Using chart reviews for 100 patients from the EHR at an extensive learning health system, we examine algorithm performance with different roadmaps. Using the larger study of $1000$ patients, we applied the final algorithm, which used a roadmap with clinician-approved additions from the LLM. Results: The algorithm recovered as much, if not more, missing data as the expert chart reviewers, depending on the roadmap. Discussion: Clinically-driven algorithms (enhanced by LLM) can recover missing EHR data with similar accuracy to chart reviews and can feasibly be applied to large samples. Extending them to monitor other dimensions of data quality (e.g., plausability) is a promising future direction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03844v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah C. Lotspeich, Abbey Collins, Brian J. Wells, Ashish K. Khanna, Joseph Rigdon, Lucy D'Agostino McGowan</dc:creator>
    </item>
    <item>
      <title>Scale dependence in hidden Markov models for animal movement</title>
      <link>https://arxiv.org/abs/2510.03958</link>
      <description>arXiv:2510.03958v1 Announce Type: cross 
Abstract: Hidden Markov models (HMMs) have been used increasingly to understand how movement patterns of animals arise from behavioural states. An animal is assumed to transition between behavioural states through time, as described by transition probabilities. Within each state, the movement typically follows a discrete-time random walk, where steps between successive observed locations are described in terms of step lengths (related to speed) and turning angles (related to tortuosity). HMMs are discrete-time models, and most of their outputs strongly depend on the temporal resolution of data. We compile known theoretical results about scale dependence in Markov chains and correlated random walks, which are the most common components of HMMs for animal movement. We also illustrate this phenomenon using simulations covering a wide range of biological scenarios. The scale dependence affects not only all model parameters, i.e., the transition probabilities and the movement parameters within each behavioural state, but also the overall classification of movement patterns into states. This highlights the importance of carefully considering the time resolution when drawing conclusions from the results of analysis. In addition, scale dependence generally precludes the analysis of tracking data collected at irregular time intervals, and the comparison (or combination) of data sets with different sampling rates. HMMs remain a valuable tool to answer questions about animal movement and behaviour, as long as these limitations are well understood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03958v1</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Th\'eo Michelot, Emma Storey</dc:creator>
    </item>
    <item>
      <title>Score-based generative emulation of impact-relevant Earth system model outputs</title>
      <link>https://arxiv.org/abs/2510.04358</link>
      <description>arXiv:2510.04358v1 Announce Type: cross 
Abstract: Policy targets evolve faster than the Couple Model Intercomparison Project cycles, complicating adaptation and mitigation planning that must often contend with outdated projections. Climate model output emulators address this gap by offering inexpensive surrogates that can rapidly explore alternative futures while staying close to Earth System Model (ESM) behavior. We focus on emulators designed to provide inputs to impact models. Using monthly ESM fields of near-surface temperature, precipitation, relative humidity, and wind speed, we show that deep generative models have the potential to model jointly the distribution of variables relevant for impacts. The specific model we propose uses score-based diffusion on a spherical mesh and runs on a single mid-range graphical processing unit. We introduce a thorough suite of diagnostics to compare emulator outputs with their parent ESMs, including their probability densities, cross-variable correlations, time of emergence, or tail behavior. We evaluate performance across three distinct ESMs in both pre-industrial and forced regimes. The results show that the emulator produces distributions that closely match the ESM outputs and captures key forced responses. They also reveal important failure cases, notably for variables with a strong regime shift in the seasonal cycle. Although not a perfect match to the ESM, the inaccuracies of the emulator are small relative to the scale of internal variability in ESM projections. We therefore argue that it shows potential to be useful in supporting impact assessment. We discuss priorities for future development toward daily resolution, finer spatial scales, and bias-aware training. Code is made available at https://github.com/shahineb/climemu.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04358v1</guid>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shahine Bouabid, Andre Nogueira Souza, Raffaele Ferrari</dc:creator>
    </item>
    <item>
      <title>Two new approaches to multiple canonical correlation analysis for repeated measures data</title>
      <link>https://arxiv.org/abs/2510.04457</link>
      <description>arXiv:2510.04457v1 Announce Type: cross 
Abstract: In classical canonical correlation analysis (CCA), the goal is to determine the linear transformations of two random vectors into two new random variables that are most strongly correlated. Canonical variables are pairs of these new random variables, while canonical correlations are correlations between these pairs. In this paper, we propose and study two generalizations of this classical method:
  (1) Instead of two random vectors we study more complex data structures that appear in important applications. In these structures, there are $L$ features, each described by $p_l$ scalars, $1 \le l \le L$. We observe $n$ such objects over $T$ time points. We derive a suitable analog of the CCA for such data. Our approach relies on embeddings into Reproducing Kernel Hilbert Spaces, and covers several related data structures as well.
  (2) We develop an analogous approach for multidimensional random processes. In this case, the experimental units are multivariate continuous, square-integrable functions over a given interval. These functions are modeled as elements of a Hilbert space, so in this case, we define the multiple functional canonical correlation analysis, MFCCA.
  We justify our approaches by their application to two data sets and suitable large sample theory. We derive consistency rates for the related transformation and correlation estimators, and show that it is possible to relax two common assumptions on the compactness of the underlying cross-covariance operators and the independence of the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04457v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomasz G\'orecki, Miros{\l}aw Krzy\'sko, Felix Gnettner, Piotr Kokoszka</dc:creator>
    </item>
    <item>
      <title>Gini-based Model Monitoring: A General Framework with an Application to Non-life Insurance Pricing</title>
      <link>https://arxiv.org/abs/2510.04556</link>
      <description>arXiv:2510.04556v1 Announce Type: cross 
Abstract: In a dynamic landscape where portfolios and environments evolve, maintaining the accuracy of pricing models is critical. To the best of our knowledge, this is the first study to systematically examine concept drift in non-life insurance pricing. We (i) provide an overview of the relevant literature and commonly used methodologies, clarify the distinction between virtual drift and concept drift, and explain their implications for long-run model performance; (ii) review and formalize common performance measures, including the Gini index and deviance loss, and articulate their interpretation; (iii) derive the asymptotic distribution of the Gini index, enabling valid inference and hypothesis testing; and (iv) present a standardized monitoring procedure that indicates when refitting is warranted. We illustrate the framework using a modified real-world portfolio with induced concept drift and discuss practical considerations and pitfalls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04556v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alexej Brauer, Paul Menzel</dc:creator>
    </item>
    <item>
      <title>Statistical inference using debiased group graphical lasso for multiple sparse precision matrices</title>
      <link>https://arxiv.org/abs/2510.04683</link>
      <description>arXiv:2510.04683v1 Announce Type: cross 
Abstract: Debiasing group graphical lasso estimates enables statistical inference when multiple Gaussian graphical models share a common sparsity pattern. We analyze the estimation properties of group graphical lasso, establishing convergence rates and model selection consistency under irrepresentability conditions. Based on these results, we construct debiased estimators that are asymptotically Gaussian, allowing hypothesis testing for linear combinations of precision matrix entries across populations. We also investigate regimes where irrepresentibility conditions does not hold, showing that consistency can still be attained in moderately high-dimensional settings. Simulation studies confirm the theoretical results, and applications to real datasets demonstrate the practical utility of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04683v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sayan Ranjan Bhowal, Debashis Paul, Gopal K Basak, Samarjit Das</dc:creator>
    </item>
    <item>
      <title>Ecosystem Recovery to Historical Targets Becomes Unattainable Under Modelled Fishing and Climate in the Barents Sea</title>
      <link>https://arxiv.org/abs/2510.04806</link>
      <description>arXiv:2510.04806v1 Announce Type: cross 
Abstract: Climate change and fisheries jointly shape the resilience of the Barents Sea marine ecosystem, yet the recovery of key fish populations to climate and anthropogenic disturbances requires further investigation. This study examines how fishing pressure and climate change, driven by the NEMO-MEDUSA Earth system model, influence the recovery times of Demersal and Planktivorous fish in the Barents Sea. We used the StrathE2EPolar end-to-end ecosystem model to simulate transient dynamics under increasing fishing pressure scenarios, and quantified recovery times for Demersal, Planktivorous, and ecosystem-wide groups relative to a shifting unfished baseline. Recovery times increased with both fishing intensity and climate change, by as much as 18 years for Demersal fish and 54 years for Planktivorous fish across all fishing scenarios. At the ecosystem level, recovery was constrained by the slow rebound of top predators, many of which experienced biomass collapse under climate change, preventing recovery to a shifting baseline. Our results suggest that fishing pressure in tandem with climate change substantially reduces ecosystem resilience, highlighting the importance of sustainable harvest strategies in a changing climate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04806v1</guid>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Hatton, Jack H Laverick, Neil Banas, Michael Heath</dc:creator>
    </item>
    <item>
      <title>Multiscale modelling of animal movement with persistent dynamics</title>
      <link>https://arxiv.org/abs/2406.15195</link>
      <description>arXiv:2406.15195v2 Announce Type: replace 
Abstract: Wild animals are commonly fitted with trackers that record their position through time, and statistical models for tracking data broadly fall into two categories: models focused on small-scale movement decisions, and models for large-scale spatial distributions. Due to this dichotomy, it is challenging to describe mathematically how animals' distributions arise from their short-term movement patterns, and to combine data sets collected at different scales. We propose a multiscale model of animal movement and space use based on the underdamped Langevin process, widely used in statistical physics. The model is convenient to describe animal movement for three reasons: it is specified in continuous time (such that its parameters are not dependent on an arbitrary time scale), its speed and direction are autocorrelated (similarly to real animal trajectories), and it has a closed form stationary distribution that we can view as a model of long-term space use. We use the common form of a resource selection function for the stationary distribution, to model the environmental drivers behind the animal's movement decisions. We further increase flexibility by allowing movement parameters to be time-varying, and find conditions under which the stationary distribution is preserved. We derive an explicit mathematical link to step selection functions, commonly used in wildlife studies, providing new theoretical results about their scale-dependence. We formulate the underdamped Langevin model as a state-space model and present a computationally efficient method of inference based on the Kalman filter and a marginal likelihood approach for mixed effect extensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15195v2</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Th\'eo Michelot, Ephraim M. Hanks</dc:creator>
    </item>
    <item>
      <title>Elastic Shape Analysis of Movement Data</title>
      <link>https://arxiv.org/abs/2409.13938</link>
      <description>arXiv:2409.13938v3 Announce Type: replace 
Abstract: Osteoarthritis (OA) is a highly prevalent degenerative joint disease, and the knee is the most commonly affected joint. Biomechanical factors, particularly forces exerted during walking, are often measured in modern studies of knee joint injury and OA, and understanding the relationship among biomechanics, clinical profiles, and OA has high clinical relevance. Biomechanical forces are typically represented as curves over time, but a standard practice in biomechanics research is to summarize these curves by a small number of discrete values (or landmarks). The objective of this work is to demonstrate the added value of analyzing full movement curves over conventional discrete summaries. We developed a shape-based representation of variation in full biomechanical curve data from the Intensive Diet and Exercise for Arthritis (IDEA) study (Messier et al., 2009, 2013), and demonstrated through nested model comparisons that our approach, compared to conventional discrete summaries, yields stronger associations with OA severity and OA-related clinical traits. Notably, our work is among the first to quantitatively evaluate the added value of analyzing full movement curves over conventional discrete summaries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13938v3</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. E. Borgert, Jan Hannig, J. D. Tucker, Liubov Arbeeva, Ashley N. Buck, Yvonne M. Golightly, Stephen P. Messier, Amanda E. Nelson, J. S. Marron</dc:creator>
    </item>
    <item>
      <title>A Probabilistic Framework for Estimating the Modal Age at Death</title>
      <link>https://arxiv.org/abs/2411.09800</link>
      <description>arXiv:2411.09800v4 Announce Type: replace 
Abstract: \noindent The modal age at death is an increasingly used measure for understanding longevity and mortality patterns. However, existing estimation methods focus on point estimates, overlooking the inherent variability and uncertainty in mortality data. This study addresses this gap by introducing a probabilistic framework for estimating the probability distribution of the modal age at death. Using a multinomial model for age-specific death counts and leveraging a Gaussian approximation, our methodology captures variability while aligning with the discrete nature of mortality data. Empirical examples are based on mortality data from six different countries. By quantifying uncertainty around the modal age at death and improving robustness to data fluctuations, this approach offers valuable insights for demographic research and policy planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09800v4</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Silvio C. Patricio, Paola Vazquez-Castillo</dc:creator>
    </item>
    <item>
      <title>Cornering in the Water: An Investigation of Dolphin Swimming Performance</title>
      <link>https://arxiv.org/abs/2411.17688</link>
      <description>arXiv:2411.17688v2 Announce Type: replace 
Abstract: Marine mammal biomechanics research has focused on straight-line swimming at consistent speeds, resulting in a lack of knowledge about how animals select movement strategies to balance cost vs performance during tasks like cornering. In this work we examine performance, maneuverability and cost tradeoffs for bottlenose dolphins (Tursiops truncatus) during prescribed swimming. During the task, animals completed two straight-line sections of swimming with a cornering event (180-degree turn). Movement kinematics were measured with a biologging tag (speed, orientation, and depth), and used to estimate the path of the animal during cornering events using a dead reckoning approach. A hydrodynamic model was used to estimate thrust power and energetic cost during lap swimming. Three animals performed the same swimming task, but the path, cornering strategy, and speed varied between individuals. From the kinematic analysis, TT02 was the fastest lap swimmer, with the highest average lap speed, along with the largest energetic cost. TT01 selected a strategy that reduced energetic cost by sacrificing task performance; the animal took about 1.5 times longer to finish each lap compared to TT02 (36 s vs 23 s). TT03 swam more slowly than TT02 (28 s vs 23 s), but at a 50% reduction in cost per lap. The improved efficiency seen in TT03's movement strategy was the result of reducing transient costs during the lap. This included selecting cornering trajectories that balanced trade-offs between distance traveled and speed loss during the turn. Results from this work provide new insight into maneuverability and movement strategies that the dolphins adopt to balance performance and cost during movement, and provide new knowledge for the design and control of bio-inspired marine robotic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17688v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingkai Xia, Junhan Zhang, Ningshan Wang, Gabriel Antoniak, Nicole West, Ding Zhang, Kenneth Alex Shorter</dc:creator>
    </item>
    <item>
      <title>Fast Emulation, Modular Calibration, and Active Learning for Simulators with Functional Response</title>
      <link>https://arxiv.org/abs/2405.16298</link>
      <description>arXiv:2405.16298v2 Announce Type: replace-cross 
Abstract: Scalable surrogate models enable efficient emulation of computer models (or simulators), particularly when dealing with large ensembles of runs. While Gaussian process (GP) models are commonly employed for emulation, they face limitations in scaling to large datasets. Furthermore, when dealing with dense functional output, such as spatial or time-series data, additional complexities arise, requiring careful handling to ensure fast emulation. This work presents a highly scalable emulator for functional data incorporating local Gaussian process regression. The emulator utilizes global GP lengthscale parameter estimates to scale the input space, leading to a substantial improvement in prediction speed. We demonstrate that our fast approximation-based emulator can serve as a viable alternative to a fully Bayesian approach for functional response, while drastically reducing computational costs. The proposed emulator is applied to quickly calibrate a multiphysics continuum hydrodynamics simulator with a large ensemble of 20000 runs. The methods presented are implemented in the R package FlaGP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16298v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Grant Hutchings, Derek Bingham, Kellin Rumsey, Earl Lawrence</dc:creator>
    </item>
    <item>
      <title>Co-factor analysis of citation networks</title>
      <link>https://arxiv.org/abs/2408.14604</link>
      <description>arXiv:2408.14604v2 Announce Type: replace-cross 
Abstract: One compelling use of citation networks is to characterize papers by their relationships to the surrounding literature. We propose a method to characterize papers by embedding them into two distinct "co-factor" spaces: one describing how papers send citations, and the other describing how papers receive citations. This approach presents several challenges. First, older documents cannot cite newer documents, and thus it is not clear that co-factors are even identifiable. We resolve this challenge by developing a co-factor model for asymmetric adjacency matrices with missing lower triangles and showing that identification is possible. We then frame estimation as a matrix completion problem and develop a specialized implementation of matrix completion because prior implementations are memory bound in our setting. Simulations show that our estimator has promising finite sample properties, and that naive approaches fail to recover latent co-factor structure. We leverage our estimator to investigate 255,780 papers published in statistics journals from 1898 to 2024, resulting in the most comprehensive topic model of the statistics literature to date. We find interpretable co-factors corresponding to many statistical subfields, including time series, variable selection, spatial methods, graphical models, GLM(M)s, causal inference, multiple testing, quantile regression, semiparametrics, dimension reduction, and several more.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14604v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/10618600.2024.2394464</arxiv:DOI>
      <dc:creator>Alex Hayes, Karl Rohe</dc:creator>
    </item>
    <item>
      <title>Comparing causal parameters with many treatments and positivity violations</title>
      <link>https://arxiv.org/abs/2410.13522</link>
      <description>arXiv:2410.13522v3 Announce Type: replace-cross 
Abstract: Comparing outcomes across treatments is essential in medicine and public policy. To do so, researchers typically estimate a set of parameters, possibly counterfactual, with each targeting a different treatment. Treatment-specific means are commonly used, but their identification requires a positivity assumption, that every subject has a non-zero probability of receiving each treatment. This is often implausible, especially when treatment can take many values. Causal parameters based on dynamic stochastic interventions offer robustness to positivity violations. However, comparing these parameters may fail to reflect the effects of the underlying target treatments because the parameters can depend on outcomes under non-target treatments. To clarify when two parameters targeting different treatments yield a useful comparison of treatment efficacy, we propose a comparability criterion: if the conditional treatment-specific mean for one treatment is greater than that for another, then the corresponding causal parameter should also be greater. Many standard parameters fail to satisfy this criterion, but we show that only a mild positivity assumption is needed to identify parameters that yield useful comparisons. We then provide two simple examples that satisfy this criterion and are identifiable under the milder positivity assumption: trimmed and smooth trimmed treatment-specific means with multi-valued treatments. For smooth trimmed treatment-specific means, we develop doubly robust-style estimators that attain parametric convergence rates under nonparametric conditions. We illustrate our methods with an analysis of dialysis providers in New York State.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13522v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alec McClean, Yiting Li, Sunjae Bae, Mara A. McAdams-DeMarco, Iv\'an D\'iaz, Wenbo Wu</dc:creator>
    </item>
    <item>
      <title>Poisson multi-Bernoulli mixture filter for trajectory measurements</title>
      <link>https://arxiv.org/abs/2504.08421</link>
      <description>arXiv:2504.08421v2 Announce Type: replace-cross 
Abstract: This paper presents a Poisson multi-Bernoulli mixture (PMBM) filter for multi-target filtering based on sensor measurements that are sets of trajectories in the last two-time step window. The proposed filter, the trajectory measurement PMBM (TM-PMBM) filter, propagates a PMBM density on the set of target states. In prediction, the filter obtains the PMBM density on the set of trajectories over the last two time steps. This density is then updated with the set of trajectory measurements. After the update step, the PMBM posterior on the set of two-step trajectories is marginalised to obtain a PMBM density on the set of target states. The filter provides a closed-form solution for multi-target filtering based on sets of trajectory measurements, estimating the set of target states at the end of each time window. Additionally, the paper proposes computationally lighter alternatives to the TM-PMBM filter by deriving a Poisson multi-Bernoulli (PMB) density through Kullback-Leibler divergence minimisation in an augmented space with auxiliary variables. The performance of the proposed filters are evaluated in a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08421v2</guid>
      <category>eess.SP</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Fontana, \'Angel F. Garc\'ia-Fern\'andez, Simon Maskell</dc:creator>
    </item>
    <item>
      <title>Learning Penalty for Optimal Partitioning via Automatic Feature Extraction</title>
      <link>https://arxiv.org/abs/2505.07413</link>
      <description>arXiv:2505.07413v2 Announce Type: replace-cross 
Abstract: Changepoint detection identifies significant shifts in data sequences, making it important in areas like finance, genetics, and healthcare. The Optimal Partitioning algorithms efficiently detect these changes, using a penalty parameter to limit the changepoints count. Determining the optimal value for this penalty can be challenging. Traditionally, this process involved manually extracting statistical features, such as sequence length or variance to make the prediction. This study proposes a novel approach that uses recurrent networks to learn this penalty directly from raw sequences by automatically extracting features. Experiments conducted on 20 benchmark genomic datasets show that this novel method generally outperforms traditional ones in changepoint detection accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07413v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tung L Nguyen, Toby Hocking</dc:creator>
    </item>
    <item>
      <title>Comparative Analysis of Richardson-Lucy Deconvolution and Data Unfolding with Mean Integrated Square Error Optimization</title>
      <link>https://arxiv.org/abs/2505.10283</link>
      <description>arXiv:2505.10283v3 Announce Type: replace-cross 
Abstract: Two maximum likelihood-based algorithms for unfolding or deconvolution are considered: the Richardson-Lucy method and the Data Unfolding method with Mean Integrated Square Error (MISE) optimization [10]. Unfolding is viewed as a procedure for estimating an unknown probability density function. Both external and internal quality assessment methods can be applied for this purpose. In some cases, external criteria exist to evaluate deconvolution quality. A typical example is the deconvolution of a blurred image, where the sharpness of the restored image serves as an indicator of quality. However, defining such external criteria can be challenging, particularly when a measurement has not been performed previously. In such instances, internal criteria are necessary to assess the quality of the result independently of external information. The article discusses two internal criteria: MISE for the unfolded distribution and the condition number of the correlation matrix of the unfolded distribution. These internal quality criteria are applied to a comparative analysis of the two methods using identical numerical data. The results of the analysis demonstrate the superiority of the Data Unfolding method with MISE optimization over the Richardson-Lucy method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10283v3</guid>
      <category>physics.data-an</category>
      <category>astro-ph.IM</category>
      <category>hep-ex</category>
      <category>nucl-ex</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikolay D. Gagunashvili</dc:creator>
    </item>
    <item>
      <title>An information metric for comparing and assessing informative interim decisions in sequential clinical trials</title>
      <link>https://arxiv.org/abs/2509.04904</link>
      <description>arXiv:2509.04904v3 Announce Type: replace-cross 
Abstract: Group sequential designs enable interim analyses and potential early stopping for efficacy or futility. While these adaptations improve trial efficiency and ethical considerations, they also introduce bias into the adapted analyses. We demonstrate how failing to account for informative interim decisions in the analysis can substantially affect posterior estimates of the treatment effect, often resulting in overly optimistic credible intervals aligned with the stopping decision. Drawing on information theory, we use the Kullback-Leibler divergence to quantify this distortion and highlight its use for post-hoc evaluation of informative interim decisions, with a focus on end-of-study inference. Unlike pointwise comparisons, this measure provides an integrated summary of this distortion on the whole parameter space. By comparing alternative decision boundaries and prior specifications, we illustrate how this measure can improve the understanding of trial results and inform the planning of future adaptive studies. We also introduce an expected version of this metric to support clinicians in choosing decision boundaries. This guidance complements traditional strategies based on type-I error rate control by offering insights into the distortion introduced to the treatment effect at each interim phase. The use of this pre-experimental measure is finally illustrated in a group sequential trial for evaluating a treatment for central nervous system disorders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04904v3</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>G. Caruso, W. F. Rosenberger, P. Mozgunov, N. Flournoy</dc:creator>
    </item>
    <item>
      <title>The Limits of Inference in Complex Systems: When Stochastic Models Become Indistinguishable</title>
      <link>https://arxiv.org/abs/2509.24977</link>
      <description>arXiv:2509.24977v2 Announce Type: replace-cross 
Abstract: Robust inference for stochastic dynamical systems is often hampered by sparse sampling and the absence of closed-form likelihoods. We introduce a Monte Carlo path-inference framework that leverages full-path statistics and bridge processes to deliver reliable parameter estimation and model selection from coarsely sampled time series, without requiring analytical solutions. Crucially, we couple mechanistic stochastic models with their inference procedures to quantify how experimental design -specifically, sampling frequency and dataset size- governs estimator precision and model distinguishability. This analysis reveals optimal sampling regimes and sharp, resolution-dependent limits beyond which competing models become empirically indistinguishable. We validate the approach across four disparate systems -trajectories of optically trapped particles, human microbiome dynamics, social-media topic mentions, and forest population time series- recovering parameters and identifying when inference is fundamentally constrained by measurement resolution, thereby clarifying ongoing debates about dominant noise sources in these systems. Together, these results establish path-based Monte Carlo as a practical, general tool for inference and model discrimination in complex systems and provide principled guidelines for designing measurements that maximize information under real-world constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24977v2</guid>
      <category>cond-mat.stat-mech</category>
      <category>physics.bio-ph</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Javier Aguilar, Miguel A. Mu\~noz, Sandro Azaele</dc:creator>
    </item>
  </channel>
</rss>
