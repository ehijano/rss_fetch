<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 May 2025 04:00:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Towards a Vision-Language Episodic Memory Framework: Large-scale Pretrained Model-Augmented Hippocampal Attractor Dynamics</title>
      <link>https://arxiv.org/abs/2505.04752</link>
      <description>arXiv:2505.04752v1 Announce Type: new 
Abstract: Modeling episodic memory (EM) remains a significant challenge in both neuroscience and AI, with existing models either lacking interpretability or struggling with practical applications. This paper proposes the Vision-Language Episodic Memory (VLEM) framework to address these challenges by integrating large-scale pretrained models with hippocampal attractor dynamics. VLEM leverages the strong semantic understanding of pretrained models to transform sensory input into semantic embeddings as the neocortex, while the hippocampus supports stable memory storage and retrieval through attractor dynamics. In addition, VLEM incorporates prefrontal working memory and the entorhinal gateway, allowing interaction between the neocortex and the hippocampus. To facilitate real-world applications, we introduce EpiGibson, a 3D simulation platform for generating episodic memory data. Experimental results demonstrate the VLEM framework's ability to efficiently learn high-level temporal representations from sensory input, showcasing its robustness, interpretability, and applicability in real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04752v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chong Li, Taiping Zeng, Xiangyang Xue, Jianfeng Feng</dc:creator>
    </item>
    <item>
      <title>Aesthetics Without Semantics</title>
      <link>https://arxiv.org/abs/2505.05331</link>
      <description>arXiv:2505.05331v1 Announce Type: cross 
Abstract: While it is easy for human observers to judge an image as beautiful or ugly, aesthetic decisions result from a combination of entangled perceptual and cognitive (semantic) factors, making the understanding of aesthetic judgements particularly challenging from a scientific point of view. Furthermore, our research shows a prevailing bias in current databases, which include mostly beautiful images, further complicating the study and prediction of aesthetic responses. We address these limitations by creating a database of images with minimal semantic content and devising, and next exploiting, a method to generate images on the ugly side of aesthetic valuations. The resulting Minimum Semantic Content (MSC) database consists of a large and balanced collection of 10,426 images, each evaluated by 100 observers. We next use established image metrics to demonstrate how augmenting an image set biased towards beautiful images with ugly images can modify, or even invert, an observed relationship between image features and aesthetics valuation. Taken together, our study reveals that works in empirical aesthetics attempting to link image content and aesthetic judgements may magnify, underestimate, or simply miss interesting effects due to a limitation of the range of aesthetic values they consider.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05331v1</guid>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <category>stat.CO</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>C. Alejandro Parraga (Comp. Sci. Dept., Engineering School, Universitat Aut\`onoma de Barcelona, Computer Vision Centre, Campus UAB, Bellaterra, 08193, Barcelona, Spain), Olivier Penacchio (Comp. Sci. Dept., Engineering School, Universitat Aut\`onoma de Barcelona, School of Psychology and Neuroscience, University of St Andrews, St Andrews, Fife KY16 9JP, United Kingdom), Marcos Mu\v{n}oz Gonzalez (Comp. Sci. Dept., Engineering School, Universitat Aut\`onoma de Barcelona), Bogdan Raducanu (Computer Vision Centre, Campus UAB, Bellaterra, 08193, Barcelona, Spain), Xavier Otazu (Comp. Sci. Dept., Engineering School, Universitat Aut\`onoma de Barcelona, Computer Vision Centre, Campus UAB, Bellaterra, 08193, Barcelona, Spain)</dc:creator>
    </item>
    <item>
      <title>From Sleep Staging to Spindle Detection: Evaluating End-to-End Automated Sleep Analysis</title>
      <link>https://arxiv.org/abs/2505.05371</link>
      <description>arXiv:2505.05371v1 Announce Type: cross 
Abstract: Automation of sleep analysis, including both macrostructural (sleep stages) and microstructural (e.g., sleep spindles) elements, promises to enable large-scale sleep studies and to reduce variance due to inter-rater incongruencies. While individual steps, such as sleep staging and spindle detection, have been studied separately, the feasibility of automating multi-step sleep analysis remains unclear. Here, we evaluate whether a fully automated analysis using state-of-the-art machine learning models for sleep staging (RobustSleepNet) and subsequent spindle detection (SUMOv2) can replicate findings from an expert-based study of bipolar disorder. The automated analysis qualitatively reproduced key findings from the expert-based study, including significant differences in fast spindle densities between bipolar patients and healthy controls, accomplishing in minutes what previously took months to complete manually. While the results of the automated analysis differed quantitatively from the expert-based study, possibly due to biases between expert raters or between raters and the models, the models individually performed at or above inter-rater agreement for both sleep staging and spindle detection. Our results demonstrate that fully automated approaches have the potential to facilitate large-scale sleep research. We are providing public access to the tools used in our automated analysis by sharing our code and introducing SomnoBot, a privacy-preserving sleep analysis platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05371v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niklas Grieger, Siamak Mehrkanoon, Philipp Ritter, Stephan Bialonski</dc:creator>
    </item>
    <item>
      <title>The role of gap junctions and clustered connectivity in emergent synchronisation patterns of inhibitory neuronal networks</title>
      <link>https://arxiv.org/abs/2402.14592</link>
      <description>arXiv:2402.14592v3 Announce Type: replace 
Abstract: Inhibitory interneurons, ubiquitous in the central nervous system, form networks connected through both chemical synapses and gap junctions. These networks are essential for regulating the activity of principal neurons, especially by inducing temporally patterned dynamic states. We aim to understand the dynamic mechanisms for synchronisation in networks of electrically and chemically coupled interneurons. We use the exact mean-field reduction to derive a neural mass model for both homogeneous and clustered networks. We first analyse a single population of neurons to understand how the two couplings interact with one another. We demonstrate that the network transitions from an asynchronous to a synchronous regime either by increasing the strength of the gap junction connectivity or the strength of the background input current. Conversely, the strength of inhibitory synapses affects the population firing rate, suggesting that electrical and chemical coupling strengths act as complementary mechanisms by which networks can tune synchronous oscillatory behavior. In line with previous work, we confirm that the depolarizing spikelet is crucial for the emergence of synchrony. Furthermore, find that the fast frequency component of the spikelet ensures robustness to heterogeneity. Next, inspired by the existence of multiple interconnected interneuron subtypes in the cerebellum, we analyse networks consisting of two clusters of cell types defined by differing chemical versus electrical coupling strengths. We show that breaking the electrical and chemical coupling symmetry between these clusters induces bistability, so that a transient external input can switch the network between synchronous and asynchronous firing. Together, our results shows the variety of cell-intrinsic and network properties that contribute to synchronisation of interneuronal networks with multiple types of coupling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14592v3</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>H\'el\`ene Todd, Mathieu Desroches, Alex Cayco-Gajic, Boris Gutkin</dc:creator>
    </item>
    <item>
      <title>Active learning of neural population dynamics using two-photon holographic optogenetics</title>
      <link>https://arxiv.org/abs/2412.02529</link>
      <description>arXiv:2412.02529v4 Announce Type: replace 
Abstract: Recent advances in techniques for monitoring and perturbing neural populations have greatly enhanced our ability to study circuits in the brain. In particular, two-photon holographic optogenetics now enables precise photostimulation of experimenter-specified groups of individual neurons, while simultaneous two-photon calcium imaging enables the measurement of ongoing and induced activity across the neural population. Despite the enormous space of potential photostimulation patterns and the time-consuming nature of photostimulation experiments, very little algorithmic work has been done to determine the most effective photostimulation patterns for identifying the neural population dynamics. Here, we develop methods to efficiently select which neurons to stimulate such that the resulting neural responses will best inform a dynamical model of the neural population activity. Using neural population responses to photostimulation in mouse motor cortex, we demonstrate the efficacy of a low-rank linear dynamical systems model, and develop an active learning procedure which takes advantage of low-rank structure to determine informative photostimulation patterns. We demonstrate our approach on both real and synthetic data, obtaining in some cases as much as a two-fold reduction in the amount of data required to reach a given predictive power. Our active stimulation design method is based on a novel active learning procedure for low-rank regression, which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02529v4</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Wagenmaker, Lu Mi, Marton Rozsa, Matthew S. Bull, Karel Svoboda, Kayvon Daie, Matthew D. Golub, Kevin Jamieson</dc:creator>
    </item>
  </channel>
</rss>
