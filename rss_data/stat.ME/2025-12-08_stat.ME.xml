<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 08 Dec 2025 05:02:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Exchangeable Gaussian Processes with application to epidemics</title>
      <link>https://arxiv.org/abs/2512.05227</link>
      <description>arXiv:2512.05227v1 Announce Type: new 
Abstract: We develop a Bayesian non-parametric framework based on multi-task Gaussian processes, appropriate for temporal shrinkage. We focus on a particular class of dynamic hierarchical models to obtain evidence-based knowledge of infectious disease burden. These models induce a parsimonious way to capture cross-dependence between groups while retaining a natural interpretation based on an underlying mean process, itself expressed as a Gaussian process. We analyse distinct types of outbreak data from recent epidemics and find that the proposed models result in improved predictive ability against competing alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05227v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lampros Bouranis, Petros Barmpounakis, Nikolaos Demiris, Konstantinos Kalogeropoulos</dc:creator>
    </item>
    <item>
      <title>A Functional Approach to Testing Overall Effect of Interaction Between DNA Methylation and SNPs</title>
      <link>https://arxiv.org/abs/2512.05276</link>
      <description>arXiv:2512.05276v1 Announce Type: new 
Abstract: We introduce a test for the overall effect of interaction between DNA methylation and a set of single nucleotide polymorphisms (SNPs) on a quantitative phenotype. The developed inference procedure is based on a functional approach that extends existing regression models in functional data analysis. Through extensive simulations, we show that the proposed test effectively controls type I error rates and highlights increased empirical power over existing methods, particularly when multiple interactions are present. The use of the proposed test is illustrated with an application to data from obesity patients and controls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05276v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yvelin Gansou, Karim Oualkacha, Marzia Angela Cremona, Lajmi Lakhal-Chaieb</dc:creator>
    </item>
    <item>
      <title>Does Rerandomization Help Beyond Covariate Adjustment? A Review and Guide for Theory and Practice</title>
      <link>https://arxiv.org/abs/2512.05290</link>
      <description>arXiv:2512.05290v1 Announce Type: new 
Abstract: Rerandomization is a modern experimental design technique that repeatedly randomizes treatment assignments until covariates are deemed balanced between treatment groups. This enhances the precision and coherence of causal effect estimators, mitigates false discoveries from p-hacking, and increases statistical power. Recent work suggests that balancing covariates via rerandomization does not alter the asymptotic precision of covariate-adjusted estimators, thereby making it unclear whether rerandomization is worthwhile if adjusted estimators are used. However, these results have two key caveats. First, these results are asymptotic, leaving finite sample performance unknown. Second, these results focus on precision, while other potential benefits, such as increased coherence among flexible estimators, remain understudied. Hence, in this paper we provide three main contributions: (i) a comprehensive review of the rerandomization literature, covering historical foundations, theoretical developments, and recent methodological advancements, (ii) an extensive simulation study examining finite-sample performance, and (iii) a practical guide for practitioners. Our study compares precision, coherence, power, and coverage of various estimators under rerandomization versus complete randomization. We find rerandomization to be a complementary design strategy that enhances the precision, robustness, and reliability of causal effect estimators, especially for smaller sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05290v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ant\^onio Carlos Herling Ribeiro Junior, Zach Branson</dc:creator>
    </item>
    <item>
      <title>Identifiability and improper solutions in the probabilistic partial least squares regression with unique variance</title>
      <link>https://arxiv.org/abs/2512.05328</link>
      <description>arXiv:2512.05328v1 Announce Type: new 
Abstract: This paper addresses theoretical issues associated with probabilistic partial least squares (PLS) regression. As in the case of factor analysis, the probabilistic PLS regression with unique variance suffers from the issues of improper solutions and lack of identifiability, both of which causes difficulties in interpreting latent variables and model parameters. Using the fact that the probabilistic PLS regression can be viewed as a special case of factor analysis, we apply a norm constraint prescription on the factor loading matrix in the probabilistic PLS regression, which was recently proposed in the context of factor analysis to avoid improper solutions. Then, we prove that the probabilistic PLS regression with this norm constraint is identifiable. We apply the probabilistic PLS regression to data on amino acid mutations in Human Immunodeficiency Virus (HIV) protease to demonstrate the validity of the norm constraint and to confirm the identifiability numerically. Utilizing the proposed constraint enables the visualization of latent variables via a biplot. We also investigate the sampling distribution of the maximum likelihood estimates (MLE) using synthetically generated data. We numerically observe that MLE is consistent and asymptotically normally distributed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05328v1</guid>
      <category>stat.ME</category>
      <category>physics.data-an</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takashi Arai</dc:creator>
    </item>
    <item>
      <title>Optimal Watermark Generation under Type I and Type II Errors</title>
      <link>https://arxiv.org/abs/2512.05333</link>
      <description>arXiv:2512.05333v1 Announce Type: new 
Abstract: Watermarking has recently emerged as a crucial tool for protecting the intellectual property of generative models and for distinguishing AI-generated content from human-generated data. Despite its practical success, most existing watermarking schemes are empirically driven and lack a theoretical understanding of the fundamental trade-off between detection power and generation fidelity. To address this gap, we formulate watermarking as a statistical hypothesis testing problem between a null distribution and its watermarked counterpart. Under explicit constraints on false-positive and false-negative rates, we derive a tight lower bound on the achievable fidelity loss, measured by a general $f$-divergence, and characterize the optimal watermarked distribution that attains this bound. We further develop a corresponding sampling rule that provides an optimal mechanism for inserting watermarks with minimal fidelity distortion. Our result establishes a simple yet broadly applicable principle linking hypothesis testing, information divergence, and watermark generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05333v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hengzhi He, Shirong Xu, Alexander Nemecek, Jiping Li, Erman Ayday, Guang Cheng</dc:creator>
    </item>
    <item>
      <title>A survival analysis of glioma patients using topological features and locations of tumors</title>
      <link>https://arxiv.org/abs/2512.05646</link>
      <description>arXiv:2512.05646v1 Announce Type: new 
Abstract: Tumor shape plays a critical role in influencing both growth and metastasis. We introduce a novel topological radiomic feature derived from persistent homology to characterize tumor shape, focusing on its association with time-to-event outcomes in gliomas. These features effectively capture diverse tumor shape patterns that are not represented by conventional radiomic measures. To incorporate these features into survival analysis, we employ a functional Cox regression model in which the topological features are represented in a functional space. We further include interaction terms between shape features and tumor location to capture lobe-specific effects. This approach enables interpretable assessment of how tumor morphology relates to survival risk. We evaluate the proposed method in two case studies using radiomic images of high-grade and low-grade gliomas. The findings suggest that the topological features serve as strong predictors of survival prognosis, remaining significant after adjusting for clinical variables, and provide additional clinically meaningful insights into tumor behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05646v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhyeong Jang, Tu Dan, Eric Vu, Chul Moon</dc:creator>
    </item>
    <item>
      <title>Efficient sequential Bayesian inference for state-space epidemic models using ensemble data assimilation</title>
      <link>https://arxiv.org/abs/2512.05650</link>
      <description>arXiv:2512.05650v1 Announce Type: new 
Abstract: Estimating latent epidemic states and model parameters from partially observed, noisy data remains a major challenge in infectious disease modeling. State-space formulations provide a coherent probabilistic framework for such inference, yet fully Bayesian estimation is often computationally prohibitive because evaluating the observed-data likelihood requires integration over all latent trajectories. The Sequential Monte Carlo squared (SMC$^2$) algorithm offers a principled approach for joint state and parameter inference, combining an outer SMC sampler over parameters with an inner particle filter that estimates the likelihood up to the current time point. Despite its theoretical appeal, this nested particle filter imposes substantial computational cost, limiting routine use in near-real-time outbreak response. We propose Ensemble SMC$^2$ (eSMC$^2$), a scalable variant that replaces the inner particle filter with an Ensemble Kalman Filter (EnKF) to approximate the incremental likelihood at each observation time. While this substitution introduces bias via a Gaussian approximation, we mitigate finite-sample effects using an unbiased Gaussian density estimator and adapt the EnKF for epidemic data through state-dependent observation variance. This makes our approach particularly suitable for overdispersed incidence data commonly encountered in infectious disease surveillance. Simulation experiments with known ground truth and an application to 2022 United States (U.S.) monkeypox incidence data demonstrate that eSMC$^2$ achieves substantial computational gains while producing posterior estimates comparable to SMC$^2$. The method accurately recovers latent epidemic trajectories and key epidemiological parameters, providing an efficient framework for sequential Bayesian inference from imperfect surveillance data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05650v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dhorasso Temfack, Jason Wyse</dc:creator>
    </item>
    <item>
      <title>Generalised Bayesian Inference using Robust divergences for von Mises-Fisher distribution</title>
      <link>https://arxiv.org/abs/2512.05668</link>
      <description>arXiv:2512.05668v1 Announce Type: new 
Abstract: This paper focusses on robust estimation of location and concentration parameters of the von Mises-Fisher distribution in the Bayesian framework. The von Mises-Fisher (or Langevin) distribution has played a central role in directional statistics. Directional data have been investigated for many decades, and more recently, they have gained increasing attention in diverse areas such as bioinformatics and text data analysis. Although outliers can significantly affect the estimation results even for directional data, the treatment of outliers remains an unresolved and challenging problem. In the frequentist framework, numerous studies have developed robust estimation methods for directional data with outliers, but, in contrast, only a few robust estimation methods have been proposed in the Bayesian framework. In this paper, we propose Bayesian inference based on density power-divergence and $\gamma$-divergence and establish their asymptotic properties and robustness. In addition, the Bayesian approach naturally provides a way to assess estimation uncertainty through the posterior distribution, which is particularly useful for small samples. Furthermore, to carry out the posterior computation, we develop the posterior computation algorithm based on the weighted Bayesian bootstrap for estimating parameters. The effectiveness of the proposed methods is demonstrated through simulation studies. Using two real datasets, we further show that the proposed method provides reliable and robust estimation even in the presence of outliers or data contamination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05668v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoyuki Nakagawa, Yasuhito Tsuruta, Sho Kazari, Kouji Tahata</dc:creator>
    </item>
    <item>
      <title>Empirical Decision Theory</title>
      <link>https://arxiv.org/abs/2512.05677</link>
      <description>arXiv:2512.05677v1 Announce Type: new 
Abstract: Analyzing decision problems under uncertainty commonly relies on idealizing assumptions about the describability of the world, with the most prominent examples being the closed world and the small world assumption. Most assumptions are operationalized by introducing states of the world, conditional on which the decision situation can be analyzed without any remaining uncertainty. Conversely, most classical decision-theoretic approaches are not applicable if the states of the world are inaccessible. We propose a decision model that retains the appeal and simplicity of the original theory, but completely overcomes the need to specify the states of the world explicitly. The main idea of our approach is to address decision problems in a radically empirical way: instead of specifying states and consequences prior to the decision analysis, we only assume a protocol of observed act--consequence pairs as model primitives. We show how optimality in such empirical decision problems can be addressed by using protocol-based empirical choice functions and discuss three approaches for deriving inferential guarantees: (I) consistent statistical estimation of choice sets, (II) consistent statistical testing of choice functions with robustness guarantees, and (III) direct inference for empirical choice functions using credal sets. We illustrate our theory with a proof-of-concept application comparing different prompting strategies in generative AI models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05677v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Christoph Jansen (Lancaster University Leipzig), Georg Schollmeyer (Ludwig-Maximilians-Universit\"at M\"unchen), Thomas Augustin (Ludwig-Maximilians-Universit\"at M\"unchen), Julian Rodemann (CISPA Helmholtz Center for Information Security Saarbr\"ucken)</dc:creator>
    </item>
    <item>
      <title>Model selection with uncertainty in estimating optimal dynamic treatment regimes</title>
      <link>https://arxiv.org/abs/2512.05695</link>
      <description>arXiv:2512.05695v1 Announce Type: new 
Abstract: Optimal dynamic treatment regimes (DTRs), as a key part of precision medicine, have progressively gained more attention recently. To inform clinical decision making, interpretable and parsimonious models for contrast functions are preferred, raising concerns about undue misspecification. It is therefore important to properly evaluate the performance of candidate interpretable models and select the one that best approximates the unknown contrast function. Moreover, since a DTR usually involves multiple decision points, an inaccurate approximation at a later decision point affects its estimation at an earlier decision point when a backward induction algorithm is applied. This paper aims to perform model selection for contrast functions in the context of learning optimal DTRs from observed data. Note that the relative performance of candidate models may heavily depend on the sample size when, for example, the comparison is made between parametric and tree-based models. Therefore, instead of investigating the limiting behavior of each candidate model and developing methods to select asymptotically the `correct' one, we focus on the finite sample performance of each model and attempt to perform model selection under a given sample size. To this end, we adopt the counterfactual cross-validation metric and propose a novel method to estimate the variance of the metric. Supplementing the cross-validation metric with its estimated variance allows us to characterize the uncertainty in model selection under a given sample size and facilitates hypothesis testing associated with a preferred model structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05695v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chunyu Wang, Brian Tom</dc:creator>
    </item>
    <item>
      <title>The Bayesian Way: Uncertainty, Learning, and Statistical Reasoning</title>
      <link>https://arxiv.org/abs/2512.05883</link>
      <description>arXiv:2512.05883v1 Announce Type: new 
Abstract: This paper offers a comprehensive introduction to Bayesian inference, combining historical context, theoretical foundations, and core analytical examples. Beginning with Bayes' theorem and the philosophical distinctions between Bayesian and frequentist approaches, we develop the inferential framework for estimation, interval construction, hypothesis testing, and prediction. Through canonical models, we illustrate how prior information and observed data are formally integrated to yield posterior distributions. We also explore key concepts including loss functions, credible intervals, Bayes factors, identifiability, and asymptotic behavior. While emphasizing analytical tractability in classical settings, we outline modern extensions that rely on simulation-based methods and discuss challenges related to prior specification and model evaluation. Though focused on foundational ideas, this paper sets the stage for applying Bayesian methods in contemporary domains such as hierarchical modeling, nonparametrics, and structured applications in time series, spatial data, networks, and political science. The goal is to provide a rigorous yet accessible entry point for students and researchers seeking to adopt a Bayesian perspective in statistical practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05883v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Sosa, Carlos A. Mart\'inez, Danna Cruz</dc:creator>
    </item>
    <item>
      <title>A Note on the Finite Sample Bias in Time Series Cross-Validation</title>
      <link>https://arxiv.org/abs/2512.05900</link>
      <description>arXiv:2512.05900v1 Announce Type: new 
Abstract: It is well known that model selection via cross validation can be biased for time series models. However, many researchers have argued that this bias does not apply when using cross-validation with vector autoregressions (VAR) or with time series models whose errors follow a martingale-like structure. I show that even under these circumstances, performing cross-validation on time series data will still generate bias in general.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05900v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Amaze Lusompa</dc:creator>
    </item>
    <item>
      <title>Designing an Optimal Sensor Network via Minimizing Information Loss</title>
      <link>https://arxiv.org/abs/2512.05940</link>
      <description>arXiv:2512.05940v1 Announce Type: new 
Abstract: Optimal experimental design is a classic topic in statistics, with many well-studied problems, applications, and solutions. The design problem we study is the placement of sensors to monitor spatiotemporal processes, explicitly accounting for the temporal dimension in our modeling and optimization. We observe that recent advancements in computational sciences often yield large datasets based on physics-based simulations, which are rarely leveraged in experimental design. We introduce a novel model-based sensor placement criterion, along with a highly-efficient optimization algorithm, which integrates physics-based simulations and Bayesian experimental design principles to identify sensor networks that "minimize information loss" from simulated data. Our technique relies on sparse variational inference and (separable) Gauss-Markov priors, and thus may adapt many techniques from Bayesian experimental design. We validate our method through a case study monitoring air temperature in Phoenix, Arizona, using state-of-the-art physics-based simulations. Our results show our framework to be superior to random or quasi-random sampling, particularly with a limited number of sensors. We conclude by discussing practical considerations and implications of our framework, including more complex modeling tools and real-world deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05940v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniel Waxman, Fernando Llorente, Katia Lamer, Petar M. Djuri\'c</dc:creator>
    </item>
    <item>
      <title>Text Rationalization for Robust Causal Effect Estimation</title>
      <link>https://arxiv.org/abs/2512.05373</link>
      <description>arXiv:2512.05373v1 Announce Type: cross 
Abstract: Recent advances in natural language processing have enabled the increasing use of text data in causal inference, particularly for adjusting confounding factors in treatment effect estimation. Although high-dimensional text can encode rich contextual information, it also poses unique challenges for causal identification and estimation. In particular, the positivity assumption, which requires sufficient treatment overlap across confounder values, is often violated at the observational level, when massive text is represented in feature spaces. Redundant or spurious textual features inflate dimensionality, producing extreme propensity scores, unstable weights, and inflated variance in effect estimates. We address these challenges with Confounding-Aware Token Rationalization (CATR), a framework that selects a sparse necessary subset of tokens using a residual-independence diagnostic designed to preserve confounding information sufficient for unconfoundedness. By discarding irrelevant texts while retaining key signals, CATR mitigates observational-level positivity violations and stabilizes downstream causal effect estimators. Experiments on synthetic data and a real-world study using the MIMIC-III database demonstrate that CATR yields more accurate, stable, and interpretable causal effect estimates than existing baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05373v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lijinghua Zhang, Hengrui Cai</dc:creator>
    </item>
    <item>
      <title>Credal and Interval Deep Evidential Classifications</title>
      <link>https://arxiv.org/abs/2512.05526</link>
      <description>arXiv:2512.05526v1 Announce Type: cross 
Abstract: Uncertainty Quantification (UQ) presents a pivotal challenge in the field of Artificial Intelligence (AI), profoundly impacting decision-making, risk assessment and model reliability. In this paper, we introduce Credal and Interval Deep Evidential Classifications (CDEC and IDEC, respectively) as novel approaches to address UQ in classification tasks. CDEC and IDEC leverage a credal set (closed and convex set of probabilities) and an interval of evidential predictive distributions, respectively, allowing us to avoid overfitting to the training data and to systematically assess both epistemic (reducible) and aleatoric (irreducible) uncertainties. When those surpass acceptable thresholds, CDEC and IDEC have the capability to abstain from classification and flag an excess of epistemic or aleatoric uncertainty, as relevant. Conversely, within acceptable uncertainty bounds, CDEC and IDEC provide a collection of labels with robust probabilistic guarantees. CDEC and IDEC are trained using standard backpropagation and a loss function that draws from the theory of evidence. They overcome the shortcomings of previous efforts, and extend the current evidential deep learning literature. Through extensive experiments on MNIST, CIFAR-10 and CIFAR-100, together with their natural OoD shifts (F-MNIST/K-MNIST, SVHN/Intel, TinyImageNet), we show that CDEC and IDEC achieve competitive predictive accuracy, state-of-the-art OoD detection under epistemic and total uncertainty, and tight, well-calibrated prediction regions that expand reliably under distribution shift. An ablation over ensemble size further demonstrates that CDEC attains stable uncertainty estimates with only a small ensemble.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05526v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michele Caprio, Shireen K. Manchingal, Fabio Cuzzolin</dc:creator>
    </item>
    <item>
      <title>Ideal Observer for Segmentation of Dead Leaves Images</title>
      <link>https://arxiv.org/abs/2512.05539</link>
      <description>arXiv:2512.05539v1 Announce Type: cross 
Abstract: The human visual environment is comprised of different surfaces that are distributed in space. The parts of a scene that are visible at any one time are governed by the occlusion of overlapping objects. In this work we consider "dead leaves" models, which replicate these occlusions when generating images by layering objects on top of each other. A dead leaves model is a generative model comprised of distributions for object position, shape, color and texture. An image is generated from a dead leaves model by sampling objects ("leaves") from these distributions until a stopping criterion is reached, usually when the image is fully covered or until a given number of leaves was sampled. Here, we describe a theoretical approach, based on previous work, to derive a Bayesian ideal observer for the partition of a given set of pixels based on independent dead leaves model distributions. Extending previous work, we provide step-by-step explanations for the computation of the posterior probability as well as describe factors that determine the feasibility of practically applying this computation. The dead leaves image model and the associated ideal observer can be applied to study segmentation decisions in a limited number of pixels, providing a principled upper-bound on performance, to which humans and vision algorithms could be compared.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05539v1</guid>
      <category>cs.CV</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Swantje Mahncke, Malte Ott</dc:creator>
    </item>
    <item>
      <title>Design-marginal calibration of Gaussian process predictive distributions: Bayesian and conformal approaches</title>
      <link>https://arxiv.org/abs/2512.05611</link>
      <description>arXiv:2512.05611v1 Announce Type: cross 
Abstract: We study the calibration of Gaussian process (GP) predictive distributions in the interpolation setting from a design-marginal perspective. Conditioning on the data and averaging over a design measure \mu, we formalize \mu-coverage for central intervals and \mu-probabilistic calibration through randomized probability integral transforms. We introduce two methods. cps-gp adapts conformal predictive systems to GP interpolation using standardized leave-one-out residuals, yielding stepwise predictive distributions with finite-sample marginal calibration. bcr-gp retains the GP posterior mean and replaces the Gaussian residual by a generalized normal model fitted to cross-validated standardized residuals. A Bayesian selection rule-based either on a posterior upper quantile of the variance for conservative prediction or on a cross-posterior Kolmogorov-Smirnov criterion for probabilistic calibration-controls dispersion and tail behavior while producing smooth predictive distributions suitable for sequential design. Numerical experiments on benchmark functions compare cps-gp, bcr-gp, Jackknife+ for GPs, and the full conformal Gaussian process, using calibration metrics (coverage, Kolmogorov-Smirnov, integral absolute error) and accuracy or sharpness through the scaled continuous ranked probability score.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05611v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aur\'elien Pion, Emmanuel Vazquez</dc:creator>
    </item>
    <item>
      <title>Developing synthetic microdata through machine learning for firm-level business surveys</title>
      <link>https://arxiv.org/abs/2512.05948</link>
      <description>arXiv:2512.05948v1 Announce Type: cross 
Abstract: Public-use microdata samples (PUMS) from the United States (US) Census Bureau on individuals have been available for decades. However, large increases in computing power and the greater availability of Big Data have dramatically increased the probability of re-identifying anonymized data, potentially violating the pledge of confidentiality given to survey respondents. Data science tools can be used to produce synthetic data that preserve critical moments of the empirical data but do not contain the records of any existing individual respondent or business. Developing public-use firm data from surveys presents unique challenges different from demographic data, because there is a lack of anonymity and certain industries can be easily identified in each geographic area. This paper briefly describes a machine learning model used to construct a synthetic PUMS based on the Annual Business Survey (ABS) and discusses various quality metrics. Although the ABS PUMS is currently being refined and results are confidential, we present two synthetic PUMS developed for the 2007 Survey of Business Owners, similar to the ABS business data. Econometric replication of a high impact analysis published in Small Business Economics demonstrates the verisimilitude of the synthetic data to the true data and motivates discussion of possible ABS use cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05948v1</guid>
      <category>cs.LG</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jorge Cisneros Paz, Timothy Wojan, Matthew Williams, Jennifer Ozawa, Robert Chew, Kimberly Janda, Timothy Navarro, Michael Floyd, Christine Task, Damon Streat</dc:creator>
    </item>
    <item>
      <title>Multivariate binary probability distribution in the Grassmann formalism</title>
      <link>https://arxiv.org/abs/2009.08482</link>
      <description>arXiv:2009.08482v2 Announce Type: replace 
Abstract: We propose a probability distribution for multivariate binary random variables. The probability distribution is expressed as principal minors of the parameter matrix, which is a matrix analogous to the inverse covariance matrix in the multivariate Gaussian distribution. In our model, the partition function, central moments, and the marginal and conditional distributions are expressed analytically. That is, summation over all possible states is not necessary for obtaining the partition function and various expected values, which is a problem with the conventional multivariate Bernoulli distribution. The proposed model has many similarities to the multivariate Gaussian distribution. For example, the marginal and conditional distributions are expressed in terms of the parameter matrix and its inverse matrix, respectively. That is, the inverse matrix represents a sort of partial correlation. The proposed distribution can be derived using Grassmann numbers, anticommuting numbers. Analytical expressions for the marginal and conditional distributions are also useful in generating random numbers for multivariate binary variables. Hence, we investigated sampling distributions of parameter estimates using synthetic datasets. The computational complexity of maximum likelihood estimation from observed data is proportional to the number of unique observed states, not to the number of all possible states as is required in the case of the conventional multivariate Bernoulli distribution. We empirically observed that the sampling distributions of the maximum likelihood estimates appear to be consistent and asymptotically normal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2009.08482v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevE.103.062104</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. E 103, 062104 (2021)</arxiv:journal_reference>
      <dc:creator>Takashi Arai</dc:creator>
    </item>
    <item>
      <title>Multi-Task Learning for Sparsity Pattern Heterogeneity: Statistical and Computational Perspectives</title>
      <link>https://arxiv.org/abs/2212.08697</link>
      <description>arXiv:2212.08697v3 Announce Type: replace 
Abstract: We consider a problem in Multi-Task Learning (MTL) where multiple linear models are jointly trained on a collection of datasets ("tasks"). A key novelty of our framework is that it allows the sparsity pattern of regression coefficients and the values of non-zero coefficients to differ across tasks while still leveraging partially shared structure. Our methods encourage models to share information across tasks through separately encouraging 1) coefficient supports, and/or 2) nonzero coefficient values to be similar. This allows models to borrow strength during variable selection even when non-zero coefficient values differ across tasks. We propose a novel mixed-integer programming formulation for our estimator. We develop custom scalable algorithms based on block coordinate descent and combinatorial local search to obtain high-quality (approximate) solutions for our estimator. Additionally, we propose a novel exact optimization algorithm to obtain globally optimal solutions. We investigate the theoretical properties of our estimators. We formally show how our estimators leverage the shared support information across tasks to achieve better variable selection performance. We evaluate the performance of our methods in simulations and two biomedical applications. Our proposed approaches appear to outperform other sparse MTL methods in variable selection and prediction accuracy. We provide the sMTL package on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.08697v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kayhan Behdin, Gabriel Loewinger, Kenneth T. Kishida, Giovanni Parmigiani, Rahul Mazumder</dc:creator>
    </item>
    <item>
      <title>Characterizing quantile-varying covariate effects under the accelerated failure time model</title>
      <link>https://arxiv.org/abs/2301.03057</link>
      <description>arXiv:2301.03057v2 Announce Type: replace 
Abstract: An important task in survival analysis is choosing a structure for the relationship between covariates of interest and the time-to-event outcome. For example, the accelerated failure time (AFT) model structures each covariate effect as a constant multiplicative shift in the outcome distribution across all survival quantiles. Though parsimonious, this structure cannot detect or capture effects that differ across quantiles of the distribution, a limitation that is analogous to only permitting proportional hazards in the Cox model. To address this, we propose a general framework for quantile-varying multiplicative effects under the AFT model. Specifically, we embed flexible regression structures within the AFT model, and derive a novel formula for interpretable effects on the quantile scale. A regression standardization scheme based on the g-formula is proposed to enable estimation of both covariate-conditional and marginal effects for an exposure of interest. We implement a user-friendly Bayesian approach for estimation and quantification of uncertainty, while accounting for left truncation and complex censoring. We emphasize the intuitive interpretation of this model through numerical and graphical tools, and illustrate its performance through simulation and application to a study of Alzheimer's disease and dementia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.03057v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/biostatistics/kxac052</arxiv:DOI>
      <arxiv:journal_reference>Biostatistics. 2024 Apr 15;25(2):449-467 PMID: 36610077; PMCID: PMC11484523</arxiv:journal_reference>
      <dc:creator>Harrison T. Reeder, Kyu Ha Lee, Sebastien Haneuse</dc:creator>
    </item>
    <item>
      <title>Detection of Model-based Planted Pseudo-cliques in Random Dot Product Graphs by the Adjacency Spectral Embedding and the Graph Encoder Embedding</title>
      <link>https://arxiv.org/abs/2312.11054</link>
      <description>arXiv:2312.11054v2 Announce Type: replace 
Abstract: In this paper, we explore the capability of both the Adjacency Spectral Embedding (ASE) and the Graph Encoder Embedding (GEE) for capturing an embedded pseudo-clique structure in the random dot product graph setting. In both theory and experiments, we demonstrate that, in the absence of additional clean (i.e., without the implanted pseudo-clique) network data, this pairing of model and methods can yield worse results than the best existing spectral clique detection methods. However, these methods can be used to asymptotically localize the pseudo-cliques if additional clean, independent network data is provided. This demonstrates at once the methods' potential ability/inability to capture modestly sized pseudo-cliques and the methods' robustness to the model contamination giving rise to the pseudo-clique structure. To further enrich our analysis, we also consider the Variational Graph Auto-Encoder (VGAE) model in our simulation and real data experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.11054v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tong Qi, Vince Lyzinski</dc:creator>
    </item>
    <item>
      <title>Powerful Large-scale Inference in High Dimensional Mediation Analysis</title>
      <link>https://arxiv.org/abs/2402.13933</link>
      <description>arXiv:2402.13933v4 Announce Type: replace 
Abstract: In genome-wide epigenetic studies, exposures (e.g., Single Nucleotide Polymorphisms) affect outcomes (e.g., gene expression) through intermediate variables such as DNA methylation. Mediation analysis offers a way to study these intermediate variables and identify the presence or absence of causal mediation effects. Testing for mediation effects lead to a composite null hypothesis. Existing methods like the Sobel's test or the Max-P test are often underpowered because 1) statistical inference is often conducted based on distributions determined under a subset of the null and 2) they are not designed to shoulder the multiple testing burden. To tackle these issues, we introduce a technique called MLFDR (Mediation Analysis using Local False Discovery Rates) for high dimensional mediation analysis, which uses the local False Discovery Rates based on the coefficients of the structural equation model specifying the mediation relationship to construct a rejection region. We have shown theoretically as well as through simulation studies that in the high-dimensional setting, the new method of identifying the mediating variables controls the FDR asymptotically and performs better with respect to power than several existing methods such as DACT (Liu et al.)and JS-mixture (Dai et al).</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13933v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Asmita Roy, Xianyang Zhang</dc:creator>
    </item>
    <item>
      <title>Parameter-Specific Bias Diagnostics in Random-Effects Panel Data Models</title>
      <link>https://arxiv.org/abs/2412.20555</link>
      <description>arXiv:2412.20555v3 Announce Type: replace 
Abstract: The Hausman specification test detects inconsistency of the random-effects estimator by comparing it with an alternative fixed-effects estimator. This note shows how a recently proposed bias diagnostic for linear mixed models can complement this test in random-effects panel-data applications. The diagnostic delivers parameter-specific internal estimates of finite-sample bias of the random-effects estimator, together with permutation-based $p$-values, from a single fitted random-effects model. We illustrate its use in a gasoline-demand panel and in a value-added model for teacher evaluation, using publicly available R packages, and we discuss how the resulting bias summaries can be incorporated into routine practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20555v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andrew T. Karl</dc:creator>
    </item>
    <item>
      <title>Conformal novelty detection for replicate point patterns with FDR or FWER control</title>
      <link>https://arxiv.org/abs/2501.18195</link>
      <description>arXiv:2501.18195v3 Announce Type: replace 
Abstract: Monte Carlo tests are widely used for computing valid p-values without requiring known distributions of test statistics. When performing multiple Monte Carlo tests, it is essential to maintain control of the type I error. Some techniques for multiplicity control pose requirements on the joint distribution of the p-values, for instance independence, which can be computationally intensive to achieve using na\"ive multiple Monte Carlo testing. We highlight in this work that multiple Monte Carlo testing is an instance of conformal novelty detection. Leveraging this insight enables a more efficient multiple Monte Carlo testing procedure, avoiding excessive simulations while still ensuring exact control over the false discovery rate or the family-wise error rate. We call this approach conformal multiple Monte Carlo testing. The performance is investigated in the context of global envelope tests for point pattern data through a simulation study and an application to a sweat gland data set. Results reveal that with a fixed number of simulations under the null hypothesis, our proposed method yields substantial improvements in power of the testing procedure as compared to the na\"ive multiple Monte Carlo testing procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18195v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.spasta.2025.100924</arxiv:DOI>
      <arxiv:journal_reference>Spatial Statistics, vol. 69, 2025</arxiv:journal_reference>
      <dc:creator>Christophe A. N. Biscio, Adrien Mazoyer, Martin V. Vejling</dc:creator>
    </item>
    <item>
      <title>Simulating transgenerational hologenomes under selection with RITHMS</title>
      <link>https://arxiv.org/abs/2502.07366</link>
      <description>arXiv:2502.07366v4 Announce Type: replace 
Abstract: A holobiont is made up of a host organism together with its microbiota. In the context of animal breeding, the holobiont can be viewed as the single unit upon which selection operates. Therefore, integrating microbiota data into genomic prediction models may be a promising approach to improve predictions of phenotypic and genetic values. Nevertheless, there is a paucity of hologenomic transgenerational data to address this hypothesis, and thus to fill this gap, we propose a new simulation framework. Our approach, an R Implementation of a Transgenerational Hologenomic Model-based Simulator (RITHMS) is an open-source package. It builds upon simulated transgenerational genotypes from the Modular Breeding Program Simulator (MoBPS) package and incorporates distinctive characteristics of the microbiota, notably vertical and horizontal transmission as well as modulation due to the environment and host genetics. In addition, RITHMS can account for a variety of selection strategies and is adaptable to different genetic architectures. We simulated transgenerational hologenomic data using RITHMS under a wide variety of scenarios, varying heritability, microbiability, and microbiota transmissibility. We found that simulated data accurately preserved key characteristics across generations, notably microbial diversity metrics, exhibited the expected behavior in terms of correlation between taxa and of modulation of vertical and horizontal transmission, response to environmental effects and the evolution of phenotypic values depending on selection strategy. Our results support the relevance of our simulation framework and illustrate its possible use for building a selection index balancing genetic gain and microbial diversity and for evaluating the impact of partially observed microbiota data. RITHMS is an advanced, flexible tool for generating transgenerational hologenomes under selection that incorporate the complex interplay between genetics, microbiota and environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07366v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sol\`ene Pety (MaIAGE, GABI, INRAE), Ingrid David (GenPhySE, INRAE), Andrea Rau (GABI, INRAE), Mahendra Mariadassou (MaIAGE, INRAE)</dc:creator>
    </item>
    <item>
      <title>Learning Joint Graphical Model with Computational Efficiency, Dynamic Regularization, and Adaptation</title>
      <link>https://arxiv.org/abs/2503.18722</link>
      <description>arXiv:2503.18722v3 Announce Type: replace 
Abstract: Multi-sourced datasets are common in studies of variable interactions, for example, individual-level fMRI integration, cross-domain recommendation, etc, where each source induces a related but distinct dependency structure. Joint learning of multiple graphical models (i.e., multiple precision matrices) has emerged as an important tool in analyzing such data. Unlike separate learning, joint learning can leverage shared structural patterns across graphs to yield more accurate results. In this paper, we present an efficient and adaptive method named MIGHT (\textbf{M}ulti-task \textbf{I}terative \textbf{G}raphical \textbf{H}ard \textbf{T}hresholding) to estimate multiple graphs jointly. We reformulate the joint model into a series of multi-task learning problems through a column-by-column manner, and solve these problems using a dynamic regularized algorithm based on iterative hard thresholding. This framework is inherently parallelizable and therefore efficient in computation. Theoretically, we derive the non-asymptotic error bound for the resulting estimator. Furthermore, the proposed algorithm is adaptive to heterogeneous column-wise signal strengths: for nodes with strong signals, our estimator achieves improved error bounds and selection consistency adaptively, and also exhibits asymptotic normality -- properties rarely explored in existing joint learning methods. The performance of our method is illustrated through numerical simulations and real data analysis on a cancer gene-expression RNA-seq dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18722v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shixiang Liu, Yanhang Zhang, Zhifan Li, Jianxin Yin</dc:creator>
    </item>
    <item>
      <title>Interpretable additive model for analyzing high-dimensional functional time series</title>
      <link>https://arxiv.org/abs/2504.19904</link>
      <description>arXiv:2504.19904v2 Announce Type: replace 
Abstract: High-dimensional functional time series offers a powerful framework for extending functional time series analysis to settings with multiple simultaneous dimensions, capturing both temporal dynamics and cross-sectional dependencies. We propose a novel, interpretable additive model tailored for such data, designed to deliver both high predictive accuracy and clear interpretability. The model features bivariate coefficient surfaces to represent relationships across panel dimensions, with sparsity introduced via penalized smoothing and group bridge regression. This enables simultaneous estimation of the surfaces and identification of significant inter-dimensional effects. Through Monte Carlo simulations and an empirical application to Japanese subnational age-specific mortality rates, we demonstrate the proposed model's superior forecasting performance and interpretability compared to existing functional time series approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19904v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haixu Wang, Tianyu Guan, Han Lin Shang</dc:creator>
    </item>
    <item>
      <title>Pairwise Difference Representations of Moments: Gini and Generalized Lagrange identities</title>
      <link>https://arxiv.org/abs/2510.22714</link>
      <description>arXiv:2510.22714v2 Announce Type: replace 
Abstract: We provide pairwise-difference (Gini-type) representations of higher-order central moments for both general random variables and empirical moments. Such representations do not require a measure of location. For third and fourth moments, this yields pairwise-difference representations of skewness and kurtosis coefficients. We show that all central moments possess such representations, so no reference to the mean is needed for moments of any order. This is done by considering i.i.d. replications of the random variables considered, by observing that central moments can be interpreted as covariances between a random variable and powers of the same variable, and by giving recursions which link the pairwise-difference representation of any moment to lower order ones. Numerical summation identities are deduced. Through a similar approach, we give analogues of the Lagrange and Binet-Cauchy identities for general random variables, along with a simple derivation of the classic Cauchy-Schwarz inequality for covariances. Finally, an application to unbiased estimation of centered moments is discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22714v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jean-Marie Dufour, Abderrahim Taamouti, Meilin Tong</dc:creator>
    </item>
    <item>
      <title>Finite-Sample Valid Rank Confidence Sets for a Broad Class of Statistical and Machine Learning Models</title>
      <link>https://arxiv.org/abs/2512.00316</link>
      <description>arXiv:2512.00316v2 Announce Type: replace 
Abstract: Ranking populations such as institutions based on certain characteristics is often of interest, and these ranks are typically estimated using samples drawn from the populations. Due to sample randomness, it is important to quantify the uncertainty associated with the estimated ranks. This becomes crucial when latent characteristics are poorly separated and where many rank estimates may be incorrectly ordered. Understanding uncertainty can help quantify and mitigate these issues and provide a fuller picture. However, this task is especially challenging because the rank parameters are discrete and the central limit theorem does not apply to the rank estimates. In this article, we propose a Repro Samples Method to address this nontrivial inference problem by developing a confidence set for the true, unobserved population ranks. This method provides finite-sample coverage guarantees and is broadly applicable to ranking problems. The effectiveness of the method is illustrated and compared with several published large sample ranking approaches using simulation studies and real data examples involving samples both from traditional statistical models and modern data science algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00316v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Onrina Chandra, Min-ge Xie</dc:creator>
    </item>
    <item>
      <title>Sequential Randomization Tests Using E-values: A Betting Approach for Clinical Trials</title>
      <link>https://arxiv.org/abs/2512.04366</link>
      <description>arXiv:2512.04366v2 Announce Type: replace 
Abstract: Sequential monitoring of randomized trials traditionally relies on parametric assumptions or asymptotic approximations. We present a nonparametric sequential test, the randomization e-process (e-RT), that derives validity solely from the randomization mechanism. Using a betting framework, e-RT constructs a test martingale by sequentially wagering on treatment assignments given observed outcomes. Under the null hypothesis of no treatment effect, the expected wealth cannot grow, guaranteeing anytime-valid Type I error control regardless of stopping rule. We prove validity and present simulation studies demonstrating calibration and power. The e-RT provides a conservative, assumption-free complement to model-based sequential analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04366v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fernando G Zampieri</dc:creator>
    </item>
    <item>
      <title>Efficient adjustment for complex covariates: Gaining efficiency with DOPE</title>
      <link>https://arxiv.org/abs/2402.12980</link>
      <description>arXiv:2402.12980v2 Announce Type: replace-cross 
Abstract: Covariate adjustment is a ubiquitous method used to estimate the average treatment effect (ATE) from observational data. Assuming a known graphical structure of the data generating model, recent results give graphical criteria for optimal adjustment, which enables efficient estimation of the ATE. However, graphical approaches are challenging for high-dimensional and complex data, and it is not straightforward to specify a meaningful graphical model of non-Euclidean data such as texts. We propose a new framework that accommodates adjustment for any subset of information expressed by the covariates, and we show that the information that is minimally sufficient for prediction of the outcome given the treatment is also most efficient for adjustment.
  Based on our theoretical results, we propose the Debiased Outcome-adapted Propensity Estimator (DOPE) for efficient estimation of the ATE, and we provide asymptotic results for DOPE under general conditions. Compared to the augmented inverse propensity weighted (AIPW) estimator, DOPE can retain its efficiency even when the covariates are highly predictive of treatment. We illustrate this with a single-index model, and with an implementation of DOPE based on neural networks, we demonstrate its performance on simulated and real data. Our results show that DOPE provides an efficient and robust methodology for ATE estimation in various observational settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12980v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Mangulad Christgau, Anton Rask Lundborg, Niels Richard Hansen</dc:creator>
    </item>
  </channel>
</rss>
