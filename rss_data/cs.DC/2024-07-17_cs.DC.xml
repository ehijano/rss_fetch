<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Jul 2024 01:44:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 17 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Quality Scalable Quantization Methodology for Deep Learning on Edge</title>
      <link>https://arxiv.org/abs/2407.11260</link>
      <description>arXiv:2407.11260v1 Announce Type: new 
Abstract: Deep Learning Architectures employ heavy computations and bulk of the computational energy is taken up by the convolution operations in the Convolutional Neural Networks. The objective of our proposed work is to reduce the energy consumption and size of CNN for using machine learning techniques in edge computing on ubiquitous computing devices. We propose Systematic Quality Scalable Design Methodology consisting of Quality Scalable Quantization on a higher abstraction level and Quality Scalable Multipliers at lower abstraction level. The first component consists of parameter compression where we approximate representation of values in filters of deep learning models by encoding in 3 bits. A shift and scale based on-chip decoding hardware is proposed which can decode these 3-bit representations to recover approximate filter values. The size of the DNN model is reduced this way and can be sent over a communication channel to be decoded on the edge computing devices. This way power is reduced by limiting data bits by approximation. In the second component we propose a quality scalable multiplier which reduces the number of partial products by converting numbers in canonic sign digit representations and further approximating the number by reducing least significant bits. These quantized CNNs provide almost same ac-curacy as network with original weights with little or no fine-tuning. The hardware for the adaptive multipliers utilize gate clocking for reducing energy consumption during multiplications. The proposed methodology greatly reduces the memory and power requirements of DNN models making it a feasible approach to deploy Deep Learning on edge computing. The experiments done on LeNet and ConvNets show an increase upto 6% of zeros and memory savings upto 82.4919% while keeping the accuracy near the state of the art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11260v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Salman Abdul Khaliq, Rehan Hafiz</dc:creator>
    </item>
    <item>
      <title>Edge-Mapping of Service Function Trees for Sensor Event Processing</title>
      <link>https://arxiv.org/abs/2407.11302</link>
      <description>arXiv:2407.11302v1 Announce Type: new 
Abstract: Fog computing offers increased performance and efficiency for Industrial Internet of Things (IIoT) applications through distributed data processing in nearby proximity to sensors. Given resource constraints and their contentious use in IoT networks, current strategies strive to optimise which data processing tasks should be selected to run on fog devices. In this paper, we advance a more effective data processing architecture for optimisation purposes. Specifically, we consider the distinct functions of sensor data streaming, multi-stream data aggregation and event handling, required by IoT applications for identifying actionable events. We retrofit this event processing pipeline into a logical architecture, structured as a service function tree (SFT), comprising service function chains. We present a novel algorithm for mapping the SFT into a fog network topology in which nodes selected to process SFT functions (microservices) have the requisite resource capacity and network speed to meet their event processing deadlines. We used simulations to validate the algorithm's effectiveness in finding a successful SFT mapping to a physical network. Overall, our approach overcomes the bottlenecks of single service placement strategies for fog computing through composite service placements of SFTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11302v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Babar Shahzaad, Alistair Barros, Colin Fidge</dc:creator>
    </item>
    <item>
      <title>Paralleling and Accelerating Arc Consistency Enforcement with Recurrent Tensor Computations</title>
      <link>https://arxiv.org/abs/2407.11388</link>
      <description>arXiv:2407.11388v1 Announce Type: new 
Abstract: We propose a new arc consistency enforcement paradigm that transforms arc consistency enforcement into recurrent tensor operations. In each iteration of the recurrence, all involved processes can be fully parallelized with tensor operations. And the number of iterations is quite small. Based on these benefits, the resulting algorithm fully leverages the power of parallelization and GPU, and therefore is extremely efficient on large and densely connected constraint networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11388v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingqi Yang</dc:creator>
    </item>
    <item>
      <title>Octopus: Experiences with a Hybrid Event-Driven Architecture for Distributed Scientific Computing</title>
      <link>https://arxiv.org/abs/2407.11432</link>
      <description>arXiv:2407.11432v1 Announce Type: new 
Abstract: Scientific research increasingly relies on distributed computational resources, storage systems, networks, and instruments, ranging from HPC and cloud systems to edge devices. Event-driven architecture (EDA) benefits applications targeting distributed research infrastructures by enabling the organization, communication, processing, reliability, and security of events generated from many sources. To support the development of scientific EDA, we introduce Octopus, a hybrid, cloud-to-edge event fabric designed to link many local event producers and consumers with cloud-hosted brokers. Octopus can be scaled to meet demand, permits the deployment of highly available Triggers for automatic event processing, and enforces fine-grained access control. We identify requirements in self-driving laboratories, scientific data automation, online task scheduling, epidemic modeling, and dynamic workflow management use cases, and present results demonstrating Octopus' ability to meet those requirements. Octopus supports producing and consuming events at a rate of over 4.2 M and 9.6 M events per second, respectively, from distributed clients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11432v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haochen Pan, Ryan Chard, Sicheng Zhou, Alok Kamatar, Rafael Vescovi, Valerie Hayot-Sasson, Andr\'e Bauer, Maxime Gonthier, Kyle Chard, Ian Foster</dc:creator>
    </item>
    <item>
      <title>Bringing Auto-tuning to HIP: Analysis of Tuning Impact and Difficulty on AMD and Nvidia GPUs</title>
      <link>https://arxiv.org/abs/2407.11488</link>
      <description>arXiv:2407.11488v1 Announce Type: new 
Abstract: Many studies have focused on developing and improving auto-tuning algorithms for Nvidia Graphics Processing Units (GPUs), but the effectiveness and efficiency of these approaches on AMD devices have hardly been studied. This paper aims to address this gap by introducing an auto-tuner for AMD's HIP. We do so by extending Kernel Tuner, an open-source Python library for auto-tuning GPU programs. We analyze the performance impact and tuning difficulty for four highly-tunable benchmark kernels on four different GPUs: two from Nvidia and two from AMD. Our results demonstrate that auto-tuning has a significantly higher impact on performance on AMD compared to Nvidia (10x vs 2x). Additionally, we show that applications tuned for Nvidia do not perform optimally on AMD, underscoring the importance of auto-tuning specifically for AMD to achieve high performance on these GPUs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11488v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Milo Lurati, Stijn Heldens, Alessio Sclocco, Ben van Werkhoven</dc:creator>
    </item>
    <item>
      <title>Reducing Tail Latencies Through Environment- and Neighbour-aware Thread Management</title>
      <link>https://arxiv.org/abs/2407.11582</link>
      <description>arXiv:2407.11582v1 Announce Type: new 
Abstract: Application tail latency is a key metric for many services, with high latencies being linked directly to loss of revenue. Modern deeply-nested micro-service architectures exacerbate tail latencies, increasing the likelihood of users experiencing them. In this work, we show how CPU overcommitment by OS threads leads to high tail latencies when applications are under heavy load. CPU overcommitment can arise from two operational factors: incorrectly determining the number of CPUs available when under a CPU quota, and the ignorance of neighbour applications and their CPU usage. We discuss different languages' solutions to obtaining the CPUs available, evaluating the impact, and discuss opportunities for a more unified language-independent interface to obtain the number of CPUs available. We then evaluate the impact of neighbour usage on tail latency and introduce a new neighbour-aware threadpool, the friendlypool, that dynamically avoids overcommitment. In our evaluation, the friendlypool reduces maximum worker latency by up to $6.7\times$ at the cost of decreasing throughput by up to $1.4\times$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11582v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Jeffery, Chris Jensen, Richard Mortier</dc:creator>
    </item>
    <item>
      <title>Personalized Conversational Travel Assistant powered by Generative AI</title>
      <link>https://arxiv.org/abs/2407.11830</link>
      <description>arXiv:2407.11830v1 Announce Type: new 
Abstract: The Tourism and Destination Management Organization (DMO) industry is rapidly evolving to adapt to new technologies and traveler expectations. Generative Artificial Intelligence (AI) offers an astonishing and innovative opportunity to enhance the tourism experience by providing personalized, interactive and engaging assistance. In this article, we propose a generative AI-based chatbot for tourism assistance. The chatbot leverages AI ability to generate realistic and creative texts, adopting the friendly persona of the well-known Italian all-knowledgeable aunties, to provide tourists with personalized information, tailored and dynamic pre, during and post recommendations and trip plans and personalized itineraries, using both text and voice commands, and supporting different languages to satisfy Italian and foreign tourists expectations. This work is under development in the Molise CTE research project, funded by the Italian Minister of the Economic Growth (MIMIT), with the aim to leverage the best emerging technologies available, such as Cloud and AI to produce state of the art solutions in the Smart City environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11830v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexio Cassani, Michele Ruberl, Antonio Salis, Giacomo Giannese, Gianluca Boanelli</dc:creator>
    </item>
    <item>
      <title>Hydra: Brokering Cloud and HPC Resources to Support the Execution of Heterogeneous Workloads at Scale</title>
      <link>https://arxiv.org/abs/2407.11967</link>
      <description>arXiv:2407.11967v1 Announce Type: new 
Abstract: Scientific discovery increasingly depends on middleware that enables the execution of heterogeneous workflows on heterogeneous platforms One of the main challenges is to design software components that integrate within the existing ecosystem to enable scale and performance across cloud and high-performance computing HPC platforms Researchers are met with a varied computing landscape which includes services available on commercial cloud platforms data and network capabilities specifically designed for scientific discovery on government-sponsored cloud platforms and scale and performance on HPC platforms We present Hydra an intra cross-cloud HPC brokering system capable of concurrently acquiring resources from commercial private cloud and HPC platforms and managing the execution of heterogeneous workflow applications on those resources This paper offers four main contributions (1) the design of brokering capabilities in the presence of task platform resource and middleware heterogeneity; (2) a reference implementation of that design with Hydra; (3) an experimental characterization of Hydra s overheads and strong weak scaling with heterogeneous workloads and platforms and, (4) the implementation of a workflow that models sea rise with Hydra and its scaling on cloud and HPC platforms</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11967v1</guid>
      <category>cs.DC</category>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3659995.3660040</arxiv:DOI>
      <dc:creator>Aymen Alsaadi, Shantenu Jha, Matteo Turilli</dc:creator>
    </item>
    <item>
      <title>Strategies for Tracking Individual IP Packets Towards DDoS</title>
      <link>https://arxiv.org/abs/2407.10985</link>
      <description>arXiv:2407.10985v1 Announce Type: cross 
Abstract: The identification of the exact path that packets are routed in the network is quite a challenge. This paper presents a novel, efficient traceback strategy in combination with a defence system against distributed denial of service (DDoS) attacks named Tracemax. A single packets can be directly traced over many more hops than the current existing techniques allow. It let good connections pass while bad ones get thwarted. Initiated by the victim the routers in the network cooperate in tracing and become automatically self-organised and self-managed. The novel concept support analyses of packet flows and transmission paths in a network infrastructure. It can effectively reduce the effect of common bandwidth and resource consumption attacks and foster in addition early warning and prevention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10985v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1515/pik-2015-0010</arxiv:DOI>
      <arxiv:journal_reference>PIK - Praxis der Informationsverarbeitung und Kommunikation 2015</arxiv:journal_reference>
      <dc:creator>Peter Hillmann, Frank Tietze, Gabi Dreo Rodosek</dc:creator>
    </item>
    <item>
      <title>Exploring the Boundaries of On-Device Inference: When Tiny Falls Short, Go Hierarchical</title>
      <link>https://arxiv.org/abs/2407.11061</link>
      <description>arXiv:2407.11061v1 Announce Type: cross 
Abstract: On-device inference holds great potential for increased energy efficiency, responsiveness, and privacy in edge ML systems. However, due to less capable ML models that can be embedded in resource-limited devices, use cases are limited to simple inference tasks such as visual keyword spotting, gesture recognition, and predictive analytics. In this context, the Hierarchical Inference (HI) system has emerged as a promising solution that augments the capabilities of the local ML by offloading selected samples to an edge server or cloud for remote ML inference. Existing works demonstrate through simulation that HI improves accuracy. However, they do not account for the latency and energy consumption on the device, nor do they consider three key heterogeneous dimensions that characterize ML systems: hardware, network connectivity, and models. In contrast, this paper systematically compares the performance of HI with on-device inference based on measurements of accuracy, latency, and energy for running embedded ML models on five devices with different capabilities and three image classification datasets. For a given accuracy requirement, the HI systems we designed achieved up to 73% lower latency and up to 77% lower device energy consumption than an on-device inference system. The key to building an efficient HI system is the availability of small-size, reasonably accurate on-device models whose outputs can be effectively differentiated for samples that require remote inference. Despite the performance gains, HI requires on-device inference for all samples, which adds a fixed overhead to its latency and energy consumption. Therefore, we design a hybrid system, Early Exit with HI (EE-HI), and demonstrate that compared to HI, EE-HI reduces the latency by up to 59.7% and lowers the device's energy consumption by up to 60.4%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11061v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Adarsh Prasad Behera, Paulius Daubaris, I\~naki Bravo, Jos\'e Gallego, Roberto Morabito, Joerg Widmer, Jaya Prakash Varma Champati</dc:creator>
    </item>
    <item>
      <title>SENTINEL: Securing Indoor Localization against Adversarial Attacks with Capsule Neural Networks</title>
      <link>https://arxiv.org/abs/2407.11091</link>
      <description>arXiv:2407.11091v1 Announce Type: cross 
Abstract: With the increasing demand for edge device powered location-based services in indoor environments, Wi-Fi received signal strength (RSS) fingerprinting has become popular, given the unavailability of GPS indoors. However, achieving robust and efficient indoor localization faces several challenges, due to RSS fluctuations from dynamic changes in indoor environments and heterogeneity of edge devices, leading to diminished localization accuracy. While advances in machine learning (ML) have shown promise in mitigating these phenomena, it remains an open problem. Additionally, emerging threats from adversarial attacks on ML-enhanced indoor localization systems, especially those introduced by malicious or rogue access points (APs), can deceive ML models to further increase localization errors. To address these challenges, we present SENTINEL, a novel embedded ML framework utilizing modified capsule neural networks to bolster the resilience of indoor localization solutions against adversarial attacks, device heterogeneity, and dynamic RSS fluctuations. We also introduce RSSRogueLoc, a novel dataset capturing the effects of rogue APs from several real-world indoor environments. Experimental evaluations demonstrate that SENTINEL achieves significant improvements, with up to 3.5x reduction in mean error and 3.4x reduction in worst-case error compared to state-of-the-art frameworks using simulated adversarial attacks. SENTINEL also achieves improvements of up to 2.8x in mean error and 2.7x in worst-case error compared to state-of-the-art frameworks when evaluated with the real-world RSSRogueLoc dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11091v1</guid>
      <category>eess.SP</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Danish Gufran, Pooja Anandathirtha, Sudeep Pasricha</dc:creator>
    </item>
    <item>
      <title>Detection of Global Anomalies on Distributed IoT Edges with Device-to-Device Communication</title>
      <link>https://arxiv.org/abs/2407.11308</link>
      <description>arXiv:2407.11308v1 Announce Type: cross 
Abstract: Anomaly detection is an important function in IoT applications for finding outliers caused by abnormal events. Anomaly detection sometimes comes with high-frequency data sampling which should be carried out at Edge devices rather than Cloud. In this paper, we consider the case that multiple IoT devices are installed in a single remote site and that they collaboratively detect anomalies from the observations with device-to-device communications. For this, we propose a fully distributed collaborative scheme for training distributed anomaly detectors with Wireless Ad Hoc Federated Learning, namely "WAFL-Autoencoder". We introduce the concept of Global Anomaly which sample is not only rare to the local device but rare to all the devices in the target domain. We also propose a distributed threshold-finding algorithm for Global Anomaly detection. With our standard benchmark-based evaluation, we have confirmed that our scheme trained anomaly detectors perfectly across the devices. We have also confirmed that the devices collaboratively found thresholds for Global Anomaly detection with low false positive rates while achieving high true positive rates with few exceptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11308v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hideya Ochiai, Riku Nishihata, Eisuke Tomiyama, Yuwei Sun, Hiroshi Esaki</dc:creator>
    </item>
    <item>
      <title>Cloud-based Semi-Quantum Money</title>
      <link>https://arxiv.org/abs/2407.11454</link>
      <description>arXiv:2407.11454v1 Announce Type: cross 
Abstract: In the 1970s, Wiesner introduced the concept of quantum money, where quantum states generated according to specific rules function as currency. These states circulate among users with quantum resources through quantum channels or face-to-face interactions. Quantum mechanics grants quantum money physical-level unforgeability but also makes minting, storing, and circulating it significantly challenging. Currently, quantum computers capable of minting and preserving quantum money have not yet emerged, and existing quantum channels are not stable enough to support the efficient transmission of quantum states for quantum money, limiting its practicality. Semi-quantum money schemes support fully classical transactions and complete classical banks, reducing dependence on quantum resources and enhancing feasibility. To further minimize the system's reliance on quantum resources, we propose a cloud-based semi-quantum money (CSQM) scheme. This scheme relies only on semi-honest third-party quantum clouds, while the rest of the system remains entirely classical. We also discuss estimating the computational power required by the quantum cloud for the scheme and conduct a security analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11454v1</guid>
      <category>quant-ph</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yichi Zhang, Siyuan Jin, Yuhan Huang, Bei Zeng, Qiming Shao</dc:creator>
    </item>
    <item>
      <title>Performance Analysis of Internet of Vehicles Mesh Networks Based on Actual Switch Models</title>
      <link>https://arxiv.org/abs/2407.11483</link>
      <description>arXiv:2407.11483v1 Announce Type: cross 
Abstract: The rapid growth of the automotive industry has exacerbated the conflict between the complex traffic environment, increasing communication demands, and limited resources. Given the imperative to mitigate traffic and network congestion, analyzing the performance of Internet of Vehicles (IoV) mesh networks is of great practical significance. Most studies focus solely on individual performance metrics and influencing factors, and the adopted simulation tools, such as OPNET, cannot achieve the dynamic link generation of IoV mesh networks. To address these problems, a network performance analysis model based on actual switches is proposed. First, a typical IoV mesh network architecture is constructed and abstracted into a mathematical model that describes how the link and topology changes over time. Then, the task generation model and the task forwarding model based on actual switches are proposed to obtain the real traffic distribution of the network. Finally, a scientific network performance indicator system is constructed. Simulation results demonstrate that, with rising task traffic and decreasing node caching capacity, the packet loss rate increases, and the task arrival rate decreases in the network. The proposed model can effectively evaluate the network performance across various traffic states and provide valuable insights for network construction and enhancement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11483v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jialin Hu, Zhiyuan Ren, Wenchi Cheng, Zhiliang Shuai, Zhao Li</dc:creator>
    </item>
    <item>
      <title>Finite State Machines-Based Path-Following Collaborative Computing Strategy for Emergency UAV Swarms</title>
      <link>https://arxiv.org/abs/2407.11531</link>
      <description>arXiv:2407.11531v1 Announce Type: cross 
Abstract: Offloading services to UAV swarms for delay-sensitive tasks in Emergency UAV Networks (EUN) can greatly enhance rescue efficiency. Most task-offloading strategies assumed that UAVs were location-fixed and capable of handling all tasks. However, in complex disaster environments, UAV locations often change dynamically, and the heterogeneity of on-board resources presents a significant challenge in optimizing task scheduling in EUN to minimize latency. To address these problems, a Finite state machines-based Path-following Collaborative computation strategy (FPC) for emergency UAV swarms is proposed. First, an Extended Finite State Machine Space-time Graph (EFSMSG) model is constructed to accurately characterize on-board resources and state transitions while shielding the EUN dynamic characteristic. Based on the EFSMSG, a mathematical model is formulated for the FPC strategy to minimize task processing delay while facilitating computation during transmission. Finally, the Constraint Selection Adaptive Binary Particle Swarm Optimization (CSABPSO) algorithm is proposed for the solution. Simulation results demonstrate that the proposed FPC strategy effectively reduces task processing delay, meeting the requirements of delay-sensitive tasks in emergency situations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11531v1</guid>
      <category>eess.SY</category>
      <category>cs.DC</category>
      <category>cs.SY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jialin Hu, Zhiyuan Ren, Wenchi Cheng</dc:creator>
    </item>
    <item>
      <title>Revolutionizing MRI Data Processing Using FSL: Preliminary Findings with the Fugaku Supercomputer</title>
      <link>https://arxiv.org/abs/2407.11742</link>
      <description>arXiv:2407.11742v1 Announce Type: cross 
Abstract: The amount of Magnetic resonance imaging data has grown tremendously recently, creating an urgent need to accelerate data processing, which requires substantial computational resources and time. In this preliminary study, we applied FMRIB Software Library commands on T1-weighted and diffusion-weighted images of a single young adult using the Fugaku supercomputer. The tensor-based measurements and subcortical structure segmentations performed on Fugaku supercomputer were highly consistent with those from conventional systems, demonstrating its reliability and significantly reduced processing time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11742v1</guid>
      <category>physics.med-ph</category>
      <category>cs.DC</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianxiang Lyu, Wataru Uchida, Zhe Sun, Christina Andica, Keita Tokuda, Rui Zou, Jie Mao, Keigo Shimoji, Koji Kamagata, Mitsuhisa Sato, Ryutaro Himeno, Shigeki Aoki</dc:creator>
    </item>
    <item>
      <title>Self-Duplicating Random Walks for Resilient Decentralized Learning on Graphs</title>
      <link>https://arxiv.org/abs/2407.11762</link>
      <description>arXiv:2407.11762v1 Announce Type: cross 
Abstract: Consider the setting of multiple random walks (RWs) on a graph executing a certain computational task. For instance, in decentralized learning via RWs, a model is updated at each iteration based on the local data of the visited node and then passed to a randomly chosen neighbor. RWs can fail due to node or link failures. The goal is to maintain a desired number of RWs to ensure failure resilience. Achieving this is challenging due to the lack of a central entity to track which RWs have failed to replace them with new ones by forking (duplicating) surviving ones. Without duplications, the number of RWs will eventually go to zero, causing a catastrophic failure of the system. We propose a decentralized algorithm called DECAFORK that can maintain the number of RWs in the graph around a desired value even in the presence of arbitrary RW failures. Nodes continuously estimate the number of surviving RWs by estimating their return time distribution and fork the RWs when failures are likely to happen. We present extensive numerical simulations that show the performance of DECAFORK regarding fast detection and reaction to failures. We further present theoretical guarantees on the performance of this algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11762v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maximilian Egger, Ghadir Ayache, Rawad Bitar, Antonia Wachter-Zeh, Salim El Rouayheb</dc:creator>
    </item>
    <item>
      <title>Enhancing Split Computing and Early Exit Applications through Predefined Sparsity</title>
      <link>https://arxiv.org/abs/2407.11763</link>
      <description>arXiv:2407.11763v1 Announce Type: cross 
Abstract: In the past decade, Deep Neural Networks (DNNs) achieved state-of-the-art performance in a broad range of problems, spanning from object classification and action recognition to smart building and healthcare. The flexibility that makes DNNs such a pervasive technology comes at a price: the computational requirements preclude their deployment on most of the resource-constrained edge devices available today to solve real-time and real-world tasks. This paper introduces a novel approach to address this challenge by combining the concept of predefined sparsity with Split Computing (SC) and Early Exit (EE). In particular, SC aims at splitting a DNN with a part of it deployed on an edge device and the rest on a remote server. Instead, EE allows the system to stop using the remote server and rely solely on the edge device's computation if the answer is already good enough. Specifically, how to apply such a predefined sparsity to a SC and EE paradigm has never been studied. This paper studies this problem and shows how predefined sparsity significantly reduces the computational, storage, and energy burdens during the training and inference phases, regardless of the hardware platform. This makes it a valuable approach for enhancing the performance of SC and EE applications. Experimental results showcase reductions exceeding 4x in storage and computational complexity without compromising performance. The source code is available at https://github.com/intelligolabs/sparsity_sc_ee.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11763v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luigi Capogrosso, Enrico Fraccaroli, Giulio Petrozziello, Francesco Setti, Samarjit Chakraborty, Franco Fummi, Marco Cristani</dc:creator>
    </item>
    <item>
      <title>PipeInfer: Accelerating LLM Inference using Asynchronous Pipelined Speculation</title>
      <link>https://arxiv.org/abs/2407.11798</link>
      <description>arXiv:2407.11798v1 Announce Type: cross 
Abstract: Inference of Large Language Models (LLMs) across computer clusters has become a focal point of research in recent times, with many acceleration techniques taking inspiration from CPU speculative execution. These techniques reduce bottlenecks associated with memory bandwidth, but also increase end-to-end latency per inference run, requiring high speculation acceptance rates to improve performance. Combined with a variable rate of acceptance across tasks, speculative inference techniques can result in reduced performance. Additionally, pipeline-parallel designs require many user requests to maintain maximum utilization. As a remedy, we propose PipeInfer, a pipelined speculative acceleration technique to reduce inter-token latency and improve system utilization for single-request scenarios while also improving tolerance to low speculation acceptance rates and low-bandwidth interconnects. PipeInfer exhibits up to a 2.15$\times$ improvement in generation speed over standard speculative inference. PipeInfer achieves its improvement through Continuous Asynchronous Speculation and Early Inference Cancellation, the former improving latency and generation speed by running single-token inference simultaneously with several speculative runs, while the latter improves speed and latency by skipping the computation of invalidated runs, even in the middle of inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11798v1</guid>
      <category>cs.CL</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Branden Butler, Sixing Yu, Arya Mazaheri, Ali Jannesari</dc:creator>
    </item>
    <item>
      <title>Scalable and Reliable Over-the-Air Federated Edge Learning</title>
      <link>https://arxiv.org/abs/2407.11807</link>
      <description>arXiv:2407.11807v1 Announce Type: cross 
Abstract: Federated edge learning (FEEL) has emerged as a core paradigm for large-scale optimization. However, FEEL still suffers from a communication bottleneck due to the transmission of high-dimensional model updates from the clients to the federator. Over-the-air computation (AirComp) leverages the additive property of multiple-access channels by aggregating the clients' updates over the channel to save communication resources. While analog uncoded transmission can benefit from the increased signal-to-noise ratio (SNR) due to the simultaneous transmission of many clients, potential errors may severely harm the learning process for small SNRs. To alleviate this problem, channel coding approaches were recently proposed for AirComp in FEEL. However, their error-correction capability degrades with an increasing number of clients. We propose a digital lattice-based code construction with constant error-correction capabilities in the number of clients, and compare to nested-lattice codes, well-known for their optimal rate and power efficiency in the point-to-point AWGN channel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11807v1</guid>
      <category>cs.IT</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maximilian Egger, Christoph Hofmeister, Cem Kaya, Rawad Bitar, Antonia Wachter-Zeh</dc:creator>
    </item>
    <item>
      <title>Capturing Periodic I/O Using Frequency Techniques</title>
      <link>https://arxiv.org/abs/2306.08601</link>
      <description>arXiv:2306.08601v2 Announce Type: replace 
Abstract: Many HPC applications perform their I/O in bursts that follow a periodic pattern. This allows for making predictions as to when a burst occurs. System providers can take advantage of such knowledge to reduce file-system contention by actively scheduling I/O bandwidth. The effectiveness of this approach, however, depends on the ability to detect and quantify the periodicity of I/O patterns online. In this paper, we introduce FTIO, an online method to detect periodic I/O phases, which is based on discrete Fourier transform (DFT), combined with outlier detection. We provide metrics that gauge the confidence in the output and tell how far from being periodic the signal is. We validate our approach with large-scale experiments on a production system and examine its limitations extensively. Our experiments show that FTIO has a mean error below 11%. Finally, we demonstrate that FTIO allowed the I/O scheduler Set- 10 to boost system utilization by 26% and reduce I/O slowdown by 56%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.08601v2</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/IPDPS57955.2024.00048</arxiv:DOI>
      <dc:creator>Ahmad Tarraf, Alexis Bandet, Francieli Boito, Guillaume Pallez, Felix Wolf</dc:creator>
    </item>
    <item>
      <title>Minimum Cost Loop Nests for Contraction of a Sparse Tensor with a Tensor Network</title>
      <link>https://arxiv.org/abs/2307.05740</link>
      <description>arXiv:2307.05740v2 Announce Type: replace 
Abstract: Sparse tensor decomposition and completion are common in numerous applications, ranging from machine learning to computational quantum chemistry. Typically, the main bottleneck in optimization of these models are contractions of a single large sparse tensor with a network of several dense matrices or tensors (SpTTN). Prior works on high-performance tensor decomposition and completion have focused on performance and scalability optimizations for specific SpTTN kernels. We present algorithms and a runtime system for identifying and executing the most efficient loop nest for any SpTTN kernel. We consider both enumeration of such loop nests for autotuning and efficient algorithms for finding the lowest cost loop-nest for simpler metrics, such as buffer size or cache miss models. Our runtime system identifies the best choice of loop nest without user guidance, and also provides a distributed-memory parallelization of SpTTN kernels. We evaluate our framework using both real-world and synthetic tensors. Our results demonstrate that our approach outperforms available generalized state-of-the-art libraries and matches the performance of specialized codes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.05740v2</guid>
      <category>cs.DC</category>
      <category>cs.MS</category>
      <category>cs.PF</category>
      <category>cs.PL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3626183.3659985</arxiv:DOI>
      <dc:creator>Raghavendra Kanakagiri, Edgar Solomonik</dc:creator>
    </item>
    <item>
      <title>Data-Locality-Aware Task Assignment and Scheduling for Distributed Job Executions</title>
      <link>https://arxiv.org/abs/2407.08584</link>
      <description>arXiv:2407.08584v3 Announce Type: replace 
Abstract: This paper investigates a data-locality-aware task assignment and scheduling problem aimed at minimizing job completion times for distributed job executions. Without prior knowledge of future job arrivals, we propose an optimal balanced task assignment algorithm (OBTA) that minimizes the completion time of each arriving job. We significantly reduce OBTA's computational overhead by narrowing the search space of potential solutions. Additionally, we extend an approximate algorithm known as water-filling (WF) and nontrivially prove that its approximation factor equals the number of task groups in the job assignment. We also design a novel heuristic, replica-deletion (RD), which outperforms WF. To further reduce the completion time of each job, we expand the problem to include job reordering, where we adjust the order of outstanding jobs following the shortest-estimated-time-first policy. Extensive trace-driven evaluations validate the performance and efficiency of the proposed algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08584v3</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Hailiang Zhao, Xueyan Tang, Peng Chen, Jianwei Yin, Shuiguang Deng</dc:creator>
    </item>
    <item>
      <title>Sparse Training for Federated Learning with Regularized Error Correction</title>
      <link>https://arxiv.org/abs/2312.13795</link>
      <description>arXiv:2312.13795v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) has attracted much interest due to the significant advantages it brings to training deep neural network (DNN) models. However, since communications and computation resources are limited, training DNN models in FL systems face challenges such as elevated computational and communication costs in complex tasks. Sparse training schemes gain increasing attention in order to scale down the dimensionality of each client (i.e., node) transmission. Specifically, sparsification with error correction methods is a promising technique, where only important updates are sent to the parameter server (PS) and the rest are accumulated locally. While error correction methods have shown to achieve a significant sparsification level of the client-to-PS message without harming convergence, pushing sparsity further remains unresolved due to the staleness effect. In this paper, we propose a novel algorithm, dubbed Federated Learning with Accumulated Regularized Embeddings (FLARE), to overcome this challenge. FLARE presents a novel sparse training approach via accumulated pulling of the updated models with regularization on the embeddings in the FL process, providing a powerful solution to the staleness effect, and pushing sparsity to an exceptional level. The performance of FLARE is validated through extensive experiments on diverse and complex models, achieving a remarkable sparsity level (10 times and more beyond the current state-of-the-art) along with significantly improved accuracy. Additionally, an open-source software package has been developed for the benefit of researchers and developers in related fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13795v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ran Greidi, Kobi Cohen</dc:creator>
    </item>
    <item>
      <title>MeanCache: User-Centric Semantic Cache for Large Language Model Based Web Services</title>
      <link>https://arxiv.org/abs/2403.02694</link>
      <description>arXiv:2403.02694v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) like ChatGPT and Llama have revolutionized natural language processing and search engine dynamics. However, these models incur exceptionally high computational costs. For instance, GPT-3 consists of 175 billion parameters, where inference demands billions of floating-point operations. Caching is a natural solution to reduce LLM inference costs on repeated queries, which constitute about 31% of the total queries. However, existing caching methods are incapable of finding semantic similarities among LLM queries nor do they operate on contextual queries, leading to unacceptable false hit-and-miss rates. This paper introduces MeanCache, a user-centric semantic cache for LLM-based services that identifies semantically similar queries to determine cache hit or miss. Using MeanCache, the response to a user's semantically similar query can be retrieved from a local cache rather than re-querying the LLM, thus reducing costs, service provider load, and environmental impact. MeanCache leverages Federated Learning (FL) to collaboratively train a query similarity model without violating user privacy. By placing a local cache in each user's device and using FL, MeanCache reduces the latency and costs and enhances model performance, resulting in lower false hit rates. MeanCache also encodes context chains for every cached query, offering a simple yet highly effective mechanism to discern contextual query responses from standalone. Our experiments benchmarked against the state-of-the-art caching method, reveal that MeanCache attains an approximately 17% higher F-score and a 20% increase in precision during semantic cache hit-and-miss decisions while performing even better on contextual queries. It also reduces the storage requirement by 83% and accelerates semantic cache hit-and-miss decisions by 11%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02694v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Waris Gill (Virginia Tech, USA), Mohamed Elidrisi (Cisco, USA), Pallavi Kalapatapu (Cisco, USA), Ammar Ahmed (University of Minnesota, Minneapolis, USA), Ali Anwar (University of Minnesota, Minneapolis, USA), Muhammad Ali Gulzar (Virginia Tech, USA)</dc:creator>
    </item>
  </channel>
</rss>
