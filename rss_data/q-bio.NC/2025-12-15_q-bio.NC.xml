<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 15 Dec 2025 05:00:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Quantifying the Dynamics of Consciousness using Hierarchical Integration, Organised Complexity and Metastability</title>
      <link>https://arxiv.org/abs/2512.10972</link>
      <description>arXiv:2512.10972v1 Announce Type: new 
Abstract: Quantifying the neural signatures of consciousness remains a major challenge in neuroscience and AI. Although many theories link consciousness to rich, multiscale, and flexible neural organisation, robust quantitative measures are still lacking. This paper presents a theory-neutral framework that characterises consciousness-related dynamics through three properties: hierarchical integration (H), cross-frequency complexity (D), and metastability (M). Candidate subsystems are identified using predictive information, temporal complexity, and state-space exploration to distinguish structured from unstructured activity. We provide mathematical definitions for all components and implement the framework in a generative model of synthetic EEG, simulating nine brain states ranging from psychedelic and wakeful to dreaming, non-REM sleep, minimally conscious, anaesthetised, and seizure-like regimes. Across single trials and Monte Carlo ensembles, the composite index reliably separates high-consciousness from impaired or non-conscious states. We further validate the framework using real EEG from the Sleep-EDF dataset alongside matched synthetic EEG designed to reproduce state-dependent oscillatory structure. Across Wake, N2, and REM sleep, synthetic data recapitulate the empirical ordering and magnitude of the index, indicating that the index captures stable and biologically meaningful distinctions. This approach provides a principled and empirically grounded tool for quantifying consciousness-related neural organisation with potential applications to both biological and artificial systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.10972v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hassan Ugail, Newton Howard</dc:creator>
    </item>
    <item>
      <title>The Homological Brain: Parity Principle and Amortized Inference</title>
      <link>https://arxiv.org/abs/2512.10976</link>
      <description>arXiv:2512.10976v1 Announce Type: new 
Abstract: Biological intelligence emerges from substrates that are slow, noisy, and energetically constrained, yet it performs rapid and coherent inference in open-ended environments. Classical computational theories, built around vector-space transformations and instantaneous error minimization, struggle to reconcile the slow timescale of synaptic plasticity with the fast timescale of perceptual synthesis. We propose a unifying framework based on algebraic topology, the Homological Brain, in which neural computation is understood as the construction and navigation of topological structure. Central to this view is the Parity Principle, a homological partition between even-dimensional scaffolds encoding stable content ($\Phi$) and odd-dimensional flows encoding dynamic context ($\Psi$). Transient contextual flows are resolved through a three-stage topological trinity transformation: Search (open-chain exploration), Closure (topological cycle formation), and Condensation (collapse of validated flows into new scaffold). This process converts high-complexity recursive search (formally modeled by Savitch's Theorem in NPSPACE) into low-complexity navigation over a learned manifold (analogous to memoized Dynamic Programming in P). In this framework, topological condensation is the mechanism that transforms a ``search problem'' into a ``navigation task'', allowing the brain to amortize past inference and achieve rapid perceptual integration. This perspective unifies the Wake-Sleep cycle, episodic-to-semantic consolidation, and dual-process theories (System 1-vs-System 2), revealing the brain as a homology engine that minimizes topological complexity to transmute high-entropy sensory flux into low-entropy, invariant cognitive structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.10976v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Xin Li</dc:creator>
    </item>
    <item>
      <title>Cognitive Mirrors: Exploring the Diverse Functional Roles of Attention Heads in LLM Reasoning</title>
      <link>https://arxiv.org/abs/2512.10978</link>
      <description>arXiv:2512.10978v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved state-of-the-art performance in a variety of tasks, but remain largely opaque in terms of their internal mechanisms. Understanding these mechanisms is crucial to improve their reasoning abilities. Drawing inspiration from the interplay between neural processes and human cognition, we propose a novel interpretability framework to systematically analyze the roles and behaviors of attention heads, which are key components of LLMs. We introduce CogQA, a dataset that decomposes complex questions into step-by-step subquestions with a chain-of-thought design, each associated with specific cognitive functions such as retrieval or logical reasoning. By applying a multi-class probing method, we identify the attention heads responsible for these functions. Our analysis across multiple LLM families reveals that attention heads exhibit functional specialization, characterized as cognitive heads. These cognitive heads exhibit several key properties: they are universally sparse, vary in number and distribution across different cognitive functions, and display interactive and hierarchical structures. We further show that cognitive heads play a vital role in reasoning tasks - removing them leads to performance degradation, while augmenting them enhances reasoning accuracy. These insights offer a deeper understanding of LLM reasoning and suggest important implications for model design, training, and fine-tuning strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.10978v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xueqi Ma, Jun Wang, Yanbei Jiang, Sarah Monazam Erfani, Tongliang Liu, James Bailey</dc:creator>
    </item>
    <item>
      <title>Targeting the Synergistic Interaction of Pathologies in Alzheimer's Disease: Rationale and Prospects for Combination Therapy</title>
      <link>https://arxiv.org/abs/2512.10981</link>
      <description>arXiv:2512.10981v1 Announce Type: new 
Abstract: Alzheimer's disease (AD) persists as a paramount challenge in neurological research, characterized by the pathological hallmarks of amyloid-beta (Abeta) plaques and neurofibrillary tangles composed of hyperphosphorylated tau. This review synthesizes the evolving understanding of AD pathogenesis, moving beyond the linear amyloid cascade hypothesis to conceptualize the disease as a cross-talk of intricately interacting pathologies, encompassing Abeta, tau, and neuroinflammation. This evolving pathophysiological understanding parallels a transformation in diagnostic paradigms, where biomarker-based strategies -- such as the AT(N) framework -- enable early disease detection during preclinical or prodromal stages. Within this new landscape, while anti-Abeta monoclonal antibodies (e.g., lecanemab, donanemab) represent a breakthrough as the first disease-modifying therapies, their modest efficacy underscores the limitation of single-target approaches. Therefore, this review explores the compelling rationale for combination therapies that simultaneously target Abeta pathology, aberrant tau, and neuroinflammation. Looking forward, we emphasize emerging technological platforms -- such as gene editing and biophysical neuromodulation -- n advancing precision medicine. Ultimately, the integration of early biomarker detection, multi-target therapeutic strategies, and AI-driven patient stratification charts a promising roadmap toward fundamentally altering the trajectory of AD. The future of AD management will be defined by preemptive, biomarker-guided, and personalized combination interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.10981v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xutong She</dc:creator>
    </item>
    <item>
      <title>Rosetta Stone of Neural Mass Models</title>
      <link>https://arxiv.org/abs/2512.10982</link>
      <description>arXiv:2512.10982v1 Announce Type: new 
Abstract: Brain dynamics dominate every level of neural organization -- from single-neuron spiking to the macroscopic waves captured by fMRI, MEG, and EEG -- yet the mathematical tools used to interrogate those dynamics remain scattered across a patchwork of traditions. Neural mass models (NMMs) (aggregate neural models) provide one of the most popular gateways into this landscape, but their sheer variety -- spanning lumped parameter models, firing-rate equations, and multi-layer generators -- demands a unifying framework that situates diverse architectures along a continuum of abstraction and biological detail. Here, we start from the idea that oscillations originate from a simple push-pull interaction between two or more neural populations. We build from the undamped harmonic oscillator and, guided by a simple push-pull motif between excitatory and inhibitory populations, climb a systematic ladder of detail. Each rung is presented first in isolation, next under forcing, and then within a coupled network, reflecting the progression from single-node to whole-brain modeling. By transforming a repertoire of disparate formalisms into a navigable ladder, we hope to turn NMM choice from a subjective act into a principled design decision, helping both theorists and experimentalists translate between scales, modalities, and interventions. In doing so, we offer a \emph{Rosetta Stone} for brain oscillation models -- one that lets the field speak a common dynamical language while preserving the dialectical richness that fuels discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.10982v1</guid>
      <category>q-bio.NC</category>
      <category>nlin.CD</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesca Castaldo, Raul de Palma Aristides, Pau Clusella, Jordi Garcia-Ojalvo, Giulio Ruffini</dc:creator>
    </item>
    <item>
      <title>Developmental Symmetry-Loss: A Free-Energy Perspective on Brain-Inspired Invariance Learning</title>
      <link>https://arxiv.org/abs/2512.10984</link>
      <description>arXiv:2512.10984v1 Announce Type: new 
Abstract: We propose Symmetry-Loss, a brain-inspired algorithmic principle that enforces invariance and equivariance through a differentiable constraint derived from environmental symmetries. The framework models learning as the iterative refinement of an effective symmetry group, paralleling developmental processes in which cortical representations align with the world's structure. By minimizing structural surprise, i.e. deviations from symmetry consistency, Symmetry-Loss operationalizes a Free-Energy--like objective for representation learning. This formulation bridges predictive-coding and group-theoretic perspectives, showing how efficient, stable, and compositional representations can emerge from symmetry-based self-organization. The result is a general computational mechanism linking developmental learning in the brain with principled representation learning in artificial systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.10984v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>nlin.AO</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arif D\"onmez</dc:creator>
    </item>
    <item>
      <title>Marti-5: A Mathematical Model of "Self in the World" as a First Step Toward Self-Awareness</title>
      <link>https://arxiv.org/abs/2512.10985</link>
      <description>arXiv:2512.10985v1 Announce Type: new 
Abstract: The existence of 'what' and 'where' pathways of information processing in the brain was proposed almost 30 years ago, but there is still a lack of a clear mathematical model that could show how these pathways work together. We propose a biologically inspired mathematical model that uses this idea to identify and separate the self from the environment and then build and use a self-model for better predictions. This is a model of neocortical columns governed by the basal ganglia to make predictions and choose the next action, where some columns act as 'what' columns and others act as 'where' columns. Based on this model, we present a reinforcement learning agent that learns purposeful behavior in a virtual environment. We evaluate the agent on the Atari games Pong and Breakout, where it successfully learns to play. We conclude that the ability to separate the self from the environment gives advantages to the agent and therefore such a model could appear in living organisms during evolution. We propose Self-Awareness Principle 1: the ability to separate the self from the world is a necessary but insufficient condition for self-awareness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.10985v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Igor Pivovarov, Sergey Shumsky</dc:creator>
    </item>
    <item>
      <title>Mathematics of natural intelligence</title>
      <link>https://arxiv.org/abs/2512.10988</link>
      <description>arXiv:2512.10988v1 Announce Type: new 
Abstract: In the process of evolution, the brain has achieved such perfection that artificial intelligence systems do not have and which needs its own mathematics. The concept of cognitome, introduced by the academician K.V. Anokhin, as the cognitive structure of the mind -- a high-order structure of the brain and a neural hypernetwork, is considered as the basis for modeling. Consciousness then is a special form of dynamics in this hypernetwork -- a large-scale integration of its cognitive elements. The cognitome, in turn, consists of interconnected COGs (cognitive groups of neurons) of two types -- functional systems and cellular ensembles. K.V. Anokhin sees the task of the fundamental theory of the brain and mind in describing these structures, their origin, functions and processes in them. The paper presents mathematical models of these structures based on new mathematical results, as well as models of different cognitive processes in terms of these models. In addition, it is shown that these models can be derived based on a fairly general principle of the brain works: \textit{the brain discovers all possible causal relationships in the external world and draws all possible conclusions from them}. Based on these results, the paper presents models of: ``natural" classification; theory of functional brain systems by P.K. Anokhin; prototypical theory of categorization by E. Roche; theory of causal models by Bob Rehter; theory of consciousness as integrated information by G. Tononi.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.10988v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Evgenii Vityaev</dc:creator>
    </item>
    <item>
      <title>Unambiguous Representations in Neural Networks: An Information-Theoretic Approach to Intentionality</title>
      <link>https://arxiv.org/abs/2512.11000</link>
      <description>arXiv:2512.11000v1 Announce Type: new 
Abstract: Representations pervade our daily experience, from letters representing sounds to bit strings encoding digital files. While such representations require externally defined decoders to convey meaning, conscious experience appears fundamentally different: a neural state corresponding to perceiving a red square cannot alternatively encode the experience of a green square. This intrinsic property of consciousness suggests that conscious representations must be unambiguous in a way that conventional representations are not. We formalize this intuition using information theory, defining representational ambiguity as the conditional entropy H(I|R) over possible interpretations I given a representation R. Through experiments on neural networks trained to classify MNIST digits, we demonstrate that relational structures in network connectivity can unambiguously encode representational content. Using both learned decoders and direct geometric matching, we achieve perfect (100%) accuracy for dropout-trained networks and 38% for standard backpropagation in identifying output neuron class identity, despite identical task performance, demonstrating that representational ambiguity can arise orthogonally to behavioral accuracy. We further show that spatial position information of input neurons can be decoded from network connectivity with R2 up to 0.844. These results provide a quantitative method for measuring representational ambiguity in neural systems and demonstrate that neural networks can exhibit the low-ambiguity representations posited as necessary (though not sufficient) by theoretical accounts of consciousness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11000v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesco L\"assig</dc:creator>
    </item>
    <item>
      <title>Multiscale Causal Geometric Deep Learning for Modeling Brain Structure</title>
      <link>https://arxiv.org/abs/2512.11738</link>
      <description>arXiv:2512.11738v1 Announce Type: new 
Abstract: Multimodal MRI offers complementary multi-scale information to characterize the brain structure. However, it remains challenging to effectively integrate multimodal MRI while achieving neuroscience interpretability. Here we propose to use Laplacian harmonics and spectral graph theory for multimodal alignment and multiscale integration. Based on the cortical mesh and connectome matrix that offer multi-scale representations, we devise Laplacian operators and spectral graph attentions to construct a shared latent space for model alignment. Next, we employ a disentangled learning combined with Graph Variational Autoencoder architectures to separate scale-specific and shared features. Lastly, we design a mutual information-informed bilevel regularizer to separate causal and non-causal factors based on the disentangled features, achieving robust model performance with enhanced interpretability. Our model outperforms baselines and other state-of-the-art models. The ablation studies confirmed the effectiveness of the proposed modules. Our model promises to offer a robust and interpretable framework for multi-scale brain structure analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11738v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengzhi Xia, Jianwei Chen, Yixuan Jiang, Qi Yan, Chao Li</dc:creator>
    </item>
    <item>
      <title>Compartmental-reaction diffusion framework for microscale dynamics of extracellular serotonin in brain tissue</title>
      <link>https://arxiv.org/abs/2512.10983</link>
      <description>arXiv:2512.10983v1 Announce Type: cross 
Abstract: Serotonin (5-hydroxytryptamine) is a major neurotransmitter whose release from densely distributed serotonergic varicosities shapes plasticity and network integration throughout the brain, yet its extracellular dynamics remain poorly understood due to the sub-micrometer and millisecond scales involved. We develop a mathematical framework that captures the coupled reaction-diffusion processes governing serotonin signaling in realistic tissue microenvironments. Formulating a two-dimensional compartmental-reaction diffusion system, we use strong localized perturbation theory to derive an asymptotically equivalent set of nonlinear integro-ODEs that preserve diffusive coupling while enabling efficient computation. We analyze period-averaged steady states, establish bounds using Jensen's inequality, obtain closed-form spike maxima and minima, and implement a fast marching-scheme solver based on sum-of-exponentials kernels. These mathematical results provide quantitative insight into how firing frequency, varicosity geometry, and uptake kinetics shape extracellular serotonin. The model reveals that varicosities form diffusively coupled microdomains capable of generating spatial "serotonin reservoirs," clarifies aspects of local versus volume transmission, and yields predictions relevant to interpreting high-resolution serotonin imaging and the actions of selective serotonin-reuptake inhibitors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.10983v1</guid>
      <category>q-bio.TO</category>
      <category>q-bio.CB</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Merlin Pelz, Skirmantas Janusonis, Gregory Handy</dc:creator>
    </item>
    <item>
      <title>Brain-Semantoks: Learning Semantic Tokens of Brain Dynamics with a Self-Distilled Foundation Model</title>
      <link>https://arxiv.org/abs/2512.11582</link>
      <description>arXiv:2512.11582v1 Announce Type: cross 
Abstract: The development of foundation models for functional magnetic resonance imaging (fMRI) time series holds significant promise for predicting phenotypes related to disease and cognition. Current models, however, are often trained using a mask-and-reconstruct objective on small brain regions. This focus on low-level information leads to representations that are sensitive to noise and temporal fluctuations, necessitating extensive fine-tuning for downstream tasks. We introduce Brain-Semantoks, a self-supervised framework designed specifically to learn abstract representations of brain dynamics. Its architecture is built on two core innovations: a semantic tokenizer that aggregates noisy regional signals into robust tokens representing functional networks, and a self-distillation objective that enforces representational stability across time. We show that this objective is stabilized through a novel training curriculum, ensuring the model robustly learns meaningful features from low signal-to-noise time series. We demonstrate that learned representations enable strong performance on a variety of downstream tasks even when only using a linear probe. Furthermore, we provide comprehensive scaling analyses indicating more unlabeled data reliably results in out-of-distribution performance gains without domain adaptation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11582v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sam Gijsen, Marc-Andre Schulz, Kerstin Ritter</dc:creator>
    </item>
    <item>
      <title>Mesoscale tissue properties and electric fields in brain stimulation -- bridging the macroscopic and microscopic scales</title>
      <link>https://arxiv.org/abs/2511.16465</link>
      <description>arXiv:2511.16465v4 Announce Type: replace-cross 
Abstract: Accurate simulations of electric fields (E-fields) in brain stimulation depend on tissue conductivity representations that link macroscopic assumptions with underlying microscopic tissue structure. Mesoscale conductivity variations can produce meaningful changes in E-fields and neural activation thresholds but remain largely absent from standard macroscopic models. Recent microscopic models have suggested substantial local E-field perturbations and could, in principle, inform mesoscale conductivity. However, the quantitative validity of microscopic models is limited by fixation-related tissue distortion and incomplete extracellular-space reconstruction. We outline approaches that bridge macro- and microscales to derive consistent mesoscale conductivity distributions, providing a foundation for accurate multiscale models of E-fields and neural activation in brain stimulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16465v4</guid>
      <category>physics.bio-ph</category>
      <category>physics.app-ph</category>
      <category>physics.med-ph</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Boshuo Wang, Torge Worbs, Minhaj A. Hussain, Aman S. Aberra, Axel Thielscher, Warren M. Grill, Angel V. Peterchev</dc:creator>
    </item>
  </channel>
</rss>
