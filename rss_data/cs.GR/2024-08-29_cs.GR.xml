<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 Aug 2024 04:00:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Micro and macro facial expressions by driven animations in realistic Virtual Humans</title>
      <link>https://arxiv.org/abs/2408.16110</link>
      <description>arXiv:2408.16110v1 Announce Type: new 
Abstract: Computer Graphics (CG) advancements have allowed the creation of more realistic Virtual Humans (VH) through modern techniques for animating the VH body and face, thereby affecting perception. From traditional methods, including blend shapes, to driven animations using facial and body tracking, these advancements can potentially enhance the perception of comfort and realism in relation to VHs. Previously, Psychology studied facial movements in humans, with some works separating expressions into macro and micro expressions. Also, some previous CG studies have analyzed how macro and micro expressions are perceived, replicating psychology studies in VHs, encompassing studies with realistic and cartoon VHs, and exploring different VH technologies. However, instead of using facial tracking animation methods, these previous studies animated the VHs using blendshapes interpolation. To understand how the facial tracking technique alters the perception of VHs, this paper extends the study to macro and micro expressions, employing two datasets to transfer real facial expressions to VHs and analyze how their expressions are perceived. Our findings suggest that transferring facial expressions from real actors to VHs significantly diminishes the accuracy of emotion perception compared to VH facial animations created by artists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16110v1</guid>
      <category>cs.GR</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.entcom.2024.100853</arxiv:DOI>
      <arxiv:journal_reference>Entertainment Computing, Volume 52, January 2025</arxiv:journal_reference>
      <dc:creator>Rubens Halbig Montanha, Giovana Nascimento Raupp, Ana Carolina Policarpo Schmitt, Victor Fl\'avio de Andrade Araujo, Soraia Raupp Musse</dc:creator>
    </item>
    <item>
      <title>Advancing Architectural Floorplan Design with Geometry-enhanced Graph Diffusion</title>
      <link>https://arxiv.org/abs/2408.16258</link>
      <description>arXiv:2408.16258v1 Announce Type: new 
Abstract: Automating architectural floorplan design is vital for housing and interior design, offering a faster, cost-effective alternative to manual sketches by architects. However, existing methods, including rule-based and learning-based approaches, face challenges in design complexity and constrained generation with extensive post-processing, and tend to obvious geometric inconsistencies such as misalignment, overlap, and gaps. In this work, we propose a novel generative framework for vector floorplan design via structural graph generation, called GSDiff, focusing on wall junction generation and wall segment prediction to capture both geometric and semantic aspects of structural graphs. To improve the geometric rationality of generated structural graphs, we propose two innovative geometry enhancement methods. In wall junction generation, we propose a novel alignment loss function to improve geometric consistency. In wall segment prediction, we propose a random self-supervision method to enhance the model's perception of the overall geometric structure, thereby promoting the generation of reasonable geometric structures. Employing the diffusion model and the Transformer model, as well as the geometry enhancement strategies, our framework can generate wall junctions, wall segments and room polygons with structural and semantic information, resulting in structural graphs that accurately represent floorplans. Extensive experiments show that the proposed method surpasses existing techniques, enabling free generation and constrained generation, marking a shift towards structure generation in architectural design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16258v1</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sizhe Hu, Wenming Wu, Yuntao Wang, Benzhu Xu, Liping Zheng</dc:creator>
    </item>
    <item>
      <title>Many-Worlds Inverse Rendering</title>
      <link>https://arxiv.org/abs/2408.16005</link>
      <description>arXiv:2408.16005v1 Announce Type: cross 
Abstract: Discontinuous visibility changes remain a major bottleneck when optimizing surfaces within a physically-based inverse renderer. Many previous works have proposed sophisticated algorithms and data structures to sample visibility silhouettes more efficiently.
  Our work presents another solution: instead of differentiating a tentative surface locally, we differentiate a volumetric perturbation of a surface. We refer this as a many-worlds representation because it models a non-interacting superposition of conflicting explanations (worlds) of the input dataset. Each world is optically isolated from others, leading to a new transport law that distinguishes our method from prior work based on exponential random media.
  The resulting Monte Carlo algorithm is simpler and more efficient than prior methods. We demonstrate that our method promotes rapid convergence, both in terms of the total iteration count and the cost per iteration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16005v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyi Zhang, Nicolas Roussel, Wenzel Jakob</dc:creator>
    </item>
    <item>
      <title>Motion-Driven Neural Optimizer for Prophylactic Braces Made by Distributed Microstructures</title>
      <link>https://arxiv.org/abs/2408.16659</link>
      <description>arXiv:2408.16659v1 Announce Type: cross 
Abstract: Joint injuries, and their long-term consequences, present a substantial global health burden. Wearable prophylactic braces are an attractive potential solution to reduce the incidence of joint injuries by limiting joint movements that are related to injury risk. Given human motion and ground reaction forces, we present a computational framework that enables the design of personalized braces by optimizing the distribution of microstructures and elasticity. As varied brace designs yield different reaction forces that influence kinematics and kinetics analysis outcomes, the optimization process is formulated as a differentiable end-to-end pipeline in which the design domain of microstructure distribution is parameterized onto a neural network. The optimized distribution of microstructures is obtained via a self-learning process to determine the network coefficients according to a carefully designed set of losses and the integrated biomechanical and physical analyses. Since knees and ankles are the most commonly injured joints, we demonstrate the effectiveness of our pipeline by designing, fabricating, and testing prophylactic braces for the knee and ankle to prevent potentially harmful joint movements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16659v1</guid>
      <category>physics.med-ph</category>
      <category>cs.GR</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xingjian Han, Yu Jiang, Weiming Wang, Guoxin Fang, Simeon Gill, Zhiqiang Zhang, Shengfa Wang, Jun Saito, Deepak Kumar, Zhongxuan Luo, Emily Whiting, Charlie C. L. Wang</dc:creator>
    </item>
    <item>
      <title>UV-free Texture Generation with Denoising and Geodesic Heat Diffusions</title>
      <link>https://arxiv.org/abs/2408.16762</link>
      <description>arXiv:2408.16762v1 Announce Type: cross 
Abstract: Seams, distortions, wasted UV space, vertex-duplication, and varying resolution over the surface are the most prominent issues of the standard UV-based texturing of meshes. These issues are particularly acute when automatic UV-unwrapping techniques are used. For this reason, instead of generating textures in automatically generated UV-planes like most state-of-the-art methods, we propose to represent textures as coloured point-clouds whose colours are generated by a denoising diffusion probabilistic model constrained to operate on the surface of 3D objects. Our sampling and resolution agnostic generative model heavily relies on heat diffusion over the surface of the meshes for spatial communication between points. To enable processing of arbitrarily sampled point-cloud textures and ensure long-distance texture consistency we introduce a fast re-sampling of the mesh spectral properties used during the heat diffusion and introduce a novel heat-diffusion-based self-attention mechanism. Our code and pre-trained models are available at github.com/simofoti/UV3-TeD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16762v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Simone Foti, Stefanos Zafeiriou, Tolga Birdal</dc:creator>
    </item>
    <item>
      <title>ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion Model</title>
      <link>https://arxiv.org/abs/2408.16767</link>
      <description>arXiv:2408.16767v1 Announce Type: cross 
Abstract: Advancements in 3D scene reconstruction have transformed 2D images from the real world into 3D models, producing realistic 3D results from hundreds of input photos. Despite great success in dense-view reconstruction scenarios, rendering a detailed scene from insufficient captured views is still an ill-posed optimization problem, often resulting in artifacts and distortions in unseen areas. In this paper, we propose ReconX, a novel 3D scene reconstruction paradigm that reframes the ambiguous reconstruction challenge as a temporal generation task. The key insight is to unleash the strong generative prior of large pre-trained video diffusion models for sparse-view reconstruction. However, 3D view consistency struggles to be accurately preserved in directly generated video frames from pre-trained models. To address this, given limited input views, the proposed ReconX first constructs a global point cloud and encodes it into a contextual space as the 3D structure condition. Guided by the condition, the video diffusion model then synthesizes video frames that are both detail-preserved and exhibit a high degree of 3D consistency, ensuring the coherence of the scene from various perspectives. Finally, we recover the 3D scene from the generated video through a confidence-aware 3D Gaussian Splatting optimization scheme. Extensive experiments on various real-world datasets show the superiority of our ReconX over state-of-the-art methods in terms of quality and generalizability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16767v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, Yueqi Duan</dc:creator>
    </item>
    <item>
      <title>From Top-Right to User-Right: Perceptual Prioritization of Point-Feature Label Positions</title>
      <link>https://arxiv.org/abs/2407.11996</link>
      <description>arXiv:2407.11996v2 Announce Type: replace-cross 
Abstract: In cartography, Geographic Information Systems (GIS), and visualization, the position of a label relative to its point feature is crucial for readability and user experience. Alongside other factors, the point-feature label placement (PFLP) is typically governed by the Position Priority Order (PPO), a systematic raking of potential label positions around a point feature according to predetermined priorities. While there is a broad consensus on factors such as avoiding label conflicts and ensuring clear label-to-feature associations, there is no agreement on PPO. Most PFLP techniques rely on traditional PPOs grounded in typographic and cartographic conventions established decades ago, which may no longer meet today's user expectations. In contrast, commercial products like Google Maps and Mapbox use non-traditional PPOs for unreported reasons. Our extensive user study introduces the Perceptual Position Priority Order (PerceptPPO), a user-validated PPO that significantly departs from traditional conventions. A key finding is that labels placed above point features are significantly preferred by users, contrary to the conventional top-right position. We also conducted a supplementary study on the preferred label density, an area scarcely explored in prior research. Finally, we performed a comparative user study assessing the perceived quality of PerceptPPO over existing PPOs, advocating its adoption in cartographic and GIS applications, as well as in other types of visualizations. Our research, supported by nearly 800 participants from 48 countries and over 45,500 pairwise comparisons, offers practical guidance for designers and application developers aiming to optimize user engagement and comprehension, paving the way for more intuitive and accessible visualizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11996v2</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Petr Bob\'ak, Ladislav \v{C}mol\'ik, Martin \v{C}ad\'ik</dc:creator>
    </item>
    <item>
      <title>Biomedical Image Segmentation: A Systematic Literature Review of Deep Learning Based Object Detection Methods</title>
      <link>https://arxiv.org/abs/2408.03393</link>
      <description>arXiv:2408.03393v2 Announce Type: replace-cross 
Abstract: Biomedical image segmentation plays a vital role in diagnosis of diseases across various organs. Deep learning-based object detection methods are commonly used for such segmentation. There exists an extensive research in this topic. However, there is no standard review on this topic. Existing surveys often lack a standardized approach or focus on broader segmentation techniques. In this paper, we conducted a systematic literature review (SLR), collected and analysed 148 articles that explore deep learning object detection methods for biomedical image segmentation. We critically analyzed these methods, identified the key challenges, and discussed the future directions. From the selected articles we extracted the results including the deep learning models, targeted imaging modalities, targeted diseases, and the metrics for the analysis of the methods. The results have been presented in tabular and/or charted forms. The results are presented in three major categories including two stage detection models, one stage detection models and point-based detection models. Each article is individually analyzed along with its pros and cons. Finally, we discuss open challenges, potential benefits, and future research directions. This SLR aims to provide the research community with a quick yet deeper understanding of these segmentation models, ultimately facilitating the development of more powerful solutions for biomedical image analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03393v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Fazli Wahid, Yingliang Ma, Dawar Khan, Muhammad Aamir, Syed U. K. Bukhari</dc:creator>
    </item>
  </channel>
</rss>
