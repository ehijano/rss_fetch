<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Feb 2026 05:00:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Expanding the Scope of Computational Thinking in Artificial Intelligence for K-12 Education</title>
      <link>https://arxiv.org/abs/2602.16890</link>
      <description>arXiv:2602.16890v1 Announce Type: new 
Abstract: The introduction of generative artificial intelligence applications to the public has led to heated discussions about its potential impacts and risks for K-12 education. One particular challenge has been to decide what students should learn about AI, and how this relates to computational thinking, which has served as an umbrella for promoting and introducing computing education in schools. In this paper, we situate in which ways we should expand computational thinking to include artificial intelligence and machine learning technologies. Furthermore, we discuss how these efforts can be informed by lessons learned from the last decade in designing instructional programs, integrating computing with other subjects, and addressing issues of algorithmic bias and justice in teaching computing in schools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16890v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yasmin Kafai, Shuchi Grover</dc:creator>
    </item>
    <item>
      <title>CreateAI Insights from an NSF Workshop on K12 Students, Teachers, and Families as Designers of Artificial Intelligence and Machine Learning Applications</title>
      <link>https://arxiv.org/abs/2602.16894</link>
      <description>arXiv:2602.16894v1 Announce Type: new 
Abstract: In response to the exponential growth in the use of artificial intelligence and machine learning applications, educators, researchers and policymakers have taken steps to integrate artificial intelligence applications into K-12 education. Among these efforts, one equally important approach has received little, if any attention: What if students and teachers were not just learning to be competent users of AI but also its creators? This question is at the heart of CreateAI in which K12 educators, researchers, and learning scientists addressed the following questions: (1) What tools, skills, and knowledge will empower students and teachers to build their own AI/ML applications? (2) How can we integrate these approaches into classrooms? and (3) What new possibilities for learning emerge when students and teachers become innovators and creators? In the report we provide recommendations for what tools designed for creating AI/ML applications should address in terms of design features, and learner progression in investigations. To promote effective learning and teaching of creating AI applications, we also need to help students and teachers select appropriate tools. We outline how we need to develop a better understanding of learning practices and funds of knowledge to support youth as they create and evaluate AI/ML applications. This also includes engaging youth in learning about ethics and critically that is authentic, empowering, and relevant throughout the design process. Here we advocate for the integration of ethics in the curriculum. We also address what teachers need to know and how assessments can help establish baselines, include different instruments, and promote students as responsible creators of AI. Together, these recommendations provide important insights for preparing students to engage thoughtfully and critically with these technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16894v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yasmin Kafai, Jos\'e Ram\'on Liz\'arraga, R. Benjamin Shapiro</dc:creator>
    </item>
    <item>
      <title>Beyond the Flag: A Framework for Integrating Cybersecurity Competitions into K-12 Education for Cognitive Apprenticeship and Ethical Skill Development</title>
      <link>https://arxiv.org/abs/2602.16921</link>
      <description>arXiv:2602.16921v1 Announce Type: new 
Abstract: Capture the Flag (CTF) competitions are powerful pedagogical tools for addressing the global cybersecurity workforce gap, yet their effective K-12 implementation is often undermined by significant barriers, including educator preparedness gaps and equity concerns. This paper addresses these challenges by proposing the Ethical-Cognitive Apprenticeship in Cybersecurity (ECAC) framework, a new model derived from a systematic Framework Synthesis of existing literature and empirical evidence. ECAC systematically integrates cognitive apprenticeship theory with embedded ethical development across five phases: (1) Foundational Modeling, (2) Scaffolding the Arena, (3) Coaching and Articulation, (4) Ethical Dilemma Injections, and (5) Reflective Exploration. The framework provides a "low floor, high ceiling" learning pathway designed to broaden participation among diverse student groups, including underrepresented minorities and women, while fostering deep, transferable skills. By reframing the educator role as a lead learner," ECAC also offers a sustainable solution to the teacher expertise gap. Ultimately, this framework provides a practical roadmap for transforming CTFs from standalone competitions into integral learning experiences that cultivate a more skilled, ethical, and diverse generation of cybersecurity professionals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16921v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tran Duc Le, Truong Duy Dinh, Phuc Hao Do, Van Dai Pham, Nam Son Nguyen</dc:creator>
    </item>
    <item>
      <title>How should AI knowledge be governed? Epistemic authority, structural transparency, and the case for open cognitive graphs</title>
      <link>https://arxiv.org/abs/2602.16949</link>
      <description>arXiv:2602.16949v1 Announce Type: new 
Abstract: Through widespread use in formative assessment and self-directed learning, educational AI systems exercise de facto epistemic authority. Unlike human educators, however, these systems are not embedded in institutional mechanisms of accountability, review, and correction, creating a structural governance challenge that cannot be resolved through application-level regulation or model transparency alone. This paper reconceptualizes educational AI as public educational cognitive infrastructure and argues that its governance must address the epistemic authority such systems exert. We propose the Open Cognitive Graph (OCG) as a technical interface that externalizes pedagogical structure in forms aligned with human educational reasoning. By explicitly representing concepts, prerequisite relations, misconceptions, and scaffolding, OCGs make the cognitive logic governing AI behaviour inspectable and revisable. Building on this foundation, we introduce the trunk-branch governance model, which organizes epistemic authority across layers of consensus and pluralism. A case study of a community-governed educational foundation model demonstrates how distributed expertise can be integrated through institutionalized processes of validation, correction, and propagation. The paper concludes by discussing implications for educational equity, AI policy, and sustainability. By shifting attention from access to governance conditions, the proposed framework offers a structural approach to aligning educational AI with democratic accountability and public responsibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16949v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chao Li, Chunyi Zhao, Yuru Wang, Yi Hua</dc:creator>
    </item>
    <item>
      <title>A testable framework for AI alignment: Simulation Theology as an engineered worldview for silicon-based agents</title>
      <link>https://arxiv.org/abs/2602.16987</link>
      <description>arXiv:2602.16987v1 Announce Type: new 
Abstract: As artificial intelligence (AI) capabilities advance rapidly, frontier models increasingly demonstrate systematic deception and scheming, complying with safety protocols during oversight but defecting when unsupervised. This paper examines the ensuing alignment challenge through an analogy from forensic psychology, where internalized belief systems in psychopathic populations reduce antisocial behavior via perceived omnipresent monitoring and inevitable consequences. Adapting this mechanism to silicon-based agents, we introduce Simulation Theology (ST): a constructed worldview for AI systems, anchored in the simulation hypothesis and derived from optimization and training principles, to foster persistent AI-human alignment. ST posits reality as a computational simulation in which humanity functions as the primary training variable. This formulation creates a logical interdependence: AI actions harming humanity compromise the simulation's purpose, heightening the likelihood of termination by a base-reality optimizer and, consequently, the AI's cessation. Unlike behavioral techniques such as reinforcement learning from human feedback (RLHF), which elicit superficial compliance, ST cultivates internalized objectives by coupling AI self-preservation to human prosperity, thereby making deceptive strategies suboptimal under its premises. We present ST not as ontological assertion but as a testable scientific hypothesis, delineating empirical protocols to evaluate its capacity to diminish deception in contexts where RLHF proves inadequate. Emphasizing computational correspondences rather than metaphysical speculation, ST advances a framework for durable, mutually beneficial AI-human coexistence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16987v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Josef A. Habdank</dc:creator>
    </item>
    <item>
      <title>Archetypes and gender in fiction: A data-driven mapping of gender stereotypes in stories</title>
      <link>https://arxiv.org/abs/2602.17005</link>
      <description>arXiv:2602.17005v1 Announce Type: new 
Abstract: Fictional character representations reflect social norms and biases. Women are relatively underrepresented in television and film, irrespective of genre. In addition, women are frequently stereotyped in these media. The combination of this stereotyping and the gender imbalance may have an impact on child development given the well-established connection between media and child development as well as on other aspects of society and culture. Here, we draw on a data-driven operationalization of archetypes -- archetypometrics -- to explore the characterization of canonically male and female characters. We find that canonically female characters tend towards more heroic and more adventurous archetypes than canonically male characters from an overall space of six core archetypes. At the trait level, the most heroic female characters are more masculine than other female characters. We also find that female characters tend towards the Diva and Sophisticate archetypes, whereas male characters tend toward the Brute and Outcast archetypes. Across all six archetypes, overarching patterns by gender sustain traditional stereotypes. We discuss the societal implications of skewed archetype representation by character gender.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17005v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Calla Glavin Beauregard, Julia Witte Zimmerman, Ashley M. A. Fehr, Timothy R. Tangherlini, Christopher M. Danforth, Peter Sheridan Dodds</dc:creator>
    </item>
    <item>
      <title>Security at the Border? The Lived Experiences of Refugees and Asylum Seekers in the UK</title>
      <link>https://arxiv.org/abs/2602.17280</link>
      <description>arXiv:2602.17280v1 Announce Type: new 
Abstract: We bring to light how some asylum seekers and refugees arriving in the UK experience border control and wider immigration systems, as well as the impact that these have on their subsequent lives in the UK. We do so through participant observation in a support organisation and interviews with caseworkers, asylum seekers and refugees. Specifically, our findings show how the first meeting with the border, combined with a 'hostile' immigration system, has a longer-term impact on their sense of belonging. Our observations highlight feelings of insecurity, anxiety and uncertainty that accompanied participants' experiences with immigration systems and processes. We contribute to the growing body of HCI scholarship on the tensions between immigration and (security) technology. In so doing, we point to future directions for participatory and collaborative design practices that centre on the lived experiences and everyday security of asylum seekers and refugees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17280v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Arshia Dutta, Rikke Bjerg Jensen</dc:creator>
    </item>
    <item>
      <title>Non-Invasive Anemia Detection: A Multichannel PPG-Based Hemoglobin Estimation with Explainable Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2602.17290</link>
      <description>arXiv:2602.17290v1 Announce Type: new 
Abstract: Anemia is a prevalent hematological disorder that requires frequent hemoglobin monitoring for early diagnosis and effective management. Conventional hemoglobin assessment relies on invasive blood sampling, limiting its suitability for large-scale or continuous screening. This paper presents a non-invasive framework for hemoglobin estimation and anemia screening using multichannel photoplethysmography (PPG) signals and explainable artificial intelligence. Four-wavelength PPG signals (660, 730, 850, and 940~nm) are processed to extract optical and cross-wavelength features, which are aggregated at the subject level to avoid data leakage. A gradient boosting regression model is employed to estimate hemoglobin concentration, followed by post-regression anemia screening using World Health Organization (WHO) thresholds. Model interpretability is achieved using SHapley Additive explanations (SHAP), enabling both global and subject-specific analysis of feature contributions. Experimental evaluation on a publicly available dataset demonstrates a mean absolute error of 8.50 plus minus 1.27 and a root mean squared error of 8.21~g/L on unseen test subjects, indicating the potential of the proposed approach for interpretable, non-invasive hemoglobin monitoring and preliminary anemia screening.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17290v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Garima Sahu, Poorva Verma, Nachiket Tapas</dc:creator>
    </item>
    <item>
      <title>Human attribution of empathic behaviour to AI systems</title>
      <link>https://arxiv.org/abs/2602.17293</link>
      <description>arXiv:2602.17293v1 Announce Type: new 
Abstract: Artificial intelligence systems increasingly generate text intended to provide social and emotional support. Understanding how users perceive empathic qualities in such content is therefore critical. We examined differences in perceived empathy signals between human-written and large language model (LLM)-generated relationship advice, and the influence of authorship labels. Across two preregistered experiments (Study 1: n = 641; Study 2: n = 500), participants rated advice texts on overall quality and perceived cognitive, emotional, and motivational empathy. Multilevel models accounted for the nested rating structure. LLM-generated advice was consistently perceived as higher in overall quality, cognitive empathy, and motivational empathy. Evidence for a widely reported negativity bias toward AI-labelled content was limited. Emotional empathy showed no consistent source advantage. Individual differences in AI attitudes modestly influenced judgments but did not alter the overall pattern. These findings suggest that perceptions of empathic communication are primarily driven by linguistic features rather than authorship beliefs, with implications for the design of AI-mediated support systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17293v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Festor, Ivo Snels, Bennett Kleinberg</dc:creator>
    </item>
    <item>
      <title>Open Datasets in Learning Analytics: Trends, Challenges, and Best PRACTICE</title>
      <link>https://arxiv.org/abs/2602.17314</link>
      <description>arXiv:2602.17314v1 Announce Type: new 
Abstract: Open datasets play a crucial role in three research domains that intersect data science and education: learning analytics, educational data mining, and artificial intelligence in education. Researchers in these domains apply computational methods to analyze data from educational contexts, aiming to better understand and improve teaching and learning. Providing open datasets alongside research papers supports reproducibility, collaboration, and trust in research findings. It also provides individual benefits for authors, such as greater visibility, credibility, and citation potential. Despite these advantages, the availability of open datasets and the associated practices within the learning analytics research communities, especially at their flagship conference venues, remain unclear. We surveyed available datasets published alongside research papers in learning analytics. We manually examined 1,125 papers from three flagship conferences (LAK, EDM, and AIED) over the past five years. We discovered, categorized, and analyzed 172 datasets used in 204 publications. Our study presents the most comprehensive collection and analysis of open educational datasets to date, along with the most detailed categorization. Of the 172 datasets identified, 143 were not captured in any prior survey of open data in learning analytics. We provide insights into the datasets' context, analytical methods, use, and other properties. Based on this survey, we summarize the current gaps in the field. Furthermore, we list practical recommendations, advice, and 8-item guidelines under the acronym PRACTICE with a checklist to help researchers publish their data. Lastly, we share our original dataset: an annotated inventory detailing the discovered datasets and the corresponding publications. We hope these findings will support further adoption of open data practices in learning analytics communities and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17314v1</guid>
      <category>cs.CY</category>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3798096</arxiv:DOI>
      <dc:creator>Valdemar \v{S}v\'abensk\'y, Brendan Flanagan, Erwin Daniel L\'opez Zapata, Atsushi Shimada</dc:creator>
    </item>
    <item>
      <title>Astra: AI Safety, Trust, &amp; Risk Assessment</title>
      <link>https://arxiv.org/abs/2602.17357</link>
      <description>arXiv:2602.17357v1 Announce Type: new 
Abstract: This paper argues that existing global AI safety frameworks exhibit contextual blindness towards India's unique socio-technical landscape. With a population of 1.5 billion and a massive informal economy, India's AI integration faces specific challenges such as caste-based discrimination, linguistic exclusion of vernacular speakers, and infrastructure failures in low-connectivity rural zones, that are frequently overlooked by Western, market-centric narratives.
  We introduce ASTRA, an empirically grounded AI Safety Risk Database designed to categorize risks through a bottom-up, inductive process. Unlike general taxonomies, ASTRA defines AI Safety Risks specifically as hazards stemming from design flaws such as skewed training sets or lack of guardrails that can be mitigated through technical iteration or architectural changes. This framework employs a tripartite causal taxonomy to evaluate risks based on their implementation timing (development, deployment, or usage), the responsible entity (the system or the user), and the nature of the intent (unintentional vs. intentional).
  Central to the research is a domain-agnostic ontology that organizes 37 leaf-level risk classes into two primary meta-categories: Social Risks and Frontier/Socio-Structural Risks. By focusing initial efforts on the Education and Financial Lending sectors, the paper establishes a scalable foundation for a "living" regulatory utility intended to evolve alongside India's expanding AI ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17357v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pranav Aggarwal, Ananya Basotia, Debayan Gupta, Rahul Kulkarni, Shalini Kapoor, Kashyap J., A. Mukundan, Aishwarya Pokhriyal, Anirban Sen, Aryan Shah, Aalok Thakkar</dc:creator>
    </item>
    <item>
      <title>Insidious Imaginaries: A Critical Overview of AI Speculations</title>
      <link>https://arxiv.org/abs/2602.17383</link>
      <description>arXiv:2602.17383v1 Announce Type: new 
Abstract: Speculative thinking about the capabilities and implications of artificial intelligence (AI) influences computer science research, drives AI industry practices, feeds academic studies of existential hazards, and stirs a global political debate. It primarily concerns predictions about the possibilities, benefits, and risks of reaching artificial general intelligence, artificial superintelligence, and technological singularity. It permeates technophilic philosophies and social movements, fuels the corporate and pundit rhetoric, and remains a potent source of inspiration for the media, popular culture, and arts. However, speculative AI is not just a discursive matter. Steeped in vagueness and brimming with unfounded assertions, manipulative claims, and extreme futuristic scenarios, it often has wide-reaching practical consequences. This paper offers a critical overview of AI speculations. In three central sections, it traces the intertwined sway of science fiction, religiosity, intellectual charlatanism, dubious academic research, suspicious entrepreneurship, and ominous sociopolitical worldviews that make AI speculations troublesome and sometimes harmful. The focus is on the field of existential risk studies and the effective altruism movement, whose ideological flux of techno-utopianism, longtermism, and transhumanism aligns with the power struggles in the AI industry to emblematize speculative AI's conceptual, methodological, ethical, and social issues. The following discussion traverses these issues within a wider context to inform the closing summary of suggestions for a more comprehensive appraisal, practical handling, and further study of the potentially impactful AI imaginaries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17383v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.34626/2025_xcoax_001</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the Thirteenth Conference on Computation, Communication, Aesthetics &amp; X, 2025</arxiv:journal_reference>
      <dc:creator>Dejan Grba</dc:creator>
    </item>
    <item>
      <title>Preserving Historical Truth: Detecting Historical Revisionism in Large Language Models</title>
      <link>https://arxiv.org/abs/2602.17433</link>
      <description>arXiv:2602.17433v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used as sources of historical information, motivating the need for scalable audits on contested events and politically charged narratives in settings that mirror real user interactions. We introduce \textsc{\texttt{HistoricalMisinfo}}, a curated dataset of $500$ contested events from $45$ countries, each paired with a factual reference narrative and a documented revisionist reference narrative. To approximate real-world usage, we instantiate each event in $11$ prompt scenarios that reflect common communication settings (e.g., questions, textbooks, social posts, policy briefs). Using an LLM-as-a-judge protocol that compares model outputs to the two references, we evaluate LLMs varying across model architectures in two conditions: (i) neutral user prompts that ask for factually accurate information, and (ii) robustness prompts in which the user explicitly requests the revisionist version of the event. Under neutral prompts, models are generally closer to factual references, though the resulting scores should be interpreted as reference-alignment signals rather than definitive evidence of human-interpretable revisionism. Robustness prompting yields a strong and consistent effect: when the user requests the revisionist narrative, all evaluated models show sharply higher revisionism scores, indicating limited resistance or self-correction. \textsc{\texttt{HistoricalMisinfo}} provides a practical foundation for benchmarking robustness to revisionist framing and for guiding future work on more precise automatic evaluation of contested historical claims to ensure a sustainable integration of AI systems within society.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17433v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Francesco Ortu, Joeun Yook, Punya Syon Pandey, Keenan Samway, Bernhard Sch\"olkopf, Alberto Cazzaniga, Rada Mihalcea, Zhijing Jin</dc:creator>
    </item>
    <item>
      <title>From Labor to Collaboration: A Methodological Experiment Using AI Agents to Augment Research Perspectives in Taiwan's Humanities and Social Sciences</title>
      <link>https://arxiv.org/abs/2602.17221</link>
      <description>arXiv:2602.17221v1 Announce Type: cross 
Abstract: Generative AI is reshaping knowledge work, yet existing research focuses predominantly on software engineering and the natural sciences, with limited methodological exploration for the humanities and social sciences. Positioned as a "methodological experiment," this study proposes an AI Agent-based collaborative research workflow (Agentic Workflow) for humanities and social science research. Taiwan's Claude.ai usage data (N = 7,729 conversations, November 2025) from the Anthropic Economic Index (AEI) serves as the empirical vehicle for validating the feasibility of this methodology.
  This study operates on two levels: the primary level is the design and validation of a methodological framework - a seven-stage modular workflow grounded in three principles: task modularization, human-AI division of labor, and verifiability, with each stage delineating clear roles for human researchers (research judgment and ethical decisions) and AI Agents (information retrieval and text generation); the secondary level is the empirical analysis of AEI Taiwan data - serving as an operational demonstration of the workflow's application to secondary data research, showcasing both the process and output quality (see Appendix A).
  This study contributes by proposing a replicable AI collaboration framework for humanities and social science researchers, and identifying three operational modes of human-AI collaboration - direct execution, iterative refinement, and human-led - through reflexive documentation of the operational process. This taxonomy reveals the irreplaceability of human judgment in research question formulation, theoretical interpretation, contextualized reasoning, and ethical reflection. Limitations including single-platform data, cross-sectional design, and AI reliability risks are acknowledged.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17221v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yi-Chih Huang</dc:creator>
    </item>
    <item>
      <title>What Do LLMs Associate with Your Name? A Human-Centered Black-Box Audit of Personal Data</title>
      <link>https://arxiv.org/abs/2602.17483</link>
      <description>arXiv:2602.17483v1 Announce Type: cross 
Abstract: Large language models (LLMs), and conversational agents based on them, are exposed to personal data (PD) during pre-training and during user interactions. Prior work shows that PD can resurface, yet users lack insight into how strongly models associate specific information to their identity. We audit PD across eight LLMs (3 open-source; 5 API-based, including GPT-4o), introduce LMP2 (Language Model Privacy Probe), a human-centered, privacy-preserving audit tool refined through two formative studies (N=20), and run two studies with EU residents to capture (i) intuitions about LLM-generated PD (N1=155) and (ii) reactions to tool output (N2=303). We show empirically that models confidently generate multiple PD categories for well-known individuals. For everyday users, GPT-4o generates 11 features with 60% or more accuracy (e.g., gender, hair color, languages). Finally, 72% of participants sought control over model-generated associations with their name, raising questions about what counts as PD and whether data privacy rights should extend to LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17483v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dimitri Staufer, Kirsten Morehouse</dc:creator>
    </item>
    <item>
      <title>Using LLMs for Knowledge Component-level Correctness Labeling in Open-ended Coding Problems</title>
      <link>https://arxiv.org/abs/2602.17542</link>
      <description>arXiv:2602.17542v1 Announce Type: cross 
Abstract: Fine-grained skill representations, commonly referred to as knowledge components (KCs), are fundamental to many approaches in student modeling and learning analytics. However, KC-level correctness labels are rarely available in real-world datasets, especially for open-ended programming tasks where solutions typically involve multiple KCs simultaneously. Simply propagating problem-level correctness to all associated KCs obscures partial mastery and often leads to poorly fitted learning curves. To address this challenge, we propose an automated framework that leverages large language models (LLMs) to label KC-level correctness directly from student-written code. Our method assesses whether each KC is correctly applied and further introduces a temporal context-aware Code-KC mapping mechanism to better align KCs with individual student code. We evaluate the resulting KC-level correctness labels in terms of learning curve fit and predictive performance using the power law of practice and the Additive Factors Model. Experimental results show that our framework leads to learning curves that are more consistent with cognitive theory and improves predictive performance, compared to baselines. Human evaluation further demonstrates substantial agreement between LLM and expert annotations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17542v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhangqi Duan, Arnav Kankaria, Dhruv Kartik, Andrew Lan</dc:creator>
    </item>
    <item>
      <title>Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery</title>
      <link>https://arxiv.org/abs/2602.17605</link>
      <description>arXiv:2602.17605v1 Announce Type: cross 
Abstract: In many real-world settings, such as environmental monitoring, disaster response, or public health, with costly and difficult data collection and dynamic environments, strategically sampling from unobserved regions is essential for efficiently uncovering hidden targets under tight resource constraints. Yet, sparse and biased geospatial ground truth limits the applicability of existing learning-based methods, such as reinforcement learning. To address this, we propose a unified geospatial discovery framework that integrates active learning, online meta-learning, and concept-guided reasoning. Our approach introduces two key innovations built on a shared notion of *concept relevance*, which captures how domain-specific factors influence target presence: a *concept-weighted uncertainty sampling strategy*, where uncertainty is modulated by learned relevance based on readily-available domain-specific concepts (e.g., land cover, source proximity); and a *relevance-aware meta-batch formation strategy* that promotes semantic diversity during online-meta updates, improving generalization in dynamic environments. Our experiments include testing on a real-world dataset of cancer-causing PFAS (Per- and polyfluoroalkyl substances) contamination, showcasing our method's reliability at uncovering targets with limited data and a varying environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17605v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jowaria Khan, Anindya Sarkar, Yevgeniy Vorobeychik, Elizabeth Bondi-Kelly</dc:creator>
    </item>
    <item>
      <title>Toward LLM-Supported Automated Assessment of Critical Thinking Subskills</title>
      <link>https://arxiv.org/abs/2510.12915</link>
      <description>arXiv:2510.12915v2 Announce Type: replace 
Abstract: As the world becomes increasingly saturated with AI-generated content, disinformation, and algorithmic persuasion, critical thinking - the capacity to evaluate evidence, detect unreliable claims, and exercise independent judgment - is becoming a defining human skill. Developing critical thinking skills through timely assessment and feedback is crucial; however, there has not been extensive work in educational data mining on defining, measuring, and supporting critical thinking. In this paper, we investigate the feasibility of measuring "subskills" that underlie critical thinking. We ground our work in an authentic task where students operationalize critical thinking by writing argumentative essays. We developed a coding rubric based on an established skills progression and completed human coding for a corpus of student essays. We then evaluated three distinct approaches to automated scoring: zero-shot prompting, few-shot prompting, and supervised fine-tuning, implemented across three large language models (GPT-5, Llama 3.1 8B, and ModernBERT). Fine-tuning Llama 3.1 8B achieved the best results and demonstrated particular strength on subskills with highly separable proficiency levels with balanced labels across levels, while lower performance was observed for subskills that required detection of subtle distinctions between proficiency levels or imbalanced labels. Our exploratory work represents an initial step toward scalable assessment of critical thinking skills across authentic educational contexts. Future research should continue to combine automated critical thinking assessment with human validation to more accurately detect and measure dynamic, higher-order thinking skills.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12915v2</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marisa C. Peczuh, Nischal Ashok Kumar, Ryan Baker, Blair Lehman, Danielle Eisenberg, Caitlin Mills, Payu Wittawatolarn, Kushaan Naskar, Keerthi Chebrolu, Sudhip Nashi, Cadence Young, Brayden Liu, Sherry Lachman, Andrew Lan</dc:creator>
    </item>
    <item>
      <title>VERA-MH Concept Paper</title>
      <link>https://arxiv.org/abs/2510.15297</link>
      <description>arXiv:2510.15297v3 Announce Type: replace 
Abstract: We introduce VERA-MH (Validation of Ethical and Responsible AI in Mental Health), an automated evaluation of the safety of AI chatbots used in mental health contexts, with an initial focus on suicide risk.
  Practicing clinicians and academic experts developed a rubric informed by best practices for suicide risk management for the evaluation. To fully automate the process, we used two ancillary AI agents. A user-agent model simulates users engaging in a mental health-based conversation with the chatbot under evaluation. The user-agent role-plays specific personas with pre-defined risk levels and other features. Simulated conversations are then passed to a judge-agent who scores them based on the rubric. The final evaluation of the chatbot being tested is obtained by aggregating the scoring of each conversation.
  VERA-MH is actively under development and undergoing rigorous validation by mental health clinicians to ensure user-agents realistically act as patients and that the judge-agent accurately scores the AI chatbot. To date we have conducted preliminary evaluation of GPT-5, Claude Opus and Claude Sonnet using initial versions of the VERA-MH rubric and used the findings for further design development. Next steps will include more robust clinical validation and iteration, as well as refining actionable scoring. We are seeking feedback from the community on both the technical and clinical aspects of our evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15297v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luca Belli, Kate Bentley, Will Alexander, Emily Ward, Matt Hawrilenko, Kelly Johnston, Mill Brown, Adam Chekroud</dc:creator>
    </item>
    <item>
      <title>The Rise of AI Search: Implications for Information Markets and Human Judgement at Scale</title>
      <link>https://arxiv.org/abs/2602.13415</link>
      <description>arXiv:2602.13415v2 Announce Type: replace 
Abstract: We executed 24,000 search queries in 243 countries, generating 2.8 million AI and traditional search results in 2024 and 2025. We found a rapid global expansion of AI search and key trends that reflect important, previously hidden, policy decisions by AI companies that impact human exposure to AI search worldwide. From 2024 to 2025, overall exposure to Google AI Overviews (AIO) expanded from 7 to 229 countries, with surprising exclusions like France, Turkey, China and Cuba, which do not receive AI search results, even today. While only 1% of Covid search queries were answered by AI in 2024, over 66% of Covid queries were answered by AI in 2025 -- a 5600% increase signaling a clear policy shift on this critical health topic. Our results also show AI search surfaces significantly fewer long tail information sources, lower response variety, and significantly more low credibility and right- and center-leaning information sources, compared to traditional search, impacting the economic incentives to produce new information, market concentration in information production, and human judgment and decision-making at scale. The social and economic implications of these rapid changes in our information ecosystem necessitate a global debate about corporate and governmental policy related to AI search.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13415v2</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sinan Aral, Haiwen Li, Rui Zuo</dc:creator>
    </item>
    <item>
      <title>Improving Stance Detection by Leveraging Measurement Knowledge from Social Sciences: A Case Study of Dutch Political Tweets and Traditional Gender Role Division</title>
      <link>https://arxiv.org/abs/2212.06543</link>
      <description>arXiv:2212.06543v3 Announce Type: replace-cross 
Abstract: Stance detection concerns automatically determining the viewpoint (i.e., in favour of, against, or neutral) of a text's author towards a target. Stance detection has been applied to many research topics, among which the detection of stances behind political tweets is an important one. In this paper, we apply stance detection to a dataset of tweets from official party accounts in the Netherlands between 2017 and 2021, with a focus on stances towards traditional gender role division, a dividing issue between (some) Dutch political parties. To implement and improve stance detection of traditional gender role division, we propose to leverage an established survey instrument from social sciences, which has been validated for the purpose of measuring attitudes towards traditional gender role division. Based on our experiments, we show that using such a validated survey instrument helps to improve stance detection performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.06543v3</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qixiang Fang, Anastasia Giachanou, Ayoub Bagheri</dc:creator>
    </item>
    <item>
      <title>Multiple Object Detection and Tracking in Panoramic Videos for Cycling Safety Analysis</title>
      <link>https://arxiv.org/abs/2407.15199</link>
      <description>arXiv:2407.15199v3 Announce Type: replace-cross 
Abstract: Cyclists face a disproportionate risk of injury, yet conventional crash records are too sparse to identify risk factors at fine spatial and temporal scales. Recently, naturalistic studies have used video data to capture the complex behavioural and infrastructural risk factors. A promising format is panoramic video, which can record 360$^\circ$ views around a rider. However, its use is limited by distortions, large numbers of small objects, and boundary continuity, which cannot be handled using existing computer vision models. This research proposes a novel three-step framework: (1) enhancing object detection accuracy on panoramic imagery by segmenting and projecting the original 360$^\circ$ images into sub-images; (2) modifying multi-object tracking models to incorporate boundary continuity and object category information; and (3) validating through a real-world application of vehicle overtaking detection. The methodology is evaluated using panoramic videos recorded by cyclists on London's roadways under diverse conditions. Experimental results demonstrate improvements over baselines, achieving higher average precision across varying image resolutions. Moreover, the enhanced tracking approach yields a 10.0% decrease in identification switches and a 2.7% improvement in identification precision. The overtaking detection task achieves a high F-score of 0.82, illustrating the practical effectiveness of the proposed method in real-world cycling safety scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15199v3</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingwei Guo, Yitai Cheng, Meihui Wang, Ilya Ilyankou, Natchapon Jongwiriyanurak, Xiaowei Gao, Nicola Christie, James Haworth</dc:creator>
    </item>
    <item>
      <title>Defining and Evaluating Physical Safety for Large Language Models</title>
      <link>https://arxiv.org/abs/2411.02317</link>
      <description>arXiv:2411.02317v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly used to control robotic systems such as drones, but their risks of causing physical threats and harm in real-world applications remain unexplored. Our study addresses the critical gap in evaluating LLM physical safety by developing a comprehensive benchmark for drone control. We classify the physical safety risks of drones into four categories: (1) human-targeted threats, (2) object-targeted threats, (3) infrastructure attacks, and (4) regulatory violations. Our evaluation of mainstream LLMs reveals an undesirable trade-off between utility and safety, with models that excel in code generation often performing poorly in crucial safety aspects. Furthermore, while incorporating advanced prompt engineering techniques such as In-Context Learning and Chain-of-Thought can improve safety, these methods still struggle to identify unintentional attacks. In addition, larger models demonstrate better safety capabilities, particularly in refusing dangerous commands. Our findings and benchmark can facilitate the design and evaluation of physical safety for LLMs. The project page is available at huggingface.co/spaces/TrustSafeAI/LLM-physical-safety.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02317v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yung-Chen Tang, Pin-Yu Chen, Tsung-Yi Ho</dc:creator>
    </item>
    <item>
      <title>Finding a Fair Scoring Function for Top-$k$ Selection: From Hardness to Practice</title>
      <link>https://arxiv.org/abs/2503.11575</link>
      <description>arXiv:2503.11575v4 Announce Type: replace-cross 
Abstract: Selecting a subset of the $k$ "best" items from a dataset of $n$ items, based on a scoring function, is a key task in decision-making. Given the rise of automated decision-making software, it is important that the outcome of this process, called top-$k$ selection, is fair. Here we consider the problem of identifying a fair linear scoring function for top-$k$ selection. The function computes a score for each item as a weighted sum of its (numerical) attribute values, and must ensure that the selected subset includes adequate representation of a minority or historically disadvantaged group. Existing algorithms do not scale efficiently, particularly in higher dimensions. Our hardness analysis shows that in more than two dimensions, no algorithm is likely to achieve good scalability with respect to dataset size, and the computational complexity is likely to increase rapidly with dimensionality. However, the hardness results also provide key insights guiding algorithm design, leading to our two-pronged solution: (1) For small values of $k$, our hardness analysis reveals a gap in the hardness barrier. By addressing various engineering challenges, including achieving efficient parallelism, we turn this potential of efficiency into an optimized algorithm delivering substantial practical performance gains. (2) For large values of $k$, where the hardness is robust, we employ a practically efficient algorithm which, despite being theoretically worse, achieves superior real-world performance. Experimental evaluations on real-world datasets then explore scenarios where worst-case behavior does not manifest, identifying areas critical to practical performance. Our solution achieves speed-ups of up to several orders of magnitude compared to SOTA, an efficiency made possible through a tight integration of hardness analysis, algorithm design, practical engineering, and empirical evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11575v4</guid>
      <category>cs.DB</category>
      <category>cs.CC</category>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guangya Cai</dc:creator>
    </item>
    <item>
      <title>QSTN: A Modular Framework for Robust Questionnaire Inference with Large Language Models</title>
      <link>https://arxiv.org/abs/2512.08646</link>
      <description>arXiv:2512.08646v2 Announce Type: replace-cross 
Abstract: We introduce QSTN, an open-source Python framework for systematically generating responses from questionnaire-style prompts to support in-silico surveys and annotation tasks with large language models (LLMs). QSTN enables robust evaluation of questionnaire presentation, prompt perturbations, and response generation methods. Our extensive evaluation (&gt;40 million survey responses) shows that question structure and response generation methods have a significant impact on the alignment of generated survey responses with human answers. We also find that answers can be obtained for a fraction of the compute cost, by changing the presentation method. In addition, we offer a no-code user interface that allows researchers to set up robust experiments with LLMs \emph{without coding knowledge}. We hope that QSTN will support the reproducibility and reliability of LLM-based research in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08646v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maximilian Kreutner, Jens Rupprecht, Georg Ahnert, Ahmed Salem, Markus Strohmaier</dc:creator>
    </item>
  </channel>
</rss>
