<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Jul 2025 04:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Spatialize v1.0: A Python/C++ Library for Ensemble Spatial Interpolation</title>
      <link>https://arxiv.org/abs/2507.17867</link>
      <description>arXiv:2507.17867v1 Announce Type: new 
Abstract: In this paper, we present Spatialize, an open-source library that implements ensemble spatial interpolation, a novel method that combines the simplicity of basic interpolation methods with the power of classical geostatistical tools, like Kriging. It leverages the richness of stochastic modelling and ensemble learning, making it robust, scalable and suitable for large datasets. In addition, Spatialize provides a powerful framework for uncertainty quantification, offering both point estimates and empirical posterior distributions. It is implemented in Python 3.x, with a C++ core for improved performance, and is designed to be easy to use, requiring minimal user intervention. This library aims to bridge the gap between expert and non-expert users of geostatistics by providing automated tools that rival traditional geostatistical methods. Here, we present a detailed description of Spatialize along with a wealth of examples of its use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17867v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alvaro F. Ega\~na, Alejandro Ehrenfeld, Felipe Garrido, Mar\'ia Jes\'us Valenzuela, Juan F. S\'anchez-P\'erez</dc:creator>
    </item>
    <item>
      <title>Multinomial thresholded LASSO for interpretable dimension reduction of human activity sequences</title>
      <link>https://arxiv.org/abs/2507.17900</link>
      <description>arXiv:2507.17900v1 Announce Type: new 
Abstract: The widespread collection of data from mobile and wearable devices has created unprecedented opportunities to study human behavior in fine temporal resolution. One common structure for such data is categorical sequences: ordered, multinomial observations across many time points. These sequences present unique statistical challenges due to their high dimensionality and complex temporal dependence, including both short- and long-term correlations. Yet, there has been relatively little methodological development focusing on principled dimension reduction specifically tailored to this type of data. In this paper, we develop and evaluate approaches to identifying "key" sequence positions which distinguish sequence types. We frame this challenge as a regression problem, introduce a variety of regularization techniques that could be applied to achieve position-based dimension reduction, and evaluate them on the motivating dataset that reflects daily time use patterns collected via a smartphone application. Results show that the thresholded LASSO, a relatively underused technique, performs better than more established methods for data with complex sequential structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17900v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zuofu Huang, Yingling Fan, James Hodges, Julian Wolfson</dc:creator>
    </item>
    <item>
      <title>Bayesian Variable Selection in Multivariate Regression Under Collinearity in the Design Matrix</title>
      <link>https://arxiv.org/abs/2507.17975</link>
      <description>arXiv:2507.17975v1 Announce Type: new 
Abstract: We consider the problem of variable selection in Bayesian multivariate linear regression models, involving multiple response and predictor variables, under multivariate normal errors. In the absence of a known covariance structure, specifying a model with a non-diagonal covariance matrix is appealing. Modeling dependency in the random errors through a non-diagonal covariance matrix is generally expected to lead to improved estimation of the regression coefficients. In this article, we highlight an interesting exception: modeling the dependency in errors can significantly worsen both estimation and prediction. We demonstrate that Bayesian multi-outcome regression models using several popular variable selection priors can suffer from poor estimation properties in low-information settings--such as scenarios with weak signals, high correlation among predictors and responses, and small sample sizes. In such cases, the simultaneous estimation of all unknown parameters in the model becomes difficult when using a non-diagonal covariance matrix. Through simulation studies and a dataset with measurements from NIR spectroscopy, we illustrate that a two-step procedure--estimating the mean and the covariance matrix separately--can provide more accurate estimates in such cases. Thus, a potential solution to avoid the problem altogether is to routinely perform an additional analysis with a diagonal covariance matrix, even if the errors are expected to be correlated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17975v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joyee Ghosh, Xun Li</dc:creator>
    </item>
    <item>
      <title>A Jarque-Bera test for skew normal data</title>
      <link>https://arxiv.org/abs/2507.18032</link>
      <description>arXiv:2507.18032v1 Announce Type: new 
Abstract: The skew normal law has been introduced in Azzalin (1985) as an alternative to adjusting asymmetric data that share important patterns with the normal law. It has been extensively studied. However, there is so much to do in order to catch the diversity and the richness of the investigation of its normal counterpart. The General Jarque-Berra Test (GJBT) has been devised by Lo et al. (2015), Da et al. (2023) for arbitrary laws with at least finite first eight moments, as a generalization of the Jarque-Bera (1987) test that was specially set up for normal data. Here, we particularize it to skew normal data. When particularized in the skew normal law, this test is proven to be extremely powerful in detecting the true model for any $\alpha\neq 0$ and rejected the normal law ($\alpha=0$) whatever be the size of the data. We introduced the use of the samples duplication method to reach a high level of efficiency for the test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18032v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Diam Ba, Gorgui Gning, Gandasor Bonyiri Onesiphore Da, Oumar Foly Sow, Gane Samb Lo</dc:creator>
    </item>
    <item>
      <title>Large-scale entity resolution via microclustering Ewens--Pitman random partitions</title>
      <link>https://arxiv.org/abs/2507.18101</link>
      <description>arXiv:2507.18101v1 Announce Type: new 
Abstract: We introduce the microclustering Ewens--Pitman model for random partitions, obtained by scaling the strength parameter of the Ewens--Pitman model linearly with the sample size. The resulting random partition is shown to have the microclustering property, namely: the size of the largest cluster grows sub-linearly with the sample size, while the number of clusters grows linearly. By leveraging the interplay between the Ewens--Pitman random partition with the Pitman--Yor process, we develop efficient variational inference schemes for posterior computation in entity resolution. Our approach achieves a speed-up of three orders of magnitude over existing Bayesian methods for entity resolution, while maintaining competitive empirical performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18101v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Beraha, Stefano Favaro</dc:creator>
    </item>
    <item>
      <title>Regression approaches for modelling genotype-environment interaction and making predictions into unseen environments</title>
      <link>https://arxiv.org/abs/2507.18125</link>
      <description>arXiv:2507.18125v1 Announce Type: new 
Abstract: In plant breeding and variety testing, there is an increasing interest in making use of environmental information to enhance predictions for new environments. Here, we will review linear mixed models that have been proposed for this purpose. The emphasis will be on predictions and on methods to assess the uncertainty of predictions for new environments. Our point of departure is straight-line regression, which may be extended to multiple environmental covariates and genotype-specific responses. When observable environmental covariates are used, this is also known as factorial regression. Early work along these lines can be traced back to Stringfield &amp; Salter (1934) and Yates &amp; Cochran (1938), who proposed a method nowadays best known as Finlay-Wilkinson regression. This method, in turn, has close ties with regression on latent environmental covariates and factor-analytic variance-covariance structures for genotype-environment interaction. Extensions of these approaches - reduced rank regression, kernel- or kinship-based approaches, random coefficient regression, and extended Finlay-Wilkinson regression - will be the focus of this paper. Our objective is to demonstrate how seemingly disparate methods are very closely linked and fall within a common model-based prediction framework. The framework considers environments as random throughout, with genotypes also modelled as random in most cases. We will discuss options for assessing uncertainty of predictions, including cross validation and model-based estimates of uncertainty. The methods are illustrated using a long-term rice variety trial dataset from Bangladesh.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18125v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maksym Hrachov, Hans-Peter Piepho, Niaz Md. Farhat Rahman, Waqas Ahmed Malik</dc:creator>
    </item>
    <item>
      <title>Moment Martingale Posteriors for Semiparametric Predictive Bayes</title>
      <link>https://arxiv.org/abs/2507.18148</link>
      <description>arXiv:2507.18148v1 Announce Type: new 
Abstract: The predictive Bayesian view involves eliciting a sequence of one-step-ahead predictive distributions in lieu of specifying a likelihood function and prior distribution. Recent methods have leveraged predictive distributions which are either nonparametric or parametric, but not a combination of the two. This paper introduces a semiparametric martingale posterior which utilizes a predictive distribution that is a mixture of a parametric and nonparametric component. The semiparametric nature of the predictive allows for regularization of the nonparametric component when the sample size is small, and robustness to model misspecification of the parametric component when the sample size is large. We call this approach the moment martingale posterior, as the core of our proposed methodology is to utilize the method of moments as the vehicle for tying the nonparametric and parametric components together. In particular, the predictives are constructed so that the moments are martingales, which allows us to verify convergence under predictive resampling. A key contribution of this work is a novel procedure based on the energy score to optimally weigh between the parametric and nonparametric components, which has desirable asymptotic properties. The effectiveness of the proposed approach is demonstrated through simulations and a real world example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18148v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiu Yin Yung, Stephen M. S. Lee, Edwin Fong</dc:creator>
    </item>
    <item>
      <title>Convergence Rate of Efficient MCMC with Ancillarity-Sufficiency Interweaving Strategy for Panel Data Models</title>
      <link>https://arxiv.org/abs/2507.18404</link>
      <description>arXiv:2507.18404v1 Announce Type: new 
Abstract: Improving Markov chain Monte Carlo algorithm efficiency is essential for enhancing computational speed and inferential accuracy in Bayesian analysis. These improvements can be effectively achieved using the ancillarity-sufficiency interweaving strategy (ASIS), an effective means of achieving such gains. Herein, we provide the first rigorous theoretical justification for applying ASIS in Bayesian hierarchical panel data models. Asymptotic analysis demonstrated that when the product of prior variance of unobserved heterogeneity and cross-sectional sample size N is sufficiently large, the latent individual effects can be sampled almost independently of their global mean. This near-independence accounts for ASIS's rapid mixing behavior and highlights its suitability for modern "tall" panel datasets. We derived simple inequalities to predict which conventional data augmentation scheme-sufficient augmentation (SA) or ancillary augmentation (AA)-yields faster convergence. By interweaving SA and AA, ASIS achieves optimal geometric rate of convergence and renders the Markov chain for the global mean parameter asymptotically independent and identically distributed. Monte Carlo experiment confirm that this theoretical efficiency ordering holds even for small panels (e.g., N=10). These findings confirm the empirical success of ASIS application across finance, marketing, and sports, laying the groundwork for its extension to models with more complex covariate structures and nonGaussian specifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18404v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Makoto Nakakita, Tomoki Toyabe, Teruo Nakatsuma, Takahiro Hoshino</dc:creator>
    </item>
    <item>
      <title>INLA-RF: A Hybrid Modeling Strategy for Spatio-Temporal Environmental Data</title>
      <link>https://arxiv.org/abs/2507.18488</link>
      <description>arXiv:2507.18488v1 Announce Type: new 
Abstract: Environmental processes often exhibit complex, non-linear patterns and discontinuities across space and time, posing significant challenges for traditional geostatistical modeling approaches. In this paper, we propose a hybrid spatio-temporal modeling framework that combines the interpretability and uncertainty quantification of Bayesian models -- estimated using the INLA-SPDE approach -- with the predictive power and flexibility of Random Forest (RF). Specifically, we introduce two novel algorithms, collectively named INLA-RF, which integrate a statistical spatio-temporal model with RF in an iterative two-stage framework. The first algorithm (INLA-RF1) incorporates RF predictions as an offset in the INLA-SPDE model, while the second (INLA-RF2) uses RF to directly correct selected latent field nodes. Both hybrid strategies enable uncertainty propagation between modeling stages, an aspect often overlooked in existing hybrid approaches. In addition, we propose a Kullback-Leibler divergence-based stopping criterion. We evaluate the predictive performance and uncertainty quantification capabilities of the proposed algorithms through two simulation studies. Results suggest that our hybrid approach enhances spatio-temporal prediction while maintaining interpretability and coherence in uncertainty estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18488v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mario Figueira, Michela Cameletti, Luca Patelli</dc:creator>
    </item>
    <item>
      <title>How weak are weak factors? Uniform inference for signal strength in signal plus noise models</title>
      <link>https://arxiv.org/abs/2507.18554</link>
      <description>arXiv:2507.18554v1 Announce Type: new 
Abstract: The paper analyzes four classical signal-plus-noise models: the factor model, spiked sample covariance matrices, the sum of a Wigner matrix and a low-rank perturbation, and canonical correlation analysis with low-rank dependencies. The objective is to construct confidence intervals for the signal strength that are uniformly valid across all regimes - strong, weak, and critical signals. We demonstrate that traditional Gaussian approximations fail in the critical regime. Instead, we introduce a universal transitional distribution that enables valid inference across the entire spectrum of signal strengths. The approach is illustrated through applications in macroeconomics and finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18554v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Bykhovskaya, Vadim Gorin, Sasha Sodin</dc:creator>
    </item>
    <item>
      <title>An omnibus goodness-of-fit test based on trigonometric moments</title>
      <link>https://arxiv.org/abs/2507.18591</link>
      <description>arXiv:2507.18591v1 Announce Type: new 
Abstract: We present a versatile omnibus goodness-of-fit test based on the first two trigonometric moments of probability-integral-transformed data, which rectifies the covariance scaling errors made by Langholz and Kronmal [J. Amer. Statist. Assoc. 86 (1991), 1077--1084]. Once properly scaled, the quadratic-form statistic asymptotically follows a $\chi_2^2$ distribution under the null hypothesis. The covariance scalings and parameter estimators are provided for $32$ null distribution families, covering heavy-tailed, light-tailed, asymmetric, and bounded-support cases, so the test is ready to be applied directly. Using recent advances in non-degenerate multivariate $U$-statistics with estimated nuisance parameters, we also showcase its asymptotic distribution under local alternatives for three specific examples. Our procedure shows excellent power; in particular, simulations testing the Laplace model against a range of $400$ alternatives reveal that it surpasses all $40$ existing tests for moderate to large sample sizes. A real-data application involving 48-hour-ahead surface temperature forecast errors further demonstrates the practical utility of the test. To ensure full reproducibility, the R code that generated our numerical results is publicly accessible online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18591v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alain Desgagn\'e, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>On Focusing Statistical Power for Searches and Measurements in Particle Physics</title>
      <link>https://arxiv.org/abs/2507.17831</link>
      <description>arXiv:2507.17831v1 Announce Type: cross 
Abstract: Particle physics experiments rely on the (generalised) likelihood ratio test (LRT) for searches and measurements, which consist of composite hypothesis tests. However, this test is not guaranteed to be optimal, as the Neyman-Pearson lemma pertains only to simple hypothesis tests. Any choice of test statistic thus implicitly determines how statistical power varies across the parameter space. An improvement in the core statistical testing methodology for general settings with composite tests would have widespread ramifications across experiments. We discuss an alternate test statistic that provides the data analyzer an ability to focus the power of the test on physics-motivated regions of the parameter space. We demonstrate the improvement from this technique compared to the LRT on a Higgs $\rightarrow\tau\tau$ dataset simulated by the ATLAS experiment and a dark matter dataset inspired by the LZ experiment. We also employ machine learning to efficiently perform the Neyman construction, which is essential to ensure statistically valid confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17831v1</guid>
      <category>hep-ph</category>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Carzon, Aishik Ghosh, Rafael Izbicki, Ann Lee, Luca Masserano, Daniel Whiteson</dc:creator>
    </item>
    <item>
      <title>Sliding Window Informative Canonical Correlation Analysis</title>
      <link>https://arxiv.org/abs/2507.17921</link>
      <description>arXiv:2507.17921v1 Announce Type: cross 
Abstract: Canonical correlation analysis (CCA) is a technique for finding correlated sets of features between two datasets. In this paper, we propose a novel extension of CCA to the online, streaming data setting: Sliding Window Informative Canonical Correlation Analysis (SWICCA). Our method uses a streaming principal component analysis (PCA) algorithm as a backend and uses these outputs combined with a small sliding window of samples to estimate the CCA components in real time. We motivate and describe our algorithm, provide numerical simulations to characterize its performance, and provide a theoretical performance guarantee. The SWICCA method is applicable and scalable to extremely high dimensions, and we provide a real-data example that demonstrates this capability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17921v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arvind Prasadan</dc:creator>
    </item>
    <item>
      <title>Identifying Neural Connectivity using Bernoulli Autoregressive Partially Linear Additive Models</title>
      <link>https://arxiv.org/abs/2507.18218</link>
      <description>arXiv:2507.18218v1 Announce Type: cross 
Abstract: Characterising the interactions between spiking neurons is central to our understanding of cognitive processes such as memory, perception and decision making. In this work, we consider the problem of inferring connectivity in the brain network from non-stationary high-dimensional spike train data. Under a binned spike count representation of these data, we propose a Bernoulli autoregressive partially linear additive (BAPLA) model to identify the effective connectivity of a population of neurons. Estimates of the model parameters are obtained using a regularised maximum likelihood estimator, where an $\ell_1$ penalty is used to find sparse and interpretable estimates of neuronal interactions. We also account for non-stationary firing rates by adding a non-parametric trend to the model and provide an inference procedure to quantify the uncertainty associated with our estimated networks of neuronal interactions. We use synthetic data to assess the performance of the BAPLA method, highlighting its ability to detect both excitatory and inhibitory interactions in various settings. Finally, we apply our method to a neural spiking dataset from the DANDI archive, where we study the interactions of neural processes in reaction to various stimulus-response type neuroscience experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18218v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carla Pinkney, Carolina Euan, Alex Gibberd</dc:creator>
    </item>
    <item>
      <title>Pseudo-Labeling for Kernel Ridge Regression under Covariate Shift</title>
      <link>https://arxiv.org/abs/2302.10160</link>
      <description>arXiv:2302.10160v4 Announce Type: replace 
Abstract: We develop and analyze a principled approach to kernel ridge regression under covariate shift. The goal is to learn a regression function with small mean squared error over a target distribution, based on unlabeled data from there and labeled data that may have a different feature distribution. We propose to split the labeled data into two subsets, and conduct kernel ridge regression on them separately to obtain a collection of candidate models and an imputation model. We use the latter to fill the missing labels and then select the best candidate accordingly. Our non-asymptotic excess risk bounds demonstrate that our estimator adapts effectively to both the structure of the target distribution and the covariate shift. This adaptation is quantified through a notion of effective sample size that reflects the value of labeled source data for the target regression task. Our estimator achieves the minimax optimal error rate up to a polylogarithmic factor, and we find that using pseudo-labels for model selection does not significantly hinder performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.10160v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaizheng Wang</dc:creator>
    </item>
    <item>
      <title>An efficient multivariate volatility model for many assets</title>
      <link>https://arxiv.org/abs/2402.00597</link>
      <description>arXiv:2402.00597v2 Announce Type: replace 
Abstract: This paper develops a flexible and computationally efficient multivariate volatility model, which allows for dynamic conditional correlations and volatility spillover effects among financial assets. The new model has desirable properties such as identifiability and computational tractability for many assets. A sufficient condition of the strict stationarity is derived for the new process. Two quasi-maximum likelihood estimation methods are proposed for the new model with and without low-rank constraints on the coefficient matrices respectively, and the asymptotic properties for both estimators are established. Moreover, a Bayesian information criterion with selection consistency is developed for order selection, and the testing for volatility spillover effects is carefully discussed. The finite sample performance of the proposed methods is evaluated in simulation studies for small and moderate dimensions. The usefulness of the new model and its inference tools is illustrated by two empirical examples for 5 stock markets and 17 industry portfolios, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00597v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenyu Li, Yuchang Lin, Qianqian Zhu, Guodong Li</dc:creator>
    </item>
    <item>
      <title>Improving Genomic Prediction using High-dimensional Secondary Phenotypes: the Genetic Latent Factor Approach</title>
      <link>https://arxiv.org/abs/2408.09876</link>
      <description>arXiv:2408.09876v3 Announce Type: replace 
Abstract: Decreasing costs and new technologies have led to an increase in the amount of data available to plant breeding programs. High-throughput phenotyping (HTP) platforms routinely generate high-dimensional datasets of secondary features that may be used to improve genomic prediction accuracy. However, integration of these data comes with challenges such as multicollinearity, parameter estimation in $p &gt; n$ settings, and the computational complexity of many standard approaches. Several methods have emerged to analyze such data, but interpretation of model parameters often remains challenging. We propose genetic latent factor best linear unbiased prediction (glfBLUP), a prediction pipeline that reduces the dimensionality of the original secondary HTP data using generative factor analysis. In short, glfBLUP uses redundancy filtered and regularized genetic and residual correlation matrices to fit a maximum likelihood factor model and estimate genetic latent factor scores. These latent factors are subsequently used in multi-trait genomic prediction. Our approach performs better than alternatives in extensive simulations and a real-world application, while producing easily interpretable and biologically relevant parameters. We discuss several possible extensions and highlight glfBLUP as the basis for a flexible and modular multi-trait genomic prediction framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09876v3</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Killian A. C. Melsen, Jonathan F. Kunst, Jos\'e Crossa, Margaret R. Krause, Fred A. van Eeuwijk, Willem Kruijer, Carel F. W. Peeters</dc:creator>
    </item>
    <item>
      <title>Heterogeneous Treatment Effects under Network Interference: A Nonparametric Approach Based on Node Connectivity</title>
      <link>https://arxiv.org/abs/2410.11797</link>
      <description>arXiv:2410.11797v3 Announce Type: replace 
Abstract: In network settings, interference between units makes causal inference more challenging as outcomes may depend on the treatments received by others in the network. Typical estimands in network settings focus on treatment effects aggregated across individuals in the population. We propose a framework for estimating node-wise counterfactual means, allowing for more granular insights into the impact of network structure on treatment effect heterogeneity. We develop a doubly robust and non-parametric estimation procedure, KECENI (Kernel Estimator of Causal Effect under Network Interference), which offers consistency and asymptotic normality under network dependence. The utility of this method is demonstrated through an application to microfinance data, revealing the impact of network characteristics on treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11797v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heejong Bong, Colin B. Fogarty, Elizaveta Levina, Ji Zhu</dc:creator>
    </item>
    <item>
      <title>Undersmoothed LASSO Models for Propensity Score Weighting and Synthetic Negative Control Exposures for Bias Detection</title>
      <link>https://arxiv.org/abs/2506.17760</link>
      <description>arXiv:2506.17760v2 Announce Type: replace 
Abstract: The propensity score (PS) is often used to control for large numbers of covariates in high-dimensional healthcare database studies. The least absolute shrinkage and selection operator (LASSO) is a data-adaptive prediction algorithm that has become the most widely used tool for large-scale PS estimation in these settings. However, recent work has shown that the use of data-adaptive algorithms for PS estimation can come at the cost of slow convergence rates, resulting in PS-based causal estimators having poor statistical properties. While this can create challenges for the use of data-driven algorithms for PS analyses, both theory and simulations have shown that LASSO PS models can converge at a fast enough rate to provide asymptotically efficient PS weighted causal estimators. In order to achieve asymptotic efficiency, however, LASSO PS weighted estimators need to be properly tuned, which requires undersmoothing the fitted LASSO model. In this paper, we discuss challenges in determining how to undersmooth LASSO models for PS weighting and consider the use of balance diagnostics to select the degree of undersmoothing. Because no tuning criteria is universally best, we propose using synthetically generated negative control exposure studies to detect bias across alternative analytic choices. Specifically, we show that synthetic negative control exposures can identify undersmoothing techniques that likely violate partial exchangeability due to lack of control for measured confounding. We use a series of numerical studies to investigate the performance of alternative balance criteria to undersmooth LASSO PS-weighted estimators, and the use of synthetic negative control exposure studies to detect biased analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17760v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Richard Wyss, Ben B. Hansen, Georg Hahn, Lars van der Laan, Kueiyu Joshua Lin</dc:creator>
    </item>
    <item>
      <title>On the Approximation of Stationary Processes using the ARMA Model</title>
      <link>https://arxiv.org/abs/2408.10610</link>
      <description>arXiv:2408.10610v3 Announce Type: replace-cross 
Abstract: We revisit an old problem related to Autoregressive Moving Average (ARMA) models, on quantifying and bounding the approximation error between a true stationary process $X_t$ and an ARMA model $Y_t$. We take the transfer function representation of an ARMA model and show that the associated $L^{\infty}$ norm provides a valid alternate norm that controls the $L^2$ norm and has structural properties comparable to the cepstral norm. We show that a certain subspace of stationary processes, which includes ARMA models, forms a Banach algebra under the $L^{\infty}$ norm that respects the group structure of $H^{\infty}$ transfer functions. The natural definition of invertibility in this algebra is consistent with the original definition of ARMA invertibility, and generalizes better to non-ARMA processes than Wiener's $\ell^1$ condition. Finally, we calculate some explicit approximation bounds in the simpler context of continuous transfer functions, and critique some heuristic ideas on Pad\'e approximations and parsimonious models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10610v3</guid>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anand Ganesh, Babhrubahan Bose, Anand Rajagopalan</dc:creator>
    </item>
    <item>
      <title>LLM Web Dynamics: Tracing Model Collapse in a Network of LLMs</title>
      <link>https://arxiv.org/abs/2506.15690</link>
      <description>arXiv:2506.15690v3 Announce Type: replace-cross 
Abstract: The increasing use of synthetic data from the public Internet has enhanced data usage efficiency in large language model (LLM) training. However, the potential threat of model collapse remains insufficiently explored. Existing studies primarily examine model collapse in a single model setting or rely solely on statistical surrogates. In this work, we introduce LLM Web Dynamics (LWD), an efficient framework for investigating model collapse at the network level. By simulating the Internet with a retrieval-augmented generation (RAG) database, we analyze the convergence pattern of model outputs. Furthermore, we provide theoretical guarantees for this convergence by drawing an analogy to interacting Gaussian Mixture Models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15690v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyu Wang, Akira Horiguchi, Lingyou Pang, Carey E. Priebe</dc:creator>
    </item>
  </channel>
</rss>
