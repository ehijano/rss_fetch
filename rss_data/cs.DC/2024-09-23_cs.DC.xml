<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Sep 2024 03:13:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Optimization of a Radiofrequency Ablation FEM Application Using Parallel Sparse Solvers</title>
      <link>https://arxiv.org/abs/2409.13036</link>
      <description>arXiv:2409.13036v1 Announce Type: new 
Abstract: Finite element method applications are a common approach to simulate a handful of phenomena but can take a lot of computing power, causing elevated waiting time to produce precise results. The radiofrequency ablation finite element method is an application to simulate the medical procedure of radiofrequency ablation, a minimally invasive liver cancer treatment. The application runs sequentially and can take up to 20 hours of execution to generate 15 minutes of simulation results. Most of this time arises from the need to solve a sparse system of linear equations. In this work, we accelerate this application by using three sparse solvers packages (MAGMA cuSOLVER, and QRMumps), including direct and iterative methods over different multicore and GPU architectures. We conducted a numerical result analysis to access the solution quality provided by the distinct solvers and their configurations, proposing the use of the peak signal-to-noise ratio metric. We were able to reduce the application execution time up to 40 times compared to the original sequential version while keeping a similar numerical quality for the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13036v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>The 2020 International Conference on High Performance Computing and Simulation (2020)</arxiv:journal_reference>
      <dc:creator>Marcelo Cogo Miletto, Claudio Schepke, Lucas Mello Schnorr</dc:creator>
    </item>
    <item>
      <title>Stabl: Blockchain Fault Tolerance</title>
      <link>https://arxiv.org/abs/2409.13142</link>
      <description>arXiv:2409.13142v1 Announce Type: new 
Abstract: Blockchain promises to make online services more fault tolerant due to their inherent distributed nature. Their ability to execute arbitrary programs in different geo-distributed regions and on diverse operating systems make them an alternative of choice to our dependence on unique software whose recent failure affected 8.5 millions of machines. As of today, it remains, however, unclear whether blockchains can truly tolerate failures.
  In this paper, we assess the fault tolerance of blockchain. To this end, we inject failures in controlled deployments of five modern blockchain systems, namely Algorand, Aptos, Avalanche, Redbelly and Solana. We introduce a novel sensitivity metric, interesting in its own right, as the difference between the integrals of two cumulative distribution functions, one obtained in a baseline environment and one obtained in an adversarial environment. Our results indicate that (i) all blockchains except Redbelly are highly impacted by the failure of a small part of their network, (ii) Avalanche and Redbelly benefit from the redundant information needed for Byzantine fault tolerance while others are hampered by it, and more dramatically (iii) Avalanche and Solana cannot recover from localised transient failures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13142v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Gramoli, Rachid Guerraoui, Andrei Lebedev, Gauthier Voron</dc:creator>
    </item>
    <item>
      <title>Performance Enhancement of the Ozaki Scheme on Integer Matrix Multiplication Unit</title>
      <link>https://arxiv.org/abs/2409.13313</link>
      <description>arXiv:2409.13313v1 Announce Type: new 
Abstract: This study was aimed at simultaneously achieving sufficient accuracy and high performance for general matrix multiplications. Recent architectures, such as NVIDIA GPUs, feature high-performance units designed for low-precision matrix multiplications in machine learning models, and next-generation architectures are expected to follow the same design principle. The key to achieving superior performance is to fully leverage such architectures. The Ozaki scheme, a highly accurate matrix multiplication algorithm using error-free transformations, enables higher-precision matrix multiplication to be performed through multiple lower-precision matrix multiplications and higher-precision matrix additions. Ootomo et al. implemented the Ozaki scheme on high-performance matrix multiplication units with the aim of achieving both sufficient accuracy and high performance. This paper proposes alternative approaches to improving performance by reducing the numbers of lower-precision matrix multiplications and higher-precision matrix additions. Numerical experiments demonstrate the accuracy of the results and conduct performance benchmarks of the proposed approaches. These approaches are expected to yield more efficient results in next-generation architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13313v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuki Uchino, Katsuhisa Ozaki, Toshiyuki Imamura</dc:creator>
    </item>
    <item>
      <title>Flexible Swapping for the Cloud</title>
      <link>https://arxiv.org/abs/2409.13327</link>
      <description>arXiv:2409.13327v1 Announce Type: new 
Abstract: Memory has become the primary cost driver in cloud data centers. Yet, a significant portion of memory allocated to VMs in public clouds remains unused. To optimize this resource, "cold" memory can be reclaimed from VMs and stored on slower storage or compressed, enabling memory overcommit. Current overcommit systems rely on general-purpose OS swap mechanisms, which are not optimized for virtualized workloads, leading to missed memory-saving opportunities and ineffective use of optimizations like prefetchers.
  This paper introduces a userspace memory management framework designed for VMs. It enables custom policies that have full control over the virtual machines' memory using a simple userspace API, supports huge page-based swapping to satisfy VM performance requirements, is easy to deploy by leveraging Linux/KVM, and supports zero-copy I/O virtualization with shared VM memory.
  Our evaluation demonstrates that an overcommit system based on our framework outperforms the state-of-the-art solutions on both micro-benchmarks and commonly used cloud workloads. Specifically our implementation outperforms the Linux Kernel baseline implementation by up to 25% while saving a similar amount of memory. We also demonstrate the benefits of custom policies by implementing workload-specific reclaimers and prefetchers that save $10\%$ additional memory, improve performance in a limited memory scenario by 30% over the Linux baseline, and recover faster from hard limit releases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13327v1</guid>
      <category>cs.DC</category>
      <category>cs.OS</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Milan Pandurov, Lukas Humbel, Dmitry Sepp, Adamos Ttofari, Leon Thomm, Do Le Quoc, Siddharth Chandrasekaran, Sharan Santhanam, Chuan Ye, Shai Bergman, Wei Wang, Sven Lundgren, Konstantinos Sagonas, Alberto Ros</dc:creator>
    </item>
    <item>
      <title>RapidOMS: FPGA-based Open Modification Spectral Library Searching with HD Computing</title>
      <link>https://arxiv.org/abs/2409.13361</link>
      <description>arXiv:2409.13361v1 Announce Type: new 
Abstract: Mass spectrometry (MS) is essential for protein analysis but faces significant challenges with large datasets and complex post-translational modifications, resulting in difficulties in spectral identification. Open Modification Search (OMS) improves the analysis of these modifications. We present RapidOMS, a solution leveraging the Samsung SmartSSD, which integrates SSD and FPGA in a near-storage configuration to minimize data movement and enhance the efficiency of large-scale database searching. RapidOMS employs hyperdimensional computing (HDC), a brain-inspired, high-dimensional data processing approach, exploiting the parallel processing and low-latency capabilities of FPGAs, making it well-suited for MS. Utilizing the parallelism and efficiency of bitwise operations in HDC, RapidOMS delivers up to a 60x speedup over the state-of-the-art (SOTA) CPU tool ANN-Solo and is 2.72x faster than the GPU tool HyperOMS. Furthermore, RapidOMS achieves an 11x improvement in energy efficiency compared to conventional systems, providing scalable, energy-efficient solutions for large-scale proteomics applications and advancing the efficient processing of proteomic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13361v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sumukh Pinge, Weihong Xu, Wout Bittremieux, Niema Moshiri, Sang-Woo Jun, Tajana Rosing</dc:creator>
    </item>
    <item>
      <title>SatFed: A Resource-Efficient LEO Satellite-Assisted Heterogeneous Federated Learning Framework</title>
      <link>https://arxiv.org/abs/2409.13503</link>
      <description>arXiv:2409.13503v1 Announce Type: new 
Abstract: Traditional federated learning (FL) frameworks rely heavily on terrestrial networks, where coverage limitations and increasing bandwidth congestion significantly hinder model convergence. Fortunately, the advancement of low-Earth orbit (LEO) satellite networks offers promising new communication avenues to augment traditional terrestrial FL. Despite this potential, the limited satellite-ground communication bandwidth and the heterogeneous operating environments of ground devices-including variations in data, bandwidth, and computing power-pose substantial challenges for effective and robust satellite-assisted FL. To address these challenges, we propose SatFed, a resource-efficient satellite-assisted heterogeneous FL framework. SatFed implements freshness-based model prioritization queues to optimize the use of highly constrained satellite-ground bandwidth, ensuring the transmission of the most critical models. Additionally, a multigraph is constructed to capture real-time heterogeneous relationships between devices, including data distribution, terrestrial bandwidth, and computing capability. This multigraph enables SatFed to aggregate satellite-transmitted models into peer guidance, enhancing local training in heterogeneous environments. Extensive experiments with real-world LEO satellite networks demonstrate that SatFed achieves superior performance and robustness compared to state-of-the-art benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13503v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxin Zhang, Zheng Lin, Zhe Chen, Zihan Fang, Wenjun Zhu, Xianhao Chen, Jin Zhao, Yue Gao</dc:creator>
    </item>
    <item>
      <title>Performance and Power: Systematic Evaluation of AI Workloads on Accelerators with CARAML</title>
      <link>https://arxiv.org/abs/2409.12994</link>
      <description>arXiv:2409.12994v1 Announce Type: cross 
Abstract: The rapid advancement of machine learning (ML) technologies has driven the development of specialized hardware accelerators designed to facilitate more efficient model training. This paper introduces the CARAML benchmark suite, which is employed to assess performance and energy consumption during the training of transformer-based large language models and computer vision models on a range of hardware accelerators, including systems from NVIDIA, AMD, and Graphcore. CARAML provides a compact, automated, extensible, and reproducible framework for assessing the performance and energy of ML workloads across various novel hardware architectures. The design and implementation of CARAML, along with a custom power measurement tool called jpwr, are discussed in detail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12994v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Chelsea Maria John, Stepan Nassyr, Carolin Penke, Andreas Herten</dc:creator>
    </item>
    <item>
      <title>FedAT: Federated Adversarial Training for Distributed Insider Threat Detection</title>
      <link>https://arxiv.org/abs/2409.13083</link>
      <description>arXiv:2409.13083v1 Announce Type: cross 
Abstract: Insider threats usually occur from within the workplace, where the attacker is an entity closely associated with the organization. The sequence of actions the entities take on the resources to which they have access rights allows us to identify the insiders. Insider Threat Detection (ITD) using Machine Learning (ML)-based approaches gained attention in the last few years. However, most techniques employed centralized ML methods to perform such an ITD. Organizations operating from multiple locations cannot contribute to the centralized models as the data is generated from various locations. In particular, the user behavior data, which is the primary source of ITD, cannot be shared among the locations due to privacy concerns. Additionally, the data distributed across various locations result in extreme class imbalance due to the rarity of attacks. Federated Learning (FL), a distributed data modeling paradigm, gained much interest recently. However, FL-enabled ITD is not yet explored, and it still needs research to study the significant issues of its implementation in practical settings. As such, our work investigates an FL-enabled multiclass ITD paradigm that considers non-Independent and Identically Distributed (non-IID) data distribution to detect insider threats from different locations (clients) of an organization. Specifically, we propose a Federated Adversarial Training (FedAT) approach using a generative model to alleviate the extreme data skewness arising from the non-IID data distribution among the clients. Besides, we propose to utilize a Self-normalized Neural Network-based Multi-Layer Perceptron (SNN-MLP) model to improve ITD. We perform comprehensive experiments and compare the results with the benchmarks to manifest the enhanced performance of the proposed FedATdriven ITD scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13083v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R G Gayathri, Atul Sajjanhar, Md Palash Uddin, Yong Xiang</dc:creator>
    </item>
    <item>
      <title>RLHFuse: Efficient RLHF Training for Large Language Models with Inter- and Intra-Stage Fusion</title>
      <link>https://arxiv.org/abs/2409.13221</link>
      <description>arXiv:2409.13221v1 Announce Type: cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) stands as a pivotal post-training technique to enhance the alignment between LLMs and human preference. The workflow of RLHF typically involves several models and tasks in a series of distinct stages. Existing RLHF training systems view each task as the smallest execution unit thus overlooking the opportunities for subtask-level optimizations. Due to the intrinsic nature of RLHF training, i.e., the data skewness in the generation stage, and the pipeline bubbles in the training stage, existing RLHF systems suffer from low GPU utilization in production deployments.
  RLHFuse breaks the traditional view of RLHF workflow as a composition of individual tasks, splitting each task into finer-grained subtasks, and performing stage fusion to improve GPU utilization. RLHFuse contains two key ideas. First, for generation and inference tasks, RLHFuse splits them into sample-level subtasks, enabling efficient inter-stage fusion to mitigate the original generation bottleneck dominated by long-tailed samples. Second, for training tasks, RLHFuse breaks them into subtasks of micro-batches. By leveraging the intuition that pipeline execution can be essentially complemented by another pipeline, RLHFuse performs intra-stage fusion to concurrently execute these subtasks in the training stage with a fused pipeline schedule, resulting in fewer pipeline bubbles. In addition, RLHFuse incorporates a series of system optimizations tailored for each stage of RLHF, making it efficient and scalable for our internal product usage. We evaluate RLHFuse on various popular LLMs and the results show that RLHFuse increases the training throughput by up to 3.7x, compared to existing state-of-the-art systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13221v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.DC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yinmin Zhong, Zili Zhang, Bingyang Wu, Shengyu Liu, Yukun Chen, Changyi Wan, Hanpeng Hu, Lei Xia, Ranchen Ming, Yibo Zhu, Xin Jin</dc:creator>
    </item>
    <item>
      <title>Noise-Robust and Resource-Efficient ADMM-based Federated Learning</title>
      <link>https://arxiv.org/abs/2409.13451</link>
      <description>arXiv:2409.13451v1 Announce Type: cross 
Abstract: Federated learning (FL) leverages client-server communications to train global models on decentralized data. However, communication noise or errors can impair model accuracy. To address this problem, we propose a novel FL algorithm that enhances robustness against communication noise while also reducing communication load. We derive the proposed algorithm through solving the weighted least-squares (WLS) regression problem as an illustrative example. We first frame WLS regression as a distributed convex optimization problem over a federated network employing random scheduling for improved communication efficiency. We then apply the alternating direction method of multipliers (ADMM) to iteratively solve this problem. To counteract the detrimental effects of cumulative communication noise, we introduce a key modification by eliminating the dual variable and implementing a new local model update at each participating client. This subtle yet effective change results in using a single noisy global model update at each client instead of two, improving robustness against additive communication noise. Furthermore, we incorporate another modification enabling clients to continue local updates even when not selected by the server, leading to substantial performance improvements. Our theoretical analysis confirms the convergence of our algorithm in both mean and the mean-square senses, even when the server communicates with a random subset of clients over noisy links at each iteration. Numerical results validate the effectiveness of our proposed algorithm and corroborate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13451v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>eess.SP</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ehsan Lari, Reza Arablouei, Vinay Chakravarthi Gogineni, Stefan Werner</dc:creator>
    </item>
    <item>
      <title>DP$^2$-FedSAM: Enhancing Differentially Private Federated Learning Through Personalized Sharpness-Aware Minimization</title>
      <link>https://arxiv.org/abs/2409.13645</link>
      <description>arXiv:2409.13645v1 Announce Type: cross 
Abstract: Federated learning (FL) is a distributed machine learning approach that allows multiple clients to collaboratively train a model without sharing their raw data. To prevent sensitive information from being inferred through the model updates shared in FL, differentially private federated learning (DPFL) has been proposed. DPFL ensures formal and rigorous privacy protection in FL by clipping and adding random noise to the shared model updates. However, the existing DPFL methods often result in severe model utility degradation, especially in settings with data heterogeneity. To enhance model utility, we propose a novel DPFL method named DP$^2$-FedSAM: Differentially Private and Personalized Federated Learning with Sharpness-Aware Minimization. DP$^2$-FedSAM leverages personalized partial model-sharing and sharpness-aware minimization optimizer to mitigate the adverse impact of noise addition and clipping, thereby significantly improving model utility without sacrificing privacy. From a theoretical perspective, we provide a rigorous theoretical analysis of the privacy and convergence guarantees of our proposed method. To evaluate the effectiveness of DP$^2$-FedSAM, we conduct extensive evaluations based on common benchmark datasets. Our results verify that our method improves the privacy-utility trade-off compared to the existing DPFL methods, particularly in heterogeneous data settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13645v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhenxiao Zhang, Yuanxiong Guo, Yanmin Gong</dc:creator>
    </item>
    <item>
      <title>TPFL: Tsetlin-Personalized Federated Learning with Confidence-Based Clustering</title>
      <link>https://arxiv.org/abs/2409.10392</link>
      <description>arXiv:2409.10392v2 Announce Type: replace 
Abstract: The world of Machine Learning (ML) has witnessed rapid changes in terms of new models and ways to process users data. The majority of work that has been done is focused on Deep Learning (DL) based approaches. However, with the emergence of new algorithms such as the Tsetlin Machine (TM) algorithm, there is growing interest in exploring alternative approaches that may offer unique advantages in certain domains or applications. One of these domains is Federated Learning (FL), in which users privacy is of utmost importance. Due to its novelty, FL has seen a surge in the incorporation of personalization techniques to enhance model accuracy while maintaining user privacy under personalized conditions. In this work, we propose a novel approach dubbed TPFL: Tsetlin-Personalized Federated Learning, in which models are grouped into clusters based on their confidence towards a specific class. In this way, clustering can benefit from two key advantages. Firstly, clients share only what they are confident about, resulting in the elimination of wrongful weight aggregation among clients whose data for a specific class may have not been enough during the training. This phenomenon is prevalent when the data are non-Independent and Identically Distributed (non-IID). Secondly, by sharing only weights towards a specific class, communication cost is substantially reduced, making TPLF efficient in terms of both accuracy and communication cost. The results of TPFL demonstrated the highest accuracy on three different datasets; namely MNIST, FashionMNIST and FEMNIST.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10392v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rasoul Jafari Gohari, Laya Aliahmadipour, Ezat Valipour</dc:creator>
    </item>
    <item>
      <title>Decentralized Kernel Ridge Regression Based on Data-Dependent Random Feature</title>
      <link>https://arxiv.org/abs/2405.07791</link>
      <description>arXiv:2405.07791v3 Announce Type: replace-cross 
Abstract: Random feature (RF) has been widely used for node consistency in decentralized kernel ridge regression (KRR). Currently, the consistency is guaranteed by imposing constraints on coefficients of features, necessitating that the random features on different nodes are identical. However, in many applications, data on different nodes varies significantly on the number or distribution, which calls for adaptive and data-dependent methods that generate different RFs. To tackle the essential difficulty, we propose a new decentralized KRR algorithm that pursues consensus on decision functions, which allows great flexibility and well adapts data on nodes. The convergence is rigorously given and the effectiveness is numerically verified: by capturing the characteristics of the data on each node, while maintaining the same communication costs as other methods, we achieved an average regression accuracy improvement of 25.5\% across six real-world data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07791v3</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TNNLS.2024.3414325</arxiv:DOI>
      <dc:creator>Ruikai Yang, Fan He, Mingzhen He, Jie Yang, Xiaolin Huang</dc:creator>
    </item>
    <item>
      <title>Celtibero: Robust Layered Aggregation for Federated Learning</title>
      <link>https://arxiv.org/abs/2408.14240</link>
      <description>arXiv:2408.14240v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) is an innovative approach to distributed machine learning. While FL offers significant privacy advantages, it also faces security challenges, particularly from poisoning attacks where adversaries deliberately manipulate local model updates to degrade model performance or introduce hidden backdoors. Existing defenses against these attacks have been shown to be effective when the data on the nodes is identically and independently distributed (i.i.d.), but they often fail under less restrictive, non-i.i.d data conditions. To overcome these limitations, we introduce Celtibero, a novel defense mechanism that integrates layered aggregation to enhance robustness against adversarial manipulation. Through extensive experiments on the MNIST and IMDB datasets, we demonstrate that Celtibero consistently achieves high main task accuracy (MTA) while maintaining minimal attack success rates (ASR) across a range of untargeted and targeted poisoning attacks. Our results highlight the superiority of Celtibero over existing defenses such as FL-Defender, LFighter, and FLAME, establishing it as a highly effective solution for securing federated learning systems against sophisticated poisoning attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14240v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Borja Molina-Coronado</dc:creator>
    </item>
  </channel>
</rss>
