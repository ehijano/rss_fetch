<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Feb 2026 05:00:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>BEMEval-Doc2Schema: Benchmarking Large Language Models for Structured Data Extraction in Building Energy Modeling</title>
      <link>https://arxiv.org/abs/2602.16926</link>
      <description>arXiv:2602.16926v1 Announce Type: new 
Abstract: Recent advances in foundation models, including large language models (LLMs), have created new opportunities to automate building energy modeling (BEM). However, systematic evaluation has remained challenging due to the absence of publicly available, task-specific datasets and standardized performance metrics. We present BEMEval, a benchmark framework designed to assess foundation models' performance across BEM tasks. The first benchmark in this suite, BEMEval-Doc2Schema, focuses on structured data extraction from building documentation, a foundational step toward automated BEM processes. BEMEval-Doc2Schema introduces the Key-Value Overlap Rate (KVOR), a metric that quantifies the alignment between LLM-generated structured outputs and ground-truth schema references. Using this framework, we evaluate two leading models (GPT-5 and Gemini 2.5) under zero-shot and few-shot prompting strategies across three datasets: HERS L100, NREL iUnit, and NIST NZERTF. Results show that Gemini 2.5 consistently outperforms GPT-5, and that few-shot prompts improve accuracy for both models. Performance also varies by schema: the EPC schema yields significantly higher KVOR scores than HPXML, reflecting its simpler and reduced hierarchical depth. By combining curated datasets, reproducible metrics, and cross-model comparisons, BEMEval-Doc2Schema establishes the first community-driven benchmark for evaluating LLMs in performing building energy modeling tasks, laying the groundwork for future research on AI-assisted BEM workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16926v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiyuan Jia, Xiaoqin Fu, Liang Zhang</dc:creator>
    </item>
    <item>
      <title>The Kinematics and Dynamics Theories of a Total Lagrangian Finite Element Analysis Framework for Finite Deformation Multibody Dynamics</title>
      <link>https://arxiv.org/abs/2602.17002</link>
      <description>arXiv:2602.17002v1 Announce Type: new 
Abstract: This work presents a Total Lagrangian finite element formulation for deformable body dynamics. We employ the TL-FEA framework to simulate the time evolution of collections of bodies whose motion is constrained by kinematic constraints and which mutually interact through contact and friction. These bodies experience large displacements, large deformations, and large rotations. A systematic approach is proposed for classifying and posing kinematic constraints acting between the bodies present in the system. We derive the governing equations for ANCF beam, ANCF shell, and tetrahedral elements, and present hyperelastic material models including St. Venant-Kirchhoff and Mooney-Rivlin formulations with their corresponding internal force contributions and consistent tangent stiffness matrices. A finite-strain Kelvin-Voigt viscous damping model is incorporated in the TL-FEA formulation for numerical stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17002v1</guid>
      <category>cs.CE</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenhao Zhou, Ganesh Arivoli, Dan Negrut</dc:creator>
    </item>
    <item>
      <title>A variational multi-phase model for elastoplastic materials with microstructure evolution</title>
      <link>https://arxiv.org/abs/2602.17492</link>
      <description>arXiv:2602.17492v1 Announce Type: new 
Abstract: A general model is formulated for elasto-plastic materials undergoing linear kinematic hardening to describe microstructure evolution associated with phase transformations. Using infinitesimal strain theory, the model is based on variational principles for inelastic materials.
  In our work we combine the so-called dissipation distance, which describes an immediate phase transition in time via an underlying probability matrix. In addition, the volume fractions of the newly emerging phases are represented by Young measures to obtain a time continuous microstructure evolution. The model is verified employing a two-dimensional benchmark test implemented by the Finite Element Method (FEM).</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17492v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah Dinkelacker-Steinhoff, Klaus Hackl</dc:creator>
    </item>
    <item>
      <title>Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation</title>
      <link>https://arxiv.org/abs/2602.16990</link>
      <description>arXiv:2602.16990v1 Announce Type: cross 
Abstract: Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16990v1</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yan Wang, Yi Han, Lingfei Qian, Yueru He, Xueqing Peng, Dongji Feng, Zhuohan Xie, Vincent Jim Zhang, Rosie Guo, Fengran Mo, Jimin Huang, Yankai Chen, Xue Liu, Jian-Yun Nie</dc:creator>
    </item>
    <item>
      <title>Impacts of Economic Policies on Wealth Distribution in Token Economies</title>
      <link>https://arxiv.org/abs/2602.17373</link>
      <description>arXiv:2602.17373v1 Announce Type: cross 
Abstract: In this paper, we analyse the impacts of exogenous and endogenous factors on wealth distribution in the Bitcoin token economy, where wealth distribution refers to the distribution of BTC between economic participants or groups of economic participants. The objective of the paper is to analyse the impact of economic policies on wealth distribution in the Bitcoin ecosystem. Different macroeconomic and microeconomic time series are used to eliminate noise in the wealth distribution time series, and the causality analysis is performed between Bitcoin Improvement Proposals (i.e., BIPs) and the cleaned wealth distribution data to reveal possible patterns in the impacts that the endogenous policies have on wealth distribution in token economies. Lastly, a structure for economic policy taxonomy in token economies is proposed where different the policy implementations are illustrated by existing BIPs. This approach highlights the actions available to the policy makers, as well as providing a technique for analysis of policy impacts in token economies and their categorization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17373v1</guid>
      <category>q-fin.GN</category>
      <category>cs.CE</category>
      <category>q-fin.CP</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rem Sadykhov, Geoff Goodell, Philip Treleaven</dc:creator>
    </item>
    <item>
      <title>Sci2Pol: Evaluating and Fine-tuning LLMs on Scientific-to-Policy Brief Generation</title>
      <link>https://arxiv.org/abs/2509.21493</link>
      <description>arXiv:2509.21493v2 Announce Type: replace 
Abstract: We propose Sci2Pol-Bench and Sci2Pol-Corpus, the first benchmark and training dataset for evaluating and fine-tuning large language models (LLMs) on policy brief generation from a scientific paper. We build Sci2Pol-Bench on a five-stage taxonomy to mirror the human writing process: (i) Autocompletion, (ii) Understanding, (iii) Summarization, (iv) Generation, and (v) Verification. It features 18 tasks in multiple-choice and open-ended formats. Specifically, for the Generation stage, we show that BERTScore and ROUGE scores fail to capture the quality of brief writing, and introduce a new LLM-based evaluation metric aligned with expert judgement. Using this benchmark, we evaluate 13 leading open-source and commercial LLMs to uncover key limitations. To improve LLM performance on brief writing, we curate the Sci2Pol-Corpus for fine-tuning. We start by linking each cited scientific paper to its corresponding policy document, drawn from 5.6 million policy records. This produces 140,000 candidate pairs. We then employ an LLM-as-a-judge to filter high-quality examples, followed by in-context polishing using three expert-written samples as references. This process yields a final set of 639 new pairs. Finally, we fine-tune three models on Sci2Pol-Corpus: LLaMA-3.18B, Gemma-12B, and Gemma-27B. Fine-tuning leads to consistent performance improvements across Sci2Pol-Bench. Notably, after fine-tuning, Gemma-27B surpasses the much larger GPT-4o and DeepSeek-V3 (671B). These demonstrate the effectiveness of our corpus in bridging the gap between science and policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21493v2</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weimin Wu, Alexander C. Furnas, Eddie Yang, Gefei Liu, Akhil Pandey Akella, Xuefeng Song, Dashun Wang, Han Liu</dc:creator>
    </item>
    <item>
      <title>FinTagging: Benchmarking LLMs for Extracting and Structuring Financial Information</title>
      <link>https://arxiv.org/abs/2505.20650</link>
      <description>arXiv:2505.20650v4 Announce Type: replace-cross 
Abstract: Accurate interpretation of numerical data in financial reports is critical for markets and regulators. Although XBRL (eXtensible Business Reporting Language) provides a standard for tagging financial figures, mapping thousands of facts to over 10k US GAAP concepts remains costly and error prone. Existing benchmarks oversimplify this task as flat, single step classification over small subsets of concepts, ignoring the hierarchical semantics of the taxonomy and the structured nature of financial documents. Consequently, these benchmarks fail to evaluate Large Language Models (LLMs) under realistic reporting conditions. To bridge this gap, we introduce FinTagging, the first comprehensive benchmark for structure aware and full scope XBRL tagging. We decompose the complex tagging process into two subtasks: (1) FinNI (Financial Numeric Identification), which extracts entities and types from heterogeneous contexts including text and tables; and (2) FinCL (Financial Concept Linking), which maps extracted entities to the full US GAAP taxonomy. This two stage formulation enables a fair assessment of LLMs' capabilities in numerical reasoning and taxonomy alignment. Evaluating diverse LLMs in zero shot settings reveals that while models generalize well in extraction, they struggle significantly with fine grained concept linking, highlighting critical limitations in domain specific structure aware reasoning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20650v4</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yan Wang, Lingfei Qian, Xueqing Peng, Yang Ren, Keyi Wang, Yi Han, Dongji Feng, Fengran Mo, Shengyuan Lin, Qinchuan Zhang, Kaiwen He, Chenri Luo, Jianxing Chen, Junwei Wu, Chen Xu, Ziyang Xu, Jimin Huang, Guojun Xiong, Xiao-Yang Liu, Qianqian Xie, Jian-Yun Nie</dc:creator>
    </item>
    <item>
      <title>FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs</title>
      <link>https://arxiv.org/abs/2510.08886</link>
      <description>arXiv:2510.08886v2 Announce Type: replace-cross 
Abstract: Going beyond simple text processing, financial auditing requires detecting semantic, structural, and numerical inconsistencies across large-scale disclosures. As financial reports are filed in XBRL, a structured XML format governed by accounting standards, auditing becomes a structured information extraction and reasoning problem involving concept alignment, taxonomy-defined relations, and cross-document consistency. Although large language models (LLMs) show promise on isolated financial tasks, their capability in professional-grade auditing remains unclear. We introduce FinAuditing, a taxonomy-aligned, structure-aware benchmark built from real XBRL filings. It contains 1,102 annotated instances averaging over 33k tokens and defines three tasks: Financial Semantic Matching (FinSM), Financial Relationship Extraction (FinRE), and Financial Mathematical Reasoning (FinMR). Evaluations of 13 state-of-the-art LLMs reveal substantial gaps in concept retrieval, taxonomy-aware relation modeling, and consistent cross-document reasoning. These findings highlight the need for realistic, structure-aware benchmarks. We release the evaluation code at https://github.com/The-FinAI/FinAuditing and the dataset at https://huggingface.co/collections/TheFinAI/finauditing. The task currently serves as the official benchmark of an ongoing public evaluation contest at https://open-finance-lab.github.io/SecureFinAI_Contest_2026/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08886v2</guid>
      <category>cs.CL</category>
      <category>cs.CE</category>
      <category>cs.IR</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yan Wang, Keyi Wang, Shanshan Yang, Jaisal Patel, Jeff Zhao, Fengran Mo, Xueqing Peng, Lingfei Qian, Jimin Huang, Guojun Xiong, Yankai Chen, V\'ictor Guti\'errez-Basulto, Xiao-Yang Liu, Xue Liu, Jian-Yun Nie</dc:creator>
    </item>
  </channel>
</rss>
