<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Feb 2026 02:42:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Benchmark to Assess Common Ground in Human-AI Collaboration</title>
      <link>https://arxiv.org/abs/2602.21337</link>
      <description>arXiv:2602.21337v1 Announce Type: new 
Abstract: AI is becoming increasingly integrated into everyday life, both in professional work environments and in leisure and entertainment contexts. This integration requires AI to move beyond acting as an assistant for informational or transactional tasks toward a genuine collaborative partner. Effective collaboration, whether between humans or between humans and AI, depends on establishing and maintaining common ground: shared beliefs, assumptions, goals, and situational awareness that enable coordinated action and efficient repair of misunderstandings. While common ground is a central concept in human collaboration, it has received limited attention in studies of human-AI collaboration. In this paper, we introduce a new benchmark grounded in theories and empirical studies of human-human collaboration. The benchmark is based on a collaborative puzzle task that requires iterative interaction, joint action, referential coordination, and repair under varying conditions of situation awareness. We validate the benchmark through a confirmatory user study in which human participants collaborate with an AI to solve the task. The results show that the benchmark reproduces established theoretical and empirical findings from human-human collaboration, while also revealing clear divergences in human-AI interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21337v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Poelitz, Finale Doshi-Velez, Si\^an Lindley</dc:creator>
    </item>
    <item>
      <title>StoryComposerAI: Supporting Human-AI Story Co-Creation Through Decomposition and Linking</title>
      <link>https://arxiv.org/abs/2602.21486</link>
      <description>arXiv:2602.21486v1 Announce Type: new 
Abstract: GenAI's ability to produce text and images is increasingly incorporated into human-AI co-creation tasks such as storytelling and video editing. However, integrating GenAI into these tasks requires enabling users to retain control over editing individual story elements while ensuring that generated visuals remain coherent with the storyline and consistent across multiple AI-generated outputs. This work examines a paradigm of creative decomposition and linking, which allows creators to clearly communicate creative intent by prompting GenAI to tailor specific story elements, such as storylines, personas, locations, and scenes, while maintaining coherence among them. We implement and evaluate StoryComposerAI, a system that exemplifies this paradigm for enhancing users' sense of control and content consistency in human-AI co-creation of digital stories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21486v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772363.3798629</arxiv:DOI>
      <dc:creator>Shuo Niu, Dylan Clements, Marina Margalit Nemanov, Hyungsin Kim</dc:creator>
    </item>
    <item>
      <title>Exploring Human-Machine Coexistence in Symmetrical Reality</title>
      <link>https://arxiv.org/abs/2602.21584</link>
      <description>arXiv:2602.21584v1 Announce Type: new 
Abstract: In the context of the evolution of artificial intelligence (AI), the interaction between humans and AI entities has become increasingly salient, challenging the conventional human-centric paradigms of human-machine interaction. To address this challenge, it is imperative to reassess the relationship between AI entities and humans. Through considering both the virtual and physical worlds, we can construct a novel descriptive framework for a world where humans and machines coexist symbiotically. This paper will introduce a fresh research direction engendered for studying harmonious human-machine coexistence across physical and virtual worlds, which has been termed "symmetrical reality". We will elucidate its key characteristics, offering innovative research insight for renovating human-machine interaction paradigms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21584v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenliang Zhang</dc:creator>
    </item>
    <item>
      <title>WatchHand: Enabling Continuous Hand Pose Tracking On Off-the-Shelf Smartwatches</title>
      <link>https://arxiv.org/abs/2602.21610</link>
      <description>arXiv:2602.21610v1 Announce Type: new 
Abstract: Tracking hand poses on wrist-wearables enables rich, expressive interactions, yet remains unavailable on commercial smartwatches, as prior implementations rely on external sensors or custom hardware, limiting their real-world applicability. To address this, we present WatchHand, the first continuous 3D hand pose tracking system implemented on off-the-shelf smartwatches using only their built-in speaker and microphone. WatchHand emits inaudible frequency-modulated continuous waves and captures their reflections from the hand. These acoustic signals are processed by a deep-learning model that estimates 3D hand poses for 20 finger joints. We evaluate WatchHand across diverse real-world conditions -- multiple smartwatch models, wearing-hands, body postures, noise conditions, pose-variation protocols -- and achieve a mean per-joint position error of 7.87 mm in cross-session tests with device remounting. Although performance drops for unseen users or gestures, the model adapts effectively with lightweight fine-tuning on small amounts of data. Overall, WatchHand lowers the barrier to smartwatch-based hand tracking by eliminating additional hardware while enabling robust, always-available interactions on millions of existing devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21610v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiwan Kim, Chi-Jung Lee, Hohurn Jung, Tianhong Catherine Yu, Ruidong Zhang, Ian Oakley, Cheng Zhang</dc:creator>
    </item>
    <item>
      <title>"Without AI, I Would Never Share This Online": Unpacking How LLMs Catalyze Women's Sharing of Gendered Experiences on Social Media</title>
      <link>https://arxiv.org/abs/2602.21686</link>
      <description>arXiv:2602.21686v1 Announce Type: new 
Abstract: Sharing gendered experiences on social media has been widely recognized as supporting women's personal sense-making and contributing to digital feminism. However, there are known concerns, such as fear of judgment and backlash, that may discourage women from posting online. In this study, we examine a recurring practice on Xiaohongshu, a popular Chinese social media platform, in which women share their gendered experiences alongside screenshots of conversations with LLMs. We conducted semi-structured interviews with 20 women to investigate whether and how interactions with LLMs might support women in articulating and sharing gendered experiences. Our findings reveal that, beyond those external concerns, women also hold self-imposed standards regarding what feels appropriate and worthwhile to share publicly. We further show how interactions with LLMs help women meet these standards and navigate such concerns. We conclude by discussing how LLMs might be carefully and critically leveraged to support women's everyday expression online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21686v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772363.3798483</arxiv:DOI>
      <dc:creator>Runhua Zhang, Ziqi Pan, Huiran Yi, Huamin Qu, Xiaojuan Ma</dc:creator>
    </item>
    <item>
      <title>Heads Up!: Towards In Situ Photogrammetry Annotations and Augmented Reality Visualizations for Guided Backcountry Skiing</title>
      <link>https://arxiv.org/abs/2602.21771</link>
      <description>arXiv:2602.21771v1 Announce Type: new 
Abstract: Backcountry skiing is an activity where a group of skiers navigate challenging environmental conditions to ski outside of managed areas. This activity requires careful monitoring and effective communication around the current weather and terrain conditions to ensure skier safety. We aim to support and facilitate this communication by providing backcountry guides with a set of in situ spatial annotation tools to communicate hazards and appropriate speeds to the ski recreationalists. A guide can use a tablet application to annotate a photogrammetry-based map of a mountainside, for example, one collected using a commercial camera drone, with hazard points, slow-down zones, and safe zones. These annotations are communicated to the skiers via visual overlays in augmented reality heads-up displays. We present a prototype consisting of a web application and a virtual reality display that mirror the guide's and skier's perspectives, enabling participatory interaction design studies in a safe environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21771v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Albert Johns, L\'aszl\'o Kop\'acsi, Michael Barz, Daniel Sonntag</dc:creator>
    </item>
    <item>
      <title>StylusPort: Investigating Teleportation using Stylus in VR</title>
      <link>https://arxiv.org/abs/2602.21799</link>
      <description>arXiv:2602.21799v1 Announce Type: new 
Abstract: With a stylus, users can both sweep sketches across models and pinpoint locations with precision. Building on this dual capability, we explore how teleportation can be integrated into stylus interaction without disrupting the flow of common stylus usage. We introduce two key ideas: flipping the stylus as an intuitive mode switch between drawing and teleportation, and using gaze to set orientation while the stylus handles positioning. In a user study that features a teleport-and-orient task, we evaluate six teleportation techniques, covering two mode-switching methods (Button and Flip) and three orientation approaches (StylusRoll, StylusPoint, and GazePoint). The results offer new insights into the relative merits and limitations of each technique. Our work contributes to knowledge about teleportation in VR and fills the gap in seamlessly integrating teleportation with stylus use in 3D.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21799v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3793420</arxiv:DOI>
      <dc:creator>Yang Liu, Qiushi Zhou, Mathias N Lystb{\ae}k, Aidan Kehoe, Mario Gutierrez, Hans Gellersen, Ken Pfeuffer</dc:creator>
    </item>
    <item>
      <title>Interactive Augmented Reality-enabled Outdoor Scene Visualization For Enhanced Real-time Disaster Response</title>
      <link>https://arxiv.org/abs/2602.21874</link>
      <description>arXiv:2602.21874v1 Announce Type: new 
Abstract: A user-centered AR interface for disaster response is presented in this work that uses 3D Gaussian Splatting (3DGS) to visualize detailed scene reconstructions, while maintaining situational awareness and keeping cognitive load low. The interface relies on a lightweight interaction approach, combining World-in-Miniature (WIM) navigation with semantic Points of Interest (POIs) that can be filtered as needed, and it is supported by an architecture designed to stream updates as reconstructions evolve. User feedback from a preliminary evaluation indicates that this design is easy to use and supports real-time coordination, with participants highlighting the value of interaction and POIs for fast decision-making in context. Thorough user-centric performance evaluation demonstrates strong usability of the developed interface and high acceptance ratios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21874v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dimitrios Apostolakis, Georgios Angelidis, Vasileios Argyriou, Panagiotis Sarigiannidis, Georgios Th. Papadopoulos</dc:creator>
    </item>
    <item>
      <title>A Critical Reflection on the Values and Assumptions in Data Visualization</title>
      <link>https://arxiv.org/abs/2602.22051</link>
      <description>arXiv:2602.22051v1 Announce Type: new 
Abstract: Visualization has matured into an established research field, producing widely adopted tools, design frameworks, and empirical foundations. As the field has grown, ideas from outside computer science have increasingly entered visualization discourse, questioning the fundamental values and assumptions on which visualization research stands. In this short position paper, we examine a set of values that we see underlying the seminal works of Jacques Bertin, John Tukey, Leland Wilkinson, Colin Ware, and Tamara Munzner. We articulate three prominent values in these texts - universality, objectivity, and efficiency - and examine how these values permeate visualization tools, curricula, and research practices. We situate these values within a broader set of critiques that call for more diverse priorities and viewpoints. By articulating these tensions, we call for our community to embrace a more pluralistic range of values to shape our future visualization tools and guidelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22051v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shehryar Saharan, Ibrahim Al-Hazwani, Miriah Meyer, Laura Garrison</dc:creator>
    </item>
    <item>
      <title>ViSTAR: Virtual Skill Training with Augmented Reality with 3D Avatars and LLM coaching agent</title>
      <link>https://arxiv.org/abs/2602.22077</link>
      <description>arXiv:2602.22077v1 Announce Type: new 
Abstract: We present ViSTAR, a Virtual Skill Training system in AR that supports self-guided basketball skill practice, with feedback on balance, posture, and timing. From a formative study with basketball players and coaches, the system addresses three challenges: understanding skills, identifying errors, and correcting mistakes. ViSTAR follows the Behavioral Skills Training (BST) framework-instruction, modeling, rehearsal, and feedback. It provides feedback through visual overlays, rhythm and timing cues, and an AI-powered coaching agent using 3D motion reconstruction. We generate verbal feedback by analyzing spatio-temporal joint data and mapping features to natural-language coaching cues via a Large Language Model (LLM). A key novelty is this feedback generation: motion features become concise coaching insights. In two studies (N=16), participants generally preferred our AI-generated feedback to coach feedback and reported that ViSTAR helped them notice posture and balance issues and refine movements beyond self-observation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22077v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunggi Lee, Hayato Saiki, Tica Lin, Eiji Ikeda, Kenji Suzuki, Chen Zhu-Tian, Hanspeter Pfister</dc:creator>
    </item>
    <item>
      <title>SocialPulse: On-Device Detection of Social Interactions in Naturalistic Settings Using Smartwatch Multimodal Sensing</title>
      <link>https://arxiv.org/abs/2602.22085</link>
      <description>arXiv:2602.22085v1 Announce Type: new 
Abstract: Social interactions are fundamental to well-being, yet automatically detecting them in daily life-particularly using wearables-remains underexplored. Most existing systems are evaluated in controlled settings, focus primarily on in-person interactions, or rely on restrictive assumptions (e.g., requiring multiple speakers within fixed temporal windows), limiting generalizability to real-world use. We present an on-watch interaction detection system designed to capture diverse interactions in naturalistic settings. A core component is a foreground speech detector trained on a public dataset. Evaluated on over 100,000 labeled foreground speech and background sound instances, the detector achieves a balanced accuracy of 85.51%, outperforming prior work by 5.11%.
  We evaluated the system in a real-world deployment (N=38), with over 900 hours of total smartwatch wear time. The system detected 1,691 interactions, 77.28% were confirmed via participant self-report, with durations ranging from under one minute to over one hour. Among correct detections, 81.45% were in-person, 15.7% virtual, and 1.85% hybrid. Leveraging participant-labeled data, we further developed a multimodal model achieving a balanced accuracy of 90.36% and a sensitivity of 91.17% on 33,698 labeled 15-second windows. These results demonstrate the feasibility of real-world interaction sensing and open the door to adaptive, context-aware systems responding to users' dynamic social environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22085v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Sabbir Ahmed, Kaitlyn Dorothy Petz, Noah French, Tanvi Lakhtakia, Aayushi Sangani, Mark Rucker, Xinyu Chen, Bethany A. Teachman, Laura E. Barnes</dc:creator>
    </item>
    <item>
      <title>Giving Meaning to Movements: Challenges and Opportunities in Expanding Communication by Pairing Unaided AAC with Speech Generated Messages</title>
      <link>https://arxiv.org/abs/2602.22131</link>
      <description>arXiv:2602.22131v1 Announce Type: new 
Abstract: Augmentative and Alternative Communication (AAC) technologies are categorized into two forms: aided AAC, which uses external devices like speech-generating systems to produce standardized output, and unaided AAC, which relies on body-based gestures for natural expression but requires shared understanding. We investigate how to combine these approaches to harness the speed and naturalness of unaided AAC while maintaining the intelligibility of aided AAC, a largely unexplored area for individuals with communication and motor impairments. Through 18 months of participatory design with AAC users, we identified key challenges and opportunities and developed AllyAAC, a wearable system with a wrist-worn IMU paired with a smartphone app. We evaluated AllyAAC in a field study with 14 participants and produced a dataset containing over 600,000 multimodal data points featuring atypical gestures--the first of its kind. Our findings reveal challenges in recognizing personalized, idiosyncratic gestures and demonstrate how to address them using Transformer-based large machine learning (ML) models with different pretraining strategies. In sum, we contribute design principles and a reference implementation for adaptive, personalized systems combining aided and unaided AAC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22131v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3791273</arxiv:DOI>
      <dc:creator>Imran Kabir, Sharon Ann Redmon, Lynn R Elko, Kevin Williams, Mitchell A Case, Dawn J Sowers, Krista Wilkinson, Syed Masum Billah</dc:creator>
    </item>
    <item>
      <title>Speculating for Epiplexity: How to Learn the Most from Speculative Design?</title>
      <link>https://arxiv.org/abs/2602.22132</link>
      <description>arXiv:2602.22132v1 Announce Type: new 
Abstract: Speculative design uses provocative "what if?" scenarios to explore possible sociotechnical futures, yet lacks rigorous criteria for assessing the quality of speculation. We address this gap by reframing speculative design through an information-theoretic lens as a resource-bounded knowledge generation process that uses provotypes to strategically embrace surprise. However, not all surprises are equally informative-some yield genuine insight while others remain aesthetic shock. Drawing on epiplexity-structured, learnable information extractable by bounded observers-we propose decomposing the knowledge generated by speculative artifacts into structured epistemic information (transferable implications about futures) and entropic noise (narrative, aesthetics, and surface-level surprise). We conclude by introducing a practical audit framework with a self-assessment questionnaire that enables designers to evaluate whether their speculations yield rich, high-epiplexity insights or remain at a superficial level. We discuss implications for peer review, design pedagogy, and policy-oriented futuring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22132v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Botao Amber Hu</dc:creator>
    </item>
    <item>
      <title>When AI Writes, Whose Voice Remains? Quantifying Cultural Marker Erasure Across World English Varieties in Large Language Models</title>
      <link>https://arxiv.org/abs/2602.22145</link>
      <description>arXiv:2602.22145v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly used to ``professionalize'' workplace communication, often at the cost of linguistic identity. We introduce "Cultural Ghosting", the systematic erasure of linguistic markers unique to non-native English varieties during text processing. Through analysis of 22,350 LLM outputs generated from 1,490 culturally marked texts (Indian, Singaporean,&amp; Nigerian English) processed by five models under three prompt conditions, we quantify this phenomenon using two novel metrics: Identity Erasure Rate (IER) &amp; Semantic Preservation Score (SPS). Across all prompts, we find an overall IER of 10.26%, with model-level variation from 3.5% to 20.5% (5.9x range). Crucially, we identify a Semantic Preservation Paradox: models maintain high semantic similarity (mean SPS = 0.748) while systematically erasing cultural markers. Pragmatic markers (politeness conventions) are 1.9x more vulnerable than lexical markers (71.5% vs. 37.1% erasure). Our experiments demonstrate that explicit cultural-preservation prompts reduce erasure by 29% without sacrificing semantic quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22145v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772363.3799085</arxiv:DOI>
      <dc:creator>Satyam Kumar Navneet, Joydeep Chandra, Yong Zhang</dc:creator>
    </item>
    <item>
      <title>A Taxonomy of Human--MLLM Interaction in Early-Stage Sketch-Based Design Ideation</title>
      <link>https://arxiv.org/abs/2602.22171</link>
      <description>arXiv:2602.22171v2 Announce Type: new 
Abstract: As multimodal large language models (MLLMs) are increasingly integrated into early-stage design tools, it is important to understand how designers collaborate with AI during ideation. In a user study with 12 participants, we analysed sketch-based design interactions with an MLLM-powered system using automatically recorded interaction logs and post-task interviews. Based on how creative responsibility was allocated between humans and the AI, we predefined four interaction modes: Human-Only, Human-Lead, AI-Lead, and Co-Evolution, and analysed how these modes manifested during sketch-based design ideation. Our results show that designers rarely rely on a single mode; instead, human-led and AI-led roles are frequently interwoven and shift across ideation instances. These findings provide an empirical basis for future work to investigate why designers shift roles with AI and how interactive systems can better support such dynamic collaboration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22171v2</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772363.3798524</arxiv:DOI>
      <dc:creator>Weiayn Shi, Kenny Tsu Wei Choo</dc:creator>
    </item>
    <item>
      <title>Codesigning Ripplet: an LLM-Assisted Assessment Authoring System Grounded in a Conceptual Model of Teachers' Workflows</title>
      <link>https://arxiv.org/abs/2602.22186</link>
      <description>arXiv:2602.22186v1 Announce Type: new 
Abstract: Assessments are critical in education, but creating them can be difficult. To address this challenge in a grounded way, we partnered with 13 teachers in a seven-month codesign process. We developed a conceptual model that characterizes the iterative dual process where teachers develop assessments while simultaneously refining requirements. To enact this model in practice, we built Ripplet, a web-based tool with multilevel reusable interactions to support assessment authoring. The extended codesign revealed that Ripplet enabled teachers to create formative assessments they would not have otherwise made, shifted their practices from generation to curation, and helped them reflect more on assessment quality. In a user study with 15 additional teachers, compared to their current practices, teachers felt the results were more worth their effort and that assessment quality improved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22186v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3790418</arxiv:DOI>
      <dc:creator>Yuan Cui, Annabel Goldman, Jovy Zhou, Xiaolin Liu, Clarissa Shieh, Joshua Yao, Mia Cheng, Matthew Kay, Fumeng Yang</dc:creator>
    </item>
    <item>
      <title>Towards Narrative Medical Visualization</title>
      <link>https://arxiv.org/abs/2108.05462</link>
      <description>arXiv:2108.05462v1 Announce Type: cross 
Abstract: Narrative visualization aims to communicate scientific results to a general audience and garners significant attention in various applications. Merging exploratory and explanatory visualization could effectively support a non-expert understanding of scientific processes. Medical research results, e.g., mechanisms of the healthy human body, explanations of pathological processes, or avoidable risk factors for diseases, are also interesting to a general audience that includes patients and their relatives. This paper discusses how narrative techniques can be applied to medical visualization to tell data-driven stories about diseases. We address the general public comprising people interested in medicine without specific medical background knowledge. We derived a general template for the narrative medical visualization of diseases. Applying this template to three diseases selected to span bone, vascular, and organ systems, we discuss how narrative techniques can support visual communication and facilitate understanding of medical data. Other scientists can adapt our proposed template to inform an audience on other diseases. With our work, we show the potential of narrative medical visualization and conclude with a comprehensive research agenda.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.05462v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Monique Meuschke, Laura Garrison, Noeska Smit, Stefan Bruckner, Kai Lawonn, Bernhard Preim</dc:creator>
    </item>
    <item>
      <title>The Language of Infographics: Toward Understanding Conceptual Metaphor Use in Scientific Storytelling</title>
      <link>https://arxiv.org/abs/2407.13416</link>
      <description>arXiv:2407.13416v1 Announce Type: cross 
Abstract: We apply an approach from cognitive linguistics by mapping Conceptual Metaphor Theory (CMT) to the visualization domain to address patterns of visual conceptual metaphors that are often used in science infographics. Metaphors play an essential part in visual communication and are frequently employed to explain complex concepts. However, their use is often based on intuition, rather than following a formal process. At present, we lack tools and language for understanding and describing metaphor use in visualization to the extent where taxonomy and grammar could guide the creation of visual components, e.g., infographics. Our classification of the visual conceptual mappings within scientific representations is based on the breakdown of visual components in existing scientific infographics. We demonstrate the development of this mapping through a detailed analysis of data collected from four domains (biomedicine, climate, space, and anthropology) that represent a diverse range of visual conceptual metaphors used in the visual communication of science. This work allows us to identify patterns of visual conceptual metaphor use within the domains, resolve ambiguities about why specific conceptual metaphors are used, and develop a better overall understanding of visual metaphor use in scientific infographics. Our analysis shows that ontological and orientational conceptual metaphors are the most widely applied to translate complex scientific concepts. To support our findings we developed a visual exploratory tool based on the collected database that places the individual infographics on a spatio-temporal scale and illustrates the breakdown of visual conceptual metaphors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13416v1</guid>
      <category>cs.IR</category>
      <category>cs.HC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TVCG.2024.3456327</arxiv:DOI>
      <dc:creator>Hana Pokojn\'a, Tobias Isenberg, Stefan Bruckner, Barbora Kozl\'ikov\'a, Laura Garrison</dc:creator>
    </item>
    <item>
      <title>Evaluating the Usage of African-American Vernacular English in Large Language Models</title>
      <link>https://arxiv.org/abs/2602.21485</link>
      <description>arXiv:2602.21485v1 Announce Type: cross 
Abstract: In AI, most evaluations of natural language understanding tasks are conducted in standardized dialects such as Standard American English (SAE). In this work, we investigate how accurately large language models (LLMs) represent African American Vernacular English (AAVE). We analyze three LLMs to compare their usage of AAVE to the usage of humans who natively speak AAVE. We first analyzed interviews from the Corpus of Regional African American Language and TwitterAAE to identify the typical contexts where people use AAVE grammatical features such as ain't. We then prompted the LLMs to produce text in AAVE and compared the model-generated text to human usage patterns. We find that, in many cases, there are substantial differences between AAVE usage in LLMs and humans: LLMs usually underuse and misuse grammatical features characteristic of AAVE. Furthermore, through sentiment analysis and manual inspection, we found that the models replicated stereotypes about African Americans. These results highlight the need for more diversity in training data and the incorporation of fairness methods to mitigate the perpetuation of stereotypes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21485v1</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deja Dunlap, R. Thomas McCoy</dc:creator>
    </item>
    <item>
      <title>The Governance of Intimacy: A Preliminary Policy Analysis of Romantic AI Platforms</title>
      <link>https://arxiv.org/abs/2602.22000</link>
      <description>arXiv:2602.22000v1 Announce Type: cross 
Abstract: Romantic AI platforms invite intimate emotional disclosure, yet their data governance practices remain underexamined. This preliminary study analyses the Privacy Policies and Terms of Service of six Western and Chinese romantic AI platforms. We find that intimate disclosures are often positioned as reusable data assets, with broad permissions for storage, analysis, and model training. We identify default training appropriation, ownership reconstruction, and intimate history assetization as key mechanisms structuring these practices, expanding platforms' rights while shifting risk onto users. Our findings surface key governance challenges in romantic AI and are intended to provoke discussion and inform future empirical and design research on human AI intimacy and its governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22000v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiao Zhan, Yifan Xu, Rongjun Ma, Shijing He, Jose Luis Martin-Navarro, Jose Such</dc:creator>
    </item>
    <item>
      <title>Detecting UX smells in Visual Studio Code using LLMs</title>
      <link>https://arxiv.org/abs/2602.22020</link>
      <description>arXiv:2602.22020v1 Announce Type: cross 
Abstract: Integrated Development Environments shape developers' daily experience, yet the empirical study of their usability and user experience (UX) remains limited. This work presents an LLM-assisted approach to detecting UX smells in Visual Studio Code by mining and classifying user-reported issues from the GitHub repository. Using a validated taxonomy and expert review, we identified recurring UX problems that affect the developer experience. Our results show that the majority of UX smells are concentrated in informativeness, clarity, intuitiveness, and efficiency, qualities that developers value most.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22020v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3786151.3788606</arxiv:DOI>
      <dc:creator>Andr\'es Rodriguez, Juan Cruz Gardey, Alejandra Garrido</dc:creator>
    </item>
    <item>
      <title>SPGen: Stochastic scanpath generation for paintings using unsupervised domain adaptation</title>
      <link>https://arxiv.org/abs/2602.22049</link>
      <description>arXiv:2602.22049v1 Announce Type: cross 
Abstract: Understanding human visual attention is key to preserving cultural heritage We introduce SPGen a novel deep learning model to predict scanpaths the sequence of eye movementswhen viewers observe paintings.
  Our architecture uses a Fully Convolutional Neural Network FCNN with differentiable fixation selection and learnable Gaussian priors to simulate natural viewing biases To address the domain gap between photographs and artworks we employ unsupervised domain adaptation via a gradient reversal layer allowing the model to transfer knowledge from natural scenes to paintings Furthermore a random noise sampler models the inherent stochasticity of eyetracking data.
  Extensive testing shows SPGen outperforms existing methods offering a powerful tool to analyze gaze behavior and advance the preservation and appreciation of artistic treasures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22049v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mohamed Amine Kerkouri, Marouane Tliba, Aladine Chetouani, Alessandro Bruno</dc:creator>
    </item>
    <item>
      <title>Dynamic Personality Adaptation in Large Language Models via State Machines</title>
      <link>https://arxiv.org/abs/2602.22157</link>
      <description>arXiv:2602.22157v1 Announce Type: cross 
Abstract: The inability of Large Language Models (LLMs) to modulate their personality expression in response to evolving dialogue dynamics hinders their performance in complex, interactive contexts. We propose a model-agnostic framework for dynamic personality simulation that employs state machines to represent latent personality states, where transition probabilities are dynamically adapted to the conversational context. Part of our architecture is a modular pipeline for continuous personality scoring that evaluates dialogues along latent axes while remaining agnostic to the specific personality models, their dimensions, transition mechanisms, or LLMs used. These scores function as dynamic state variables that systematically reconfigure the system prompt, steering behavioral alignment throughout the interaction.We evaluate this framework by operationalizing the Interpersonal Circumplex (IPC) in a medical education setting. Results demonstrate that the system successfully adapts its personality state to user inputs, but also influences user behavior, thereby facilitating de-escalation training. Notably, the scoring pipeline maintains comparable precision even when utilizing lightweight, fine-tuned classifiers instead of large-scale LLMs. This work demonstrates the feasibility of modular, personality-adaptive architectures for education, customer support, and broader human-computer interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22157v1</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leon Pielage, Ole H\"atscher, Mitja Back, Bernhard Marschall, Benjamin Risse</dc:creator>
    </item>
    <item>
      <title>Cross, Dwell, or Pinch: Designing and Evaluating Around-Device Selection Methods for Unmodified Smartwatches</title>
      <link>https://arxiv.org/abs/2503.02308</link>
      <description>arXiv:2503.02308v2 Announce Type: replace 
Abstract: Smartwatches offer powerful features, but their small touchscreens limit the expressiveness of the input that can be achieved. To address this issue, we present, and open-source, the first sonar-based around-device input on an unmodified consumer smartwatch. We achieve this using a fine-grained, one-dimensional sonar-based finger-tracking system. In addition, we use this system to investigate the fundamental issue of how to trigger selections during around-device smartwatch input through two studies. The first examines the methods of double-crossing, dwell, and finger tap in a binary task, while the second considers a subset of these designs in a multi-target task and in the presence and absence of haptic feedback. Results showed double-crossing was optimal for binary tasks, while dwell excelled in multi-target scenarios, and haptic feedback enhanced comfort but not performance. These findings offer design insights for future around-device smartwatch interfaces that can be directly deployed on today's consumer hardware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02308v2</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiwan Kim, Jiwan Son, Ian Oakley</dc:creator>
    </item>
    <item>
      <title>Rethinking User Empowerment in AI Recommender System: Innovating Transparent and Controllable Interfaces</title>
      <link>https://arxiv.org/abs/2509.11098</link>
      <description>arXiv:2509.11098v2 Announce Type: replace 
Abstract: AI-driven recommender systems are often perceived as personalization black boxes, limiting users' ability to understand how their data shapes content (information asymmetry) or to influence system behavior meaningfully (power asymmetry). This study explores how design can strengthen user agency by integrating transparency with actionable control. We developed a provotype that introduces new interface features for managing data use, discovering varied content, and configuring context-based recommending modes. The walkthroughs and interviews with 19 participants show how these features help users interpret personalization signals, understand how their actions influence outcomes, address concerns from unwanted inference to narrow feeds (e.g., filter bubbles), and build trust in the system. We also identify strategies for promoting adoption and awareness of agency-enhancing features. Overall, our findings reaffirm users' desire for active influence over personalization and contribute concrete interface mechanisms with empirical insights for designing recommender systems that foreground user autonomy and fairness in AI-driven content delivery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11098v2</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengke Wu, Weizi Liu, Yanyun Wang, Weiyu Ding, Mike Yao</dc:creator>
    </item>
    <item>
      <title>ClearFairy: Capturing Creative Workflows through Decision Structuring, In-Situ Questioning, and Rationale Inference</title>
      <link>https://arxiv.org/abs/2509.14537</link>
      <description>arXiv:2509.14537v2 Announce Type: replace 
Abstract: Capturing professionals' decision-making in creative workflows (e.g., UI/UX) is essential for reflection, collaboration, and knowledge sharing, yet existing methods often leave rationales incomplete and implicit decisions hidden. To address this, we present the CLEAR approach, which structures reasoning into cognitive decision steps-linked units of actions, artifacts, and explanations making decisions traceable with generative AI. Building on CLEAR, we introduce ClearFairy, a think-aloud AI assistant for UI design that detects weak explanations, asks lightweight clarifying questions, and infers missing rationales. In a study with twelve professionals, 85% of ClearFairy's inferred rationales were accepted (as-is or with revisions). Notably, the system increased "strong explanations"-rationales providing sufficient causal reasoning-from 14% to 83% without adding cognitive demand. Furthermore, exploratory applications demonstrate that captured steps can enhance generative AI agents in Figma, yielding predictions better aligned with professionals and producing coherent outcomes. We release a dataset of 417 decision steps to support future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14537v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kihoon Son, DaEun Choi, Tae Soo Kim, Young-Ho Kim, Sangdoo Yun, Juho Kim</dc:creator>
    </item>
    <item>
      <title>ConsentDiff at Scale: Longitudinal Audits of Web Privacy Policy Changes and UI Frictions</title>
      <link>https://arxiv.org/abs/2512.04316</link>
      <description>arXiv:2512.04316v5 Announce Type: replace 
Abstract: Web privacy is experienced via two public artifacts: site utterances in policy texts, and the actions users are required to take during consent interfaces. In the extensive cross-section audits we've studied, there is a lack of longitudinal data detailing how these artifacts are changing together, and if interfaces are actually doing what they promise in policy. ConsentDiff provides that longitudinal view. We build a reproducible pipeline that snapshots sites every month, semantically aligns policy clauses to track clause-level churn, and classifies consent-UI patterns by pulling together DOM signals with cues provided by screenshots. We introduce a novel weighted claim-UI alignment score, connecting common policy claims to observable predicates, and enabling comparisons over time, regions, and verticals. Our measurements suggest continued policy churn, systematic changes to eliminate a higher-friction banner design, and significantly higher alignment where rejecting is visible and lower friction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04316v5</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoze Guo</dc:creator>
    </item>
    <item>
      <title>CalmReminder: A Design Probe for Parental Engagement with Children with Hyperactivity, Augmented by Real-Time Motion Sensing with a Watch</title>
      <link>https://arxiv.org/abs/2602.16893</link>
      <description>arXiv:2602.16893v2 Announce Type: replace 
Abstract: Families raising children with ADHD often experience heightened stress and reactive parenting. While digital interventions promise personalization, many remain one-size-fits-all and fail to reflect parents' lived practices. We present CalmReminder, a watch-based system that detects children's calm moments and delivers just-in-time prompts to parents. Through a four-week deployment with 16 families (twelve completed) of children with ADHD, we compared notification strategies ranging from hourly to random to only when the child was inferred to be calm. Our sensing-based notifications were frequently perceived as arriving during calm moments. More importantly, parents adopted the system in diverse ways: using notifications for praise, mindfulness, activity planning, or conversation. These findings show that parents are not passive recipients but active designers, reshaping interventions to fit their parenting styles. We contribute a calm detection pipeline, empirical insights into families' flexible appropriation of notifications, and design implications for intervention systems that foster agency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16893v2</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3791871</arxiv:DOI>
      <dc:creator>Riku Arakawa, Shreya Bali, Anupama Sitaraman, Woosuk Seo, Sam Shaaban, Oliver Lindheim, Traci M. Kennedy, Mayank Goel</dc:creator>
    </item>
    <item>
      <title>Evidotes: Integrating Scientific Evidence and Anecdotes to Support Uncertainties Triggered by Peer Health Posts</title>
      <link>https://arxiv.org/abs/2602.16900</link>
      <description>arXiv:2602.16900v2 Announce Type: replace 
Abstract: Peer health posts surface new uncertainties, such as questions and concerns for readers. Prior work focused primarily on improving relevance and accuracy fails to address users' diverse information needs and emotions triggered. Instead, we propose directly addressing these by information augmentation. We introduce Evidotes, an information support system that augments individual posts with relevant scientific and anecdotal information retrieved using three user-selectable lenses (dive deeper, focus on positivity, and big picture). In a mixed-methods study with 17 chronic illness patients, Evidotes improved self-reported information satisfaction (3.2-&gt;4.6) and reduced self-reported emotional cost (3.4-&gt;1.9) compared to participants' baseline browsing. Moreover, by co-presenting sources, Evidotes unlocked information symbiosis: anecdotes made research accessible and contextual, while research helped filter and generalize peer stories. Our work enables an effective integration of scientific evidence and human anecdotes to help users better manage health uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16900v2</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3791089</arxiv:DOI>
      <dc:creator>Shreya Bali, Riku Arakawa, Peace Odiase, Tongshuang Wu, Mayank Goel</dc:creator>
    </item>
    <item>
      <title>MagHeart: Exploring Playful Avatar Co-Creation and Shared Heartbeats for Icebreaking in Hybrid Meetings</title>
      <link>https://arxiv.org/abs/2602.18676</link>
      <description>arXiv:2602.18676v2 Announce Type: replace 
Abstract: Hybrid meetings often begin with social awkwardness and asymmetric participation, particularly for remote attendees who lack access to informal, co-present interaction. We present MagHeart, a multimodal system that explores symmetric icebreaking in hybrid meetings through playful LEGO-based avatar co-creation and a tangible magnetic device that represents a remote participant's heartbeat as an ambient presence cue. By combining creative co-creation with abstract bio-feedback, MagHeart rethinks how remote participants can become materially and perceptually present during meeting openings. We report findings from a scenario-based exploratory study combining quantitative and qualitative data, examining participants' anticipated engagement, perceived social presence, and future-use intentions from both co-located and remote perspectives. Our results highlight opportunities for playful, embodied icebreakers to support early hybrid interaction, while also surfacing tensions around privacy, distraction, and contextual appropriateness. This work contributes design insights and open questions for future hybrid meeting tools that balance playfulness, embodiment, and social sensitivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18676v2</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Black Sun, Haiyang Xu, Ge Kacy Fu, Liyue Da, Eve Hoggan</dc:creator>
    </item>
    <item>
      <title>Toward Safe and Human-Aligned Game Conversational Recommendation via Multi-Agent Decomposition</title>
      <link>https://arxiv.org/abs/2504.20094</link>
      <description>arXiv:2504.20094v3 Announce Type: replace-cross 
Abstract: Conversational recommender systems (CRS) have advanced with large language models, showing strong results in domains like movies. These domains typically involve fixed content and passive consumption, where user preferences can be matched by genre or theme. In contrast, games present distinct challenges: fast-evolving catalogs, interaction-driven preferences (e.g., skill level, mechanics, hardware), and increased risk of unsafe responses in open-ended conversation. We propose MATCHA, a multi-agent framework for CRS that assigns specialized agents for intent parsing, tool-augmented retrieval, multi-LLM ranking with reflection, explanation, and risk control which enabling finer personalization, long-tail coverage, and stronger safety. Evaluated on real user request dataset, MATCHA outperforms six baselines across eight metrics, improving Hit@5 by 20%, reducing popularity bias by 24%, and achieving 97.9% adversarial defense. Human and virtual-judge evaluations confirm improved explanation quality and user alignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20094v3</guid>
      <category>cs.IR</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zheng Hui, Xiaokai Wei, Yexi Jiang, Kevin Gao, Chen Wang, Frank Ong, Se-eun Yoon, Rachit Pareek, Michelle Gong</dc:creator>
    </item>
    <item>
      <title>The Hammock Plot: Where Categorical and Numerical Data Relax Together</title>
      <link>https://arxiv.org/abs/2506.13630</link>
      <description>arXiv:2506.13630v3 Announce Type: replace-cross 
Abstract: Effective methods for visualizing data involving multiple variables, including categorical ones, are limited. The hammock plot (Schonlau 2003) visualizes both categorical and numerical variables using parallel coordinates. We introduce the Stata implementation hammock. We give numerous examples that explore highlighting, missing values, putting axes on the same scale, and tracing an observation across variables. Further, we discuss parallel univariate plots as an edge case of hammock plots. We also present and make publicly available a new dataset on the 2020 Tour de France.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13630v3</guid>
      <category>stat.AP</category>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthias Schonlau, Tiancheng Yang</dc:creator>
    </item>
    <item>
      <title>Federated Learning in Offline and Online EMG Decoding: A Privacy and Performance Perspective</title>
      <link>https://arxiv.org/abs/2507.12652</link>
      <description>arXiv:2507.12652v2 Announce Type: replace-cross 
Abstract: Neural interfaces offer a pathway to intuitive, high-bandwidth interaction, but the sensitive nature of neural data creates significant privacy hurdles for large-scale model training. Federated learning (FL) has emerged as a promising privacy-preserving solution, yet its efficacy in real-time, online neural interfaces remains unexplored. In this study, we 1) propose a conceptual framework for applying FL to the distinct constraints of neural interface application and 2) provide a systematic evaluation of FL-based neural decoding using high-dimensional electromyography (EMG) across both offline simulations and a real-time, online user study. While offline results suggest that FL can simultaneously enhance performance and privacy, our online experiments reveal a more complex landscape. We found that standard FL assumptions struggle to translate to real-time, sequential interactions with human-decoder co-adaptation. Our results show that while FL retains privacy advantages, it introduces performance tensions not predicted by offline simulations. These findings identify a critical gap in current FL methodologies and highlight the need for specialized algorithms designed to navigate the unique co-adaptive dynamics of sequential-user neural decoding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12652v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kai Malcolm, C\'esar Uribe, Momona Yamagami</dc:creator>
    </item>
    <item>
      <title>Toward a Decision Support System for Energy-Efficient Ferry Operation on Lake Constance based on Optimal Control</title>
      <link>https://arxiv.org/abs/2512.11786</link>
      <description>arXiv:2512.11786v2 Announce Type: replace-cross 
Abstract: The maritime sector is undergoing a disruptive technological change driven by three main factors: autonomy, decarbonization, and digital transformation. Addressing these factors necessitates a reassessment of inland vessel operations. This paper presents the design and development of a decision support system for ferry operations based on a shrinking-horizon optimal control framework. The problem formulation incorporates a mathematical model of the ferry's dynamics and environmental disturbances, specifically water currents and wind, which can significantly influence the dynamics. Real-world data and illustrative scenarios demonstrate the potential of the proposed system to effectively support ferry crews by providing real-time guidance. This enables enhanced operational efficiency while maintaining predefined maneuver durations. The findings suggest that optimal control applications hold substantial promise for advancing future ferry operations on inland waters. A video of the real-world ferry MS Insel Mainau operating on Lake Constance is available at: https://youtu.be/i1MjCdbEQyE</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11786v2</guid>
      <category>eess.SY</category>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hannes Homburger, Bastian J\"ackl, Stefan Wirtensohn, Christian Stopp, Maximilian T. Fischer, Moritz Diehl, Daniel A. Keim, Johannes Reuter</dc:creator>
    </item>
    <item>
      <title>Stop Saying "AI"</title>
      <link>https://arxiv.org/abs/2602.17729</link>
      <description>arXiv:2602.17729v2 Announce Type: replace-cross 
Abstract: Across academia, industry, and government, ``AI'' has become central in research and development, regulatory debates, and promises of ever faster and more capable decision-making and action. In numerous domains, especially safety-critical ones, there are significant concerns over how ``AI'' may affect decision-making, responsibility, or the likelihood of mistakes (to name only a few categories of critique). However, for most critiques, the target is generally ``AI'', a broad term admitting many (types of) systems used for a variety of tasks and each coming with its own set of limitations, challenges, and potential use cases. In this article, we focus on the military domain as a case study and present both a loose enumerative taxonomy of systems captured under the umbrella term ``military AI'', as well as discussion of the challenges of each. In doing so, we highlight that critiques of one (type of) system will not always transfer to other (types of) systems. Building on this, we argue that in order for debates to move forward fruitfully, it is imperative that the discussions be made more precise and that ``AI'' be excised from debates to the extent possible. Researchers, developers, and policy-makers should make clear exactly what systems they have in mind and what possible benefits and risks attend the deployment of those particular systems. While we focus on AI in the military as an exemplar for the overall trends in discussions of ``AI'', the argument's conclusions are broad and have import for discussions of AI across a host of domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17729v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan G. Wood (Institute of Air Transportation Systems, Hamburg University of Technology, Ethics + Emerging Sciences Group, California Polytechnic State University San Luis Obispo, Center for Environmental and Technology Ethics - Prague), Scott Robbins (Academy for Responsible Research, Teaching, and Innovation, Karlsruhe Institute of Technology), Eduardo Zegarra Berodt (Institute of Air Transportation Systems, Hamburg University of Technology), Anton Graf von Westerholt (Institute of Air Transportation Systems, Hamburg University of Technology), Michelle Behrndt (Institute of Air Transportation Systems, Hamburg University of Technology, Department of Philosophy, University of Hamburg), Hauke Budig (Institute of Air Transportation Systems, Hamburg University of Technology), Daniel Kloock-Schreiber (Institute of Air Transportation Systems, Hamburg University of Technology)</dc:creator>
    </item>
  </channel>
</rss>
