<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Apr 2025 01:52:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Performance Evaluation of Scheduling Scheme in O-RAN 5G Network using NS-3</title>
      <link>https://arxiv.org/abs/2504.00417</link>
      <description>arXiv:2504.00417v1 Announce Type: new 
Abstract: The integration of Open Radio Access Network (O-RAN) principles into 5G networks introduces a paradigm shift in how radio resources are managed and optimized. O-RAN's open architecture enables the deployment of intelligent applications (xApps) that can dynamically adapt to varying network conditions and user demands. In this paper, we present radio resource scheduling schemes -- a possible O-RAN-compliant xApp can be designed. This xApp facilitates the implementation of customized scheduling strategies, tailored to meet the diverse Quality-of-Service (QoS) requirements of emerging 5G use cases, such as enhanced mobile broadband (eMBB), massive machine-type communications (mMTC), and ultra-reliable low-latency communications (URLLC).
  We have tested the implemented scheduling schemes within an ns-3 simulation environment, integrated with the O-RAN framework. The evaluation includes the implementation of the Max-Throughput (MT) scheduling policy -- which prioritizes resource allocation based on optimal channel conditions, the Proportional-Fair (PF) scheduling policy -- which balances fairness with throughput, and compared with the default Round Robin (RR) scheduler. In addition, the implemented scheduling schemes support dynamic Time Division Duplex (TDD), allowing flexible configuration of Downlink (DL) and Uplink (UL) switching for bidirectional transmissions, ensuring efficient resource utilization across various scenarios. The results demonstrate resource allocation's effectiveness under MT and PF scheduling policies. To assess the efficiency of this resource allocation, we analyzed the Modulation Coding Scheme (MCS), the number of symbols, and Transmission Time Intervals (TTIs) allocated per user, and compared them with the throughput achieved. The analysis revealed a consistent relationship between these factors and the observed throughput.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00417v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>A. K. Subudhi, A. Piccioni, V. Gudepu, A. Marotta, F. Graziosi, R. M. Hegde, K. Kondepu</dc:creator>
    </item>
    <item>
      <title>Next Generation LoRaWAN: Integrating Multi-Hop Communications at 2.4 GHz</title>
      <link>https://arxiv.org/abs/2504.00489</link>
      <description>arXiv:2504.00489v1 Announce Type: new 
Abstract: The Internet of Things (IoT) revolution demands scalable, energy-efficient communication protocols supporting widespread device deployments. The LoRa technology, coupled with the LoRaWAN protocol, has emerged as a leading Low Power Wide Area Network (LPWAN) solution, traditionally leveraging sub-GHz frequency bands for reliable long-range communication. However, these bands face constraints such as limited data rates and strict duty cycle regulations. Recent advancements have introduced the 2.4 GHz spectrum, offering superior data rates and unrestricted transmission opportunities at the cost of reduced coverage and severe interference. To solve this trade-off, this paper proposes a novel hybrid approach integrating multi-band (i.e., sub-GHz and 2.4 GHz) and multi-hop communication into LoRaWAN, while preserving compatibility with the existing standard. The proposed network architecture retains Gateways (GWs) and End Devices (EDs) operating within the sub-GHz frequency while introducing multi-band Relays (RLs) that act as forwarding nodes for 2.4 GHz EDs. Utilizing our previously developed open-source and standards-compliant simulation framework, we evaluate the network performance of our solution under realistic deployment scenarios. The results demonstrate substantial improvements compared to standard single-band and single-hop LoRaWAN networks, demonstrating the potential of this approach to redefine LPWAN capabilities and bridge the gap between current solutions and next-generation IoT applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00489v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Riccardo Marini, Giampaolo Cuozzo</dc:creator>
    </item>
    <item>
      <title>Performance Analysis, Lessons Learned and Practical Advice for a 6G Inter-Provider DApp on the Ethereum Blockchain</title>
      <link>https://arxiv.org/abs/2504.00555</link>
      <description>arXiv:2504.00555v1 Announce Type: new 
Abstract: This paper presents a multi-contract blockchain framework for inter-provider agreements in 6G networks, emphasizing performance analysis under a realistic Proof-of-Stake (PoS) setting on Ethereum's Sepolia testnet. We begin by quantifying Ethereum Virtual Machine (EVM)-based gas usage for critical operations such as provider registration, service addition, and SLA penalty enforcement, observing that cold writes and deep data structures can each inflate gas consumption by up to 20\%. We then examine block-level dynamics when multiple transactions execute concurrently, revealing that moderate concurrency (e.g., 30--50 simultaneous transactions) can fill blocks to 80--90\% of their gas limit and nearly double finalization times from around 15~seconds to over 30~seconds. Finally, we synthesize these insights into a practical design guide, demonstrating that flattening nested mappings, consolidating storage writes, and selectively timing high-impact transactions can markedly reduce costs and latency spikes. Collectively, our findings underscore the importance of EVM-specific optimizations and transaction scheduling for large-scale decentralized applications in 6G telecom scenarios. The implementation is available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00555v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Farhana Javed, Josep Mangues-Bafalluy</dc:creator>
    </item>
    <item>
      <title>Geometry Based UAV Trajectory Planning for Mixed User Traffic in mmWave Communication</title>
      <link>https://arxiv.org/abs/2504.00689</link>
      <description>arXiv:2504.00689v1 Announce Type: new 
Abstract: Unmanned aerial vehicle (UAV) assisted communication is a revolutionary technology that has been recently presented as a potential candidate for beyond fifth-generation millimeter wave (mmWave) communications. Although mmWaves can offer a notably high data rate, their high penetration and propagation losses mean that line of sight (LoS) is necessary for effective communication. Due to the presence of obstacles and user mobility, UAV trajectory planning plays a crucial role in improving system performance. In this work, we propose a novel computational geometry-based trajectory planning scheme by considering the user mobility, the priority of the delay sensitive ultra-reliable low-latency communications (URLLC) and the high throughput requirements of the enhanced mobile broadband (eMBB) traffic. Specifically, we use geometric tools like Apollonius circle and minimum enclosing ball of balls to find the optimal position of the UAV that supports uninterrupted connections to the URLLC users and maximizes the aggregate throughput of the eMBB users. Finally, the numerical results demonstrate the benefits of the suggested approach over an existing state of the art benchmark scheme in terms of sum throughput obtained by URLLC and eMBB users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00689v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sk Abid Hasan, Lakshmikanta Sau, Sasthi C. Ghosh</dc:creator>
    </item>
    <item>
      <title>Empirical Analysis of 5G TDD Patterns Configurations for Industrial Automation Traffic</title>
      <link>https://arxiv.org/abs/2504.00912</link>
      <description>arXiv:2504.00912v1 Announce Type: new 
Abstract: The digital transformation driven by Industry 4.0 relies on networks that support diverse traffic types with strict deterministic end-to-end latency and mobility requirements. To meet these requirements, future industrial automation networks will use time-sensitive networking, integrating 5G as wireless access points to connect production lines with time-sensitive networking bridges and the enterprise edge cloud. However, achieving deterministic end-to-end latency remains a challenge, particularly due to the variable packet transmission delay introduced by the 5G system. While time-sensitive networking bridges typically operate with latencies in the range of hundreds of microseconds, 5G systems may experience delays ranging from a few to several hundred milliseconds. This paper investigates the potential of configuring the 5G time division duplex pattern to minimize packet transmission delay in industrial environments. Through empirical measurements using a commercial 5G system, we evaluate different TDD configurations under varying traffic loads, packet sizes and full buffer status report activation. Based on our findings, we provide practical configuration recommendations for satisfying requirements in industrial automation, helping private network providers increase the adoption of 5G.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00912v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oscar Adamuz-Hinojosa, Felix Delgado-Ferro, N\'uria Dom\`enech, Jorge Navarro-Ortiz, Pablo Mu\~noz, Seyed Mahdi Darroudi, Pablo Ameigeiras, Juan M. Lopez-Soler</dc:creator>
    </item>
    <item>
      <title>Sustainable Open-Data Management for Field Research: A Cloud-Based Approach in the Underlandscape Project</title>
      <link>https://arxiv.org/abs/2503.16042</link>
      <description>arXiv:2503.16042v1 Announce Type: cross 
Abstract: Field-based research projects require a robust suite of ICT services to support data acquisition, documentation, storage, and dissemination. A key challenge lies in ensuring the sustainability of data management - not only during the project's funded period but also beyond its conclusion, when maintenance and support often depend on voluntary efforts. In the Underlandscape project, we tackled this challenge by extensively leveraging public cloud services while minimizing reliance on complex custom infrastructure. This paper provides a comprehensive overview of the project's final infrastructure, detailing the adopted data formats, the cloud-based solutions enabling data management, and the custom applications developed for system integration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16042v1</guid>
      <category>cs.DB</category>
      <category>cs.CE</category>
      <category>cs.NI</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.13140/RG.2.2.13940.05761</arxiv:DOI>
      <dc:creator>Augusto Ciuffoletti, Letizia Chiti</dc:creator>
    </item>
    <item>
      <title>Are We There Yet? A Measurement Study of Efficiency for LLM Applications on Mobile Devices</title>
      <link>https://arxiv.org/abs/2504.00002</link>
      <description>arXiv:2504.00002v1 Announce Type: cross 
Abstract: Recent advancements in large language models (LLMs) have prompted interest in deploying these models on mobile devices to enable new applications without relying on cloud connectivity. However, the efficiency constraints of deploying LLMs on resource-limited devices present significant challenges. In this paper, we conduct a comprehensive measurement study to evaluate the efficiency tradeoffs between mobile-based, edge-based, and cloud-based deployments for LLM applications. We implement AutoLife-Lite, a simplified LLM-based application that analyzes smartphone sensor data to infer user location and activity contexts. Our experiments reveal that: (1) Only small-size LLMs (&lt;4B parameters) can run successfully on powerful mobile devices, though they exhibit quality limitations compared to larger models; (2) Model compression is effective in lower the hardware requirement, but may lead to significant performance degradation; (3) The latency to run LLMs on mobile devices with meaningful output is significant (&gt;30 seconds), while cloud services demonstrate better time efficiency (&lt;10 seconds); (4) Edge deployments offer intermediate tradeoffs between latency and model capabilities, with different results on CPU-based and GPU-based settings. These findings provide valuable insights for system designers on the current limitations and future directions for on-device LLM applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00002v1</guid>
      <category>cs.PF</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.NI</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiao Yan, Yi Ding</dc:creator>
    </item>
    <item>
      <title>Rack Position Optimization in Large-Scale Heterogeneous Data Centers</title>
      <link>https://arxiv.org/abs/2504.00277</link>
      <description>arXiv:2504.00277v1 Announce Type: cross 
Abstract: As rapidly growing AI computational demands accelerate the need for new hardware installation and maintenance, this work explores optimal data center resource management by balancing operational efficiency with fault tolerance through strategic rack positioning considering diverse resources and locations. Traditional mixed-integer programming (MIP) approaches often struggle with scalability, while heuristic methods may result in significant sub-optimality. To address these issues, this paper presents a novel two-tier optimization framework using a high-level deep reinforcement learning (DRL) model to guide a low-level gradient-based heuristic for local search. The high-level DRL agent employs Leader Reward for optimal rack type ordering, and the low-level heuristic efficiently maps racks to positions, minimizing movement counts and ensuring fault-tolerant resource distribution. This approach allows scalability to over 100,000 positions and 100 rack types. Our method outperformed the gradient-based heuristic by 7\% on average and the MIP solver by over 30\% in objective value. It achieved a 100\% success rate versus MIP's 97.5\% (within a 20-minute limit), completing in just 2 minutes compared to MIP's 1630 minutes (i.e., almost 4 orders of magnitude improvement). Unlike the MIP solver, which showed performance variability under time constraints and high penalties, our algorithm consistently delivered stable, efficient results - an essential feature for large-scale data center management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00277v1</guid>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>math.OC</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chang-Lin Chen, Jiayu Chen, Tian Lan, Zhaoxia Zhao, Hongbo Dong, Vaneet Aggarwal</dc:creator>
    </item>
    <item>
      <title>Towards a Decentralised Application-Centric Orchestration Framework in the Cloud-Edge Continuum</title>
      <link>https://arxiv.org/abs/2504.00761</link>
      <description>arXiv:2504.00761v1 Announce Type: cross 
Abstract: The efficient management of complex distributed applications in the Cloud-Edge continuum, including their deployment on heterogeneous computing resources and run-time operations, presents significant challenges. Resource management solutions -- also called orchestrators -- play a pivotal role by automating and managing tasks such as resource discovery, optimisation, application deployment, and lifecycle management, whilst ensuring the desired system performance. This paper introduces Swarmchestrate, a decentralised, application-centric orchestration framework inspired by the self-organising principles of Swarms. Swarmchestrate addresses the end-to-end management of distributed applications, from submission to optimal resource allocation across cloud and edge providers, as well as dynamic reconfiguration. Our initial findings include the implementation of the application deployment phase within a Cloud-Edge simulation environment, demonstrating the potential of Swarmchestrate. The results offer valuable insight into the coordination of resource offerings between various providers and optimised resource allocation, providing a foundation for designing scalable and efficient infrastructures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00761v1</guid>
      <category>cs.DC</category>
      <category>cs.NE</category>
      <category>cs.NI</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amjad Ullah, Andras Markus, Hac{\i} \.Ismail Aslan, Tamas Kiss, Jozsef Kovacs, James Deslauriers, Amy L. Murphy, Yiming Wang Odej Kao</dc:creator>
    </item>
    <item>
      <title>A Highly Scalable LLM Clusters with Optical Interconnect</title>
      <link>https://arxiv.org/abs/2411.01503</link>
      <description>arXiv:2411.01503v3 Announce Type: replace 
Abstract: We propose \emph{LumosCore} to build high-bandwidth and large-scale data center networks for LLM jobs. By replacing the core-layer electrical packet switches by optical circuit switches, \emph{LumosCore} could achieves $2\times$ increase in bandwidth or $8\times$ increase in network size. We offer the detailed design of \emph{LumosCore} at both deployment stage and running stage. At deployment stage, we propose Interleaved Wiring, which is compatible with all possible logical topologies. At running stage, we design polynomial-time algorithms for GPU placement, logical topology generating and OCS reconfiguration to minimize network contention and reduce impact to scheduled jobs. We evaluate \emph{LumosCore} using both testbed experiments and large-scale simulation. Compared to traditional hybrid optical/electrical architectures, \emph{LumosCore} increases the end-to-end training throughput by up to 39.5\% on a 128-node testbed. Compared to the state-of-art Clos architectures, \emph{LumosCore} reduces the average job completion time by up to 34.1\% in a 16k simulation platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01503v3</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinchi Han, Yongxi Lv, Shizhen Zhao, Zhuotao Liu, Ximeng Liu, Xinbing Wang</dc:creator>
    </item>
    <item>
      <title>Joint Beamforming and Trajectory Optimization for Multi-UAV-Assisted Integrated Sensing and Communication Systems</title>
      <link>https://arxiv.org/abs/2503.16915</link>
      <description>arXiv:2503.16915v2 Announce Type: replace 
Abstract: In this paper, we investigate beamforming design and trajectory optimization for a multi-unmanned aerial vehicle (UAV)-assisted integrated sensing and communication (ISAC) system. The proposed system employs multiple UAVs equipped with dual-functional radar-communication capabilities to simultaneously perform target sensing and provide communication services to users. We formulate a joint optimization problem that aims to maximize the sum rate of users while maintaining target sensing performance through coordinated beamforming and UAV trajectory design. To address this challenging non-convex problem, we develop a block coordinated descent (BCD)-based iterative algorithm that decomposes the original problem into tractable subproblems. Then, the beamforming design problem is addressed using fractional programming, while the UAV trajectory is refined through the deep deterministic policy gradient (DDPG) algorithm. The simulation results demonstrate that the proposed joint optimization approach achieves significant performance improvements in both communication throughput and sensing accuracy compared to conventional, separated designs. We also show that proper coordination of multiple UAVs through optimized trajectories and beamforming patterns can effectively balance the tradeoff between sensing and communication objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16915v2</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Kyaw Tun, Nway Nway Ei, Sheikh Salman Hassan, Cedomir Stefanovic, Nguyen Van Huynh, Madyan Alsenwi, Choong Seon Hong</dc:creator>
    </item>
    <item>
      <title>DT-DDNN: A Physical Layer Security Attack Detector in 5G RF Domain for CAVs</title>
      <link>https://arxiv.org/abs/2403.02645</link>
      <description>arXiv:2403.02645v3 Announce Type: replace-cross 
Abstract: The Synchronization Signal Block (SSB) is a fundamental component of the 5G New Radio (NR) air interface, crucial for the initial access procedure of Connected and Automated Vehicles (CAVs), and serves several key purposes in the network's operation. However, due to the predictable nature of SSB transmission, including the Primary and Secondary Synchronization Signals (PSS and SSS), jamming attacks are critical threats. These attacks, which can be executed without requiring high power or complex equipment, pose substantial risks to the 5G network, particularly as a result of the unencrypted transmission of control signals. Leveraging RF domain knowledge, this work presents a novel deep learning-based technique for detecting jammers in CAV networks. Unlike the existing jamming detection algorithms that mostly rely on network parameters, we introduce a double-threshold deep learning jamming detector by focusing on the SSB. The detection method is focused on RF domain features and improves the robustness of the network without requiring integration with the pre-existing network infrastructure. By integrating a preprocessing block to extract PSS correlation and energy per null resource elements (EPNRE) characteristics, our method distinguishes between normal and jammed received signals with high precision. Additionally, by incorporating of Discrete Wavelet Transform (DWT), the efficacy of training and detection are optimized. A double-threshold double Deep Neural Network (DT-DDNN) is also introduced to the architecture complemented by a deep cascade learning model to increase the sensitivity of the model to variations of signal-to-jamming noise ratio (SJNR). Results show that the proposed method achieves 96.4% detection rate in extra low jamming power, i.e., SJNR between 15 to 30 dB. Further, performance of DT-DDNN is validated by analyzing real 5G signals obtained from a practical testbed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02645v3</guid>
      <category>eess.SP</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ghazal Asemian, Mohammadreza Amini, Burak Kantarci, Melike Erol-Kantarci</dc:creator>
    </item>
    <item>
      <title>RG-Attn: Radian Glue Attention for Multi-modality Multi-agent Cooperative Perception</title>
      <link>https://arxiv.org/abs/2501.16803</link>
      <description>arXiv:2501.16803v2 Announce Type: replace-cross 
Abstract: Cooperative perception offers an optimal solution to overcome the perception limitations of single-agent systems by leveraging Vehicle-to-Everything (V2X) communication for data sharing and fusion across multiple agents. However, most existing approaches focus on single-modality data exchange, limiting the potential of both homogeneous and heterogeneous fusion across agents. This overlooks the opportunity to utilize multi-modality data per agent, restricting the system's performance. In the automotive industry, manufacturers adopt diverse sensor configurations, resulting in heterogeneous combinations of sensor modalities across agents. To harness the potential of every possible data source for optimal performance, we design a robust LiDAR and camera cross-modality fusion module, Radian-Glue-Attention (RG-Attn), applicable to both intra-agent cross-modality fusion and inter-agent cross-modality fusion scenarios, owing to the convenient coordinate conversion by transformation matrix and the unified sampling/inversion mechanism. We also propose two different architectures, named Paint-To-Puzzle (PTP) and Co-Sketching-Co-Coloring (CoS-CoCo), for conducting cooperative perception. PTP aims for maximum precision performance and achieves smaller data packet size by limiting cross-agent fusion to a single instance, but requiring all participants to be equipped with LiDAR. In contrast, CoS-CoCo supports agents with any configuration-LiDAR-only, camera-only, or LiDAR-camera-both, presenting more generalization ability. Our approach achieves state-of-the-art (SOTA) performance on both real and simulated cooperative perception datasets. The code is now available at GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16803v2</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <category>cs.NI</category>
      <category>eess.IV</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lantao Li, Kang Yang, Wenqi Zhang, Xiaoxue Wang, Chen Sun</dc:creator>
    </item>
  </channel>
</rss>
