<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Dec 2025 05:02:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Complementary strengths of the Neyman-Rubin and graphical causal frameworks</title>
      <link>https://arxiv.org/abs/2512.09130</link>
      <description>arXiv:2512.09130v1 Announce Type: new 
Abstract: This article contributes to the discussion on the relationship between the Neyman-Rubin and the graphical frameworks for causal inference. We present specific examples of data-generating mechanisms - such as those involving undirected or deterministic relationships and cycles - where analyses using a directed acyclic graph are challenging, but where the tools from the Neyman-Rubin causal framework are readily applicable. We also provide examples of data-generating mechanisms with M-bias, trapdoor variables, and complex front-door structures, where the application of the Neyman-Rubin approach is complicated, but the graphical approach is directly usable. The examples offer insights into commonly used causal inference frameworks and aim to improve comprehension of the languages for causal reasoning among a broad audience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09130v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tetiana Gorbach, Xavier de Luna, Juha Karvanen, Ingeborg Waernbaum</dc:creator>
    </item>
    <item>
      <title>IntegralGP: Volumetric estimation of subterranean geochemical properties in mineral deposits by fusing assay data with different spatial supports</title>
      <link>https://arxiv.org/abs/2512.09151</link>
      <description>arXiv:2512.09151v1 Announce Type: new 
Abstract: This article presents an Integral Gaussian Process (IntegralGP) framework for volumetric estimation of subterranean properties in mineral deposits. It provides a unified representation for data with different spatial supports, which enables blasthole geochemical assays to be properly modelled as interval observations rather than points. This approach is shown to improve regression performance and boundary delineation. A core contribution is a description of the mathematical changes to the covariance expressions which allow these benefits to be realised. The gradient and anti-derivatives are obtained to facilitate learning of the kernel hyperparameters. Numerical stability issues are also discussed. To illustrate its application, an IntegralGP data fusion algorithm is described. The objective is to assimilate line-based blasthole assays and update a block model that provides long-range prediction of Fe concentration beneath the drilled bench. Heteroscedastic GP is used to fuse chemically compatible but spatially incongruous data with different resolutions and sample spacings. Domain knowledge embodied in the structure and empirical distribution of the block model must be generally preserved while local inaccuracies are corrected. Using validation measurements within the predicted bench, our experiments demonstrate an improvement in bench-below grade prediction performance. For material classification, IntegralGP fusion reduces the absolute error and model bias in categorical prediction, especially instances where waste blocks are mistakenly classified as high-grade.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09151v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Expert Systems with Applications 298A (2026) 129429</arxiv:journal_reference>
      <dc:creator>Anna Chlingaryan, Arman Melkumyan, Raymond Leung</dc:creator>
    </item>
    <item>
      <title>Prenatal alcohol exposure and child cognition: semi-continuous exposures, causal inference and evidence synthesis</title>
      <link>https://arxiv.org/abs/2512.09237</link>
      <description>arXiv:2512.09237v1 Announce Type: new 
Abstract: We address the challenge of causal inference status and the dose-response effects with a semi-continuous exposure. A two-stage approach is proposed using estimating equation for multiple outcomes with large sample properties derived for the resulting estimators. Homogeneity tests are developed to assess whether causal effects of exposure status and the dose-response effects are the same across multiple outcomes. A global homogeneity test is also developed to assess whether the effect of exposure status (exposed/not exposed) and the dose-response effect of the continuous exposure level are each equal across all outcomes. The methods of estimation and testing are rigorously evaluated in simulation studies and applied to a motivating study on the effects of prenatal alcohol exposure on childhood cognition defined by executive function (EF), academic achievement in math, and learning and memory (LM).</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09237v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoya Wang, Richard J. Cook, Yeying Zhu, Tugba Akkaya-Hocagil, R. Colin Carter, Sandra W. Jacobson, Joseph L. Jacobson, Louise M. Ryan</dc:creator>
    </item>
    <item>
      <title>MoDaH achieves rate optimal batch correction</title>
      <link>https://arxiv.org/abs/2512.09259</link>
      <description>arXiv:2512.09259v1 Announce Type: new 
Abstract: Batch effects pose a significant challenge in the analysis of single-cell omics data, introducing technical artifacts that confound biological signals. While various computational methods have achieved empirical success in correcting these effects, they lack the formal theoretical guarantees required to assess their reliability and generalization. To bridge this gap, we introduce Mixture-Model-based Data Harmonization (MoDaH), a principled batch correction algorithm grounded in a rigorous statistical framework.
  Under a new Gaussian-mixture-model with explicit parametrization of batch effects, we establish the minimax optimal error rates for batch correction and prove that MoDaH achieves this rate by leveraging the recent theoretical advances in clustering data from anisotropic Gaussian mixtures. This constitutes, to the best of our knowledge, the first theoretical guarantee for batch correction. Extensive experiments on diverse single-cell RNA-seq and spatial proteomics datasets demonstrate that MoDaH not only attains theoretical optimality but also achieves empirical performance comparable to or even surpassing those of state-of-the-art heuristics (e.g., Harmony, Seurat-V5, and LIGER), effectively balancing the removal of technical noise with the conservation of biological signal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09259v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>q-bio.GN</category>
      <category>stat.TH</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Cao, Zongming Ma</dc:creator>
    </item>
    <item>
      <title>Vaccine sieve analysis on deep sequencing data using competing risks Cox regression with failure type subject to misclassification</title>
      <link>https://arxiv.org/abs/2512.09262</link>
      <description>arXiv:2512.09262v1 Announce Type: new 
Abstract: Understanding how vaccines perform against different pathogen genotypes is crucial for developing effective prevention strategies, particularly for highly genetically diverse pathogens like HIV. Sieve analysis is a statistical framework used to determine whether a vaccine selectively prevents acquisition of certain genotypes while allowing breakthrough of other genotypes that evade immune responses. Traditionally, these analyses are conducted with a single sequence available per individual acquiring the pathogen. However, modern sequencing technology can provide detailed characterization of intra-individual viral diversity by capturing up to hundreds of pathogen sequences per person. In this work, we introduce methodology that extends sieve analysis to account for intra-individual viral diversity. Our approach estimates vaccine efficacy against viral populations with varying true (unobservable) frequencies of vaccine-mismatched mutations. To account for differential resolution of information from differing sequence counts per person, we use competing risks Cox regression with modeled causes of failure and propose an empirical Bayes approach for the classification model. Simulation studies demonstrate that our approach reduces bias, provides nominal confidence interval coverage, and improves statistical power compared to conventional methods. We apply our method to the HVTN 705 Imbokodo trial, which assessed the efficacy of a heterologous vaccine regimen in preventing HIV-1 acquisition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09262v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Peng, Michal Juraska, Pamela A. Shaw, Peter B. Gilbert</dc:creator>
    </item>
    <item>
      <title>On the inverse of covariance matrices for unbalanced crossed designs</title>
      <link>https://arxiv.org/abs/2512.09273</link>
      <description>arXiv:2512.09273v1 Announce Type: new 
Abstract: This paper addresses a long-standing open problem in the analysis of linear mixed models with crossed random effects under unbalanced designs: how to find an analytic expression for the inverse of $\mathbf{V}$, the covariance matrix of the observed response. The inverse matrix $\mathbf{V}^{-1}$ is required for likelihood-based estimation and inference. However, for unbalanced crossed designs, $\mathbf{V}$ is dense and the lack of a closed-form representation for $\mathbf{V}^{-1}$, until now, has made using likelihood-based methods computationally challenging and difficult to analyse mathematically. We use the Khatri--Rao product to represent $\mathbf{V}$ and then to construct a modified covariance matrix whose inverse admits an exact spectral decomposition. Building on this construction, we obtain an elegant and simple approximation to $\mathbf{V}^{-1}$ for asymptotic unbalanced designs. For non-asymptotic settings, we derive an accurate and interpretable approximation under mildly unbalanced data and establish an exact inverse representation as a low-rank correction to this approximation, applicable to arbitrary degrees of unbalance. Simulation studies demonstrate the accuracy, stability, and computational tractability of the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09273v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyang Lyu, S. A. Sisson, A. H. Welsh</dc:creator>
    </item>
    <item>
      <title>Balancing Weights for Causal Mediation Analysis</title>
      <link>https://arxiv.org/abs/2512.09337</link>
      <description>arXiv:2512.09337v1 Announce Type: new 
Abstract: This paper develops methods for estimating the natural direct and indirect effects in causal mediation analysis. The efficient influence function-based estimator (EIF-based estimator) and the inverse probability weighting estimator (IPW estimator), which are standard in causal mediation analysis, both rely on the inverse of the estimated propensity scores, and thus they are vulnerable to two key issues (i) instability and (ii) finite-sample covariate imbalance. We propose estimators based on the weights obtained by an algorithm that directly penalizes weight dispersion while enforcing approximate covariate and mediator balance, thereby improving stability and mitigating bias in finite samples. We establish the convergence rates of the proposed weights and show that the resulting estimators are asymptotically normal and achieve the semiparametric efficiency bound. Monte Carlo simulations demonstrate that the proposed estimator outperforms not only the EIF-based estimator and the IPW estimator but also the regression imputation estimator in challenging scenarios with model misspecification. Furthermore, the proposed method is applied to a real dataset from a study examining the effects of media framing on immigration attitudes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09337v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kentaro Kawato</dc:creator>
    </item>
    <item>
      <title>Model-robust Inference for Seamless II/III Trials with Covariate Adaptive Randomization</title>
      <link>https://arxiv.org/abs/2512.09430</link>
      <description>arXiv:2512.09430v1 Announce Type: new 
Abstract: Seamless phase II/III trials have become a cornerstone of modern drug development, offering a means to accelerate evaluation while maintaining statistical rigor. However, most existing inference procedures are model-based, designed primarily for continuous outcomes, and often neglect the stratification used in covariate-adaptive randomization (CAR), limiting their practical relevance. In this paper, we propose a unified, model-robust framework for seamless phase II/III trials grounded in generalized linear models (GLMs), enabling valid inference across diverse outcome types, estimands, and CAR schemes. Using Z-estimation, we derive the asymptotic properties of treatment effect estimators and explicitly characterize how their variance depends on the underlying randomization procedure.Based on these results, we develop adjusted Wald tests that, together with Dunnett's multiple-comparison procedure and the inverse chi-square combination method, ensure valid overall Type I error. Extensive simulation studies and a trial example demonstrate that the proposed model-robust tests achieve superior power and reliable inference compared to conventional approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09430v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun Yi, Lucy Xia</dc:creator>
    </item>
    <item>
      <title>Multiply-robust Estimator of Cumulative Incidence Function Difference for Right-Censored Competing Risks Data</title>
      <link>https://arxiv.org/abs/2512.09433</link>
      <description>arXiv:2512.09433v1 Announce Type: new 
Abstract: In causal inference, estimating the average treatment effect is a central objective, and in the context of competing risks data, this effect can be quantified by the cause-specific cumulative incidence function (CIF) difference. While doubly robust estimators give a more robust way to estimate the causal effect from the observational study, they remain inconsistent if both models are misspecified. To improve the robustness, we develop a multiply robust estimator for the difference in cause-specific CIFs using right-censored competing risks data. The proposed framework integrates the pseudo-value approach, which transforms the censored, time-dependent CIF into a complete-data outcome, with the multiply robust estimation framework. By specifying multiple candidate models for both the propensity score and the outcome regression, the resulting estimator is consistent and asymptotically unbiased, provided that at least one of the multiple propensity score or outcome regression models is correctly specified. Simulation studies show our multiply robust estimator remains virtually unbiased and maintains nominal coverage rates under various model misspecification scenarios and a wide range of choices for the censoring rate. Finally, the proposed multiply robust model is illustrated using the Right Heart Catheterization dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09433v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifei Tian, Ying Wu</dc:creator>
    </item>
    <item>
      <title>Calibration with Bagging of the Principal Components on a Large Number of Auxiliary Variables</title>
      <link>https://arxiv.org/abs/2512.09505</link>
      <description>arXiv:2512.09505v1 Announce Type: new 
Abstract: Calibration is a widely used method in survey sampling to adjust weights so that estimated totals of some chosen calibration variables match known population totals or totals obtained from other sources. When a large number of auxiliary variables are included as calibration variables, the variance of the total estimator can increase, and the calibration weights can become highly dispersed. To address these issues, we propose a solution inspired by bagging and principal component decomposition. With our approach, the principal components of the auxiliary variables are constructed. Several samples of calibration variables are selected without replacement and with unequal probabilities from among the principal components. For each sample, a system of weights is obtained. The final weights are the average weights of these different weighting systems. With our proposed method, it is possible to calibrate exactly for some of the main auxiliary variables. For the other auxiliary variables, the weights cannot be calibrated exactly. The proposed method allows us to obtain a total estimator whose variance does not explode when new auxiliary variables are added and to obtain very low scatter weights. Finally, our proposed method allows us to obtain a single weighting system that can be applied to several variables of interest of a survey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09505v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Caren Hasler, Arnaud Tripet, Yves Till\'e</dc:creator>
    </item>
    <item>
      <title>A Bayesian Approach for Robust Longitudinal Envelope Models</title>
      <link>https://arxiv.org/abs/2512.09553</link>
      <description>arXiv:2512.09553v1 Announce Type: new 
Abstract: The envelope model provides a dimension-reduction framework for multivariate linear regression. However, existing envelope methods typically assume normally distributed random errors and do not accommodate repeated measures in longitudinal studies. To address these limitations, we propose the robust longitudinal envelope model (RoLEM). RoLEM employs a scale mixture of matrix-variate normal distributions to model random errors, allowing it to handle potential outliers, and incorporates flexible correlation structures for repeated measurements. In addition, we introduce new prior and proposal distributions on the Grassmann manifold to facilitate Bayesian inference for RoLEM. Simulation studies and real data analysis demonstrate the superior performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09553v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peng Zeng, Yushan Mu</dc:creator>
    </item>
    <item>
      <title>Uniform-over-dimension location tests for multivariate and high-dimensional data</title>
      <link>https://arxiv.org/abs/2512.09659</link>
      <description>arXiv:2512.09659v1 Announce Type: new 
Abstract: Asymptotic methods for hypothesis testing in high-dimensional data usually require the dimension of the observations to increase to infinity, often with an additional relationship between the dimension (say, $p$) and the sample size (say, $n$). On the other hand, multivariate asymptotic testing methods are valid for fixed dimension only and their implementations typically require the sample size to be large compared to the dimension to yield desirable results. In practical scenarios, it is usually not possible to determine whether the dimension of the data conform to the conditions required for the validity of the high-dimensional asymptotic methods for hypothesis testing, or whether the sample size is large enough compared to the dimension of the data. In this work, we first describe the notion of uniform-over-$p$ convergences and subsequently, develop a uniform-over-dimension central limit theorem. An asymptotic test for the two-sample equality of locations is developed, which now holds uniformly over the dimension of the observations. Using simulated and real data, it is demonstrated that the proposed test exhibits better performance compared to several popular tests in the literature for high-dimensional data as well as the usual scaled two-sample tests for multivariate data, including the Hotelling's $T^2$ test for multivariate Gaussian data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09659v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ritabrata Karmakar, Joydeep Chowdhury, Subhajit Dutta, Marc G. Genton</dc:creator>
    </item>
    <item>
      <title>RECAP Framework v1.0: A Multi-Layer Inheritance Architecture for Evidence Synthesis</title>
      <link>https://arxiv.org/abs/2512.09821</link>
      <description>arXiv:2512.09821v1 Announce Type: new 
Abstract: Evidence synthesis has advanced through improved reporting standards, bias assessment tools, and analytic methods, but current workflows remain limited by a single-layer structure in which conceptual, methodological, and procedural decisions are made on the same level. This forces each project to rebuild its methodological foundations from scratch, leading to inconsistencies, conceptual drift, and unstable reasoning across projects. RECAP Framework v1.0 introduces a three-layer meta-architecture consisting of methodological laws (Grandparent), domain-level abstractions (Parent), and project-level implementations (Child). The framework defines an inheritance system with strict rules for tiering, routing, and contamination control to preserve construct clarity, enforce inferential discipline, and support reproducibility across multi-project evidence ecosystems. RECAP provides a formal governance layer for evidence synthesis and establishes the foundation for a methodological lineage designed to stabilize reasoning across research programs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09821v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hung Kuan Lee</dc:creator>
    </item>
    <item>
      <title>Predictor-Informed Bayesian Nonparametric Clustering</title>
      <link>https://arxiv.org/abs/2512.09826</link>
      <description>arXiv:2512.09826v1 Announce Type: new 
Abstract: In this project we are interested in performing clustering of observations such that the cluster membership is influenced by a set of predictors. To that end, we employ the Bayesian nonparameteric Common Atoms Model, which is a nested clustering algorithm that utilizes a (fixed) group membership for each observation to encourage more similar clustering of members of the same group. CAM operates by assuming each group has its own vector of cluster probabilities, which are themselves clustered to allow similar clustering for some groups. We extend this approach by treating the group membership as an unknown latent variable determined as a flexible nonparametric form of the covariate vector. Consequently, observations with similar predictor values will be in the same latent group and are more likely to be clustered together than observations with disparate predictors. We propose a pyramid group model that flexibly partitions the predictor space into these latent group memberships. This pyramid model operates similarly to a Bayesian regression tree process except that it uses the same splitting rule for at all nodes at the same tree depth which facilitates improved mixing. We outline a block Gibbs sampler to perform posterior inference from our model. Our methodology is demonstrated in simulation and real data examples. In the real data application, we utilize the RAND Health and Retirement Study to cluster and predict patient outcomes in terms of the number of overnight hospital stays.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09826v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Md Yasin Ali Parh, Jeremy T. Gaskins</dc:creator>
    </item>
    <item>
      <title>Causal Attribution of Model Performance Gaps in Medical Imaging Under Distribution Shifts</title>
      <link>https://arxiv.org/abs/2512.09094</link>
      <description>arXiv:2512.09094v1 Announce Type: cross 
Abstract: Deep learning models for medical image segmentation suffer significant performance drops due to distribution shifts, but the causal mechanisms behind these drops remain poorly understood. We extend causal attribution frameworks to high-dimensional segmentation tasks, quantifying how acquisition protocols and annotation variability independently contribute to performance degradation. We model the data-generating process through a causal graph and employ Shapley values to fairly attribute performance changes to individual mechanisms. Our framework addresses unique challenges in medical imaging: high-dimensional outputs, limited samples, and complex mechanism interactions. Validation on multiple sclerosis (MS) lesion segmentation across 4 centers and 7 annotators reveals context-dependent failure modes: annotation protocol shifts dominate when crossing annotators (7.4% $\pm$ 8.9% DSC attribution), while acquisition shifts dominate when crossing imaging centers (6.5% $\pm$ 9.1%). This mechanism-specific quantification enables practitioners to prioritize targeted interventions based on deployment context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09094v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pedro M. Gordaliza, Nataliia Molchanova, Jaume Banus, Thomas Sanchez, Meritxell Bach Cuadra</dc:creator>
    </item>
    <item>
      <title>WTNN: Weibull-Tailored Neural Networks for survival analysis</title>
      <link>https://arxiv.org/abs/2512.09163</link>
      <description>arXiv:2512.09163v1 Announce Type: cross 
Abstract: The Weibull distribution is a commonly adopted choice for modeling the survival of systems subject to maintenance over time. When only proxy indicators and censored observations are available, it becomes necessary to express the distribution's parameters as functions of time-dependent covariates. Deep neural networks provide the flexibility needed to learn complex relationships between these covariates and operational lifetime, thereby extending the capabilities of traditional regression-based models. Motivated by the analysis of a fleet of military vehicles operating in highly variable and demanding environments, as well as by the limitations observed in existing methodologies, this paper introduces WTNN, a new neural network-based modeling framework specifically designed for Weibull survival studies. The proposed architecture is specifically designed to incorporate qualitative prior knowledge regarding the most influential covariates, in a manner consistent with the shape and structure of the Weibull distribution. Through numerical experiments, we show that this approach can be reliably trained on proxy and right-censored data, and is capable of producing robust and interpretable survival predictions that can improve existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09163v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabrielle Rives, Olivier Lopez, Nicolas Bousquet</dc:creator>
    </item>
    <item>
      <title>Debiased Bayesian Inference for High-dimensional Regression Models</title>
      <link>https://arxiv.org/abs/2512.09257</link>
      <description>arXiv:2512.09257v1 Announce Type: cross 
Abstract: There has been significant progress in Bayesian inference based on sparsity-inducing (e.g., spike-and-slab and horseshoe-type) priors for high-dimensional regression models. The resulting posteriors, however, in general do not possess desirable frequentist properties, and the credible sets thus cannot serve as valid confidence sets even asymptotically. We introduce a novel debiasing approach that corrects the bias for the entire Bayesian posterior distribution. We establish a new Bernstein-von Mises theorem that guarantees the frequentist validity of the debiased posterior. We demonstrate the practical performance of our proposal through Monte Carlo simulations and two empirical applications in economics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09257v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qihui Chen, Zheng Fang, Ruixuan Liu</dc:creator>
    </item>
    <item>
      <title>Self-Supervised Learning with Gaussian Processes</title>
      <link>https://arxiv.org/abs/2512.09322</link>
      <description>arXiv:2512.09322v1 Announce Type: cross 
Abstract: Self supervised learning (SSL) is a machine learning paradigm where models learn to understand the underlying structure of data without explicit supervision from labeled samples. The acquired representations from SSL have demonstrated useful for many downstream tasks including clustering, and linear classification, etc. To ensure smoothness of the representation space, most SSL methods rely on the ability to generate pairs of observations that are similar to a given instance. However, generating these pairs may be challenging for many types of data. Moreover, these methods lack consideration of uncertainty quantification and can perform poorly in out-of-sample prediction settings. To address these limitations, we propose Gaussian process self supervised learning (GPSSL), a novel approach that utilizes Gaussian processes (GP) models on representation learning. GP priors are imposed on the representations, and we obtain a generalized Bayesian posterior minimizing a loss function that encourages informative representations. The covariance function inherent in GPs naturally pulls representations of similar units together, serving as an alternative to using explicitly defined positive samples. We show that GPSSL is closely related to both kernel PCA and VICReg, a popular neural network-based SSL method, but unlike both allows for posterior uncertainties that can be propagated to downstream tasks. Experiments on various datasets, considering classification and regression tasks, demonstrate that GPSSL outperforms traditional methods in terms of accuracy, uncertainty quantification, and error control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09322v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yunshan Duan, Sinead Williamson</dc:creator>
    </item>
    <item>
      <title>CFLight: Enhancing Safety with Traffic Signal Control through Counterfactual Learning</title>
      <link>https://arxiv.org/abs/2512.09368</link>
      <description>arXiv:2512.09368v1 Announce Type: cross 
Abstract: Traffic accidents result in millions of injuries and fatalities globally, with a significant number occurring at intersections each year. Traffic Signal Control (TSC) is an effective strategy for enhancing safety at these urban junctures. Despite the growing popularity of Reinforcement Learning (RL) methods in optimizing TSC, these methods often prioritize driving efficiency over safety, thus failing to address the critical balance between these two aspects. Additionally, these methods usually need more interpretability. CounterFactual (CF) learning is a promising approach for various causal analysis fields. In this study, we introduce a novel framework to improve RL for safety aspects in TSC. This framework introduces a novel method based on CF learning to address the question: ``What if, when an unsafe event occurs, we backtrack to perform alternative actions, and will this unsafe event still occur in the subsequent period?'' To answer this question, we propose a new structure causal model to predict the result after executing different actions, and we propose a new CF module that integrates with additional ``X'' modules to promote safe RL practices. Our new algorithm, CFLight, which is derived from this framework, effectively tackles challenging safety events and significantly improves safety at intersections through a near-zero collision control strategy. Through extensive numerical experiments on both real-world and synthetic datasets, we demonstrate that CFLight reduces collisions and improves overall traffic performance compared to conventional RL methods and the recent safe RL model. Moreover, our method represents a generalized and safe framework for RL methods, opening possibilities for applications in other domains. The data and code are available in the github https://github.com/MJLee00/CFLight-Enhancing-Safety-with-Traffic-Signal-Control-through-Counterfactual-Learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09368v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mingyuan Li, Chunyu Liu, Zhuojun Li, Xiao Liu, Guangsheng Yu, Bo Du, Jun Shen, Qiang Wu</dc:creator>
    </item>
    <item>
      <title>Bayesian Model Selection with an Application to Cosmology</title>
      <link>https://arxiv.org/abs/2512.09724</link>
      <description>arXiv:2512.09724v1 Announce Type: cross 
Abstract: We investigate cosmological parameter inference and model selection from a Bayesian perspective. Type Ia supernova data from the Dark Energy Survey (DES-SN5YR) are used to test the \(\Lambda\)CDM, \(w\)CDM, and CPL cosmological models. Posterior inference is performed via Hamiltonian Monte Carlo using the No-U-Turn Sampler (NUTS) implemented in NumPyro and analyzed with ArviZ in Python. Bayesian model comparison is conducted through Bayes factors computed using the \texttt{bridgesampling} library in R. The results indicate that all three models demonstrate similar predictive performance, but \(w\)CDM shows stronger evidence relative to \(\Lambda\)CDM and CPL. We conclude that, under the assumptions and data used in this study, \(w\)CDM provides a better description of cosmological expansion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09724v1</guid>
      <category>stat.AP</category>
      <category>astro-ph.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikoloz Gigiberia</dc:creator>
    </item>
    <item>
      <title>Network Meta Analysis of Mean Survival</title>
      <link>https://arxiv.org/abs/2512.09732</link>
      <description>arXiv:2512.09732v1 Announce Type: cross 
Abstract: Decisions based upon pairwise comparisons of multiple treatments are naturally performed in terms of the mean survival of the selected study arms or functions thereof. However, synthesis of treatment comparisons is usually performed on surrogates of the mean survival, such as hazard ratios or restricted mean survival times. Thus, network meta-analysis techniques may suffer from the limitations of these approaches, such as incorrect proportional hazards assumption or short-term follow-up periods. We propose a Bayesian framework for the network meta-analysis of the main outcome informing the decision, the mean survival of a treatment. Its derivation involves extrapolation of the observed survival curves. We use methods for stable extrapolation that integrate long term evidence based upon mortality projections. Extrapolations are performed using flexible poly-hazard parametric models and M-spline-based methods. We assess the computational and statistical efficiency of different techniques using a simulation study and apply the developed methods to two real data sets. The proposed method is formulated within a decision theoretic framework for cost-effectiveness analyses, where the `best' treatment is to be selected and incorporating the associated cost information is straightforward.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09732v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anastasios Apsemidis, Dimitris Mavridis, Nikolaos Demiris</dc:creator>
    </item>
    <item>
      <title>Detecting and Localizing Anomalous Cliques in Inhomogeneous Networks using Egonets</title>
      <link>https://arxiv.org/abs/1807.08925</link>
      <description>arXiv:1807.08925v3 Announce Type: replace 
Abstract: Cliques, or fully connected subgraphs, are among the most important and well-studied graph motifs in network science. We consider the problem of finding a statisti- cally anomalous clique hidden in a large network. There are two parts to this problem: (1) detection, i.e., determining whether an anomalous clique is present, and (2) localization, i.e., determining which vertices of the network constitute the detected clique. While this problem has been extensively studied under the homogeneous Erdos-Renyi model, little progress has been made beyond this simple setting, and no existing method can perform detection and localization in inhomogeneous networks within finite time. To address this gap, we first show that in homogeneous networks, the anomalousness of a clique depends solely on its size. This property does not carry over to inhomogeneous networks, where the identity of the vertices forming the clique plays a critical role, and a smaller clique can be more anomalous than a larger one. Building on this insight, we propose a unified method for clique detection and localization based on a class of subgraphs called egonets. The proposed method generalizes to a wide variety of inhomogeneous network models and is naturally amenable to parallel computing. We establish the theoretical properties of the proposed method and demonstrate its empirical performance through simulation studies and application to two real world networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:1807.08925v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhankar Bhadra, Srijan Sengupta</dc:creator>
    </item>
    <item>
      <title>Quasi Model-Assisted Estimators under Nonresponse in Sample Surveys</title>
      <link>https://arxiv.org/abs/2208.04621</link>
      <description>arXiv:2208.04621v2 Announce Type: replace 
Abstract: In the presence of auxiliary information, model-assisted estimators rely on a working model linking the variable of interest to the auxiliary variables in order to improve the efficiency of the Horvitz-Thompson estimator. Model-assisted estimators cannot be directly computed with nonresponse since the values of the variable of interest is missing for a part of the sample units. In this article, we present and study a class of quasi-model-assisted estimators that extend model-assisted estimators to settings with non-ignorable nonresponse. These estimators combine a working model and a response model. The former is used to improve the efficiency, the latter to reweight the nonrespondents. A wide range of statistical learning methods can be used to estimate either of these models. We show that several well-known existing estimators are particular cases of quasi-model-assisted estimators. We examine the behavior of these estimators through a simulation study. The results illustrate how these estimators remain competitive in terms of bias and variance when one of the two models is poorly specified.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.04621v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Caren Hasler, Esther Eustache</dc:creator>
    </item>
    <item>
      <title>High-dimensional Newey-Powell Test Via Approximate Message Passing</title>
      <link>https://arxiv.org/abs/2311.05056</link>
      <description>arXiv:2311.05056v2 Announce Type: replace 
Abstract: We propose a high-dimensional extension of the heteroscedasticity test proposed in Newey and Powell (1987). Our test is based on expectile regression in the proportional asymptotic regime where n/p \to \delta \in (0,1]. The asymptotic analysis of the test statistic uses the Approximate Message Passing (AMP) algorithm, from which we obtain the limiting distribution of the test and establish its asymptotic power. The numerical performance of the test is validated through an extensive simulation study. As real-data applications, we present the analysis based on ``international economic growth" data (Belloni et al., 2011), which is found to be homoscedastic, and ``supermarket" data (Lan et al., 2016), which is found to be heteroscedastic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05056v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jing Zhou, Hui Zou</dc:creator>
    </item>
    <item>
      <title>Sampling from density power divergence-based generalized posterior distribution via stochastic optimization</title>
      <link>https://arxiv.org/abs/2501.07790</link>
      <description>arXiv:2501.07790v2 Announce Type: replace 
Abstract: Robust Bayesian inference using density power divergence (DPD) has emerged as a promising approach for handling outliers in statistical estimation. Although the DPD-based posterior offers theoretical guarantees of robustness, its practical implementation faces significant computational challenges, particularly for general parametric models with intractable integral terms. These challenges are specifically pronounced in high-dimensional settings, where traditional numerical integration methods are inadequate and computationally expensive. Herein, we propose a novel {approximate} sampling methodology that addresses these limitations by integrating the loss-likelihood bootstrap with a stochastic gradient descent algorithm specifically designed for DPD-based estimation. Our approach enables efficient and scalable sampling from DPD-based posteriors for a broad class of parametric models, including those with intractable integrals. We further extend it to accommodate generalized linear models. Through comprehensive simulation studies, we demonstrate that our method efficiently samples from DPD-based posteriors, offering superior computational scalability compared to conventional methods, specifically in high-dimensional settings. The results also highlight its ability to handle complex parametric models with intractable integral terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07790v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naruki Sonobe, Tomotaka Momozaki, Tomoyuki Nakagawa</dc:creator>
    </item>
    <item>
      <title>Estimation of Treatment Effects based on Kernel Matching</title>
      <link>https://arxiv.org/abs/2502.10958</link>
      <description>arXiv:2502.10958v2 Announce Type: replace 
Abstract: Kernel matching is a widely used technique for estimating treatment effects, particularly valuable in observational studies where randomized controlled trials are not feasible. While kernel-matching approaches have demonstrated practical advantages in exploiting similarities between treated and control units, their theoretical properties have remained only partially explored. In this paper, we make a key contribution by establishing the asymptotic normality and consistency of kernel-matching estimators for both the average treatment effect (ATE) and the average treatment effect on the treated (ATT) through influence function techniques, thereby providing a rigorous theoretical foundation for their use in causal inference. Furthermore, we derive the asymptotic distributions of the ATE and ATT estimators when the propensity score is estimated rather than known, extending the theoretical guarantees to the practically relevant cases. Through extensive Monte Carlo simulations, the estimators exhibit consistently improved performance over standard treatment-effect estimators. We further illustrate the method by analyzing the National Supported Work Demonstration job-training data with the kernel-matching estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10958v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chong Ding, Zheng Li, Hon Keung Tony Ng, Wei Gao</dc:creator>
    </item>
    <item>
      <title>Exact identifiability analysis for a class of partially observed near-linear stochastic differential equation models</title>
      <link>https://arxiv.org/abs/2503.19241</link>
      <description>arXiv:2503.19241v3 Announce Type: replace 
Abstract: Stochasticity plays a key role in many biological systems, necessitating the calibration of stochastic mathematical models to interpret associated data. For model parameters to be estimated reliably, it is typically the case that they must be structurally identifiable. Yet, while theory underlying structural identifiability analysis for deterministic differential equation models is highly developed, there are currently no tools for the general assessment of stochastic models. In this work, we present a differential algebra-based framework for the structural identifiability analysis of linear and a class of near-linear partially observed stochastic differential equation (SDE) models. Our framework is based on a deterministic recurrence relation that describes the dynamics of the statistical moments of the system of SDEs. From this relation, we iteratively form a series of necessarily satisfied equations involving only the observed moments, from which we are able to establish structurally identifiable parameter combinations. We demonstrate our framework for a suite of linear (two- and $n$-dimensional) and non-linear (two-dimensional) models. Most importantly, we define the notion of structural identifiability for SDE models and establish the effect of the initial condition on identifiability. We conclude with a discussion on the applicability and limitations of our approach, and potential future research directions in this understudied area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19241v3</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander P Browning, Michael J Chappell, Hamid Rahkooy, Torkel E Loman, Ruth E Baker</dc:creator>
    </item>
    <item>
      <title>A Restricted Latent Class Hidden Markov Model for Polytomous Responses, Polytomous Attributes, and Covariates: Identifiability and Application</title>
      <link>https://arxiv.org/abs/2503.20940</link>
      <description>arXiv:2503.20940v4 Announce Type: replace 
Abstract: We introduce a restricted latent class exploratory model for longitudinal data with ordinal attributes and respondent-specific covariates. Responses follow a time inhomogeneous hidden Markov model where the probability of a particular latent state at a time point is conditional on values at the previous time point of the respondent's covariates and latent state. We prove that the model is identifiable, state a Bayesian formulation, and demonstrate its efficacy in a variety of scenarios through two simulation studies. We apply the model to response data from a mathematics examination, comparing the results to a previously published confirmatory analysis, and also apply it to emotional state response data which was measured over a several-day period.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20940v4</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Alan Wayman, Steven Andrew Culpepper, Jeff Douglas, Jesse Bowers</dc:creator>
    </item>
    <item>
      <title>Two-stage Estimation for Causal Inference Involving a Semi-continuous Exposure</title>
      <link>https://arxiv.org/abs/2511.20985</link>
      <description>arXiv:2511.20985v3 Announce Type: replace 
Abstract: Methods for causal inference are well developed for binary and continuous exposures, but in many settings, the exposure has a substantial mass at zero-such exposures are called semi-continuous. We propose a general causal framework for such semi-continuous exposures, together with a novel two-stage estimation strategy. A two-part propensity structure is introduced for the semi-continuous exposure, with one component for exposure status (exposed vs unexposed) and another for the exposure level among those exposed, and incorporates both into a marginal structural model that disentangles the effects of exposure status and dose. The two-stage procedure sequentially targets the causal dose-response among exposed individuals and the causal effect of exposure status at a reference dose, allowing flexibility in the choice of propensity score methods in the second stage. We establish consistency and asymptotic normality for the resulting estimators, and characterise their limiting values under misspecification of the propensity score models. Simulation studies evaluate finite sample performance and robustness, and an application to a study of prenatal alcohol exposure and child cognition demonstrates how the proposed methods can be used to address a range of scientific questions about both exposure status and exposure intensity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20985v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoya Wang, Richard J. Cook, Yeying Zhu, Tugba Akkaya-Hocagil, R. Colin Carter, Sandra W. Jacobson, Joseph L. Jacobson, Louise M. Ryan</dc:creator>
    </item>
    <item>
      <title>Sequential Randomization Tests Using e-values: Applications for trial monitoring</title>
      <link>https://arxiv.org/abs/2512.04366</link>
      <description>arXiv:2512.04366v3 Announce Type: replace 
Abstract: Sequential monitoring of randomized trials traditionally relies on parametric assumptions or asymptotic approximations. We discuss a nonparametric sequential test and its application to continuous and time-to-event endpoints that derives validity solely from the randomization mechanism. Using a betting framework, these tests constructs a test martingale by sequentially wagering on treatment assignments given observed outcomes. Under the null hypothesis of no treatment effect, the expected wealth cannot grow, guaranteeing anytime-valid Type I error control regardless of stopping rule. We prove validity and present simulation studies demonstrating calibration and power. These methods provide a conservative, assumption-free complement to model-based sequential analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04366v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fernando G Zampieri</dc:creator>
    </item>
    <item>
      <title>ADOPT: Additive Optimal Transport Regression</title>
      <link>https://arxiv.org/abs/2512.08118</link>
      <description>arXiv:2512.08118v2 Announce Type: replace 
Abstract: Regression analysis for responses taking values in general metric spaces has received increasing attention, particularly for settings with Euclidean predictors $X \in \mathbb{R}^p$ and non-Euclidean responses $Y$ in metric spaces. While additive regression is a powerful tool for enhancing interpretability and mitigating the curse of dimensionality in the presence of multivariate predictors, its direct extension is hindered by the absence of vector space operations in general metric spaces. We propose a novel framework for additive optimal transport regression, which incorporates additive structure through optimal geodesic transports. A key idea is to extend the notion of optimal transports in Wasserstein spaces to general geodesic metric spaces. This unified approach accommodates a wide range of responses, including probability distributions, symmetric positive definite (SPD) matrices with various metrics and spherical data. The practical utility of the method is illustrated with correlation matrices derived from resting state fMRI brain imaging data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08118v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wookyeong Song, Hans-Georg M\"uller</dc:creator>
    </item>
    <item>
      <title>Kernel Three Pass Regression Filter</title>
      <link>https://arxiv.org/abs/2405.07292</link>
      <description>arXiv:2405.07292v4 Announce Type: replace-cross 
Abstract: We forecast a single time series using a high-dimensional set of predictors. When these predictors share common underlying dynamics, an approximate latent factor model provides a powerful characterization of their co-movements Bai(2003). These latent factors succinctly summarize the data and can also be used for prediction, alleviating the curse of dimensionality in high-dimensional prediction exercises, see Stock &amp; Watson (2002a). However, forecasting using these latent factors suffers from two potential drawbacks. First, not all pervasive factors among the set of predictors may be relevant, and using all of them can lead to inefficient forecasts. The second shortcoming is the assumption of linear dependence of predictors on the underlying factors. The first issue can be addressed by using some form of supervision, which leads to the omission of irrelevant information. One example is the three-pass regression filter proposed by Kelly &amp; Pruitt (2015). We extend their framework to cases where the form of dependence might be nonlinear by developing a new estimator, which we refer to as the Kernel Three-Pass Regression Filter (K3PRF). This alleviates the aforementioned second shortcoming. The estimator is computationally efficient and performs well empirically. The short-term performance matches or exceeds that of established models, while the long-term performance shows significant improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07292v4</guid>
      <category>econ.EM</category>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rajveer Jat, Daanish Padha</dc:creator>
    </item>
    <item>
      <title>Automatic Inference for Value-Added Regressions</title>
      <link>https://arxiv.org/abs/2503.19178</link>
      <description>arXiv:2503.19178v2 Announce Type: replace-cross 
Abstract: A large empirical literature regresses outcomes on empirical Bayes shrinkage estimates of value-added, yet little is known about whether this approach leads to unbiased estimates and valid inference for the downstream regression coefficients. We study a general class of empirical Bayes estimators and the properties of the resulting regression coefficients. We show that estimators can be asymptotically biased and inference can be invalid if the shrinkage estimator does not account for heteroskedasticity in the noise when estimating value added. By contrast, shrinkage estimators properly constructed to model this heteroskedasticity perform an automatic bias correction: the associated regression estimator is asymptotically unbiased, asymptotically normal, and efficient in the sense that it is asymptotically equivalent to regressing on the true (latent) value-added. Further, OLS standard errors from regressing on shrinkage estimates are consistent in this case. As such, efficient inference is easy for practitioners to implement: simply regress outcomes on shrinkage estimates of value-added that account for noise heteroskedasticity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19178v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tian Xie</dc:creator>
    </item>
    <item>
      <title>Statistical Properties of Rectified Flow</title>
      <link>https://arxiv.org/abs/2511.03193</link>
      <description>arXiv:2511.03193v3 Announce Type: replace-cross 
Abstract: Rectified flow (Liu et al., 2022; Liu, 2022; Wu et al., 2023) is a method for defining a transport map between two distributions, and enjoys popularity in machine learning, although theoretical results supporting the validity of these methods are scant. The rectified flow can be regarded as an approximation to optimal transport, but in contrast to other transport methods that require optimization over a function space, computing the rectified flow only requires standard statistical tools such as regression or density estimation, which we leverage to develop empirical versions of transport maps. We study some structural properties of the rectified flow, including existence, uniqueness, and regularity, as well as the related statistical properties, such as rates of convergence and central limit theorems, for some selected estimators. To do so, we analyze the bounded and unbounded cases separately as each presents unique challenges. In both cases, we are able to establish convergence at faster rates than those for the usual nonparametric regression and density estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03193v3</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gonzalo Mena, Arun Kumar Kuchibhotla, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Online Price Competition under Generalized Linear Demands</title>
      <link>https://arxiv.org/abs/2511.10718</link>
      <description>arXiv:2511.10718v3 Announce Type: replace-cross 
Abstract: We study sequential price competition among $N$ sellers, each influenced by the pricing decisions of their rivals. Specifically, the demand function for each seller $i$ follows the single index model $\lambda_i(\mathbf{p}) = \mu_i(\langle \boldsymbol{\theta}_{i,0}, \mathbf{p} \rangle)$, with known increasing link $\mu_i$ and unknown parameter $\boldsymbol{\theta}_{i,0}$, where the vector $\mathbf{p}$ denotes the vector of prices offered by all the sellers simultaneously at a given instant. Each seller observes only their own realized demand -- unobservable to competitors -- and the prices set by rivals. Our framework generalizes existing approaches that focus solely on linear demand models. We propose a novel decentralized policy, PML-GLUCB, that combines penalized MLE with an upper-confidence pricing rule, removing the need for coordinated exploration phases across sellers -- which is integral to previous linear models -- and accommodating both binary and real-valued demand observations. Relative to a dynamic benchmark policy, each seller achieves $O(N^{2}\sqrt{T}\log(T))$ regret, which essentially matches the optimal rate known in the linear setting. A significant technical contribution of our work is the development of a variant of the elliptical potential lemma -- typically applied in single-agent systems -- adapted to our competitive multi-agent environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10718v3</guid>
      <category>cs.GT</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Bracale, Moulinath Banerjee, Cong Shi, Yuekai Sun</dc:creator>
    </item>
  </channel>
</rss>
