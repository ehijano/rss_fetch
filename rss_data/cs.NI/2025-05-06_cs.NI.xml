<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 07 May 2025 01:46:59 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Fragment-Level Macro-Diversity Reception in LoRaWAN Networks with LR-FHSS</title>
      <link>https://arxiv.org/abs/2505.01689</link>
      <description>arXiv:2505.01689v1 Announce Type: new 
Abstract: The rapid expansion of Internet of Things (IoT) deployments demands wireless protocols that combine high scalability with robust performance. Long Range-Frequency Hopping Spread Spectrum (LR-FHSS) extends LoRaWAN by increasing capacity and resilience through frequency hopping and redundancy. However, current deployments require packet reconstruction at a single gateway, limiting the benefits of LR-FHSS. This paper proposes a macro-diversity reception strategy where multiple gateways collectively receive and combine payload fragments. We develop a stochastic geometry-based analytical model that captures the impact of header repetition, payload fragmentation, and coding redundancy. Closed-form expressions quantify success probabilities under interference, and numerical evaluations demonstrate significant capacity gains over nearest-gateway reception. These results highlight the potential of fragment-level macro-diversity to improve scalability and reliability in future LPWAN deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01689v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samer Lahoud, Kinda Khawam</dc:creator>
    </item>
    <item>
      <title>Semantics-Aware Unified Terrestrial Non-Terrestrial 6G Networks</title>
      <link>https://arxiv.org/abs/2505.01796</link>
      <description>arXiv:2505.01796v1 Announce Type: new 
Abstract: The integration of Terrestrial and Non-Terrestrial Networks (TN-NTNs), which was introduced in 5G, is progressing toward a unified and seamless network of networks in Sixth-Generation (6G). This evolution leads to a significant increase in the volume of generated and communicated data, imposing technical and operational requirements accompanied by a higher cost and energy consumption. Efficiently managing the generation and transmission of data in these highly complex unified networks has become essential. In this article, we investigate the semantics-aware information handling problem within unified TN-NTNs, where data communication between the distant TN nodes is enabled via an NTN. To this end, an Internet of Things (IoT) monitoring system is employed, where status updates from a remote IoT device are communicated to a destination monitor via a constellation of Low Earth Orbit (LEO) satellites. We leverage semantic metrics that capture the timeliness, relevance, and utility of information to provide the most informative data for timely and informed decision-making and eventually reduce the volume of transmitted and processed data. The outcome is significantly lower energy consumption, memory, control, and processing requirements (up to 73% lower energy charging demands compared to the state-of-the-art), all without compromising the conveyed information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01796v1</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erfan Delfani, Agapi Mesodiakaki, Nikolaos Pappas</dc:creator>
    </item>
    <item>
      <title>Model Context Protocol-based Internet of Experts For Wireless Environment-aware LLM Agents</title>
      <link>https://arxiv.org/abs/2505.01834</link>
      <description>arXiv:2505.01834v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit strong general-purpose reasoning abilities but lack access to wireless environment information due to the absence of native sensory input and domain-specific priors. Previous attempts to apply LLMs in wireless systems either depend on retraining with network-specific data, which compromises language generalization, or rely on manually scripted interfaces, which hinder scalability. To overcome these limitations, we propose a Model Context Protocol (MCP)-based Internet of Experts (IoX) framework that equips LLMs with wireless environment-aware reasoning capabilities. The framework incorporates a set of lightweight expert models, each trained to solve a specific deterministic task in wireless communications, such as detecting a specific wireless attribute, e.g., line-of-sight propagation, Doppler effects, or fading conditions. Through MCP, the LLM can selectively query and interpret expert outputs at inference time, without modifying its own parameters. This architecture enables modular, extensible, and interpretable reasoning over wireless contexts. Evaluated across multiple mainstream LLMs, the proposed wireless environment-aware LLM agents achieve 40%-50% improvements in classification tasks over LLM-only baselines. More broadly, the MCP-based design offers a viable paradigm for future LLMs to inherit structured wireless network management capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01834v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zongxi Liu, Hongyang Du</dc:creator>
    </item>
    <item>
      <title>Harnessing the Power of LLMs, Informers and Decision Transformers for Intent-driven RAN Management in 6G</title>
      <link>https://arxiv.org/abs/2505.01841</link>
      <description>arXiv:2505.01841v1 Announce Type: new 
Abstract: Intent-driven network management is critical for managing the complexity of 5G and 6G networks. It enables adaptive, on-demand management of the network based on the objectives of the network operators. In this paper, we propose an innovative three-step framework for intent-driven network management based on Generative AI (GenAI) algorithms. First, we fine-tune a Large Language Model (LLM) on a custom dataset using a Quantized Low-Rank Adapter (QLoRA) to enable memory-efficient intent processing within limited computational resources. A Retrieval Augmented Generation (RAG) module is included to support dynamic decision-making. Second, we utilize a transformer architecture for time series forecasting to predict key parameters, such as power consumption, traffic load, and packet drop rate, to facilitate intent validation proactively. Lastly, we introduce a Hierarchical Decision Transformer with Goal Awareness (HDTGA) to optimize the selection and orchestration of network applications and hence, optimize the network. Our intent guidance and processing approach improves BERTScore by 6% and the semantic similarity score by 9% compared to the base LLM model. Again, the proposed predictive intent validation approach can successfully rule out the performance-degrading intents with an average of 88% accuracy. Finally, compared to the baselines, the proposed HDTGA algorithm increases throughput at least by 19.3%, reduces delay by 48.5%, and boosts energy efficiency by 54.9%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01841v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Arafat Habib, Pedro Enrique Iturria Rivera, Yigit Ozcan, Medhat Elsayed, Majid Bavand, Raimundas Gaigalas, Melike Erol-Kantarci</dc:creator>
    </item>
    <item>
      <title>Trustworthy Inter-Provider Agreements in 6G Using a Privacy-Enabled Hybrid Blockchain Framework</title>
      <link>https://arxiv.org/abs/2505.02513</link>
      <description>arXiv:2505.02513v1 Announce Type: new 
Abstract: Inter-provider agreements are central to 6G networks, where administrative domains must securely and dynamically share services. To address the dual need for transparency and confidentiality, we propose a privacy-enabled hybrid blockchain setup using Hyperledger Besu, integrating both public and private transaction workflows. The system enables decentralized service registration, selection, and SLA breach reporting through role-based smart contracts and privacy groups. We design and deploy a proof-of-concept implementation, evaluating performance using end-to-end latency as a key metric within privacy groups. Results show that public interactions maintain stable latency, while private transactions incur additional overhead due to off-chain coordination. The block production rate governed by IBFT 2.0 had limited impact on private transaction latency, due to encryption and peer synchronization. Lessons learned highlight design considerations for smart contract structure, validator management, and scalability patterns suitable for dynamic inter-domain collaboration. Our findings offer practical insights for deploying trustworthy agreement systems in 6G networks using privacy-enabled hybrid blockchains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02513v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Farhana Javed, Josep Mangues-Bafalluy</dc:creator>
    </item>
    <item>
      <title>Antifragility of RIS-assisted Communication Systems under Jamming Attacks</title>
      <link>https://arxiv.org/abs/2505.02565</link>
      <description>arXiv:2505.02565v1 Announce Type: new 
Abstract: Antifragility of communication systems is defined as measure of benefits gained from the adverse events and variability of its environment. In this paper, we introduce the notion of antifragility in Reconfigurable Intelligent Surface (RIS) assisted communication systems affected by a jamming attack. We analyzed the antifragility of the two hop systems, where the wireless path contains source node, RIS, destination node, and a eavesdropping/jamming node. We propose and analyze the antifragility performance for several jamming models, such as Digital Radio Frequency Memory (DRFM) and phase and amplitude shifting. Our paper shows that antifragility throughput can indeed be achieved under certain power thresholds and for various jamming models. In particular, high jamming power combined with low baseline data rates yields an antifragile gain factor of approximately five times. The results confirm that reconfigurable intelligent surfaces, when coupled with an antifragile design philosophy, can convert hostile interference from a liability into a throughput gain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02565v1</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mounir Bensalem, Thomas R\"othig, Admela Jukan</dc:creator>
    </item>
    <item>
      <title>World Model-Based Learning for Long-Term Age of Information Minimization in Vehicular Networks</title>
      <link>https://arxiv.org/abs/2505.01712</link>
      <description>arXiv:2505.01712v1 Announce Type: cross 
Abstract: Traditional reinforcement learning (RL)-based learning approaches for wireless networks rely on expensive trial-and-error mechanisms and real-time feedback based on extensive environment interactions, which leads to low data efficiency and short-sighted policies. These limitations become particularly problematic in complex, dynamic networks with high uncertainty and long-term planning requirements. To address these limitations, in this paper, a novel world model-based learning framework is proposed to minimize packet-completeness-aware age of information (CAoI) in a vehicular network. Particularly, a challenging representative scenario is considered pertaining to a millimeter-wave (mmWave) vehicle-to-everything (V2X) communication network, which is characterized by high mobility, frequent signal blockages, and extremely short coherence time. Then, a world model framework is proposed to jointly learn a dynamic model of the mmWave V2X environment and use it to imagine trajectories for learning how to perform link scheduling. In particular, the long-term policy is learned in differentiable imagined trajectories instead of environment interactions. Moreover, owing to its imagination abilities, the world model can jointly predict time-varying wireless data and optimize link scheduling in real-world wireless and V2X networks. Thus, during intervals without actual observations, the world model remains capable of making efficient decisions. Extensive experiments are performed on a realistic simulator based on Sionna that integrates physics-based end-to-end channel modeling, ray-tracing, and scene geometries with material properties. Simulation results show that the proposed world model achieves a significant improvement in data efficiency, and achieves 26% improvement and 16% improvement in CAoI, respectively, compared to the model-based RL (MBRL) method and the model-free RL (MFRL) method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01712v1</guid>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lingyi Wang, Rashed Shelim, Walid Saad, Naren Ramakrishnan</dc:creator>
    </item>
    <item>
      <title>Rate-Limited Closed-Loop Distributed ISAC Systems: An Autoencoder Approach</title>
      <link>https://arxiv.org/abs/2505.01780</link>
      <description>arXiv:2505.01780v1 Announce Type: cross 
Abstract: In closed-loop distributed multi-sensor integrated sensing and communication (ISAC) systems, performance often hinges on transmitting high-dimensional sensor observations over rate-limited networks. In this paper, we first present a general framework for rate-limited closed-loop distributed ISAC systems, and then propose an autoencoder-based observation compression method to overcome the constraints imposed by limited transmission capacity. Building on this framework, we conduct a case study using a closed-loop linear quadratic regulator (LQR) system to analyze how the interplay among observation, compression, and state dimensions affects reconstruction accuracy, state estimation error, and control performance. In multi-sensor scenarios, our results further show that optimal resource allocation initially prioritizes low-noise sensors until the compression becomes lossless, after which resources are reallocated to high-noise sensors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01780v1</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guangjin Pan, Zhixing Li, Ay\c{c}a \"Oz\c{c}elikkale, Christian H\"ager, Musa Furkan Keskin, Henk Wymeersch</dc:creator>
    </item>
    <item>
      <title>Energy Efficiency Maximization for CR-NOMA based Smart Grid Communication Network</title>
      <link>https://arxiv.org/abs/2505.02530</link>
      <description>arXiv:2505.02530v1 Announce Type: cross 
Abstract: Managing massive data flows effectively and resolving spectrum shortages are two challenges that Smart Grid Communication Networks (SGCN) must overcome. To address these problems, we provide a combined optimization approach that makes use of Cognitive Radio (CR) and Non-Orthogonal Multiple Access (NOMA) technologies. Our work focuses on using user pairing (UP) and power allocation (PA) techniques to maximize energy efficiency (EE) in SGCN, particularly within Neighbourhood Area Networks (NANs). We develop a joint optimization problem that takes into account the real-world limitations of a CR-NOMA setting. This problem is NP-hard, nonlinear, and nonconvex by nature. To address the computational complexity of the problem, we use the Block Coordinate Descent (BCD) method, which breaks the problem into UP and PA subproblems. Initially, we proposed the Zebra-Optimization User Pairing (ZOUP) algorithm to tackle the UP problem, which outperforms both Orthogonal Multiple Access (OMA) and non-optimized NOMA (UPWO) by 78.8\% and 13.6\%, respectively, at a SNR of 15 dB. Based on the ZOUP pairs, we subsequently proposed the PA approach, i.e., ZOUPPA, which significantly outperforms UPWO and ZOUP by 53.2\% and 25.4\%, respectively, at an SNR of 15 dB. A detailed analysis of key parameters, including varying SNRs, power allocation constants, path loss exponents, user density, channel availability, and coverage radius, underscores the superiority of our approach. By facilitating the effective use of communication resources in SGCN, our research opens the door to more intelligent and energy-efficient grid systems. Our work tackles important issues in SGCN and lays the groundwork for future developments in smart grid communication technologies by combining modern optimization approaches with CR-NOMA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02530v1</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mubashar Sarfraz, Sheraz Alam, Sajjad A. Ghauri, Asad Mahmood</dc:creator>
    </item>
    <item>
      <title>Adaptive Budgeted Multi-Armed Bandits for IoT with Dynamic Resource Constraints</title>
      <link>https://arxiv.org/abs/2505.02640</link>
      <description>arXiv:2505.02640v1 Announce Type: cross 
Abstract: Internet of Things (IoT) systems increasingly operate in environments where devices must respond in real time while managing fluctuating resource constraints, including energy and bandwidth. Yet, current approaches often fall short in addressing scenarios where operational constraints evolve over time. To address these limitations, we propose a novel Budgeted Multi-Armed Bandit framework tailored for IoT applications with dynamic operational limits. Our model introduces a decaying violation budget, which permits limited constraint violations early in the learning process and gradually enforces stricter compliance over time. We present the Budgeted Upper Confidence Bound (UCB) algorithm, which adaptively balances performance optimization and compliance with time-varying constraints. We provide theoretical guarantees showing that Budgeted UCB achieves sublinear regret and logarithmic constraint violations over the learning horizon. Extensive simulations in a wireless communication setting show that our approach achieves faster adaptation and better constraint satisfaction than standard online learning methods. These results highlight the framework's potential for building adaptive, resource-aware IoT systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02640v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shubham Vaishnav, Praveen Kumar Donta, Sindri Magn\'usson</dc:creator>
    </item>
    <item>
      <title>Bandwidth Efficient Livestreaming in Mobile Wireless Networks: A Peer-to-Peer ACIDE Solution</title>
      <link>https://arxiv.org/abs/2310.14283</link>
      <description>arXiv:2310.14283v3 Announce Type: replace 
Abstract: In mobile wireless networks, livestreaming in high user density areas presents two typical challenges: the wireless bandwidth is depleted and the number of users is limited. In this study, a media distribution model utilizing peer to peer communications, Active Control in an Intelligent and Distributed Environment, is proposed for bandwidth efficient livestreaming. The basic idea is to group users with identical livestream interest in a cluster of n peers. Instead of sending n copies of a livestream package, only one copy is sent to the cluster. A package is divided into n blocks. Each user receives one block from the base station and the remaining n-1 blocks from the other peers. Two optimization problems are addressed. The first problem is minimizing the bandwidth needed to guarantee a continuous live media play on all peers. A solution is proposed to find the optimal block sizes such that the wireless bandwidth is minimized. The second problem is maximizing the number of peers admitted to a cluster, given a fixed wireless bandwidth. This problem is NP-complete and a greedy strategy is proposed to calculate a feasible solution for peer selection. The proposed model improves the bandwidth efficiency and allows more users to be served.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.14283v3</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrei Negulescu, Weijia Shang</dc:creator>
    </item>
    <item>
      <title>Time-Constrained Erasure Correction for Data Recovery in UAV-LoRa-WuR Networks</title>
      <link>https://arxiv.org/abs/2403.09782</link>
      <description>arXiv:2403.09782v3 Announce Type: replace 
Abstract: We described two erasure-correction schemes for data recovery in UAV-LoRa-WuR networks. Our results show that unless the maximum number for redundant frames a sensor can send per data-collection cycle is very small, erasure coding provides noticeable improvements over an uncoded transmissions. Whether to employ coding -- and if so, which type -- should be determined based on the sensors' energy budget (which dictates the maximum redundancy), the UAV's hovering time, and the node density. The analytical framework presented above aids in this decision making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09782v3</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kushwanth Sistu, Siddhartha S. Borkotoky</dc:creator>
    </item>
    <item>
      <title>Hyper-parameter Optimization for Wireless Network Traffic Prediction Models with A Novel Meta-Learning Framework</title>
      <link>https://arxiv.org/abs/2409.14535</link>
      <description>arXiv:2409.14535v3 Announce Type: replace 
Abstract: This paper proposes a novel meta-learning based hyper-parameter optimization framework for wireless network traffic prediction (NTP) models. The primary objective is to accumulate and leverage the acquired hyper-parameter optimization experience, enabling the rapid determination of optimal hyper-parameters for new tasks. In this paper, an attention-based deep neural network (ADNN) is employed as the base-learner to address specific NTP tasks. The meta-learner is an innovative framework that integrates meta-learning with the k-nearest neighbor algorithm (KNN), genetic algorithm (GA), and gated residual network (GRN). Specifically, KNN is utilized to identify a set of candidate hyper-parameter selection strategies for a new task, which then serves as the initial population for GA, while a GRN-based chromosome screening module accelerates the validation of offspring chromosomes, ultimately determining the optimal hyper-parameters. Experimental results demonstrate that, compared to traditional methods such as Bayesian optimization (BO), GA, and particle swarm optimization (PSO), the proposed framework determines optimal hyper-parameters more rapidly, significantly reduces optimization time, and enhances the performance of the base-learner. It achieves an optimal balance between optimization efficiency and prediction accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14535v3</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liangzhi Wang, Jie Zhang, Yuan Gao, Jiliang Zhang, Guiyi Wei, Haibo Zhou, Bin Zhuge, Zitian Zhang</dc:creator>
    </item>
    <item>
      <title>GDSG: Graph Diffusion-based Solution Generator for Optimization Problems in MEC Networks</title>
      <link>https://arxiv.org/abs/2412.08296</link>
      <description>arXiv:2412.08296v3 Announce Type: replace 
Abstract: Optimization is crucial for MEC networks to function efficiently and reliably, most of which are NP-hard and lack efficient approximation algorithms. This leads to a paucity of optimal solution, constraining the effectiveness of conventional deep learning approaches. Most existing learning-based methods necessitate extensive optimal data and fail to exploit the potential benefits of suboptimal data that can be obtained with greater efficiency and effectiveness. Taking the multi-server multi-user computation offloading (MSCO) problem, which is widely observed in systems like Internet-of-Vehicles (IoV) and Unmanned Aerial Vehicle (UAV) networks, as a concrete scenario, we present a Graph Diffusion-based Solution Generation (GDSG) method. This approach is designed to work with suboptimal datasets while converging to the optimal solution large probably. We transform the optimization issue into distribution-learning and offer a clear explanation of learning from suboptimal training datasets. We build GDSG as a multi-task diffusion model utilizing a Graph Neural Network (GNN) to acquire the distribution of high-quality solutions. We use a simple and efficient heuristic approach to obtain a sufficient amount of training data composed entirely of suboptimal solutions. In our implementation, we enhance the backbone GNN and achieve improved generalization. GDSG also reaches nearly 100\% task orthogonality, ensuring no interference between the discrete and continuous generation tasks. We further reveal that this orthogonality arises from the diffusion-related training loss, rather than the neural network architecture itself. The experiments demonstrate that GDSG surpasses other benchmark methods on both the optimal and suboptimal training datasets. The MSCO datasets has open-sourced at this http URL, as well as the GDSG algorithm codes at https://github.com/qiyu3816/GDSG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08296v3</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruihuai Liang, Bo Yang, Pengyu Chen, Xuelin Cao, Zhiwen Yu, M\'erouane Debbah, Dusit Niyato, H. Vincent Poor, Chau Yuen</dc:creator>
    </item>
    <item>
      <title>Energy-Efficient Flying LoRa Gateways: A Multi-Agent Reinforcement Learning Approach</title>
      <link>https://arxiv.org/abs/2502.03377</link>
      <description>arXiv:2502.03377v3 Announce Type: replace 
Abstract: As next-generation Internet of Things (NG-IoT) networks continue to grow, the number of connected devices is rapidly increasing, along with their energy demands. This creates challenges for resource management and sustainability. Energy-efficient communication, particularly for power-limited IoT devices, is therefore a key research focus. In this paper, we deployed flying LoRa gateways mounted on unmanned aerial vehicles (UAVs) to collect data from LoRa end devices and transmit it to a central server. Our primary objective is to maximize the global system energy efficiency of wireless LoRa networks by joint optimization of transmission power, spreading factor, bandwidth, and user association. To solve this challenging problem, we model the problem as a partially observable Markov decision process (POMDP), where each flying LoRa GW acts as a learning agent using a cooperative multi-agent reinforcement learning (MARL). Simulation results demonstrate that our proposed method, based on the multi-agent proximal policy optimization algorithm, significantly improves the global system energy efficiency and surpasses the popular MARL and other conventional schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03377v3</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdullahi Isa Ahmed, Jamal Bentahar, El Mehdi Amhoud</dc:creator>
    </item>
    <item>
      <title>VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with Delayed Hits</title>
      <link>https://arxiv.org/abs/2504.20335</link>
      <description>arXiv:2504.20335v2 Announce Type: replace 
Abstract: Caches are fundamental to latency-sensitive systems like Content Delivery Networks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit phenomenon where multiple requests for an object occur during its fetch from the remote server after a miss significantly inflates user-perceived latency. While recent algorithms acknowledge delayed hits by estimating the resulting aggregate delay, they predominantly focus on its mean value. We identify and demonstrate that such approaches are insufficient, as the real aggregate delay frequently exhibits substantial variance in the true production system, leading to suboptimal latency performance when ignored. Thus, we propose VA-CDH, a variance-aware method to optimize latency for caching with delayed hits. It employs a novel ranking function that explicitly incorporates both the empirically estimated mean and standard deviation of aggregate delay, allowing caching decisions to account for its variation. We derive the analytical distribution of aggregate delay under Poisson arrivals as a theoretical contribution, offering more statistical insight beyond the mean value. Through the simulations conducted on synthetic and real-world datasets, we show that VA-CDH reduces the total latency by 1%-6% approximately compared to state-of-the-art algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20335v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bowen Jiang, Chaofan Ma, Duo Wang</dc:creator>
    </item>
    <item>
      <title>OptiReduce: Resilient and Tail-Optimal AllReduce for Distributed Deep Learning in the Cloud</title>
      <link>https://arxiv.org/abs/2310.06993</link>
      <description>arXiv:2310.06993v2 Announce Type: replace-cross 
Abstract: We present OptiReduce, a new collective-communication system for the cloud with bounded, predictable completion times for deep-learning jobs in the presence of varying computation (stragglers) and communication (congestion and gradient drops) variabilities. OptiReduce exploits the inherent resiliency and the stochastic nature of distributed deep-learning (DDL) training and fine-tuning to work with approximated (or lost) gradients -- providing an efficient balance between (tail) performance and the resulting accuracy of the trained models.
  Exploiting this domain-specific characteristic of DDL, OptiReduce introduces (1) mechanisms (e.g., unreliable bounded transport with adaptive timeout) to improve the DDL jobs' tail execution time, and (2) strategies (e.g., Transpose AllReduce and Hadamard Transform) to mitigate the impact of gradient drops on model accuracy. Our evaluation shows that OptiReduce achieves 70% and 30% faster time-to-accuracy (TTA), on average, when operating in shared, cloud environments (e.g., CloudLab) compared to Gloo and NCCL, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.06993v2</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>In 22nd USENIX Symposium on Networked Systems Design and Implementation (NSDI 25), pp. 685-703. 2025</arxiv:journal_reference>
      <dc:creator>Ertza Warraich, Omer Shabtai, Khalid Manaa, Shay Vargaftik, Yonatan Piasetzky, Matty Kadosh, Lalith Suresh, Muhammad Shahbaz</dc:creator>
    </item>
    <item>
      <title>GenAINet: Enabling Wireless Collective Intelligence via Knowledge Transfer and Reasoning</title>
      <link>https://arxiv.org/abs/2402.16631</link>
      <description>arXiv:2402.16631v3 Announce Type: replace-cross 
Abstract: Generative Artificial Intelligence (GenAI) and communication networks are expected to have groundbreaking synergies for 6G. Connecting GenAI agents via a wireless network can potentially unleash the power of Collective Intelligence (CI) and pave the way for Artificial General Intelligence (AGI). However, current wireless networks are designed as a "data pipe" and are not suited to accommodate and leverage the power of GenAI. In this paper, we propose the GenAINet framework in which distributed GenAI agents communicate knowledge (facts, experiences, and methods) to accomplish arbitrary tasks. We first propose an architecture for a single GenAI agent and then provide a network architecture integrating GenAI capabilities to manage both network protocols and applications. Building on this, we investigate effective communication and reasoning problems by proposing a semantic-native GenAINet. Specifically, GenAI agents extract semantics from heterogeneous raw data, build and maintain a knowledge model representing the semantic relationships among pieces of knowledge, which is retrieved by GenAI models for planning and reasoning. Under this paradigm, different levels of collaboration can be achieved flexibly depending on the complexity of targeted tasks. Furthermore, we conduct two case studies in which, through wireless device queries, we demonstrate that extracting, compressing and transferring common knowledge can improve query accuracy while reducing communication costs; and in the wireless power control problem, we show that distributed agents can complete general tasks independently through collaborative reasoning without predefined communication protocols. Finally, we discuss challenges and future research directions in applying Large Language Models (LLMs) in 6G networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16631v3</guid>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hang Zou, Qiyang Zhao, Samson Lasaulce, Lina Bariah, Mehdi Bennis, Merouane Debbah</dc:creator>
    </item>
    <item>
      <title>Private Information Retrieval on Multigraph-Based Replicated Storage</title>
      <link>https://arxiv.org/abs/2501.17845</link>
      <description>arXiv:2501.17845v2 Announce Type: replace-cross 
Abstract: We consider the private information retrieval (PIR) problem for a multigraph-based replication system, where each set of $r$ files is stored on two of the servers according to an underlying $r$-multigraph. Our goal is to establish upper and lower bounds on the PIR capacity of the $r$-multigraph. Specifically, we first propose a construction for multigraph-based PIR systems that leverages the symmetry of the underlying graph-based PIR scheme, deriving a capacity lower bound for such multigraphs. Then, we establish a general upper bound using linear programming, expressed as a function of the underlying graph parameters. Our bounds are demonstrated to be tight for PIR systems on multipaths for even number of vertices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17845v2</guid>
      <category>cs.IT</category>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shreya Meel, Xiangliang Kong, Thomas Jacob Maranzatto, Itzhak Tamo, Sennur Ulukus</dc:creator>
    </item>
  </channel>
</rss>
