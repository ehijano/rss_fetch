<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 27 Oct 2025 15:16:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>To MCMC or not to MCMC: Evaluating non-MCMC methods for Bayesian penalized regression</title>
      <link>https://arxiv.org/abs/2510.20947</link>
      <description>arXiv:2510.20947v1 Announce Type: new 
Abstract: Markov Chain Monte Carlo (MCMC) sampling is computationally expensive, especially for complex models. Alternative methods make simplifying assumptions about the posterior to reduce computational burden, but their impact on predictive performance remains unclear. This paper compares MCMC and non-MCMC methods for high-dimensional penalized regression, examining when computational shortcuts are justified for prediction tasks.
  We conduct a comprehensive simulation study using high-dimensional tabular data, then validate findings with empirical datasets featuring both continuous and binary outcomes. An in-depth analysis of one dataset provides a step-by-step tutorial implementing various algorithms in R.
  Our results show that mean-field variational inference consistently performs comparably to MCMC methods. In simulations, mean-field VI exhibited 3-90\% higher MSE across scenarios while reducing runtime by 7-30x compared to Hamiltonian Monte Carlo. Empirical datasets revealed dramatic speed-ups (100-400x) in some cases with similar or superior predictive performance. However, performance varied: some cases showed over 100x MSE increases with only 30x speed-ups, highlighting the context-dependent nature of these trade-offs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20947v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian D. van Leeuwen, Sara van Erp</dc:creator>
    </item>
    <item>
      <title>A Multiscale Approach for Enhancing Weak Signal Detection</title>
      <link>https://arxiv.org/abs/2510.20828</link>
      <description>arXiv:2510.20828v1 Announce Type: cross 
Abstract: Stochastic resonance (SR), a phenomenon originally introduced in climate modeling, enhances signal detection by leveraging optimal noise levels within non-linear systems. Traditional SR techniques, mainly based on single-threshold detectors, are limited to signals whose behavior does not depend on time. Often large amounts of noise are needed to detect weak signals, which can distort complex signal characteristics. To address these limitations, this study explores multi-threshold systems and the application of SR in multiscale applications using wavelet transforms. In the multiscale domain signals can be analyzed at different levels of resolution to better understand the underlying dynamics.
  We propose a double-threshold detection system that integrates two single-threshold detectors to enhance weak signal detection. We evaluate it both in the original data domain and in the multiscale domain using simulated and real-world signals and compare its performance with existing methods.
  Experimental results demonstrate that, in the original data domain, the proposed double-threshold detector significantly improves weak signal detection compared to conventional single-threshold approaches. Its performance is further improved in the frequency domain, requiring lower noise levels while outperforming existing detection systems. This study advances SR-based detection methodologies by introducing a robust approach to weak signal identification, with potential applications in various disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20828v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dixon Vimalajeewa, Ursula U. Muller, Brani Vidakovic</dc:creator>
    </item>
    <item>
      <title>Bayesian analysis of flexible Heckman selection models using Hamiltonian Monte Carlo</title>
      <link>https://arxiv.org/abs/2510.20942</link>
      <description>arXiv:2510.20942v1 Announce Type: cross 
Abstract: The Heckman selection model is widely used in econometric analysis and other social sciences to address sample selection bias in data modeling. A common assumption in Heckman selection models is that the error terms follow an independent bivariate normal distribution. However, real-world data often deviates from this assumption, exhibiting heavy-tailed behavior, which can lead to inconsistent estimates if not properly addressed. In this paper, we propose a Bayesian analysis of Heckman selection models that replace the Gaussian assumption with well-known members of the class of scale mixture of normal distributions, such as the Student's-t and contaminated normal distributions. For these complex structures, Stan's default No-U-Turn sampler is utilized to obtain posterior simulations. Through extensive simulation studies, we compare the performance of the Heckman selection models with normal, Student's-t and contaminated normal distributions. We also demonstrate the broad applicability of this methodology by applying it to medical care and labor supply data. The proposed algorithms are implemented in the R package HeckmanStan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20942v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Heeju Lim, Victor E. Lachos, Victor H. Lachos</dc:creator>
    </item>
    <item>
      <title>SLIM: Stochastic Learning and Inference in Overidentified Models</title>
      <link>https://arxiv.org/abs/2510.20996</link>
      <description>arXiv:2510.20996v1 Announce Type: cross 
Abstract: We propose SLIM (Stochastic Learning and Inference in overidentified Models), a scalable stochastic approximation framework for nonlinear GMM. SLIM forms iterative updates from independent mini-batches of moments and their derivatives, producing unbiased directions that ensure almost-sure convergence. It requires neither a consistent initial estimator nor global convexity and accommodates both fixed-sample and random-sampling asymptotics. We further develop an optional second-order refinement and inference procedures based on random scaling and plug-in methods, including plug-in, debiased plug-in, and online versions of the Sargan--Hansen $J$-test tailored to stochastic learning. In Monte Carlo experiments based on a nonlinear EASI demand system with 576 moment conditions, 380 parameters, and $n = 10^5$, SLIM solves the model in under 1.4 hours, whereas full-sample GMM in Stata on a powerful laptop converges only after 18 hours. The debiased plug-in $J$-test delivers satisfactory finite-sample inference, and SLIM scales smoothly to $n = 10^6$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20996v1</guid>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiaohong Chen, Min Seong Kim, Sokbae Lee, Myung Hwan Seo, Myunghyun Song</dc:creator>
    </item>
    <item>
      <title>Expectation-propagation for Bayesian empirical likelihood inference</title>
      <link>https://arxiv.org/abs/2510.21174</link>
      <description>arXiv:2510.21174v1 Announce Type: cross 
Abstract: Bayesian inference typically relies on specifying a parametric model that approximates the data-generating process. However, misspecified models can yield poor convergence rates and unreliable posterior calibration. Bayesian empirical likelihood offers a semi-parametric alternative by replacing the parametric likelihood with a profile empirical likelihood defined through moment constraints, thereby avoiding explicit distributional assumptions. Despite these advantages, Bayesian empirical likelihood faces substantial computational challenges, including the need to solve a constrained optimization problem for each likelihood evaluation and difficulties with non-convex posterior support, particularly in small-sample settings. This paper introduces a variational approach based on expectation-propagation to approximate the Bayesian empirical-likelihood posterior, balancing computational cost and accuracy without altering the target posterior via adjustments such as pseudo-observations. Empirically, we show that our approach can achieve a superior cost-accuracy trade-off relative to existing methods, including Hamiltonian Monte Carlo and variational Bayes. Theoretically, we show that the approximation and the Bayesian empirical-likelihood posterior are asymptotically equivalent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21174v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kenyon Ng, Weichang Yu, Howard D. Bondell</dc:creator>
    </item>
    <item>
      <title>SurVigilance: An Application for Accessing Global Pharmacovigilance Data</title>
      <link>https://arxiv.org/abs/2510.21572</link>
      <description>arXiv:2510.21572v1 Announce Type: cross 
Abstract: Even though several publicly accessible pharmacovigilance databases are available, extracting data from them is a technically challenging process. Existing tools typically focus on a single database. We present SurVigilance, an open-source tool that streamlines the process of retrieving safety data from seven major pharmacovigilance databases. SurVigilance provides a graphical user interface as well as functions for programmatic access, thus enabling integration into existing research workflows. SurVigilance utilizes a modular architecture to provide access to the heterogeneous sources. By reducing the technical barriers to accessing safety data, SurVigilance aims to facilitate pharmacovigilance research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21572v1</guid>
      <category>cs.DB</category>
      <category>stat.CO</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Raktim Mukhopadhyay, Marianthi Markatou</dc:creator>
    </item>
    <item>
      <title>MECfda: An R Package for Bias Correction Due to Measurement Error in Functional and Scalar Covariates in Scalar-on-Function Regression Models</title>
      <link>https://arxiv.org/abs/2510.21661</link>
      <description>arXiv:2510.21661v1 Announce Type: cross 
Abstract: Functional data analysis (FDA) deals with high-resolution data recorded over a continuum, such as time, space or frequency. Device-based assessments of physical activity or sleep are objective yet still prone to measurement error. We present MECfda, an R package that (i) fits scalar-on-function, generalized scalar-on-function, and functional quantile regression models, and (ii) provides bias-corrected estimation when functional covariates are measured with error. By unifying these tools under a consistent syntax, MECfda enables robust inference for FDA applications that involve noisy functional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21661v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heyang Ji, Carmen Tekwe</dc:creator>
    </item>
    <item>
      <title>Learning Latent Variable Models via Jarzynski-adjusted Langevin Algorithm</title>
      <link>https://arxiv.org/abs/2505.18427</link>
      <description>arXiv:2505.18427v2 Announce Type: replace 
Abstract: We utilise a sampler originating from nonequilibrium statistical mechanics, termed here Jarzynski-adjusted Langevin algorithm (JALA), to build statistical estimation methods in latent variable models. We achieve this by leveraging Jarzynski's equality and developing algorithms based on a weighted version of the unadjusted Langevin algorithm (ULA) with recursively updated weights. Adapting this for latent variable models, we develop a sequential Monte Carlo (SMC) method that provides the maximum marginal likelihood estimate of the parameters, termed JALA-EM. Under suitable regularity assumptions on the marginal likelihood, we provide a nonasymptotic analysis of the JALA-EM scheme implemented with stochastic gradient descent and show that it provably converges to the maximum marginal likelihood estimate. We demonstrate the performance of JALA-EM on a variety of latent variable models and show that it performs comparably to existing methods in terms of accuracy and computational efficiency. Importantly, the ability to recursively estimate marginal likelihoods - an uncommon feature among scalable methods - makes our approach particularly suited for model selection, which we validate through dedicated experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18427v2</guid>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Cuin, Davide Carbone, O. Deniz Akyildiz</dc:creator>
    </item>
    <item>
      <title>Multilevel neural simulation-based inference</title>
      <link>https://arxiv.org/abs/2506.06087</link>
      <description>arXiv:2506.06087v3 Announce Type: replace-cross 
Abstract: Neural simulation-based inference (SBI) is a popular set of methods for Bayesian inference when models are only available in the form of a simulator. These methods are widely used in the sciences and engineering, where writing down a likelihood can be significantly more challenging than constructing a simulator. However, the performance of neural SBI can suffer when simulators are computationally expensive, thereby limiting the number of simulations that can be performed. In this paper, we propose a novel approach to neural SBI which leverages multilevel Monte Carlo techniques for settings where several simulators of varying cost and fidelity are available. We demonstrate through both theoretical analysis and extensive experiments that our method can significantly enhance the accuracy of SBI methods given a fixed computational budget.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06087v3</guid>
      <category>stat.ML</category>
      <category>astro-ph.CO</category>
      <category>astro-ph.IM</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuga Hikida, Ayush Bharti, Niall Jeffrey, Fran\c{c}ois-Xavier Briol</dc:creator>
    </item>
    <item>
      <title>BASIN: Bayesian mAtrix variate normal model with Spatial and sparsIty priors in Non-negative deconvolution</title>
      <link>https://arxiv.org/abs/2510.16130</link>
      <description>arXiv:2510.16130v2 Announce Type: replace-cross 
Abstract: Spatial transcriptomics allows researchers to visualize and analyze gene expression within the precise location of tissues or cells. It provides spatially resolved gene expression data but often lacks cellular resolution, necessitating cell type deconvolution to infer cellular composition at each spatial location. In this paper we propose BASIN for cell type deconvolution, which models deconvolution as a nonnegative matrix factorization (NMF) problem incorporating graph Laplacian prior. Rather than find a deterministic optima like other recent methods, we propose a matrix variate Bayesian NMF method with nonnegativity and sparsity priors, in which the variables are maintained in their matrix form to derive a more efficient matrix normal posterior. BASIN employs a Gibbs sampler to approximate the posterior distribution of cell type proportions and other parameters, offering a distribution of possible solutions, enhancing robustness and providing inherent uncertainty quantification. The performance of BASIN is evaluated on different spatial transcriptomics datasets and outperforms other deconvolution methods in terms of accuracy and efficiency. The results also show the effect of the incorporated priors and reflect a truncated matrix normal distribution as we expect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16130v2</guid>
      <category>q-bio.QM</category>
      <category>stat.CO</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiasen Zhang, Xi Qiao, Liangliang Zhang, Weihong Guo</dc:creator>
    </item>
  </channel>
</rss>
