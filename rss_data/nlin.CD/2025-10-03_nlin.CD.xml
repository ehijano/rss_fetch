<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>nlin.CD updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/nlin.CD</link>
    <description>nlin.CD updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/nlin.CD" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 03 Oct 2025 04:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Analysis of persistent and antipersistent time series with the Visibility Graph method</title>
      <link>https://arxiv.org/abs/2510.01202</link>
      <description>arXiv:2510.01202v1 Announce Type: new 
Abstract: In this work, we investigate a range of time series, including Gaussian noises (white, pink, and blue), stochastic processes (Ornstein-Uhlenbeck, fractional Brownian motion, and Levy flights), and chaotic systems (the logistic map), using the Visibility Graph (VG) method. We focus on the minimum number of data points required to use VG and on two key descriptors: the degree distribution P(k), which often follows a power law P(k) ~ k^-gamma, and the Hurst exponent H, which identifies persistent and antipersistent time series. While the VG method has attracted growing attention in recent years, its ability to consistently characterize time series from diverse dynamical systems remains unclear. Our analysis shows that the reliable application of the VG method requires a minimum of 1000 data points. Furthermore, we find that for time series with a Hurst exponent H &lt;= 0.5, the corresponding critical exponent satisfies gamma &gt;= 2. These results clarify the sensitivity of the VG method and provide practical guidelines for its application in the analysis of stochastic and chaotic time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01202v1</guid>
      <category>nlin.CD</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Macarena C\'adiz, Iv\'an Gallo-M\'endez, Pablo S. Moya, Denisse Past\'en</dc:creator>
    </item>
    <item>
      <title>RG theory of spontaneous stochasticity for Sabra model of turbulence</title>
      <link>https://arxiv.org/abs/2510.01204</link>
      <description>arXiv:2510.01204v1 Announce Type: new 
Abstract: We consider fluctuating Sabra models of turbulence, which exhibit the phenomenon of spontaneous stochasticity: their solutions converge to a stochastic process in the ideal limit, when both viscosity and small-scale noise vanish. In this paper, we develop a renormalization group (RG) approach to explain this phenomenon. Here, RG is understood as an exact relation between the stochastic properties of systems with different dissipative and noise terms, in contrast to the Kadanoff-Wilson coarse-graining procedure, which involves small-scale integration. We argue that the stochastic process in the ideal limit is represented as a fixed point of the RG operator. The existence of such a fixed point confirms not only the convergence in the ideal limit, but also the universality of the spontaneously stochastic process, i.e. its independence from the type of dissipation and noise. The dominant eigenmode of the linearized RG operator determines the leading correction in the convergence process. The RG eigenvalue $\rho \approx 0.84 \exp(2.28i)$ is universal and it turns out to be complex, which explains the rather slow and oscillatory convergence in the ideal limit. These universality predictions are accurately confirmed by numerical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01204v1</guid>
      <category>nlin.CD</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexei A. Mailybaev</dc:creator>
    </item>
    <item>
      <title>Quantum Signatures of Strange Attractors</title>
      <link>https://arxiv.org/abs/2510.01416</link>
      <description>arXiv:2510.01416v1 Announce Type: cross 
Abstract: In classical mechanics, driven systems with dissipation often exhibit complex, fractal dynamics known as strange attractors. This paper addresses the fundamental question of how such structures manifest in the quantum realm. We investigate the quantum Duffing oscillator, a paradigmatic chaotic system, using the Caldirola-Kanai (CK) framework, where dissipation is integrated directly into a time-dependent Hamiltonian. By employing the Husimi distribution to represent the quantum state in phase space, we present the first visualization of a quantum strange attractor within this model. Our simulations demonstrate how an initially simple Gaussian wave packet is stretched, folded, and sculpted by the interplay of chaotic dynamics and energy loss, causing it to localize onto a structure that beautifully mirrors the classical attractor. This quantum "photograph" is inherently smoothed, blurring the infinitely fine fractal details of its classical counterpart as a direct consequence of the uncertainty principle. We supplement this analysis by examining the out-of-time-ordered correlator (OTOC), which shows that stronger dissipation clarifies the exponential growth associated with the classical Lyapunov exponent, thereby confirming the model's semiclassical behavior. This work offers a compelling geometric perspective on open chaotic quantum systems and sheds new light on the quantum-classical transition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01416v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>nlin.CD</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bence D\'ardai, G\'abor Vattay</dc:creator>
    </item>
    <item>
      <title>Testing Stability and Robustness in Three Cryptographic Chaotic Systems</title>
      <link>https://arxiv.org/abs/2510.02184</link>
      <description>arXiv:2510.02184v1 Announce Type: cross 
Abstract: In practical applications, it is crucial that the drive-response systems, although identical in all respects, are synchronized at all times, even if there is noise present. In this work, we test the stability and robustness of three distinct and well-known cryptographic chaotic systems, and compare the results in relation to the desired security.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02184v1</guid>
      <category>cs.CR</category>
      <category>nlin.CD</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>N. A. Anagnostopoulos, K. Konstantinidis, A. N. Miliou, S. G. Stavrinides</dc:creator>
    </item>
    <item>
      <title>A Novel Approach for Estimating Largest Lyapunov Exponents in One-Dimensional Chaotic Time Series Using Machine Learning</title>
      <link>https://arxiv.org/abs/2507.04868</link>
      <description>arXiv:2507.04868v3 Announce Type: replace 
Abstract: Understanding and quantifying chaos from data remains challenging. We present a data-driven method for estimating the largest Lyapunov exponent (LLE) from one-dimensional chaotic time series using machine learning. A predictor is trained to produce out-of-sample, multi-horizon forecasts; the LLE is then inferred from the exponential growth of the geometrically averaged forecast error (GMAE) across the horizon, which serves as a proxy for trajectory divergence. We validate the approach on four canonical 1D maps-logistic, sine, cubic, and Chebyshev-achieving R2pos &gt; 0.99 against reference LLE curves with series as short as M = 450. Among baselines, KNN yields the closest fits (KNN-R comparable; RF larger deviations). By design the estimator targets positive exponents: in periodic/stable regimes it returns values indistinguishable from zero. Noise robustness is assessed by adding zero-mean white measurement noise and summarizing performance versus the average SNR over parameter sweeps: accuracy saturates for SNRm &gt; 30 dB and collapses below 27 dB, a conservative sensor-level benchmark. The method is simple, computationally efficient, and model-agnostic, requiring only stationarity and the presence of a dominant positive exponent. It offers a practical route to LLE estimation in experimental settings where only scalar time-series measurements are available, with extensions to higher-dimensional and irregularly sampled data left for future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04868v3</guid>
      <category>nlin.CD</category>
      <category>cs.AI</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1063/5.0289352</arxiv:DOI>
      <dc:creator>A. Velichko, M. Belyaev, P. Boriskov</dc:creator>
    </item>
    <item>
      <title>Learning Beyond Experience: Generalizing to Unseen State Space with Reservoir Computing</title>
      <link>https://arxiv.org/abs/2506.05292</link>
      <description>arXiv:2506.05292v2 Announce Type: replace-cross 
Abstract: Machine learning techniques offer an effective approach to modeling dynamical systems solely from observed data. However, without explicit structural priors -- built-in assumptions about the underlying dynamics -- these techniques typically struggle to generalize to aspects of the dynamics that are poorly represented in the training data. Here, we demonstrate that reservoir computing -- a simple, efficient, and versatile machine learning framework often used for data-driven modeling of dynamical systems -- can generalize to unexplored regions of state space without explicit structural priors. First, we describe a multiple-trajectory training scheme for reservoir computers that supports training across a collection of disjoint time series, enabling effective use of available training data. Then, applying this training scheme to multistable dynamical systems, we show that RCs trained on trajectories from a single basin of attraction can achieve out-of-domain generalization by capturing system behavior in entirely unobserved basins.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05292v2</guid>
      <category>cs.LG</category>
      <category>math.DS</category>
      <category>nlin.CD</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Declan A. Norton, Yuanzhao Zhang, Michelle Girvan</dc:creator>
    </item>
  </channel>
</rss>
