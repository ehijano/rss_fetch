<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 03 Mar 2025 05:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Robust statistical inference for accelerated life-tests with one-shot devices under log-logistic distributions</title>
      <link>https://arxiv.org/abs/2502.20467</link>
      <description>arXiv:2502.20467v1 Announce Type: new 
Abstract: A one-shot device is a unit that operates only once, after which it is either destroyed or needs to be rebuilt. For this type of device, the operational status can only be assessed at a specific inspection time, determining whether failure occurred before or after it. Consequently, lifetimes are subject to left- or right-censoring. One-shot devices are usually highly reliables. To analyze the reliability of such products, an accelerated life test (ALT) plan is typically employed by subjecting the devices to increased levels of stress factors, thus allowing life characteristics observed under high-stress conditions to be extrapolated to normal operating conditions. By accelerating the degradation process, ALT significantly reduces both the time required for testing and the associated experimental costs.
  Recently, robust inferential methods have gained considerable interest in statistical analysis. Among them, weighted minimum density power divergence estimators (WMDPDEs) are widely recognized for their robust statistical properties with small loss of efficiency. In this work, robust WMDPDE and associated statistical tests are developed under a log-logistic lifetime distribution with multiple stresses. Explicit expressions for the estimating equations and asymptotic distribution of the estimators are obtained. Further, a Monte Carlo simulation study is presented to evaluate the performance of the WMDPDE in practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20467v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mar\'ia Gonz\'alez-Calder\'on, Mar\'ia Jaenada, Leandro Pardo</dc:creator>
    </item>
    <item>
      <title>Characterizing the Training-Conditional Coverage of Full Conformal Inference in High Dimensions</title>
      <link>https://arxiv.org/abs/2502.20579</link>
      <description>arXiv:2502.20579v1 Announce Type: new 
Abstract: We study the coverage properties of full conformal regression in the proportional asymptotic regime where the ratio of the dimension and the sample size converges to a constant. In this setting, existing theory tells us only that full conformal inference is unbiased, in the sense that its average coverage lies at the desired level when marginalized over both the new test point and the training data. Considerably less is known about the behaviour of these methods conditional on the training set. As a result, the exact benefits of full conformal inference over much simpler alternative methods is unclear. This paper investigates the behaviour of full conformal inference and natural uncorrected alternatives for a broad class of $L_2$-regularized linear regression models. We show that in the proportional asymptotic regime the training-conditional coverage of full conformal inference concentrates at the target value. On the other hand, simple alternatives that directly compare test and training residuals realize constant undercoverage bias. While these results demonstrate the necessity of full conformal in correcting for high-dimensional overfitting, we also show that this same methodology is redundant for the related task of tuning the regularization level. In particular, we show that full conformal inference still yields asymptotically valid coverage when the regularization level is selected using only the training set, without consideration of the test point. Simulations show that our asymptotic approximations are accurate in finite samples and can be readily extended to other popular full conformal variants, such as full conformal quantile regression and the LASSO, that do not directly meet our assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20579v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Isaac Gibbs, Emmanuel J. Cand\`es</dc:creator>
    </item>
    <item>
      <title>Minimax Optimal Kernel Two-Sample Tests with Random Features</title>
      <link>https://arxiv.org/abs/2502.20755</link>
      <description>arXiv:2502.20755v1 Announce Type: new 
Abstract: Reproducing Kernel Hilbert Space (RKHS) embedding of probability distributions has proved to be an effective approach, via MMD (maximum mean discrepancy) for nonparametric hypothesis testing problems involving distributions defined over general (non-Euclidean) domains. While a substantial amount of work has been done on this topic, only recently, minimax optimal two-sample tests have been constructed that incorporate, unlike MMD, both the mean element and a regularized version of the covariance operator. However, as with most kernel algorithms, the computational complexity of the optimal test scales cubically in the sample size, limiting its applicability. In this paper, we propose a spectral regularized two-sample test based on random Fourier feature (RFF) approximation and investigate the trade-offs between statistical optimality and computational efficiency. We show the proposed test to be minimax optimal if the approximation order of RFF (which depends on the smoothness of the likelihood ratio and the decay rate of the eigenvalues of the integral operator) is sufficiently large. We develop a practically implementable permutation-based version of the proposed test with a data-adaptive strategy for selecting the regularization parameter and the kernel. Finally, through numerical experiments on simulated and benchmark datasets, we demonstrate that the proposed RFF-based test is computationally efficient and performs almost similar (with a small drop in power) to the exact test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20755v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soumya Mukherjee, Bharath K. Sriperumbudur</dc:creator>
    </item>
    <item>
      <title>Location Characteristics of Conditional Selective Confidence Intervals via Polyhedral Methods</title>
      <link>https://arxiv.org/abs/2502.20917</link>
      <description>arXiv:2502.20917v1 Announce Type: new 
Abstract: We examine the location characteristics of a conditional selective confidence interval based on the polyhedral method. This interval is constructed from the distribution of a test statistic conditional upon the event of statistical significance. In the case of a one-sided test, the behavior of the interval varies depending on whether the parameter is highly significant or only marginally significant. When the parameter is highly significant, the interval is similar to the usual confidence interval derived without considering selection. However, when the parameter is only marginally significant, the interval falls into an extreme range and deviates greatly from the estimated value of the parameter. In contrast, an interval conditional on two-sided significance does not yield extreme results, although it may exclude the estimated parameter value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20917v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Andreas Dzemski, Ryo Okui, Wenjie Wang</dc:creator>
    </item>
    <item>
      <title>Modeling discrete common-shock risks through matrix distributions</title>
      <link>https://arxiv.org/abs/2502.21172</link>
      <description>arXiv:2502.21172v1 Announce Type: new 
Abstract: We introduce a novel class of bivariate common-shock discrete phase-type (CDPH) distributions to describe dependencies in loss modeling, with an emphasis on those induced by common shocks. By constructing two jointly evolving terminating Markov chains that share a common evolution up to a random time corresponding to the common shock component, and then proceed independently, we capture the essential features of risk events influenced by shared and individual-specific factors. We derive explicit expressions for the joint distribution of the termination times and prove various class and distributional properties, facilitating tractable analysis of the risks. Extending this framework, we model random sums where aggregate claims are sums of continuous phase-type random variables with counts determined by these termination times, and show that their joint distribution belongs to the multivariate phase-type or matrix-exponential class. We develop estimation procedures for the CDPH distributions using the expectation-maximization algorithm and demonstrate the applicability of our models through simulation studies and an application to bivariate insurance claim frequency data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21172v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Bladt, Eric C. K. Cheung, Oscar Peralta, Jae-Kyung Woo</dc:creator>
    </item>
    <item>
      <title>Halfspace Representations of Path Polytopes of Trees</title>
      <link>https://arxiv.org/abs/2502.21204</link>
      <description>arXiv:2502.21204v1 Announce Type: cross 
Abstract: Given a tree $T$, its path polytope is the convex hull of the edge indicator vectors for the paths between any two distinct leaves in $T$. These polytopes arise naturally in polyhedral geometry and applications, such as phylogenetics, tropical geometry, and algebraic statistics. We provide a minimal halfspace representation of these polytopes. The construction is made inductively using toric fiber products.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21204v1</guid>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amer Goel, Aida Maraj, Alvaro Ribot</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Random Unknowns via Modifications of Extended Likelihood</title>
      <link>https://arxiv.org/abs/2310.09955</link>
      <description>arXiv:2310.09955v3 Announce Type: replace 
Abstract: Fisher's likelihood is widely used for statistical inference for fixed unknowns. This paper aims to extend two important likelihood-based methods, namely the maximum likelihood procedure for point estimation and the confidence procedure for interval estimation, to embrace a broader class of statistical models with additional random unknowns. We propose the new h-likelihood and the h-confidence by modifying extended likelihoods. Maximization of the h-likelihood yields both maximum likelihood estimators of fixed unknowns and asymptotically optimal predictors for random unknowns, achieving the generalized Cram\'er-Rao lower bound. The h-likelihood further offers advantages in scalability for large datasets and complex models. The h-confidence could yield a valid interval estimation and prediction by maintaining the coverage probability for both fixed and random unknowns in small samples. We study approximate methods for the h-likelihood and h-confidence, which can be applied to a general class of models with additional random unknowns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.09955v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hangbin Lee, Youngjo Lee</dc:creator>
    </item>
    <item>
      <title>Bootstrapping Fisher Market Equilibrium and First-Price Pacing Equilibrium</title>
      <link>https://arxiv.org/abs/2402.02303</link>
      <description>arXiv:2402.02303v5 Announce Type: replace 
Abstract: The linear Fisher market (LFM) is a basic equilibrium model from economics, which also has applications in fair and efficient resource allocation. First-price pacing equilibrium (FPPE) is a model capturing budget-management mechanisms in first-price auctions. In certain practical settings such as advertising auctions, there is an interest in performing statistical inference over these models. A popular methodology for general statistical inference is the bootstrap procedure. Yet, for LFM and FPPE there is no existing theory for the valid application of bootstrap procedures. In this paper, we introduce and devise several statistically valid bootstrap inference procedures for LFM and FPPE. The most challenging part is to bootstrap general FPPE, which reduces to bootstrapping constrained M-estimators, a largely unexplored problem. We devise a bootstrap procedure for FPPE under mild degeneracy conditions by using the powerful tool of epi-convergence theory. Experiments with synthetic and semi-real data verify our theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02303v5</guid>
      <category>math.ST</category>
      <category>cs.GT</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luofeng Liao, Christian Kroer</dc:creator>
    </item>
    <item>
      <title>Glivenko-Cantelli classes for real-valued empirical functions of stationary $\alpha$-mixing and $\beta$-mixing sequences</title>
      <link>https://arxiv.org/abs/2502.20206</link>
      <description>arXiv:2502.20206v2 Announce Type: replace 
Abstract: In this paper, we extend the classical Glivenko-Cantelli theorem to real-valued empirical functions under dependence structures characterized by $\alpha$-mixing and $\beta$-mixing conditions. We investigate sufficient conditions ensuring that families of real-valued functions exhibit the Glivenko-Cantelli (GC) property in these dependent settings. Our analysis focuses on function classes satisfying uniform entropy conditions and establishes deviation bounds under mixing coefficients that decay at appropriate rates. Our findings refine existing literature by relaxing independence assumptions and highlighting the role of dependence in empirical process convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20206v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ousmane Coulibaly, Harouna Sangar\'e</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Knockoffs Inference for Time Series Data</title>
      <link>https://arxiv.org/abs/2112.09851</link>
      <description>arXiv:2112.09851v3 Announce Type: replace-cross 
Abstract: We make some initial attempt to establish the theoretical and methodological foundation for the model-X knockoffs inference for time series data. We suggest the method of time series knockoffs inference (TSKI) by exploiting the ideas of subsampling and e-values to address the difficulty caused by the serial dependence. We also generalize the robust knockoffs inference in Barber, Cand\`es, and Samworth to the time series setting to relax the assumption of known covariate distribution required by model-X knockoffs, since such an assumption is overly stringent for time series data. We establish sufficient conditions under which TSKI achieves the asymptotic false discovery rate (FDR) control. Our technical analysis reveals the effects of serial dependence and unknown covariate distribution on the FDR control. We conduct a power analysis of TSKI using the Lasso coefficient difference knockoff statistic under the generalized linear time series models. The finite-sample performance of TSKI is illustrated with several simulation examples and an economic inflation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.09851v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Journal of the American Statistical Association 2025</arxiv:journal_reference>
      <dc:creator>Chien-Ming Chi, Yingying Fan, Ching-Kang Ing, Jinchi Lv</dc:creator>
    </item>
    <item>
      <title>Semidiscrete optimal transport with unknown costs</title>
      <link>https://arxiv.org/abs/2310.00786</link>
      <description>arXiv:2310.00786v3 Announce Type: replace-cross 
Abstract: Semidiscrete optimal transport is a challenging generalization of the classical transportation problem in linear programming. The goal is to design a joint distribution for two random variables (one continuous, one discrete) with fixed marginals, in a way that minimizes expected cost. We formulate a novel variant of this problem in which the cost functions are unknown, but can be learned through noisy observations; however, only one function can be sampled at a time. We develop a semi-myopic algorithm that couples online learning with stochastic approximation, and prove that it achieves optimal convergence rates, despite the non-smoothness of the stochastic gradient and the lack of strong concavity in the objective function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.00786v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinchu Zhu, Ilya O. Ryzhov</dc:creator>
    </item>
    <item>
      <title>Identification by non-Gaussianity in structural threshold and smooth transition vector autoregressive models</title>
      <link>https://arxiv.org/abs/2404.19707</link>
      <description>arXiv:2404.19707v3 Announce Type: replace-cross 
Abstract: Linear structural vector autoregressive models can be identified statistically without imposing restrictions on the model if the shocks are mutually independent and at most one of them is Gaussian. We show that this result extends to structural threshold and smooth transition vector autoregressive models incorporating a time-varying impact matrix defined as a weighted sum of the impact matrices of the regimes. We also discuss the problem of labelling the shocks, estimation of the parameters, and stationarity the model. The introduced methods are implemented to the accompanying R package sstvars. Our empirical application studies the effects of the climate policy uncertainty shock on the U.S. macroeconomy. In a structural logistic smooth transition vector autoregressive model consisting of two regimes, we find that a positive climate policy uncertainty shock decreases production and increases inflation in times of both low and high economic policy uncertainty, but its inflationary effects are stronger in the periods of high economic policy uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19707v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Savi Virolainen</dc:creator>
    </item>
    <item>
      <title>Revisiting Optimal Proportions for Binary Responses: Insights from Incorporating the Absent Perspective of Type-I Error Rate Control</title>
      <link>https://arxiv.org/abs/2502.06381</link>
      <description>arXiv:2502.06381v2 Announce Type: replace-cross 
Abstract: This work revisits optimal response-adaptive designs from a type-I error rate perspective, highlighting when and how much these allocations exacerbate type-I error rate inflation - an issue previously undocumented. We explore a range of approaches from the literature that can be applied to reduce type-I error rate inflation. However, we found that all of these approaches fail to give a robust solution to the problem. To address this, we derive two optimal proportions, incorporating the more robust score test (instead of the Wald test) with finite sample estimators (instead of the unknown true values) in the formulation of the optimization problem. One proportion optimizes statistical power and the other minimizes the total number failures in a trial while maintaining a predefined power level. Through simulations based on an early-phase and a confirmatory trial we provide crucial practical insight into how these new optimal proportion designs can offer substantial patient outcomes advantages while controlling type-I error rate. While we focused on binary outcomes, the framework offers valuable insights that naturally extend to other outcome types, multi-armed trials and alternative measures of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06381v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Pin, Sof\'ia S. Villar, William F. Rosenberger</dc:creator>
    </item>
    <item>
      <title>Tensor Product Neural Networks for Functional ANOVA Model</title>
      <link>https://arxiv.org/abs/2502.15215</link>
      <description>arXiv:2502.15215v3 Announce Type: replace-cross 
Abstract: Interpretability for machine learning models is becoming more and more important as machine learning models become more complex. The functional ANOVA model, which decomposes a high-dimensional function into a sum of lower dimensional functions (commonly referred to as components), is one of the most popular tools for interpretable AI, and recently, various neural networks have been developed for estimating each component in the functional ANOVA model. However, such neural networks are highly unstable when estimating each component since the components themselves are not uniquely defined. That is, there are multiple functional ANOVA decompositions for a given function. In this paper, we propose a novel neural network which guarantees a unique functional ANOVA decomposition and thus is able to estimate each component stably and accurately. We call our proposed neural network ANOVA Tensor Product Neural Network (ANOVA-TPNN) since it is motivated by the tensor product basis expansion. Theoretically, we prove that ANOVA-TPNN can approximate any smooth function well. Empirically, we show that ANOVA-TPNN provide much more stable estimation of each component and thus much more stable interpretation when training data and initial values of the model parameters vary than existing neural networks do.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15215v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seokhun Park, Insung Kong, Yongchan Choi, Chanmoo Park, Yongdai Kim</dc:creator>
    </item>
  </channel>
</rss>
