<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 08 Jan 2026 05:01:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Progressive Bayesian Confidence Architectures for Cold-Start Personal Health Analytics: Formalizing Early Insight Through Posterior Contraction and Risk-Aware Interpretation</title>
      <link>https://arxiv.org/abs/2601.03299</link>
      <description>arXiv:2601.03299v1 Announce Type: new 
Abstract: Personal health analytics systems face a persistent cold-start dilemma: users expect meaningful insights early in data collection, while conventional statistical inference requires data volumes that often exceed engagement horizons. Existing approaches either delay inference until fixed statistical thresholds are met -- leading to user disengagement -- or surface heuristic insights without formal uncertainty quantification, risking false confidence. We propose a progressive Bayesian confidence architecture that formalizes early-stage inference through phased interpretation of posterior uncertainty. Drawing on Bayesian updating and epistemic strategies from financial risk modeling under sparse observations, we map posterior contraction to interpretable tiers of insight, ranging from exploratory directional evidence to robust associative inference. We demonstrate the framework's performance through controlled experimentation with synthetic N-of-1 health data, showing that calibrated early insights can be generated within 5--7 days while maintaining explicit epistemic humility. Compared to fixed-threshold baselines requiring 30+ days of data, the proposed approach yields earlier directional signals (mean: 5.3 vs 31.7 days, p&lt;0.001) while controlling false discovery rates below 6% (5.9% at day 30) despite 26-day earlier detection, compared to 0% FDR for fixed-threshold baselines that delay insights by 30 days. In addition, we show strong uncertainty calibration (76% credible interval coverage for ground-truth correlations at day 90). This work contributes a methodological framework for uncertainty-aware early inference in personalized health analytics that bridges the gap between user engagement requirements and statistical rigor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03299v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>q-bio.QM</category>
      <category>stat.TH</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Richik Chakraborty</dc:creator>
    </item>
    <item>
      <title>On estimands in target trial emulation</title>
      <link>https://arxiv.org/abs/2601.03377</link>
      <description>arXiv:2601.03377v1 Announce Type: new 
Abstract: The target trial framework enables causal inference from longitudinal observational data by emulating randomized trials initiated at multiple time points. Precision is often improved by pooling information across trials, with standard models typically assuming - among other things - a time-constant treatment effect. However, this obscures interpretation when the true treatment effect varies, which we argue to be likely as a result of relying on noncollapsible estimands. To address these challenges, this paper introduces a model-free strategy for target trial analysis, centered around the choice of the estimand, rather than model specification. This ensures that treatment effects remain clearly interpretable for well-defined populations even under model misspecification. We propose estimands suitable for different study designs, and develop accompanying G-computation and inverse probability weighted estimators. Applications on simulations and real data on antimicrobial de-escalation in an intensive care unit setting demonstrate the greater clarity and reliability of the proposed methodology over traditional techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03377v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edoardo Efrem Gervasoni, Liesbet De Bus, Stijn Vansteelandt, Oliver Dukes</dc:creator>
    </item>
    <item>
      <title>Measures of classification bias derived from sample size analysis</title>
      <link>https://arxiv.org/abs/2601.03453</link>
      <description>arXiv:2601.03453v1 Announce Type: new 
Abstract: We propose the use of a simple intuitive principle for measuring algorithmic classification bias: the significance of the differences in a classifier's error rates across the various demographics is inversely commensurate with the sample size required to statistically detect them. That is, if large sample sizes are required to statistically establish biased behavior, the algorithm is less biased, and vice versa. In a simple setting, we assume two distinct demographics, and non-parametric estimates of the error rates on them, e1 and e2, respectively. We use a well-known approximate formula for the sample size of the chi-squared test, and verify some basic desirable properties of the proposed measure. Next, we compare the proposed measure with two other commonly used statistics, the difference e2-e1 and the ratio e2/e1 of the error rates. We establish that the proposed measure is essentially different in that it can rank algorithms for bias differently, and we discuss some of its advantages over the other two measures. Finally, we briefly discuss how some of the desirable properties of the proposed measure emanate from fundamental characteristics of the method, rather than the approximate sample size formula we used, and thus, are expected to hold in more complex settings with more than two demographics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03453v1</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ioannis Ivrissimtzis, Shauna Concannon, Matthew Houliston, Graham Roberts</dc:creator>
    </item>
    <item>
      <title>Improving operating characteristics of clinical trials by augmenting control arm using propensity score-weighted borrowing-by-parts power prior</title>
      <link>https://arxiv.org/abs/2601.03480</link>
      <description>arXiv:2601.03480v1 Announce Type: new 
Abstract: Borrowing external data can improve estimation efficiency but may introduce bias when populations differ in covariate distributions or outcome variability. A proper balance needs to be maintained between the two datasets to justify the borrowing. We propose a propensity score weighting borrowing-by-parts power prior (PSW-BPP) that integrates causal covariate adjustment through propensity score weighting with a flexible Bayesian borrowing approach to address these challenges in a unified framework. The proposed approach first applies propensity score weighting to align the covariate distribution of the external data with that of the current study, thereby targeting a common estimand and reducing confounding due to population heterogeneity. The weighted external likelihood is then incorporated into a Bayesian model through a borrowing-by-parts power prior, which allows distinct power parameters for the mean and variance components of the likelihood, enabling differential and calibrated information borrowing. Additionally, we adopt the idea of the minimal plausibility index (mPI) to calculate the power parameters. This separate borrowing provides greater robustness to prior-data conflict compared with traditional power prior methods that impose a single borrowing parameter. We study the operating characteristics of PSW-BPP through extensive simulation and a real data example. Simulation studies demonstrate that PSW-BPP yields more efficient and stable estimation than no borrowing and fixed borrowing, particularly under moderate covariate imbalance and outcome heterogeneity. The proposed framework offers a principled and extensible methodological contribution for Bayesian inference with external data in observational and hybrid study designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03480v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Apu Chandra Das (for the Alzheimer Disease Neuroimaging Initiative), Sakib Salam (for the Alzheimer Disease Neuroimaging Initiative), Aninda Roy (for the Alzheimer Disease Neuroimaging Initiative), Rakhi Chowdhury (for the Alzheimer Disease Neuroimaging Initiative), Antar Chandra Das (for the Alzheimer Disease Neuroimaging Initiative), Ashim Chandra Das (for the Alzheimer Disease Neuroimaging Initiative)</dc:creator>
    </item>
    <item>
      <title>Differentially Private Bayesian Inference for Gaussian Copula Correlations</title>
      <link>https://arxiv.org/abs/2601.03497</link>
      <description>arXiv:2601.03497v1 Announce Type: new 
Abstract: Gaussian copulas are widely used to estimate multivariate distributions and relationships. We present algorithms for estimating Gaussian copula correlations that ensure differential privacy. We first convert data values into sets of two-way tables of counts above and below marginal medians. We then add noise to these counts to satisfy differential privacy. We utilize the one-to-one correspondence between the true counts and the copula correlation to estimate a posterior distribution of the copula correlation given the noisy counts, marginalizing over the distribution of the underlying true counts using a composite likelihood. We also present an alternative, maximum likelihood approach for point estimation. Using simulation studies, we compare these methods to extant methods in the literature for computing differentially private copula correlations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03497v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuo Wang, Joseph Feldman, Jerome P. Reiter</dc:creator>
    </item>
    <item>
      <title>Propagating Surrogate Uncertainty in Bayesian Inverse Problems</title>
      <link>https://arxiv.org/abs/2601.03532</link>
      <description>arXiv:2601.03532v1 Announce Type: new 
Abstract: Standard Bayesian inference schemes are infeasible for inverse problems with computationally expensive forward models. A common solution is to replace the model with a cheaper surrogate. To avoid overconfident conclusions, it is essential to acknowledge the surrogate approximation by propagating its uncertainty. At present, a variety of distinct uncertainty propagation methods have been suggested, with little understanding of how they vary. To fill this gap, we propose a mixture distribution termed the expected posterior (EP) as a general baseline for uncertainty-aware posterior approximation, justified by decision theoretic and modular Bayesian inference arguments. We then investigate the expected unnormalized posterior (EUP), a popular heuristic alternative, analyzing when it may deviate from the EP baseline. Our results show that this heuristic can break down when the surrogate uncertainty is highly non-uniform over the design space, as can be the case when the log-likelihood is emulated by a Gaussian process. Finally, we present the random kernel preconditioned Crank-Nicolson (RKpCN) algorithm, an approximate Markov chain Monte Carlo scheme that provides practical EP approximation in the challenging setting involving infinite-dimensional Gaussian process surrogates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03532v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Gerard Roberts, Michael Dietze, Jonathan Huggins</dc:creator>
    </item>
    <item>
      <title>Small area estimation of dependent extreme value indices</title>
      <link>https://arxiv.org/abs/2601.03647</link>
      <description>arXiv:2601.03647v1 Announce Type: new 
Abstract: In extreme value analysis, tail behavior of a heavy-tailed data distribution is modeled by a Pareto-type distribution in which the so-called extreme value index (EVI) controls the tail behavior. For heavy-tailed data obtained from multiple population subgroups, or areas, this study efficiently predicts the EVIs of all areas using information among areas. For this purpose, we propose a mixed effects model, which is a useful approach in small area estimation. In this model, we represent differences among areas in the EVIs by latent variables called random effects. Using correlated random effects across areas, we incorporate the relations among areas into the model. The obtained model achieves simultaneous prediction of EVIs of all areas. Herein, we describe parameter estimation and random effect prediction in the model, and clarify theoretical properties of the estimator. Additionally, numerical experiments are presented to demonstrate the effectiveness of the proposed method. As an application of our model, we provide a risk assessment of heavy rainfall in Japan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03647v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Koki Momoki, Takuma Yoshida</dc:creator>
    </item>
    <item>
      <title>Multi-transport Distributional Regression</title>
      <link>https://arxiv.org/abs/2601.03674</link>
      <description>arXiv:2601.03674v1 Announce Type: new 
Abstract: We study distribution-on-distribution regression problems in which a response distribution depends on multiple distributional predictors. Such settings arise naturally in applications where the outcome distribution is driven by several heterogeneous distributional sources, yet remain challenging due to the nonlinear geometry of the Wasserstein space. We propose an intrinsic regression framework that aggregates predictor-specific transported distributions through a weighted Fr\'echet mean in the Wasserstein space. The resulting model admits multiple distributional predictors, assigns interpretable weights quantifying their relative contributions, and defines a flexible regression operator that is invariant to auxiliary construction choices, such as the selection of a reference distribution. From a theoretical perspective, we establish identifiability of the induced regression operator and derive asymptotic guarantees for its estimation under a predictive Wasserstein semi-norm, which directly characterizes convergence of the composite prediction map. Extensive simulation studies and a real data application demonstrate the improved predictive performance and interpretability of the proposed approach compared with existing Wasserstein regression methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03674v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuanying Chen, Tongyu Li, Yang Bai, Zhenhua Lin</dc:creator>
    </item>
    <item>
      <title>Maximum smoothed likelihood method for the combination of multiple diagnostic tests, with application to the ROC estimation</title>
      <link>https://arxiv.org/abs/2601.03675</link>
      <description>arXiv:2601.03675v1 Announce Type: new 
Abstract: In medical diagnostics, leveraging multiple biomarkers can significantly improve classification accuracy compared to using a single biomarker. While existing methods based on exponential tilting or density ratio models have shown promise, their assumptions may be overly restrictive in practice. In this paper, we adopt a flexible semiparametric model that relates the density ratio of diseased to healthy subjects through an unknown monotone transformation of a linear combination of biomarkers. To enhance estimation efficiency, we propose a smoothed likelihood framework that exploits the smoothness in the underlying densities and transformation function. Building on the maximum smoothed likelihood methodology, we construct estimators for the model parameters and the associated probability density functions. We develop an effective computational algorithm for implementation, derive asymptotic properties of the proposed estimators, and establish procedures for estimating the receiver operating characteristic (ROC) curve and the area under the curve (AUC). Through simulation studies and a real-data application, we demonstrate that the proposed method yields more accurate and efficient estimates than existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03675v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Fangyong Zheng, Pengfei Li, Tao Yu</dc:creator>
    </item>
    <item>
      <title>Non-Homogeneous Markov-Switching Generalized Additive Models for Location, Scale, and Shape</title>
      <link>https://arxiv.org/abs/2601.03760</link>
      <description>arXiv:2601.03760v1 Announce Type: new 
Abstract: We propose an extension of Markov-switching generalized additive models for location, scale, and shape (MS-GAMLSS) that allows covariates to influence not only the parameters of the state-dependent distributions but also the state transition probabilities. Traditional MS-GAMLSS, which combine distributional regression with hidden Markov models, typically assume time-homogeneous (i.e., constant) transition probabilities, thereby preventing regime shifts from responding to covariate-driven changes. Our approach overcomes this limitation by modeling the transition probabilities as smooth functions of covariates, enabling a flexible, data-driven characterization of covariate-dependent regime dynamics. Estimation is carried out within a penalized likelihood framework, where automatic smoothness selection controls model complexity and guards against overfitting. We evaluate the proposed methodology through simulations and applications to daily Lufthansa stock prices and Spanish energy prices. Our results show that incorporating macroeconomic indicators into the transition probabilities yields additional insights into market dynamics. Data and R code to reproduce the results are available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03760v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katharina Ammann, Timo Adam, Jan-Ole Koslik</dc:creator>
    </item>
    <item>
      <title>Multi-agent Optimization of Non-cooperative Multimodal Mobility Systems</title>
      <link>https://arxiv.org/abs/2601.03777</link>
      <description>arXiv:2601.03777v1 Announce Type: new 
Abstract: While multimodal mobility systems have the potential to bring many benefits to travelers, drivers, the environment, and traffic congestion, such systems typically involve multiple non-cooperative decision-makers who may selfishly optimize their own objectives without considering the overall system benefits. This paper aims to investigate market-based interactions of travelers and ride-sourcing drivers in the context of multimodal mobility systems. We propose a unified mathematical modeling framework to capture the decentralized travelers and drivers' decision-making process and balance the network's demand and supply by equilibrium pricing. Such a model allows analyses of the impact of decentralized decision-making on multimodal mobility efficiencies. The proposed formulation can be further convexified to efficiently compute the equilibrium ride-sourcing prices. We conduct numerical experiments on different settings of transportation networks to gain policy insights. We find that travelers prefer ride-sourcing and multimodal transportation more than the driving option when they are more sensitive to prices. We also find that travelers may need to be subsidized to use multimodal transportation when there is fewer transit hubs in the network or, ride-sourcing drivers become too sensitive to the prices. However, we find that more transit hubs in the network increases the total empty VMT of ride-sourcing drivers by increasing the total relocation time. The proposed model can be used by policymakers and platform operators to design pricing and subsidy schemes that align individual decision-making with system-level efficiency and evaluate the trade-offs between accessibility and environmental impacts in multimodal transportation networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03777v1</guid>
      <category>stat.ME</category>
      <category>cs.SY</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md Nafees Fuad Rafi, Zhaomiao Guo</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Precision Matrix Quadratic Forms: Estimation Framework for $p &gt; n$</title>
      <link>https://arxiv.org/abs/2601.03815</link>
      <description>arXiv:2601.03815v1 Announce Type: new 
Abstract: We propose a novel estimation framework for quadratic functionals of precision matrices in high-dimensional settings, particularly in regimes where the feature dimension $p$ exceeds the sample size $n$. Traditional moment-based estimators with bias correction remain consistent when $p&lt;n$ (i.e., $p/n \to c &lt;1$). However, they break down entirely once $p&gt;n$, highlighting a fundamental distinction between the two regimes due to rank deficiency and high-dimensional complexity. Our approach resolves these issues by combining a spectral-moment representation with constrained optimization, resulting in consistent estimation under mild moment conditions.
  The proposed framework provides a unified approach for inference on a broad class of high-dimensional statistical measures. We illustrate its utility through two representative examples: the optimal Sharpe ratio in portfolio optimization and the multiple correlation coefficient in regression analysis. Simulation studies demonstrate that the proposed estimator effectively overcomes the fundamental $p&gt;n$ barrier where conventional methods fail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03815v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shizhe Hong, Weiming Li, Guangming Pan</dc:creator>
    </item>
    <item>
      <title>Asymptotic distribution of the likelihood ratio test statistic with inequality-constrained nuisance parameters</title>
      <link>https://arxiv.org/abs/2601.03909</link>
      <description>arXiv:2601.03909v1 Announce Type: new 
Abstract: The asymptotic distribution of the likelihood-ratio statistic for testing parameters on the boundary is well known to be a chi-squared mixture. The mixture weights have been shown to correspond to the intrinsic volumes of an associated tangent cone, unifying a wide range of previously isolated special cases. While the weights are fully understood for an arbitrary number of parameters of interest on the boundary, much less is known when nuisance parameters are also constrained to the boundary, a situation that frequently arises in applications. We provide the first general characterization of the asymptotic distribution of the likelihood-ratio test statistic when both the number of parameters of interest and the number of nuisance parameters on the boundary are arbitrary. We analyze how the cone geometry changes when moving from a problem with K parameters of interest on the boundary to one with K-m parameters of interest and m nuisances. In the orthogonal case we show that the resulting change in the chi-bar weights admits a closed-form difference pattern that redistributes probability mass across adjacent degrees of freedom, and that this pattern remains the dominant component of the weight shift under arbitrary covariance structures when the nuisance vector is one-dimensional. For a generic number of nuisance parameters, we introduce a new rank-based aggregation of intrinsic volumes that yields an accurate approximation of the mixture weights. Comprehensive simulations support the theory and demonstrate the accuracy of the proposed approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03909v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Clara Bertinelli Salucci</dc:creator>
    </item>
    <item>
      <title>Online robust covariance matrix estimation and outlier detection</title>
      <link>https://arxiv.org/abs/2601.03957</link>
      <description>arXiv:2601.03957v1 Announce Type: new 
Abstract: Robust estimation of the covariance matrix and detection of outliers remain major challenges in statistical data analysis, particularly when the proportion of contaminated observations increases with the size of the dataset. Outliers can severely bias parameter estimates and induce a masking effect, whereby some outliers conceal the presence of other outliers, further complicating their detection. Although many approaches have been proposed for covariance estimation and outlier detection, to our knowledge, none of these methods have been implemented in an online setting. In this paper, we focus on online covariance matrix estimation and outlier detection. Specifically, we propose a new method for simultaneously and online estimating the geometric median and variance, which allows us to calculate the Mahalanobis distance for each incoming data point before deciding whether it should be considered an outlier. To mitigate the masking effect, robust estimation techniques for the mean and variance are required. Our approach uses the geometric median for robust estimation of the location and the median covariance matrix for robust estimation of the dispersion parameters. The new online methods proposed for parameter estimation and outlier detection allow real-time identification of outliers as data are observed sequentially. The performance of our methods is demonstrated on simulated datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03957v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Guillot, Antoine Godichon-Baggioni, St\'ephane Robin, Laure Sansonnet</dc:creator>
    </item>
    <item>
      <title>On the estimation of inclusion probabilities for weighted analyses of nested case control studies</title>
      <link>https://arxiv.org/abs/2601.04066</link>
      <description>arXiv:2601.04066v1 Announce Type: new 
Abstract: Nested case-control (NCC) studies are a widely adopted design in epidemiology to investigate exposure-disease relationships. This paper examines weighted analyses in NCC studies, focusing on two prominent weighting methods: Kaplan-Meier (KM) weights and Generalized Additive Model (GAM) weights. We consider three target estimands: log-hazard ratios, conditional survival, and associations between exposures. While KM- and GAM-weights are generally robust, we identify specific scenarios where they can lead to biased estimates. We demonstrate that KM-weights can lead to biased estimates when a proportion of the originating cohort is effectively ineligible for NCC selection, particularly with small case proportions or numerous matching factors. Instead, GAM-weights can yield biased results if interactions between matching factors influence disease risk and are not adequately incorporated into weight calculation. Using Directed Acyclic Graphs (DAGs), we develop a framework to systematically determine which variables should be included in weight calculations. We show that the optimal set of variables depends on the target estimand and the causal relationships between matching factors, exposures, and disease risk. We illustrate our findings with both synthetic and real data from the European Prospective Investigation into Cancer and nutrition (EPIC) study. Additionally, we extend the application of GAM-weights to "untypical" NCC studies, where only a subset of cases are included. Our work provides crucial insights for conducting accurate and robust weighted analyses in NCC studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04066v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomeu L\'opez-Nieto-Veitch, Rossella De Sabbata, Ryung Kim, Sven Ove Samuelsen, Nathalie C. St{\o}er, Vivian Viallon</dc:creator>
    </item>
    <item>
      <title>Prediction Intervals for Interim Events in Randomized Clinical Trials with Time-to-Event Endpoints</title>
      <link>https://arxiv.org/abs/2601.04192</link>
      <description>arXiv:2601.04192v1 Announce Type: new 
Abstract: Time-to-event endpoints are central to evaluate treatment efficacy across many disease areas. Many trial protocols include interim analyses within group-sequential designs that control type I error via spending functions or boundary methods. The corresponding operating characteristics depend on the number of looks and the information accrued. Planning interim analyses with time-to-event endpoints is challenging because statistical information depends on the number of observed events. Ensuring adequate follow-up to accrue the required events is therefore critical, making interim prediction of information at scheduled looks and at the final analysis essential. While several methods have been developed to predict the calendar time required to reach a target number of events, to the best of our knowledge there is no established framework that addresses the prediction of the number of events at a future date with corresponding prediction intervals. Starting from an prediction interval approach originally developed in reliability engineering for the number of future component failures, we reformulated and extended it to the context of interim monitoring in clinical trials. This adaptation yields a general framework for event-count prediction intervals in the clinical setting, taking the patient as the unit of analysis and accommodating a range of parametric survival models, patient-level covariates, stagged entry and possible dependence between entry dates and lost to follow-up. Prediction intervals are obtained in a frequentist framework from a bootstrap estimator of the conditional distribution of future events. The performance of the proposed approach is investigated via simulation studies and illustrated by analyzing a real-world phase III trial in childhood acute lymphoblastic leukaemia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04192v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Edoardo Ratti, Federico L. Perlino, Stefania Galimberti, Maria G. Valsecchi</dc:creator>
    </item>
    <item>
      <title>Governance of Technological Transition: A Predator-Prey Analysis of AI Capital in China's Economy and Its Policy Implications</title>
      <link>https://arxiv.org/abs/2601.03547</link>
      <description>arXiv:2601.03547v1 Announce Type: cross 
Abstract: The rapid integration of Artificial Intelligence (AI) into China's economy presents a classic governance challenge: how to harness its growth potential while managing its disruptive effects on traditional capital and labor markets. This study addresses this policy dilemma by modeling the dynamic interactions between AI capital, physical capital, and labor within a Lotka-Volterra predator-prey framework. Using annual Chinese data (2016-2023), we quantify the interaction strengths, identify stable equilibria, and perform a global sensitivity analysis. Our results reveal a consistent pattern where AI capital acts as the 'prey', stimulating both physical capital accumulation and labor compensation (wage bill), while facing only weak constraining feedback. The equilibrium points are stable nodes, indicating a policy-mediated convergence path rather than volatile cycles. Critically, the sensitivity analysis shows that the labor market equilibrium is overwhelmingly driven by AI-related parameters, whereas the physical capital equilibrium is also influenced by its own saturation dynamics. These findings provide a systemic, quantitative basis for policymakers: (1) to calibrate AI promotion policies by recognizing the asymmetric leverage points in capital vs. labor markets; (2) to anticipate and mitigate structural rigidities that may arise from current regulatory settings; and (3) to prioritize interventions that foster complementary growth between AI and traditional economic structures while ensuring broad-base distribution of technological gains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03547v1</guid>
      <category>econ.GN</category>
      <category>cs.CY</category>
      <category>econ.EM</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kunpeng Wang, Jiahui Hu</dc:creator>
    </item>
    <item>
      <title>pintervals: an R package for model-agnostic prediction intervals</title>
      <link>https://arxiv.org/abs/2601.03994</link>
      <description>arXiv:2601.03994v1 Announce Type: cross 
Abstract: The \pkg{pintervals} package aims to provide a unified framework for constructing prediction intervals and calibrating predictions in a model-agnostic setting using set-aside calibration data. It comprises routines to construct conformal as well as parametric and bootstrapped prediction intervals from any model that outputs point predictions. Several R packages and functions already exist for constructing prediction intervals, but they often focus on specific modeling frameworks or types of predictions, or require manual customization for different models or applications. By providing a consistent interface for a variety of prediction interval construction approaches (all model-agnostic), \pkg{pintervals} allows researchers to apply and compare them across different modeling frameworks and applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03994v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Randahl, Anders Hjort, Jonathan P. Williams</dc:creator>
    </item>
    <item>
      <title>Ridge Estimation of High Dimensional Two-Way Fixed Effect Regression</title>
      <link>https://arxiv.org/abs/2601.04101</link>
      <description>arXiv:2601.04101v1 Announce Type: cross 
Abstract: We study a ridge estimator for the high-dimensional two-way fixed effect regression model with a sparse bipartite network. We develop concentration inequalities showing that when the ridge parameters increase as the log of the network size, the bias, and the variance-covariance matrix of the vector of estimated fixed effects converge to deterministic equivalents that depend only on the expected network. We provide simulations and an application using administrative data on wages for worker-firm matches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04101v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junnan He, Jean-Marc Robin</dc:creator>
    </item>
    <item>
      <title>On integral priors for multiple comparison in Bayesian model selection</title>
      <link>https://arxiv.org/abs/2406.14184</link>
      <description>arXiv:2406.14184v5 Announce Type: replace 
Abstract: Noninformative priors constructed for estimation purposes are usually not appropriate for model selection and testing. The methodology of integral priors was developed to get prior distributions for Bayesian model selection when comparing two models, modifying initial improper reference priors. We propose a generalization of this methodology to more than two models. Our approach adds an artificial copy of each model under comparison by compactifying the parametric space and creating an ergodic Markov chain across all models that returns the integral priors as marginals of the stationary distribution. Besides the guarantee of their existence and the lack of paradoxes attached to estimation reference priors, an additional advantage of this methodology is that the simulation of this Markov chain is straightforward as it only requires simulations of imaginary training samples for all models and from the corresponding posterior distributions. We present some examples, including situations where other methodologies need specific adjustments or do not produce a satisfactory answer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14184v5</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Diego Salmer\'on, Juan Antonio Cano, Christian P. Robert</dc:creator>
    </item>
    <item>
      <title>The causal interpretation of acceleration factors</title>
      <link>https://arxiv.org/abs/2409.01983</link>
      <description>arXiv:2409.01983v2 Announce Type: replace 
Abstract: In studies of time-to-event outcomes with unmeasured heterogeneity, the hazard ratio for treatment is known to have a complex causal interpretation. Accelerated failure time (AFT) models, which assess the effect on the survival time ratio scale, are often suggested as a better alternative because they model a parameter with direct causal interpretation while allowing straightforward adjustment for measured confounders. In this work, we formalize the causal interpretation of the acceleration factor in AFT models using structural causal models and data under independent censoring. We prove that the acceleration factor is a valid causal effect measure, even in the presence of frailty and treatment effect heterogeneity. Through simulations, we show that the acceleration factor better captures the causal effect than the hazard ratio when both AFT and conditional proportional hazards models apply. Additionally, we extend the interpretation to systems with time-dependent acceleration factors, illustrating the impossibility of distinguishing between a time-varying homogeneous effect and unmeasured effect heterogeneity. While the causal interpretation of acceleration factors is promising, we caution practitioners about potential challenges for the interpretation in the presence of effect heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01983v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mari Brathovde, Hein Putter, Morten Valberg, Richard A. J. Post</dc:creator>
    </item>
    <item>
      <title>LASSO Inference for High Dimensional Predictive Regressions</title>
      <link>https://arxiv.org/abs/2409.10030</link>
      <description>arXiv:2409.10030v3 Announce Type: replace 
Abstract: LASSO inflicts shrinkage bias on estimated coefficients, which undermines asymptotic normality and invalidates standard inferential procedures based on the t-statistic. Given cross sectional data, the desparsified LASSO has emerged as a well-known remedy for correcting the shrinkage bias. In the context of high dimensional predictive regression, the desparsified LASSO faces an additional challenge: the Stambaugh bias arising from nonstationary regressors modeled as local unit roots. To restore standard inference, we propose a novel estimator called IVX-desparsified LASSO (XDlasso). XDlasso simultaneously eliminates both shrinkage bias and Stambaugh bias and does not require prior knowledge about the identities of nonstationary and stationary regressors. We establish the asymptotic properties of XDlasso for hypothesis testing, and our theoretical findings are supported by Monte Carlo simulations. Applying our method to real-world applications from the FRED-MD database, we investigate two important empirical questions: (i) the predictability of the U.S. stock returns based on the earnings-price ratio, and (ii) the predictability of the U.S. inflation using the unemployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10030v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhan Gao, Ji Hyung Lee, Ziwei Mei, Zhentao Shi</dc:creator>
    </item>
    <item>
      <title>Causal Invariance Learning via Efficient Nonconvex Optimization</title>
      <link>https://arxiv.org/abs/2412.11850</link>
      <description>arXiv:2412.11850v3 Announce Type: replace 
Abstract: Identifying the causal relationship among variables from observational data is an important yet challenging task. This work focuses on identifying the direct causes of an outcome and estimating their magnitude, i.e., learning the causal outcome model. Data from multiple environments provide valuable opportunities to uncover causality by exploiting the invariance principle that the causal outcome model holds across heterogeneous environments. Based on the invariance principle, we propose the Negative Weighted Distributionally Robust Optimization (NegDRO) framework to learn an invariant prediction model. NegDRO minimizes the worst-case combination of risks across multiple environments and enforces invariance by allowing potential negative weights. Under the additive interventions regime, we establish three major contributions: (i) On the statistical side, we provide sufficient and nearly necessary identification conditions under which the invariant prediction model coincides with the causal outcome model; (ii) On the optimization side, despite the nonconvexity of NegDRO, we establish its benign optimization landscape, where all stationary points lie close to the true causal outcome model; (iii) On the computational side, we develop a gradient-based algorithm that provably converges to the causal outcome model, with non-asymptotic convergence rates in both sample size and gradient-descent iterations. In particular, our method avoids exhaustive combinatorial searches over exponentially many subsets of covariates found in the literature, ensuring scalability even when the dimension of the covariates is large. To our knowledge, this is the first causal invariance learning method that finds the approximate global optimality for a nonconvex optimization problem efficiently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11850v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenyu Wang, Yifan Hu, Peter B\"uhlmann, Zijian Guo</dc:creator>
    </item>
    <item>
      <title>Estimation of large approximate dynamic matrix factor models based on the EM algorithm and Kalman filtering</title>
      <link>https://arxiv.org/abs/2502.04112</link>
      <description>arXiv:2502.04112v3 Announce Type: replace 
Abstract: This paper considers an approximate dynamic matrix factor model that accounts for the time series nature of the data by explicitly modelling the time evolution of the factors. We study estimation of the model parameters based on the Expectation Maximization (EM) algorithm, implemented jointly with the Kalman smoother which gives estimates of the factors. We establish the consistency of the estimated loadings and factor matrices as the sample size $T$ and the matrix dimensions $p_1$ and $p_2$ diverge to infinity. We then extend this approach to: (a) the case of arbitrary patterns of missing data and (b) the presence of common stochastic trends. The finite sample properties of the estimators are assessed through a large simulation study and two applications on: (i) a financial dataset of volatility proxies and (ii) a macroeconomic dataset covering the main euro area countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04112v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Barigozzi, Luca Trapin</dc:creator>
    </item>
    <item>
      <title>EW D-optimal Designs for Experiments with Mixed Factors</title>
      <link>https://arxiv.org/abs/2505.00629</link>
      <description>arXiv:2505.00629v3 Announce Type: replace 
Abstract: We characterize EW D-optimal designs as robust designs against unknown parameter values for experiments under a general parametric model with discrete and continuous factors. When a pilot study is available, we recommend sample-based EW D-optimal designs for subsequent experiments. Otherwise, we recommend EW D-optimal designs under a prior distribution for model parameters. We propose an EW ForLion algorithm for finding EW D-optimal designs with mixed factors, and justify that the designs found by our algorithm are EW D-optimal. To facilitate potential users in practice, we also develop a rounding algorithm that converts an approximate design with mixed factors to exact designs with prespecified grid points and the total number of experimental units. By applying our algorithms for real experiments under multinomial logistic models or generalized linear models, we show that our designs are highly efficient with respect to locally D-optimal designs and more robust against parameter value misspecifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00629v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siting Lin, Yifei Huang, Jie Yang</dc:creator>
    </item>
    <item>
      <title>Ill-Conditioned Orthogonal Scores in Double Machine Learning</title>
      <link>https://arxiv.org/abs/2512.07083</link>
      <description>arXiv:2512.07083v2 Announce Type: replace 
Abstract: Double Machine Learning is often justified by nuisance-rate conditions, yet finite-sample reliability also depends on the conditioning of the orthogonal-score Jacobian. This conditioning is typically assumed rather than tracked. When residualized treatment variance is small, the Jacobian is ill-conditioned and small systematic nuisance errors can be amplified, so nominal confidence intervals may look precise yet systematically under-cover. Our main result is an exact identity for the cross-fitted PLR-DML estimator, with no Taylor approximation. From this identity, we derive a stochastic-order bound that separates oracle noise from a conditioning-amplified nuisance remainder and yields a sufficiency condition for root-n-inference. We further connect the amplification factor to semiparametric efficiency geometry via the Riesz representer and use a triangular-array framework to characterize regimes as residual treatment variation weakens. These results motivate an out-of-fold diagnostic that summarizes the implied amplification scale. We do not propose universal thresholds. Instead, we recommend reporting the diagnostic alongside cross-learner sensitivity summaries as a fragility assessment, illustrated in simulation and an empirical example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07083v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Saco</dc:creator>
    </item>
    <item>
      <title>Exponentially weighted estimands and the exponential family: filtering, prediction and smoothing</title>
      <link>https://arxiv.org/abs/2512.16745</link>
      <description>arXiv:2512.16745v2 Announce Type: replace 
Abstract: We propose using a discounted version of a convex combination of the log-likelihood with the corresponding expected log-likelihood such that when they are maximized they yield a filter, predictor and smoother for time series. This paper then focuses on working out the implications of this in the case of the canonical exponential family. The results are simple exact filters, predictors and smoothers with linear recursions. A theory for these models is developed and the models are illustrated on simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16745v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Donker van Heel, Neil Shephard</dc:creator>
    </item>
    <item>
      <title>Correcting Mode Proportion Bias in Generalized Bayesian Inference via a Weighted Kernel Stein Discrepancy</title>
      <link>https://arxiv.org/abs/2503.02108</link>
      <description>arXiv:2503.02108v2 Announce Type: replace-cross 
Abstract: Generalized Bayesian Inference (GBI) provides a flexible framework for updating prior distributions using various loss functions instead of the traditional likelihoods, thereby enhancing the model robustness to model misspecification. However, GBI often suffers the problem associated with intractable likelihoods. Kernelized Stein Discrepancy (KSD), as utilized in a recent study, addresses this challenge by relying only on the gradient of the log-likelihood. Despite this innovation, KSD-Bayes suffers from critical pathologies, including insensitivity to well-separated modes in multimodal posteriors. To address this limitation, we propose a weighted KSD method that retains computational efficiency while effectively capturing multimodal structures. Our method improves the GBI framework for handling intractable multimodal posteriors while maintaining key theoretical properties such as posterior consistency and asymptotic normality. Experimental results demonstrate that our method substantially improves mode sensitivity compared to standard KSD-Bayes, while retaining robust performance in unimodal settings and in the presence of outliers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02108v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elham Afzali, Saman Muthukumarana, Liqun Wang</dc:creator>
    </item>
    <item>
      <title>ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering</title>
      <link>https://arxiv.org/abs/2510.04514</link>
      <description>arXiv:2510.04514v2 Announce Type: replace-cross 
Abstract: Recent multimodal LLMs have shown promise in chart-based visual question answering, but their performance declines sharply on unannotated charts-those requiring precise visual interpretation rather than relying on textual shortcuts. To address this, we introduce ChartAgent, a novel agentic framework that explicitly performs visual reasoning directly within the chart's spatial domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively decomposes queries into visual subtasks and actively manipulates and interacts with chart images through specialized actions such as drawing annotations, cropping regions (e.g., segmenting pie slices, isolating bars), and localizing axes, using a library of chart-specific vision tools to fulfill each subtask. This iterative reasoning process closely mirrors human cognitive strategies for chart comprehension. ChartAgent achieves state-of-the-art accuracy on the ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07% absolute gain overall and 17.31% on unannotated, numerically intensive queries. Furthermore, our analyses show that ChartAgent is (a) effective across diverse chart types, (b) achieves the highest scores across varying visual and reasoning complexity levels, and (c) serves as a plug-and-play framework that boosts performance across diverse underlying LLMs. Our work is among the first to demonstrate visually grounded reasoning for chart understanding using tool-augmented multimodal agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04514v2</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>stat.ME</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rachneet Kaur, Nishan Srishankar, Zhen Zeng, Sumitra Ganesh, Manuela Veloso</dc:creator>
    </item>
    <item>
      <title>Uncovering Treatment Effect Heterogeneity in Pragmatic Gerontology Trials</title>
      <link>https://arxiv.org/abs/2511.14893</link>
      <description>arXiv:2511.14893v2 Announce Type: replace-cross 
Abstract: Detecting heterogeneity in treatment response enriches the interpretation of gerontologic trials. In aging research, estimating the effect of the intervention on clinically meaningful outcomes faces analytical challenges when it is truncated by death. For example, in the Whole Systems Demonstrator trial, a large cluster-randomized study evaluating telecare among older adults, the overall effect of the intervention on quality of life was found to be null. However, this marginal intervention estimate obscures potential heterogeneity of individuals responding to the intervention, particularly among those who survive to the end of follow-up. To explore this heterogeneity, we adopt a causal framework grounded in principal stratification, targeting the Survivor Average Causal Effect (SACE)-the treatment effect among "always-survivors," or those who would survive regardless of treatment assignment. We extend this framework using Bayesian Additive Regression Trees (BART), a nonparametric machine learning method, to flexibly model both latent principal strata and stratum-specific potential outcomes. This enables the estimation of the Conditional SACE (CSACE), allowing us to uncover variation in treatment effects across subgroups defined by baseline characteristics. Our analysis reveals that despite the null average effect, some subgroups experience distinct quality of life benefits (or lack thereof) from telecare, highlighting opportunities for more personalized intervention strategies. This study demonstrates how embedding machine learning methods, such as BART, within a principled causal inference framework can offer deeper insights into trial data with complex features including truncation by death and clustering-key considerations in analyzing pragmatic gerontology trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14893v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changjun Li, Heather Allore, Michael O. Harhay, Fan Li, Guangyu Tong</dc:creator>
    </item>
  </channel>
</rss>
