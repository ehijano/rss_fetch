<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Sep 2025 04:03:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A 200-Line Python Micro-Benchmark Suite for NISQ Circuit Compilers</title>
      <link>https://arxiv.org/abs/2509.16205</link>
      <description>arXiv:2509.16205v1 Announce Type: new 
Abstract: We present microbench.py, a compact (approx. 200 lines) Python script that automates the collection of key compiler metrics, i.e., gate depth, two-qubit-gate count, wall-clock compilation time, and memory footprint, across multiple open-source quantum circuit transpilers. The suite ships with six didactic circuits (3 to 8 qubits) implementing fundamental quantum algorithms and supports Qiskit, tket, Cirq, and the Qiskit-Braket provider; in this paper we showcase results for Qiskit 0.46 and Braket 1.16. The entire run completes in under three minutes on a laptop, emits a single CSV plus publisheable plot, and reproduces the figure here with one command. We release the code under the MIT licence to serve as a quick-start regression harness for NISQ compiler research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16205v1</guid>
      <category>cs.ET</category>
      <category>cs.PL</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Juhani Merilehto</dc:creator>
    </item>
    <item>
      <title>DarwinWafer: A Wafer-Scale Neuromorphic Chip</title>
      <link>https://arxiv.org/abs/2509.16213</link>
      <description>arXiv:2509.16213v1 Announce Type: new 
Abstract: Neuromorphic computing promises brain-like efficiency, yet today's multi-chip systems scale over PCBs and incur orders-of-magnitude penalties in bandwidth, latency, and energy, undermining biological algorithms and system efficiency. We present DarwinWafer, a hyperscale system-on-wafer that replaces off-chip interconnects with wafer-scale, high-density integration of 64 Darwin3 chiplets on a 300 mm silicon interposer. A GALS NoC within each chiplet and an AER-based asynchronous wafer fabric with hierarchical time-step synchronization provide low-latency, coherent operation across the wafer. Each chiplet implements 2.35 M neurons and 0.1 B synapses, yielding 0.15 B neurons and 6.4 B synapses per wafer.At 333 MHz and 0.8 V, DarwinWafer consumes ~100 W and achieves 4.9 pJ/SOP, with 64 TSOPS peak throughput (0.64 TSOPS/W). Realization is enabled by a holistic chiplet-interposer co-design flow (including an in-house interposer-bump planner with early SI/PI and electro-thermal closure) and a warpage-tolerant assembly that fans out I/O via PCBlets and compliant pogo-pin connections, enabling robust, demountable wafer-to-board integration. Measurements confirm 10 mV supply droop and a uniform thermal profile (34-36 {\deg}C) under ~100 W. Application studies demonstrate whole-brain simulations: two zebrafish brains per chiplet with high connectivity fidelity (Spearman r = 0.896) and a mouse brain mapped across 32 chiplets (r = 0.645). To our knowledge, DarwinWafer represents a pioneering demonstration of wafer-scale neuromorphic computing, establishing a viable and scalable path toward large-scale, brain-like computation on silicon by replacing PCB-level interconnects with high-density, on-wafer integration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16213v1</guid>
      <category>cs.ET</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaolei Zhu, Xiaofei Jin, Ziyang Kang, Chonghui Sun, Junjie Feng, Dingwen Hu, Zengyi Wang, Hanyue Zhuang, Qian Zheng, Huajin Tang, Shi Gu, Xin Du, De Ma, Gang Pan</dc:creator>
    </item>
    <item>
      <title>PrediPrune: Reducing Verification Overhead in Souper with Machine Learning Driven Pruning</title>
      <link>https://arxiv.org/abs/2509.16497</link>
      <description>arXiv:2509.16497v1 Announce Type: new 
Abstract: Souper is a powerful enumerative superoptimizer that enhances the runtime performance of programs by optimizing LLVM intermediate representation (IR) code. However, its verification process, which relies on a computationally expensive SMT solver to validate optimization candidates, must explore a large search space. This large search space makes the verification process particularly expensive, increasing the burden to incorporate Souper into compilation tools. We propose PrediPrune, a stochastic candidate pruning strategy that effectively reduces the number of invalid candidates passed to the SMT solver. By utilizing machine learning techniques to predict the validity of candidates based on features extracted from the code, PrediPrune prunes unlikely candidates early, decreasing the verification workload. When combined with the state-of-the-art approach (Dataflow), PrediPrune decreases compilation time by 51% compared to the Baseline and by 12% compared to using only Dataflow, emphasizing the effectiveness of the combined approach that integrates a purely ML-based method (PrediPrune) with a purely non-ML based (Dataflow) method. Additionally, PrediPrune offers a flexible interface to trade-off compilation time and optimization opportunities, allowing end users to adjust the balance according to their needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16497v1</guid>
      <category>cs.ET</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ange-Thierry Ishimwe, Raghuveer Shivakumar, Heewoo Kim, Tamara Lehman, Joseph Izraelevitz</dc:creator>
    </item>
    <item>
      <title>Governed By Agents: A Survey On The Role Of Agentic AI In Future Computing Environments</title>
      <link>https://arxiv.org/abs/2509.16676</link>
      <description>arXiv:2509.16676v1 Announce Type: new 
Abstract: The emergence of agentic Artificial Intelligence (AI), which can operate autonomously, demonstrate goal-directed behavior, and adaptively learn, indicates the onset of a massive change in today's computing infrastructure. This study investigates how agentic AI models' multiple characteristics may impact the architecture, governance, and operation under which computing environments function. Agentic AI has the potential to reduce reliance on extremely large (public) cloud environments due to resource efficiency, especially with processing and/or storage. The aforementioned characteristics provide us with an opportunity to canvas the likelihood of strategic migration in computing infrastructures away from massive public cloud services, towards more locally distributed architectures: edge computing and on-premises computing infrastructures. Many of these likely migrations will be spurred by factors like on-premises processing needs, diminished data consumption footprints, and cost savings. This study examines how a solution for implementing AI's autonomy could result in a re-architecture of the systems and model a departure from today's governance models to help us manage these increasingly autonomous agents, and an operational overhaul of processes over a very diverse computing systems landscape that bring together computing via cloud, edge, and on-premises computing solutions. To enable us to explore these intertwined decisions, it will be fundamentally important to understand how to best position agentic AI, and to navigate the future state of computing infrastructures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16676v1</guid>
      <category>cs.ET</category>
      <category>cs.AI</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nauman Ali Murad, Safia Baloch</dc:creator>
    </item>
    <item>
      <title>Machine Learning in Near-Field Communication for 6G: A Survey</title>
      <link>https://arxiv.org/abs/2509.16723</link>
      <description>arXiv:2509.16723v1 Announce Type: new 
Abstract: 6G wireless communication networks are expected to use extremely large-scale antenna arrays (ELAAs) to support higher throughput, massive connectivity, and improved system performance. ELAAs would fundamentally alter wave characteristics, transforming them from plane waves into spherical waves, thereby operating in the near field. Near-field communications (NFC) offer unique advantages to enhance system performance, but also present significant challenges in channel modeling, computational complexity, and beamforming design. The use of machine learning (ML) is emerging as a powerful approach to tackle such challenges and has the capabilities to enable intelligent, secure, and efficient 6G wireless communications. In this survey, we discuss ML-driven approaches for NFC. We first outline the fundamental concepts of NFC and ML. We then discuss ML applications in channel estimation, beamforming design, and security enhancement. We also highlight key challenges (e.g., data privacy and computational overhead). Finally, we discuss open issues and future directions to emphasize the role of advanced ML techniques in near-field system design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16723v1</guid>
      <category>cs.ET</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amjad Iqbal, Ala'a Al-Habashna, Gabriel Wainer, Gary Boudreau</dc:creator>
    </item>
    <item>
      <title>Hijacking Living Cells with Surface Engineering for the Internet of Bio-Nano Things</title>
      <link>https://arxiv.org/abs/2509.17227</link>
      <description>arXiv:2509.17227v1 Announce Type: new 
Abstract: The Internet of Bio-Nano Things (IoBNT) promises to revolutionize healthcare by interfacing the cyber domain with the living systems at unprecedented resolution. Realizing this vision hinges on the development of Bio-Nano Things (BNTs), i.e., functional nodes capable of sensing, actuation, and communications within biological environments. Existing BNT architectures, e.g., nanomaterial-based, biosynthetic, and passive molecular agents, face significant limitations, including toxicity, lack of autonomy, or the safety and metabolic burdens associated with genetic modification. This paper posits a fourth paradigm: the transient hijacking of living cells via non-genetic cell surface engineering (NG-CSE) to enable living BNTs. NGCSE allows for the precise, reversible functionalization of cell membranes with synthetic molecular machinery, reprogramming cellular functions and interactions without altering the genome. It uniquely combines the inherent biocompatibility and agency of living cells with the programmability enabled by nanotechnology, mitigating the risks of genetic engineering. We critically review the toolbox of NG-CSE and explore the opportunities it unlocks for IoBNT, including programmable cell-cell communication, dynamic network topologies, and improved bio-cyber interfacing. Moreover, we propose novel IoBNT architectures that leverage these capabilities, such as circulating sentinel networks exploiting cellular agency for continuous liquid biopsy, and rationally designed, in vitro biocomputers exploiting interkingdom interactions. We also outline the critical challenges in modeling and exploiting cellular agency with NG-CSE, providing a roadmap for the effective utilization of NG-CSE-enabled living BNTs within IoBNT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17227v1</guid>
      <category>cs.ET</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ekin Ince, Murat Kuscu</dc:creator>
    </item>
    <item>
      <title>Truth Without Comprehension: A BlueSky Agenda for Steering the Fourth Mathematical Crisis</title>
      <link>https://arxiv.org/abs/2509.17290</link>
      <description>arXiv:2509.17290v1 Announce Type: new 
Abstract: Machine-generated proofs are poised to reach large-scale, human-unreadable artifacts. They foreshadow what we call the Fourth Mathematical Crisis. This crisis crystallizes around three fundamental tensions: trusting proofs that no human can inspect, understanding results that no one can fully read, and verifying systems that themselves resist verification. As a minimal yet principled response, we propose the Human Understandability (HU) meta-axiom, which requires that every proof admits at least one projection that is resource-bounded, divergence-measured, and acceptable to a verifier. Confronting these questions opens a timely research agenda and points toward new directions in scalable reasoning, interpretable inference, and epistemic trust for the era of machine-scale mathematics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17290v1</guid>
      <category>cs.ET</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Runlong Yu, Xiaowei Jia</dc:creator>
    </item>
    <item>
      <title>DiffQ: Unified Parameter Initialization for Variational Quantum Algorithms via Diffusion Models</title>
      <link>https://arxiv.org/abs/2509.17324</link>
      <description>arXiv:2509.17324v1 Announce Type: new 
Abstract: Variational Quantum Algorithms (VQAs) are widely used in the noisy intermediate-scale quantum (NISQ) era, but their trainability and performance depend critically on initialization parameters that shape the optimization landscape. Existing machine learning-based initializers achieve state-of-the-art results yet remain constrained to single-task domains and small datasets of only hundreds of samples. We address these limitations by reformulating VQA parameter initialization as a generative modeling problem and introducing DiffQ, a parameter initializer based on the Denoising Diffusion Probabilistic Model (DDPM). To support robust training and evaluation, we construct a dataset of 15,085 instances spanning three domains and five representative tasks. Experiments demonstrate that DiffQ surpasses baselines, reducing initial loss by up to 8.95 and convergence steps by up to 23.4%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17324v1</guid>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>quant-ph</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chi Zhang, Mengxin Zheng, Qian Lou, Fan Chen</dc:creator>
    </item>
    <item>
      <title>Evaluating the Energy Efficiency of NPU-Accelerated Machine Learning Inference on Embedded Microcontrollers</title>
      <link>https://arxiv.org/abs/2509.17533</link>
      <description>arXiv:2509.17533v1 Announce Type: new 
Abstract: The deployment of machine learning (ML) models on microcontrollers (MCUs) is constrained by strict energy, latency, and memory requirements, particularly in battery-operated and real-time edge devices. While software-level optimizations such as quantization and pruning reduce model size and computation, hardware acceleration has emerged as a decisive enabler for efficient embedded inference. This paper evaluates the impact of Neural Processing Units (NPUs) on MCU-based ML execution, using the ARM Cortex-M55 core combined with the Ethos-U55 NPU on the Alif Semiconductor Ensemble E7 development board as a representative platform. A rigorous measurement methodology was employed, incorporating per-inference net energy accounting via GPIO-triggered high-resolution digital multimeter synchronization and idle-state subtraction, ensuring accurate attribution of energy costs. Experimental results across six representative ML models -including MiniResNet, MobileNetV2, FD-MobileNet, MNIST, TinyYolo, and SSD-MobileNet- demonstrate substantial efficiency gains when inference is offloaded to the NPU. For moderate to large networks, latency improvements ranged from 7x to over 125x, with per-inference net energy reductions up to 143x. Notably, the NPU enabled execution of models unsupported on CPU-only paths, such as SSD-MobileNet, highlighting its functional as well as efficiency advantages. These findings establish NPUs as a cornerstone of energy-aware embedded AI, enabling real-time, power-constrained ML inference at the MCU level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17533v1</guid>
      <category>cs.ET</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anastasios Fanariotis, Theofanis Orphanoudakis, Vasilis Fotopoulos</dc:creator>
    </item>
    <item>
      <title>Single-Cell Universal Logic-in-Memory Using 2T-nC FeRAM: An Area and Energy-Efficient Approach for Bulk Bitwise Computation</title>
      <link>https://arxiv.org/abs/2509.17963</link>
      <description>arXiv:2509.17963v1 Announce Type: new 
Abstract: This work presents a novel approach to configure 2T-nC ferroelectric RAM (FeRAM) for performing single cell logic-in-memory operations, highlighting its advantages in energy-efficient computation over conventional DRAM-based approaches. Unlike conventional 1T-1C dynamic RAM (DRAM), which incurs refresh overhead, 2T-nC FeRAM offers a promising alternative as a non-volatile memory solution with low energy consumption. Our key findings include the potential of quasi-nondestructive readout (QNRO) sensing in 2T-nC FeRAM for logic-in-memory (LiM) applications, demonstrating its inherent capability to perform inverting logic without requiring external modifications, a feature absent in traditional 1T-1C DRAM. We successfully implement the MINORITY function within a single cell of 2T-nC FeRAM, enabling universal NAND and NOR logic, validated through SPICE simulations and experimental data. Additionally, the research investigates the feasibility of 3D integration with 2T-nC FeRAM, showing substantial improvements in storage and computational density, facilitating bulk-bitwise computation. Our evaluation of eight real-world, data-intensive applications reveals that 2T-nC FeRAM achieves 2x higher performance and 2.5x lower energy consumption compared to DRAM. Furthermore, the thermal stability of stacked 2T-nC FeRAM is validated, confirming its reliable operation when integrated on a compute die. These findings emphasize the advantages of 2T-nC FeRAM for LiM, offering superior performance and energy efficiency over conventional DRAM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17963v1</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rudra Biswas, Jiahui Duan, Shan Deng, Xuezhong Niu, Yixin Qin, Prapti Panigrahi, Varun Parekh, Rajiv Joshi, Kai Ni, Vijaykrishnan Narayanan</dc:creator>
    </item>
    <item>
      <title>Machine Learning for Quantum Noise Reduction</title>
      <link>https://arxiv.org/abs/2509.16242</link>
      <description>arXiv:2509.16242v1 Announce Type: cross 
Abstract: Quantum noise fundamentally limits the utility of near-term quantum devices, making error mitigation essential for practical quantum computation. While traditional quantum error correction codes require substantial qubit overhead and complex syndrome decoding, we propose a machine learning approach that directly reconstructs clean quantum states from noisy density matrices without additional qubits. We formulate quantum noise reduction as a supervised learning problem using a convolutional neural network (CNN) autoencoder architecture with a novel fidelity-aware composite loss function. Our method is trained and evaluated on a comprehensive synthetic dataset of 10,000 density matrices derived from random 5-qubit quantum circuits, encompassing five noise types (depolarizing, amplitude damping, phase damping, bit-flip, and mixed noise) across four intensity levels (0.05-0.20). The CNN successfully reconstructs quantum states across all noise conditions, achieving an average fidelity improvement from 0.298 to 0.774 ({\Delta} = 0.476). Notably, the model demonstrates superior performance on complex mixed noise scenarios and higher noise intensities, with mixed noise showing the highest corrected fidelity (0.807) and improvement (0.567). The approach effectively preserves both diagonal elements (populations) and off-diagonal elements (quantum coherences), making it suitable for entanglement-dependent quantum algorithms. While phase damping presents fundamental information-theoretic limitations, our results suggest that CNN-based density matrix reconstruction offers a promising, resource-efficient alternative to traditional quantum error correction for NISQ-era devices. This data-driven approach could enable practical quantum advantage with fewer physical qubits than conventional error correction schemes require.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16242v1</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Karan Kendre</dc:creator>
    </item>
    <item>
      <title>Atomic-Scale Insights into the Switching Mechanisms of RRAM Devices</title>
      <link>https://arxiv.org/abs/2509.16512</link>
      <description>arXiv:2509.16512v1 Announce Type: cross 
Abstract: The growing energy demands of information and communication technologies, driven by data-intensive computing and the von Neumann bottleneck, underscore the need for energy-efficient alternatives. Resistive random-access memory (RRAM) devices have emerged as promising candidates for beyond von Neumann computing paradigms, such as neuromorphic computing, offering voltage-history-dependent switching that mimics synaptic and neural behaviors. Atomic-scale mechanisms, such as defect-driven filament formation and ionic transport, govern these switching processes. In this work, we present a comprehensive characterization of Tantalum Oxide based RRAM devices featuring both oxygen-rich and oxygen-deficient switching layers. We analyze the dominant conduction mechanisms underpinning resistive switching and systematically evaluate how oxygen stoichiometry influences device behavior. Leveraging a bottom-up design methodology, we link material composition to electrical performance metrics-such as endurance, cycle-to-cycle variability, and multilevel resistance states-providing actionable guidelines for optimizing RRAM architectures for energy-efficient memory and computing applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16512v1</guid>
      <category>physics.app-ph</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.ET</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Tawsif Rahman Chowdhury, Alireza Moazzeni, Gozde Tutuncuoglu</dc:creator>
    </item>
    <item>
      <title>Person Identification from Egocentric Human-Object Interactions using 3D Hand Pose</title>
      <link>https://arxiv.org/abs/2509.16557</link>
      <description>arXiv:2509.16557v1 Announce Type: cross 
Abstract: Human-Object Interaction Recognition (HOIR) and user identification play a crucial role in advancing augmented reality (AR)-based personalized assistive technologies. These systems are increasingly being deployed in high-stakes, human-centric environments such as aircraft cockpits, aerospace maintenance, and surgical procedures. This research introduces I2S (Interact2Sign), a multi stage framework designed for unobtrusive user identification through human object interaction recognition, leveraging 3D hand pose analysis in egocentric videos. I2S utilizes handcrafted features extracted from 3D hand poses and per forms sequential feature augmentation: first identifying the object class, followed by HOI recognition, and ultimately, user identification. A comprehensive feature extraction and description process was carried out for 3D hand poses, organizing the extracted features into semantically meaningful categories: Spatial, Frequency, Kinematic, Orientation, and a novel descriptor introduced in this work, the Inter-Hand Spatial Envelope (IHSE). Extensive ablation studies were conducted to determine the most effective combination of features. The optimal configuration achieved an impressive average F1-score of 97.52% for user identification, evaluated on a bimanual object manipulation dataset derived from the ARCTIC and H2O datasets. I2S demonstrates state-of-the-art performance while maintaining a lightweight model size of under 4 MB and a fast inference time of 0.1 seconds. These characteristics make the proposed framework highly suitable for real-time, on-device authentication in security-critical, AR-based systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16557v1</guid>
      <category>cs.CV</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Hamza, Danish Hamid, Muhammad Tahir Akram</dc:creator>
    </item>
    <item>
      <title>MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE</title>
      <link>https://arxiv.org/abs/2509.17238</link>
      <description>arXiv:2509.17238v1 Announce Type: cross 
Abstract: The generation quality of large language models (LLMs) is often improved by utilizing inference-time sequence-level scaling methods (e.g., Chain-of-Thought). We introduce hyper-parallel scaling, a complementary framework that improves prediction quality at the token level. Hyper-parallel scaling computes and aggregates multiple output proposals for a single token from the model. We implement this concept in Mixture-of-Experts (MoE) models, which we refer to as Roster of Experts (RoE). RoE is a training-free inference algorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects controlled stochasticity into the expert routing mechanism, enabling it to sample multiple diverse experts for each token and aggregate their outputs for a more accurate final prediction.To overcome the computational cost, we introduce an efficient batching strategy and a specialized KV-caching mechanism that minimizes compute and memory overhead. For example, RoE enables a 7B MoE model to match the performance of a 10.5B MoE model while using 30% less compute for inference. These gains are achieved without any fine-tuning of model parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17238v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soheil Zibakhsh, Mohammad Samragh, Kumari Nishu, Lauren Hannah, Arnav Kundu, Minsik Cho</dc:creator>
    </item>
    <item>
      <title>Automated Facility Enumeration for Building Compliance Checking using Door Detection and Large Language Models</title>
      <link>https://arxiv.org/abs/2509.17283</link>
      <description>arXiv:2509.17283v1 Announce Type: cross 
Abstract: Building compliance checking (BCC) is a critical process for ensuring that constructed facilities meet regulatory standards. A core component of BCC is the accurate enumeration of facility types and their spatial distribution. Despite its importance, this problem has been largely overlooked in the literature, posing a significant challenge for BCC and leaving a critical gap in existing workflows. Performing this task manually is time-consuming and labor-intensive. Recent advances in large language models (LLMs) offer new opportunities to enhance automation by combining visual recognition with reasoning capabilities. In this paper, we introduce a new task for BCC: automated facility enumeration, which involves validating the quantity of each facility type against statutory requirements. To address it, we propose a novel method that integrates door detection with LLM-based reasoning. We are the first to apply LLMs to this task and further enhance their performance through a Chain-of-Thought (CoT) pipeline. Our approach generalizes well across diverse datasets and facility types. Experiments on both real-world and synthetic floor plan data demonstrate the effectiveness and robustness of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17283v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Licheng Zhan, Bach Le, Naveed Akhtar, Tuan Ngo</dc:creator>
    </item>
    <item>
      <title>VQEzy: An Open-Source Dataset for Parameter Initialize in Variational Quantum Eigensolvers</title>
      <link>https://arxiv.org/abs/2509.17322</link>
      <description>arXiv:2509.17322v1 Announce Type: cross 
Abstract: Variational Quantum Eigensolvers (VQEs) are a leading class of noisy intermediate-scale quantum (NISQ) algorithms, whose performance is highly sensitive to parameter initialization. Although recent machine learning-based initialization methods have achieved state-of-the-art performance, their progress has been limited by the lack of comprehensive datasets. Existing resources are typically restricted to a single domain, contain only a few hundred instances, and lack complete coverage of Hamiltonians, ansatz circuits, and optimization trajectories. To overcome these limitations, we introduce VQEzy, the first large-scale dataset for VQE parameter initialization. VQEzy spans three major domains and seven representative tasks, comprising 12,110 instances with full VQE specifications and complete optimization trajectories. The dataset is available online, and will be continuously refined and expanded to support future research in VQE optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17322v1</guid>
      <category>cs.LG</category>
      <category>cs.ET</category>
      <category>quant-ph</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chi Zhang, Mengxin Zheng, Qian Lou, Hui Min Leung, Fan Chen</dc:creator>
    </item>
    <item>
      <title>Diff-GNSS: Diffusion-based Pseudorange Error Estimation</title>
      <link>https://arxiv.org/abs/2509.17397</link>
      <description>arXiv:2509.17397v1 Announce Type: cross 
Abstract: Global Navigation Satellite Systems (GNSS) are vital for reliable urban positioning. However, multipath and non-line-of-sight reception often introduce large measurement errors that degrade accuracy. Learning-based methods for predicting and compensating pseudorange errors have gained traction, but their performance is limited by complex error distributions. To address this challenge, we propose Diff-GNSS, a coarse-to-fine GNSS measurement (pseudorange) error estimation framework that leverages a conditional diffusion model to capture such complex distributions. Firstly, a Mamba-based module performs coarse estimation to provide an initial prediction with appropriate scale and trend. Then, a conditional denoising diffusion layer refines the estimate, enabling fine-grained modeling of pseudorange errors. To suppress uncontrolled generative diversity and achieve controllable synthesis, three key features related to GNSS measurement quality are used as conditions to precisely guide the reverse denoising process. We further incorporate per-satellite uncertainty modeling within the diffusion stage to assess the reliability of the predicted errors. We have collected and publicly released a real-world dataset covering various scenes. Experiments on public and self-collected datasets show that DiffGNSS consistently outperforms state-of-the-art baselines across multiple metrics. To the best of our knowledge, this is the first application of diffusion models to pseudorange error estimation. The proposed diffusion-based refinement module is plug-and-play and can be readily integrated into existing networks to markedly improve estimation accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17397v1</guid>
      <category>cs.CV</category>
      <category>cs.ET</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaqi Zhu, Shouyi Lu, Ziyao Li, Guirong Zhuo, Lu Xiong</dc:creator>
    </item>
    <item>
      <title>Propuesta de implementaci\'on de cat\'alogos federados para espacios de datos sobre DataHub</title>
      <link>https://arxiv.org/abs/2509.17649</link>
      <description>arXiv:2509.17649v1 Announce Type: cross 
Abstract: In the digital era, data spaces are emerging as key ecosystems for the secure and controlled exchange of information among participants. To achieve this, components such as metadata catalogs and data space connectors are essential. This document proposes an implementation and integration solution for both elements, considering standardization guidelines for data formats, metadata, and protocols, which ensures interoperability. A hybrid solution is presented: DataHub is used as a federated catalog for robust metadata management, leveraging its advanced ingestion, governance, and lineage capabilities. On the other hand, a custom implementation, Rainbow Catalog, manages ODRL policies for access and usage. This integration makes it possible to query datasets from DataHub and associate them with ODRL policies, facilitating negotiation and transfer flows defined by the Dataspace Protocol. The result is a system that combines the power of DataHub for large-scale cataloging with the policy management of the connector crucial for sovereignty and trust in data spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17649v1</guid>
      <category>cs.DB</category>
      <category>cs.ET</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Carlos Aparicio de Santiago, Pablo Vi\~nuales Esquinas, Irene Plaza Ortiz, Andres Munoz-Arcentales, Gabriel Huecas, Joaqu\'in Salvach\'ua, Enrique Barra</dc:creator>
    </item>
    <item>
      <title>Towards Seeing Bones at Radio Frequency</title>
      <link>https://arxiv.org/abs/2509.17979</link>
      <description>arXiv:2509.17979v1 Announce Type: cross 
Abstract: Wireless sensing literature has long aspired to achieve X-ray-like vision at radio frequencies. Yet, state-of-the-art wireless sensing literature has yet to generate the archetypal X-ray image: one of the bones beneath flesh. In this paper, we explore MCT, a penetration-based RF-imaging system for imaging bones at mm-resolution, one that significantly exceeds prior penetration-based RF imaging literature. Indeed the long wavelength, significant attenuation and complex diffraction that occur as RF propagates through flesh, have long limited imaging resolution (to several centimeters at best). We address these concerns through a novel penetration-based synthetic aperture algorithm, coupled with a learning-based pipeline to correct for diffraction-induced artifacts. A detailed evaluation of meat models demonstrates a resolution improvement from sub-decimeter to sub-centimeter over prior art in RF penetrative imaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17979v1</guid>
      <category>cs.GR</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiwen Song, Hongyang Li, Kuang Yuan, Ran Bi, Swarun Kumar</dc:creator>
    </item>
    <item>
      <title>HuMam: Humanoid Motion Control via End-to-End Deep Reinforcement Learning with Mamba</title>
      <link>https://arxiv.org/abs/2509.18046</link>
      <description>arXiv:2509.18046v1 Announce Type: cross 
Abstract: End-to-end reinforcement learning (RL) for humanoid locomotion is appealing for its compact perception-action mapping, yet practical policies often suffer from training instability, inefficient feature fusion, and high actuation cost. We present HuMam, a state-centric end-to-end RL framework that employs a single-layer Mamba encoder to fuse robot-centric states with oriented footstep targets and a continuous phase clock. The policy outputs joint position targets tracked by a low-level PD loop and is optimized with PPO. A concise six-term reward balances contact quality, swing smoothness, foot placement, posture, and body stability while implicitly promoting energy saving. On the JVRC-1 humanoid in mc-mujoco, HuMam consistently improves learning efficiency, training stability, and overall task performance over a strong feedforward baseline, while reducing power consumption and torque peaks. To our knowledge, this is the first end-to-end humanoid RL controller that adopts Mamba as the fusion backbone, demonstrating tangible gains in efficiency, stability, and control economy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18046v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yinuo Wang, Yuanyang Qi, Jinzhao Zhou, Gavin Tao</dc:creator>
    </item>
    <item>
      <title>Molecular Communication Channel as a Physical Reservoir Computer</title>
      <link>https://arxiv.org/abs/2504.17022</link>
      <description>arXiv:2504.17022v2 Announce Type: replace 
Abstract: Molecular Communication (MC) channels are characterized by significant memory and nonlinear dynamics arising from diffusion and receptor kinetics. While often viewed as impairments to reliable data transmission, this work introduces a paradigm shift by reconceptualizing these intrinsic physical properties as computational resources. We frame a canonical point-to-point MC channel, comprising ligand diffusion and reversible ligand-receptor binding at a spherical receiver, as a Physical Reservoir Computer (PRC). Utilizing deterministic mean-field modeling and particle-based spatial stochastic simulations, we demonstrate the MC system's inherent capability for complex temporal information processing on standard chaotic time-series benchmarks. We comprehensively evaluate performance using both task-specific Normalized Root Mean Square Error (NRMSE) and the task-independent Information Processing Capacity (IPC). Our results reveal a non-monotonic dependence of computational power on key biophysical parameters (receptor kinetic rates, diffusion coefficient, and transmitter-receiver distance), identifying optimal operational regimes where memory and nonlinearity are balanced. These findings establish the MC channel as a viable computational substrate, paving the way for novel architectures in \emph{wetware} artificial intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17022v2</guid>
      <category>cs.ET</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mustafa Uzun, Kaan Burak Ikiz, Murat Kuscu</dc:creator>
    </item>
    <item>
      <title>Spin-NeuroMem: A Low-Power Neuromorphic Associative Memory Design Based on Spintronic Devices</title>
      <link>https://arxiv.org/abs/2404.02463</link>
      <description>arXiv:2404.02463v4 Announce Type: replace-cross 
Abstract: Biologically-inspired computing models have made significant progress in recent years, but the conventional von Neumann architecture is inefficient for the large-scale matrix operations and massive parallelism required by these models. This paper presents Spin-NeuroMem, a low-power circuit design of Hopfield network for the function of associative memory. Spin-NeuroMem is equipped with energy-efficient spintronic synapses which utilize magnetic tunnel junctions (MTJs) to store weight matrices of multiple associative memories. The proposed synapse design achieves as low as 17.4% power consumption compared to the state-of-the-art synapse designs. Spin-NeuroMem also encompasses a novel voltage converter with a 53.3% reduction in transistor usage for effective Hopfield network computation. In addition, we propose an associative memory simulator for the first time, which achieves a 5Mx speedup with a comparable associative memory effect. By harnessing the potential of spintronic devices, this work paves the way for the development of energy-efficient and scalable neuromorphic computing systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02463v4</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <category>physics.app-ph</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10825-025-02415-1</arxiv:DOI>
      <arxiv:journal_reference>16 September 2025, Volume 24, article number 180, (2025)</arxiv:journal_reference>
      <dc:creator>Siqing Fu, Lizhou Wu, Tiejun Li, Chunyuan Zhang, Jianmin Zhang, Sheng Ma</dc:creator>
    </item>
    <item>
      <title>STAMP-2.5D: Structural and Thermal Aware Methodology for Placement in 2.5D Integration</title>
      <link>https://arxiv.org/abs/2504.21140</link>
      <description>arXiv:2504.21140v2 Announce Type: replace-cross 
Abstract: Chiplet-based architectures and advanced packaging has emerged as transformative approaches in semiconductor design. While conventional physical design for 2.5D heterogeneous systems typically prioritizes wirelength reduction through tight chiplet packing, this strategy creates thermal bottlenecks and intensifies coefficient of thermal expansion (CTE) mismatches, compromising long-term reliability. Addressing these challenges requires holistic consideration of thermal performance, mechanical stress, and interconnect efficiency. We introduce STAMP-2.5D, the first automated floorplanning methodology that simultaneously optimizes these critical factors. Our approach employs finite element analysis to simulate temperature distributions and stress profiles across chiplet configurations while minimizing interconnect wirelength. Experimental results demonstrate that our thermal structural aware automated floorplanning approach reduces overall stress by 11% while maintaining excellent thermal performance with a negligible 0.5% temperature increase and simultaneously reducing total wirelength by 11% compared to temperature-only optimization. Additionally, we conduct an exploratory study on the effects of temperature gradients on structural integrity, providing crucial insights for reliability-conscious chiplet design. STAMP-2.5D establishes a robust platform for navigating critical trade-offs in advanced semiconductor packaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21140v2</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Varun Darshana Parekh, Zachary Wyatt Hazenstab, Srivatsa Rangachar Srinivasa, Krishnendu Chakrabarty, Kai Ni, Vijaykrishnan Narayanan</dc:creator>
    </item>
    <item>
      <title>HARPT: A Corpus for Analyzing Consumers' Trust and Privacy Concerns in Electronic Health Apps</title>
      <link>https://arxiv.org/abs/2506.19268</link>
      <description>arXiv:2506.19268v3 Announce Type: replace-cross 
Abstract: We present Health App Reviews for Privacy &amp; Trust (HARPT), a large-scale annotated corpus of user reviews from Electronic Health (eHealth) applications (apps) aimed at advancing research in user privacy and trust. The dataset comprises 480K user reviews labeled in seven categories that capture critical aspects of trust in applications (TA), trust in providers (TP), and privacy concerns (PC). Our multistage strategy integrated keyword-based filtering, iterative manual labeling with review, targeted data augmentation, and weak supervision using transformer-based classifiers. In parallel, we manually annotated a curated subset of 7,000 reviews to support the development and evaluation of machine learning models. We benchmarked a broad range of models, providing a baseline for future work. HARPT is released under an open resource license to support reproducible research in usable privacy and trust in digital libraries and health informatics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19268v3</guid>
      <category>cs.HC</category>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timoteo Kelly, Abdulkadir Korkmaz, Samuel Mallet, Connor Souders, Sadra Aliakbarpour, Praveen Rao</dc:creator>
    </item>
    <item>
      <title>Interpretability-Aware Pruning for Efficient Medical Image Analysis</title>
      <link>https://arxiv.org/abs/2507.08330</link>
      <description>arXiv:2507.08330v2 Announce Type: replace-cross 
Abstract: Deep learning has driven significant advances in medical image analysis, yet its adoption in clinical practice remains constrained by the large size and lack of transparency in modern models. Advances in interpretability techniques such as DL-Backtrace, Layer-wise Relevance Propagation, and Integrated Gradients make it possible to assess the contribution of individual components within neural networks trained on medical imaging tasks. In this work, we introduce an interpretability-guided pruning framework that reduces model complexity while preserving both predictive performance and transparency. By selectively retaining only the most relevant parts of each layer, our method enables targeted compression that maintains clinically meaningful representations. Experiments across multiple medical image classification benchmarks demonstrate that this approach achieves high compression rates with minimal loss in accuracy, paving the way for lightweight, interpretable models suited for real-world deployment in healthcare settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08330v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikita Malik, Pratinav Seth, Neeraj Kumar Singh, Chintan Chitroda, Vinay Kumar Sankarapu</dc:creator>
    </item>
  </channel>
</rss>
