<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Sep 2025 04:00:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>\textit{No One-Size-Fits-All}: A Workload-Driven Characterization of Bit-Parallel vs. Bit-Serial Data Layouts for Processing-using-Memory</title>
      <link>https://arxiv.org/abs/2509.22980</link>
      <description>arXiv:2509.22980v1 Announce Type: new 
Abstract: Processing-in-Memory (PIM) is a promising approach to overcoming the memory-wall bottleneck. However, the PIM community has largely treated its two fundamental data layouts, Bit-Parallel (BP) and Bit-Serial (BS), as if they were interchangeable. This implicit "one-layout-fits-all" assumption, often hard-coded into existing evaluation frameworks, creates a critical gap: architects lack systematic, workload-driven guidelines for choosing the optimal data layout for their target applications.
  To address this gap, this paper presents the first systematic, workload-driven characterization of BP and BS PIM architectures. We develop iso-area, cycle-accurate BP and BS PIM architectural models and conduct a comprehensive evaluation using a diverse set of benchmarks. Our suite includes both fine-grained microworkloads from MIMDRAM to isolate specific operational characteristics, and large-scale applications from the PIMBench suite, such as the VGG network, to represent realistic end-to-end workloads.
  Our results quantitatively demonstrate that no single layout is universally superior; the optimal choice is strongly dependent on workload characteristics. BP excels on control-flow-intensive tasks with irregular memory access patterns, whereas BS shows substantial advantages in massively parallel, low-precision (e.g., INT4/INT8) computations common in AI. Based on this characterization, we distill a set of actionable design guidelines for architects. This work challenges the prevailing one-size-fits-all view on PIM data layouts and provides a principled foundation for designing next-generation, workload-aware, and potentially hybrid PIM systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22980v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyao Zhang, Elaheh Sadredini</dc:creator>
    </item>
    <item>
      <title>Enhanced Hybrid Temporal Computing Using Deterministic Summations for Ultra-Low-Power Accelerators</title>
      <link>https://arxiv.org/abs/2509.22999</link>
      <description>arXiv:2509.22999v1 Announce Type: new 
Abstract: This paper presents an accuracy-enhanced Hybrid Temporal Computing (E-HTC) framework for ultra-low-power hardware accelerators with deterministic additions. Inspired by the recently proposed HTC architecture, which leverages pulse-rate and temporal data encoding to reduce switching activity and energy consumption but loses accuracy due to its multiplexer (MUX)-based scaled addition, we propose two bitstream addition schemes: (1) an Exact Multiple-input Binary Accumulator (EMBA), which performs precise binary accumulation, and (2) a Deterministic Threshold-based Scaled Adder (DTSA), which employs threshold logic for scaled addition. These adders are integrated into a multiplier accumulator (MAC) unit supporting both unipolar and bipolar encodings. To validate the framework, we implement two accelerators: a Finite Impulse Response (FIR) filter and an 8-point Discrete Cosine Transform (DCT)/iDCT engine. Results on a 4x4 MAC show that, in unipolar mode, E-HTC matches the RMSE of state-of-the-art Counter-Based Stochastic Computing (CBSC) MAC, improves accuracy by 94% over MUX-based HTC, and reduces power and area by 23% and 7% compared to MUX-based HTC and 64% and 74% compared to CBSC. In bipolar mode, E-HTC MAC achieves 2.09% RMSE -- an 83% improvement over MUX-based HTC -- and approaches CBSC's 1.40% RMSE with area and power savings of 28% and 43% vs. MUX-based HTC and about 76% vs. CBSC. In FIR experiments, both E-HTC variants yield PSNR gains of 3--5 dB (30--45% RMSE reduction) while saving 13% power and 3% area. For DCT/iDCT, E-HTC boosts PSNR by 10--13 dB (70--75% RMSE reduction) while saving area and power over both MUX- and CBSC-based designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22999v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sachin Sachdeva, Jincong Lu, Wantong Li, Sheldon X. -D. Tan</dc:creator>
    </item>
    <item>
      <title>A Near-Cache Architectural Framework for Cryptographic Computing</title>
      <link>https://arxiv.org/abs/2509.23179</link>
      <description>arXiv:2509.23179v1 Announce Type: new 
Abstract: Recent advancements in post-quantum cryptographic algorithms have led to their standardization by the National Institute of Standards and Technology (NIST) to safeguard information security in the post-quantum era. These algorithms, however, employ public keys and signatures that are 3 to 9$\times$ longer than those used in pre-quantum cryptography, resulting in significant performance and energy efficiency overheads. A critical bottleneck identified in our analysis is the cache bandwidth. This limitation motivates the adoption of on-chip in-/near-cache computing, a computing paradigm that offers high-performance, exceptional energy efficiency, and flexibility to accelerate post-quantum cryptographic algorithms. Our analysis of existing works reveals challenges in integrating in-/near-cache computing into modern computer systems and performance limitations due to external bandwidth limitation, highlighting the need for innovative solutions that can seamlessly integrate into existing systems without performance and energy efficiency issues. In this paper, we introduce a near-cache-slice computing paradigm with support of customization and virtual address, named Crypto-Near-Cache (CNC), designed to accelerate post-quantum cryptographic algorithms and other applications. By placing SRAM arrays with bitline computing capability near cache slices, high internal bandwidth and short data movement are achieved with native support of virtual addressing. An ISA extension to facilitate CNC is also proposed, with detailed discussion on the implementation aspects of the core/cache datapath.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23179v1</guid>
      <category>cs.AR</category>
      <category>cs.CR</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyao Zhang, Elaheh Sadredini</dc:creator>
    </item>
    <item>
      <title>AssertGen: Enhancement of LLM-aided Assertion Generation through Cross-Layer Signal Bridging</title>
      <link>https://arxiv.org/abs/2509.23674</link>
      <description>arXiv:2509.23674v1 Announce Type: new 
Abstract: Assertion-based verification (ABV) serves as a crucial technique for ensuring that register-transfer level (RTL) designs adhere to their specifications. While Large Language Model (LLM) aided assertion generation approaches have recently achieved remarkable progress, existing methods are still unable to effectively identify the relationship between design specifications and RTL designs, which leads to the insufficiency of the generated assertions. To address this issue, we propose AssertGen, an assertion generation framework that automatically generates SystemVerilog assertions (SVA). AssertGen first extracts verification objectives from specifications using a chain-of-thought (CoT) reasoning strategy, then bridges corresponding signals between these objectives and the RTL code to construct a cross-layer signal chain, and finally generates SVAs based on the LLM. Experimental results demonstrate that AssertGen outperforms the existing state-of-the-art methods across several key metrics, such as pass rate of formal property verification (FPV), cone of influence (COI), proof core and mutation testing coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23674v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongqin Lyu, Yonghao Wang, Yunlin Du, Mingyu Shi, Zhiteng Chao, Wenxing Li, Tiancheng Wang, Huawei Li</dc:creator>
    </item>
    <item>
      <title>ASIC-based Compression Accelerators for Storage Systems: Design, Placement, and Profiling Insights</title>
      <link>https://arxiv.org/abs/2509.23693</link>
      <description>arXiv:2509.23693v1 Announce Type: new 
Abstract: Lossless compression imposes significant computational over head on datacenters when performed on CPUs. Hardware compression and decompression processing units (CDPUs) can alleviate this overhead, but optimal algorithm selection, microarchitectural design, and system-level placement of CDPUs are still not well understood. We present the design of an ASIC-based in-storage CDPU and provide a comprehensive end-to-end evaluation against two leading ASIC accelerators, Intel QAT 8970 and QAT 4xxx. The evaluation spans three dominant CDPU placement regimes: peripheral, on-chip, and in-storage. Our results reveal: (i) acute sensitivity of throughput and latency to CDPU placement and interconnection, (ii) strong correlation between compression efficiency and data patterns/layouts, (iii) placement-driven divergences between microbenchmark gains and real-application speedups, (iv) discrepancies between module and system-level power efficiency, and (v) scalability and multi-tenant interference is sues of various CDPUs. These findings motivate a placement-aware, cross-layer rethinking of hardware (de)compression for hyperscale storage infrastructures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23693v1</guid>
      <category>cs.AR</category>
      <category>cs.OS</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3767295.3769384</arxiv:DOI>
      <dc:creator>Tao Lu, Jiapin Wang, Yelin Shan, Xiangping Zhang, Xiang Chen</dc:creator>
    </item>
    <item>
      <title>AssertFix: Empowering Automated Assertion Fix via Large Language Models</title>
      <link>https://arxiv.org/abs/2509.23972</link>
      <description>arXiv:2509.23972v1 Announce Type: new 
Abstract: Assertion-based verification (ABV) is critical in ensuring that register-transfer level (RTL) designs conform to their functional specifications. SystemVerilog Assertions (SVA) effectively specify design properties, but writing and maintaining them manually is challenging and error-prone. Although recent progress of assertion generation methods leveraging large language models (LLMs) have shown great potential in improving assertion quality, they typically treat assertion generation as a final step, leaving the burden of fixing of the incorrect assertions to human effects, which may significantly limits the application of these methods. To address the above limitation, we propose an automatic assertion fix framework based on LLMs, named AssertFix. AsserFix accurately locates the RTL code related to the incorrect assertion, systematically identifies the root causes of the assertion errors, classifies the error type and finally applies dedicated fix strategies to automatically correct these errors, improving the overall quality of the generated assertions. Experimental results show that AssertFix achieves noticeable improvements in both fix rate and verification coverage across the Opencore benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23972v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongqin Lyu, Yunlin Du, Yonghao Wang, Zhiteng Chao, Tiancheng Wang, Huawei Li</dc:creator>
    </item>
    <item>
      <title>Fault Injection in On-Chip Interconnects: A Comparative Study of Wishbone, AXI-Lite, and AXI</title>
      <link>https://arxiv.org/abs/2509.24929</link>
      <description>arXiv:2509.24929v1 Announce Type: new 
Abstract: Fault injection attacks exploit physical disturbances to compromise the functionality and security of integrated circuits. As System on Chip (SoC) architectures grow in complexity, the vulnerability of on chip communication fabrics has become increasingly prominent. Buses, serving as interconnects among various IP cores, represent potential vectors for fault-based exploitation. In this study, we perform simulation-driven fault injection across three mainstream bus protocols Wishbone, AXI Lite, and AXI. We systematically examine fault success rates, spatial vulnerability distributions, and timing dependencies to characterize how faults interact with bus-level transactions. The results uncover consistent behavioral patterns across protocols, offering practical insights for both attack modeling and the development of resilient SoC designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24929v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongwei Zhao, Vianney Lapotre, Guy Gogniat</dc:creator>
    </item>
    <item>
      <title>Cognition Engines: A Row-Scale HVDC Architecture for Computational Continuity of AI</title>
      <link>https://arxiv.org/abs/2509.22680</link>
      <description>arXiv:2509.22680v1 Announce Type: cross 
Abstract: AI training creates synchronized, step-dominant surges with millisecond edges that destabilize constant-power loads (Choukse et al., 2025; arXiv:2508.14318). We propose a physics-anchored row-scale $\pm 400$ Vdc architecture that makes Computational Continuity a structural property. DRUs supply fast energy via controlled droop; SSTs regulate average power with bounded ramps and no reverse power flow and no high-frequency export at the PCC; import is subjected to a bounded dP/dt envelope; film capacitance and clamps absorb the first edge. The contract is explicit: $\pm 1\%$ steady-band, $\leq 2\%$ transient deviation, $\leq 3$ ms recovery, $\geq 45^{\circ}$ margin, reserve floors intact, yields spine and lowest branches. Recharge is valley-following (admitted only below Avg with MW headroom; $\leq 5$ kW/s per row ramps). Protection is time-graded (branch $\mu$s, row ms, MW seconds). Scaling preserves invariants from row to pod/hall/campus without retuning. Conformance is by waveform evidence (microsecond branch clears, $2\%/50$ ms holds, FLISR with no reverse power flow and no high-frequency export at the PCC). The result is not tuning but a contract for continuity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22680v1</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Paul Churnock</dc:creator>
    </item>
    <item>
      <title>ZKProphet: Understanding Performance of Zero-Knowledge Proofs on GPUs</title>
      <link>https://arxiv.org/abs/2509.22684</link>
      <description>arXiv:2509.22684v1 Announce Type: cross 
Abstract: Zero-Knowledge Proofs (ZKP) are protocols which construct cryptographic proofs to demonstrate knowledge of a secret input in a computation without revealing any information about the secret. ZKPs enable novel applications in private and verifiable computing such as anonymized cryptocurrencies and blockchain scaling and have seen adoption in several real-world systems. Prior work has accelerated ZKPs on GPUs by leveraging the inherent parallelism in core computation kernels like Multi-Scalar Multiplication (MSM). However, we find that a systematic characterization of execution bottlenecks in ZKPs, as well as their scalability on modern GPU architectures, is missing in the literature. This paper presents ZKProphet, a comprehensive performance study of Zero-Knowledge Proofs on GPUs. Following massive speedups of MSM, we find that ZKPs are bottlenecked by kernels like Number-Theoretic Transform (NTT), as they account for up to 90% of the proof generation latency on GPUs when paired with optimized MSM implementations. Available NTT implementations under-utilize GPU compute resources and often do not employ architectural features like asynchronous compute and memory operations. We observe that the arithmetic operations underlying ZKPs execute exclusively on the GPU's 32-bit integer pipeline and exhibit limited instruction-level parallelism due to data dependencies. Their performance is thus limited by the available integer compute units. While one way to scale the performance of ZKPs is adding more compute units, we discuss how runtime parameter tuning for optimizations like precomputed inputs and alternative data representations can extract additional speedup. With this work, we provide the ZKP community a roadmap to scale performance on GPUs and construct definitive GPU-accelerated ZKPs for their application requirements and available hardware resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22684v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.CR</category>
      <category>cs.PF</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tarunesh Verma (Computer Science and Engineering, University of Michigan, USA), Yichao Yuan (Computer Science and Engineering, University of Michigan, USA), Nishil Talati (Computer Science and Engineering, University of Michigan, USA), Todd Austin (Computer Science and Engineering, University of Michigan, USA)</dc:creator>
    </item>
    <item>
      <title>CryptoSRAM: Enabling High-Throughput Cryptography on MCUs via In-SRAM Computing</title>
      <link>https://arxiv.org/abs/2509.22986</link>
      <description>arXiv:2509.22986v1 Announce Type: cross 
Abstract: Secure communication is a critical requirement for Internet of Things (IoT) devices, which are often based on Microcontroller Units (MCUs). Current cryptographic solutions, which rely on software libraries or dedicated hardware accelerators, are fundamentally limited by the performance and energy costs of data movement between memory and processing units. This paper introduces CryptoSRAM, an in-SRAM computing architecture that performs cryptographic operations directly within the MCU's standard SRAM array. By repurposing the memory array into a massively parallel processing fabric, CryptoSRAM eliminates the data movement bottleneck. This approach is well-suited to MCUs, which utilize physical addressing and Direct Memory Access (DMA) to manage SRAM, allowing for seamless integration with minimal hardware overhead. Our analysis shows that for common cryptographic kernels, CryptoSRAM achieves throughput improvements of up to 74$\times$ and 67$\times$ for AES and SHA3, respectively, compared to a software implementation. Furthermore, our solution delivers up to 6$\times$ higher throughput than existing hardware accelerators for AES. CryptoSRAM demonstrates a viable and efficient architecture for secure communication in next-generation IoT systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22986v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyao Zhang, Elaheh Sadredini</dc:creator>
    </item>
    <item>
      <title>FedBit: Accelerating Privacy-Preserving Federated Learning via Bit-Interleaved Packing and Cross-Layer Co-Design</title>
      <link>https://arxiv.org/abs/2509.23091</link>
      <description>arXiv:2509.23091v1 Announce Type: cross 
Abstract: Federated learning (FL) with fully homomorphic encryption (FHE) effectively safeguards data privacy during model aggregation by encrypting local model updates before transmission, mitigating threats from untrusted servers or eavesdroppers in transmission. However, the computational burden and ciphertext expansion associated with homomorphic encryption can significantly increase resource and communication overhead. To address these challenges, we propose FedBit, a hardware/software co-designed framework optimized for the Brakerski-Fan-Vercauteren (BFV) scheme. FedBit employs bit-interleaved data packing to embed multiple model parameters into a single ciphertext coefficient, thereby minimizing ciphertext expansion and maximizing computational parallelism. Additionally, we integrate a dedicated FPGA accelerator to handle cryptographic operations and an optimized dataflow to reduce the memory overhead. Experimental results demonstrate that FedBit achieves a speedup of two orders of magnitude in encryption and lowers average communication overhead by 60.7%, while maintaining high accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23091v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiangchen Meng, Yangdi Lyu</dc:creator>
    </item>
    <item>
      <title>Length-Matching Routing for Programmable Photonic Circuits Using Best-First Strategy</title>
      <link>https://arxiv.org/abs/2509.23463</link>
      <description>arXiv:2509.23463v1 Announce Type: cross 
Abstract: In the realm of programmable photonic integrated circuits (PICs), precise wire length control is crucial for the performance of on-chip programmable components such as optical ring resonators, Mach-Zehnder interferometers, and optical true time-delay lines. Unlike conventional routing algorithms that prioritize shortest-path solutions, these photonic components require exact-length routing to maintain the desired optical properties.
  To address these challenges, this paper presents different length-matching routing strategies to find exact-length paths while balancing search space and runtime efficiently. We propose a novel admissible heuristic estimator and a pruning method, designed to enhance the accuracy and efficiency of the search process. The algorithms are derived from the Best-First search with modified evaluation functions. For two-pin length-matching routing, we formally prove that the proposed algorithms are complete under monotonic heuristics. For multi-pin length-matching challenges, we introduce a pin-ordering mechanism based on detour margins to reduce the likelihood of prematurely blocking feasible routes. Through evaluations on various length-matching benchmarks, we analyze runtime and heuristic performance, demonstrating the effectiveness of the proposed approaches across different layout scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23463v1</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoke Wang, Dirk Stroobandt</dc:creator>
    </item>
    <item>
      <title>BiHDTrans: binary hyperdimensional transformer for efficient multivariate time series classification</title>
      <link>https://arxiv.org/abs/2509.24425</link>
      <description>arXiv:2509.24425v1 Announce Type: cross 
Abstract: The proliferation of Internet-of-Things (IoT) devices has led to an unprecedented volume of multivariate time series (MTS) data, requiring efficient and accurate processing for timely decision-making in resource-constrained edge environments. Hyperdimensional (HD) computing, with its inherent efficiency and parallelizability, has shown promise in classification tasks but struggles to capture complex temporal patterns, while Transformers excel at sequence modeling but incur high computational and memory overhead. We introduce BiHDTrans, an efficient neurosymbolic binary hyperdimensional Transformer that integrates self-attention into the HD computing paradigm, unifying the representational efficiency of HD computing with the temporal modeling power of Transformers. Empirically, BiHDTrans outperforms state-of-the-art (SOTA) HD computing models by at least 14.47% and achieves 6.67% higher accuracy on average than SOTA binary Transformers. With hardware acceleration on FPGA, our pipelined implementation leverages the independent and identically distributed properties of high-dimensional representations, delivering 39.4 times lower inference latency than SOTA binary Transformers. Theoretical analysis shows that binarizing in holographic high-dimensional space incurs significantly less information distortion than directly binarizing neural networks, explaining BiHDTrans's superior accuracy. Furthermore, dimensionality experiments confirm that BiHDTrans remains competitive even with a 64% reduction in hyperspace dimensionality, surpassing SOTA binary Transformers by 1-2% in accuracy with 4.4 times less model size, as well as further reducing the latency by 49.8% compare to the full-dimensional baseline. Together, these contributions bridge the gap between the expressiveness of Transformers and the efficiency of HD computing, enabling accurate, scalable, and low-latency MTS classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24425v1</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingtao Zhang, Yi Liu, Qi Shen, Changhong Wang</dc:creator>
    </item>
    <item>
      <title>OptGM: An Optimized Gate Merging Method to Mitigate NBTI in Digital Circuits</title>
      <link>https://arxiv.org/abs/2506.21487</link>
      <description>arXiv:2506.21487v2 Announce Type: replace 
Abstract: This paper presents OptGM, an optimized gate merging method designed to mitigate negative bias temperature instability (NBTI) in digital circuits. First, the proposed approach effectively identifies NBTI-critical internal nodes, defined as those with a signal probability exceeding a predefined threshold. Next, based on the proposed optimized algorithm, the sensitizer gate (which drives the critical node) and the sensitive gate (which is fed by it) are merged into a new complex gate. This complex gate preserves the original logic while eliminating NBTI-critical nodes. Finally, to evaluate the effectiveness of OptGM, we assess it on several combinational and sequential benchmark circuits. Simulation results demonstrate that, on average, the number of NBTI-critical transistors (i.e., PMOS transistors connected to critical nodes), NBTI-induced delay degradation, and the total transistor count are reduced by 89.29%, 23.87%, and 6.47%, respectively. Furthermore, OptGM enhances performance per cost (PPC) by 12.8% on average, with minimal area overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21487v2</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maryam Ghane, Amir M. Hajisadeghi, Hamid R. Zarandi</dc:creator>
    </item>
    <item>
      <title>SnipSnap: A Joint Compression Format and Dataflow Co-Optimization Framework for Efficient Sparse LLM Accelerator Design</title>
      <link>https://arxiv.org/abs/2509.17072</link>
      <description>arXiv:2509.17072v2 Announce Type: replace 
Abstract: The growing scale of large language models (LLMs) has intensified demands on computation and memory, making efficient inference a key challenge. While sparsity can reduce these costs, existing design space exploration (DSE) frameworks often overlook compression formats, a key factor for leveraging sparsity on accelerators. This paper proposes SnipSnap, a joint compression format and dataflow co-optimization framework for efficient sparse LLM accelerator design. SnipSnap introduces: (1) a hierarchical compression format encoding to expand the design space; (2) an adaptive compression engine for selecting formats under diverse sparsity; and (3) a progressive co-search workflow that jointly optimizes dataflow and compression formats. SnipSnap achieves 18.24% average memory energy savings via format optimization, along with 2248.3$\times$ and 21.0$\times$ speedups over Sparseloop and DiMO-Sparse frameworks, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17072v2</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junyi Wu, Chao Fang, Zhongfeng Wang</dc:creator>
    </item>
    <item>
      <title>DASICS White Paper: Enhancing Memory Protection with Dynamic Compartmentalization</title>
      <link>https://arxiv.org/abs/2310.06435</link>
      <description>arXiv:2310.06435v2 Announce Type: replace-cross 
Abstract: In the existing software development ecosystem, security issues introduced by third-party code cannot be overlooked. Among these security concerns, memory access vulnerabilities stand out prominently, leading to risks such as the theft or tampering of sensitive data. To address this issue, software-based defense mechanisms have been established at the programming language, compiler, and operating system levels. However, as a trade-off, these mechanisms significantly reduce software execution efficiency. Hardware-software co-design approaches have sought to either construct entirely isolated trusted execution environments or attempt to partition security domains within the same address space. While such approaches enhance efficiency compared to pure software methods, they also encounter challenges related to granularity of protection, performance overhead, and portability. In response to these challenges, we present the DASICS (Dynamic in-Address-Space Isolation by Code Segments) secure processor design, which offers dynamic and flexible security protection across multiple privilege levels, addressing data flow protection, control flow protection, and secure system calls. We have implemented hardware FPGA prototypes and software QEMU simulator prototypes based on DASICS, along with necessary modifications to system software for adaptability. We illustrate the protective mechanisms and effectiveness of DASICS with two practical examples and provide potential real-world use cases where DASICS could be applied.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.06435v2</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Jin, Yibin Xu, Chengyuan Yang, Han Wang, Tianyi Huang, Tianyue Lu, Mingyu Chen</dc:creator>
    </item>
    <item>
      <title>Experience Deploying Containerized GenAI Services at an HPC Center</title>
      <link>https://arxiv.org/abs/2509.20603</link>
      <description>arXiv:2509.20603v2 Announce Type: replace-cross 
Abstract: Generative Artificial Intelligence (GenAI) applications are built from specialized components -- inference servers, object storage, vector and graph databases, and user interfaces -- interconnected via web-based APIs. While these components are often containerized and deployed in cloud environments, such capabilities are still emerging at High-Performance Computing (HPC) centers. In this paper, we share our experience deploying GenAI workloads within an established HPC center, discussing the integration of HPC and cloud computing environments. We describe our converged computing architecture that integrates HPC and Kubernetes platforms running containerized GenAI workloads, helping with reproducibility. A case study illustrates the deployment of the Llama Large Language Model (LLM) using a containerized inference server (vLLM) across both Kubernetes and HPC platforms using multiple container runtimes. Our experience highlights practical considerations and opportunities for the HPC container community, guiding future research and tool development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20603v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3731599.3767356</arxiv:DOI>
      <dc:creator>Angel M. Beltre, Jeff Ogden, Kevin Pedretti</dc:creator>
    </item>
  </channel>
</rss>
