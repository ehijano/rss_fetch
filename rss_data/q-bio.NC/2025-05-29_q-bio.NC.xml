<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 May 2025 04:01:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Exploring Holography in Neuro-Vascular Dynamics</title>
      <link>https://arxiv.org/abs/2505.22680</link>
      <description>arXiv:2505.22680v1 Announce Type: new 
Abstract: The holonomic brain theory, originally formulated to account for the need of non-local memory encoding in cognitive systems, could gain new theoretical traction when integrated with holographic principles from physics, most notably the AdS/CFT correspondence. Recent findings in neuroscience suggest that conformal field theories (CFTs), emerging at critical points across spatiotemporal scales in neural dynamics, are essential for brain function. Concurrently, black-brane geometries, long studied in gravitational physics, can find unexpected analogues in the interplay of active matter dynamics and the brain s neuroanatomical organization. Motivated by these parallels, we posit a generalized holographic framework and interrogate its validity through the fluid/gravity duality; a correspondence linking hydrodynamic equations to gravitational spacetime metrics. In this work, we explore the holographic principles at the Navier-Stokes regime, demonstrating that holography can model key neurophysiological mechanisms: cerebral autoregulation (the brain s hemodynamic self-stabilization) and neurovascular coupling (the dynamic neuron-bloodflow interplay). This work bridges holography, active matter physics, and neuroscience, proposing a unified framework to decode the brain s multiscale organization, its resilience to perturbations, and its computational capabilities. By grounding neurovascular physiology in gravitational duals, we open pathways to reinterpret brain function through the lens of emergent spacetime geometry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22680v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Kerskens</dc:creator>
    </item>
    <item>
      <title>ConnectomeDiffuser: Generative AI Enables Brain Network Construction from Diffusion Tensor Imaging</title>
      <link>https://arxiv.org/abs/2505.22683</link>
      <description>arXiv:2505.22683v1 Announce Type: new 
Abstract: Brain network analysis plays a crucial role in diagnosing and monitoring neurodegenerative disorders such as Alzheimer's disease (AD). Existing approaches for constructing structural brain networks from diffusion tensor imaging (DTI) often rely on specialized toolkits that suffer from inherent limitations: operator subjectivity, labor-intensive workflows, and restricted capacity to capture complex topological features and disease-specific biomarkers. To overcome these challenges and advance computational neuroimaging instrumentation, ConnectomeDiffuser is proposed as a novel diffusion-based framework for automated end-to-end brain network construction from DTI. The proposed model combines three key components: (1) a Template Network that extracts topological features from 3D DTI scans using Riemannian geometric principles, (2) a diffusion model that generates comprehensive brain networks with enhanced topological fidelity, and (3) a Graph Convolutional Network classifier that incorporates disease-specific markers to improve diagnostic accuracy. ConnectomeDiffuser demonstrates superior performance by capturing a broader range of structural connectivity and pathology-related information, enabling more sensitive analysis of individual variations in brain networks. Experimental validation on datasets representing two distinct neurodegenerative conditions demonstrates significant performance improvements over other brain network methods. This work contributes to the advancement of instrumentation in the context of neurological disorders, providing clinicians and researchers with a robust, generalizable measurement framework that facilitates more accurate diagnosis, deeper mechanistic understanding, and improved therapeutic monitoring of neurodegenerative diseases such as AD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22683v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xuhang Chen, Michael Kwok-Po Ng, Kim-Fung Tsang, Chi-Man Pun, Shuqiang Wang</dc:creator>
    </item>
    <item>
      <title>Early Assessment of Artificial Lower Extremity Sensory Response Times and Proprioceptive Acuity via Sensory Cortex Electrical Stimulation</title>
      <link>https://arxiv.org/abs/2505.22691</link>
      <description>arXiv:2505.22691v1 Announce Type: new 
Abstract: Bi-directional brain computer interfaces (BD-BCIs) may restore brain-controlled walking and artificial leg sensation after spinal cord injury. Current BD-BCIs provide only simplistic "tingling" feedback, which lacks proprioceptive information to perceive critical gait events (leg swing, double support). This information must also be perceived adequately fast to facilitate timely motor responses. Here, we investigated utilizing primary sensory cortex (S1) direct cortical electrical stimulation (DCES) to deliver leg proprioceptive information and measured response times to artificial leg sensations. Subjects with subdural electrocorticogram electrodes over S1 leg areas participated in two tasks: (1) Proprioceptive acuity: subjects identified the difference between DCES-induced percepts emulating various leg swing speeds; (2) Sensory response: measuring subjects' reaction time to DCES-induced leg sensations, with DCES-hand, visual and auditory control conditions. Three subjects were recruited. Only one completed the proprioceptive assessment, achieving 80%, 70%, 60%, and 53% accuracy in discriminating between fast/slow, fast/medium, medium/slow, and same speeds, respectively (p-value=1.9x10$^{-5}$). Response times for leg/hand percepts were 1007$\pm$413/599$\pm$171 ms, visual leg/hand responses were 528$\pm$137/384$\pm$84 ms, and auditory leg/hand responses were 393$\pm$106/352$\pm$93 ms, respectively. These results suggest proprioceptive information can be delivered artificially, but perception may be significantly delayed. Future work should address improving acuity, reducing response times, and expanding sensory modalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22691v1</guid>
      <category>q-bio.NC</category>
      <category>q-bio.OT</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Won Joon Sohn, Jeffrey Lim, Po T. Wang, Susan J. Shaw, Michelle Armacost, Hui Gong, Brian Lee, Darrin Lee, Payam Heydari, Richard A. Andersen, Charles Y. Liu, Zoran Nenadic, An H. Do</dc:creator>
    </item>
    <item>
      <title>Self-orthogonalizing attractor neural networks emerging from the free energy principle</title>
      <link>https://arxiv.org/abs/2505.22749</link>
      <description>arXiv:2505.22749v1 Announce Type: new 
Abstract: Attractor dynamics are a hallmark of many complex systems, including the brain. Understanding how such self-organizing dynamics emerge from first principles is crucial for advancing our understanding of neuronal computations and the design of artificial intelligence systems. Here we formalize how attractor networks emerge from the free energy principle applied to a universal partitioning of random dynamical systems. Our approach obviates the need for explicitly imposed learning and inference rules and identifies emergent, but efficient and biologically plausible inference and learning dynamics for such self-organizing systems. These result in a collective, multi-level Bayesian active inference process. Attractors on the free energy landscape encode prior beliefs; inference integrates sensory data into posterior beliefs; and learning fine-tunes couplings to minimize long-term surprise. Analytically and via simulations, we establish that the proposed networks favor approximately orthogonalized attractor representations, a consequence of simultaneously optimizing predictive accuracy and model complexity. These attractors efficiently span the input subspace, enhancing generalization and the mutual information between hidden causes and observable effects. Furthermore, while random data presentation leads to symmetric and sparse couplings, sequential data fosters asymmetric couplings and non-equilibrium steady-state dynamics, offering a natural extension to conventional Boltzmann Machines. Our findings offer a unifying theory of self-organizing attractor networks, providing novel insights for AI and neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22749v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tamas Spisak, Karl Friston</dc:creator>
    </item>
    <item>
      <title>The global communication pathways of the human brain transcend the cortical-subcortical-cerebellar division</title>
      <link>https://arxiv.org/abs/2505.22893</link>
      <description>arXiv:2505.22893v1 Announce Type: new 
Abstract: Neural communication across the cortex, subcortex, and cerebellum is orchestrated by the structural connectome, forming the indispensable anatomical framework for capabilities spanning from elementary motor actions to higher cognitive functions. Yet, despite this importance, the core organizational rules that govern this connectivity remain insufficiently understood. Here we show, for the first time, how the integrated cortical, subcortical, and cerebellar brain areas shape the structural architecture of the whole brain. We find dense structural clusters, which differ in composition and arrangement, vertically transverse the canonical cortical, subcortical, and cerebellar boundaries. These clusters are centralized by a global rich club of predominantly subcortical, alongside cortical hub regions. Congruently, we find that subcortical hubs are not only the most widely connected brain areas but are also leading overall structural integration. Nearly all larger subcortical structures encompass these hub regions, but they also exhibit brain regions with fewer but more specialized connections, pointing toward functional heterogeneity in these structures themselves. Our findings move beyond traditional cortico-centric analysis, offering an initial and global perspective for understanding overall structural connectivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22893v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julian Schulte, Mario Senden, Gustavo Deco, Xenia Kobeleva, Gorka Zamora-L\'opez</dc:creator>
    </item>
    <item>
      <title>An open-source Modular Online Psychophysics Platform (MOPP)</title>
      <link>https://arxiv.org/abs/2505.23137</link>
      <description>arXiv:2505.23137v1 Announce Type: new 
Abstract: In recent years, there is a growing need and opportunity to use online platforms for psychophysics research. Online experiments make it possible to evaluate large and diverse populations remotely and quickly, complementing laboratory-based research. However, developing and running online psychophysics experiments poses several challenges: i) a high barrier-to-entry for researchers who often need to learn complex code-based platforms, ii) an uncontrolled experimental environment, and iii) questionable credibility of the participants. Here, we introduce an open-source Modular Online Psychophysics Platform (MOPP) to address these challenges. Through the simple web-based interface of MOPP, researchers can build modular experiments, share them with others, and copy or modify tasks from each others environments. MOPP provides built-in features to calibrate for viewing distance and to measure visual acuity. It also includes email-based and IP-based authentication, and reCAPTCHA verification. We developed five example psychophysics tasks, that come preloaded in the environment, and ran a pilot experiment which was hosted on the AWS (Amazon Web Services) cloud. Pilot data collected for these tasks yielded similar results to those reported in laboratory settings. MOPP can thus help researchers collect large psychophysics datasets online, with reduced turnaround time, and in a standardized manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23137v1</guid>
      <category>q-bio.NC</category>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuval Samoilov-Kats, Matan Noach, Noam Beer, Yuval Efrati, Adam Zaidel</dc:creator>
    </item>
    <item>
      <title>Bridging Critical Gaps in Convergent Learning: How Representational Alignment Evolves Across Layers, Training, and Distribution Shifts</title>
      <link>https://arxiv.org/abs/2502.18710</link>
      <description>arXiv:2502.18710v2 Announce Type: replace 
Abstract: Understanding convergent learning -- the degree to which independently trained neural systems -- whether multiple artificial networks or brains and models -- arrive at similar internal representations -- is crucial for both neuroscience and AI. Yet, the literature remains narrow in scope -- typically examining just a handful of models with one dataset, relying on one alignment metric, and evaluating networks at a single post-training checkpoint. We present a large-scale audit of convergent learning, spanning dozens of vision models and thousands of layer-pair comparisons, to close these long-standing gaps. First, we pit three alignment families against one another -- linear regression (affine-invariant), orthogonal Procrustes (rotation-/reflection-invariant), and permutation/soft-matching (unit-order-invariant). We find that orthogonal transformations align representations nearly as effectively as more flexible linear ones, and although permutation scores are lower, they significantly exceed chance, indicating a privileged representational basis. Tracking convergence throughout training further shows that nearly all eventual alignment crystallizes within the first epoch -- well before accuracy plateaus -- indicating it is largely driven by shared input statistics and architectural biases, not by the final task solution. Finally, when models are challenged with a battery of out-of-distribution images, early layers remain tightly aligned, whereas deeper layers diverge in proportion to the distribution shift. These findings fill critical gaps in our understanding of representational convergence, with implications for neuroscience and AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18710v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chaitanya Kapoor, Sudhanshu Srivastava, Meenakshi Khosla</dc:creator>
    </item>
    <item>
      <title>Measuring and Controlling Solution Degeneracy across Task-Trained Recurrent Neural Networks</title>
      <link>https://arxiv.org/abs/2410.03972</link>
      <description>arXiv:2410.03972v2 Announce Type: replace-cross 
Abstract: Task-trained recurrent neural networks (RNNs) are widely used in neuroscience and machine learning to model dynamical computations. To gain mechanistic insight into how neural systems solve tasks, prior work often reverse-engineers individual trained networks. However, different RNNs trained on the same task and achieving similar performance can exhibit strikingly different internal solutions-a phenomenon known as solution degeneracy. Here, we develop a unified framework to systematically quantify and control solution degeneracy across three levels: behavior, neural dynamics, and weight space. We apply this framework to 3,400 RNNs trained on four neuroscience-relevant tasks-flip-flop memory, sine wave generation, delayed discrimination, and path integration-while systematically varying task complexity, learning regime, network size, and regularization. We find that higher task complexity and stronger feature learning reduce degeneracy in neural dynamics but increase it in weight space, with mixed effects on behavior. In contrast, larger networks and structural regularization reduce degeneracy at all three levels. These findings empirically validate the Contravariance Principle and provide practical guidance for researchers aiming to tailor RNN solutions-whether to uncover shared neural mechanisms or to model individual variability observed in biological systems. This work provides a principled framework for quantifying and controlling solution degeneracy in task-trained RNNs, offering new tools for building more interpretable and biologically grounded models of neural computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03972v2</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>cs.NE</category>
      <category>math.IT</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ann Huang, Satpreet H. Singh, Flavio Martinelli, Kanaka Rajan</dc:creator>
    </item>
    <item>
      <title>Of Mice and Machines: A Comparison of Learning Between Real World Mice and RL Agents</title>
      <link>https://arxiv.org/abs/2505.12204</link>
      <description>arXiv:2505.12204v2 Announce Type: replace-cross 
Abstract: Recent advances in reinforcement learning (RL) have demonstrated impressive capabilities in complex decision-making tasks. This progress raises a natural question: how do these artificial systems compare to biological agents, which have been shaped by millions of years of evolution? To help answer this question, we undertake a comparative study of biological mice and RL agents in a predator-avoidance maze environment. Through this analysis, we identify a striking disparity: RL agents consistently demonstrate a lack of self-preservation instinct, readily risking ``death'' for marginal efficiency gains. These risk-taking strategies are in contrast to biological agents, which exhibit sophisticated risk-assessment and avoidance behaviors. Towards bridging this gap between the biological and artificial, we propose two novel mechanisms that encourage more naturalistic risk-avoidance behaviors in RL agents. Our approach leads to the emergence of naturalistic behaviors, including strategic environment assessment, cautious path planning, and predator avoidance patterns that closely mirror those observed in biological systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12204v2</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuo Han, German Espinosa, Junda Huang, Daniel A. Dombeck, Malcolm A. MacIver, Bradly C. Stadie</dc:creator>
    </item>
  </channel>
</rss>
