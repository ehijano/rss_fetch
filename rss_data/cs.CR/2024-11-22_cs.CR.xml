<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CR</link>
    <description>cs.CR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Nov 2024 05:00:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Multi-Server Information-Sharing Environment for Cross-Party Collaboration on A Private Cloud</title>
      <link>https://arxiv.org/abs/2411.13580</link>
      <description>arXiv:2411.13580v1 Announce Type: new 
Abstract: Interoperability remains the key problem in multi-discipline collaboration based on building information modeling (BIM). Although various methods have been proposed to solve the technical issues of interoperability, such as data sharing and data consistency; organizational issues, including data ownership and data privacy, remain unresolved to date. These organizational issues prevent different stakeholders from sharing their data due to concerns regarding losing control of the data. This study proposes a multi-server information-sharing approach on a private cloud after analyzing the requirements for cross-party collaboration to address the aforementioned issues and prepare for massive data handling in the near future. This approach adopts a global controller to track the location, ownership and privacy of the data, which are stored in different servers that are controlled by different parties. Furthermore, data consistency conventions, parallel sub-model extraction, and sub-model integration with model verification are investigated in depth to support information sharing in a distributed environment and to maintain data consistency. Thus, with this approach, the ownership and privacy of the data can be controlled by its owner while still enabling certain required data to be shared with other parties. Application of the multi-server approach for information interoperability and cross-party collaboration is illustrated using a real construction project of an airport terminal. Validation shows that the proposed approach is feasible for maintaining the ownership and privacy of the data while supporting cross-party data sharing and collaboration at the same time, thus avoiding possible legal problems regarding data copyrights or other legal issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13580v1</guid>
      <category>cs.CR</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.autcon.2017.06.021</arxiv:DOI>
      <arxiv:journal_reference>Automation in Construction,2017</arxiv:journal_reference>
      <dc:creator>Jianping Zhang, Qiang Liu, Zhenzhong Hu, Jiarui Lin, Fangqiang Yu</dc:creator>
    </item>
    <item>
      <title>Browser Extension for Fake URL Detection</title>
      <link>https://arxiv.org/abs/2411.13581</link>
      <description>arXiv:2411.13581v1 Announce Type: new 
Abstract: In recent years, Cyber attacks have increased in number, and with them, the intensity of the attacks and their potential to damage the user have also increased significantly. In an ever-advancing world, users find it difficult to keep up with the latest developments in technology, which can leave them vulnerable to attacks. To avoid such situations we need tools to deter such attacks, for this machine learning models are among the best options. This paper presents a Browser Extension that uses machine learning models to enhance online security by integrating three crucial functionalities: Malicious URL detection, Spam Email detection and Network logs analysis. The proposed solution uses LGBM classifier for classification of Phishing websites, the model has been trained on a dataset with 87 features, this model achieved an accuracy of 96.5% with a precision of 96.8% and F1 score of 96.49%. The Model for Spam email detection uses Multinomial NB algorithm which has been trained on a dataset with over 5500 messages, this model achieved an accuracy of 97.09% with a precision of 100%. The results demonstrate the effectiveness of using machine learning models for cyber security.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13581v1</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Latesh G. Malik, Rohini Shambharkar, Shivam Morey, Shubhlak Kanpate, Vedika Raut</dc:creator>
    </item>
    <item>
      <title>Enhanced FIWARE-Based Architecture for Cyberphysical Systems With Tiny Machine Learning and Machine Learning Operations: A Case Study on Urban Mobility Systems</title>
      <link>https://arxiv.org/abs/2411.13583</link>
      <description>arXiv:2411.13583v1 Announce Type: new 
Abstract: The rise of AI and the Internet of Things is accelerating the digital transformation of society. Mobility computing presents specific barriers due to its real-time requirements, decentralization, and connectivity through wireless networks. New research on edge computing and tiny machine learning (tinyML) explores the execution of AI models on low-performance devices to address these issues. However, there are not many studies proposing agnostic architectures that manage the entire lifecycle of intelligent cyberphysical systems. This article extends a previous architecture based on FIWARE software components to implement the machine learning operations flow, enabling the management of the entire tinyML lifecycle in cyberphysical systems. We also provide a use case to showcase how to implement the FIWARE architecture through a complete example of a smart traffic system. We conclude that the FIWARE ecosystem constitutes a real reference option for developing tinyML and edge computing in cyberphysical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13583v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/MITP.2024.3421968</arxiv:DOI>
      <arxiv:journal_reference>IT Professional ( Volume: 26, Issue: 5, Sept.-Oct. 2024)</arxiv:journal_reference>
      <dc:creator>Javier Conde, Andr\'es Munoz-Arcentales, \'Alvaro Alonso, Joaqu\'in Salvach\'ua, Gabriel Huecas</dc:creator>
    </item>
    <item>
      <title>Artificial Intelligence in Cybersecurity: Building Resilient Cyber Diplomacy Frameworks</title>
      <link>https://arxiv.org/abs/2411.13585</link>
      <description>arXiv:2411.13585v1 Announce Type: new 
Abstract: This paper explores how automation and artificial intelligence (AI) are transforming U.S. cyber diplomacy. Leveraging these technologies helps the U.S. manage the complexity and urgency of cyber diplomacy, improving decision-making, efficiency, and security. As global inter connectivity grows, cyber diplomacy, managing national interests in the digital space has become vital. The ability of AI and automation to quickly process vast data volumes enables timely responses to cyber threats and opportunities. This paper underscores the strategic integration of these tools to maintain U.S. competitive advantage and secure national interests. Automation enhances diplomatic communication and data processing, freeing diplomats to focus on strategic decisions. AI supports predictive analytics and real time decision making, offering critical insights and proactive measures during high stakes engagements. Case studies show AIs effectiveness in monitoring cyber activities and managing international cyber policy. Challenges such as ethical concerns, security vulnerabilities, and reliance on technology are also addressed, emphasizing human oversight and strong governance frameworks. Ensuring proper ethical guidelines and cybersecurity measures allows the U.S. to harness the benefits of automation and AI while mitigating risks. By adopting these technologies, U.S. cyber diplomacy can become more proactive and effective, navigating the evolving digital landscape with greater agility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13585v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Stoltz</dc:creator>
    </item>
    <item>
      <title>Preserving Expert-Level Privacy in Offline Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2411.13598</link>
      <description>arXiv:2411.13598v1 Announce Type: new 
Abstract: The offline reinforcement learning (RL) problem aims to learn an optimal policy from historical data collected by one or more behavioural policies (experts) by interacting with an environment. However, the individual experts may be privacy-sensitive in that the learnt policy may retain information about their precise choices. In some domains like personalized retrieval, advertising and healthcare, the expert choices are considered sensitive data. To provably protect the privacy of such experts, we propose a novel consensus-based expert-level differentially private offline RL training approach compatible with any existing offline RL algorithm. We prove rigorous differential privacy guarantees, while maintaining strong empirical performance. Unlike existing work in differentially private RL, we supplement the theory with proof-of-concept experiments on classic RL environments featuring large continuous state spaces, demonstrating substantial improvements over a natural baseline across multiple tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13598v1</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Navodita Sharma, Vishnu Vinod, Abhradeep Thakurta, Alekh Agarwal, Borja Balle, Christoph Dann, Aravindan Raghuveer</dc:creator>
    </item>
    <item>
      <title>Efficient Streaming Voice Steganalysis in Challenging Detection Scenarios</title>
      <link>https://arxiv.org/abs/2411.13612</link>
      <description>arXiv:2411.13612v1 Announce Type: new 
Abstract: In recent years, there has been an increasing number of information hiding techniques based on network streaming media, focusing on how to covertly and efficiently embed secret information into real-time transmitted network media signals to achieve concealed communication. The misuse of these techniques can lead to significant security risks, such as the spread of malicious code, commands, and viruses. Current steganalysis methods for network voice streams face two major challenges: efficient detection under low embedding rates and short duration conditions. These challenges arise because, with low embedding rates (e.g., as low as 10%) and short transmission durations (e.g., only 0.1 second), detection models struggle to acquire sufficiently rich sample features, making effective steganalysis difficult. To address these challenges, this paper introduces a Dual-View VoIP Steganalysis Framework (DVSF). The framework first randomly obfuscates parts of the native steganographic descriptors in VoIP stream segments, making the steganographic features of hard-to-detect samples more pronounced and easier to learn. It then captures fine-grained local features related to steganography, building on the global features of VoIP. Specially constructed VoIP segment triplets further adjust the feature distances within the model. Ultimately, this method effectively address the detection difficulty in VoIP. Extensive experiments demonstrate that our method significantly improves the accuracy of streaming voice steganalysis in these challenging detection scenarios, surpassing existing state-of-the-art methods and offering superior near-real-time performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13612v1</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pengcheng Zhou, Zhengyang Fang, Zhongliang Yang, Zhili Zhou, Linna Zhou</dc:creator>
    </item>
    <item>
      <title>CryptoFormalEval: Integrating LLMs and Formal Verification for Automated Cryptographic Protocol Vulnerability Detection</title>
      <link>https://arxiv.org/abs/2411.13627</link>
      <description>arXiv:2411.13627v1 Announce Type: new 
Abstract: Cryptographic protocols play a fundamental role in securing modern digital infrastructure, but they are often deployed without prior formal verification. This could lead to the adoption of distributed systems vulnerable to attack vectors. Formal verification methods, on the other hand, require complex and time-consuming techniques that lack automatization. In this paper, we introduce a benchmark to assess the ability of Large Language Models (LLMs) to autonomously identify vulnerabilities in new cryptographic protocols through interaction with Tamarin: a theorem prover for protocol verification. We created a manually validated dataset of novel, flawed, communication protocols and designed a method to automatically verify the vulnerabilities found by the AI agents. Our results about the performances of the current frontier models on the benchmark provides insights about the possibility of cybersecurity applications by integrating LLMs with symbolic reasoning systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13627v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.SC</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Cristian Curaba, Denis D'Ambrosi, Alessandro Minisini, Natalia P\'erez-Campanero Antol\'in</dc:creator>
    </item>
    <item>
      <title>PairSonic: Helping Groups Securely Exchange Contact Information</title>
      <link>https://arxiv.org/abs/2411.13693</link>
      <description>arXiv:2411.13693v1 Announce Type: new 
Abstract: Securely exchanging contact information is essential for establishing trustworthy communication channels that facilitate effective online collaboration. However, current methods are neither user-friendly nor scalable for large groups of users. In response, we introduce PairSonic, a novel group pairing protocol that extends trust from physical encounters to online communication. PairSonic simplifies the pairing process by automating the tedious verification tasks of previous methods through an acoustic out-of-band channel using smartphones' built-in hardware. Our protocol not only facilitates connecting users for computer-supported collaboration, but also provides a more user-friendly and scalable solution to the authentication ceremonies currently used in end-to-end encrypted messengers like Signal or WhatsApp. PairSonic is available as open-source software: https://github.com/seemoo-lab/pairsonic</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13693v1</guid>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <category>cs.NI</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3678884.3681818</arxiv:DOI>
      <arxiv:journal_reference>ACM CSCW 2024</arxiv:journal_reference>
      <dc:creator>Florentin Putz, Steffen Haesler, Thomas V\"olkl, Maximilian Gehring, Nils Rollshausen, Matthias Hollick</dc:creator>
    </item>
    <item>
      <title>Test Security in Remote Testing Age: Perspectives from Process Data Analytics and AI</title>
      <link>https://arxiv.org/abs/2411.13699</link>
      <description>arXiv:2411.13699v1 Announce Type: new 
Abstract: The COVID-19 pandemic has accelerated the implementation and acceptance of remotely proctored high-stake assessments. While the flexible administration of the tests brings forth many values, it raises test security-related concerns. Meanwhile, artificial intelligence (AI) has witnessed tremendous advances in the last five years. Many AI tools (such as the very recent ChatGPT) can generate high-quality responses to test items. These new developments require test security research beyond the statistical analysis of scores and response time. Data analytics and AI methods based on clickstream process data can get us deeper insight into the test-taking process and hold great promise for securing remotely administered high-stakes tests. This chapter uses real-world examples to show that this is indeed the case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13699v1</guid>
      <category>cs.CR</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiangang Hao, Michael Fauss</dc:creator>
    </item>
    <item>
      <title>AttentionBreaker: Adaptive Evolutionary Optimization for Unmasking Vulnerabilities in LLMs through Bit-Flip Attacks</title>
      <link>https://arxiv.org/abs/2411.13757</link>
      <description>arXiv:2411.13757v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have revolutionized natural language processing (NLP), excelling in tasks like text generation and summarization. However, their increasing adoption in mission-critical applications raises concerns about hardware-based threats, particularly bit-flip attacks (BFAs). BFAs, enabled by fault injection methods such as Rowhammer, target model parameters in memory, compromising both integrity and performance. Identifying critical parameters for BFAs in the vast parameter space of LLMs poses significant challenges. While prior research suggests transformer-based architectures are inherently more robust to BFAs compared to traditional deep neural networks, we challenge this assumption. For the first time, we demonstrate that as few as three bit-flips can cause catastrophic performance degradation in an LLM with billions of parameters. Current BFA techniques are inadequate for exploiting this vulnerability due to the difficulty of efficiently identifying critical parameters within the immense parameter space. To address this, we propose AttentionBreaker, a novel framework tailored for LLMs that enables efficient traversal of the parameter space to identify critical parameters. Additionally, we introduce GenBFA, an evolutionary optimization strategy designed to refine the search further, isolating the most critical bits for an efficient and effective attack. Empirical results reveal the profound vulnerability of LLMs to AttentionBreaker. For example, merely three bit-flips (4.129 x 10^-9% of total parameters) in the LLaMA3-8B-Instruct 8-bit quantized (W8) model result in a complete performance collapse: accuracy on MMLU tasks drops from 67.3% to 0%, and Wikitext perplexity skyrockets from 12.6 to 4.72 x 10^5. These findings underscore the effectiveness of AttentionBreaker in uncovering and exploiting critical vulnerabilities within LLM architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13757v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sanjay Das, Swastik Bhattacharya, Souvik Kundu, Shamik Kundu, Anand Menon, Arnab Raha, Kanad Basu</dc:creator>
    </item>
    <item>
      <title>$d_X$-Privacy for Text and the Curse of Dimensionality</title>
      <link>https://arxiv.org/abs/2411.13784</link>
      <description>arXiv:2411.13784v1 Announce Type: new 
Abstract: A widely used method to ensure privacy of unstructured text data is the multidimensional Laplace mechanism for $d_X$-privacy, which is a relaxation of differential privacy for metric spaces. We identify an intriguing peculiarity of this mechanism. When applied on a word-by-word basis, the mechanism either outputs the original word, or completely dissimilar words, and very rarely any semantically similar words. We investigate this observation in detail, and tie it to the fact that the distance of the nearest neighbor of a word in any word embedding model (which are high-dimensional) is much larger than the relative difference in distances to any of its two consecutive neighbors. We also show that the dot product of the multidimensional Laplace noise vector with any word embedding plays a crucial role in designating the nearest neighbor. We derive the distribution, moments and tail bounds of this dot product. We further propose a fix as a post-processing step, which satisfactorily removes the above-mentioned issue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13784v1</guid>
      <category>cs.CR</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hassan Jameel Asghar, Robin Carpentier, Benjamin Zi Hao Zhao, Dali Kaafar</dc:creator>
    </item>
    <item>
      <title>Robust Steganography with Boundary-Preserving Overflow Alleviation and Adaptive Error Correction</title>
      <link>https://arxiv.org/abs/2411.13819</link>
      <description>arXiv:2411.13819v1 Announce Type: new 
Abstract: With the rapid evolution of the Internet, the vast amount of data has created opportunities for fostering the development of steganographic techniques. However, traditional steganographic techniques encounter challenges due to distortions in online social networks, such as JPEG recompression. Presently, research into the lossy operations of spatial truncation in JPEG recompression remains limited. Existing methods aim to ensure the stability of the quantized coefficients by reducing the effects of spatial truncation. Nevertheless, these approaches may induce notable alterations to image pixels, potentially compromising anti-steganalysis performance. In this study, we analyzed the overflow characteristics of spatial blocks and observed that pixel values at the boundaries of spatial blocks are more prone to overflow. Building upon this observation, we proposed a preprocessing method that performs overflow removal operations based on the actual overflow conditions of spatial blocks. After preprocessing, our algorithm enhances coefficient stability while minimizing modifications to spatial block boundaries, favoring image quality preservation. Subsequently, we employed adaptive error correction coding to reduce coding redundancy, thereby augmenting robustness and mitigating its impact on anti-steganalysis performance. The experimental results indicate that the proposed method possesses a strong embedding capacity, maintaining a high level of robustness while enhancing security.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13819v1</guid>
      <category>cs.CR</category>
      <category>cs.MM</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Cheng, Zhenlin Luo, Zhaoxia Yin</dc:creator>
    </item>
    <item>
      <title>Designing a Secure Device-to-Device File Transfer Mechanism</title>
      <link>https://arxiv.org/abs/2411.13827</link>
      <description>arXiv:2411.13827v1 Announce Type: new 
Abstract: Secure, reliable, and fast transfer of files across the Internet is a problem attempted to be solved through many application-layer protocols. In this paper, we aim to design a secure, reliable, opendesign, and performant file transfer protocol that is inspired by the WebRTC protocol stack. Traditionally, transferring files involves a publicly exposed (available on the public network) third-party server that serves the uploaded files to the receiver. Here, the third party server has to bear the storage and bandwidth cost to transfer the files between the two parties. We propose a protocol that uses a relay server to relay the files from the client to the server. A relay server has several advantages over a regular file-hosting server. Firstly, a relay server does not retain the uploaded files, it simply relays them. Secondly, a relay server has a full-duplex communication channel and therefore the receiver is not required to wait for the sender to upload the files completely. In this paper, we study available file transfer approaches and their known flaws. We propose our idea and compare our stack with the WebRTC stack. Finally, we perform empirical analysis and, benchmark our device-to-device transfer approach along with other available options including WebRTC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13827v1</guid>
      <category>cs.CR</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chaitanya Rahalkar, Anushka Virgaonkar</dc:creator>
    </item>
    <item>
      <title>Next-Generation Phishing: How LLM Agents Empower Cyber Attackers</title>
      <link>https://arxiv.org/abs/2411.13874</link>
      <description>arXiv:2411.13874v1 Announce Type: new 
Abstract: The escalating threat of phishing emails has become increasingly sophisticated with the rise of Large Language Models (LLMs). As attackers exploit LLMs to craft more convincing and evasive phishing emails, it is crucial to assess the resilience of current phishing defenses. In this study we conduct a comprehensive evaluation of traditional phishing detectors, such as Gmail Spam Filter, Apache SpamAssassin, and Proofpoint, as well as machine learning models like SVM, Logistic Regression, and Naive Bayes, in identifying both traditional and LLM-rephrased phishing emails. We also explore the emerging role of LLMs as phishing detection tools, a method already adopted by companies like NTT Security Holdings and JPMorgan Chase. Our results reveal notable declines in detection accuracy for rephrased emails across all detectors, highlighting critical weaknesses in current phishing defenses. As the threat landscape evolves, our findings underscore the need for stronger security controls and regulatory oversight on LLM-generated content to prevent its misuse in creating advanced phishing attacks. This study contributes to the development of more effective Cyber Threat Intelligence (CTI) by leveraging LLMs to generate diverse phishing variants that can be used for data augmentation, harnessing the power of LLMs to enhance phishing detection, and paving the way for more robust and adaptable threat detection systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13874v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khalifa Afane, Wenqi Wei, Ying Mao, Junaid Farooq, Juntao Chen</dc:creator>
    </item>
    <item>
      <title>RISecure-PUF: Multipurpose PUF-Driven Security Extensions with Lookaside Buffer in RISC-V</title>
      <link>https://arxiv.org/abs/2411.14025</link>
      <description>arXiv:2411.14025v1 Announce Type: new 
Abstract: RISC-V's limited security features hinder its use in confidential computing and heterogeneous platforms. This paper introduces RISecure-PUF, a security extension utilizing existing Physical Unclonable Functions for key generation and secure protocol purposes. A one-way hash function is integrated to ensure provable security against modeling attacks, while a lookaside buffer accelerates batch sampling and minimizes reliance on error correction codes. Implemented on the Genesys 2 FPGA, RISecure-PUF improves at least $2.72\times$ in batch scenarios with negligible hardware overhead and a maximum performance reduction of $10.7\%$, enabled by reusing the hash function module in integrated environments such as cryptographic engines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14025v1</guid>
      <category>cs.CR</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chenghao Chen, Xiaolin Zhang, Kailun Qin, Tengfei Wang, Yipeng Shi, Tianyi Huang, Chi Zhang, Dawu Gu</dc:creator>
    </item>
    <item>
      <title>Relation-aware based Siamese Denoising Autoencoder for Malware Few-shot Classification</title>
      <link>https://arxiv.org/abs/2411.14029</link>
      <description>arXiv:2411.14029v1 Announce Type: new 
Abstract: When malware employs an unseen zero-day exploit, traditional security measures such as vulnerability scanners and antivirus software can fail to detect them. This is because these tools rely on known patches and signatures, which do not exist for new zero-day attacks. Furthermore, existing machine learning methods, which are trained on specific and occasionally outdated malware samples, may struggle to adapt to features in new malware. To address this issue, there is a need for a more robust machine learning model that can identify relationships between malware samples without being trained on a particular malware feature set. This is particularly crucial in the field of cybersecurity, where the number of malware samples is limited and obfuscation techniques are widely used. Current approaches using stacked autoencoders aim to remove the noise introduced by obfuscation techniques through reconstruction of the input. However, this approach ignores the semantic relationships between features across different malware samples. To overcome this limitation, we propose a novel Siamese Neural Network (SNN) that uses relation-aware embeddings to calculate more accurate similarity probabilities based on semantic details of different malware samples. In addition, by using entropy images as inputs, our model can extract better structural information and subtle differences in malware signatures, even in the presence of obfuscation techniques. Evaluations on two large malware sample sets using the N-shot and N-way methods show that our proposed model is highly effective in predicting previously unseen malware, even in the presence of obfuscation techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14029v1</guid>
      <category>cs.CR</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinting Zhu, Julian Jang-Jaccard, Ian Welch, Harith AI-Sahaf, Seyit Camtepe, Aeryn Dunmore, Cybersecurity Lab</dc:creator>
    </item>
    <item>
      <title>RAG-Thief: Scalable Extraction of Private Data from Retrieval-Augmented Generation Applications with Agent-based Attacks</title>
      <link>https://arxiv.org/abs/2411.14110</link>
      <description>arXiv:2411.14110v1 Announce Type: new 
Abstract: While large language models (LLMs) have achieved notable success in generative tasks, they still face limitations, such as lacking up-to-date knowledge and producing hallucinations. Retrieval-Augmented Generation (RAG) enhances LLM performance by integrating external knowledge bases, providing additional context which significantly improves accuracy and knowledge coverage. However, building these external knowledge bases often requires substantial resources and may involve sensitive information. In this paper, we propose an agent-based automated privacy attack called RAG-Thief, which can extract a scalable amount of private data from the private database used in RAG applications. We conduct a systematic study on the privacy risks associated with RAG applications, revealing that the vulnerability of LLMs makes the private knowledge bases suffer significant privacy risks. Unlike previous manual attacks which rely on traditional prompt injection techniques, RAG-Thief starts with an initial adversarial query and learns from model responses, progressively generating new queries to extract as many chunks from the knowledge base as possible. Experimental results show that our RAG-Thief can extract over 70% information from the private knowledge bases within customized RAG applications deployed on local machines and real-world platforms, including OpenAI's GPTs and ByteDance's Coze. Our findings highlight the privacy vulnerabilities in current RAG applications and underscore the pressing need for stronger safeguards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14110v1</guid>
      <category>cs.CR</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Changyue Jiang, Xudong Pan, Geng Hong, Chenfu Bao, Min Yang</dc:creator>
    </item>
    <item>
      <title>AnywhereDoor: Multi-Target Backdoor Attacks on Object Detection</title>
      <link>https://arxiv.org/abs/2411.14243</link>
      <description>arXiv:2411.14243v1 Announce Type: new 
Abstract: As object detection becomes integral to many safety-critical applications, understanding its vulnerabilities is essential. Backdoor attacks, in particular, pose a significant threat by implanting hidden backdoor in a victim model, which adversaries can later exploit to trigger malicious behaviors during inference. However, current backdoor techniques are limited to static scenarios where attackers must define a malicious objective before training, locking the attack into a predetermined action without inference-time adaptability. Given the expressive output space in object detection, including object existence detection, bounding box estimation, and object classification, the feasibility of implanting a backdoor that provides inference-time control with a high degree of freedom remains unexplored. This paper introduces AnywhereDoor, a flexible backdoor attack tailored for object detection. Once implanted, AnywhereDoor enables adversaries to specify different attack types (object vanishing, fabrication, or misclassification) and configurations (untargeted or targeted with specific classes) to dynamically control detection behavior. This flexibility is achieved through three key innovations: (i) objective disentanglement to support a broader range of attack combinations well beyond what existing methods allow; (ii) trigger mosaicking to ensure backdoor activations are robust, even against those object detectors that extract localized regions from the input image for recognition; and (iii) strategic batching to address object-level data imbalances that otherwise hinders a balanced manipulation. Extensive experiments demonstrate that AnywhereDoor provides attackers with a high degree of control, achieving an attack success rate improvement of nearly 80% compared to adaptations of existing methods for such flexible control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14243v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jialin Lu, Junjie Shan, Ziqi Zhao, Ka-Ho Chow</dc:creator>
    </item>
    <item>
      <title>Pulsar Consensus</title>
      <link>https://arxiv.org/abs/2411.14245</link>
      <description>arXiv:2411.14245v1 Announce Type: new 
Abstract: In this paper, we informally introduce the Pulsar proof of stake consensus paper and discuss the relevant design decisions and considerations. The Pulsar protocol we propose is designed to facilitate the creation of a proof of stake sidechain for a proof of work blockchain. We present an overview of a novel composable density-based chain selection rule for proof of stake systems which can be seen as a superset of some standard existing longest chain rules for proof of stake protocols. We discuss the Pulsar protocol in comparison to existing proof of stake protocols and define its benefits over existing designs while defining the limitations of the work. Pulsar is currently implemented in the Mintlayer proof of stake Bitcoin sidechain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14245v1</guid>
      <category>cs.CR</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samer Afach, Benjamin Marsh, Enrico Rubboli</dc:creator>
    </item>
    <item>
      <title>Adaptive Anomaly Detection for Identifying Attacks in Cyber-Physical Systems: A Systematic Literature Review</title>
      <link>https://arxiv.org/abs/2411.14278</link>
      <description>arXiv:2411.14278v1 Announce Type: new 
Abstract: Modern cyberattacks in cyber-physical systems (CPS) rapidly evolve and cannot be deterred effectively with most current methods which focused on characterizing past threats. Adaptive anomaly detection (AAD) is among the most promising techniques to detect evolving cyberattacks focused on fast data processing and model adaptation. AAD has been researched in the literature extensively; however, to the best of our knowledge, our work is the first systematic literature review (SLR) on the current research within this field. We present a comprehensive SLR, gathering 397 relevant papers and systematically analyzing 65 of them (47 research and 18 survey papers) on AAD in CPS studies from 2013 to 2023 (November). We introduce a novel taxonomy considering attack types, CPS application, learning paradigm, data management, and algorithms. Our analysis indicates, among other findings, that reviewed works focused on a single aspect of adaptation (either data processing or model adaptation) but rarely in both at the same time. We aim to help researchers to advance the state of the art and help practitioners to become familiar with recent progress in this field. We identify the limitations of the state of the art and provide recommendations for future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14278v1</guid>
      <category>cs.CR</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pablo Moriano, Steven C. Hespeler, Mingyan Li, Maria Mahbub</dc:creator>
    </item>
    <item>
      <title>Securing Legacy Communication Networks via Authenticated Cyclic Redundancy Integrity Check</title>
      <link>https://arxiv.org/abs/2411.14394</link>
      <description>arXiv:2411.14394v1 Announce Type: new 
Abstract: Integrating modern communication technologies into legacy systems, such as Industrial Control Systems and in-vehicle networks, invalidates the assumptions of isolated and trusted operating environments. Security incidents like the 2015 Ukraine power grid attack and the 2021 compromise of a U.S. water treatment facility demonstrate how increased interconnectivity, paired with insufficient security measures, expose these critical systems to cyber threats, posing risks to national and public safety. These attacks were favored by the lack of proper message authentication, highlighting its importance as a primary countermeasure to enhance system security. Solutions proposed in the literature remain largely unadopted in practice due to challenges such as preserving backward compatibility, additional hardware requirements, and limited computational resources on legacy devices. Moreover, many solutions are protocol-specific, necessitating complex and costly multiple implementations in heterogeneous systems.
  In this paper, we propose Authenticated Cyclic Redundancy Integrity Check (ACRIC), a novel security mechanism that overcomes these limitations by leveraging a cryptographic computation of the existing Cyclyic Redundancy Check (CRC) field to ensure message integrity protection and authentication. ACRIC preserves backward compatibility without requiring additional hardware and is protocol agnostic. This makes it applicable across various systems, suitable for diverse legacy network protocols including point-to-point and broadcast communications. Experimental results, supported by formal verification and real-world testing, demonstrate that ACRIC offers robust security with minimal transmission overhead (&lt;&lt; 1 ms). This proves ACRIC's practicality, cost-effectiveness, and suitability for real-world adoption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14394v1</guid>
      <category>cs.CR</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alessandro Lotto, Alessandro Brighente, Mauro Conti</dc:creator>
    </item>
    <item>
      <title>Why the p-norms $p{=}1$, $p{=}2$ and $p{=}\infty$ are so special? An answer based on spatial uniformity</title>
      <link>https://arxiv.org/abs/2411.13567</link>
      <description>arXiv:2411.13567v1 Announce Type: cross 
Abstract: Among all metrics based on p-norms, the Manhattan (p=1), euclidean (p=2) and Chebyshev distances (p=infinity) are the most widely used for their interpretability, simplicity and technical convenience. But these are not the only arguments for the ubiquity of these three p-norms. This article proves that there is a volume-surface correspondence property that is unique to them. More precisely, it is shown that sampling uniformly from the volume of an n-dimensional p-ball and projecting to its surface is equivalent to directly sampling uniformly from its surface if and only if p is 1, 2 or infinity. Sampling algorithms and their implementations in Python are also provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13567v1</guid>
      <category>math.ST</category>
      <category>cs.CR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Pinz\'on</dc:creator>
    </item>
    <item>
      <title>Differentially Private Learning Beyond the Classical Dimensionality Regime</title>
      <link>https://arxiv.org/abs/2411.13682</link>
      <description>arXiv:2411.13682v1 Announce Type: cross 
Abstract: We initiate the study of differentially private learning in the proportional dimensionality regime, in which the number of data samples $n$ and problem dimension $d$ approach infinity at rates proportional to one another, meaning that $d / n \to \delta$ as $n \to \infty$ for an arbitrary, given constant $\delta \in (0, \infty)$. This setting is significantly more challenging than that of all prior theoretical work in high-dimensional differentially private learning, which, despite the name, has assumed that $\delta = 0$ or is sufficiently small for problems of sample complexity $O(d)$, a regime typically considered "low-dimensional" or "classical" by modern standards in high-dimensional statistics.
  We provide sharp theoretical estimates of the error of several well-studied differentially private algorithms for robust linear regression and logistic regression, including output perturbation, objective perturbation, and noisy stochastic gradient descent, in the proportional dimensionality regime. The $1 + o(1)$ factor precision of our error estimates enables a far more nuanced understanding of the price of privacy of these algorithms than that afforded by existing, coarser analyses, which are essentially vacuous in the regime we consider.
  We incorporate several probabilistic tools that have not previously been used to analyze differentially private learning algorithms, such as a modern Gaussian comparison inequality and recent universality laws with origins in statistical physics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13682v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cynthia Dwork, Pranay Tankala, Linjun Zhang</dc:creator>
    </item>
    <item>
      <title>Sounds Good? Fast and Secure Contact Exchange in Groups</title>
      <link>https://arxiv.org/abs/2411.13694</link>
      <description>arXiv:2411.13694v1 Announce Type: cross 
Abstract: Trustworthy digital communication requires the secure exchange of contact information, but current approaches lack usability and scalability for larger groups of users. We evaluate the usability of two secure contact exchange systems: the current state of the art, SafeSlinger, and our newly designed protocol, PairSonic, which extends trust from physical encounters to spontaneous online communication. Our lab study (N=45) demonstrates PairSonic's superior usability, automating the tedious verification tasks from previous approaches via an acoustic out-of-band channel. Although participants significantly preferred our system, minimizing user effort surprisingly decreased the perceived security for some users, who associated security with complexity. We discuss user perceptions of the different protocol components and identify remaining usability barriers for CSCW application scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13694v1</guid>
      <category>cs.HC</category>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3686964</arxiv:DOI>
      <arxiv:journal_reference>Proc. ACM Hum.-Comput. Interact. 8, CSCW2, Article 425 (November 2024), 44 pages</arxiv:journal_reference>
      <dc:creator>Florentin Putz, Steffen Haesler, Matthias Hollick</dc:creator>
    </item>
    <item>
      <title>A Survey on Adversarial Robustness of LiDAR-based Machine Learning Perception in Autonomous Vehicles</title>
      <link>https://arxiv.org/abs/2411.13778</link>
      <description>arXiv:2411.13778v1 Announce Type: cross 
Abstract: In autonomous driving, the combination of AI and vehicular technology offers great potential. However, this amalgamation comes with vulnerabilities to adversarial attacks. This survey focuses on the intersection of Adversarial Machine Learning (AML) and autonomous systems, with a specific focus on LiDAR-based systems. We comprehensively explore the threat landscape, encompassing cyber-attacks on sensors and adversarial perturbations. Additionally, we investigate defensive strategies employed in countering these threats. This paper endeavors to present a concise overview of the challenges and advances in securing autonomous driving systems against adversarial threats, emphasizing the need for robust defenses to ensure safety and security.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13778v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junae Kim, Amardeep Kaur</dc:creator>
    </item>
    <item>
      <title>GASP: Efficient Black-Box Generation of Adversarial Suffixes for Jailbreaking LLMs</title>
      <link>https://arxiv.org/abs/2411.14133</link>
      <description>arXiv:2411.14133v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown impressive proficiency across a range of natural language processing tasks yet remain vulnerable to adversarial prompts, known as jailbreak attacks, carefully designed to elicit harmful responses from LLMs. Traditional methods rely on manual heuristics, which suffer from limited generalizability. While being automatic, optimization-based attacks often produce unnatural jailbreak prompts that are easy to detect by safety filters or require high computational overhead due to discrete token optimization. Witnessing the limitations of existing jailbreak methods, we introduce Generative Adversarial Suffix Prompter (GASP), a novel framework that combines human-readable prompt generation with Latent Bayesian Optimization (LBO) to improve adversarial suffix creation in a fully black-box setting. GASP leverages LBO to craft adversarial suffixes by efficiently exploring continuous embedding spaces, gradually optimizing the model to improve attack efficacy while balancing prompt coherence through a targeted iterative refinement procedure. Our experiments show that GASP can generate natural jailbreak prompts, significantly improving attack success rates, reducing training times, and accelerating inference speed, thus making it an efficient and scalable solution for red-teaming LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14133v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.CV</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Advik Raj Basani, Xiao Zhang</dc:creator>
    </item>
    <item>
      <title>Translating C To Rust: Lessons from a User Study</title>
      <link>https://arxiv.org/abs/2411.14174</link>
      <description>arXiv:2411.14174v1 Announce Type: cross 
Abstract: Rust aims to offer full memory safety for programs, a guarantee that untamed C programs do not enjoy. How difficult is it to translate existing C code to Rust? To get a complementary view from that of automatic C to Rust translators, we report on a user study asking humans to translate real-world C programs to Rust. Our participants are able to produce safe Rust translations, whereas state-of-the-art automatic tools are not able to do so. Our analysis highlights that the high-level strategy taken by users departs significantly from those of automatic tools we study. We also find that users often choose zero-cost (static) abstractions for temporal safety, which addresses a predominant component of runtime costs in other full memory safety defenses. User-provided translations showcase a rich landscape of specialized strategies to translate the same C program in different ways to safe Rust, which future automatic translators can consider.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14174v1</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <category>cs.PL</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.14722/ndss.2025.241407</arxiv:DOI>
      <dc:creator>Ruishi Li, Bo Wang, Tianyu Li, Prateek Saxena, Ashish Kundu</dc:creator>
    </item>
    <item>
      <title>Adversarial Poisoning Attack on Quantum Machine Learning Models</title>
      <link>https://arxiv.org/abs/2411.14412</link>
      <description>arXiv:2411.14412v1 Announce Type: cross 
Abstract: With the growing interest in Quantum Machine Learning (QML) and the increasing availability of quantum computers through cloud providers, addressing the potential security risks associated with QML has become an urgent priority. One key concern in the QML domain is the threat of data poisoning attacks in the current quantum cloud setting. Adversarial access to training data could severely compromise the integrity and availability of QML models. Classical data poisoning techniques require significant knowledge and training to generate poisoned data, and lack noise resilience, making them ineffective for QML models in the Noisy Intermediate Scale Quantum (NISQ) era. In this work, we first propose a simple yet effective technique to measure intra-class encoder state similarity (ESS) by analyzing the outputs of encoding circuits. Leveraging this approach, we introduce a quantum indiscriminate data poisoning attack, QUID. Through extensive experiments conducted in both noiseless and noisy environments (e.g., IBM\_Brisbane's noise), across various architectures and datasets, QUID achieves up to $92\%$ accuracy degradation in model performance compared to baseline models and up to $75\%$ accuracy degradation compared to random label-flipping. We also tested QUID against state-of-the-art classical defenses, with accuracy degradation still exceeding $50\%$, demonstrating its effectiveness. This work represents the first attempt to reevaluate data poisoning attacks in the context of QML.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14412v1</guid>
      <category>quant-ph</category>
      <category>cs.CR</category>
      <category>cs.CV</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Satwik Kundu, Swaroop Ghosh</dc:creator>
    </item>
    <item>
      <title>Learning Fair Robustness via Domain Mixup</title>
      <link>https://arxiv.org/abs/2411.14424</link>
      <description>arXiv:2411.14424v1 Announce Type: cross 
Abstract: Adversarial training is one of the predominant techniques for training classifiers that are robust to adversarial attacks. Recent work, however has found that adversarial training, which makes the overall classifier robust, it does not necessarily provide equal amount of robustness for all classes. In this paper, we propose the use of mixup for the problem of learning fair robust classifiers, which can provide similar robustness across all classes. Specifically, the idea is to mix inputs from the same classes and perform adversarial training on mixed up inputs. We present a theoretical analysis of this idea for the case of linear classifiers and show that mixup combined with adversarial training can provably reduce the class-wise robustness disparity. This method not only contributes to reducing the disparity in class-wise adversarial risk, but also the class-wise natural risk. Complementing our theoretical analysis, we also provide experimental results on both synthetic data and the real world dataset (CIFAR-10), which shows improvement in class wise disparities for both natural and adversarial risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14424v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Meiyu Zhong, Ravi Tandon</dc:creator>
    </item>
    <item>
      <title>White-box Membership Inference Attacks against Diffusion Models</title>
      <link>https://arxiv.org/abs/2308.06405</link>
      <description>arXiv:2308.06405v3 Announce Type: replace 
Abstract: Diffusion models have begun to overshadow GANs and other generative models in industrial applications due to their superior image generation performance. The complex architecture of these models furnishes an extensive array of attack features. In light of this, we aim to design membership inference attacks (MIAs) catered to diffusion models. We first conduct an exhaustive analysis of existing MIAs on diffusion models, taking into account factors such as black-box/white-box models and the selection of attack features. We found that white-box attacks are highly applicable in real-world scenarios, and the most effective attacks presently are white-box. Departing from earlier research, which employs model loss as the attack feature for white-box MIAs, we employ model gradients in our attack, leveraging the fact that these gradients provide a more profound understanding of model responses to various samples. We subject these models to rigorous testing across a range of parameters, including training steps, sampling frequency, diffusion steps, and data variance. Across all experimental settings, our method consistently demonstrated near-flawless attack performance, with attack success rate approaching 100% and attack AUCROC near 1.0. We also evaluate our attack against common defense mechanisms, and observe our attacks continue to exhibit commendable performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.06405v3</guid>
      <category>cs.CR</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Pang, Tianhao Wang, Xuhui Kang, Mengdi Huai, Yang Zhang</dc:creator>
    </item>
    <item>
      <title>Magmaw: Modality-Agnostic Adversarial Attacks on Machine Learning-Based Wireless Communication Systems</title>
      <link>https://arxiv.org/abs/2311.00207</link>
      <description>arXiv:2311.00207v3 Announce Type: replace 
Abstract: Machine Learning (ML) has been instrumental in enabling joint transceiver optimization by merging all physical layer blocks of the end-to-end wireless communication systems. Although there have been a number of adversarial attacks on ML-based wireless systems, the existing methods do not provide a comprehensive view including multi-modality of the source data, common physical layer protocols, and wireless domain constraints. This paper proposes Magmaw, a novel wireless attack methodology capable of generating universal adversarial perturbations for any multimodal signal transmitted over a wireless channel. We further introduce new objectives for adversarial attacks on downstream applications. We adopt the widely-used defenses to verify the resilience of Magmaw. For proof-of-concept evaluation, we build a real-time wireless attack platform using a software-defined radio system. Experimental results demonstrate that Magmaw causes significant performance degradation even in the presence of strong defense mechanisms. Furthermore, we validate the performance of Magmaw in two case studies: encrypted communication channel and channel modality-based ML model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00207v3</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jung-Woo Chang, Ke Sun, Nasimeh Heydaribeni, Seira Hidano, Xinyu Zhang, Farinaz Koushanfar</dc:creator>
    </item>
    <item>
      <title>Hybrid Online Certificate Status Protocol with Certificate Revocation List for Smart Grid Public Key Infrastructure</title>
      <link>https://arxiv.org/abs/2401.10787</link>
      <description>arXiv:2401.10787v5 Announce Type: replace 
Abstract: Hsu et al. (2022) proposed a cryptographic scheme within the public key infrastructure to bolster the security of smart grid meters. Their proposal involved developing the Certificate Management over CMS mechanism to establish Simple Certificate Enrollment Protocol and Enrollment over Secure Transport protocol. Additionally, they implemented Online Certificate Status Protocol (OCSP) services to independently query the status of certificates. However, their implementation featured a single OCSP server handling all query requests. Considering the typical scenario in smart grid PKI environments with over tens of thousands of end-meters, we introduced a Hybrid Online Certificate Status Protocol mechanism. This approach decreases demand of query resources from the client to OCSP servers collaborating with Certificate Revocation Lists. Our simulations, mimicking meter behavior, demonstrated increased efficiency, creating a more robust architecture tailored to the smart grid meter landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10787v5</guid>
      <category>cs.CR</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hong-Sheng Huang, Zhe-Yi Jiang, Hsuan-Tung Chen, Hung-Min Sun</dc:creator>
    </item>
    <item>
      <title>The Variant of Designated Verifier Signature Scheme with Message Recovery</title>
      <link>https://arxiv.org/abs/2403.07820</link>
      <description>arXiv:2403.07820v3 Announce Type: replace 
Abstract: In this work, we introduce a strong Designated Verifier Signature (DVS) scheme that incorporates a message recovery mechanism inspired by the concept of the Universal Designated Verifier Signature (UDVS) scheme. It is worth noting that Saeednia's strong designated verifier signature scheme fails to guarantee the privacy of the signature, making it unsuitable for certain applications such as medical record certificates or voting systems. To overcome this limitation, we extend Lee's strong designated verifier signature with a message recovery scheme to develop a universal designated verifier signature scheme. This universal designated verifier scheme is crafted to safeguard the privacy of signature holders, ensuring that only designated verifiers can authenticate the true signer and recover the messages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07820v3</guid>
      <category>cs.CR</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hong-Sheng Huang, Yu-Lei Fu, Han-Yu Lin</dc:creator>
    </item>
    <item>
      <title>Watermark-based Attribution of AI-Generated Content</title>
      <link>https://arxiv.org/abs/2404.04254</link>
      <description>arXiv:2404.04254v3 Announce Type: replace 
Abstract: Several companies have deployed watermark-based detection to identify AI-generated content. However, attribution--the ability to trace back to the user of a generative AI (GenAI) service who created a given piece of AI-generated content--remains largely unexplored despite its growing importance. In this work, we aim to bridge this gap by conducting the first systematic study on watermark-based, user-level attribution of AI-generated content. Our key idea is to assign a unique watermark to each user of the GenAI service and embed this watermark into the AI-generated content created by that user. Attribution is then performed by identifying the user whose watermark best matches the one extracted from the given content. This approach, however, faces a key challenge: How should watermarks be selected for users to maximize attribution performance? To address the challenge, we first theoretically derive lower bounds on detection and attribution performance through rigorous probabilistic analysis for any given set of user watermarks. Then, we select watermarks for users to maximize these lower bounds, thereby optimizing detection and attribution performance. Our theoretical and empirical results show that watermark-based attribution inherits both the accuracy and (non-)robustness properties of the underlying watermark. Specifically, attribution remains highly accurate when the watermarked AI-generated content is either not post-processed or subjected to common post-processing such as JPEG compression, as well as black-box adversarial post-processing with limited query budgets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04254v3</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhengyuan Jiang, Moyang Guo, Yuepeng Hu, Neil Zhenqiang Gong</dc:creator>
    </item>
    <item>
      <title>TransLinkGuard: Safeguarding Transformer Models Against Model Stealing in Edge Deployment</title>
      <link>https://arxiv.org/abs/2404.11121</link>
      <description>arXiv:2404.11121v2 Announce Type: replace 
Abstract: Proprietary large language models (LLMs) have been widely applied in various scenarios. Additionally, deploying LLMs on edge devices is trending for efficiency and privacy reasons. However, edge deployment of proprietary LLMs introduces new security challenges: edge-deployed models are exposed as white-box accessible to users, enabling adversaries to conduct effective model stealing (MS) attacks. Unfortunately, existing defense mechanisms fail to provide effective protection. Specifically, we identify four critical protection properties that existing methods fail to simultaneously satisfy: (1) maintaining protection after a model is physically copied; (2) authorizing model access at request level; (3) safeguarding runtime reverse engineering; (4) achieving high security with negligible runtime overhead. To address the above issues, we propose TransLinkGuard, a plug-and-play model protection approach against model stealing on edge devices. The core part of TransLinkGuard is a lightweight authorization module residing in a secure environment, e.g., TEE. The authorization module can freshly authorize each request based on its input. Extensive experiments show that TransLinkGuard achieves the same security protection as the black-box security guarantees with negligible overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11121v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3664647.3680786</arxiv:DOI>
      <dc:creator>Qinfeng Li, Zhiqiang Shen, Zhenghan Qin, Yangfan Xie, Xuhong Zhang, Tianyu Du, Jianwei Yin</dc:creator>
    </item>
    <item>
      <title>Mens Sana In Corpore Sano: Sound Firmware Corpora for Vulnerability Research</title>
      <link>https://arxiv.org/abs/2404.11977</link>
      <description>arXiv:2404.11977v4 Announce Type: replace 
Abstract: Firmware corpora for vulnerability research should be scientifically sound. Yet, several practical challenges complicate the creation of sound corpora: Sample acquisition, e.g., is hard and one must overcome the barrier of proprietary or encrypted data. As image contents are unknown prior analysis, it is hard to select high-quality samples that can satisfy scientific demands. Ideally, we help each other out by sharing data. But here, sharing is problematic due to copyright laws. Instead, papers must carefully document each step of corpus creation: If a step is unclear, replicability is jeopardized. This has cascading effects on result verifiability, representativeness, and, thus, soundness.
  Despite all challenges, how can we maintain the soundness of firmware corpora? This paper thoroughly analyzes the problem space and investigates its impact on research: We distill practical binary analysis challenges that significantly influence corpus creation. We use these insights to derive guidelines that help researchers to nurture corpus replicability and representativeness. We apply them to 44 top tier papers and systematically analyze scientific corpus creation practices. Our comprehensive analysis confirms that there is currently no common ground in related work. It shows the added value of our guidelines, as they discover methodical issues in corpus creation and unveil miniscule step stones in documentation. These blur visions on representativeness, hinder replicability, and, thus, negatively impact the soundness of otherwise excellent work.
  Finally, we show the feasibility of our guidelines and build a new, replicable corpus for large-scale analyses on Linux firmware: LFwC. We share rich meta data for good (and proven) replicability. We verify unpacking, deduplicate, identify contents, provide ground truth, and show LFwC's utility for research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11977v4</guid>
      <category>cs.CR</category>
      <category>cs.DL</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.14722/ndss.2025.230669</arxiv:DOI>
      <dc:creator>Ren\'e Helmke, Elmar Padilla, Nils Aschenbruck</dc:creator>
    </item>
    <item>
      <title>An Enhanced Online Certificate Status Protocol for Public Key Infrastructure with Smart Grid and Energy Storage System</title>
      <link>https://arxiv.org/abs/2409.10929</link>
      <description>arXiv:2409.10929v3 Announce Type: replace 
Abstract: The efficiency of checking certificate status is one of the key indicators in the public key infrastructure (PKI). This prompted researchers to design the Online Certificate Status Protocol (OCSP) standard, defined in RFC 6960, to guide developers in implementing OCSP components. However, as the environment increasingly relies on PKI for identity authentication, it is essential to protect the communication between clients and servers from rogue elements. This can be achieved by using SSL/TLS techniques to establish a secure channel, allowing Certificate Authorities (CAs) to safely transfer certificate status information. In this work, we introduce the OCSP Stapling approach to optimize OCSP query costs in our smart grid environment. This approach reduces the number of queries from the Device Language Message Specification (DLMS) server to the OCSP server. Our experimental results show that OCSP stapling increases both efficiency and security, creating a more robust architecture for the smart grid.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10929v3</guid>
      <category>cs.CR</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hong-Sheng Huang, Cheng-Che Chuang, Jhih-Zen Shih, Hsuan-Tung Chen, Hung-Min Sun</dc:creator>
    </item>
    <item>
      <title>Winemaking: Extracting Essential Insights for Efficient Threat Detection in Audit Logs</title>
      <link>https://arxiv.org/abs/2411.02775</link>
      <description>arXiv:2411.02775v2 Announce Type: replace 
Abstract: Advanced Persistent Threats (APTs) are continuously evolving, leveraging their stealthiness and persistence to put increasing pressure on current provenance-based Intrusion Detection Systems (IDS). This evolution exposes several critical issues: (1) The dense interaction between malicious and benign nodes within provenance graphs introduces neighbor noise, hindering effective detection; (2) The complex prediction mechanisms of existing APTs detection models lead to the insufficient utilization of prior knowledge embedded in the data; (3) The high computational cost makes detection impractical.
  To address these challenges, we propose Winemaking, a lightweight threat detection system built on a knowledge distillation framework, capable of node-level detection within audit log provenance graphs. Specifically, Winemaking applies graph Laplacian regularization to reduce neighbor noise, obtaining smoothed and denoised graph signals. Subsequently, Winemaking employs a teacher model based on GNNs to extract knowledge, which is then distilled into a lightweight student model. The student model is designed as a trainable combination of a feature transformation module and a personalized PageRank random walk label propagation module, with the former capturing feature knowledge and the latter learning label and structural knowledge. After distillation, the student model benefits from the knowledge of the teacher model to perform precise threat detection. We evaluate Winemaking through extensive experiments on three public datasets and compare its performance against several state-of-the-art IDS solutions. The results demonstrate that Winemaking achieves outstanding detection accuracy across all scenarios and the detection time is 1.4 to 5.2 times faster than the current state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02775v2</guid>
      <category>cs.CR</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Weiheng Wu, Wei Qiao, Wenhao Yan, Bo Jiang, Yuling Liu, Baoxu Liu, Zhigang Lu, JunRong Liu</dc:creator>
    </item>
    <item>
      <title>Fixing Security Vulnerabilities with AI in OSS-Fuzz</title>
      <link>https://arxiv.org/abs/2411.03346</link>
      <description>arXiv:2411.03346v2 Announce Type: replace 
Abstract: Critical open source software systems undergo significant validation in the form of lengthy fuzz campaigns. The fuzz campaigns typically conduct a biased random search over the domain of program inputs, to find inputs which crash the software system. Such fuzzing is useful to enhance the security of software systems in general since even closed source software may use open source components. Hence testing open source software is of paramount importance. Currently OSS-Fuzz is the most significant and widely used infrastructure for continuous validation of open source systems. Unfortunately even though OSS-Fuzz has identified more than 10,000 vulnerabilities across 1000 or more software projects, the detected vulnerabilities may remain unpatched, as vulnerability fixing is often manual in practice. In this work, we rely on the recent progress in Large Language Model (LLM) agents for autonomous program improvement including bug fixing. We customise the well-known AutoCodeRover agent for fixing security vulnerabilities. This is because LLM agents like AutoCodeRover fix bugs from issue descriptions via code search. Instead for security patching, we rely on the test execution of the exploit input to extract code elements relevant to the fix. Our experience with OSS-Fuzz vulnerability data shows that LLM agent autonomy is useful for successful security patching, as opposed to approaches like Agentless where the control flow is fixed. More importantly our findings show that we cannot measure quality of patches by code similarity of the patch with reference codes (as in CodeBLEU scores used in VulMaster), since patches with high CodeBLEU scores still fail to pass given the given exploit input. Our findings indicate that security patch correctness needs to consider dynamic attributes like test executions as opposed to relying of standard text/code similarity metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03346v2</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuntong Zhang, Jiawei Wang, Dominic Berzin, Martin Mirchev, Dongge Liu, Abhishek Arya, Oliver Chang, Abhik Roychoudhury</dc:creator>
    </item>
    <item>
      <title>Privacy-Aware Data Acquisition under Data Similarity in Regression Markets</title>
      <link>https://arxiv.org/abs/2312.02611</link>
      <description>arXiv:2312.02611v2 Announce Type: replace-cross 
Abstract: Data markets facilitate decentralized data exchange for applications such as prediction, learning, or inference. The design of these markets is challenged by varying privacy preferences as well as data similarity among data owners. Related works have often overlooked how data similarity impacts pricing and data value through statistical information leakage. We demonstrate that data similarity and privacy preferences are integral to market design and propose a query-response protocol using local differential privacy for a two-party data acquisition mechanism. In our regression data market model, we analyze strategic interactions between privacy-aware owners and the learner as a Stackelberg game over the asked price and privacy factor. Finally, we numerically evaluate how data similarity affects market participation and traded data value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02611v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.GT</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shashi Raj Pandey, Pierre Pinson, Petar Popovski</dc:creator>
    </item>
    <item>
      <title>Quantum cryptographic protocols with dual messaging system via 2D alternate quantum walk of a genuine single-photon entangled state</title>
      <link>https://arxiv.org/abs/2405.00663</link>
      <description>arXiv:2405.00663v3 Announce Type: replace-cross 
Abstract: A single-photon entangled state (or single-particle entangled state (SPES) in general) can offer a more secure way of encoding and processing quantum information than their multi-photon (or multi-particle) counterparts. The SPES generated via a 2D alternate quantum-walk setup from initially separable states can be either 3-way or 2-way entangled. This letter shows that the generated genuine three-way and nonlocal two-way SPES can be used as cryptographic keys to securely encode two distinct messages simultaneously. We detail the message encryption-decryption steps and show the resilience of the 3-way and 2-way SPES-based cryptographic protocols against eavesdropper attacks like intercept-and-resend and man-in-the-middle. We also detail the experimental realization of these protocols using a single photon, with the three degrees of freedom being OAM, path, and polarization. We have proved that the protocols have unconditional security for quantum communication tasks. The ability to simultaneously encode two distinct messages using the generated SPES showcases the versatility and efficiency of the proposed cryptographic protocol. This capability could significantly improve the throughput of quantum communication systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00663v3</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.CR</category>
      <category>math.QA</category>
      <category>physics.optics</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>Journal of Physics A: Mathematical and Theoretical (2024)</arxiv:journal_reference>
      <dc:creator>Dinesh Kumar Panda, Colin Benjamin</dc:creator>
    </item>
    <item>
      <title>SNIP: Speculative Execution and Non-Interference Preservation for Compiler Transformations</title>
      <link>https://arxiv.org/abs/2407.15080</link>
      <description>arXiv:2407.15080v3 Announce Type: replace-cross 
Abstract: We address the problem of preserving non-interference across compiler transformations under speculative semantics. We develop a proof method that ensures the preservation uniformly across all source programs. The basis of our proof method is a new form of simulation relation. It operates over directives that model the attacker's control over the micro-architectural state, and it accounts for the fact that the compiler transformation may change the influence of the micro-architectural state on the execution (and hence the directives). Using our proof method, we show the correctness of dead code elimination. When we tried to prove register allocation correct, we identified a previously unknown weakness that introduces violations to non-interference. We have confirmed the weakness for a mainstream compiler on code from the libsodium cryptographic library. To reclaim security once more, we develop a novel static analysis that operates on a product of source program and register-allocated program. Using the analysis, we present an automated fix to existing register allocation implementations. We prove the correctness of the fixed register allocations with our proof method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15080v3</guid>
      <category>cs.PL</category>
      <category>cs.CR</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>S\"oren van der Wall, Roland Meyer</dc:creator>
    </item>
    <item>
      <title>"I Don't Use AI for Everything": Exploring Utility, Attitude, and Responsibility of AI-empowered Tools in Software Development</title>
      <link>https://arxiv.org/abs/2409.13343</link>
      <description>arXiv:2409.13343v2 Announce Type: replace-cross 
Abstract: AI-empowered tools have emerged as a transformative force, fundamentally reshaping the software development industry and promising far-reaching impacts across diverse sectors. This study investigates the adoption, impact, and security considerations of AI-empowered tools in the software development process. Through semi-structured interviews with 19 software practitioners from diverse backgrounds, we explore three key aspects: the utility of AI tools, developers' attitudes towards them, and security and privacy responsibilities. Our findings reveal widespread adoption of AI tools across various stages of software development. Developers generally express positive attitudes towards AI, viewing it as an efficiency-enhancing assistant rather than a job replacement threat. However, they also recognized limitations in AI's ability to handle complex, unfamiliar, or highly specialized tasks in software development. Regarding security and privacy, we found varying levels of risk awareness among developers, with larger companies implementing more comprehensive risk management strategies. Our study provides insights into the current state of AI adoption in software development and offers recommendations for practitioners, organizations, AI providers, and regulatory bodies to effectively navigate the integration of AI in the software industry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13343v2</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Shidong Pan, Litian Wang, Tianyi Zhang, Zhenchang Xing, Yanjie Zhao, Qinghua Lu, Xiaoyu Sun</dc:creator>
    </item>
  </channel>
</rss>
