<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Sep 2025 01:49:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Large-Scale Curve Time Series with Common Stochastic Trends</title>
      <link>https://arxiv.org/abs/2509.11060</link>
      <description>arXiv:2509.11060v1 Announce Type: new 
Abstract: This paper studies high-dimensional curve time series with common stochastic trends. A dual functional factor model structure is adopted with a high-dimensional factor model for the observed curve time series and a low-dimensional factor model for the latent curves with common trends. A functional PCA technique is applied to estimate the common stochastic trends and functional factor loadings. Under some regularity conditions we derive the mean square convergence and limit distribution theory for the developed estimates, allowing the dimension and sample size to jointly diverge to infinity. We propose an easy-to-implement criterion to consistently select the number of common stochastic trends and further discuss model estimation when the nonstationary factors are cointegrated. Extensive Monte-Carlo simulations and two empirical applications to large-scale temperature curves in Australia and log-price curves of S&amp;P 500 stocks are conducted, showing finite-sample performance and providing practical implementations of the new methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11060v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Degui Li, Yu-Ning Li, Peter C. B. Phillips</dc:creator>
    </item>
    <item>
      <title>Fairness-Aware and Interpretable Policy Learning</title>
      <link>https://arxiv.org/abs/2509.12119</link>
      <description>arXiv:2509.12119v1 Announce Type: new 
Abstract: Fairness and interpretability play an important role in the adoption of decision-making algorithms across many application domains. These requirements are intended to avoid undesirable group differences and to alleviate concerns related to transparency. This paper proposes a framework that integrates fairness and interpretability into algorithmic decision making by combining data transformation with policy trees, a class of interpretable policy functions. The approach is based on pre-processing the data to remove dependencies between sensitive attributes and decision-relevant features, followed by a tree-based optimization to obtain the policy. Since data pre-processing compromises interpretability, an additional transformation maps the parameters of the resulting tree back to the original feature space. This procedure enhances fairness by yielding policy allocations that are pairwise independent of sensitive attributes, without sacrificing interpretability. Using administrative data from Switzerland to analyze the allocation of unemployed individuals to active labor market programs (ALMP), the framework is shown to perform well in a realistic policy setting. Effects of integrating fairness and interpretability constraints are measured through the change in expected employment outcomes. The results indicate that, for this particular application, fairness can be substantially improved at relatively low cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12119v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nora Bearth, Michael Lechner, Jana Mareckova, Fabian Muny</dc:creator>
    </item>
    <item>
      <title>What is in a Price? Estimating Willingness-to-Pay with Bayesian Hierarchical Models</title>
      <link>https://arxiv.org/abs/2509.11089</link>
      <description>arXiv:2509.11089v1 Announce Type: cross 
Abstract: For premium consumer products, pricing strategy is not about a single number, but about understanding the perceived monetary value of the features that justify a higher cost. This paper proposes a robust methodology to deconstruct a product's price into the tangible value of its constituent parts. We employ Bayesian Hierarchical Conjoint Analysis, a sophisticated statistical technique, to solve this high-stakes business problem using the Apple iPhone as a universally recognizable case study. We first simulate a realistic choice based conjoint survey where consumers choose between different hypothetical iPhone configurations. We then develop a Bayesian Hierarchical Logit Model to infer consumer preferences from this choice data. The core innovation of our model is its ability to directly estimate the Willingness-to-Pay (WTP) in dollars for specific feature upgrades, such as a "Pro" camera system or increased storage. Our results demonstrate that the model successfully recovers the true, underlying feature valuations from noisy data, providing not just a point estimate but a full posterior probability distribution for the dollar value of each feature. This work provides a powerful, practical framework for data-driven product design and pricing strategy, enabling businesses to make more intelligent decisions about which features to build and how to price them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11089v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Srijesh Pillai, Rajesh Kumar Chandrawat</dc:creator>
    </item>
    <item>
      <title>The Honest Truth About Causal Trees: Accuracy Limits for Heterogeneous Treatment Effect Estimation</title>
      <link>https://arxiv.org/abs/2509.11381</link>
      <description>arXiv:2509.11381v1 Announce Type: cross 
Abstract: Recursive decision trees have emerged as a leading methodology for heterogeneous causal treatment effect estimation and inference in experimental and observational settings. These procedures are fitted using the celebrated CART (Classification And Regression Tree) algorithm [Breiman et al., 1984], or custom variants thereof, and hence are believed to be "adaptive" to high-dimensional data, sparsity, or other specific features of the underlying data generating process. Athey and Imbens [2016] proposed several "honest" causal decision tree estimators, which have become the standard in both academia and industry. We study their estimators, and variants thereof, and establish lower bounds on their estimation error. We demonstrate that these popular heterogeneous treatment effect estimators cannot achieve a polynomial-in-$n$ convergence rate under basic conditions, where $n$ denotes the sample size. Contrary to common belief, honesty does not resolve these limitations and at best delivers negligible logarithmic improvements in sample size or dimension. As a result, these commonly used estimators can exhibit poor performance in practice, and even be inconsistent in some settings. Our theoretical insights are empirically validated through simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11381v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Jason M. Klusowski, Ruiqi Rae Yu</dc:creator>
    </item>
    <item>
      <title>Teacher bias or measurement error?</title>
      <link>https://arxiv.org/abs/2401.04200</link>
      <description>arXiv:2401.04200v4 Announce Type: replace 
Abstract: Subjective teacher evaluations play a key role in shaping students' educational trajectories. Previous studies have shown that students of low socioeconomic status (SES) receive worse subjective evaluations than their high SES peers, even when they score similarly on objective standardized tests. This is often interpreted as evidence of teacher bias. Measurement error in test scores challenges this interpretation. We discuss how both classical and non-classical measurement error in test scores generate a biased coefficient of the conditional SES gap, and consider three empirical strategies to address this bias. Using administrative data from the Netherlands, where secondary school track recommendations are pivotal teacher judgments, we find that measurement error explains 35 to 43% of the conditional SES gap in track recommendations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.04200v4</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas van Huizen, Madelon Jacobs, Matthijs Oosterveen</dc:creator>
    </item>
    <item>
      <title>Regression Discontinuity Design with Spillovers</title>
      <link>https://arxiv.org/abs/2404.06471</link>
      <description>arXiv:2404.06471v2 Announce Type: replace 
Abstract: This paper studies regression discontinuity designs (RDD) when linear-in-means spillovers occur between units that are close in their running variable. We show that the RDD estimand depends on the ratio of two terms: (1) the radius over which spillovers occur and (2) the choice of bandwidth used for the local linear regression. RDD estimates direct treatment effect when radius is of larger order than the bandwidth and total treatment effect when radius is of smaller order than the bandwidth. When the two are of similar order, the RDD estimand need not have a causal interpretation. To recover direct and spillover effects in the intermediate regime, we propose to incorporate estimated spillover terms into local linear regression. Our estimator is consistent and asymptotically normal and we provide bias-aware confidence intervals for direct treatment effects and spillovers. In the setting of Gonzalez (2021), we detect endogenous spillovers in voter fraud during the 2009 Afghan Presidential election. We also clarify when the donut-hole design addresses spillovers in RDD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06471v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Auerbach, Yong Cai, Ahnaf Rafi</dc:creator>
    </item>
    <item>
      <title>Uniform Inference in High-Dimensional Threshold Regression Models</title>
      <link>https://arxiv.org/abs/2404.08105</link>
      <description>arXiv:2404.08105v3 Announce Type: replace 
Abstract: We develop a uniform inference theory for high-dimensional slope parameters in threshold regression models, allowing for either cross-sectional or time series data. We first establish oracle inequalities for prediction errors, and L1 estimation errors for the Lasso estimator of the slope parameters and the threshold parameter, accommodating heteroskedastic non-subgaussian error terms and non-subgaussian covariates. Next, we derive the asymptotic distribution of tests involving an increasing number of slope parameters by debiasing (or desparsifying) the Lasso estimator in cases with no threshold effect and with a fixed threshold effect. We show that the asymptotic distributions in both cases are the same, allowing us to perform uniform inference without specifying whether the model is a linear or threshold regression. Additionally, we extend the theory to accommodate time series data under the near-epoch dependence assumption. Finally, we identify statistically significant factors influencing cross-country economic growth and quantify the effects of military news shocks on US government spending and GDP, while also estimating a data-driven threshold point in both applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08105v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiatong Li, Hongqiang Yan</dc:creator>
    </item>
    <item>
      <title>Identification by non-Gaussianity in structural threshold and smooth transition vector autoregressive models</title>
      <link>https://arxiv.org/abs/2404.19707</link>
      <description>arXiv:2404.19707v5 Announce Type: replace 
Abstract: We show that structural smooth transition vector autoregressive models are statistically identified if the shocks are mutually independent and at most one of them is Gaussian. This extends a known identification result for linear structural vector autoregressions to a time-varying impact matrix. We also propose an estimation method, show how a blended identification strategy can be adopted to address weak identification, and establish a sufficient condition for ergodic stationarity. The introduced methods are implemented in the accompanying R package sstvars. Our empirical application finds that a positive climate policy uncertainty shock reduces production and raises inflation under both low and high economic policy uncertainty, but its effects, particularly on inflation, are stronger during the latter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19707v5</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Savi Virolainen</dc:creator>
    </item>
    <item>
      <title>On the Lower Confidence Band for the Optimal Welfare in Policy Learning</title>
      <link>https://arxiv.org/abs/2410.07443</link>
      <description>arXiv:2410.07443v3 Announce Type: replace 
Abstract: We study inference on the optimal welfare in a policy learning problem and propose reporting a lower confidence band (LCB). A natural approach to constructing an LCB is to invert a one-sided t-test based on an efficient estimator for the optimal welfare. However, we show that for an empirically relevant class of DGPs, such an LCB can be first-order dominated by an LCB based on a welfare estimate for a suitable suboptimal treatment policy. We show that such first-order dominance is possible if and only if the optimal treatment policy is not ``well-separated'' from the rest, in the sense of the commonly imposed margin condition. When this condition fails, standard debiased inference methods are not applicable. We show that uniformly valid and easy-to-compute LCBs can be constructed analytically by inverting moment-inequality tests with the maximum and quasi-likelihood-ratio test statistics. As an empirical illustration, we revisit the National JTPA study and find that the proposed LCBs achieve reliable coverage and competitive length.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07443v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kirill Ponomarev, Vira Semenova</dc:creator>
    </item>
    <item>
      <title>Weak instrumental variables due to nonlinearities in panel data: A Super Learner Control Function estimator</title>
      <link>https://arxiv.org/abs/2504.03228</link>
      <description>arXiv:2504.03228v4 Announce Type: replace 
Abstract: A triangular structural panel data model with additive separable individual-specific effects is used to model the causal effect of a covariate on an outcome variable when there are unobservable confounders with some of them time-invariant. In this setup, a linear reduced-form equation might be problematic when the conditional mean of the endogenous covariate and the instrumental variables is nonlinear. The reason is that ignoring the nonlinearity could lead to weak instruments As a solution, we propose a triangular simultaneous equation model for panel data with additive separable individual-specific fixed effects composed of a linear structural equation with a nonlinear reduced form equation. The parameter of interest is the structural parameter of the endogenous variable. The identification of this parameter is obtained under the assumption of available exclusion restrictions and using a control function approach. Estimating the parameter of interest is done using an estimator that we call Super Learner Control Function estimator (SLCFE). The estimation procedure is composed of two main steps and sample splitting. We estimate the control function using a super learner using sample splitting. In the following step, we use the estimated control function to control for endogeneity in the structural equation. Sample splitting is done across the individual dimension. We perform a Monte Carlo simulation to test the performance of the estimators proposed. We conclude that the Super Learner Control Function Estimators significantly outperform Within 2SLS estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03228v4</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Monika Avila Marquez</dc:creator>
    </item>
    <item>
      <title>Higher-order Gini indices: An axiomatic approach</title>
      <link>https://arxiv.org/abs/2508.10663</link>
      <description>arXiv:2508.10663v2 Announce Type: replace-cross 
Abstract: Via an axiomatic approach, we characterize the family of n-th order Gini deviation, defined as the expected range over n independent draws from a distribution, to quantify joint dispersion across multiple observations. This family extends the classical Gini deviation, which relies solely on pairwise comparisons. The normalized version is called a high-order Gini coefficient. The generalized indices grow increasingly sensitive to tail inequality as n increases, offering a more nuanced view of distributional extremes. The higher-order Gini deviations admit a Choquet integral representation, inheriting the desirable properties of coherent deviation measures. Furthermore, we show that both the n-th order Gini deviation and the n-th order Gini coefficient are statistically n-observation elicitable, allowing for direct computation through empirical risk minimization. Data analysis using World Inequality Database data reveals that higher-order Gini coefficients capture disparities that the classical Gini coefficient may fail to reflect, particularly in cases of extreme income or wealth concentration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10663v2</guid>
      <category>q-fin.MF</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xia Han, Ruodu Wang, Qinyu Wu</dc:creator>
    </item>
  </channel>
</rss>
