<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Dec 2024 05:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Modeling EEG Spectral Features through Warped Functional Mixed Membership Models</title>
      <link>https://arxiv.org/abs/2412.08762</link>
      <description>arXiv:2412.08762v1 Announce Type: new 
Abstract: A common concern in the field of functional data analysis is the challenge of temporal misalignment, which is typically addressed using curve registration methods. Currently, most of these methods assume the data is governed by a single common shape or a finite mixture of population level shapes. We introduce more flexibility using mixed membership models. Individual observations are assumed to partially belong to different clusters, allowing variation across multiple functional features. We propose a Bayesian hierarchical model to estimate the underlying shapes, as well as the individual time-transformation functions and levels of membership. Motivating this work is data from EEG signals in children with autism spectrum disorder (ASD). Our method agrees with the neuroimaging literature, recovering the 1/f pink noise feature distinctly from the peak in the alpha band. Furthermore, the introduction of a regression component in the estimation of time-transformation functions quantifies the effect of age and clinical designation on the location of the peak alpha frequency (PAF).</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08762v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emma Landry, Damla Senturk, Shafali Jeste, Charlotte DiStefano, Abigail Dickinson, Donatello Telesca</dc:creator>
    </item>
    <item>
      <title>A Debiased Estimator for the Mediation Functional in Ultra-High-Dimensional Setting in the Presence of Interaction Effects</title>
      <link>https://arxiv.org/abs/2412.08827</link>
      <description>arXiv:2412.08827v1 Announce Type: new 
Abstract: Mediation analysis is crucial in many fields of science for understanding the mechanisms or processes through which an independent variable affects an outcome, thereby providing deeper insights into causal relationships and improving intervention strategies. Despite advances in analyzing the mediation effect with fixed/low-dimensional mediators and covariates, our understanding of estimation and inference of mediation functional in the presence of (ultra)-high-dimensional mediators and covariates is still limited. In this paper, we present an estimator for mediation functional in a high-dimensional setting that accommodates the interaction between covariates and treatment in generating mediators, as well as interactions between both covariates and treatment and mediators and treatment in generating the response. We demonstrate that our estimator is $\sqrt{n}$-consistent and asymptotically normal, thus enabling reliable inference on direct and indirect treatment effects with asymptotically valid confidence intervals. A key technical contribution of our work is to develop a multi-step debiasing technique, which may also be valuable in other statistical settings with similar structural complexities where accurate estimation depends on debiasing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08827v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shi Bo, AmirEmad Ghassami, Debarghya Mukherjee</dc:creator>
    </item>
    <item>
      <title>A Two-Stage Approach for Segmenting Spatial Point Patterns Applied to Multiplex Imaging</title>
      <link>https://arxiv.org/abs/2412.08828</link>
      <description>arXiv:2412.08828v1 Announce Type: new 
Abstract: Recent advances in multiplex imaging have enabled researchers to locate different types of cells within a tissue sample. This is especially relevant for tumor immunology, as clinical regimes corresponding to different stages of disease or responses to treatment may manifest as different spatial arrangements of tumor and immune cells. Spatial point pattern modeling can be used to partition multiplex tissue images according to these regimes. To this end, we propose a two-stage approach: first, local intensities and pair correlation functions are estimated from the spatial point pattern of cells within each image, and the pair correlation functions are reduced in dimension via spectral decomposition of the covariance function. Second, the estimates are clustered in a Bayesian hierarchical model with spatially-dependent cluster labels. The clusters correspond to regimes of interest that are present across subjects; the cluster labels segment the spatial point patterns according to those regimes. Through Markov Chain Monte Carlo sampling, we jointly estimate and quantify uncertainty in the cluster assignment and spatial characteristics of each cluster. Simulations demonstrate the performance of the method, and it is applied to a set of multiplex immunofluorescence images of diseased pancreatic tissue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08828v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alvin Sheng, Brian J. Reich, Ana-Maria Staicu, Santhoshi N. Krishnan, Arvind Rao, Timothy L. Frankel</dc:creator>
    </item>
    <item>
      <title>Dynamic prediction of an event using multiple longitudinal markers: a model averaging approach</title>
      <link>https://arxiv.org/abs/2412.08857</link>
      <description>arXiv:2412.08857v1 Announce Type: new 
Abstract: Dynamic event prediction, using joint modeling of survival time and longitudinal variables, is extremely useful in personalized medicine. However, the estimation of joint models including many longitudinal markers is still a computational challenge because of the high number of random effects and parameters to be estimated. In this paper, we propose a model averaging strategy to combine predictions from several joint models for the event, including one longitudinal marker only or pairwise longitudinal markers. The prediction is computed as the weighted mean of the predictions from the one-marker or two-marker models, with the time-dependent weights estimated by minimizing the time-dependent Brier score. This method enables us to combine a large number of predictions issued from joint models to achieve a reliable and accurate individual prediction. Advantages and limits of the proposed methods are highlighted in a simulation study by comparison with the predictions from well-specified and misspecified all-marker joint models as well as the one-marker and two-marker joint models. Using the PBC2 data set, the method is used to predict the risk of death in patients with primary biliary cirrhosis. The method is also used to analyze a French cohort study called the 3C data. In our study, seventeen longitudinal markers are considered to predict the risk of death.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08857v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Reza Hashemi, Taban Baghfalaki, Viviane Philipps, Helene Jacqmin-Gadda</dc:creator>
    </item>
    <item>
      <title>Beyond forecast leaderboards: Measuring individual model importance based on contribution to ensemble accuracy</title>
      <link>https://arxiv.org/abs/2412.08916</link>
      <description>arXiv:2412.08916v1 Announce Type: new 
Abstract: Ensemble forecasts often outperform forecasts from individual standalone models, and have been used to support decision-making and policy planning in various fields. As collaborative forecasting efforts to create effective ensembles grow, so does interest in understanding individual models' relative importance in the ensemble. To this end, we propose two practical methods that measure the difference between ensemble performance when a given model is or is not included in the ensemble: a leave-one-model-out algorithm and a leave-all-subsets-of-models-out algorithm, which is based on the Shapley value. We explore the relationship between these metrics, forecast accuracy, and the similarity of errors, both analytically and through simulations. We illustrate this measure of the value a component model adds to an ensemble in the presence of other models using US COVID-19 death forecasts. This study offers valuable insight into individual models' unique features within an ensemble, which standard accuracy metrics alone cannot reveal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08916v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minsu Kim, Evan L. Ray, Nicholas G. Reich</dc:creator>
    </item>
    <item>
      <title>A cheat sheet for probability distributions of orientational data</title>
      <link>https://arxiv.org/abs/2412.08934</link>
      <description>arXiv:2412.08934v1 Announce Type: new 
Abstract: The need for statistical models of orientations arises in many applications in engineering and computer science. Orientational data appear as sets of angles, unit vectors, rotation matrices or quaternions. In the field of directional statistics, a lot of advances have been made in modelling such types of data. However, only a few of these tools are used in engineering and computer science applications. Hence, this paper aims to serve as a cheat sheet for those probability distributions of orientations. Models for 1-DOF, 2-DOF and 3-DOF orientations are discussed. For each of them, expressions for the density function, fitting to data, and sampling are presented. The paper is written with a compromise between engineering and statistics in terms of notation and terminology. A Python library with functions for some of these models is provided. Using this library, two examples of applications to real data are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08934v1</guid>
      <category>stat.ME</category>
      <category>cs.RO</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>P. C. Lopez-Custodio</dc:creator>
    </item>
    <item>
      <title>The square array design</title>
      <link>https://arxiv.org/abs/2412.09166</link>
      <description>arXiv:2412.09166v1 Announce Type: new 
Abstract: This paper is concerned with the construction of augmented row-column designs for unreplicated trials. The idea is predicated on the representation of a $k \times t$ equireplicate incomplete-block design with $t$ treatments in $t$ blocks of size $k$, termed an auxiliary block design, as a $t \times t$ square array design with $k$ controls, where $k&lt;t$. In essence, the treatments in the $j$th block of the auxiliary block design specify the column coordinates in the $j$th row of the square array design for $j=1, \ldots, t$. The construction can be regarded as an extension of the representation of a Youden square as a partial latin square for unreplicated trials introduced by Fisher in 1938 and, independently, by Federer and Raghavarao in 1975. The properties of the designs, in particular in relation to connectedness and randomization, are explored and particular attention is given to square array designs which minimize the average variances of the estimates of paired comparisons between test-lines and controls and between test-line and test-line effects. The use of equireplicate cyclic designs as auxiliary block designs is highlighted and provides a flexible and workable family of augmented row-column square array designs. The theory underlying the square array designs so constructed is shown to mirror that of the cyclic incomplete-block designs delineated in the paper of David and Wolock in 1965. In addition, equireplicate incomplete-block designs which are not cyclic are introduced as auxiliary block designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09166v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R. A. Bailey, L. M. Haines</dc:creator>
    </item>
    <item>
      <title>Accounting for Multiple Covariates in Non-Stationary Geostatistical Modelling</title>
      <link>https://arxiv.org/abs/2412.09225</link>
      <description>arXiv:2412.09225v1 Announce Type: new 
Abstract: Model-based geostatistics (MBG) is a subfield of spatial statistics focused on predicting spatially continuous phenomena using data collected at discrete locations. Geostatistical models often rely on the assumptions of stationarity and isotropy for practical and conceptual simplicity. However, an alternative perspective involves considering non-stationarity, where statistical characteristics vary across the study area. While previous work has explored non-stationary processes, particularly those leveraging covariate information to address non-stationarity, this research expands upon these concepts by incorporating multiple covariates and proposing different ways for constructing non-stationary processes. Through a simulation study, the significance of selecting the appropriate non-stationary process is demonstrated. The proposed approach is then applied to analyse malaria prevalence data in Mozambique, showcasing its practical utility</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09225v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olatunji Johnson, Bedilu A Ejigu, Ezra Gayawan</dc:creator>
    </item>
    <item>
      <title>Nonparametric estimation of the total treatment effect with multiple outcomes in the presence of terminal events</title>
      <link>https://arxiv.org/abs/2412.09304</link>
      <description>arXiv:2412.09304v1 Announce Type: new 
Abstract: As standards of care advance, patients are living longer and once-fatal diseases are becoming manageable. Clinical trials increasingly focus on reducing disease burden, which can be quantified by the timing and occurrence of multiple non-fatal clinical events. Most existing methods for the analysis of multiple event-time data require stringent modeling assumptions that can be difficult to verify empirically, leading to treatment efficacy estimates that forego interpretability when the underlying assumptions are not met. Moreover, most existing methods do not appropriately account for informative terminal events, such as premature treatment discontinuation or death, which prevent the occurrence of subsequent events. To address these limitations, we derive and validate estimation and inference procedures for the area under the mean cumulative function (AUMCF), an extension of the restricted mean survival time to the multiple event-time setting. The AUMCF is nonparametric, clinically interpretable, and properly accounts for terminal competing risks. To enable covariate adjustment, we also develop an augmentation estimator that provides efficiency at least equaling, and often exceeding, the unadjusted estimator. The utility and interpretability of the AUMCF are illustrated with extensive simulation studies and through an analysis of multiple heart-failure-related endpoints using data from the Beta-Blocker Evaluation of Survival Trial (BEST) clinical trial. Our open-source R package MCC makes conducting AUMCF analyses straightforward and accessible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09304v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jessica Gronsbell, Zachary R. McCaw, Isabella-Emmanuella Nogues, Xiangshan Kong, Tianxi Cai, Lu Tian, LJ Wei</dc:creator>
    </item>
    <item>
      <title>Assessing the replicability of RCTs in RWE emulations</title>
      <link>https://arxiv.org/abs/2412.09334</link>
      <description>arXiv:2412.09334v1 Announce Type: new 
Abstract: Background: The standard regulatory approach to assess replication success is the two-trials rule, requiring both the original and the replication study to be significant with effect estimates in the same direction. The sceptical p-value was recently presented as an alternative method for the statistical assessment of the replicability of study results. Methods: We compare the statistical properties of the sceptical p-value and the two-trials rule. We illustrate the performance of the different methods using real-world evidence emulations of randomized, controlled trials (RCTs) conducted within the RCT DUPLICATE initiative. Results: The sceptical p-value depends not only on the two p-values, but also on sample size and effect size of the two studies. It can be calibrated to have the same Type-I error rate as the two-trials rule, but has larger power to detect an existing effect. In the application to the results from the RCT DUPLICATE initiative, the sceptical p-value leads to qualitatively similar results than the two-trials rule, but tends to show more evidence for treatment effects compared to the two-trials rule. Conclusion: The sceptical p-value represents a valid statistical measure to assess the replicability of study results and is especially useful in the context of real-world evidence emulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09334v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeanette K\"oppe, Charlotte Micheloud, Stella Erdmann, Rachel Heyard, Leonhard Held</dc:creator>
    </item>
    <item>
      <title>Inference under Staggered Adoption: Case Study of the Affordable Care Act</title>
      <link>https://arxiv.org/abs/2412.09482</link>
      <description>arXiv:2412.09482v1 Announce Type: new 
Abstract: Panel data consists of a collection of $N$ units that are observed over $T$ units of time. A policy or treatment is subject to staggered adoption if different units take on treatment at different times and remains treated (or never at all). Assessing the effectiveness of such a policy requires estimating the treatment effect, corresponding to the difference between outcomes for treated versus untreated units. We develop inference procedures that build upon a computationally efficient matrix estimator for treatment effects in panel data. Our routines return confidence intervals (CIs) both for individual treatment effects, as well as for more general bilinear functionals of treatment effects, with prescribed coverage guarantees. We apply these inferential methods to analyze the effectiveness of Medicaid expansion portion of the Affordable Care Act. Based on our analysis, Medicaid expansion has led to substantial reductions in uninsurance rates, has reduced infant mortality rates, and has had no significant effects on healthcare expenditures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09482v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Xia, Yuling Yan, Martin J. Wainwright</dc:creator>
    </item>
    <item>
      <title>The Extended Crosswise Model Adjusted for Random Answering</title>
      <link>https://arxiv.org/abs/2412.09506</link>
      <description>arXiv:2412.09506v1 Announce Type: new 
Abstract: The Extended Crosswise Model is a popular randomized response design that employs a sensitive and a randomized innocuous statement, and asks respondents if one of these statements is true, or that none or both are true. The model has a degree of freedom to test for response biases, but is unable to detect random answering. In this paper, we propose two new methods to indirectly estimate and correct for random answering. One method uses a non-sensitive control statement and a quasi-randomized innocuous statement to which both answers are known to estimate the proportion of random respondents. The other method assigns less weight in the estimation procedure to respondents who complete the survey in an unrealistically short time. For four surveys among elite athletes, we use these methods to correct the prevalence estimates of doping use for random answering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09506v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Khadiga H. A. Sayed, Maarten J. L. F. Cruyff, Andrea Petr\'oczi, Peter G. M. van der Heijden</dc:creator>
    </item>
    <item>
      <title>Bayesian nonparametric mixtures of Archimedean copulas</title>
      <link>https://arxiv.org/abs/2412.09539</link>
      <description>arXiv:2412.09539v1 Announce Type: new 
Abstract: Copula-based dependence modelling often relies on parametric formulations. This is mathematically convenient but can be statistically inefficient if the parametric families are not suitable for the data and model in focus. To improve the flexibility in modeling dependence, we consider a Bayesian nonparametric mixture model of Archimedean copulas which can capture complex dependence patterns and can be extended to arbitrary dimensions. In particular we use the Poisson-Dirichlet process as mixing distribution over the single parameter of the Archimedean copulas. Properties of the mixture model are studied for the main Archimedenan families and posterior distributions are sampled via their full conditional distributions. Performance of the model is via numerical experiments involving simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09539v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruyi Pan, Luis E. Nieto-Barajas, Radu V. Craiu</dc:creator>
    </item>
    <item>
      <title>Beyond Reweighting: On the Predictive Role of Covariate Shift in Effect Generalization</title>
      <link>https://arxiv.org/abs/2412.08869</link>
      <description>arXiv:2412.08869v1 Announce Type: cross 
Abstract: Many existing approaches to generalizing statistical inference amidst distribution shift operate under the covariate shift assumption, which posits that the conditional distribution of unobserved variables given observable ones is invariant across populations. However, recent empirical investigations have demonstrated that adjusting for shift in observed variables (covariate shift) is often insufficient for generalization. In other words, covariate shift does not typically ``explain away'' the distribution shift between settings. As such, addressing the unknown yet non-negligible shift in the unobserved variables given observed ones (conditional shift) is crucial for generalizable inference.
  In this paper, we present a series of empirical evidence from two large-scale multi-site replication studies to support a new role of covariate shift in ``predicting'' the strength of the unknown conditional shift. Analyzing 680 studies across 65 sites, we find that even though the conditional shift is non-negligible, its strength can often be bounded by that of the observable covariate shift. However, this pattern only emerges when the two sources of shifts are quantified by our proposed standardized, ``pivotal'' measures. We then interpret this phenomenon by connecting it to similar patterns that can be theoretically derived from a random distribution shift model. Finally, we demonstrate that exploiting the predictive role of covariate shift leads to reliable and efficient uncertainty quantification for target estimates in generalization tasks with partially observed data. Overall, our empirical and theoretical analyses suggest a new way to approach the problem of distributional shift, generalizability, and external validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08869v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ying Jin, Naoki Egami, Dominik Rothenh\"ausler</dc:creator>
    </item>
    <item>
      <title>Explicit modeling of density dependence in spatial capture-recapture models</title>
      <link>https://arxiv.org/abs/2412.09431</link>
      <description>arXiv:2412.09431v1 Announce Type: cross 
Abstract: Density dependence occurs at the individual level but is often evaluated at the population level, leading to difficulties or even controversies in detecting such a process. Bayesian individual-based models such as spatial capture-recapture (SCR) models provide opportunities to study density dependence at the individual level, but such an approach remains to be developed and evaluated. In this study, we developed a SCR model that links habitat use to apparent survival and recruitment through density dependent processes at the individual level. Using simulations, we found that the model can properly inform habitat use, but tends to underestimate the effect of density dependence on apparent survival and recruitment. The reason for such underestimations is likely due to the fact that SCR models have difficulties in identifying the locations of unobserved individuals while assuming they are uniformly distributed. How to accurately estimate the locations of unobserved individuals, and thus density dependence, remains a challenging topic in spatial statistics and statistical ecology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09431v1</guid>
      <category>q-bio.PE</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qing Zhao, Yunyi Shen</dc:creator>
    </item>
    <item>
      <title>Gradient descent inference in empirical risk minimization</title>
      <link>https://arxiv.org/abs/2412.09498</link>
      <description>arXiv:2412.09498v1 Announce Type: cross 
Abstract: Gradient descent is one of the most widely used iterative algorithms in modern statistical learning. However, its precise algorithmic dynamics in high-dimensional settings remain only partially understood, which has therefore limited its broader potential for statistical inference applications.
  This paper provides a precise, non-asymptotic distributional characterization of gradient descent iterates in a broad class of empirical risk minimization problems, in the so-called mean-field regime where the sample size is proportional to the signal dimension. Our non-asymptotic state evolution theory holds for both general non-convex loss functions and non-Gaussian data, and reveals the central role of two Onsager correction matrices that precisely characterize the non-trivial dependence among all gradient descent iterates in the mean-field regime.
  Although the Onsager correction matrices are typically analytically intractable, our state evolution theory facilitates a generic gradient descent inference algorithm that consistently estimates these matrices across a broad class of models. Leveraging this algorithm, we show that the state evolution can be inverted to construct (i) data-driven estimators for the generalization error of gradient descent iterates and (ii) debiased gradient descent iterates for inference of the unknown signal. Detailed applications to two canonical models--linear regression and (generalized) logistic regression--are worked out to illustrate model-specific features of our general theory and inference methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09498v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiyang Han, Xiaocong Xu</dc:creator>
    </item>
    <item>
      <title>Quantile Fourier Transform, Quantile Series, and Nonparametric Estimation of Quantile Spectra</title>
      <link>https://arxiv.org/abs/2211.05844</link>
      <description>arXiv:2211.05844v3 Announce Type: replace 
Abstract: A nonparametric method is proposed for estimating the quantile spectra and cross-spectra introduced in Li (2012; 2014) as bivariate functions of frequency and quantile level. The method is based on the quantile discrete Fourier transform (QDFT) defined by trigonometric quantile regression and the quantile series (QSER) defined by the inverse Fourier transform of the QDFT. A nonparametric spectral estimator is constructed from the autocovariance function of the QSER using the lag-window (LW) approach. Smoothing techniques are also employed to reduce the statistical variability of the LW estimator across quantiles when the underlying spectrum varies smoothly with respect to the quantile level. The performance of the proposed estimation method is evaluated through a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.05844v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ta-Hsin Li</dc:creator>
    </item>
    <item>
      <title>The effect of estimating prevalences on the population-wise error rate</title>
      <link>https://arxiv.org/abs/2304.09988</link>
      <description>arXiv:2304.09988v3 Announce Type: replace 
Abstract: The population-wise error rate (PWER) is a type I error rate for clinical trials with multiple target populations. In such trials, a treatment is tested for its efficacy in each population. The PWER is defined as the probability that a randomly selected, future patient will be exposed to an inefficient treatment based on the study results. It can be understood and computed as an average of strata-specific family wise error rates and involves the prevalences of these strata. A major issue of this concept is that the prevalences are usually unknown in practice, so that the PWER cannot be directly controlled. Instead, one could use an estimator based on the given sample, like their maximum-likelihood estimator under a multinomial distribution. In this article, we demonstrate through simulations that this does not substantially inflate the true PWER. We differentiate between the expected PWER, which is almost perfectly controlled, and study-specific values of the PWER which are conditioned on all subgroup sample sizes and vary within a narrow range. Thereby, we consider up to eight different overlapping populations and moderate to large sample sizes. In these settings, we also consider the maximum strata-wise family wise error rate, which is found to be, on average, at least bounded by twice the significance level used for PWER control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.09988v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Remi Luschei, Werner Brannath</dc:creator>
    </item>
    <item>
      <title>A Double Machine Learning Approach for the Evaluation of COVID-19 Vaccine Effectiveness under the Test-Negative Design: Analysis of Qu\'ebec Administrative Data</title>
      <link>https://arxiv.org/abs/2310.04578</link>
      <description>arXiv:2310.04578v2 Announce Type: replace 
Abstract: The test-negative design (TND), which is routinely used for monitoring seasonal flu vaccine effectiveness (VE), has recently become integral to COVID-19 vaccine surveillance, notably in Qu\'ebec, Canada. Some studies have addressed the identifiability and estimation of causal parameters under the TND, but efficiency bounds for nonparametric estimators of the target parameter under the unconfoundedness assumption have not yet been investigated. Motivated by the goal of improving adjustment for measured confounders when estimating COVID-19 VE among community-dwelling people aged $\geq 60$ years in Qu\'ebec, we propose a one-step doubly robust and locally efficient estimator called TNDDR (TND doubly robust), which utilizes cross-fitting (sample splitting) and can incorporate machine learning techniques to estimate the nuisance functions and thus improve control for measured confounders. We derive the efficient influence function (EIF) for the marginal expectation of the outcome under a vaccination intervention, explore the von Mises expansion, and establish the conditions for $\sqrt{n}-$consistency, asymptotic normality and double robustness of TNDDR. The proposed estimator is supported by both theoretical and empirical justifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.04578v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cong Jiang, Denis Talbot, Sara Carazo, Mireille E Schnitzer</dc:creator>
    </item>
    <item>
      <title>Balancing Weights for Non-monotone Missing Data</title>
      <link>https://arxiv.org/abs/2402.08873</link>
      <description>arXiv:2402.08873v2 Announce Type: replace 
Abstract: Balancing weights have been widely applied to single or monotone missingness due to empirical advantages over likelihood-based methods and inverse probability weighting approaches. This paper considers non-monotone missing data under the complete-case missing variable condition (CCMV), a case of missing not at random (MNAR). Using relationships between each missing pattern and the complete-case subsample, we construct a weighted estimator for estimation, where the weight is a sum of ratios of the conditional probability of observing a particular missing pattern versus that of observing the complete-case, given the variables observed in the corresponding missing pattern. However, plug-in estimators of the propensity odds can be unbounded and lead to unstable estimation. Using further relations between propensity odds and balancing of moments across response patterns, we employ tailored loss functions, each encouraging empirical balance across patterns to estimate propensity odds flexibly using a functional basis expansion. We propose two penalizations to control propensity odds model smoothness and empirical imbalance. We study the asymptotic properties of the proposed estimators and show that they are consistent under mild smoothness assumptions. Asymptotic normality and efficiency are developed. Simulation results show the superior performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08873v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Jianing Dong, Raymond K. W. Wong, Kwun Chuen Gary Chan</dc:creator>
    </item>
    <item>
      <title>Quantifying Individual Risk for Binary Outcome</title>
      <link>https://arxiv.org/abs/2402.10537</link>
      <description>arXiv:2402.10537v2 Announce Type: replace 
Abstract: Understanding treatment effect heterogeneity is crucial for reliable decision-making in treatment evaluation and selection. The conditional average treatment effect (CATE) is widely used to capture treatment effect heterogeneity induced by observed covariates and to design individualized treatment policies. However, it is an average metric within subpopulations, which prevents it from revealing individual-level risks, potentially leading to misleading results. This article fills this gap by examining individual risk for binary outcomes, specifically focusing on the fraction negatively affected (FNA), a metric that quantifies the percentage of individuals experiencing worse outcomes under treatment compared with control. Even under the strong ignorability assumption, FNA is still unidentifiable, and the existing Frechet-Hoeffding bounds are usually too wide and attainable only under extreme data-generating processes. By invoking mild conditions on the value range of the Pearson correlation coefficient between potential outcomes, we obtain improved bounds compared with previous studies. We show that paradoxically, even with a positive CATE, the lower bound on FNA can be positive, i.e., in the best-case scenario many units will be harmed if they receive treatment. Additionally, we establish a nonparametric sensitivity analysis framework for FNA using the Pearson correlation coefficient as the sensitivity parameter, thereby exploring the relationships among the correlation coefficient, FNA, and CATE. We also propose a method for selecting the range of correlation coefficients. Furthermore, we propose nonparametric estimators for the refined FNA bounds and prove their consistency and asymptotic normality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10537v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peng Wu, Peng Ding, Zhi Geng, Yue Liu</dc:creator>
    </item>
    <item>
      <title>Robust Estimation and Inference for Categorical Data</title>
      <link>https://arxiv.org/abs/2403.11954</link>
      <description>arXiv:2403.11954v3 Announce Type: replace 
Abstract: While there is a rich literature on robust methodologies for contamination in continuously distributed data, contamination in categorical data is largely overlooked. This is regrettable because many datasets are categorical and oftentimes suffer from contamination. Examples include inattentive responding and bot responses in questionnaires or zero-inflated count data. We propose a novel class of contamination-robust estimators of models for categorical data, coined $C$-estimators (``$C$'' for categorical). We show that the countable and possibly finite sample space of categorical data results in non-standard theoretical properties. Notably, in contrast to classic robustness theory, $C$-estimators can be simultaneously robust \textit{and} fully efficient at the postulated model. In addition, a certain particularly robust specification fails to be asymptotically Gaussian at the postulated model, but is asymptotically Gaussian in the presence of contamination. We furthermore propose a diagnostic test to identify categorical outliers and demonstrate the enhanced robustness of $C$-estimators in a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11954v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Welz</dc:creator>
    </item>
    <item>
      <title>Doubly robust estimation and inference for a log-concave counterfactual density</title>
      <link>https://arxiv.org/abs/2403.19917</link>
      <description>arXiv:2403.19917v2 Announce Type: replace 
Abstract: We consider the problem of causal inference based on observational data (or the related missing data problem) with a binary or discrete treatment variable. In that context, we study inference for the counterfactual density functions and contrasts thereof, which can provide more nuanced information than counterfactual means and the average treatment effect. We impose the shape-constraint of log-concavity, a type of unimodality constraint, on the counterfactual densities, and then develop doubly robust estimators of the log-concave counterfactual density based on augmented inverse-probability weighted pseudo-outcomes. We provide conditions under which the estimator is consistent in various global metrics. We also develop asymptotically valid pointwise confidence intervals for the counterfactual density functions and differences and ratios thereof, which serve as a building block for more comprehensive analyses of distributional differences. We also present a method for using our estimator to implement density confidence bands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19917v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daeyoung Ham, Ted Westling, Charles R. Doss</dc:creator>
    </item>
    <item>
      <title>Doubly-robust inference and optimality in structure-agnostic models with smoothness</title>
      <link>https://arxiv.org/abs/2405.08525</link>
      <description>arXiv:2405.08525v2 Announce Type: replace 
Abstract: We study the problem of constructing an estimator of the average treatment effect (ATE) with observational data. The celebrated doubly-robust, augmented-IPW (AIPW) estimator generally requires consistent estimation of both nuisance functions for standard root-n inference, and moreover that the product of the errors of the nuisances should shrink at a rate faster than $n^{-1/2}$. A recent strand of research has aimed to understand the extent to which the AIPW estimator can be improved upon (in a minimax sense). Under structural assumptions on the nuisance functions, the AIPW estimator is typically not minimax-optimal, and improvements can be made using higher-order influence functions (Robins et al, 2017). Conversely, without any assumptions on the nuisances beyond the mean-square-error rates at which they can be estimated, the rate achieved by the AIPW estimator is already optimal (Balakrishnan et al, 2023; Jin and Syrgkanis, 2024).
  We make three main contributions. First, we propose a new hybrid class of distributions that combine structural agnosticism regarding the nuisance function space with additional smoothness constraints. Second, we calculate minimax lower bounds for estimating the ATE in the new class, as well as in the pure structure-agnostic one. Third, we propose a new estimator of the ATE that enjoys doubly-robust asymptotic linearity; it can yield asymptotically valid Wald-type confidence intervals even when the propensity score or the outcome model is inconsistently estimated, or estimated at a slow rate. Under certain conditions, we show that its rate of convergence in the new class can be much faster than that achieved by the AIPW estimator and, in particular, matches the minimax lower bound rate, thereby establishing its optimality. Finally, we complement our theoretical findings with simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08525v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Bonvini, Edward H. Kennedy, Oliver Dukes, Sivaraman Balakrishnan</dc:creator>
    </item>
    <item>
      <title>Bayesian Deep Generative Models for Replicated Networks with Multiscale Overlapping Clusters</title>
      <link>https://arxiv.org/abs/2405.20936</link>
      <description>arXiv:2405.20936v3 Announce Type: replace 
Abstract: Our interest is in replicated network data with multiple networks observed across the same set of nodes. Examples include brain connection networks, in which nodes corresponds to brain regions and replicates to different individuals, and ecological networks, in which nodes correspond to species and replicates to samples collected at different locations and/or times. Our goal is to infer a hierarchical structure of the nodes at a population level, while performing multi-resolution clustering of the individual replicates. In brain connectomics, the focus is on inferring common relationships among the brain regions, while characterizing inter-individual variability in an easily interpretable manner. To accomplish this, we propose a Bayesian hierarchical model, while providing theoretical support in terms of identifiability and posterior consistency, and design efficient methods for posterior computation. We provide novel technical tools for proving model identifiability, which are of independent interest. Our simulations and application to brain connectome data provide support for the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20936v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuren Zhou, Yuqi Gu, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Nested exemplar latent space models for dimension reduction in dynamic networks</title>
      <link>https://arxiv.org/abs/2412.07604</link>
      <description>arXiv:2412.07604v2 Announce Type: replace 
Abstract: Dynamic latent space models are widely used for characterizing changes in networks and relational data over time. These models assign to each node latent attributes that characterize connectivity with other nodes, with these latent attributes dynamically changing over time. Node attributes can be organized as a three-way tensor with modes corresponding to nodes, latent space dimension, and time. Unfortunately, as the number of nodes and time points increases, the number of elements of this tensor becomes enormous, leading to computational and statistical challenges, particularly when data are sparse. We propose a new approach for massively reducing dimensionality by expressing the latent node attribute tensor as low rank. This leads to an interesting new nested exemplar latent space model, which characterizes the node attribute tensor as dependent on low-dimensional exemplar traits for each node, weights for each latent space dimension, and exemplar curves characterizing time variation. We study properties of this framework, including expressivity, and develop efficient Bayesian inference algorithms. The approach leads to substantial advantages in simulations and applications to ecological networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07604v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jennifer Noelle Kampe, Luca Alessandro Silva, Tomas Roslin, David Brian Dunson</dc:creator>
    </item>
    <item>
      <title>Tests of Missing Completely At Random based on sample covariance matrices</title>
      <link>https://arxiv.org/abs/2401.05256</link>
      <description>arXiv:2401.05256v2 Announce Type: replace-cross 
Abstract: We study the problem of testing whether the missing values of a potentially high-dimensional dataset are Missing Completely at Random (MCAR). We relax the problem of testing MCAR to the problem of testing the compatibility of a collection of covariance matrices, motivated by the fact that this procedure is feasible when the dimension grows with the sample size. Our first contributions are to define a natural measure of the incompatibility of a collection of correlation matrices, which can be characterised as the optimal value of a Semi-definite Programming (SDP) problem, and to establish a key duality result allowing its practical computation and interpretation. By analysing the concentration properties of the natural plug-in estimator for this measure, we propose a novel hypothesis test, which is calibrated via a bootstrap procedure and demonstrates power against any distribution with incompatible covariance matrices. By considering key examples of missingness structures, we demonstrate that our procedures are minimax rate optimal in certain cases. We further validate our methodology with numerical simulations that provide evidence of validity and power, even when data are heavy tailed. Furthermore, tests of compatibility can be used to test the feasibility of positive semi-definite matrix completion problems with noisy observations, and thus our results may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.05256v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Bordino, Thomas B. Berrett</dc:creator>
    </item>
    <item>
      <title>Synthetic Potential Outcomes and Causal Mixture Identifiability</title>
      <link>https://arxiv.org/abs/2405.19225</link>
      <description>arXiv:2405.19225v3 Announce Type: replace-cross 
Abstract: Heterogeneous data from multiple populations, sub-groups, or sources is often represented as a ``mixture model'' with a single latent class influencing all of the observed covariates. Heterogeneity can be resolved at multiple levels by grouping populations according to different notions of similarity. This paper proposes grouping with respect to the causal response of an intervention or perturbation on the system. This definition is distinct from previous notions, such as similar covariate values (e.g. clustering) or similar correlations between covariates (e.g. Gaussian mixture models). To solve the problem, we ``synthetically sample'' from a counterfactual distribution using higher-order multi-linear moments of the observable data. To understand how these ``causal mixtures'' fit in with more classical notions, we develop a hierarchy of mixture identifiability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19225v3</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bijan Mazaheri, Chandler Squires, Caroline Uhler</dc:creator>
    </item>
  </channel>
</rss>
