<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Jun 2024 02:50:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Chorioallantoic Membrane Model: A 3D in vivo Testbed for Design and Analysis of MC Systems</title>
      <link>https://arxiv.org/abs/2406.09875</link>
      <description>arXiv:2406.09875v1 Announce Type: new 
Abstract: Molecular Communications (MC) research is increasingly focused on applications within the human body, such as health monitoring and drug delivery. These applications require testing in realistic and living environments. Thus, advancing experimental MC research to the next level requires the development of in vivo experimental testbeds. In this paper, we introduce the Chorioallantoic Membrane ( CAM ) model as a versatile 3D in vivo MC testbed. The CAM is a highly vascularized membrane formed in fertilized chicken eggs and has gained significance in various research fields, including bioengineering, cancer research, and drug development. Its versatility, reproducibility, and realistic biological properties make the CAM model perfectly suited for next-generation MC testbeds, facilitating the transition from proof-of-concept systems to practical applications. We provide a comprehensive introduction to the CAM model, its properties, and its applications in practical research. Additionally, we present a characterization of the CAM model as an MC system. As a preliminary experimental study, we investigate the distribution of fluorescent molecules in the closed-loop vascular system of the CAM model. We also derive an approximate analytical model for the propagation of molecules in closed-loop systems, and show that the proposed model is able to approximate molecule propagation in the CAM model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09875v1</guid>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maximilian Sch\"afer, Andreas Ettner-Sitter, Lukas Brand, Sebastian Lotter, Fardad Vakilipoor, Thiha Aung, Silke Haerteis, Robert Schober</dc:creator>
    </item>
    <item>
      <title>Exemplar LCA-Decoder: A Scalable Framework for On-Chip Learning</title>
      <link>https://arxiv.org/abs/2406.10066</link>
      <description>arXiv:2406.10066v1 Announce Type: new 
Abstract: Neuromorphic computing has recently gained significant attention as a promising combined approach for developing energy-efficient, parallel computing systems inspired by the human brain. Efficient training algorithms are imperative for the effective processing of data on neuromorphic platforms; however, their absence remains a notable gap in the field. In this paper, we reduce the gap by proposing an innovative encoder-decoder technique that leverages sparse coding and the Locally Competitive Algorithm (LCA) to provide a computationally efficient and power-conscious algorithm specifically designed for neuromorphic platforms. Using Exemplar LCA-Decoder we reduce the computational demands and memory requirements associated with training Spiking Neural Networks (SNNs) using error backpropagation methods. Our results show notable test accuracy on ImageNet and CIFAR10/100 datasets, surpassing the previously achieved SNN accuracy on these datasets. Additionally, Exemplar LCA-Decoder is scalable and allows expanding the model and adding new data points and classes cost-effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10066v1</guid>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sanaz Mahmoodi Takaghaj, Jack Sampson</dc:creator>
    </item>
    <item>
      <title>Walking Noise: On Layer-Specific Robustness of Neural Architectures against Noisy Computations and Associated Characteristic Learning Dynamics</title>
      <link>https://arxiv.org/abs/2212.10430</link>
      <description>arXiv:2212.10430v2 Announce Type: replace-cross 
Abstract: Deep neural networks are extremely successful in various applications, however they exhibit high computational demands and energy consumption. This is exacerbated by stuttering technology scaling, prompting the need for novel approaches to handle increasingly complex neural architectures. At the same time, alternative computing technologies such as analog computing, which promise groundbreaking improvements in energy efficiency, are inevitably fraught with noise and inaccurate calculations. Such noisy computations are more energy efficient, and, given a fixed power budget, also more time efficient. However, like any kind of unsafe optimization, they require countermeasures to ensure functionally correct results.
  This work considers noisy computations in an abstract form, and gears to understand the implications of such noise on the accuracy of neural network classifiers as an exemplary workload. We propose a methodology called Walking Noise which injects layer-specific noise to measure the robustness and to provide insights on the learning dynamics. In more detail, we investigate the implications of additive, multiplicative and mixed noise for different classification tasks and model architectures. While noisy training significantly increases robustness for all noise types, we observe in particular that it results in increased weight magnitudes and thus inherently improves the signal-to-noise ratio for additive noise injection. Contrarily, training with multiplicative noise can lead to a form of self-binarization of the model parameters, leading to extreme robustness. We conclude with a discussion of the use of this methodology in practice, among others, discussing its use for tailored multi-execution in noisy environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.10430v2</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hendrik Borras, Bernhard Klein, Holger Fr\"oning</dc:creator>
    </item>
  </channel>
</rss>
