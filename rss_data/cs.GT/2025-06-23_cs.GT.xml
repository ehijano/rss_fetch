<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Jun 2025 04:01:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Ultra-Efficient Contracts: Breaking the Substitutes Barrier in Combinatorial Contracts</title>
      <link>https://arxiv.org/abs/2506.18008</link>
      <description>arXiv:2506.18008v1 Announce Type: new 
Abstract: We study the optimal contract problem in the framework of combinatorial contracts, introduced by Duetting et al. [FOCS'21], where a principal delegates the execution of a project to an agent, and the agent can choose any subset from a given set of costly actions. At the core of the model is a reward function - a monotone set function that maps each set of actions taken by the agent into an expected reward to the principal. To incentivize the agent, the principal offers a contract specifying the fraction of the reward to be paid, and the agent responds with their optimal action set. The goal is to compute the contract that maximizes the principal's expected utility.
  Previous work showed that when the reward function is gross substitutes (GS), the optimal contract can be computed in polynomial time, but the problem is NP-hard for the broader class of Submodular functions. This raised the question: is GS the true boundary of tractability for the optimal contract problem? We prove that tractability extends to the strictly broader class of Ultra functions. Interestingly, GS constitutes precisely the intersection of Ultra and Submodular functions, and our result reveals that it is Ultra - not Submodular - that drives tractability, overturning the prevailing belief that the submodularity component of GS is essential. We further extend tractability beyond additive costs, handling costs that are additive plus symmetric. Our results require new techniques, as prior approaches relied on the submodularity of GS. To the best of our knowledge, this is the first application of Ultra functions in a prominent economic setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18008v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michal Feldman, Liat Yashin</dc:creator>
    </item>
    <item>
      <title>Thresholds for sensitive optimality and Blackwell optimality in stochastic games</title>
      <link>https://arxiv.org/abs/2506.18545</link>
      <description>arXiv:2506.18545v1 Announce Type: new 
Abstract: We investigate refinements of the mean-payoff criterion in two-player zero-sum perfect-information stochastic games. A strategy is Blackwell optimal if it is optimal in the discounted game for all discount factors sufficiently close to $1$. The notion of $d$-sensitive optimality interpolates between mean-payoff optimality (corresponding to the case $d=-1$) and Blackwell optimality ($d=+\infty$). The Blackwell threshold $\alpha_{\sf Bw} \in [0,1[$ is the discount factor above which all optimal strategies in the discounted game are guaranteed to be Blackwell optimal. The $d$-sensitive threshold $\alpha_{\sf d} \in [0,1[$ is defined analogously. Bounding $\alpha_{\sf Bw}$ and $\alpha_{\sf d}$ are fundamental problems in algorithmic game theory, since these thresholds control the complexity for computing Blackwell and $d$-sensitive optimal strategies, by reduction to discounted games which can be solved in $O\left((1-\alpha)^{-1}\right)$ iterations. We provide the first bounds on the $d$-sensitive threshold $\alpha_{\sf d}$ beyond the case $d=-1$, and we establish improved bounds for the Blackwell threshold $\alpha_{\sf Bw}$. This is achieved by leveraging separation bounds on algebraic numbers, relying on Lagrange bounds and more advanced techniques based on Mahler measures and multiplicity theorems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18545v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>St\'ephane Gaubert, Julien Grand-Cl\'ement, Ricardo D. Katz</dc:creator>
    </item>
    <item>
      <title>Agentic Markets: Game Dynamics and Equilibrium in Markets with Learning Agents</title>
      <link>https://arxiv.org/abs/2506.18571</link>
      <description>arXiv:2506.18571v1 Announce Type: new 
Abstract: Autonomous and learning agents increasingly participate in markets - setting prices, placing bids, ordering inventory. Such agents are not just aiming to optimize in an uncertain environment; they are making decisions in a game-theoretical environment where the decision of one agent influences the profit of other agents. While game theory usually predicts outcomes of strategic interaction as an equilibrium, it does not capture how repeated interaction of learning agents arrives at a certain outcome. This article surveys developments in modeling agent behavior as dynamical systems, with a focus on projected gradient and no-regret learning algorithms. In general, learning in games can lead to all types of dynamics, including convergence to equilibrium, but also cycles and chaotic behavior. It is important to understand when we can expect efficient equilibrium in automated markets and when this is not the case. Thus, we analyze when and how learning agents converge to an equilibrium of a market game, drawing on tools from variational inequalities and Lyapunov stability theory. Special attention is given to the stability of projected dynamics and the convergence to equilibrium sets as limiting outcomes. Overall, the paper provides mathematical foundations for analyzing stability and convergence in agentic markets driven by autonomous, learning agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18571v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Bichler, Julius Durmann, Matthias Oberlechner</dc:creator>
    </item>
    <item>
      <title>Robust Committee Voting, or The Other Side of Representation</title>
      <link>https://arxiv.org/abs/2506.18643</link>
      <description>arXiv:2506.18643v1 Announce Type: new 
Abstract: We study approval-based committee voting from a novel perspective. While extant work largely centers around proportional representation of the voters, we shift our focus to the candidates while preserving proportionality. Intuitively, candidates supported by similar voter groups should receive comparable representation. Since deterministic voting rules cannot achieve this ideal, we develop randomized voting rules that satisfy ex-ante neutrality, monotonicity, and continuity, while maintaining strong ex-post proportionality guarantees.
  Continuity of the candidate selection probabilities proves to be the most demanding of our ex-ante desiderata. We provide it via voting rules that are algorithmically stable, a stronger notion of robustness which captures the continuity of the committee distribution under small changes. First, we introduce Softmax-GJCR, a randomized variant of the Greedy Justified Candidate Rule (GJCR) [Brill and Peters, 2023], which carefully leverages slack in GJCR to satisfy our ex-ante properties. This polynomial-time algorithm satisfies EJR+ ex post, assures ex-ante monotonicity and neutrality, and provides $O(k^3/n)$-stability (ignoring $\log$ factors). Building on our techniques for Softmax-GJCR, we further show that stronger stability guarantees can be attained by (i) allowing exponential running time, (ii) relaxing EJR+ to an approximate $\alpha$-EJR+, and (iii) relaxing EJR+ to JR.
  We finally demonstrate the utility of stable voting rules in other settings. In online dynamic committee voting, we show that stable voting rules imply dynamic voting rules with low expected recourse, and illustrate this reduction for Softmax-GJCR. Our voting rules also satisfy a stronger form of stability that coincides with differential privacy, suggesting their applicability in privacy-sensitive domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18643v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gregory Kehne, Ulrike Schmidt-Kraepelin, Krzysztof Sornat</dc:creator>
    </item>
    <item>
      <title>Fair Allocation with Money: What is Your Objective?</title>
      <link>https://arxiv.org/abs/2506.18794</link>
      <description>arXiv:2506.18794v1 Announce Type: new 
Abstract: When allocating indivisible items, there are various ways to use monetary transfers for eliminating envy. Particularly, one can apply a balanced vector of transfer payments, or charge each agent a positive amount, or -- contrarily -- give each agent a positive amount as a ``subsidy''. In each model, one can aim to minimize the amount of payments used; this aim translates into different optimization objectives in each setting. This note compares the various models, and the relations between upper and lower bounds for these objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18794v1</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noga Klein Elmalem, Rica Gonen, Erel Segal-Halevi</dc:creator>
    </item>
    <item>
      <title>On the convergence of computational methods for the online bin stretching problem</title>
      <link>https://arxiv.org/abs/2506.17271</link>
      <description>arXiv:2506.17271v1 Announce Type: cross 
Abstract: Online bin stretching is an online packing problem where some of the best known lower and upper bounds were found through computational searches. The limiting factor in obtaining better bounds with such methods is the computational time allowed. However, there is still no theoretical guarantee that such methods do converge towards the optimal online performance. This paper shows that such methods do, in fact, converge; moreover, bounds on the gap to the optimal are also given. These results frame a theoretical foundation for the convergence of computational approaches for online problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17271v1</guid>
      <category>math.OC</category>
      <category>cs.DM</category>
      <category>cs.GT</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antoine Lhomme, Nicolas Catusse, Nadia Brauner</dc:creator>
    </item>
    <item>
      <title>Perceptual Rationality: An Evolutionary Game Theory of Perceptually Rational Decision-Making</title>
      <link>https://arxiv.org/abs/2506.17724</link>
      <description>arXiv:2506.17724v1 Announce Type: cross 
Abstract: Understanding how biological organisms make decisions is of fundamental importance in understanding behavior. Such an understanding within evolutionary game theory so far has been sought by appealing to bounded rationality. Here, we present a perceptual rationality framework in the context of group cooperative interactions, where individuals make rational decisions based on their evolvable perception of the environment. We show that a simple public goods game accounts for power law distributed perceptual diversity. Incorporating the evolution of social information use into the framework reveals that rational decision-making is a natural root of the evolution of consistent personality differences and power-law distributed behavioral diversity. The behavioral diversity, core to the perceptual rationality approach, can lead to ever-shifting polymorphism or cyclic dynamics, through which different rational personality types coexist and engage in mutualistic, complementary, or competitive and exploitative relationships. This polymorphism can lead to non-monotonic evolution as external environmental conditions change. The framework provides predictions consistent with some large-scale eco-evolutionary patterns and illustrates how the evolution of social structure can modify large-scale eco-evolutionary patterns. Furthermore, consistent with most empirical evidence and in contrast to most theoretical predictions, our work suggests diversity is often detrimental to public good provision, especially in strong social dilemmas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17724v1</guid>
      <category>q-bio.PE</category>
      <category>cs.GT</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Salahshour</dc:creator>
    </item>
    <item>
      <title>Dynamic Evolution of Complex Networks: A Reinforcement Learning Approach Applying Evolutionary Games to Community Structure</title>
      <link>https://arxiv.org/abs/2506.17925</link>
      <description>arXiv:2506.17925v1 Announce Type: cross 
Abstract: Complex networks serve as abstract models for understanding real-world complex systems and provide frameworks for studying structured dynamical systems. This article addresses limitations in current studies on the exploration of individual birth-death and the development of community structures within dynamic systems. To bridge this gap, we propose a networked evolution model that includes the birth and death of individuals, incorporating reinforcement learning through games among individuals. Each individual has a lifespan following an arbitrary distribution, engages in games with network neighbors, selects actions using Q-learning in reinforcement learning, and moves within a two-dimensional space. The developed theories are validated through extensive experiments. Besides, we observe the evolution of cooperative behaviors and community structures in systems both with and without the birth-death process. The fitting of real-world populations and networks demonstrates the practicality of our model. Furthermore, comprehensive analyses of the model reveal that exploitation rates and payoff parameters determine the emergence of communities, learning rates affect the speed of community formation, discount factors influence stability, and two-dimensional space dimensions dictate community size. Our model offers a novel perspective on real-world community development and provides a valuable framework for studying population dynamics behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17925v1</guid>
      <category>cs.SI</category>
      <category>cs.GT</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TPAMI.2025.3579895</arxiv:DOI>
      <dc:creator>Bin Pi, Liang-Jian Deng, Minyu Feng, Matja\v{z} Perc, J\"urgen Kurths</dc:creator>
    </item>
    <item>
      <title>Reply to "Emergent LLM behaviors are observationally equivalent to data leakage"</title>
      <link>https://arxiv.org/abs/2506.18600</link>
      <description>arXiv:2506.18600v1 Announce Type: cross 
Abstract: A potential concern when simulating populations of large language models (LLMs) is data contamination, i.e. the possibility that training data may shape outcomes in unintended ways. While this concern is important and may hinder certain experiments with multi-agent models, it does not preclude the study of genuinely emergent dynamics in LLM populations. The recent critique by Barrie and T\"ornberg [1] of the results of Flint Ashery et al. [2] offers an opportunity to clarify that self-organisation and model-dependent emergent dynamics can be studied in LLM populations, highlighting how such dynamics have been empirically observed in the specific case of social conventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18600v1</guid>
      <category>cs.CL</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ariel Flint Ashery, Luca Maria Aiello, Andrea Baronchelli</dc:creator>
    </item>
    <item>
      <title>Multi-Agent Online Control with Adversarial Disturbances</title>
      <link>https://arxiv.org/abs/2506.18814</link>
      <description>arXiv:2506.18814v1 Announce Type: cross 
Abstract: Multi-agent control problems involving a large number of agents with competing and time-varying objectives are increasingly prevalent in applications across robotics, economics, and energy systems. In this paper, we study online control in multi-agent linear dynamical systems with disturbances. In contrast to most prior work in multi-agent control, we consider an online setting where disturbances are adversarial and where each agent seeks to minimize its own, adversarial sequence of convex losses. In this setting, we investigate the robustness of gradient-based controllers from single-agent online control, with a particular focus on understanding how individual regret guarantees are influenced by the number of agents in the system. Under minimal communication assumptions, we prove near-optimal sublinear regret bounds that hold uniformly for all agents. Finally, when the objectives of the agents are aligned, we show that the multi-agent control problem induces a time-varying potential game for which we derive equilibrium gap guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18814v1</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anas Barakat, John Lazarsfeld, Georgios Piliouras, Antonios Varvitsiotis</dc:creator>
    </item>
    <item>
      <title>Collaborative Mean Estimation Among Heterogeneous Strategic Agents: Individual Rationality, Fairness, and Truthful Contribution</title>
      <link>https://arxiv.org/abs/2407.15881</link>
      <description>arXiv:2407.15881v2 Announce Type: replace 
Abstract: We study a collaborative learning problem where $m$ agents aim to estimate a vector $\mu =(\mu_1,\ldots,\mu_d)\in \mathbb{R}^d$ by sampling from associated univariate normal distributions $\{\mathcal{N}(\mu_k, \sigma^2)\}_{k\in[d]}$. Agent $i$ incurs a cost $c_{i,k}$ to sample from $\mathcal{N}(\mu_k, \sigma^2)$. Instead of working independently, agents can exchange data, collecting cheaper samples and sharing them in return for costly data, thereby reducing both costs and estimation error. We design a mechanism to facilitate such collaboration, while addressing two key challenges: ensuring individually rational (IR) and fair outcomes so all agents benefit, and preventing strategic behavior (e.g. non-collection, data fabrication) to avoid socially undesirable outcomes. We design a mechanism and an associated Nash equilibrium (NE) which minimizes the social penalty-sum of agents' estimation errors and collection costs-while being IR for all agents. We achieve a $\mathcal{O}(\sqrt{m})$-approximation to the minimum social penalty in the worst case and an $\mathcal{O}(1)$-approximation under favorable conditions. Additionally, we establish three hardness results: no nontrivial mechanism guarantees (i) a dominant strategy equilibrium where agents report truthfully, (ii) is IR for every strategy profile of other agents, (iii) or avoids a worst-case $\Omega(\sqrt{m})$ price of stability in any NE. Finally, by integrating concepts from axiomatic bargaining, we demonstrate that our mechanism supports fairer outcomes than one which minimizes social penalty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15881v2</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex Clinton, Yiding Chen, Xiaojin Zhu, Kirthevasan Kandasamy</dc:creator>
    </item>
    <item>
      <title>IDCAIS: Inter-Defender Collision-Aware Interception Strategy against Multiple Attackers</title>
      <link>https://arxiv.org/abs/2112.12098</link>
      <description>arXiv:2112.12098v3 Announce Type: replace-cross 
Abstract: In the prior literature on multi-agent area defense games, the assignments of the defenders to the attackers are done based on a cost metric associated only with the interception of the attackers. In contrast to that, this paper presents an Inter-Defender Collision-Aware Interception Strategy (IDCAIS) for defenders to intercept attackers in order to defend a protected area, such that the defender-to-attacker assignment protocol not only takes into account an interception-related cost but also takes into account any possible future collisions among the defenders on their optimal interception trajectories. In particular, in this paper, the defenders are assigned to intercept attackers using a mixed-integer quadratic program (MIQP) that: 1) minimizes the sum of times taken by defenders to capture the attackers under time-optimal control, as well as 2) helps eliminate or delay possible future collisions among the defenders on the optimal trajectories. To prevent inevitable collisions on optimal trajectories or collisions arising due to time-sub-optimal behavior by the attackers, a minimally augmented control using exponential control barrier function (ECBF) is also provided. Simulations show the efficacy of the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.12098v3</guid>
      <category>eess.SY</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vishnu S. Chipade, Xinyi Wang, Dimitra Panagou</dc:creator>
    </item>
    <item>
      <title>The Hive Mind is a Single Reinforcement Learning Agent</title>
      <link>https://arxiv.org/abs/2410.17517</link>
      <description>arXiv:2410.17517v3 Announce Type: replace-cross 
Abstract: Decision-making is an essential attribute of any intelligent agent or group. Natural systems are known to converge to optimal strategies through at least two distinct mechanisms: collective decision-making via imitation of others, and individual trial-and-error. This paper establishes an equivalence between these two paradigms by drawing from the well-established collective decision-making model of nest-site selection in swarms of honey bees. We show that the emergent distributed cognition (sometimes referred to as the hive mind ) arising from individual bees following simple, local imitation-based rules is equivalent to a single online reinforcement learning (RL) agent interacting with many parallel environments. The update rule through which this macro-agent learns is a bandit algorithm that we coin Maynard-Cross Learning. Our analysis implies that a group of cognition-limited organisms can be on-par with a more complex, reinforcement-enabled entity, substantiating the idea that group-level intelligence may explain how seemingly simple and blind individual behaviors are selected in nature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17517v3</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karthik Soma, Yann Bouteiller, Heiko Hamann, Giovanni Beltrame</dc:creator>
    </item>
    <item>
      <title>Drivers of cooperation in social dilemmas on higher-order networks</title>
      <link>https://arxiv.org/abs/2502.09446</link>
      <description>arXiv:2502.09446v2 Announce Type: replace-cross 
Abstract: Understanding cooperation in social dilemmas requires models that capture the complexity of real-world interactions. While network frameworks have provided valuable insights to model the evolution of cooperation, they are unable to encode group interactions properly. Here, we introduce a general higher-order network framework for multi-player games on structured populations. Our model considers multi-dimensional strategies, based on the observation that social behaviours are affected by the size of the group interaction. We investigate dynamical and structural coupling between different orders of interactions, revealing the crucial role of nested multilevel interactions, and showing how such features can enhance cooperation beyond the limit of traditional models with uni-dimensional strategies. Our work identifies the key drivers promoting cooperative behaviour commonly observed in real-world group social dilemmas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09446v2</guid>
      <category>physics.soc-ph</category>
      <category>cs.GT</category>
      <category>cs.SI</category>
      <category>q-bio.PE</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1098/rsif.2025.0134</arxiv:DOI>
      <dc:creator>Onkar Sadekar, Andrea Civilini, Vito Latora, Federico Battiston</dc:creator>
    </item>
    <item>
      <title>Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures</title>
      <link>https://arxiv.org/abs/2506.06832</link>
      <description>arXiv:2506.06832v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) define probability measures on text. By considering the implicit knowledge question of what it means for an LLM to know such a measure and what it entails algorithmically, we are naturally led to formulate a series of tasks that go beyond generative sampling, involving forms of summarization, counterfactual thinking, anomaly detection, originality search, reverse prompting, debating, creative solving, etc. These tasks can be formulated as games based on LLM measures, which we call Cross-Entropy (Xent) Games. Xent Games can be single-player or multi-player. They involve cross-entropy scores and cross-entropy constraints, and can be expressed as simple computational graphs and programs. We show the Xent Game space is large enough to contain a wealth of interesting examples, while being constructible from basic game-theoretic consistency axioms. We then discuss how the Xent Game space can be used to measure the abilities of LLMs. This leads to the construction of Xent Game measures: finite families of Xent Games that can be used as capability benchmarks, built from a given scope, by extracting a covering measure. To address the unbounded scope problem associated with the challenge of measuring general abilities, we propose to explore the space of Xent Games in a coherent fashion, using ideas inspired by evolutionary dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06832v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.GT</category>
      <category>cs.IT</category>
      <category>cs.NE</category>
      <category>math.IT</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cl\'ement Hongler, Andrew Emil</dc:creator>
    </item>
    <item>
      <title>Autocratic strategies in Cournot oligopoly game</title>
      <link>https://arxiv.org/abs/2506.16038</link>
      <description>arXiv:2506.16038v2 Announce Type: replace-cross 
Abstract: An oligopoly is a market in which the price of a goods is controlled by a few firms. Cournot introduced the simplest game-theoretic model of oligopoly, where profit-maximizing behavior of each firm results in market failure. Furthermore, when the Cournot oligopoly game is infinitely repeated, firms can tacitly collude to monopolize the market. Such tacit collusion is realized by the same mechanism as direct reciprocity in the repeated prisoner's dilemma game, where mutual cooperation can be realized whereas defection is favorable for both prisoners in one-shot game. Recently, in the repeated prisoner's dilemma game, a class of strategies called zero-determinant strategies attracts much attention in the context of direct reciprocity. Zero-determinant strategies are autocratic strategies which unilaterally control payoffs of players. There were many attempts to find zero-determinant strategies in other games and to extend them so as to apply them to broader situations. In this paper, first, we show that zero-determinant strategies exist even in the repeated Cournot oligopoly game. Especially, we prove that an averagely unbeatable zero-determinant strategy exists, which is guaranteed to obtain the average payoff of the opponents. Second, we numerically show that the averagely unbeatable zero-determinant strategy can be used to promote collusion when it is used against an adaptively learning player, whereas it cannot promote collusion when it is used against two adaptively learning players. Our findings elucidate some negative impact of zero-determinant strategies in oligopoly market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16038v2</guid>
      <category>physics.soc-ph</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiko Ueda, Shoma Yagi, Genki Ichinose</dc:creator>
    </item>
  </channel>
</rss>
