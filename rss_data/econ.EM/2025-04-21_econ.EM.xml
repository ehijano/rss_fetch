<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 Apr 2025 02:53:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>How Much Weak Overlap Can Doubly Robust T-Statistics Handle?</title>
      <link>https://arxiv.org/abs/2504.13273</link>
      <description>arXiv:2504.13273v1 Announce Type: new 
Abstract: In the presence of sufficiently weak overlap, it is known that no regular root-n-consistent estimators exist and standard estimators may fail to be asymptotically normal. This paper shows that a thresholded version of the standard doubly robust estimator is asymptotically normal with well-calibrated Wald confidence intervals even when constructed using nonparametric estimates of the propensity score and conditional mean outcome. The analysis implies a cost of weak overlap in terms of black-box nuisance rates, borne when the semiparametric bound is infinite, and the contribution of outcome smoothness to the outcome regression rate, which is incurred even when the semiparametric bound is finite. As a byproduct of this analysis, I show that under weak overlap, the optimal global regression rate is the same as the optimal pointwise regression rate, without the usual polylogarithmic penalty. The high-level conditions yield new rules of thumb for thresholding in practice. In simulations, thresholded AIPW can exhibit moderate overrejection in small samples, but I am unable to reject a null hypothesis of exact coverage in large samples. In an empirical application, the clipped AIPW estimator that targets the standard average treatment effect yields similar precision to a heuristic 10% fixed-trimming approach that changes the target sample.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13273v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jacob Dorn</dc:creator>
    </item>
    <item>
      <title>Using Multiple Outcomes to Adjust Standard Errors for Spatial Correlation</title>
      <link>https://arxiv.org/abs/2504.13295</link>
      <description>arXiv:2504.13295v1 Announce Type: new 
Abstract: Empirical research in economics often examines the behavior of agents located in a geographic space. In such cases, statistical inference is complicated by the interdependence of economic outcomes across locations. A common approach to account for this dependence is to cluster standard errors based on a predefined geographic partition. A second strategy is to model dependence in terms of the distance between units. Dependence, however, does not necessarily stop at borders and is typically not determined by distance alone. This paper introduces a method that leverages observations of multiple outcomes to adjust standard errors for cross-sectional dependence. Specifically, a researcher, while interested in a particular outcome variable, often observes dozens of other variables for the same units. We show that these outcomes can be used to estimate dependence under the assumption that the cross-sectional correlation structure is shared across outcomes. We develop a procedure, which we call Thresholding Multiple Outcomes (TMO), that uses this estimate to adjust standard errors in a given regression setting. We show that adjustments of this form can lead to sizable reductions in the bias of standard errors in calibrated U.S. county-level regressions. Re-analyzing nine recent papers, we find that the proposed correction can make a substantial difference in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13295v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stefano DellaVigna, Guido Imbens, Woojin Kim, David M. Ritzwoller</dc:creator>
    </item>
    <item>
      <title>The heterogeneous causal effects of the EU's Cohesion Fund</title>
      <link>https://arxiv.org/abs/2504.13223</link>
      <description>arXiv:2504.13223v1 Announce Type: cross 
Abstract: This paper quantifies the causal effect of cohesion policy on EU regional output and investment focusing on one of its least studied instruments, i.e., the Cohesion Fund (CF). We employ modern causal inference methods to estimate not only the local average treatment effect but also its time-varying and heterogeneous effects across regions. Utilizing this method, we propose a novel framework for evaluating the effectiveness of CF as an EU cohesion policy tool. Specifically, we estimate the time varying distribution of the CF's causal effects across EU regions and derive key distribution metrics useful for policy evaluation. Our analysis shows that relying solely on average treatment effects masks significant heterogeneity and can lead to misleading conclusions about the effectiveness of the EU's cohesion policy. We find that the impact of the CF is frontloaded, peaking within the first seven years after a region's initial inclusion in the program. The distribution of the effects during this first seven-year cycle of funding is right skewed with relatively thick tails. This indicates positive effects but unevenly distributed across regions. Moreover, the magnitude of the CF effect is inversely related to a region's relative position in the initial distribution of output, i.e., relatively poorer recipient regions experience higher effects compared to relatively richer regions. Finally, we find a non-linear relationship with diminishing returns, whereby the impact of CF declines as the ratio of CF funds received to a region's gross value added (GVA) increases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13223v1</guid>
      <category>econ.GN</category>
      <category>econ.EM</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Angelos Alexopoulos, Ilias Kostarakos, Christos Mylonakis, Petros Varthalitis</dc:creator>
    </item>
    <item>
      <title>Bayesian Model Averaging in Causal Instrumental Variable Models</title>
      <link>https://arxiv.org/abs/2504.13520</link>
      <description>arXiv:2504.13520v1 Announce Type: cross 
Abstract: Instrumental variables are a popular tool to infer causal effects under unobserved confounding, but choosing suitable instruments is challenging in practice. We propose gIVBMA, a Bayesian model averaging procedure that addresses this challenge by averaging across different sets of instrumental variables and covariates in a structural equation model. Our approach extends previous work through a scale-invariant prior structure and accommodates non-Gaussian outcomes and treatments, offering greater flexibility than existing methods. The computational strategy uses conditional Bayes factors to update models separately for the outcome and treatments. We prove that this model selection procedure is consistent. By explicitly accounting for model uncertainty, gIVBMA allows instruments and covariates to switch roles and provides robustness against invalid instruments. In simulation experiments, gIVBMA outperforms current state-of-the-art methods. We demonstrate its usefulness in two empirical applications: the effects of malaria and institutions on income per capita and the returns to schooling. A software implementation of gIVBMA is available in Julia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13520v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gregor Steiner, Mark Steel</dc:creator>
    </item>
    <item>
      <title>Nuclear Norm Regularized Estimation of Panel Regression Models</title>
      <link>https://arxiv.org/abs/1810.10987</link>
      <description>arXiv:1810.10987v4 Announce Type: replace 
Abstract: In this paper we investigate panel regression models with interactive fixed effects. We propose two new estimation methods that are based on minimizing convex objective functions. The first method minimizes the sum of squared residuals with a nuclear (trace) norm regularization. The second method minimizes the nuclear norm of the residuals. We establish the consistency of the two resulting estimators. Those estimators have a very important computational advantage compared to the existing least squares (LS) estimator, in that they are defined as minimizers of a convex objective function. In addition, the nuclear norm penalization helps to resolve a potential identification problem for interactive fixed effect models, in particular when the regressors are low-rank and the number of the factors is unknown. We also show how to construct estimators that are asymptotically equivalent to the least squares (LS) estimator in Bai (2009) and Moon and Weidner (2017) by using our nuclear norm regularized or minimized estimators as initial values for a finite number of LS minimizing iteration steps. This iteration avoids any non-convex minimization, while the original LS estimation problem is generally non-convex, and can have multiple local minima.</description>
      <guid isPermaLink="false">oai:arXiv.org:1810.10987v4</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hyungsik Roger Moon, Martin Weidner</dc:creator>
    </item>
    <item>
      <title>A Simple and Computationally Trivial Estimator for Grouped Fixed Effects Models</title>
      <link>https://arxiv.org/abs/2203.08879</link>
      <description>arXiv:2203.08879v5 Announce Type: replace 
Abstract: This paper introduces a new fixed effects estimator for linear panel data models with clustered time patterns of unobserved heterogeneity. The method avoids non-convex and combinatorial optimization by combining a preliminary consistent estimator of the slope coefficient, an agglomerative pairwise-differencing clustering of cross-sectional units, and a pooled ordinary least squares regression. Asymptotic guarantees are established in a framework where $T$ can grow at any power of $N$, as both $N$ and $T$ approach infinity. Unlike most existing approaches, the proposed estimator is computationally straightforward and does not require a known upper bound on the number of groups. As existing approaches, this method leads to a consistent estimation of well-separated groups and an estimator of common parameters asymptotically equivalent to the infeasible regression controlling for the true groups. An application revisits the statistical association between income and democracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.08879v5</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Mugnier</dc:creator>
    </item>
    <item>
      <title>Doubly Robust Inference on Causal Derivative Effects for Continuous Treatments</title>
      <link>https://arxiv.org/abs/2501.06969</link>
      <description>arXiv:2501.06969v2 Announce Type: replace-cross 
Abstract: Statistical methods for causal inference with continuous treatments mainly focus on estimating the mean potential outcome function, commonly known as the dose-response curve. However, it is often not the dose-response curve but its derivative function that signals the treatment effect. In this paper, we investigate nonparametric inference on the derivative of the dose-response curve with and without the positivity condition. Under the positivity and other regularity conditions, we propose a doubly robust (DR) inference method for estimating the derivative of the dose-response curve using kernel smoothing. When the positivity condition is violated, we demonstrate the inconsistency of conventional inverse probability weighting (IPW) and DR estimators, and introduce novel bias-corrected IPW and DR estimators. In all settings, our DR estimator achieves asymptotic normality at the standard nonparametric rate of convergence with nonparametric efficiency guarantees. Additionally, our approach reveals an interesting connection to nonparametric support and level set estimation problems. Finally, we demonstrate the applicability of our proposed estimators through simulations and a case study of evaluating a job training program.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06969v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yikun Zhang, Yen-Chi Chen</dc:creator>
    </item>
    <item>
      <title>A Powerful Bootstrap Test of Independence in High Dimensions</title>
      <link>https://arxiv.org/abs/2503.21715</link>
      <description>arXiv:2503.21715v2 Announce Type: replace-cross 
Abstract: This paper proposes a nonparametric test of pairwise independence of one random variable from a large pool of other random variables. The test statistic is the maximum of several Chatterjee's rank correlations and critical values are computed via a block multiplier bootstrap. The test is shown to asymptotically control size uniformly over a large class of data-generating processes, even when the number of variables is much larger than sample size. The test is consistent against any fixed alternative. It can be combined with a stepwise procedure for selecting those variables from the pool that violate independence, while controlling the family-wise error rate. All formal results leave the dependence among variables in the pool completely unrestricted. In simulations, we find that our test is very powerful, outperforming existing tests in most scenarios considered, particularly in high dimensions and/or when the variables in the pool are dependent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21715v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mauricio Olivares, Tomasz Olma, Daniel Wilhelm</dc:creator>
    </item>
  </channel>
</rss>
