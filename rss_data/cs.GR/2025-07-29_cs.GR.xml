<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Jul 2025 01:29:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D Gaussian Splatting</title>
      <link>https://arxiv.org/abs/2507.19718</link>
      <description>arXiv:2507.19718v1 Announce Type: new 
Abstract: Real-time path tracing is rapidly becoming the standard for rendering in entertainment and professional applications. In scientific visualization, volume rendering plays a crucial role in helping researchers analyze and interpret complex 3D data. Recently, photorealistic rendering techniques have gained popularity in scientific visualization, yet they face significant challenges. One of the most prominent issues is slow rendering performance and high pixel variance caused by Monte Carlo integration. In this work, we introduce a novel radiance caching approach for path-traced volume rendering. Our method leverages advances in volumetric scene representation and adapts 3D Gaussian splatting to function as a multi-level, path-space radiance cache. This cache is designed to be trainable on the fly, dynamically adapting to changes in scene parameters such as lighting configurations and transfer functions. By incorporating our cache, we achieve less noisy, higher-quality images without increasing rendering costs. To evaluate our approach, we compare it against a baseline path tracer that supports uniform sampling and next-event estimation and the state-of-the-art for neural radiance caching. Through both quantitative and qualitative analyses, we demonstrate that our path-space radiance cache is a robust solution that is easy to integrate and significantly enhances the rendering quality of volumetric visualization applications while maintaining comparable computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19718v1</guid>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Bauer, Qi Wu, Hamid Gadirov, Kwan-Liu Ma</dc:creator>
    </item>
    <item>
      <title>Taking Language Embedded 3D Gaussian Splatting into the Wild</title>
      <link>https://arxiv.org/abs/2507.19830</link>
      <description>arXiv:2507.19830v1 Announce Type: new 
Abstract: Recent advances in leveraging large-scale Internet photo collections for 3D reconstruction have enabled immersive virtual exploration of landmarks and historic sites worldwide. However, little attention has been given to the immersive understanding of architectural styles and structural knowledge, which remains largely confined to browsing static text-image pairs. Therefore, can we draw inspiration from 3D in-the-wild reconstruction techniques and use unconstrained photo collections to create an immersive approach for understanding the 3D structure of architectural components? To this end, we extend language embedded 3D Gaussian splatting (3DGS) and propose a novel framework for open-vocabulary scene understanding from unconstrained photo collections. Specifically, we first render multiple appearance images from the same viewpoint as the unconstrained image with the reconstructed radiance field, then extract multi-appearance CLIP features and two types of language feature uncertainty maps-transient and appearance uncertainty-derived from the multi-appearance features to guide the subsequent optimization process. Next, we propose a transient uncertainty-aware autoencoder, a multi-appearance language field 3DGS representation, and a post-ensemble strategy to effectively compress, learn, and fuse language features from multiple appearances. Finally, to quantitatively evaluate our method, we introduce PT-OVS, a new benchmark dataset for assessing open-vocabulary segmentation performance on unconstrained photo collections. Experimental results show that our method outperforms existing methods, delivering accurate open-vocabulary segmentation and enabling applications such as interactive roaming with open-vocabulary queries, architectural style pattern recognition, and 3D scene editing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19830v1</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuze Wang, Yue Qi</dc:creator>
    </item>
    <item>
      <title>ChoreoMuse: Robust Music-to-Dance Video Generation with Style Transfer and Beat-Adherent Motion</title>
      <link>https://arxiv.org/abs/2507.19836</link>
      <description>arXiv:2507.19836v1 Announce Type: new 
Abstract: Modern artistic productions increasingly demand automated choreography generation that adapts to diverse musical styles and individual dancer characteristics. Existing approaches often fail to produce high-quality dance videos that harmonize with both musical rhythm and user-defined choreography styles, limiting their applicability in real-world creative contexts. To address this gap, we introduce ChoreoMuse, a diffusion-based framework that uses SMPL format parameters and their variation version as intermediaries between music and video generation, thereby overcoming the usual constraints imposed by video resolution. Critically, ChoreoMuse supports style-controllable, high-fidelity dance video generation across diverse musical genres and individual dancer characteristics, including the flexibility to handle any reference individual at any resolution. Our method employs a novel music encoder MotionTune to capture motion cues from audio, ensuring that the generated choreography closely follows the beat and expressive qualities of the input music. To quantitatively evaluate how well the generated dances match both musical and choreographic styles, we introduce two new metrics that measure alignment with the intended stylistic cues. Extensive experiments confirm that ChoreoMuse achieves state-of-the-art performance across multiple dimensions, including video quality, beat alignment, dance diversity, and style adherence, demonstrating its potential as a robust solution for a wide range of creative applications. Video results can be found on our project page: https://choreomuse.github.io.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19836v1</guid>
      <category>cs.GR</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.MM</category>
      <category>cs.SD</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuanchen Wang, Heng Wang, Weidong Cai</dc:creator>
    </item>
    <item>
      <title>Neural Shell Texture Splatting: More Details and Fewer Primitives</title>
      <link>https://arxiv.org/abs/2507.20200</link>
      <description>arXiv:2507.20200v1 Announce Type: new 
Abstract: Gaussian splatting techniques have shown promising results in novel view synthesis, achieving high fidelity and efficiency. However, their high reconstruction quality comes at the cost of requiring a large number of primitives. We identify this issue as stemming from the entanglement of geometry and appearance in Gaussian Splatting. To address this, we introduce a neural shell texture, a global representation that encodes texture information around the surface. We use Gaussian primitives as both a geometric representation and texture field samplers, efficiently splatting texture features into image space. Our evaluation demonstrates that this disentanglement enables high parameter efficiency, fine texture detail reconstruction, and easy textured mesh extraction, all while using significantly fewer primitives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20200v1</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xin Zhang, Anpei Chen, Jincheng Xiong, Pinxuan Dai, Yujun Shen, Weiwei Xu</dc:creator>
    </item>
    <item>
      <title>Methodology for intelligent injection point location based on geometric algorithms and discrete topologies for virtual digital twin environments</title>
      <link>https://arxiv.org/abs/2507.20922</link>
      <description>arXiv:2507.20922v1 Announce Type: new 
Abstract: This article presents an innovative methodology for locating injection points in injection-molded parts using intelligent models with geometric algorithms for discrete topologies. The first algorithm calculates the center of mass of the discrete model based on the center of mass of each triangular facet in the system, ensuring uniform molten plastic distribution during mold cavity filling. Two sub-algorithms intelligently evaluate the geometry and optimal injection point location. The first sub-algorithm generates a geometric matrix based on a two-dimensional nodal quadrature adapted to the part's bounding box. The second sub-algorithm projects the nodal matrix and associated circular areas orthogonally on the part's surface along the demolding direction. The optimal injection point location is determined by minimizing the distance to the center of mass from the first algorithm's result. This novel methodology has been validated through rheological simulations in six case studies with complex geometries. The results demonstrate uniform and homogeneous molten plastic distribution with minimal pressure loss during the filling phase. Importantly, this methodology does not require expert intervention, reducing time and costs associated with manual injection mold feed system design. It is also adaptable to various design environments and virtual twin systems, not tied to specific CAD software. The validated results surpass the state of the art, offering an agile alternative for digital twin applications in new product design environments, reducing dependence on experts, facilitating designer training, and ultimately cutting costs</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20922v1</guid>
      <category>cs.GR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.6036/11004</arxiv:DOI>
      <arxiv:journal_reference>Mercado-Colmenero, J. M., Torres-Alba, A., Martin-Donate, C. (2024). DYNA, 99(1), 44-50</arxiv:journal_reference>
      <dc:creator>J. Mercado Colmenero, A. Torres Alba, C. Martin Donate</dc:creator>
    </item>
    <item>
      <title>Visual Analytics Using Tensor Unified Linear Comparative Analysis</title>
      <link>https://arxiv.org/abs/2507.19988</link>
      <description>arXiv:2507.19988v1 Announce Type: cross 
Abstract: Comparing tensors and identifying their (dis)similar structures is fundamental in understanding the underlying phenomena for complex data. Tensor decomposition methods help analysts extract tensors' essential characteristics and aid in visual analytics for tensors. In contrast to dimensionality reduction (DR) methods designed only for analyzing a matrix (i.e., second-order tensor), existing tensor decomposition methods do not support flexible comparative analysis. To address this analysis limitation, we introduce a new tensor decomposition method, named tensor unified linear comparative analysis (TULCA), by extending its DR counterpart, ULCA, for tensor analysis. TULCA integrates discriminant analysis and contrastive learning schemes for tensor decomposition, enabling flexible comparison of tensors. We also introduce an effective method to visualize a core tensor extracted from TULCA into a set of 2D visualizations. We integrate TULCA's functionalities into a visual analytics interface to support analysts in interpreting and refining the TULCA results. We demonstrate the efficacy of TULCA and the visual analytics interface with computational evaluations and two case studies, including an analysis of log data collected from a supercomputer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19988v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naoki Okami, Kazuki Miyake, Naohisa Sakamoto, Jorji Nonaka, Takanori Fujiwara</dc:creator>
    </item>
    <item>
      <title>Aggregation-aware MLP: An Unsupervised Approach for Graph Message-passing</title>
      <link>https://arxiv.org/abs/2507.20127</link>
      <description>arXiv:2507.20127v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have become a dominant approach to learning graph representations, primarily because of their message-passing mechanisms. However, GNNs typically adopt a fixed aggregator function such as Mean, Max, or Sum without principled reasoning behind the selection. This rigidity, especially in the presence of heterophily, often leads to poor, problem dependent performance. Although some attempts address this by designing more sophisticated aggregation functions, these methods tend to rely heavily on labeled data, which is often scarce in real-world tasks. In this work, we propose a novel unsupervised framework, "Aggregation-aware Multilayer Perceptron" (AMLP), which shifts the paradigm from directly crafting aggregation functions to making MLP adaptive to aggregation. Our lightweight approach consists of two key steps: First, we utilize a graph reconstruction method that facilitates high-order grouping effects, and second, we employ a single-layer network to encode varying degrees of heterophily, thereby improving the capacity and applicability of the model. Extensive experiments on node clustering and classification demonstrate the superior performance of AMLP, highlighting its potential for diverse graph learning scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20127v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xuanting Xie, Bingheng Li, Erlin Pan, Zhao Kang, Wenyu Chen</dc:creator>
    </item>
    <item>
      <title>Signed Higher-Order Interactions for Brain Disorder Diagnosis via Multi-Channel Transformers</title>
      <link>https://arxiv.org/abs/2507.20205</link>
      <description>arXiv:2507.20205v2 Announce Type: cross 
Abstract: Accurately characterizing higher-order interactions of brain regions and extracting interpretable organizational patterns from Functional Magnetic Resonance Imaging data is crucial for brain disease diagnosis. Current graph-based deep learning models primarily focus on pairwise or triadic patterns while neglecting signed higher-order interactions, limiting comprehensive understanding of brain-wide communication. We propose HOI-Brain, a novel computational framework leveraging signed higher-order interactions and organizational patterns in fMRI data for brain disease diagnosis. First, we introduce a co-fluctuation measure based on Multiplication of Temporal Derivatives to detect higher-order interactions with temporal resolution. We then distinguish positive and negative synergistic interactions, encoding them in signed weighted simplicial complexes to reveal brain communication insights. Using Persistent Homology theory, we apply two filtration processes to these complexes to extract signed higher-dimensional neural organizations spatiotemporally. Finally, we propose a multi-channel brain Transformer to integrate heterogeneous topological features. Experiments on Alzheimer' s disease, Parkinson' s syndrome, and autism spectrum disorder datasets demonstrate our framework' s superiority, effectiveness, and interpretability. The identified key brain regions and higher-order patterns align with neuroscience literature, providing meaningful biological insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20205v2</guid>
      <category>q-bio.NC</category>
      <category>cs.GR</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dengyi Zhao, Zhiheng Zhou, Guiying Yan, Dongxiao Yu, Xingqin Qi</dc:creator>
    </item>
    <item>
      <title>Endoscopic Depth Estimation Based on Deep Learning: A Survey</title>
      <link>https://arxiv.org/abs/2507.20881</link>
      <description>arXiv:2507.20881v1 Announce Type: cross 
Abstract: Endoscopic depth estimation is a critical technology for improving the safety and precision of minimally invasive surgery. It has attracted considerable attention from researchers in medical imaging, computer vision, and robotics. Over the past decade, a large number of methods have been developed. Despite the existence of several related surveys, a comprehensive overview focusing on recent deep learning-based techniques is still limited. This paper endeavors to bridge this gap by systematically reviewing the state-of-the-art literature. Specifically, we provide a thorough survey of the field from three key perspectives: data, methods, and applications, covering a range of methods including both monocular and stereo approaches. We describe common performance evaluation metrics and summarize publicly available datasets. Furthermore, this review analyzes the specific challenges of endoscopic scenes and categorizes representative techniques based on their supervision strategies and network architectures. The application of endoscopic depth estimation in the important area of robot-assisted surgery is also reviewed. Finally, we outline potential directions for future research, such as domain adaptation, real-time implementation, and enhanced model generalization, thereby providing a valuable starting point for researchers to engage with and advance the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20881v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ke Niu, Zeyun Liu, Xue Feng, Heng Li, Kaize Shi</dc:creator>
    </item>
    <item>
      <title>Multi-Person Interaction Generation from Two-Person Motion Priors</title>
      <link>https://arxiv.org/abs/2505.17860</link>
      <description>arXiv:2505.17860v2 Announce Type: replace 
Abstract: Generating realistic human motion with high-level controls is a crucial task for social understanding, robotics, and animation. With high-quality MOCAP data becoming more available recently, a wide range of data-driven approaches have been presented. However, modelling multi-person interactions still remains a less explored area. In this paper, we present Graph-driven Interaction Sampling, a method that can generate realistic and diverse multi-person interactions by leveraging existing two-person motion diffusion models as motion priors. Instead of training a new model specific to multi-person interaction synthesis, our key insight is to spatially and temporally separate complex multi-person interactions into a graph structure of two-person interactions, which we name the Pairwise Interaction Graph. We thus decompose the generation task into simultaneous single-person motion generation conditioned on one other's motion. In addition, to reduce artifacts such as interpenetrations of body parts in generated multi-person interactions, we introduce two graph-dependent guidance terms into the diffusion sampling scheme. Unlike previous work, our method can produce various high-quality multi-person interactions without having repetitive individual motions. Extensive experiments demonstrate that our approach consistently outperforms existing methods in reducing artifacts when generating a wide range of two-person and multi-person interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17860v2</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wenning Xu, Shiyu Fan, Paul Henderson, Edmond S. L. Ho</dc:creator>
    </item>
    <item>
      <title>GATE: Geometry-Aware Trained Encoding</title>
      <link>https://arxiv.org/abs/2506.08161</link>
      <description>arXiv:2506.08161v2 Announce Type: replace 
Abstract: The encoding of input parameters is one of the fundamental building blocks of neural network algorithms. Its goal is to map the input data to a higher-dimensional space, typically supported by trained feature vectors. The mapping is crucial for the efficiency and approximation quality of neural networks. We propose a novel geometry-aware encoding called GATE that stores feature vectors on the surface of triangular meshes. Our encoding is suitable for neural rendering-related algorithms, for example, neural radiance caching. It also avoids limitations of previous hash-based encoding schemes, such as hash collisions, selection of resolution versus scene size, and divergent memory access. Our approach decouples feature vector density from geometry density using mesh colors, while allowing for finer control over neural network training and adaptive level-of-detail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08161v2</guid>
      <category>cs.GR</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jakub Bok\v{s}ansk\'y, Daniel Meister, Carsten Benthin</dc:creator>
    </item>
  </channel>
</rss>
