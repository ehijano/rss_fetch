<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Feb 2025 02:53:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Tight Bounds for some Classical Problems Parameterized by Cutwidth</title>
      <link>https://arxiv.org/abs/2502.15884</link>
      <description>arXiv:2502.15884v1 Announce Type: new 
Abstract: Cutwidth is a widely studied parameter that quantifies how well a graph can be decomposed along small edge-cuts. It complements pathwidth, which captures decomposition by small vertex separators, and it is well-known that cutwidth upper-bounds pathwidth. The SETH-tight parameterized complexity of problems on graphs of bounded pathwidth (and treewidth) has been actively studied over the past decade while for cutwidth the complexity of many classical problems remained open.
  For Hamiltonian Cycle, it is known that a $(2+\sqrt{2})^{\operatorname{pw}} n^{O(1)}$ algorithm is optimal for pathwidth under SETH~[Cygan et al.\ JACM 2022]. Van Geffen et al.~[J.\ Graph Algorithms Appl.\ 2020] and Bojikian et al.~[STACS 2023] asked which running time is optimal for this problem parameterized by cutwidth. We answer this question with $(1+\sqrt{2})^{\operatorname{ctw}} n^{O(1)}$ by providing matching upper and lower bounds. Second, as our main technical contribution, we close the gap left by van Heck~[2018] for Partition Into Triangles (and Triangle Packing) by improving both upper and lower bound and getting a tight bound of $\sqrt[3]{3}^{\operatorname{ctw}} n^{O(1)}$, which to our knowledge exhibits the only known tight non-integral basis apart from Hamiltonian Cycle. We show that cuts inducing a disjoint union of paths of length three (unions of so-called $Z$-cuts) lie at the core of the complexity of the problem -- usually lower-bound constructions use simpler cuts inducing either a matching or a disjoint union of bicliques. Finally, we determine the optimal running times for Max Cut ($2^{\operatorname{ctw}} n^{O(1)}$) and Induced Matching ($3^{\operatorname{ctw}} n^{O(1)}$) by providing matching lower bounds for the existing algorithms -- the latter result also answers an open question for treewidth by Chaudhary and Zehavi~[WG 2023].</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15884v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Narek Bojikian, Vera Chekan, Stefan Kratsch</dc:creator>
    </item>
    <item>
      <title>Compression Barriers for Autoregressive Transformers</title>
      <link>https://arxiv.org/abs/2502.15955</link>
      <description>arXiv:2502.15955v1 Announce Type: new 
Abstract: A key limitation of autoregressive Transformers is the large memory needed at inference-time to cache all previous key-value (KV) embeddings. Prior works address this by compressing the KV cache, but often assume specific structural properties of the embeddings. This raises the following natural question: Can truly sublinear space utilization be achieved without such assumptions? In this work, we answer this question in the negative. Any algorithm for attention-based token generation must use $\Theta(nd)$ space, where $n$ is the number of tokens generated so far and $d = \Omega(\log n)$ is the dimension of the KV embeddings. Our proof involves a reduction from a classic communication complexity problem and uses a randomized construction that leverages properties of projections in the spirit of the Johnson-Linderstrauss lemma. For the low-dimensional regime $d = o(\log n)$, we show that any algorithm requires $\Omega(d\cdot e^d)$ space and prove, using tight bounds on covering numbers, that SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this bound. Further, we investigate how sparsity assumptions enable token generation in truly sublinear space, presenting impossibility results and proposing a new KV cache compression algorithm for sliding window attention when the value cache outside the window is unmasked. Finally, we analyze token generation's time complexity, using an indistinguishability argument to prove that no non-adaptive algorithm can compute attention online in sublinear time for all tokens.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15955v1</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Themistoklis Haris, Krzysztof Onak</dc:creator>
    </item>
    <item>
      <title>Learning Neural Networks with Distribution Shift: Efficiently Certifiable Guarantees</title>
      <link>https://arxiv.org/abs/2502.16021</link>
      <description>arXiv:2502.16021v1 Announce Type: new 
Abstract: We give the first provably efficient algorithms for learning neural networks with distribution shift. We work in the Testable Learning with Distribution Shift framework (TDS learning) of Klivans et al. (2024), where the learner receives labeled examples from a training distribution and unlabeled examples from a test distribution and must either output a hypothesis with low test error or reject if distribution shift is detected. No assumptions are made on the test distribution.
  All prior work in TDS learning focuses on classification, while here we must handle the setting of nonconvex regression. Our results apply to real-valued networks with arbitrary Lipschitz activations and work whenever the training distribution has strictly sub-exponential tails. For training distributions that are bounded and hypercontractive, we give a fully polynomial-time algorithm for TDS learning one hidden-layer networks with sigmoid activations. We achieve this by importing classical kernel methods into the TDS framework using data-dependent feature maps and a type of kernel matrix that couples samples from both train and test distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16021v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gautam Chandrasekaran, Adam R. Klivans, Lin Lin Lee, Konstantinos Stavropoulos</dc:creator>
    </item>
    <item>
      <title>The Parameterized Landscape of Labeled Graph Contractions</title>
      <link>https://arxiv.org/abs/2502.16096</link>
      <description>arXiv:2502.16096v1 Announce Type: new 
Abstract: In this work, we study the problem of computing a maximum common contraction of two vertex-labeled graphs, i.e. how to make them identical by contracting as little edges as possible in the two graphs. We study the problem from a parameterized complexity point of view, using parameters such as the maximum degree, the degeneracy, the clique-width or treewidth of the input graphs as well as the number of allowed contractions. We put this complexity in perspective with that of the labeled contractibility problem, i.e determining whether a labeled graph is a contraction of another. Surprisingly, our results indicate very little difference between these problems in terms of parameterized complexity status. We only prove their status to differ when parameterizing by both the degeneracy and the number of allowed contractions, showing W[1]-hardness of the maximum common contraction problem in this case, whereas the contractibility problem is FPT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16096v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Manuel Lafond, Bertrand Marchand</dc:creator>
    </item>
    <item>
      <title>Testing whether a subgraph is convex or isometric</title>
      <link>https://arxiv.org/abs/2502.16193</link>
      <description>arXiv:2502.16193v1 Announce Type: new 
Abstract: We consider the following two algorithmic problems: given a graph $G$ and a subgraph $H\subset G$, decide whether $H$ is an isometric or a geodesically convex subgraph of $G$. It is relatively easy to see that the problems can be solved by computing the distances between all pairs of vertices. We provide a conditional lower bound showing that, for sparse graphs with $n$ vertices and $\Theta(n)$ edges, we cannot expect to solve the problem in $O(n^{2-\varepsilon})$ time for any constant $\varepsilon&gt;0$. We also show that the problem can be solved in subquadratic time for planar graphs and in near-linear time for graphs of bounded treewidth. Finally, we provide a near-linear time algorithm for the setting where $G$ is a plane graph and $H$ is defined by a few cycles in $G$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16193v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sergio Cabello</dc:creator>
    </item>
    <item>
      <title>Planar Network Diversion</title>
      <link>https://arxiv.org/abs/2502.16714</link>
      <description>arXiv:2502.16714v1 Announce Type: new 
Abstract: Network Diversion is a graph problem that has been extensively studied in both the network-analysis and operations-research communities as a measure of how robust a network is against adversarial disruption. This problem is especially well motivated in transportation networks, which are often assumed to be planar. Motivated by this and recent theoretical advances for Network Diversion on planar input graphs, we develop a fast O(n log n) time algorithm and present a practical implementation of this algorithm that is able to solve instances with millions of vertices in a matter of seconds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16714v1</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Bentert, P{\aa}l Gr{\o}n{\aa}s Drange, Fedor V. Fomin, Steinar Simonnes</dc:creator>
    </item>
    <item>
      <title>A Parameterized Complexity Analysis of Bounded Height Depth-first Search Trees</title>
      <link>https://arxiv.org/abs/2502.16723</link>
      <description>arXiv:2502.16723v1 Announce Type: new 
Abstract: Computing bounded depth decompositions is a bottleneck in many applications of the treedepth parameter. The fastest known algorithm, which is due to Reidl, Rossmanith, S\'{a}nchez Villaamil, and Sikdar [ICALP 2014], runs in $2^{\mathcal{O}(k^2)}\cdot n$ time and it is a big open problem whether the dependency on $k$ can be improved to $2^{o(k^2)}\cdot n^{\mathcal{O}(1)}$. We show that the related problem of finding DFS trees of bounded height can be solved faster in $2^{\mathcal{O}(k \log k)}\cdot n$ time. As DFS trees are treedepth decompositions, this circumvents the above mentioned bottleneck for this subclass of graphs of bounded treedepth. This problem has recently found attention independently under the name Minimum Height Lineal Topology (MinHLT) and our algorithm gives a positive answer to an open problem posed by Golovach [Dagstuhl Reports, 2023]. We complement our main result by studying the complexity of MinHLT and related problems in several other settings. First, we show that it remains NP-complete on chordal graphs, and give an FPT-algorithm on chordal graphs for the dual problem, asking for a DFS tree of height at most $n-k$, parameterized by $k$. The parameterized complexity of Dual MinHLT on general graphs is wide open. Lastly, we show that Dual MinHLT and two other problems concerned with finding DFS trees with few or many leaves are FPT parameterized by $k$ plus the treewidth of the input graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16723v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lars Jaffke, Paloma T. de Lima, Wojciech Nadara, Emmanuel Sam</dc:creator>
    </item>
    <item>
      <title>Potential-Based Greedy Matching for Dynamic Delivery Pooling</title>
      <link>https://arxiv.org/abs/2502.16862</link>
      <description>arXiv:2502.16862v1 Announce Type: new 
Abstract: We study the problem of pooling together delivery orders into a single trip, a strategy widely adopted by platforms to reduce total travel distance. Similar to other dynamic matching settings, the pooling decisions involve a trade-off between immediate reward and holding jobs for potentially better opportunities in the future. In this paper, we introduce a new heuristic dubbed potential-based greedy (PB), which aims to keep longer-distance jobs in the system, as they have higher potential reward (distance savings) from being pooled with other jobs in the future. This algorithm is simple in that it depends solely on the topology of the space, and does not rely on forecasts or partial information about future demand arrivals. We prove that PB significantly improves upon a naive greedy approach in terms of worst-case performance on the line. Moreover, we conduct extensive numerical experiments using both synthetic and real-world order-level data from the Meituan platform. Our simulations show that PB consistently outperforms not only the naive greedy heuristic but a number of benchmark algorithms, including (i) batching-based heuristics that are widely used in practice, and (ii) forecast-aware heuristics that are given the correct probability distributions (in synthetic data) or a best-effort forecast (in real data). We attribute the surprising unbeatability of PB to the fact that it is specialized for rewards defined by distance saved in delivery pooling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16862v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongyao Ma, Will Ma, Matias Romero</dc:creator>
    </item>
    <item>
      <title>Minimizers in Semi-Dynamic Strings</title>
      <link>https://arxiv.org/abs/2502.17199</link>
      <description>arXiv:2502.17199v1 Announce Type: new 
Abstract: Minimizers sampling is one of the most widely-used mechanisms for sampling strings. Let $S=S[0]\ldots S[n-1]$ be a string over an alphabet $\Sigma$. In addition, let $w\geq 2$ and $k\geq 1$ be two integers and $\rho=(\Sigma^k,\leq)$ be a total order on $\Sigma^k$. The minimizer of window $X=S[i\mathinner{.\,.} i+w+k-2]$ is the smallest position in $[i,i+w-1]$ where the smallest length-$k$ substring of $S[i\mathinner{.\,.} i+w+k-2]$ based on $\rho$ starts. The set of minimizers for all $i\in[0,n-w-k+1]$ is the set $\mathcal{M}_{w,k,\rho}(S)$ of the minimizers of $S$. The set $\mathcal{M}_{w,k,\rho}(S)$ can be computed in $\mathcal{O}(n)$ time. The folklore algorithm for this computation computes the minimizer of every window in $\mathcal{O}(1)$ amortized time using $\mathcal{O}(w)$ working space. It is thus natural to pose the following two questions:
  Question 1: Can we efficiently support other dynamic updates on the window?
  Question 2: Can we improve on the $\mathcal{O}(w)$ working space?
  We answer both questions in the affirmative:
  1. We term a string $X$ semi-dynamic when one is allowed to insert or delete a letter at any of its ends. We show a data structure that maintains a semi-dynamic string $X$ and supports minimizer queries in $X$ in $\mathcal{O}(1)$ time with amortized $\mathcal{O}(1)$ time per update operation.
  2. We show that this data structure can be modified to occupy strongly sublinear space without increasing the asymptotic complexity of its operations. To the best of our knowledge, this yields the first algorithm for computing $\mathcal{M}_{w,k,\rho}(S)$ in $\mathcal{O}(n)$ time using $\mathcal{O}(\sqrt{w})$ working space.
  We complement our theoretical results with a concrete application and an experimental evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17199v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wiktor Zuba, Oded Lachish, Solon P. Pissis</dc:creator>
    </item>
    <item>
      <title>Towards Efficient Contrastive PAC Learning</title>
      <link>https://arxiv.org/abs/2502.15962</link>
      <description>arXiv:2502.15962v1 Announce Type: cross 
Abstract: We study contrastive learning under the PAC learning framework. While a series of recent works have shown statistical results for learning under contrastive loss, based either on the VC-dimension or Rademacher complexity, their algorithms are inherently inefficient or not implying PAC guarantees. In this paper, we consider contrastive learning of the fundamental concept of linear representations. Surprisingly, even under such basic setting, the existence of efficient PAC learners is largely open. We first show that the problem of contrastive PAC learning of linear representations is intractable to solve in general. We then show that it can be relaxed to a semi-definite program when the distance between contrastive samples is measured by the $\ell_2$-norm. We then establish generalization guarantees based on Rademacher complexity, and connect it to PAC guarantees under certain contrastive large-margin conditions. To the best of our knowledge, this is the first efficient PAC learning algorithm for contrastive learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15962v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jie Shen</dc:creator>
    </item>
    <item>
      <title>Assortment optimization given basket shopping behavior using the Ising model</title>
      <link>https://arxiv.org/abs/2502.16260</link>
      <description>arXiv:2502.16260v1 Announce Type: cross 
Abstract: In markets where customers tend to purchase baskets of products rather than single products, assortment optimization is a major challenge for retailers. Removing a product from a retailer's assortment can result in a severe drop in aggregate demand if this product is a complement to other products. Therefore, accounting for the complementarity effect is essential when making assortment decisions. In this paper, we develop a modeling framework designed to address this problem. We model customers' choices using a Markov random field -- in particular, the Ising model -- which captures pairwise demand dependencies as well as the individual attractiveness of each product. Using the Ising model allows us to leverage existing methodologies for various purposes including parameter estimation and efficient simulation of customer choices. We formulate the assortment optimization problem under this model and show that its decision version is NP-hard. We also provide multiple theoretical insights into the structure of the optimal assortments based on the graphical representation of the Ising model, and propose several heuristic algorithms that can be used to obtain high-quality solutions to the assortment optimization problem. Our numerical analysis demonstrates that the developed simulated annealing procedure leads to an expected profit gain of 15% compared to offering an unoptimized assortment (where all products are included) and around 5% compared to using a revenue-ordered heuristic algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16260v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrey Vasilyev, Sebastian Maier, Ralf W. Seifert</dc:creator>
    </item>
    <item>
      <title>Verifying Classification with Limited Disclosure</title>
      <link>https://arxiv.org/abs/2502.16352</link>
      <description>arXiv:2502.16352v1 Announce Type: cross 
Abstract: We consider the multi-party classification problem introduced by Dong, Hartline, and Vijayaraghavan (2022) motivated by electronic discovery. In this problem, our goal is to design a protocol that guarantees the requesting party receives nearly all responsive documents while minimizing the disclosure of nonresponsive documents. We develop verification protocols that certify the correctness of a classifier by disclosing a few nonresponsive documents.
  We introduce a combinatorial notion called the Leave-One-Out dimension of a family of classifiers and show that the number of nonresponsive documents disclosed by our protocol is at most this dimension in the realizable setting, where a perfect classifier exists in this family. For linear classifiers with a margin, we characterize the trade-off between the margin and the number of nonresponsive documents that must be disclosed for verification. Specifically, we establish a trichotomy in this requirement: for $d$ dimensional instances, when the margin exceeds $1/3$, verification can be achieved by revealing only $O(1)$ nonresponsive documents; when the margin is exactly $1/3$, in the worst case, at least $\Omega(d)$ nonresponsive documents must be disclosed; when the margin is smaller than $1/3$, verification requires $\Omega(e^d)$ nonresponsive documents. We believe this result is of independent interest with applications to coding theory and combinatorial geometry. We further extend our protocols to the nonrealizable setting defining an analogous combinatorial quantity robust Leave-One-Out dimension, and to scenarios where the protocol is tolerant to misclassification errors by Alice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16352v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.DS</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Siddharth Bhandari, Liren Shan</dc:creator>
    </item>
    <item>
      <title>Monotonicity Testing of High-Dimensional Distributions with Subcube Conditioning</title>
      <link>https://arxiv.org/abs/2502.16355</link>
      <description>arXiv:2502.16355v1 Announce Type: cross 
Abstract: We study monotonicity testing of high-dimensional distributions on $\{-1,1\}^n$ in the model of subcube conditioning, suggested and studied by Canonne, Ron, and Servedio~\cite{CRS15} and Bhattacharyya and Chakraborty~\cite{BC18}. Previous work shows that the \emph{sample complexity} of monotonicity testing must be exponential in $n$ (Rubinfeld, Vasilian~\cite{RV20}, and Aliakbarpour, Gouleakis, Peebles, Rubinfeld, Yodpinyanee~\cite{AGPRY19}). We show that the subcube \emph{query complexity} is $\tilde{\Theta}(n/\varepsilon^2)$, by proving nearly matching upper and lower bounds. Our work is the first to use directed isoperimetric inequalities (developed for function monotonicity testing) for analyzing a distribution testing algorithm. Along the way, we generalize an inequality of Khot, Minzer, and Safra~\cite{KMS18} to real-valued functions on $\{-1,1\}^n$.
  We also study uniformity testing of distributions that are promised to be monotone, a problem introduced by Rubinfeld, Servedio~\cite{RS09} , using subcube conditioning. We show that the query complexity is $\tilde{\Theta}(\sqrt{n}/\varepsilon^2)$. Our work proves the lower bound, which matches (up to poly-logarithmic factors) the uniformity testing upper bound for general distributions (Canonne, Chen, Kamath, Levi, Waingarten~\cite{CCKLW21}). Hence, we show that monotonicity does not help, beyond logarithmic factors, in testing uniformity of distributions with subcube conditional queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16355v1</guid>
      <category>math.ST</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deeparnab Chakrabarty, Xi Chen, Simeon Ristic, C. Seshadhri, Erik Waingarten</dc:creator>
    </item>
    <item>
      <title>Simultaneous Swap Regret Minimization via KL-Calibration</title>
      <link>https://arxiv.org/abs/2502.16387</link>
      <description>arXiv:2502.16387v1 Announce Type: cross 
Abstract: Calibration is a fundamental concept that aims at ensuring the reliability of probabilistic predictions by aligning them with real-world outcomes. There is a surge of studies on new calibration measures that are easier to optimize compared to the classical $\ell_1$-Calibration while still having strong implications for downstream applications. One recent such example is the work by Fishelson et al. (2025) who show that it is possible to achieve $O(T^{1/3})$ pseudo $\ell_2$-Calibration error via minimizing pseudo swap regret of the squared loss, which in fact implies the same bound for all bounded proper losses with a smooth univariate form. In this work, we significantly generalize their result in the following ways: (a) in addition to smooth univariate forms, our algorithm also simultaneously achieves $O(T^{1/3})$ swap regret for any proper loss with a twice continuously differentiable univariate form (such as Tsallis entropy); (b) our bounds hold not only for pseudo swap regret that measures losses using the forecaster's distributions on predictions, but also hold for the actual swap regret that measures losses using the forecaster's actual realized predictions.
  We achieve so by introducing a new stronger notion of calibration called (pseudo) KL-Calibration, which we show is equivalent to the (pseudo) swap regret for log loss. We prove that there exists an algorithm that achieves $O(T^{1/3})$ KL-Calibration error and provide an explicit algorithm that achieves $O(T^{1/3})$ pseudo KL-Calibration error. Moreover, we show that the same algorithm achieves $O(T^{1/3}(\log T)^{-1/3}\log(T/\delta))$ swap regret w.p. $\ge 1-\delta$ for any proper loss with a smooth univariate form, which implies $O(T^{1/3})$ $\ell_2$-Calibration error. A technical contribution of our work is a new randomized rounding procedure and a non-uniform discretization scheme to minimize the swap regret for log loss.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16387v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haipeng Luo, Spandan Senapati, Vatsal Sharan</dc:creator>
    </item>
    <item>
      <title>Worst-case Error Bounds for Online Learning of Smooth Functions</title>
      <link>https://arxiv.org/abs/2502.16388</link>
      <description>arXiv:2502.16388v1 Announce Type: cross 
Abstract: Online learning is a model of machine learning where the learner is trained on sequential feedback. We investigate worst-case error for the online learning of real functions that have certain smoothness constraints. Suppose that $\mathcal{F}_q$ is the class of all absolutely continuous functions $f: [0, 1] \rightarrow \mathbb{R}$ such that $\|f'\|_q \le 1$, and $\operatorname{opt}_p(\mathcal{F}_q)$ is the best possible upper bound on the sum of the $p^{\text{th}}$ powers of absolute prediction errors for any number of trials guaranteed by any learner. We show that for any $\delta, \epsilon \in (0, 1)$, $\operatorname{opt}_{1+\delta} (\mathcal{F}_{1+\epsilon}) = O(\min(\delta, \epsilon)^{-1})$. Combined with the previous results of Kimber and Long (1995) and Geneson and Zhou (2023), we achieve a complete characterization of the values of $p, q \ge 1$ that result in $\operatorname{opt}_p(\mathcal{F}_q)$ being finite, a problem open for nearly 30 years.
  We study the learning scenarios of smooth functions that also belong to certain special families of functions, such as polynomials. We prove a conjecture by Geneson and Zhou (2023) that it is not any easier to learn a polynomial in $\mathcal{F}_q$ than it is to learn any general function in $\mathcal{F}_q$. We also define a noisy model for the online learning of smooth functions, where the learner may receive incorrect feedback up to $\eta \ge 1$ times, denoting the worst-case error bound as $\operatorname{opt}^{\text{nf}}_{p, \eta} (\mathcal{F}_q)$. We prove that $\operatorname{opt}^{\text{nf}}_{p, \eta} (\mathcal{F}_q)$ is finite if and only if $\operatorname{opt}_p(\mathcal{F}_q)$ is. Moreover, we prove for all $p, q \ge 2$ and $\eta \ge 1$ that $\operatorname{opt}^{\text{nf}}_{p, \eta} (\mathcal{F}_q) = \Theta (\eta)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16388v1</guid>
      <category>cs.LG</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weian Xie</dc:creator>
    </item>
    <item>
      <title>Improved Margin Generalization Bounds for Voting Classifiers</title>
      <link>https://arxiv.org/abs/2502.16462</link>
      <description>arXiv:2502.16462v1 Announce Type: cross 
Abstract: In this paper we establish a new margin-based generalization bound for voting classifiers, refining existing results and yielding tighter generalization guarantees for widely used boosting algorithms such as AdaBoost (Freund and Schapire, 1997). Furthermore, the new margin-based generalization bound enables the derivation of an optimal weak-to-strong learner: a Majority-of-3 large-margin classifiers with an expected error matching the theoretical lower bound. This result provides a more natural alternative to the Majority-of-5 algorithm by (H\o gsgaard et al. 2024) , and matches the Majority-of-3 result by (Aden-Ali et al. 2024) for the realizable prediction model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16462v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mikael M{\o}ller H{\o}gsgaard, Kasper Green Larsen</dc:creator>
    </item>
    <item>
      <title>Efficiency in the Roommates Problem</title>
      <link>https://arxiv.org/abs/2502.16960</link>
      <description>arXiv:2502.16960v1 Announce Type: cross 
Abstract: We propose an $O(n^2)$-time algorithm to determine whether a given matching is efficient in the roommates problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16960v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>econ.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keita Kuwahara</dc:creator>
    </item>
    <item>
      <title>From Chinese Postman to Salesman and Beyond I: Approximating Shortest Tours $\delta$-Covering All Points on All Edges</title>
      <link>https://arxiv.org/abs/2410.10613</link>
      <description>arXiv:2410.10613v2 Announce Type: replace 
Abstract: A well-studied continuous model of graphs, introduced by Dearing and Francis [Transportation Science, 1974], considers each edge as a continuous unit-length interval of points. For $\delta \geq 0$, we introduce the problem $\delta$-Tour, where the objective is to find the shortest tour that comes within a distance of $\delta$ of every point on every edge. It can be observed that 0-Tour is essentially equivalent to the Chinese Postman Problem, which is solvable in polynomial time. In contrast, 1/2-Tour is essentially equivalent to the Graphic Traveling Salesman Problem (TSP), which is NP-hard but admits a constant-factor approximation in polynomial time. We investigate $\delta$-Tour for other values of $\delta$, noting that the problem's behavior and the insights required to understand it differ significantly across various $\delta$ regimes. We design polynomial-time approximation algorithms summarized as follows:
  (1) For every fixed $0 &lt; \delta &lt; 3/2$, the problem $\delta$-Tour admits a constant-factor approximation.
  (2) For every fixed $\delta \geq 3/2$, the problem admits an $O(\log{n})$-approximation.
  (3) If $\delta$ is considered to be part of the input, then the problem admits an $O(\log^3{n})$-approximation.
  This is the first of two articles on the $\delta$-Tour problem. In the second one we complement the approximation algorithms presented here with inapproximability results and related to parameterized complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10613v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabian Frei, Ahmed Ghazy, Tim A. Hartmann, Florian H\"orsch, D\'aniel Marx</dc:creator>
    </item>
    <item>
      <title>Complexity of Paired Domination Problems on Circle and $k$-Polygon Graphs</title>
      <link>https://arxiv.org/abs/2411.19473</link>
      <description>arXiv:2411.19473v5 Announce Type: replace 
Abstract: A set $D \subseteq V$ is a dominating set of a graph $G$ if every vertex in $V - D$ is adjacent to at least one vertex in $D$. A dominating set $D$ is a paired-dominating set if the subgraph of $G$ induced by $D$ contains a perfect matching. In this paper, we prove that determining the minimum paired-dominating set in circle graphs is NP-complete. We further present an $O(n(\frac{n}{k^2-k})^{2k^2-2k})$-time algorithm for finding the minimum paired-dominating set in $k$-polygon graphs, a subclass of circle graphs. Additionally, we refine the existing algorithm of Elmallah and Stewart for computing the minimum dominating set in $k$-polygon graphs, reducing its time complexity from $O(n^{4k^2+3})$ to $O(n^{3k-5})$, and further extend it to find the minimum total dominating set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19473v5</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>math.CO</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ta-Yu Mu, Ching-Chi Lin</dc:creator>
    </item>
    <item>
      <title>Optimal Distributed Replacement Paths</title>
      <link>https://arxiv.org/abs/2502.15378</link>
      <description>arXiv:2502.15378v2 Announce Type: replace 
Abstract: We study the replacement paths problem in the $\mathsf{CONGEST}$ model of distributed computing. Given an $s$-$t$ shortest path $P$, the goal is to compute, for every edge $e$ in $P$, the shortest-path distance from $s$ to $t$ avoiding $e$. For unweighted directed graphs, we establish the tight randomized round complexity bound for this problem as $\widetilde{\Theta}(n^{2/3} + D)$ by showing matching upper and lower bounds. Our upper bound extends to $(1+\epsilon)$-approximation for weighted directed graphs. Our lower bound applies even to the second simple shortest path problem, which asks only for the smallest replacement path length. These results improve upon the very recent work of Manoharan and Ramachandran (SIROCCO 2024), who showed a lower bound of $\widetilde{\Omega}(n^{1/2} + D)$ and an upper bound of $\widetilde{O}(n^{2/3} + \sqrt{n h_{st}} + D)$, where $h_{st}$ is the number of hops in the given $s$-$t$ shortest path $P$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15378v2</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi-Jun Chang, Yanyu Chen, Dipan Dey, Gopinath Mishra, Hung Thuan Nguyen, Bryce Sanchez</dc:creator>
    </item>
    <item>
      <title>Hamiltonian Property Testing</title>
      <link>https://arxiv.org/abs/2403.02968</link>
      <description>arXiv:2403.02968v3 Announce Type: replace-cross 
Abstract: Locality is a fundamental feature of many physical time evolutions. Assumptions on locality and related structural properties also underlie recently proposed procedures for learning an unknown Hamiltonian from access to the induced time evolution. However, no protocols to rigorously test whether an unknown Hamiltonian is local were known. We investigate Hamiltonian locality testing as a property testing problem, where the task is to determine whether an unknown $n$-qubit Hamiltonian $H$ is $k$-local or $\varepsilon$-far from all $k$-local Hamiltonians, given access to the time evolution along $H$. First, we emphasize the importance of the chosen distance measure: With respect to the operator norm, a worst-case distance measure, incoherent quantum locality testers require $\tilde{\Omega}(2^n)$ many time evolution queries and an expected total evolution time of $\tilde{\Omega}(2^n / \varepsilon)$, and even coherent testers need $\Omega(2^{n/2})$ many queries and $\Omega(2^{n/2}/\varepsilon)$ total evolution time. In contrast, when distances are measured according to the normalized Frobenius norm, corresponding to an average-case distance, we give a sample-, time-, and computationally efficient incoherent Hamiltonian locality testing algorithm based on randomized measurements. In fact, our procedure can be used to simultaneously test a wide class of Hamiltonian properties beyond locality. Finally, we prove that learning a general Hamiltonian remains exponentially hard with this average-case distance, thereby establishing an exponential separation between Hamiltonian testing and learning. Our work initiates the study of property testing for quantum Hamiltonians, demonstrating that a broad class of Hamiltonian properties is efficiently testable even with limited quantum capabilities, and positioning Hamiltonian testing as an independent area of research alongside Hamiltonian learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02968v3</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Bluhm, Matthias C. Caro, Aadil Oufkir</dc:creator>
    </item>
    <item>
      <title>High-Temperature Gibbs States are Unentangled and Efficiently Preparable</title>
      <link>https://arxiv.org/abs/2403.16850</link>
      <description>arXiv:2403.16850v2 Announce Type: replace-cross 
Abstract: We show that thermal states of local Hamiltonians are separable above a constant temperature. Specifically, for a local Hamiltonian $H$ on a graph with degree $\mathfrak{d}$, its Gibbs state at inverse temperature $\beta$, denoted by $\rho = e^{-\beta H}/ \operatorname{tr}(e^{-\beta H})$, is a classical distribution over product states for all $\beta &lt; 1/(c\mathfrak{d})$, where $c$ is a constant. This proof of sudden death of thermal entanglement resolves the fundamental question of whether many-body systems can exhibit entanglement at high temperature.
  Moreover, we show that we can efficiently sample from the distribution over product states. In particular, for any $\beta &lt; 1/( c \mathfrak{d}^2)$, we can prepare a state $\varepsilon$-close to $\rho$ in trace distance with a depth-one quantum circuit and $\operatorname{poly}(n, 1/\varepsilon)$ classical overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16850v2</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ainesh Bakshi, Allen Liu, Ankur Moitra, Ewin Tang</dc:creator>
    </item>
    <item>
      <title>Metric Distortion of Line-up Elections: The Right Person for the Right Job</title>
      <link>https://arxiv.org/abs/2405.04020</link>
      <description>arXiv:2405.04020v2 Announce Type: replace-cross 
Abstract: We provide mechanisms and new metric distortion bounds for line-up elections. In such elections, a set of $n$ voters, $m$ candidates, and $\ell$ positions are all located in a metric space. The goal is to choose a set of candidates and assign them to different positions, so as to minimize the total cost of the voters. The cost of each voter consists of the distances from itself to the chosen candidates (measuring how much the voter likes the chosen candidates, or how similar it is to them), as well as the distances from the candidates to the positions they are assigned to (measuring the fitness of the candidates for their positions). Our mechanisms, however, do not know the exact distances, and instead produce good outcomes while only using a smaller amount of information, resulting in small distortion.
  We consider several different types of information: ordinal voter preferences, ordinal position preferences, and knowing the exact locations of candidates and positions, but not those of voters. In each of these cases, we provide constant distortion bounds, thus showing that only a small amount of information is enough to form outcomes close to optimum in line-up elections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04020v2</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher Jerrett, Yue Han, Elliot Anshelevich</dc:creator>
    </item>
    <item>
      <title>Private Online Learning via Lazy Algorithms</title>
      <link>https://arxiv.org/abs/2406.03620</link>
      <description>arXiv:2406.03620v2 Announce Type: replace-cross 
Abstract: We study the problem of private online learning, specifically, online prediction from experts (OPE) and online convex optimization (OCO). We propose a new transformation that transforms lazy online learning algorithms into private algorithms. We apply our transformation for differentially private OPE and OCO using existing lazy algorithms for these problems. Our final algorithms obtain regret, which significantly improves the regret in the high privacy regime $\varepsilon \ll 1$, obtaining $\sqrt{T \log d} + T^{1/3} \log(d)/\varepsilon^{2/3}$ for DP-OPE and $\sqrt{T} + T^{1/3} \sqrt{d}/\varepsilon^{2/3}$ for DP-OCO. We also complement our results with a lower bound for DP-OPE, showing that these rates are optimal for a natural family of low-switching private algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03620v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hilal Asi, Tomer Koren, Daogao Liu, Kunal Talwar</dc:creator>
    </item>
    <item>
      <title>Communication-efficient Vertical Federated Learning via Compressed Error Feedback</title>
      <link>https://arxiv.org/abs/2406.14420</link>
      <description>arXiv:2406.14420v3 Announce Type: replace-cross 
Abstract: Communication overhead is a known bottleneck in federated learning (FL). To address this, lossy compression is commonly used on the information communicated between the server and clients during training. In horizontal FL, where each client holds a subset of the samples, such communication-compressed training methods have recently seen significant progress. However, in their vertical FL counterparts, where each client holds a subset of the features, our understanding remains limited. To address this, we propose an error feedback compressed vertical federated learning (EF-VFL) method to train split neural networks. In contrast to previous communication-compressed methods for vertical FL, EF-VFL does not require a vanishing compression error for the gradient norm to converge to zero for smooth nonconvex problems. By leveraging error feedback, our method can achieve a $\mathcal{O}(1/T)$ convergence rate for a sufficiently large batch size, improving over the state-of-the-art $\mathcal{O}(1/\sqrt{T})$ rate under $\mathcal{O}(1/\sqrt{T})$ compression error, and matching the rate of uncompressed methods. Further, when the objective function satisfies the Polyak-{\L}ojasiewicz inequality, our method converges linearly. In addition to improving convergence, our method also supports the use of private labels. Numerical experiments show that EF-VFL significantly improves over the prior art, confirming our theoretical results. The code for this work can be found at https://github.com/Valdeira/EF-VFL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14420v3</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pedro Valdeira, Jo\~ao Xavier, Cl\'audia Soares, Yuejie Chi</dc:creator>
    </item>
    <item>
      <title>Characterization of Circular-arc Graphs: II. McConnell Flipping</title>
      <link>https://arxiv.org/abs/2408.10892</link>
      <description>arXiv:2408.10892v2 Announce Type: replace-cross 
Abstract: McConnell [FOCS 2001] presented a flipping transformation from circular-arc graphs to interval graphs with certain patterns of representations. Beyond its algorithmic implications, this transformation is instrumental in identifying all minimal graphs that are not circular-arc graphs. We conduct a structural study of this transformation, and for $C_{4}$-free graphs, we achieve a complete characterization of these patterns. This characterization allows us, among other things, to identify all minimal chordal graphs that are not circular-arc graphs in a companion paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10892v2</guid>
      <category>math.CO</category>
      <category>cs.DS</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yixin Cao, Tomasz Krawczyk</dc:creator>
    </item>
    <item>
      <title>Quantum speedups in solving near-symmetric optimization problems by low-depth QAOA</title>
      <link>https://arxiv.org/abs/2411.04979</link>
      <description>arXiv:2411.04979v2 Announce Type: replace-cross 
Abstract: We present new advances towards achieving exponential quantum speedups for solving optimization problems by low-depth quantum algorithms. Specifically, we focus on families of combinatorial optimization problems that exhibit symmetry and contain planted solutions. We rigorously prove that the 1-step Quantum Approximate Optimization Algorithm (QAOA) can achieve a success probability of $\Omega(1/\sqrt{n})$, and sometimes $\Omega(1)$, for finding the exact solution in many cases. This allows us to prove a separation of $O(1)$ quantum queries and $\Omega(n/\log n)$ classical queries required to find the planted solution in the latter setting. Furthermore, we construct near-symmetric optimization problems by randomly sampling the individual clauses of symmetric problems, and prove that the QAOA maintains a strong success probability in this setting even when the symmetry is broken. Finally, we construct various families of near-symmetric Max-SAT problems and benchmark state-of-the-art classical solvers, discovering instances where all known general-purpose classical algorithms require exponential time. Therefore, our results indicate that low-depth QAOA may achieve an exponential quantum speedup for optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04979v2</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ashley Montanaro, Leo Zhou</dc:creator>
    </item>
    <item>
      <title>Hyperplanes Avoiding Problem and Integer Points Counting in Polyhedra</title>
      <link>https://arxiv.org/abs/2411.07030</link>
      <description>arXiv:2411.07030v2 Announce Type: replace-cross 
Abstract: In our work, we consider the problem of computing a vector $x \in Z^n$ of minimum $\|\cdot\|_p$-norm such that $a^\top x \not= a_0$, for any vector $(a,a_0)$ from a given subset of $Z^n$ of size $m$. In other words, we search for a vector of minimum norm that avoids a given finite set of hyperplanes, which is natural to call as the $\textit{Hyperplanes Avoiding Problem}$. This problem naturally appears as a subproblem in Barvinok-type algorithms for counting integer points in polyhedra. We show that:
  1) With respect to $\|\cdot\|_1$, the problem admits a feasible solution $x$ with $\|x\|_1 \leq (m+n)/2$, and show that such solution can be constructed by a deterministic polynomial-time algorithm with $O(n \cdot m)$ operations. Moreover, this inequality is the best possible. This is a significant improvement over the previous randomized algorithm, which computes $x$ with a guaranty $\|x\|_{1} \leq n \cdot m$. The original approach of A.~Barvinok can guarantee only $\|x\|_1 = O\bigl((n \cdot m)^n\bigr)$. To prove this result, we use a newly established algorithmic variant of the Combinatorial Nullstellensatz;
  2) The problem is NP-hard with respect to any norm $\|\cdot\|_p$, for $p \in \bigl(R_{\geq 1} \cup \{\infty\}\bigr)$.
  3) As an application, we show that the problem to count integer points in a polytope $P = \{x \in R^n \colon A x \leq b\}$, for given $A \in Z^{m \times n}$ and $b \in Q^m$, can be solved by an algorithm with $O\bigl(\nu^2 \cdot n^3 \cdot \Delta^3 \bigr)$ operations, where $\nu$ is the maximum size of a normal fan triangulation of $P$, and $\Delta$ is the maximum value of rank-order subdeterminants of $A$. As a further application, it provides a refined complexity bound for the counting problem in polyhedra of bounded codimension. For example, in the polyhedra of the Unbounded Subset-Sum problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07030v2</guid>
      <category>cs.CC</category>
      <category>cs.CG</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Grigorii Dakhno, Dmitry Gribanov, Nikita Kasianov, Anastasiia Kats, Andrey Kupavskii, Nikita Kuz'min</dc:creator>
    </item>
    <item>
      <title>Weighted Envy Freeness With Bounded Subsidies</title>
      <link>https://arxiv.org/abs/2411.12696</link>
      <description>arXiv:2411.12696v4 Announce Type: replace-cross 
Abstract: We explore solutions for fairly allocating indivisible items among agents assigned weights representing their entitlements. Our fairness goal is weighted-envy-freeness (WEF), where each agent deems their allocated portion relative to their entitlement at least as favorable as any other's relative to their own. In many cases, achieving WEF necessitates monetary transfers, which can be modeled as third-party subsidies. The goal is to attain WEF with bounded subsidies.
  Previous work in the unweighted setting of subsidies relied on basic characterizations of EF that fail in the weighted settings. This makes our new setting challenging and theoretically intriguing. We present polynomial-time algorithms that compute WEF-able allocations with an upper bound on the subsidy per agent in three distinct additive valuation scenarios: (1) general, (2) identical, and (3) binary. When all weights are equal, our bounds reduce to the bounds derived in the literature for the unweighted setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12696v4</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noga Klein Elmalem, Rica Gonen, Erel Segal-Halevi</dc:creator>
    </item>
  </channel>
</rss>
