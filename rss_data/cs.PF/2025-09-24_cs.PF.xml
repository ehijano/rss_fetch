<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Sep 2025 04:01:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Are We Scaling the Right Thing? A System Perspective on Test-Time Scaling</title>
      <link>https://arxiv.org/abs/2509.19645</link>
      <description>arXiv:2509.19645v1 Announce Type: new 
Abstract: Test-time scaling (TTS) has recently emerged as a promising direction to exploit the hidden reasoning capabilities of pre-trained large language models (LLMs). However, existing scaling methods narrowly focus on the compute-optimal Pareto-frontier, ignoring the simple fact that compute-optimal is not always system-optimal. In this work, we propose a system-driven perspective on TTS, analyzing how reasoning models scale against practical metrics, such as latency and cost-per-token. By evaluating the impact of popular optimizations such as tensor parallelism and speculative decoding, our preliminary analysis reveals the limitations of current methods and calls for a paradigm shift toward holistic, system-aware evaluations that capture the true essence of scaling laws at inference time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19645v1</guid>
      <category>cs.PF</category>
      <category>cs.AI</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Youpeng Zhao, Jinpeng LV, Di Wu, Jun Wang, Christopher Gooley</dc:creator>
    </item>
    <item>
      <title>Impact of RHIs and ipSIC on Active RIS-NOMA Systems with Low-Precision ADCs</title>
      <link>https://arxiv.org/abs/2509.19383</link>
      <description>arXiv:2509.19383v1 Announce Type: cross 
Abstract: This study evaluates the performance of an active reconfigurable intelligent surface (ARIS)-assisted non-orthogonal multiple access (NOMA) system employing low-precision analog-to-digital converters (ADCs). Analytical approximations for the outage probability (OP) are derived, considering residual hardware impairments (RHIs) and imperfect successive interference cancellation (ipSIC). Additionally, we analyze the asymptotic OP, system throughput, and diversity order at high signal-to-noise ratios (SNRs). Simulation results demonstrate that the proposed quantized ARIS-NOMA system outperforms its passive counterpart (PRIS-NOMA), achieving lower OP and higher throughput with reduced transmit power requirements and fewer reflecting elements. Moreover, the outage performance of both quantized ARIS-NOMA and PRIS-NOMA systems demonstrates significant improvement as the number of reflecting elements increases. The negative impacts of low-precision ADCs can be effectively mitigated by optimizing transmit power and scaling the number of reflecting elements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19383v1</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>cs.PF</category>
      <category>math.IT</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qianqian Li, Hua Li, Shiya Hao, Lintao Li, Xiaoming Dai</dc:creator>
    </item>
    <item>
      <title>HPL-MxP Benchmark: Mixed-Precision Algorithms, Iterative Refinement, and Scalable Data Generation</title>
      <link>https://arxiv.org/abs/2509.19618</link>
      <description>arXiv:2509.19618v1 Announce Type: cross 
Abstract: We present a mixed-precision benchmark called HPL-MxP that uses both a lower-precision LU factorization with a non-stationary iterative refinement based on GMRES. We evaluate the numerical stability of one of the methods of generating the input matrix in a scalable fashion and show how the diagonal scaling affects the solution quality in terms of the backward-error. Some of the performance results at large scale supercomputing installations produced Exascale-level compute throughput numbers thus proving the viability of the proposed benchmark for evaluating such machines. We also present the potential of the benchmark to continue increasing its use with proliferation of hardware accelerators for AI workloads whose reliable evaluation continues to pose a particular challenge for the users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19618v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>cs.PF</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1177/10943420251382476</arxiv:DOI>
      <arxiv:journal_reference>International Journal of High Performance Computing Applications, 2025</arxiv:journal_reference>
      <dc:creator>Jack Dongarra, Piotr Luszczek</dc:creator>
    </item>
    <item>
      <title>Characterizing Adaptive Mesh Refinement on Heterogeneous Platforms with Parthenon-VIBE</title>
      <link>https://arxiv.org/abs/2509.19701</link>
      <description>arXiv:2509.19701v1 Announce Type: cross 
Abstract: Hero-class HPC simulations rely on Adaptive Mesh Refinement (AMR) to reduce compute and memory demands while maintaining accuracy. This work analyzes the performance of Parthenon, a block-structured AMR benchmark, on CPU-GPU systems. We show that smaller mesh blocks and deeper AMR levels degrade GPU performance due to increased communication, serial overheads, and inefficient GPU utilization. Through detailed profiling, we identify inefficiencies, low occupancy, and memory access bottlenecks. We further analyze rank scalability and memory constraints, and propose optimizations to improve GPU throughput and reduce memory footprint. Our insights can inform future AMR deployments on Department of Energy's upcoming heterogeneous supercomputers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19701v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akash Poptani, Alireza Khadem, Scott Mahlke, Jonah Miller, Joshua Dolence, Reetuparna Das</dc:creator>
    </item>
    <item>
      <title>LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale Architectures</title>
      <link>https://arxiv.org/abs/2508.13523</link>
      <description>arXiv:2508.13523v2 Announce Type: replace-cross 
Abstract: Since its inception in 1995, LAMMPS has grown to be a world-class molecular dynamics code, with thousands of users, over one million lines of code, and multi-scale simulation capabilities. We discuss how LAMMPS has adapted to the modern heterogeneous computing landscape by integrating the Kokkos performance portability library into the existing C++ code. We investigate performance portability of simple pairwise, many-body reactive, and machine-learned force-field interatomic potentials. We present results on GPUs across different vendors and generations, and analyze performance trends, probing FLOPS throughput, memory bandwidths, cache capabilities, and thread-atomic operation performance. Finally, we demonstrate strong scaling on three exascale machines -- OLCF Frontier, ALCF Aurora, and NNSA El Capitan -- as well as on the CSCS Alps supercomputer, for the three potentials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13523v2</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <category>physics.comp-ph</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3731599.3767498</arxiv:DOI>
      <dc:creator>Anders Johansson, Evan Weinberg, Christian R. Trott, Megan J. McCarthy, Stan G. Moore</dc:creator>
    </item>
  </channel>
</rss>
