<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 16 Apr 2024 04:01:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Multiply-Robust Causal Change Attribution</title>
      <link>https://arxiv.org/abs/2404.08839</link>
      <description>arXiv:2404.08839v1 Announce Type: new 
Abstract: Comparing two samples of data, we observe a change in the distribution of an outcome variable. In the presence of multiple explanatory variables, how much of the change can be explained by each possible cause? We develop a new estimation strategy that, given a causal model, combines regression and re-weighting methods to quantify the contribution of each causal mechanism. Our proposed methodology is multiply robust, meaning that it still recovers the target parameter under partial misspecification. We prove that our estimator is consistent and asymptotically normal. Moreover, it can be incorporated into existing frameworks for causal attribution, such as Shapley values, which will inherit the consistency and large-sample distribution properties. Our method demonstrates excellent performance in Monte Carlo simulations, and we show its usefulness in an empirical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08839v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victor Quintas-Martinez, Mohammad Taha Bahadori, Eduardo Santiago, Jeff Mu, Dominik Janzing, David Heckerman</dc:creator>
    </item>
    <item>
      <title>Projection matrices and the sweep operator</title>
      <link>https://arxiv.org/abs/2404.08883</link>
      <description>arXiv:2404.08883v1 Announce Type: new 
Abstract: These notes have been adapted from an undergraduate course given by Professor Alan James at the University of Adelaide from around 1965 and onwards. This adaption has put a focus on the definition of projection matrices and the sweep operator. These devices were at the heart of the development of the statistical package Genstat which initially focussed on the analysis of variance using the sweep operator. The notes provide an algebraic background to the sweep operator which has since been used to effect in a number of experimental design settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08883v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>A. T. James, E. R. Williams</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Genomic Data with Multiple Heterogeneous Outcomes</title>
      <link>https://arxiv.org/abs/2404.09119</link>
      <description>arXiv:2404.09119v1 Announce Type: new 
Abstract: With the evolution of single-cell RNA sequencing techniques into a standard approach in genomics, it has become possible to conduct cohort-level causal inferences based on single-cell-level measurements. However, the individual gene expression levels of interest are not directly observable; instead, only repeated proxy measurements from each individual's cells are available, providing a derived outcome to estimate the underlying outcome for each of many genes. In this paper, we propose a generic semiparametric inference framework for doubly robust estimation with multiple derived outcomes, which also encompasses the usual setting of multiple outcomes when the response of each unit is available. To reliably quantify the causal effects of heterogeneous outcomes, we specialize the analysis to the standardized average treatment effects and the quantile treatment effects. Through this, we demonstrate the use of the semiparametric inferential results for doubly robust estimators derived from both Von Mises expansions and estimating equations. A multiple testing procedure based on the Gaussian multiplier bootstrap is tailored for doubly robust estimators to control the false discovery exceedance rate. Applications in single-cell CRISPR perturbation analysis and individual-level differential expression analysis demonstrate the utility of the proposed methods and offer insights into the usage of different estimands for causal inference in genomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09119v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin-Hong Du, Zhenghao Zeng, Edward H. Kennedy, Larry Wasserman, Kathryn Roeder</dc:creator>
    </item>
    <item>
      <title>Treatment Effect Heterogeneity and Importance Measures for Multivariate Continuous Treatments</title>
      <link>https://arxiv.org/abs/2404.09126</link>
      <description>arXiv:2404.09126v1 Announce Type: new 
Abstract: Estimating the joint effect of a multivariate, continuous exposure is crucial, particularly in environmental health where interest lies in simultaneously evaluating the impact of multiple environmental pollutants on health. We develop novel methodology that addresses two key issues for estimation of treatment effects of multivariate, continuous exposures. We use nonparametric Bayesian methodology that is flexible to ensure our approach can capture a wide range of data generating processes. Additionally, we allow the effect of the exposures to be heterogeneous with respect to covariates. Treatment effect heterogeneity has not been well explored in the causal inference literature for multivariate, continuous exposures, and therefore we introduce novel estimands that summarize the nature and extent of the heterogeneity, and propose estimation procedures for new estimands related to treatment effect heterogeneity. We provide theoretical support for the proposed models in the form of posterior contraction rates and show that it works well in simulated examples both with and without heterogeneity. We apply our approach to a study of the health effects of simultaneous exposure to the components of PM$_{2.5}$ and find that the negative health effects of exposure to these environmental pollutants is exacerbated by low socioeconomic status and age.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09126v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heejun Shin, Antonio Linero, Michelle Audirac, Kezia Irene, Danielle Braun, Joseph Antonelli</dc:creator>
    </item>
    <item>
      <title>Extreme quantile regression with deep learning</title>
      <link>https://arxiv.org/abs/2404.09154</link>
      <description>arXiv:2404.09154v1 Announce Type: new 
Abstract: Estimation of extreme conditional quantiles is often required for risk assessment of natural hazards in climate and geo-environmental sciences and for quantitative risk management in statistical finance, econometrics, and actuarial sciences. Interest often lies in extrapolating to quantile levels that exceed any past observations. Therefore, it is crucial to use a statistical framework that is well-adapted and especially designed for this purpose, and here extreme-value theory plays a key role. This chapter reviews how extreme quantile regression may be performed using theoretically-justified models, and how modern deep learning approaches can be harnessed in this context to enhance the model's performance in complex high-dimensional settings. The power of deep learning combined with the rigor of theoretically-justified extreme-value methods opens the door to efficient extreme quantile regression, in cases where both the number of covariates and the quantile level of interest can be simultaneously ``extreme''.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09154v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jordan Richards, Rapha\"el Huser</dc:creator>
    </item>
    <item>
      <title>Bayesian modeling of co-occurrence microbial interaction networks</title>
      <link>https://arxiv.org/abs/2404.09194</link>
      <description>arXiv:2404.09194v1 Announce Type: new 
Abstract: The human body consists of microbiomes associated with the development and prevention of several diseases. These microbial organisms form several complex interactions that are informative to the scientific community for explaining disease progression and prevention. Contrary to the traditional view of the microbiome as a singular, assortative network, we introduce a novel statistical approach using a weighted stochastic infinite block model to analyze the complex community structures within microbial co-occurrence microbial interaction networks. Our model defines connections between microbial taxa using a novel semi-parametric rank-based correlation method on their transformed relative abundances within a fully connected network framework. Employing a Bayesian nonparametric approach, the proposed model effectively clusters taxa into distinct communities while estimating the number of communities. The posterior summary of the taxa community membership is obtained based on the posterior probability matrix, which could naturally solve the label switching problem. Through simulation studies and real-world application to microbiome data from postmenopausal patients with recurrent urinary tract infections, we demonstrate that our method has superior clustering accuracy over alternative approaches. This advancement provides a more nuanced understanding of microbiome organization, with significant implications for disease research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09194v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tejasv Bedi, Bencong Zhu, Michael L. Neugent, Kevin C. Lutz, Nicole J. De Nisco, Qiwei Li</dc:creator>
    </item>
    <item>
      <title>A Unified Combination Framework for Dependent Tests with Applications to Microbiome Association Studies</title>
      <link>https://arxiv.org/abs/2404.09353</link>
      <description>arXiv:2404.09353v1 Announce Type: new 
Abstract: We introduce a novel meta-analysis framework to combine dependent tests under a general setting, and utilize it to synthesize various microbiome association tests that are calculated from the same dataset. Our development builds upon the classical meta-analysis methods of aggregating $p$-values and also a more recent general method of combining confidence distributions, but makes generalizations to handle dependent tests. The proposed framework ensures rigorous statistical guarantees, and we provide a comprehensive study and compare it with various existing dependent combination methods. Notably, we demonstrate that the widely used Cauchy combination method for dependent tests, referred to as the vanilla Cauchy combination in this article, can be viewed as a special case within our framework. Moreover, the proposed framework provides a way to address the problem when the distributional assumptions underlying the vanilla Cauchy combination are violated. Our numerical results demonstrate that ignoring the dependence among the to-be-combined components may lead to a severe size distortion phenomenon. Compared to the existing $p$-value combination methods, including the vanilla Cauchy combination method, the proposed combination framework can handle the dependence accurately and utilizes the information efficiently to construct tests with accurate size and enhanced power. The development is applied to Microbiome Association Studies, where we aggregate information from multiple existing tests using the same dataset. The combined tests harness the strengths of each individual test across a wide range of alternative spaces, %resulting in a significant enhancement of testing power across a wide range of alternative spaces, enabling more efficient and meaningful discoveries of vital microbiome associations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09353v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiufan Yu, Linjun Zhang, Arun Srinivasan, Min-ge Xie, Lingzhou Xue</dc:creator>
    </item>
    <item>
      <title>Two-stage Spatial Regression Models for Spatial Confounding</title>
      <link>https://arxiv.org/abs/2404.09358</link>
      <description>arXiv:2404.09358v1 Announce Type: new 
Abstract: Public health data are often spatially dependent, but standard spatial regression methods can suffer from bias and invalid inference when the independent variable is associated with spatially-correlated residuals. This could occur if, for example, there is an unmeasured environmental contaminant. Geoadditive structural equation modeling (gSEM), in which an estimated spatial trend is removed from both the explanatory and response variables before estimating the parameters of interest, has previously been proposed as a solution, but there has been little investigation of gSEM's properties with point-referenced data. We link gSEM to results on double machine learning and semiparametric regression based on two-stage procedures. We propose using these semiparametric estimators for spatial regression using Gaussian processes with Mat\`ern covariance to estimate the spatial trends, and term this class of estimators Double Spatial Regression (DSR). We derive regularity conditions for root-$n$ asymptotic normality and consistency and closed-form variance estimation, and show that in simulations where standard spatial regression estimators are highly biased and have poor coverage, DSR can mitigate bias more effectively than competitors and obtain nominal coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09358v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nate Wiecha, Brian J. Reich</dc:creator>
    </item>
    <item>
      <title>A Bayesian Joint Modelling for Misclassified Interval-censoring and Competing Risks</title>
      <link>https://arxiv.org/abs/2404.09362</link>
      <description>arXiv:2404.09362v1 Announce Type: new 
Abstract: In active surveillance of prostate cancer, cancer progression is interval-censored and the examination to detect progression is subject to misclassification, usually false negatives. Meanwhile, patients may initiate early treatment before progression detection, constituting a competing risk. We developed the Misclassification-Corrected Interval-censored Cause-specific Joint Model (MCICJM) to estimate the association between longitudinal biomarkers and cancer progression in this setting. The sensitivity of the examination is considered in the likelihood of this model via a parameter that may be set to a specific value if the sensitivity is known, or for which a prior distribution can be specified if the sensitivity is unknown. Our simulation results show that misspecification of the sensitivity parameter or ignoring it entirely impacts the model parameters, especially the parameter uncertainty and the baseline hazards. Moreover, specification of a prior distribution for the sensitivity parameter may reduce the risk of misspecification in settings where the exact sensitivity is unknown, but may cause identifiability issues. Thus, imposing restrictions on the baseline hazards is recommended. A trade-off between modelling with a sensitivity constant at the risk of misspecification and a sensitivity prior at the cost of flexibility needs to be decided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09362v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhenwei Yang, Dimitris Rizopoulos, Eveline A. M. Heijnsdijk, Lisa F. Newcomb, Nicole S. Erler</dc:creator>
    </item>
    <item>
      <title>General Bayesian inference for causal effects using covariate balancing procedure</title>
      <link>https://arxiv.org/abs/2404.09414</link>
      <description>arXiv:2404.09414v1 Announce Type: new 
Abstract: In observational studies, the propensity score plays a central role in estimating causal effects of interest. The inverse probability weighting (IPW) estimator is especially commonly used. However, if the propensity score model is misspecified, the IPW estimator may produce biased estimates of causal effects. Previous studies have proposed some robust propensity score estimation procedures; these methods, however, require consideration of parameters that dominate the uncertainty of sampling and treatment allocation. In this manuscript, we propose a novel Bayesian estimating procedure that necessitates deciding the parameter probability, rather than deterministically. Since both the IPW estimator and the propensity score estimator can be derived as solutions to certain loss functions, the general Bayesian paradigm, which does not require the consideration of the full likelihood, can be applied. In this sense, our proposed method only requires the same level of assumptions as ordinary causal inference contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09414v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shunichiro Orihara, Tomotaka Momozaki, Tomoyuki Nakagawa</dc:creator>
    </item>
    <item>
      <title>Overfitting Reduction in Convex Regression</title>
      <link>https://arxiv.org/abs/2404.09528</link>
      <description>arXiv:2404.09528v1 Announce Type: new 
Abstract: Convex regression is a method for estimating an unknown function $f_0$ from a data set of $n$ noisy observations when $f_0$ is known to be convex. This method has played an important role in operations research, economics, machine learning, and many other areas. It has been empirically observed that the convex regression estimator produces inconsistent estimates of $f_0$ and extremely large subgradients near the boundary of the domain of $f_0$ as $n$ increases. In this paper, we provide theoretical evidence of this overfitting behaviour. We also prove that the penalised convex regression estimator, one of the variants of the convex regression estimator, exhibits overfitting behaviour. To eliminate this behaviour, we propose two new estimators by placing a bound on the subgradients of the estimated function. We further show that our proposed estimators do not exhibit the overfitting behaviour by proving that (a) they converge to $f_0$ and (b) their subgradients converge to the gradient of $f_0$, both uniformly over the domain of $f_0$ with probability one as $n \rightarrow \infty$. We apply the proposed methods to compute the cost frontier function for Finnish electricity distribution firms and confirm their superior performance in predictive power over some existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09528v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiqiang Liao, Sheng Dai, Eunji Lim, Timo Kuosmanen</dc:creator>
    </item>
    <item>
      <title>Optimal Cut-Point Estimation for functional digital biomarkers: Application to Continuous Glucose Monitoring</title>
      <link>https://arxiv.org/abs/2404.09716</link>
      <description>arXiv:2404.09716v1 Announce Type: new 
Abstract: Establish optimal cut points plays a crucial role in epidemiology and biomarker discovery, enabling the development of effective and practical clinical decision criteria. While there is extensive literature to define optimal cut off over scalar biomarkers, there is a notable lack of general methodologies for analyzing statistical objects in more complex spaces of functions and graphs, which are increasingly relevant in digital health applications. This paper proposes a new general methodology to define optimal cut points for random objects in separable Hilbert spaces. The paper is motivated by the need for creating new clinical rules for diabetes mellitus disease, exploiting the functional information of a continuous diabetes monitor (CGM) as a digital biomarker. More specifically, we provide the functional cut off to identify diabetes cases with CGM information based on glucose distributional functional representations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09716v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oscar Lado-Baleato, Marcos Matabuena, Carla D\'iaz-Louzao, Francisco Gude</dc:creator>
    </item>
    <item>
      <title>Biclustering bipartite networks via extended Mixture of Latent Trait Analyzers</title>
      <link>https://arxiv.org/abs/2404.09823</link>
      <description>arXiv:2404.09823v1 Announce Type: new 
Abstract: In the context of network data, bipartite networks are of particular interest, as they provide a useful description of systems representing relationships between sending and receiving nodes. In this framework, we extend the Mixture of Latent Trait Analyzers (MLTA) to perform a joint clustering of sending and receiving nodes, as in the biclustering framework. In detail, sending nodes are partitioned into clusters (called components) via a finite mixture of latent trait models. In each component, receiving nodes are partitioned into clusters (called segments) by adopting a flexible and parsimonious specification of the linear predictor. Dependence between receiving nodes is modeled via a multidimensional latent trait, as in the original MLTA specification. The proposal also allows for the inclusion of concomitant variables in the latent layer of the model, with the aim of understanding how they influence component formation. To estimate model parameters, an EM-type algorithm based on a Gauss-Hermite approximation of intractable integrals is proposed. A simulation study is conducted to test the performance of the model in terms of clustering and parameters' recovery. The proposed model is applied to a bipartite network on pediatric patients possibly affected by appendicitis with the objective of identifying groups of patients (sending nodes) being similar with respect to subsets of clinical conditions (receiving nodes).</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09823v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dalila Failli, Maria Francesca Marino, Francesca Martella</dc:creator>
    </item>
    <item>
      <title>sfislands: An R Package for Accommodating Islands and Disjoint Zones in Areal Spatial Modelling</title>
      <link>https://arxiv.org/abs/2404.09863</link>
      <description>arXiv:2404.09863v1 Announce Type: new 
Abstract: Fitting areal models which use a spatial weights matrix to represent relationships between geographical units can be a cumbersome task, particularly when these units are not well-behaved. The two chief aims of sfislands are to simplify the process of creating an appropriate neighbourhood matrix, and to quickly visualise the predictions of subsequent models. The package uses visual aids in the form of easily-generated maps to help this process. This paper demonstrates how sfislands could be useful to researchers. It begins by describing the package's functions in the context of a proposed workflow. It then presents three worked examples showing a selection of potential use-cases. These range from earthquakes in Indonesia, to river crossings in London, and hierarchical models of output areas in Liverpool. We aim to show how the sfislands package streamlines much of the human workflow involved in creating and examining such models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09863v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin Horan, Katarina Domijan, Chris Brunsdon</dc:creator>
    </item>
    <item>
      <title>A spatio-temporal model to detect potential outliers in disease mapping</title>
      <link>https://arxiv.org/abs/2404.09882</link>
      <description>arXiv:2404.09882v1 Announce Type: new 
Abstract: Spatio-temporal disease mapping models are commonly used to estimate the relative risk of a disease over time and across areas. For each area and time point, the disease count is modelled with a Poisson distribution whose mean is the product of an offset and the disease relative risk. This relative risk is commonly decomposed in the log scale as the sum of fixed and latent effects. The Rushworth model allows for spatio-temporal autocorrelation of the random effects. We build on the Rushworth model to accommodate and identify potentially outlying areas with respect to their disease relative risk evolution, after taking into account the fixed effects. An area may display outlying behaviour at some points in time but not all. At each time point, we assume the latent effects to be spatially structured and include scaling parameters in the precision matrix, to allow for heavy-tails. Two prior specifications are considered for the scaling parameters: one where they are independent across space and one with spatial autocorrelation. We investigate the performance of the different prior specifications of the proposed model through simulation studies and analyse the weekly evolution of the number of COVID-19 cases across the 33 boroughs of Montreal and the 96 French departments during the second wave. In Montreal, 6 boroughs are found to be potentially outlying. In France, the model with spatially structured scaling parameters identified 21 departments as potential outliers. We find that these departments tend to be close to each other and within common French regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09882v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victoire Michal, Alexandra M. Schmidt</dc:creator>
    </item>
    <item>
      <title>Pseudo P-values for Assessing Covariate Balance in a Finite Study Population with Application to the California Sugar Sweetened Beverage Tax Study</title>
      <link>https://arxiv.org/abs/2404.09960</link>
      <description>arXiv:2404.09960v1 Announce Type: new 
Abstract: Assessing covariate balance (CB) is a common practice in various types of evaluation studies. Two-sample descriptive statistics, such as the standardized mean difference, have been widely applied in the scientific literature to assess the goodness of CB. Studies in health policy, health services research, built and social environment research, and many other fields often involve a finite number of units that may be subject to different treatment levels. Our case study, the California Sugar Sweetened Beverage (SSB) Tax Study, include 332 study cities in the state of California, among which individual cities may elect to levy a city-wide excise tax on SSB sales. Evaluating the balance of covariates between study cities with and without the tax policy is essential for assessing the effects of the policy on health outcomes of interest. In this paper, we introduce the novel concepts of the pseudo p-value and the standardized pseudo p-value, which are descriptive statistics to assess the overall goodness of CB between study arms in a finite study population. While not meant as a hypothesis test, the pseudo p-values bear superficial similarity to the classic p-value, which makes them easy to apply and interpret in applications. We discuss some theoretical properties of the pseudo p-values and present an algorithm to calculate them. We report a numerical simulation study to demonstrate their performance. We apply the pseudo p-values to the California SSB Tax study to assess the balance of city-level characteristics between the two study arms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09960v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bing Han, Margo A. Sidell</dc:creator>
    </item>
    <item>
      <title>A fully Bayesian approach for the imputation and analysis of derived outcome variables with missingness</title>
      <link>https://arxiv.org/abs/2404.09966</link>
      <description>arXiv:2404.09966v1 Announce Type: new 
Abstract: Derived variables are variables that are constructed from one or more source variables through established mathematical operations or algorithms. For example, body mass index (BMI) is a derived variable constructed from two source variables: weight and height. When using a derived variable as the outcome in a statistical model, complications arise when some of the source variables have missing values. In this paper, we propose how one can define a single fully Bayesian model to simultaneously impute missing values and sample from the posterior. We compare our proposed method with alternative approaches that rely on multiple imputation, and, with a simulated dataset, consider how best to estimate the risk of microcephaly in newborns exposed to the ZIKA virus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09966v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harlan Campbell, Tim Morris, Paul Gustafson</dc:creator>
    </item>
    <item>
      <title>Musical Listening Qualia: A Multivariate Approach</title>
      <link>https://arxiv.org/abs/2404.08694</link>
      <description>arXiv:2404.08694v1 Announce Type: cross 
Abstract: French and American participants listened to new music stimuli and evaluated the stimuli using either adjectives or quantitative musical dimensions. Results were analyzed using correspondence analysis (CA), hierarchical cluster analysis (HCA), multiple factor analysis (MFA), and partial least squares correlation (PLSC). French and American listeners differed when they described the musical stimuli using adjectives, but not when using the quantitative dimensions. The present work serves as a case study in research methodology that allows for a balance between relaxing experimental control and maintaining statistical rigor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08694v1</guid>
      <category>cs.IR</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/25742442.2022.2036074</arxiv:DOI>
      <arxiv:journal_reference>Auditory Perception &amp; Cognition, 2022, 5 (1-2), pp.46-75.</arxiv:journal_reference>
      <dc:creator>Brendon Mizener (JUNIA), Mathilde Vandenberghe-Descamps (JUNIA), Herv\'e Abdi (JUNIA), Sylvie Chollet (JUNIA)</dc:creator>
    </item>
    <item>
      <title>Prevalence estimation methods for time-dependent antibody kinetics of infected and vaccinated individuals: a graph-theoretic approach</title>
      <link>https://arxiv.org/abs/2404.09059</link>
      <description>arXiv:2404.09059v1 Announce Type: cross 
Abstract: Immune events such as infection, vaccination, and a combination of the two result in distinct time-dependent antibody responses in affected individuals. These responses and event prevalences combine non-trivially to govern antibody levels sampled from a population. Time-dependence and disease prevalence pose considerable modeling challenges that need to be addressed to provide a rigorous mathematical underpinning of the underlying biology. We propose a time-inhomogeneous Markov chain model for event-to-event transitions coupled with a probabilistic framework for anti-body kinetics and demonstrate its use in a setting in which individuals can be infected or vaccinated but not both. We prove the equivalency of this approach to the framework developed in our previous work. Synthetic data are used to demonstrate the modeling process and conduct prevalence estimation via transition probability matrices. This approach is ideal to model sequences of infections and vaccinations, or personal trajectories in a population, making it an important first step towards a mathematical characterization of reinfection, vaccination boosting, and cross-events of infection after vaccination or vice versa.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09059v1</guid>
      <category>q-bio.PE</category>
      <category>math.PR</category>
      <category>physics.bio-ph</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prajakta Bedekar, Rayanne A. Luke, Anthony J. Kearsley</dc:creator>
    </item>
    <item>
      <title>Controlling the False Discovery Rate in Subspace Selection</title>
      <link>https://arxiv.org/abs/2404.09142</link>
      <description>arXiv:2404.09142v1 Announce Type: cross 
Abstract: Controlling the false discovery rate (FDR) is a popular approach to multiple testing, variable selection, and related problems of simultaneous inference. In many contemporary applications, models are not specified by discrete variables, which necessitates a broadening of the scope of the FDR control paradigm. Motivated by the ubiquity of low-rank models for high-dimensional matrices, we present methods for subspace selection in principal components analysis that provide control on a geometric analog of FDR that is adapted to subspace selection. Our methods crucially rely on recently-developed tools from random matrix theory, in particular on a characterization of the limiting behavior of eigenvectors and the gaps between successive eigenvalues of large random matrices. Our procedure is parameter-free, and we show that it provides FDR control in subspace selection for common noise models considered in the literature. We demonstrate the utility of our algorithm with numerical experiments on synthetic data and on problems arising in single-cell RNA sequencing and hyperspectral imaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09142v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mateo D\'iaz, Venkat Chandrasekaran</dc:creator>
    </item>
    <item>
      <title>Statistics of extremes for natural hazards: landslides and earthquakes</title>
      <link>https://arxiv.org/abs/2404.09156</link>
      <description>arXiv:2404.09156v1 Announce Type: cross 
Abstract: In this chapter, we illustrate the use of split bulk-tail models and subasymptotic models motivated by extreme-value theory in the context of hazard assessment for earthquake-induced landslides. A spatial joint areal model is presented for modeling both landslides counts and landslide sizes, paying particular attention to extreme landslides, which are the most devastating ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09156v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rishikesh Yadav, Luigi Lombardo, Rapha\"el Huser</dc:creator>
    </item>
    <item>
      <title>Statistics of Extremes for Neuroscience</title>
      <link>https://arxiv.org/abs/2404.09157</link>
      <description>arXiv:2404.09157v1 Announce Type: cross 
Abstract: This chapter illustrates how tools from univariate and multivariate statistics of extremes can complement classical methods used to study brain signals and enhance the understanding of brain activity and connectivity during specific cognitive tasks or abnormal episodes, such as an epileptic seizure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09157v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paolo V. Redondo, Matheus B. Guerrero, Rapha\"el Huser, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>Quantifying fair income distribution in Thailand</title>
      <link>https://arxiv.org/abs/2404.09629</link>
      <description>arXiv:2404.09629v1 Announce Type: cross 
Abstract: Given a vast concern about high income inequality in Thailand as opposed to empirical findings around the world showing people's preference for fair income inequality over unfair income equality, it is therefore important to examine whether inequality in income distribution in Thailand over the past three decades is fair, and what fair inequality in income distribution in Thailand should be. To quantitatively measure fair income distribution, this study employs the fairness benchmarks that are derived from the distributions of athletes' salaries in professional sports which satisfy the concepts of distributive justice and procedural justice, the no-envy principle of fair allocation, and the general consensus or the international norm criterion of a meaningful benchmark. By using the data on quintile income shares and the income Gini index of Thailand from the National Social and Economic Development Council, this study finds that, throughout the period from 1988 to 2021, the Thai income earners in the bottom 20%, the second 20%, and the top 20% receive income shares more than the fair shares whereas those in the third 20% and the fourth 20% receive income shares less than the fair shares. Provided that there are infinite combinations of quintile income shares that can have the same value of income Gini index but only one of them is regarded as fair, this study demonstrates the use of fairness benchmarks as a practical guideline for designing policies with an aim to achieve fair income distribution in Thailand. Moreover, a comparative analysis is conducted by employing the method for estimating optimal (fair) income distribution representing feasible income equality in order to provide an alternative recommendation on what optimal (fair) income distribution characterizing feasible income equality in Thailand should be.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09629v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1371/journal.pone.0301693</arxiv:DOI>
      <arxiv:journal_reference>PLoS ONE (2024)</arxiv:journal_reference>
      <dc:creator>Thitithep Sitthiyot, Kanyarat Holasut</dc:creator>
    </item>
    <item>
      <title>Amplitude-Phase Fusion for Enhanced Electrocardiogram Morphological Analysis</title>
      <link>https://arxiv.org/abs/2404.09729</link>
      <description>arXiv:2404.09729v1 Announce Type: cross 
Abstract: Considering the variability of amplitude and phase patterns in electrocardiogram (ECG) signals due to cardiac activity and individual differences, existing entropy-based studies have not fully utilized these two patterns and lack integration. To address this gap, this paper proposes a novel fusion entropy metric, morphological ECG entropy (MEE) for the first time, specifically designed for ECG morphology, to comprehensively describe the fusion of amplitude and phase patterns. MEE is computed based on beat-level samples, enabling detailed analysis of each cardiac cycle. Experimental results demonstrate that MEE achieves rapid, accurate, and label-free localization of abnormal ECG arrhythmia regions. Furthermore, MEE provides a method for assessing sample diversity, facilitating compression of imbalanced training sets (via representative sample selection), and outperforms random pruning. Additionally, MEE exhibits the ability to describe areas of poor quality. By discussing, it proves the robustness of MEE value calculation to noise interference and its low computational complexity. Finally, we integrate this method into a clinical interactive interface to provide a more convenient and intuitive user experience. These findings indicate that MEE serves as a valuable clinical descriptor for ECG characterization. The implementation code can be referenced at the following link: https://github.com/fdu-harry/ECG-MEE-metric.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09729v1</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuaicong Hu, Yanan Wang, Jian Liu, Jingyu Lin, Shengmei Qin, Zhenning Nie, Zhifeng Yao, Wenjie Cai, Cuiwei Yang</dc:creator>
    </item>
    <item>
      <title>Statistical learning for constrained functional parameters in infinite-dimensional models with applications in fair machine learning</title>
      <link>https://arxiv.org/abs/2404.09847</link>
      <description>arXiv:2404.09847v1 Announce Type: cross 
Abstract: Constrained learning has become increasingly important, especially in the realm of algorithmic fairness and machine learning. In these settings, predictive models are developed specifically to satisfy pre-defined notions of fairness. Here, we study the general problem of constrained statistical machine learning through a statistical functional lens. We consider learning a function-valued parameter of interest under the constraint that one or several pre-specified real-valued functional parameters equal zero or are otherwise bounded. We characterize the constrained functional parameter as the minimizer of a penalized risk criterion using a Lagrange multiplier formulation. We show that closed-form solutions for the optimal constrained parameter are often available, providing insight into mechanisms that drive fairness in predictive models. Our results also suggest natural estimators of the constrained parameter that can be constructed by combining estimates of unconstrained parameters of the data generating distribution. Thus, our estimation procedure for constructing fair machine learning algorithms can be applied in conjunction with any statistical learning approach and off-the-shelf software. We demonstrate the generality of our method by explicitly considering a number of examples of statistical fairness constraints and implementing the approach using several popular learning approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09847v1</guid>
      <category>stat.ML</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Razieh Nabi, Nima S. Hejazi, Mark J. van der Laan, David Benkeser</dc:creator>
    </item>
    <item>
      <title>Invariant Subspace Decomposition</title>
      <link>https://arxiv.org/abs/2404.09962</link>
      <description>arXiv:2404.09962v1 Announce Type: cross 
Abstract: We consider the task of predicting a response Y from a set of covariates X in settings where the conditional distribution of Y given X changes over time. For this to be feasible, assumptions on how the conditional distribution changes over time are required. Existing approaches assume, for example, that changes occur smoothly over time so that short-term prediction using only the recent past becomes feasible. In this work, we propose a novel invariance-based framework for linear conditionals, called Invariant Subspace Decomposition (ISD), that splits the conditional distribution into a time-invariant and a residual time-dependent component. As we show, this decomposition can be utilized both for zero-shot and time-adaptation prediction tasks, that is, settings where either no or a small amount of training data is available at the time points we want to predict Y at, respectively. We propose a practical estimation procedure, which automatically infers the decomposition using tools from approximate joint matrix diagonalization. Furthermore, we provide finite sample guarantees for the proposed estimator and demonstrate empirically that it indeed improves on approaches that do not use the additional invariant structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09962v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Margherita Lazzaretto, Jonas Peters, Niklas Pfister</dc:creator>
    </item>
    <item>
      <title>Universal Inference Meets Random Projections: A Scalable Test for Log-concavity</title>
      <link>https://arxiv.org/abs/2111.09254</link>
      <description>arXiv:2111.09254v4 Announce Type: replace 
Abstract: Shape constraints yield flexible middle grounds between fully nonparametric and fully parametric approaches to modeling distributions of data. The specific assumption of log-concavity is motivated by applications across economics, survival modeling, and reliability theory. However, there do not currently exist valid tests for whether the underlying density of given data is log-concave. The recent universal inference methodology provides a valid test. The universal test relies on maximum likelihood estimation (MLE), and efficient methods already exist for finding the log-concave MLE. This yields the first test of log-concavity that is provably valid in finite samples in any dimension, for which we also establish asymptotic consistency results. Empirically, we find that a random projections approach that converts the d-dimensional testing problem into many one-dimensional problems can yield high power, leading to a simple procedure that is statistically and computationally efficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.09254v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robin Dunn, Aditya Gangrade, Larry Wasserman, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Penalized Estimation of Frailty-Based Illness-Death Models for Semi-Competing Risks</title>
      <link>https://arxiv.org/abs/2202.00618</link>
      <description>arXiv:2202.00618v3 Announce Type: replace 
Abstract: Semi-competing risks refers to the survival analysis setting where the occurrence of a non-terminal event is subject to whether a terminal event has occurred, but not vice versa. Semi-competing risks arise in a broad range of clinical contexts, with a novel example being the pregnancy condition preeclampsia, which can only occur before the `terminal' event of giving birth. Models that acknowledge semi-competing risks enable investigation of relationships between covariates and the joint timing of the outcomes, but methods for model selection and prediction of semi-competing risks in high dimensions are lacking. Instead, researchers commonly analyze only a single or composite outcome, losing valuable information and limiting clinical utility -- in the obstetric setting, this means ignoring valuable insight into timing of delivery after preeclampsia has onset. To address this gap we propose a novel penalized estimation framework for frailty-based illness-death multi-state modeling of semi-competing risks. Our approach combines non-convex and structured fusion penalization, inducing global sparsity as well as parsimony across submodels. We perform estimation and model selection via a pathwise routine for non-convex optimization, and prove the first statistical error bound results in this setting. We present a simulation study investigating estimation error and model selection performance, and a comprehensive application of the method to joint risk modeling of preeclampsia and timing of delivery using pregnancy data from an electronic health record.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.00618v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1111/biom.13761</arxiv:DOI>
      <arxiv:journal_reference>Biometrics, 79(3), 1657-1669</arxiv:journal_reference>
      <dc:creator>Harrison T. Reeder, Junwei Lu, Sebastien Haneuse</dc:creator>
    </item>
    <item>
      <title>Using Instruments for Selection to Adjust for Selection Bias in Mendelian Randomization</title>
      <link>https://arxiv.org/abs/2208.02657</link>
      <description>arXiv:2208.02657v3 Announce Type: replace 
Abstract: Selection bias is a common concern in epidemiologic studies. In the literature, selection bias is often viewed as a missing data problem. Popular approaches to adjust for bias due to missing data, such as inverse probability weighting, rely on the assumption that data are missing at random and can yield biased results if this assumption is violated. In observational studies with outcome data missing not at random, Heckman's sample selection model can be used to adjust for bias due to missing data. In this paper, we review Heckman's method and a similar approach proposed by Tchetgen Tchetgen and Wirth (2017). We then discuss how to apply these methods to Mendelian randomization analyses using individual-level data, with missing data for either the exposure or outcome or both. We explore whether genetic variants associated with participation can be used as instruments for selection. We then describe how to obtain missingness-adjusted Wald ratio, two-stage least squares and inverse variance weighted estimates. The two methods are evaluated and compared in simulations, with results suggesting that they can both mitigate selection bias but may yield parameter estimates with large standard errors in some settings. In an illustrative real-data application, we investigate the effects of body mass index on smoking using data from the Avon Longitudinal Study of Parents and Children.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.02657v3</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Apostolos Gkatzionis, Eric J. Tchetgen Tchetgen, Jon Heron, Kate Northstone, Kate Tilling</dc:creator>
    </item>
    <item>
      <title>mpower: An R Package for Power Analysis of Exposure Mixture Studies via Monte Carlo Simulations</title>
      <link>https://arxiv.org/abs/2209.08036</link>
      <description>arXiv:2209.08036v2 Announce Type: replace 
Abstract: Estimating sample size and statistical power is an essential part of a good study design. This R package allows users to conduct power analysis based on Monte Carlo simulations in settings in which consideration of the correlations between predictors is important. It runs power analyses given a data generative model and an inference model. It can set up a data generative model that preserves dependence structures among variables given existing data (continuous, binary, or ordinal) or high-level descriptions of the associations. Users can generate power curves to assess the trade-offs between sample size, effect size, and power of a design. This paper presents tutorials and examples focusing on applications for environmental mixture studies when predictors tend to be moderately to highly correlated. It easily interfaces with several existing and newly developed analysis strategies for assessing associations between exposures and health outcomes. However, the package is sufficiently general to facilitate power simulations in a wide variety of settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.08036v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s12561-023-09385-7</arxiv:DOI>
      <arxiv:journal_reference>Stat Biosci (2023)</arxiv:journal_reference>
      <dc:creator>Phuc H. Nguyen, Stephanie M. Engel, Amy H. Herring</dc:creator>
    </item>
    <item>
      <title>Fast Variational Inference for Bayesian Factor Analysis in Single and Multi-Study Settings</title>
      <link>https://arxiv.org/abs/2305.13188</link>
      <description>arXiv:2305.13188v2 Announce Type: replace 
Abstract: Factors models are routinely used to analyze high-dimensional data in both single-study and multi-study settings. Bayesian inference for such models relies on Markov Chain Monte Carlo (MCMC) methods which scale poorly as the number of studies, observations, or measured variables increase. To address this issue, we propose variational inference algorithms to approximate the posterior distribution of Bayesian latent factor models using the multiplicative gamma process shrinkage prior. The proposed algorithms provide fast approximate inference at a fraction of the time and memory of MCMC-based implementations while maintaining comparable accuracy in characterizing the data covariance matrix. We conduct extensive simulations to evaluate our proposed algorithms and show their utility in estimating the model for high-dimensional multi-study gene expression data in ovarian cancers. Overall, our proposed approaches enable more efficient and scalable inference for factor models, facilitating their use in high-dimensional settings. An R package VIMSFA implementing our methods is available on GitHub (github.com/blhansen/VI-MSFA).</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.13188v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Blake Hansen, Alejandra Avalos-Pacheco, Massimiliano Russo, Roberta De Vito</dc:creator>
    </item>
    <item>
      <title>Discrete forecast reconciliation</title>
      <link>https://arxiv.org/abs/2305.18809</link>
      <description>arXiv:2305.18809v3 Announce Type: replace 
Abstract: This paper presents a formal framework and proposes algorithms to extend forecast reconciliation to discrete-valued data to extend forecast reconciliation to discrete-valued data, including low counts. A novel method is introduced based on recasting the optimisation of scoring rules as an assignment problem, which is solved using quadratic programming. The proposed framework produces coherent joint probabilistic forecasts for count hierarchical time series. Two discrete reconciliation algorithms are also proposed and compared against generalisations of the top-down and bottom-up approaches for count data. Two simulation experiments and two empirical examples are conducted to validate that the proposed reconciliation algorithms improve forecast accuracy. The empirical applications are forecasting criminal offences in Washington D.C. and product unit sales in the M5 dataset. Compared to benchmarks, the proposed framework shows superior performance in both simulations and empirical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.18809v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bohan Zhang, Anastasios Panagiotelis, Yanfei Kang</dc:creator>
    </item>
    <item>
      <title>Sandwich Boosting for Accurate Estimation in Partially Linear Models for Grouped Data</title>
      <link>https://arxiv.org/abs/2307.11401</link>
      <description>arXiv:2307.11401v2 Announce Type: replace 
Abstract: We study partially linear models in settings where observations are arranged in independent groups but may exhibit within-group dependence. Existing approaches estimate linear model parameters through weighted least squares, with optimal weights (given by the inverse covariance of the response, conditional on the covariates) typically estimated by maximising a (restricted) likelihood from random effects modelling or by using generalised estimating equations. We introduce a new 'sandwich loss' whose population minimiser coincides with the weights of these approaches when the parametric forms for the conditional covariance are well-specified, but can yield arbitrarily large improvements in linear parameter estimation accuracy when they are not. Under relatively mild conditions, our estimated coefficients are asymptotically Gaussian and enjoy minimal variance among estimators with weights restricted to a given class of functions, when user-chosen regression methods are used to estimate nuisance functions. We further expand the class of functional forms for the weights that may be fitted beyond parametric models by leveraging the flexibility of modern machine learning methods within a new gradient boosting scheme for minimising the sandwich loss. We demonstrate the effectiveness of both the sandwich loss and what we call 'sandwich boosting' in a variety of settings with simulated and real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.11401v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elliot H. Young, Rajen D. Shah</dc:creator>
    </item>
    <item>
      <title>Simulation-Based Prior Knowledge Elicitation for Parametric Bayesian Models</title>
      <link>https://arxiv.org/abs/2308.11672</link>
      <description>arXiv:2308.11672v2 Announce Type: replace 
Abstract: A central characteristic of Bayesian statistics is the ability to consistently incorporate prior knowledge into various modeling processes. In this paper, we focus on translating domain expert knowledge into corresponding prior distributions over model parameters, a process known as prior elicitation. Expert knowledge can manifest itself in diverse formats, including information about raw data, summary statistics, or model parameters. A major challenge for existing elicitation methods is how to effectively utilize all of these different formats in order to formulate prior distributions that align with the expert's expectations, regardless of the model structure. To address these challenges, we develop a simulation-based elicitation method that can learn the hyperparameters of potentially any parametric prior distribution from a wide spectrum of expert knowledge using stochastic gradient descent. We validate the effectiveness and robustness of our elicitation method in four representative case studies covering linear models, generalized linear models, and hierarchical models. Our results support the claim that our method is largely independent of the underlying model structure and adaptable to various elicitation techniques, including quantile-based, moment-based, and histogram-based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.11672v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Florence Bockting, Stefan T. Radev, Paul-Christian B\"urkner</dc:creator>
    </item>
    <item>
      <title>Design-Based Causal Inference with Missing Outcomes: Missingness Mechanisms, Imputation-Assisted Randomization Tests, and Covariate Adjustment</title>
      <link>https://arxiv.org/abs/2310.18556</link>
      <description>arXiv:2310.18556v3 Announce Type: replace 
Abstract: Design-based causal inference, also known as randomization-based or finite-population causal inference, is one of the most widely used causal inference frameworks, largely due to the merit that its statistical validity can be guaranteed by the study design (e.g., randomized experiments) and does not require assuming specific outcome-generating distributions or super-population models. Despite its advantages, design-based causal inference can still suffer from other data-related issues, among which outcome missingness is a prevalent and significant challenge. This work systematically studies the outcome missingness problem in design-based causal inference. First, we propose a general and flexible outcome missingness mechanism that can facilitate finite-population-exact randomization tests for the null effect. Second, under this flexible missingness mechanism, we propose a general framework called "imputation and re-imputation" for conducting finite-population-exact randomization tests in design-based causal inference with missing outcomes. This framework can incorporate any imputation algorithms (from linear models to advanced machine learning-based imputation algorithms) while ensuring finite-population-exact type-I error rate control. Third, we extend our framework to conduct covariate adjustment in randomization tests and construct finite-population-valid confidence sets with missing outcomes. Our framework is evaluated via extensive simulation studies and applied to a cluster randomized experiment called the Work, Family, and Health Study. Open-source Python and R packages are also developed for implementation of our framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18556v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyu Heng, Jiawei Zhang, Yang Feng</dc:creator>
    </item>
    <item>
      <title>Factor copula models for non-Gaussian longitudinal data</title>
      <link>https://arxiv.org/abs/2402.00668</link>
      <description>arXiv:2402.00668v2 Announce Type: replace 
Abstract: This article presents factor copula approaches to model temporal dependency of non-Gaussian (continuous/discrete) longitudinal data. Factor copula models are canonical vine copulas which explain the underlying dependence structure of a multivariate data through latent variables, and therefore can be easily interpreted and implemented to unbalanced longitudinal data. We develop regression models for continuous, binary and ordinal longitudinal data including covariates, by using factor copula constructions with subject-specific latent variables. Considering homogeneous within-subject dependence, our proposed models allow for feasible parametric inference in moderate to high dimensional situations, using two-stage (IFM) estimation method. We assess the finite sample performance of the proposed models with extensive simulation studies. In the empirical analysis, the proposed models are applied for analysing different longitudinal responses of two real world data sets. Moreover, we compare the performances of these models with some widely used random effect models using standard model selection techniques and find substantial improvements. Our studies suggest that factor copula models can be good alternatives to random effect models and can provide better insights to temporal dependency of longitudinal data of arbitrary nature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00668v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhajit Chattopadhyay</dc:creator>
    </item>
    <item>
      <title>Hypergraph adjusted plus-minus</title>
      <link>https://arxiv.org/abs/2403.20214</link>
      <description>arXiv:2403.20214v2 Announce Type: replace 
Abstract: In team sports, traditional ranking statistics do not allow for the simultaneous evaluation of both individuals and combinations of players. Metrics for individual player rankings often fail to consider the interaction effects between groups of players, while methods for assessing full lineups cannot be used to identify the value of lower-order combinations of players (pairs, trios, etc.). Given that player and lineup rankings are inherently dependent on each other, these limitations may affect the accuracy of performance evaluations. To address this, we propose a novel adjusted box score plus-minus (APM) approach that allows for the simultaneous ranking of individual players, lower-order combinations of players, and full lineups. The method adjusts for the complete dependency structure and is motivated by the connection between APM and the hypergraph representation of a team. We discuss the similarities of our approach to other advanced metrics, demonstrate it using NBA data from 2012-2022, and suggest potential directions for future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20214v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathaniel Josephs, Elizabeth Upton</dc:creator>
    </item>
    <item>
      <title>A General Identification Algorithm For Data Fusion Problems Under Systematic Selection</title>
      <link>https://arxiv.org/abs/2404.06602</link>
      <description>arXiv:2404.06602v2 Announce Type: replace 
Abstract: Causal inference is made challenging by confounding, selection bias, and other complications. A common approach to addressing these difficulties is the inclusion of auxiliary data on the superpopulation of interest. Such data may measure a different set of variables, or be obtained under different experimental conditions than the primary dataset. Analysis based on multiple datasets must carefully account for similarities between datasets, while appropriately accounting for differences.
  In addition, selection of experimental units into different datasets may be systematic; similar difficulties are encountered in missing data problems. Existing methods for combining datasets either do not consider this issue, or assume simple selection mechanisms.
  In this paper, we provide a general approach, based on graphical causal models, for causal inference from data on the same superpopulation that is obtained under different experimental conditions. Our framework allows both arbitrary unobserved confounding, and arbitrary selection processes into different experimental regimes in our data.
  We describe how systematic selection processes may be organized into a hierarchy similar to censoring processes in missing data: selected completely at random (SCAR), selected at random (SAR), and selected not at random (SNAR). In addition, we provide a general identification algorithm for interventional distributions in this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06602v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaron J. R. Lee, AmirEmad Ghassami, Ilya Shpitser</dc:creator>
    </item>
    <item>
      <title>Safe subspace screening for the adaptive nuclear norm regularized trace regression</title>
      <link>https://arxiv.org/abs/2404.07459</link>
      <description>arXiv:2404.07459v2 Announce Type: replace 
Abstract: Matrix form data sets arise in many areas, so there are lots of works about the matrix regression models. One special model of these models is the adaptive nuclear norm regularized trace regression, which has been proven have good statistical performances. In order to accelerate the computation of this model, we consider the technique called screening rule. According to matrix decomposition and optimal condition of the model, we develop a safe subspace screening rule that can be used to identify inactive subspace of the solution decomposition and reduce the dimension of the solution. To evaluate the efficiency of the safe subspace screening rule, we embed this result into the alternating direction method of multipliers algorithm under a sequence of the tuning parameters. Under this process, each solution under the tuning parameter provides a matrix decomposition space. Then, the safe subspace screening rule is applied to eliminate inactive subspace, reduce the solution dimension and accelerate the computation process. Some numerical experiments are implemented on simulation data sets and real data sets, which illustrate the efficiency of our screening rule.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07459v2</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pan Shang, Lingchen Kong</dc:creator>
    </item>
    <item>
      <title>View selection in multi-view stacking: Choosing the meta-learner</title>
      <link>https://arxiv.org/abs/2010.16271</link>
      <description>arXiv:2010.16271v3 Announce Type: replace-cross 
Abstract: Multi-view stacking is a framework for combining information from different views (i.e. different feature sets) describing the same set of objects. In this framework, a base-learner algorithm is trained on each view separately, and their predictions are then combined by a meta-learner algorithm. In a previous study, stacked penalized logistic regression, a special case of multi-view stacking, has been shown to be useful in identifying which views are most important for prediction. In this article we expand this research by considering seven different algorithms to use as the meta-learner, and evaluating their view selection and classification performance in simulations and two applications on real gene-expression data sets. Our results suggest that if both view selection and classification accuracy are important to the research at hand, then the nonnegative lasso, nonnegative adaptive lasso and nonnegative elastic net are suitable meta-learners. Exactly which among these three is to be preferred depends on the research context. The remaining four meta-learners, namely nonnegative ridge regression, nonnegative forward selection, stability selection and the interpolating predictor, show little advantages in order to be preferred over the other three.</description>
      <guid isPermaLink="false">oai:arXiv.org:2010.16271v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11634-024-00587-5</arxiv:DOI>
      <arxiv:journal_reference>Advances in Data Analysis and Classification (2024)</arxiv:journal_reference>
      <dc:creator>Wouter van Loon, Marjolein Fokkema, Botond Szabo, Mark de Rooij</dc:creator>
    </item>
    <item>
      <title>Exploration of the search space of Gaussian graphical models for paired data</title>
      <link>https://arxiv.org/abs/2303.05561</link>
      <description>arXiv:2303.05561v2 Announce Type: replace-cross 
Abstract: We consider the problem of learning a Gaussian graphical model in the case where the observations come from two dependent groups sharing the same variables. We focus on a family of coloured Gaussian graphical models specifically suited for the paired data problem. Commonly, graphical models are ordered by the submodel relationship so that the search space is a lattice, called the model inclusion lattice. We introduce a novel order between models, named the twin order. We show that, embedded with this order, the model space is a lattice that, unlike the model inclusion lattice, is distributive. Furthermore, we provide the relevant rules for the computation of the neighbours of a model. The latter are more efficient than the same operations in the model inclusion lattice, and are then exploited to achieve a more efficient exploration of the search space. These results can be applied to improve the efficiency of both greedy and Bayesian model search procedures. Here we implement a stepwise backward elimination procedure and evaluate its performance by means of simulations. Finally, the procedure is applied to learn a brain network from fMRI data where the two groups correspond to the left and right hemispheres, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.05561v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Journal of Machine Learning Research (2024)</arxiv:journal_reference>
      <dc:creator>Alberto Roverato, Dung Ngoc Nguyen</dc:creator>
    </item>
    <item>
      <title>Towards Characterizing Domain Counterfactuals For Invertible Latent Causal Models</title>
      <link>https://arxiv.org/abs/2306.11281</link>
      <description>arXiv:2306.11281v3 Announce Type: replace-cross 
Abstract: Answering counterfactual queries has important applications such as explainability, robustness, and fairness but is challenging when the causal variables are unobserved and the observations are non-linear mixtures of these latent variables, such as pixels in images. One approach is to recover the latent Structural Causal Model (SCM), which may be infeasible in practice due to requiring strong assumptions, e.g., linearity of the causal mechanisms or perfect atomic interventions. Meanwhile, more practical ML-based approaches using naive domain translation models to generate counterfactual samples lack theoretical grounding and may construct invalid counterfactuals. In this work, we strive to strike a balance between practicality and theoretical guarantees by analyzing a specific type of causal query called domain counterfactuals, which hypothesizes what a sample would have looked like if it had been generated in a different domain (or environment). We show that recovering the latent SCM is unnecessary for estimating domain counterfactuals, thereby sidestepping some of the theoretic challenges. By assuming invertibility and sparsity of intervention, we prove domain counterfactual estimation error can be bounded by a data fit term and intervention sparsity term. Building upon our theoretical results, we develop a theoretically grounded practical algorithm that simplifies the modeling process to generative model estimation under autoregressive and shared parameter constraints that enforce intervention sparsity. Finally, we show an improvement in counterfactual estimation over baseline methods through extensive simulated and image-based experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.11281v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeyu Zhou, Ruqi Bai, Sean Kulinski, Murat Kocaoglu, David I. Inouye</dc:creator>
    </item>
    <item>
      <title>On the Computational Complexity of Private High-dimensional Model Selection</title>
      <link>https://arxiv.org/abs/2310.07852</link>
      <description>arXiv:2310.07852v3 Announce Type: replace-cross 
Abstract: We consider the problem of model selection in a high-dimensional sparse linear regression model under privacy constraints. We propose a differentially private best subset selection method with strong utility properties by adopting the well-known exponential mechanism for selecting the best model. We propose an efficient Metropolis-Hastings algorithm and establish that it enjoys polynomial mixing time to its stationary distribution. Furthermore, we also establish approximate differential privacy for the estimates of the mixed Metropolis-Hastings chain. Finally, we perform some illustrative experiments that show the strong utility of our algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07852v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saptarshi Roy, Zehua Wang, Ambuj Tewari</dc:creator>
    </item>
    <item>
      <title>Temporal Aggregation for the Synthetic Control Method</title>
      <link>https://arxiv.org/abs/2401.12084</link>
      <description>arXiv:2401.12084v2 Announce Type: replace-cross 
Abstract: The synthetic control method (SCM) is a popular approach for estimating the impact of a treatment on a single unit with panel data. Two challenges arise with higher frequency data (e.g., monthly versus yearly): (1) achieving excellent pre-treatment fit is typically more challenging; and (2) overfitting to noise is more likely. Aggregating data over time can mitigate these problems but can also destroy important signal. In this paper, we bound the bias for SCM with disaggregated and aggregated outcomes and give conditions under which aggregating tightens the bounds. We then propose finding weights that balance both disaggregated and aggregated series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12084v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liyang Sun, Eli Ben-Michael, Avi Feller</dc:creator>
    </item>
    <item>
      <title>Doubly Robust Inference in Causal Latent Factor Models</title>
      <link>https://arxiv.org/abs/2402.11652</link>
      <description>arXiv:2402.11652v2 Announce Type: replace-cross 
Abstract: This article introduces a new estimator of average treatment effects under unobserved confounding in modern data-rich environments featuring large numbers of units and outcomes. The proposed estimator is doubly robust, combining outcome imputation, inverse probability weighting, and a novel cross-fitting procedure for matrix completion. We derive finite-sample and asymptotic guarantees, and show that the error of the new estimator converges to a mean-zero Gaussian distribution at a parametric rate. Simulation results demonstrate the practical relevance of the formal properties of the estimators analyzed in this article.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11652v2</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Abadie, Anish Agarwal, Raaz Dwivedi, Abhin Shah</dc:creator>
    </item>
    <item>
      <title>Double Cross-fit Doubly Robust Estimators: Beyond Series Regression</title>
      <link>https://arxiv.org/abs/2403.15175</link>
      <description>arXiv:2403.15175v2 Announce Type: replace-cross 
Abstract: Doubly robust estimators with cross-fitting have gained popularity in causal inference due to their favorable structure-agnostic error guarantees. However, when additional structure, such as H\"{o}lder smoothness, is available then more accurate "double cross-fit doubly robust" (DCDR) estimators can be constructed by splitting the training data and undersmoothing nuisance function estimators on independent samples. We study a DCDR estimator of the Expected Conditional Covariance, a functional of interest in causal inference and conditional independence testing, and derive a series of increasingly powerful results with progressively stronger assumptions. We first provide a structure-agnostic error analysis for the DCDR estimator with no assumptions on the nuisance functions or their estimators. Then, assuming the nuisance functions are H\"{o}lder smooth, but without assuming knowledge of the true smoothness level or the covariate density, we establish that DCDR estimators with several linear smoothers are semiparametric efficient under minimal conditions and achieve fast convergence rates in the non-$\sqrt{n}$ regime. When the covariate density and smoothnesses are known, we propose a minimax rate-optimal DCDR estimator based on undersmoothed kernel regression. Moreover, we show an undersmoothed DCDR estimator satisfies a slower-than-$\sqrt{n}$ central limit theorem, and that inference is possible even in the non-$\sqrt{n}$ regime. Finally, we support our theoretical results with simulations, providing intuition for double cross-fitting and undersmoothing, demonstrating where our estimator achieves semiparametric efficiency while the usual "single cross-fit" estimator fails, and illustrating asymptotic normality for the undersmoothed DCDR estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15175v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alec McClean, Sivaraman Balakrishnan, Edward H. Kennedy, Larry Wasserman</dc:creator>
    </item>
  </channel>
</rss>
