<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Apr 2024 04:00:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 18 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Duality induced by an embedding structure of determinantal point process</title>
      <link>https://arxiv.org/abs/2404.11024</link>
      <description>arXiv:2404.11024v1 Announce Type: new 
Abstract: This paper investigates the information geometrical structure of a determinantal point process (DPP). It demonstrates that a DPP is embedded in the exponential family of log-linear models. The extent of deviation from an exponential family is analyzed using the $\mathrm{e}$-embedding curvature tensor, which identifies partially flat parameters of a DPP. On the basis of this embedding structure, the duality related to a marginal kernel and an $L$-ensemble kernel is discovered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11024v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hideitsu Hino, Keisuke Yano</dc:creator>
    </item>
    <item>
      <title>Estimating conditional hazard functions and densities with the highly-adaptive lasso</title>
      <link>https://arxiv.org/abs/2404.11083</link>
      <description>arXiv:2404.11083v1 Announce Type: new 
Abstract: We consider estimation of conditional hazard functions and densities over the class of multivariate c\`adl\`ag functions with uniformly bounded sectional variation norm when data are either fully observed or subject to right-censoring. We demonstrate that the empirical risk minimizer is either not well-defined or not consistent for estimation of conditional hazard functions and densities. Under a smoothness assumption about the data-generating distribution, a highly-adaptive lasso estimator based on a particular data-adaptive sieve achieves the same convergence rate as has been shown to hold for the empirical risk minimizer in settings where the latter is well-defined. We use this result to study a highly-adaptive lasso estimator of a conditional hazard function based on right-censored data. We also propose a new conditional density estimator and derive its convergence rate. Finally, we show that the result is of interest also for settings where the empirical risk minimizer is well-defined, because the highly-adaptive lasso depends on a much smaller number of basis function than the empirical risk minimizer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11083v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anders Munch, Thomas A. Gerds, Mark J. van der Laan, Helene C. W. Rytgaard</dc:creator>
    </item>
    <item>
      <title>Consistency of empirical distributions of sequences of graph statistics in networks with dependent edges</title>
      <link>https://arxiv.org/abs/2404.11438</link>
      <description>arXiv:2404.11438v1 Announce Type: new 
Abstract: One of the first steps in applications of statistical network analysis is frequently to produce summary charts of important features of the network. Many of these features take the form of sequences of graph statistics counting the number of realized events in the network, examples of which include the degree distribution, as well as the edgewise shared partner distribution, and more. We provide conditions under which the empirical distributions of sequences of graph statistics are consistent in the $\ell_{\infty}$-norm in settings where edges in the network are dependent. We accomplish this by elaborating a weak dependence condition which ensures that we can obtain exponential inequalities which bound probabilities of deviations of graph statistics from the expected value. We apply this concentration inequality to empirical distributions of sequences of graph statistics and derive non-asymptotic bounds on the $\ell_{\infty}$-error which hold with high probability. Our non-asymptotic results are then extended to demonstrate uniform convergence almost surely in selected examples. We illustrate theoretical results through examples, simulation studies, and an application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11438v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan R. Stewart</dc:creator>
    </item>
    <item>
      <title>Rates of convergence and normal approximations for estimators of local dependence random graph models</title>
      <link>https://arxiv.org/abs/2404.11464</link>
      <description>arXiv:2404.11464v1 Announce Type: new 
Abstract: Local dependence random graph models are a class of block models for network data which allow for dependence among edges under a local dependence assumption defined around the block structure of the network. Since being introduced by Schweinberger and Handcock (2015), research in the statistical network analysis and network science literatures have demonstrated the potential and utility of this class of models. In this work, we provide the first statistical disclaimers which provide conditions under which estimation and inference procedures can be expected to provide accurate and valid inferences. This is accomplished by deriving convergence rates of inference procedures for local dependence random graph models based on a single observation of the graph, allowing both the number of model parameters and the sizes of blocks to tend to infinity. First, we derive the first non-asymptotic bounds on the $\ell_2$-error of maximum likelihood estimators, along with convergence rates. Second, and more importantly, we derive the first non-asymptotic bounds on the error of the multivariate normal approximation. In so doing, we introduce the first principled approach to providing statistical disclaimers through quantifying the uncertainty about statistical conclusions based on data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11464v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan R. Stewart</dc:creator>
    </item>
    <item>
      <title>Matern Correlation: A Panoramic Primer</title>
      <link>https://arxiv.org/abs/2404.11427</link>
      <description>arXiv:2404.11427v1 Announce Type: cross 
Abstract: Matern correlation is of pivotal importance in spatial statistics and machine learning. This paper serves as a panoramic primer for this correlation with an emphasis on the exposition of its changing behavior and smoothness properties in response to the change of its two parameters. Such exposition is achieved through a series of simulation studies, the use of an interactive 3D visualization applet, and a practical modeling example, all tailored for a wide-ranging statistical audience. Meanwhile, the thorough understanding of these parameter-smoothness relationships, in turn, serves as a pragmatic guide for researchers in their real-world modeling endeavors, such as setting appropriate initial values for these parameters and parameter-fine-tuning in their Bayesian modeling practice or simulation studies involving the Matern correlation. Derived problems surrounding Matern, such as inconsistent parameter inference, extended forms of Matern and limitations of Matern, are also explored and surveyed to impart a panoramic view of this correlation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11427v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoqing Chen</dc:creator>
    </item>
    <item>
      <title>Improved bounds and inference on optimal regimes</title>
      <link>https://arxiv.org/abs/2404.11510</link>
      <description>arXiv:2404.11510v1 Announce Type: cross 
Abstract: Point identification of causal effects requires strong assumptions that are unreasonable in many practical settings. However, informative bounds on these effects can often be derived under plausible assumptions. Even when these bounds are wide or cover null effects, they can guide practical decisions based on formal decision theoretic criteria. Here we derive new results on optimal treatment regimes in settings where the effect of interest is bounded. These results are driven by consideration of superoptimal regimes; we define regimes that leverage an individual's natural treatment value, which is typically ignored in the existing literature. We obtain (sharp) bounds for the value function of superoptimal regimes, and provide performance guarantees relative to conventional optimal regimes. As a case study, we consider a commonly studied Marginal Sensitivity Model and illustrate that the superoptimal regime can be identified when conventional optimal regimes are not. We similarly illustrate this property in an instrumental variable setting. Finally, we derive efficient estimators for upper and lower bounds on the superoptimal value in instrumental variable settings, building on recent results on covariate adjusted Balke-Pearl bounds. These estimators are applied to study the effect of prompt ICU admission on survival.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11510v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Julien D. Laurendeau, Aaron L. Sarvet, Mats J. Stensrud</dc:creator>
    </item>
    <item>
      <title>A Bayesian level-set inversion method for simultaneous reconstruction of absorption and diffusion coefficients in diffuse optical tomography</title>
      <link>https://arxiv.org/abs/2404.11552</link>
      <description>arXiv:2404.11552v1 Announce Type: cross 
Abstract: In this article, we propose a non-parametric Bayesian level-set method for simultaneous reconstruction of piecewise constant diffusion and absorption coefficients in diffuse optical tomography (DOT). We show that the Bayesian formulation of the corresponding inverse problem is well-posed and the posterior measure as a solution to the inverse problem satisfies a Lipschitz estimate with respect to the measured data in terms of Hellinger distance. We reduce the problem to a shape-reconstruction problem and use level-set priors for the parameters of interest. We demonstrate the efficacy of the proposed method using numerical simulations by performing reconstructions of the original phantom using two reconstruction methods. Posing the inverse problem in a Bayesian paradigm allows us to do statistical inference for the parameters of interest whereby we are able to quantify the uncertainty in the reconstructions for both the methods. This illustrates a key advantage of Bayesian methods over traditional algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11552v1</guid>
      <category>stat.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anuj Abhishek, Thilo Strauss, Taufiquar Khan</dc:creator>
    </item>
    <item>
      <title>Change Acceleration and Detection</title>
      <link>https://arxiv.org/abs/1710.00915</link>
      <description>arXiv:1710.00915v4 Announce Type: replace 
Abstract: A novel sequential change detection problem is proposed, in which the goal is to not only detect but also accelerate the change. Specifically, it is assumed that the sequentially collected observations are responses to treatments selected in real time. The assigned treatments determine the pre-change and post-change distributions of the responses and also influence when the change happens. The goal is to find a treatment assignment rule and a stopping rule that minimize the expected total number of observations subject to a user-specified bound on the false alarm probability. The optimal solution is obtained under a general Markovian change-point model. Moreover, an alternative procedure is proposed, whose applicability is not restricted to Markovian change-point models and whose design requires minimal computation. For a large class of change-point models, the proposed procedure is shown to achieve the optimal performance in an asymptotic sense. Finally, its performance is found in simulation studies to be comparable to the optimal, uniformly with respect to the error probability.</description>
      <guid isPermaLink="false">oai:arXiv.org:1710.00915v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanglei Song, Georgios Fellouris</dc:creator>
    </item>
    <item>
      <title>Precise Asymptotics for Spectral Methods in Mixed Generalized Linear Models</title>
      <link>https://arxiv.org/abs/2211.11368</link>
      <description>arXiv:2211.11368v3 Announce Type: replace 
Abstract: In a mixed generalized linear model, the objective is to learn multiple signals from unlabeled observations: each sample comes from exactly one signal, but it is not known which one. We consider the prototypical problem of estimating two statistically independent signals in a mixed generalized linear model with Gaussian covariates. Spectral methods are a popular class of estimators which output the top two eigenvectors of a suitable data-dependent matrix. However, despite the wide applicability, their design is still obtained via heuristic considerations, and the number of samples $n$ needed to guarantee recovery is super-linear in the signal dimension $d$. In this paper, we develop exact asymptotics on spectral methods in the challenging proportional regime in which $n, d$ grow large and their ratio converges to a finite constant. By doing so, we are able to optimize the design of the spectral method, and combine it with a simple linear estimator, in order to minimize the estimation error. Our characterization exploits a mix of tools from random matrices, free probability and the theory of approximate message passing algorithms. Numerical simulations for mixed linear regression and phase retrieval demonstrate the advantage enabled by our analysis over existing designs of spectral methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.11368v3</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yihan Zhang, Marco Mondelli, Ramji Venkataramanan</dc:creator>
    </item>
    <item>
      <title>Optimization-based frequentist confidence intervals for functionals in constrained inverse problems: Resolving the Burrus conjecture</title>
      <link>https://arxiv.org/abs/2310.02461</link>
      <description>arXiv:2310.02461v3 Announce Type: replace 
Abstract: We present an optimization-based framework to construct confidence intervals for functionals in constrained inverse problems, ensuring valid one-at-a-time frequentist coverage guarantees. Our approach builds upon the now-called strict bounds intervals, originally pioneered by Burrus (1965) and Rust and Burrus (1972), which offer ways to directly incorporate any side information about the parameters during inference without introducing external biases. This family of methods allows for uncertainty quantification in ill-posed inverse problems without needing to select a regularizing prior. By tying optimization-based intervals to an inversion of a constrained likelihood ratio test, we translate interval coverage guarantees into type I error control and characterize the resulting interval via solutions to optimization problems. Along the way, we refute the Burrus conjecture, which posited that, for possibly rank-deficient linear Gaussian models with positivity constraints, a correction based on the quantile of the chi-squared distribution with one degree of freedom suffices to shorten intervals while maintaining frequentist coverage guarantees. Our framework provides a novel approach to analyzing the conjecture, and we construct a counterexample employing a stochastic dominance argument, which we also use to disprove a general form of the conjecture. We illustrate our framework with several numerical examples and provide directions for extensions beyond the Rust-Burrus method for nonlinear, non-Gaussian settings with general constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.02461v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pau Batlle, Pratik Patil, Michael Stanley, Houman Owhadi, Mikael Kuusela</dc:creator>
    </item>
    <item>
      <title>Local Projection Inference in High Dimensions</title>
      <link>https://arxiv.org/abs/2209.03218</link>
      <description>arXiv:2209.03218v3 Announce Type: replace-cross 
Abstract: In this paper, we estimate impulse responses by local projections in high-dimensional settings. We use the desparsified (de-biased) lasso to estimate the high-dimensional local projections, while leaving the impulse response parameter of interest unpenalized. We establish the uniform asymptotic normality of the proposed estimator under general conditions. Finally, we demonstrate small sample performance through a simulation study and consider two canonical applications in macroeconomic research on monetary policy and government spending.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.03218v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/ectj/utae012</arxiv:DOI>
      <dc:creator>Robert Adamek, Stephan Smeekes, Ines Wilms</dc:creator>
    </item>
    <item>
      <title>Provable Reward-Agnostic Preference-Based Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2305.18505</link>
      <description>arXiv:2305.18505v3 Announce Type: replace-cross 
Abstract: Preference-based Reinforcement Learning (PbRL) is a paradigm in which an RL agent learns to optimize a task using pair-wise preference-based feedback over trajectories, rather than explicit reward signals. While PbRL has demonstrated practical success in fine-tuning language models, existing theoretical work focuses on regret minimization and fails to capture most of the practical frameworks. In this study, we fill in such a gap between theoretical PbRL and practical algorithms by proposing a theoretical reward-agnostic PbRL framework where exploratory trajectories that enable accurate learning of hidden reward functions are acquired before collecting any human feedback. Theoretical analysis demonstrates that our algorithm requires less human feedback for learning the optimal policy under preference-based models with linear parameterization and unknown transitions, compared to the existing theoretical literature. Specifically, our framework can incorporate linear and low-rank MDPs with efficient sample complexity. Additionally, we investigate reward-agnostic RL with action-based comparison feedback and introduce an efficient querying algorithm tailored to this scenario.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.18505v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenhao Zhan, Masatoshi Uehara, Wen Sun, Jason D. Lee</dc:creator>
    </item>
    <item>
      <title>Adaptive Lasso, Transfer Lasso, and Beyond: An Asymptotic Perspective</title>
      <link>https://arxiv.org/abs/2308.15838</link>
      <description>arXiv:2308.15838v2 Announce Type: replace-cross 
Abstract: This paper presents a comprehensive exploration of the theoretical properties inherent in the Adaptive Lasso and the Transfer Lasso. The Adaptive Lasso, a well-established method, employs regularization divided by initial estimators and is characterized by asymptotic normality and variable selection consistency. In contrast, the recently proposed Transfer Lasso employs regularization subtracted by initial estimators with the demonstrated capacity to curtail non-asymptotic estimation errors. A pivotal question thus emerges: Given the distinct ways the Adaptive Lasso and the Transfer Lasso employ initial estimators, what benefits or drawbacks does this disparity confer upon each method? This paper conducts a theoretical examination of the asymptotic properties of the Transfer Lasso, thereby elucidating its differentiation from the Adaptive Lasso. Informed by the findings of this analysis, we introduce a novel method, one that amalgamates the strengths and compensates for the weaknesses of both methods. The paper concludes with validations of our theory and comparisons of the methods via simulation experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.15838v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masaaki Takada, Hironori Fujisawa</dc:creator>
    </item>
    <item>
      <title>Breaking the Heavy-Tailed Noise Barrier in Stochastic Optimization Problems</title>
      <link>https://arxiv.org/abs/2311.04161</link>
      <description>arXiv:2311.04161v2 Announce Type: replace-cross 
Abstract: We consider stochastic optimization problems with heavy-tailed noise with structured density. For such problems, we show that it is possible to get faster rates of convergence than $\mathcal{O}(K^{-2(\alpha - 1)/\alpha})$, when the stochastic gradients have finite moments of order $\alpha \in (1, 2]$. In particular, our analysis allows the noise norm to have an unbounded expectation. To achieve these results, we stabilize stochastic gradients, using smoothed medians of means. We prove that the resulting estimates have negligible bias and controllable variance. This allows us to carefully incorporate them into clipped-SGD and clipped-SSTM and derive new high-probability complexity bounds in the considered setup.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04161v2</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikita Puchkin, Eduard Gorbunov, Nikolay Kutuzov, Alexander Gasnikov</dc:creator>
    </item>
    <item>
      <title>On $q$-Order Statistics</title>
      <link>https://arxiv.org/abs/2311.12634</link>
      <description>arXiv:2311.12634v2 Announce Type: replace-cross 
Abstract: Building on the notion of $q$-integral introduced by Thomae in 1869, we introduce $q$-order statistics (that, is $q$-analogues of the classical order statistics, for $0&lt;q&lt;1$) which arise from dependent and not identically distributed $q$-continuous random variables and study their distributional properties. We study the $q$-distribution functions and the $q$-density functions of the relative $q$-ordered random variables. We focus on $q$-ordered variables arising from dependent and not identically $q$-uniformly distributed random variables and we derive their $q$-distributions, including $q$-power law, $q$-beta and $q$-Dirichlet distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.12634v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Malvina Vamvakari</dc:creator>
    </item>
    <item>
      <title>Extending Mean-Field Variational Inference via Entropic Regularization: Theory and Computation</title>
      <link>https://arxiv.org/abs/2404.09113</link>
      <description>arXiv:2404.09113v2 Announce Type: replace-cross 
Abstract: Variational inference (VI) has emerged as a popular method for approximate inference for high-dimensional Bayesian models. In this paper, we propose a novel VI method that extends the naive mean field via entropic regularization, referred to as $\Xi$-variational inference ($\Xi$-VI). $\Xi$-VI has a close connection to the entropic optimal transport problem and benefits from the computationally efficient Sinkhorn algorithm. We show that $\Xi$-variational posteriors effectively recover the true posterior dependency, where the dependence is downweighted by the regularization parameter. We analyze the role of dimensionality of the parameter space on the accuracy of $\Xi$-variational approximation and how it affects computational considerations, providing a rough characterization of the statistical-computational trade-off in $\Xi$-VI. We also investigate the frequentist properties of $\Xi$-VI and establish results on consistency, asymptotic normality, high-dimensional asymptotics, and algorithmic stability. We provide sufficient criteria for achieving polynomial-time approximate inference using the method. Finally, we demonstrate the practical advantage of $\Xi$-VI over mean-field variational inference on simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09113v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bohan Wu, David Blei</dc:creator>
    </item>
  </channel>
</rss>
