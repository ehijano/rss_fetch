<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Sep 2024 04:00:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Spatial Deep Convolutional Neural Networks</title>
      <link>https://arxiv.org/abs/2409.07559</link>
      <description>arXiv:2409.07559v1 Announce Type: new 
Abstract: Spatial prediction problems often use Gaussian process models, which can be computationally burdensome in high dimensions. Specification of an appropriate covariance function for the model can be challenging when complex non-stationarities exist. Recent work has shown that pre-computed spatial basis functions and a feed-forward neural network can capture complex spatial dependence structures while remaining computationally efficient. This paper builds on this literature by tailoring spatial basis functions for use in convolutional neural networks. Through both simulated and real data, we demonstrate that this approach yields more accurate spatial predictions than existing methods. Uncertainty quantification is also considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07559v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qi Wang, Paul A. Parker, Robert B. Lund</dc:creator>
    </item>
    <item>
      <title>Debiased high-dimensional regression calibration for errors-in-variables log-contrast models</title>
      <link>https://arxiv.org/abs/2409.07568</link>
      <description>arXiv:2409.07568v1 Announce Type: new 
Abstract: Motivated by the challenges in analyzing gut microbiome and metagenomic data, this work aims to tackle the issue of measurement errors in high-dimensional regression models that involve compositional covariates. This paper marks a pioneering effort in conducting statistical inference on high-dimensional compositional data affected by mismeasured or contaminated data. We introduce a calibration approach tailored for the linear log-contrast model. Under relatively lenient conditions regarding the sparsity level of the parameter, we have established the asymptotic normality of the estimator for inference. Numerical experiments and an application in microbiome study have demonstrated the efficacy of our high-dimensional calibration strategy in minimizing bias and achieving the expected coverage rates for confidence intervals. Moreover, the potential application of our proposed methodology extends well beyond compositional data, suggesting its adaptability for a wide range of research contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07568v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huali Zhao, Tianying Wang</dc:creator>
    </item>
    <item>
      <title>Determining number of factors under stability considerations</title>
      <link>https://arxiv.org/abs/2409.07617</link>
      <description>arXiv:2409.07617v1 Announce Type: new 
Abstract: This paper proposes a novel method for determining the number of factors in linear factor models under stability considerations. An instability measure is proposed based on the principal angle between the estimated loading spaces obtained by data splitting. Based on this measure, criteria for determining the number of factors are proposed and shown to be consistent. This consistency is obtained using results from random matrix theory, especially the complete delocalization of non-outlier eigenvectors. The advantage of the proposed methods over the existing ones is shown via weaker asymptotic requirements for consistency, simulation studies and a real data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07617v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sze Ming Lee, Yunxiao Chen</dc:creator>
    </item>
    <item>
      <title>A model-based approach for clustering binned data</title>
      <link>https://arxiv.org/abs/2409.07738</link>
      <description>arXiv:2409.07738v1 Announce Type: new 
Abstract: Binned data often appears in different fields of research, and it is generated after summarizing the original data in a sequence of pairs of bins (or their midpoints) and frequencies. There may exist different reasons to only provide this summary, but more importantly, it is necessary being able to perform statistical analyses based only on it. We present a Bayesian nonparametric model for clustering applicable for binned data. Clusters are modeled via random partitions, and within them a model-based approach is assumed. Inferences are performed by a Markov chain Monte Carlo method and the complete proposal is tested using simulated and real data. Having particular interest in studying marine populations, we analyze samples of Lobatus (Strobus) gigas' lengths and found the presence of up to three cohorts along the year.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07738v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asael Fabian Mart\'inez, Carlos D\'iaz-Avalos</dc:creator>
    </item>
    <item>
      <title>Generalized Independence Test for Modern Data</title>
      <link>https://arxiv.org/abs/2409.07745</link>
      <description>arXiv:2409.07745v1 Announce Type: new 
Abstract: The test of independence is a crucial component of modern data analysis. However, traditional methods often struggle with the complex dependency structures found in high-dimensional data. To overcome this challenge, we introduce a novel test statistic that captures intricate relationships using similarity and dissimilarity information derived from the data. The statistic exhibits strong power across a broad range of alternatives for high-dimensional data, as demonstrated in extensive simulation studies. Under mild conditions, we show that the new test statistic converges to the $\chi^2_4$ distribution under the permutation null distribution, ensuring straightforward type I error control. Furthermore, our research advances the moment method in proving the joint asymptotic normality of multiple double-indexed permutation statistics. We showcase the practical utility of this new test with an application to the Genotype-Tissue Expression dataset, where it effectively measures associations between human tissues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07745v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingshuo Liu, Doudou Zhou, Hao Chen</dc:creator>
    </item>
    <item>
      <title>Robust and efficient estimation in the presence of a randomly censored covariate</title>
      <link>https://arxiv.org/abs/2409.07795</link>
      <description>arXiv:2409.07795v1 Announce Type: new 
Abstract: In Huntington's disease research, a current goal is to understand how symptoms change prior to a clinical diagnosis. Statistically, this entails modeling symptom severity as a function of the covariate 'time until diagnosis', which is often heavily right-censored in observational studies. Existing estimators that handle right-censored covariates have varying statistical efficiency and robustness to misspecified models for nuisance distributions (those of the censored covariate and censoring variable). On one extreme, complete case estimation, which utilizes uncensored data only, is free of nuisance distribution models but discards informative censored observations. On the other extreme, maximum likelihood estimation is maximally efficient but inconsistent when the covariate's distribution is misspecified. We propose a semiparametric estimator that is robust and efficient. When the nuisance distributions are modeled parametrically, the estimator is doubly robust, i.e., consistent if at least one distribution is correctly specified, and semiparametric efficient if both models are correctly specified. When the nuisance distributions are estimated via nonparametric or machine learning methods, the estimator is consistent and semiparametric efficient. We show empirically that the proposed estimator, implemented in the R package sparcc, has its claimed properties, and we apply it to study Huntington's disease symptom trajectories using data from the Enroll-HD study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07795v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seong-ho Lee, Brian D. Richardson, Yanyuan Ma, Karen S. Marder, Tanya P. Garcia</dc:creator>
    </item>
    <item>
      <title>Bootstrap Adaptive Lasso Solution Path Unit Root Tests</title>
      <link>https://arxiv.org/abs/2409.07859</link>
      <description>arXiv:2409.07859v1 Announce Type: new 
Abstract: We propose sieve wild bootstrap analogues to the adaptive Lasso solution path unit root tests of Arnold and Reinschl\"ussel (2024) arXiv:2404.06205 to improve finite sample properties and extend their applicability to a generalised framework, allowing for non-stationary volatility. Numerical evidence shows the bootstrap to improve the tests' precision for error processes that promote spurious rejections of the unit root null, depending on the detrending procedure. The bootstrap mitigates finite-sample size distortions and restores asymptotically valid inference when the data features time-varying unconditional variance. We apply the bootstrap tests to real residential property prices of the top six Eurozone economies and find evidence of stationarity to be period-specific, supporting the conjecture that exuberance in the housing market characterises the development of Euro-era residential property prices in the recent past.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07859v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin C. Arnold, Thilo Reinschl\"ussel</dc:creator>
    </item>
    <item>
      <title>Cellwise outlier detection in heterogeneous populations</title>
      <link>https://arxiv.org/abs/2409.07881</link>
      <description>arXiv:2409.07881v1 Announce Type: new 
Abstract: Real-world applications may be affected by outlying values. In the model-based clustering literature, several methodologies have been proposed to detect units that deviate from the majority of the data (rowwise outliers) and trim them from the parameter estimates. However, the discarded observations can encompass valuable information in some observed features. Following the more recent cellwise contamination paradigm, we introduce a Gaussian mixture model for cellwise outlier detection. The proposal is estimated via an Expectation-Maximization (EM) algorithm with an additional step for flagging the contaminated cells of a data matrix and then imputing -- instead of discarding -- them before the parameter estimation. This procedure adheres to the spirit of the EM algorithm by treating the contaminated cells as missing values. We analyze the performance of the proposed model in comparison with other existing methodologies through a simulation study with different scenarios and illustrate its potential use for clustering, outlier detection, and imputation on three real data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07881v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giorgia Zaccaria, Luis A. Garc\'ia-Escudero, Francesca Greselin, Agust\'in Mayo-\'Iscar</dc:creator>
    </item>
    <item>
      <title>Multiple tests for restricted mean time lost with competing risks data</title>
      <link>https://arxiv.org/abs/2409.07917</link>
      <description>arXiv:2409.07917v1 Announce Type: new 
Abstract: Easy-to-interpret effect estimands are highly desirable in survival analysis. In the competing risks framework, one good candidate is the restricted mean time lost (RMTL). It is defined as the area under the cumulative incidence function up to a prespecified time point and, thus, it summarizes the cumulative incidence function into a meaningful estimand. While existing RMTL-based tests are limited to two-sample comparisons and mostly to two event types, we aim to develop general contrast tests for factorial designs and an arbitrary number of event types based on a Wald-type test statistic. Furthermore, we avoid the often-made, rather restrictive continuity assumption on the event time distribution. This allows for ties in the data, which often occur in practical applications, e.g., when event times are measured in whole days. In addition, we develop more reliable tests for RMTL comparisons that are based on a permutation approach to improve the small sample performance. In a second step, multiple tests for RMTL comparisons are developed to test several null hypotheses simultaneously. Here, we incorporate the asymptotically exact dependence structure between the local test statistics to gain more power. The small sample performance of the proposed testing procedures is analyzed in simulations and finally illustrated by analyzing a real data example about leukemia patients who underwent bone marrow transplantation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07917v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Merle Munko, Dennis Dobler, Marc Ditzhaus</dc:creator>
    </item>
    <item>
      <title>Community detection in multi-layer networks by regularized debiased spectral clustering</title>
      <link>https://arxiv.org/abs/2409.07956</link>
      <description>arXiv:2409.07956v1 Announce Type: new 
Abstract: Community detection is a crucial problem in the analysis of multi-layer networks. In this work, we introduce a new method, called regularized debiased sum of squared adjacency matrices (RDSoS), to detect latent communities in multi-layer networks. RDSoS is developed based on a novel regularized Laplacian matrix that regularizes the debiased sum of squared adjacency matrices. In contrast, the classical regularized Laplacian matrix typically regularizes the adjacency matrix of a single-layer network. Therefore, at a high level, our regularized Laplacian matrix extends the classical regularized Laplacian matrix to multi-layer networks. We establish the consistency property of RDSoS under the multi-layer stochastic block model (MLSBM) and further extend RDSoS and its theoretical results to the degree-corrected version of the MLSBM model. The effectiveness of the proposed methods is evaluated and demonstrated through synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07956v1</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huan Qing</dc:creator>
    </item>
    <item>
      <title>Review of Recent Advances in Gaussian Process Regression Methods</title>
      <link>https://arxiv.org/abs/2409.08112</link>
      <description>arXiv:2409.08112v1 Announce Type: new 
Abstract: Gaussian process (GP) methods have been widely studied recently, especially for large-scale systems with big data and even more extreme cases when data is sparse. Key advantages of these methods consist in: 1) the ability to provide inherent ways to assess the impact of uncertainties (especially in the data, and environment) on the solutions, 2) have efficient factorisation based implementations and 3) can be implemented easily in distributed manners and hence provide scalable solutions. This paper reviews the recently developed key factorised GP methods such as the hierarchical off-diagonal low-rank approximation methods and GP with Kronecker structures. An example illustrates the performance of these methods with respect to accuracy and computational complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08112v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenyi Lyu, Xingchi Liu, Lyudmila Mihaylova</dc:creator>
    </item>
    <item>
      <title>Ratio Divergence Learning Using Target Energy in Restricted Boltzmann Machines: Beyond Kullback--Leibler Divergence Learning</title>
      <link>https://arxiv.org/abs/2409.07679</link>
      <description>arXiv:2409.07679v1 Announce Type: cross 
Abstract: We propose ratio divergence (RD) learning for discrete energy-based models, a method that utilizes both training data and a tractable target energy function. We apply RD learning to restricted Boltzmann machines (RBMs), which are a minimal model that satisfies the universal approximation theorem for discrete distributions. RD learning combines the strength of both forward and reverse Kullback-Leibler divergence (KLD) learning, effectively addressing the "notorious" issues of underfitting with the forward KLD and mode-collapse with the reverse KLD. Since the summation of forward and reverse KLD seems to be sufficient to combine the strength of both approaches, we include this learning method as a direct baseline in numerical experiments to evaluate its effectiveness. Numerical experiments demonstrate that RD learning significantly outperforms other learning methods in terms of energy function fitting, mode-covering, and learning stability across various discrete energy-based models. Moreover, the performance gaps between RD learning and the other learning methods become more pronounced as the dimensions of target models increase.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07679v1</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuichi Ishida, Yuma Ichikawa, Aki Dote, Toshiyuki Miyazawa, Koji Hukushima</dc:creator>
    </item>
    <item>
      <title>Fused $L_{1/2}$ prior for large scale linear inverse problem with Gibbs bouncy particle sampler</title>
      <link>https://arxiv.org/abs/2409.07874</link>
      <description>arXiv:2409.07874v1 Announce Type: cross 
Abstract: In this paper, we study Bayesian approach for solving large scale linear inverse problems arising in various scientific and engineering fields. We propose a fused $L_{1/2}$ prior with edge-preserving and sparsity-promoting properties and show that it can be formulated as a Gaussian mixture Markov random field. Since the density function of this family of prior is neither log-concave nor Lipschitz, gradient-based Markov chain Monte Carlo methods can not be applied to sample the posterior. Thus, we present a Gibbs sampler in which all the conditional posteriors involved have closed form expressions. The Gibbs sampler works well for small size problems but it is computationally intractable for large scale problems due to the need for sample high dimensional Gaussian distribution. To reduce the computation burden, we construct a Gibbs bouncy particle sampler (Gibbs-BPS) based on a piecewise deterministic Markov process. This new sampler combines elements of Gibbs sampler with bouncy particle sampler and its computation complexity is an order of magnitude smaller. We show that the new sampler converges to the target distribution. With computed tomography examples, we demonstrate that the proposed method shows competitive performance with existing popular Bayesian methods and is highly efficient in large scale problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07874v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiongwen Ke, Yanan Fan, Qingping Zhou</dc:creator>
    </item>
    <item>
      <title>Randomized Spline Trees for Functional Data Classification: Theory and Application to Environmental Time Series</title>
      <link>https://arxiv.org/abs/2409.07879</link>
      <description>arXiv:2409.07879v1 Announce Type: cross 
Abstract: Functional data analysis (FDA) and ensemble learning can be powerful tools for analyzing complex environmental time series. Recent literature has highlighted the key role of diversity in enhancing accuracy and reducing variance in ensemble methods.This paper introduces Randomized Spline Trees (RST), a novel algorithm that bridges these two approaches by incorporating randomized functional representations into the Random Forest framework. RST generates diverse functional representations of input data using randomized B-spline parameters, creating an ensemble of decision trees trained on these varied representations. We provide a theoretical analysis of how this functional diversity contributes to reducing generalization error and present empirical evaluations on six environmental time series classification tasks from the UCR Time Series Archive. Results show that RST variants outperform standard Random Forests and Gradient Boosting on most datasets, improving classification accuracy by up to 14\%. The success of RST demonstrates the potential of adaptive functional representations in capturing complex temporal patterns in environmental data. This work contributes to the growing field of machine learning techniques focused on functional data and opens new avenues for research in environmental time series analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07879v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Donato Riccio, Fabrizio Maturo, Elvira Romano</dc:creator>
    </item>
    <item>
      <title>Causal inference and racial bias in policing: New estimands and the importance of mobility data</title>
      <link>https://arxiv.org/abs/2409.08059</link>
      <description>arXiv:2409.08059v1 Announce Type: cross 
Abstract: Studying racial bias in policing is a critically important problem, but one that comes with a number of inherent difficulties due to the nature of the available data. In this manuscript we tackle multiple key issues in the causal analysis of racial bias in policing. First, we formalize race and place policing, the idea that individuals of one race are policed differently when they are in neighborhoods primarily made up of individuals of other races. We develop an estimand to study this question rigorously, show the assumptions necessary for causal identification, and develop sensitivity analyses to assess robustness to violations of key assumptions. Additionally, we investigate difficulties with existing estimands targeting racial bias in policing. We show for these estimands, and the estimands developed in this manuscript, that estimation can benefit from incorporating mobility data into analyses. We apply these ideas to a study in New York City, where we find a large amount of racial bias, as well as race and place policing, and that these findings are robust to large violations of untestable assumptions. We additionally show that mobility data can make substantial impacts on the resulting estimates, suggesting it should be used whenever possible in subsequent studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08059v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuochao Huang, Brenden Beck, Joseph Antonelli</dc:creator>
    </item>
    <item>
      <title>Machine Learning for Two-Sample Testing under Right-Censored Data: A Simulation Study</title>
      <link>https://arxiv.org/abs/2409.08201</link>
      <description>arXiv:2409.08201v1 Announce Type: cross 
Abstract: The focus of this study is to evaluate the effectiveness of Machine Learning (ML) methods for two-sample testing with right-censored observations. To achieve this, we develop several ML-based methods with varying architectures and implement them as two-sample tests. Each method is an ensemble (stacking) that combines predictions from classical two-sample tests. This paper presents the results of training the proposed ML methods, examines their statistical power compared to classical two-sample tests, analyzes the distribution of test statistics for the proposed methods when the null hypothesis is true, and evaluates the significance of the features incorporated into the proposed methods. All results from numerical experiments were obtained from a synthetic dataset generated using the Smirnov transform (Inverse Transform Sampling) and replicated multiple times through Monte Carlo simulation. To test the two-sample problem with right-censored observations, one can use the proposed two-sample methods. All necessary materials (source code, example scripts, dataset, and samples) are available on GitHub and Hugging Face.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08201v1</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Petr Philonenko, Sergey Postovalov</dc:creator>
    </item>
    <item>
      <title>Bayesian inference of vector autoregressions with tensor decompositions</title>
      <link>https://arxiv.org/abs/2211.01727</link>
      <description>arXiv:2211.01727v5 Announce Type: replace 
Abstract: Vector autoregressions (VARs) are popular model for analyzing multivariate economic time series. However, VARs can be over-parameterized if the numbers of variables and lags are moderately large. Tensor VAR, a recent solution to over-parameterization, treats the coefficient matrix as a third-order tensor and estimates the corresponding tensor decomposition to achieve parsimony. In this paper, we employ the Tensor VAR structure with a CANDECOMP/PARAFAC (CP) decomposition and conduct Bayesian inference to estimate parameters. Firstly, we determine the rank by imposing the Multiplicative Gamma Prior to the tensor margins, i.e. elements in the decomposition, and accelerate the computation with an adaptive inferential scheme. Secondly, to obtain interpretable margins, we propose an interweaving algorithm to improve the mixing of margins and identify the margins using a post-processing procedure. In an application to the US macroeconomic data, our models outperform standard VARs in point and density forecasting and yield a summary of the dynamic of the US economy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.01727v5</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiyong Luo, Jim E. Griffin</dc:creator>
    </item>
    <item>
      <title>ADDIS-Graphs for online error control with application to platform trials</title>
      <link>https://arxiv.org/abs/2301.11711</link>
      <description>arXiv:2301.11711v3 Announce Type: replace 
Abstract: In contemporary research, online error control is often required, where an error criterion, such as familywise error rate (FWER) or false discovery rate (FDR), shall remain under control while testing an a priori unbounded sequence of hypotheses. The existing online literature mainly considered large-scale designs and constructed blackbox-like algorithms for these. However, smaller studies, such as platform trials, require high flexibility and easy interpretability to take study objectives into account and facilitate the communication. Another challenge in platform trials is that due to the shared control arm some of the p-values are dependent and significance levels need to be prespecified before the decisions for all the past treatments are available. We propose ADDIS-Graphs with FWER control that due to their graphical structure perfectly adapt to such settings and provably uniformly improve the state-of-the-art method. We introduce several extensions of these ADDIS-Graphs, including the incorporation of information about the joint distribution of the p-values and a version for FDR control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.11711v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lasse Fischer, Marta Bofill Roig, Werner Brannath</dc:creator>
    </item>
    <item>
      <title>Accounting for multiplicity in machine learning benchmark performance</title>
      <link>https://arxiv.org/abs/2303.07272</link>
      <description>arXiv:2303.07272v5 Announce Type: replace 
Abstract: Machine learning methods are commonly evaluated and compared by their performance on data sets from public repositories. This allows for multiple methods, oftentimes several thousands, to be evaluated under identical conditions and across time. The highest ranked performance on a problem is referred to as state-of-the-art (SOTA) performance, and is used, among other things, as a reference point for publication of new methods. Using the highest-ranked performance as an estimate for SOTA is a biased estimator, giving overly optimistic results. The mechanisms at play are those of multiplicity, a topic that is well-studied in the context of multiple comparisons and multiple testing, but has, as far as the authors are aware of, been nearly absent from the discussion regarding SOTA estimates. The optimistic state-of-the-art estimate is used as a standard for evaluating new methods, and methods with substantial inferior results are easily overlooked. In this article, we provide a probability distribution for the case of multiple classifiers so that known analyses methods can be engaged and a better SOTA estimate can be provided. We demonstrate the impact of multiplicity through a simulated example with independent classifiers. We show how classifier dependency impacts the variance, but also that the impact is limited when the accuracy is high. Finally, we discuss three real-world examples; Kaggle competitions that demonstrate various aspects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.07272v5</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kajsa M{\o}llersen, Einar Holsb{\o}</dc:creator>
    </item>
    <item>
      <title>Identification and multiply robust estimation in causal mediation analysis across principal strata</title>
      <link>https://arxiv.org/abs/2304.10025</link>
      <description>arXiv:2304.10025v4 Announce Type: replace 
Abstract: We consider assessing causal mediation in the presence of a post-treatment event (examples include noncompliance, a clinical event, or death). We identify natural mediation effects for the entire study population and for each principal stratum characterized by the joint potential values of the post-treatment event. We derive the efficient influence function for each mediation estimand, which motivates a set of multiply robust estimators for inference. The multiply robust estimators are consistent under four types of misspecifications and are efficient when all nuisance models are correctly specified. We also develop a nonparametric efficient estimator that leverages data-adaptive machine learners to achieve efficient inference and discuss sensitivity methods to address key identification assumptions. We illustrate our methods via simulations and two real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.10025v4</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chao Cheng, Fan Li</dc:creator>
    </item>
    <item>
      <title>Multivariate extensions of the Multilevel Best Linear Unbiased Estimator for ensemble-variational data assimilation</title>
      <link>https://arxiv.org/abs/2306.07017</link>
      <description>arXiv:2306.07017v2 Announce Type: replace 
Abstract: Multilevel estimators aim at reducing the variance of Monte Carlo statistical estimators, by combining samples generated with simulators of different costs and accuracies. In particular, the recent work of Schaden and Ullmann (2020) on the multilevel best linear unbiased estimator (MLBLUE) introduces a framework unifying several multilevel and multifidelity techniques. The MLBLUE is reintroduced here using a variance minimization approach rather than the regression approach of Schaden and Ullmann. We then discuss possible extensions of the scalar MLBLUE to a multidimensional setting, i.e. from the expectation of scalar random variables to the expectation of random vectors. Several estimators of increasing complexity are proposed: a) multilevel estimators with scalar weights, b) with element-wise weights, c) with spectral weights and d) with general matrix weights. The computational cost of each method is discussed. We finally extend the MLBLUE to the estimation of second-order moments in the multidimensional case, i.e. to the estimation of covariance matrices. The multilevel estimators proposed are d) a multilevel estimator with scalar weights and e) with element-wise weights. In large-dimension applications such as data assimilation for geosciences, the latter estimator is computationnally unaffordable. As a remedy, we also propose f) a multilevel covariance matrix estimator with optimal multilevel localization, inspired by the optimal localization theory of M\'en\'etrier and Aulign\'e (2015). Some practical details on weighted MLMC estimators of covariance matrices are given in appendix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.07017v2</guid>
      <category>stat.ME</category>
      <category>physics.data-an</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mayeul Destouches, Paul Mycek, Selime G\"urol</dc:creator>
    </item>
    <item>
      <title>Fusion regression methods with repeated functional data</title>
      <link>https://arxiv.org/abs/2308.01747</link>
      <description>arXiv:2308.01747v3 Announce Type: replace 
Abstract: Linear regression and classification methods with repeated functional data are considered. For each statistical unit in the sample, a real-valued parameter is observed over time under different conditions related by some neighborhood structure (spatial, group, etc.). Two regression methods based on fusion penalties are proposed to consider the dependence induced by this structure. These methods aim to obtain parsimonious coefficient regression functions, by determining if close conditions are associated with common regression coefficient functions. The first method is a generalization to functional data of the variable fusion methodology based on the 1-nearest neighbor. The second one relies on the group fusion lasso penalty which assumes some grouping structure of conditions and allows for homogeneity among the regression coefficient functions within groups. Numerical simulations and an application of electroencephalography data are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.01747v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Issam-Ali Moindji\'e, Cristian Preda, Sophie Dabo-Niang</dc:creator>
    </item>
    <item>
      <title>Graph of Graphs: From Nodes to Supernodes in Graphical Models</title>
      <link>https://arxiv.org/abs/2310.11741</link>
      <description>arXiv:2310.11741v2 Announce Type: replace 
Abstract: High-dimensional data analysis typically focuses on low-dimensional structure, often to aid interpretation and computational efficiency. Graphical models provide a powerful methodology for learning the conditional independence structure in multivariate data by representing variables as nodes and dependencies as edges. Inference is often focused on individual edges in the latent graph. Nonetheless, there is increasing interest in determining more complex structures, such as communities of nodes, for multiple reasons, including more effective information retrieval and better interpretability. In this work, we propose a hierarchical graphical model where we first cluster nodes and then, at the higher level, investigate the relationships among groups of nodes. Specifically, nodes are partitioned into supernodes with a data-coherent size-biased tessellation prior which combines ideas from Bayesian nonparametrics and Voronoi tessellations. This construct also allows accounting for the dependence of nodes within supernodes. At the higher level, dependence structure among supernodes is modeled through a Gaussian graphical model, where the focus of inference is on superedges. We provide theoretical justification for our modeling choices. We design tailored Markov chain Monte Carlo schemes, which also enable parallel computations. We demonstrate the effectiveness of our approach for large-scale structure learning in simulations and a transcriptomics application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.11741v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maria De Iorio, Willem van den Boom, Alexandros Beskos, Ajay Jasra, Andrea Cremaschi</dc:creator>
    </item>
    <item>
      <title>SymmPI: Predictive Inference for Data with Group Symmetries</title>
      <link>https://arxiv.org/abs/2312.16160</link>
      <description>arXiv:2312.16160v3 Announce Type: replace 
Abstract: Quantifying the uncertainty of predictions is a core problem in modern statistics. Methods for predictive inference have been developed under a variety of assumptions, often -- for instance, in standard conformal prediction -- relying on the invariance of the distribution of the data under special groups of transformations such as permutation groups. Moreover, many existing methods for predictive inference aim to predict unobserved outcomes in sequences of feature-outcome observations. Meanwhile, there is interest in predictive inference under more general observation models (e.g., for partially observed features) and for data satisfying more general distributional symmetries (e.g., rotationally invariant or coordinate-independent observations in physics). Here we propose SymmPI, a methodology for predictive inference when data distributions have general group symmetries in arbitrary observation models. Our methods leverage the novel notion of distributional equivariant transformations, which process the data while preserving their distributional invariances. We show that SymmPI has valid coverage under distributional invariance and characterize its performance under distribution shift, recovering recent results as special cases. We apply SymmPI to predict unobserved values associated to vertices in a network, where the distribution is unchanged under relabelings that keep the network structure unchanged. In several simulations in a two-layer hierarchical model, and in an empirical data analysis example, SymmPI performs favorably compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.16160v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Edgar Dobriban, Mengxin Yu</dc:creator>
    </item>
    <item>
      <title>Borrowing from historical control data in a Bayesian time-to-event model with flexible baseline hazard function</title>
      <link>https://arxiv.org/abs/2401.06082</link>
      <description>arXiv:2401.06082v3 Announce Type: replace 
Abstract: There is currently a focus on statistical methods which can use historical trial information to help accelerate the discovery, development and delivery of medicine. Bayesian methods can be constructed so that the borrowing is "dynamic" in the sense that the similarity of the data helps to determine how much information is used. In the time to event setting with one historical data set, a popular model for a range of baseline hazards is the piecewise exponential model where the time points are fixed and a borrowing structure is imposed on the model. Although convenient for implementation this approach effects the borrowing capability of the model. We propose a Bayesian model which allows the time points to vary and a dependency to be placed between the baseline hazards. This serves to smooth the posterior baseline hazard improving both model estimation and borrowing characteristics. We explore a variety of prior structures for the borrowing within our proposed model and assess their performance against established approaches. We demonstrate that this leads to improved type I error in the presence of prior data conflict and increased power. We have developed accompanying software which is freely available and enables easy implementation of the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06082v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Darren A. V. Scott, Alex Lewin</dc:creator>
    </item>
    <item>
      <title>How to achieve model-robust inference in stepped wedge trials with model-based methods?</title>
      <link>https://arxiv.org/abs/2401.15680</link>
      <description>arXiv:2401.15680v4 Announce Type: replace 
Abstract: A stepped wedge design is a unidirectional crossover design where clusters are randomized to distinct treatment sequences. While model-based analysis of stepped wedge designs is standard practice to evaluate treatment effects accounting for clustering and adjusting for covariates, their properties under misspecification have not been systematically explored. In this article, we focus on model-based methods, including linear mixed models and generalized estimating equations with an independence, simple exchangeable, or nested exchangeable working correlation structure. We study when a potentially misspecified working model can offer consistent estimation of the marginal treatment effect estimands, which are defined nonparametrically with potential outcomes and may be functions of calendar time and/or exposure time. We prove a central result that consistency for nonparametric estimands usually requires a correctly specified treatment effect structure, but generally not the remaining aspects of the working model (functional form of covariates, random effects, and residual distribution), and valid inference is obtained via the sandwich variance estimator. Furthermore, an additional g-computation step is required to achieve model-robust inference under non-identity link functions or for ratio estimands. The theoretical results are illustrated via several simulation experiments and re-analysis of a completed stepped wedge cluster randomized trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15680v4</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bingkai Wang, Xueqi Wang, Fan Li</dc:creator>
    </item>
    <item>
      <title>Penalized G-estimation for effect modifier selection in a structural nested mean model for repeated outcomes</title>
      <link>https://arxiv.org/abs/2402.00154</link>
      <description>arXiv:2402.00154v3 Announce Type: replace 
Abstract: Effect modification occurs when the impact of the treatment on an outcome varies based on the levels of other covariates known as effect modifiers. Modeling these effect differences is important for etiological goals and for purposes of optimizing treatment. Structural nested mean models (SNMMs) are useful causal models for estimating the potentially heterogeneous effect of a time-varying exposure on the mean of an outcome in the presence of time-varying confounding. A data-adaptive selection approach is necessary if the effect modifiers are unknown a priori and need to be identified. Although variable selection techniques are available for estimating the conditional average treatment effects using marginal structural models or for developing optimal dynamic treatment regimens, all of these methods consider a single end-of-follow-up outcome. In the context of an SNMM for repeated outcomes, we propose a doubly robust penalized G-estimator for the causal effect of a time-varying exposure with a simultaneous selection of effect modifiers and prove the oracle property of our estimator. We conduct a simulation study for the evaluation of its performance in finite samples and verification of its double-robustness property. Our work is motivated by the study of hemodiafiltration for treating patients with end-stage renal disease at the Centre Hospitalier de l'Universit\'e de Montr\'eal. We apply the proposed method to investigate the effect heterogeneity of dialysis facility on the repeated session-specific hemodiafiltration outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00154v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ajmery Jaman, Guanbo Wang, Ashkan Ertefaie, Mich\`ele Bally, Ren\'ee L\'evesque, Robert W. Platt, Mireille E. Schnitzer</dc:creator>
    </item>
    <item>
      <title>Estimability conditions for complex carryover effects in crossover designs</title>
      <link>https://arxiv.org/abs/2402.16362</link>
      <description>arXiv:2402.16362v2 Announce Type: replace 
Abstract: It has been argued for many years that models used to analyze data from crossover designs are not appropriate when simple carryover effects are assumed. Furthermore, a statistical model that could estimate complex carry-over effects in crossover designs had never been found. However, in this paper, the estimability conditions of the complex carryover effects and a theoretical result that supports them are found. In addition, a simulation example is developed in a non-linear dose-response test for a typical AB/BA crossover design with repeated measures. This simulation shows that a semiparametric model can detect complex carryover effects and that this estimation improves the precision of the estimators of the treatment effect. It is concluded that when there are at least five replicates in each observation period per individual, semiparametric statistical models provide a good estimator of the treatment effect and reduce bias with respect to models that assume the absence of carryover effects or simplex carryover effects. Furthermore, an application of the methodology is shown and the wealth of analysis gained by estimating complex carryover effects is evident.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16362v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>N. A. Cruz, O. O. Melo, C. A. Martinez</dc:creator>
    </item>
    <item>
      <title>Sampling low-fidelity outputs for estimation of high-fidelity density and its tails</title>
      <link>https://arxiv.org/abs/2402.17984</link>
      <description>arXiv:2402.17984v2 Announce Type: replace 
Abstract: In a multifidelity setting, data are available under the same conditions from two (or more) sources, e.g. computer codes, one being lower-fidelity but computationally cheaper, and the other higher-fidelity and more expensive. This work studies for which low-fidelity outputs, one should obtain high-fidelity outputs, if the goal is to estimate the probability density function of the latter, especially when it comes to the distribution tails and extremes. It is suggested to approach this problem from the perspective of the importance sampling of low-fidelity outputs according to some proposal distribution, combined with special considerations for the distribution tails based on extreme value theory. The notion of an optimal proposal distribution is introduced and investigated, in both theory and simulations. The approach is motivated and illustrated with an application to estimate the probability density function of record extremes of ship motions, obtained through two computer codes of different fidelities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17984v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minji Kim, Vladas Pipiras, Kevin O'Connor, Themistoklis Sapsis</dc:creator>
    </item>
    <item>
      <title>Using LASSO for Variable Selection in Exponential Random Graph models</title>
      <link>https://arxiv.org/abs/2407.15674</link>
      <description>arXiv:2407.15674v2 Announce Type: replace 
Abstract: The paper demonstrates the use of LASSO-based estimation in network models. Taking the Exponential Random Graph Model (ERGM) as a flexible and widely used model for network data analysis, the paper focuses on the question of how to specify the (sufficient) statistics, that define the model structure. This includes both, endogenous network statistics (e.g. twostars, triangles, etc.) as well as statistics involving exogenous covariates; on the node as well as on the edge level. LASSO estimation is a penalized estimation that shrinks some of the parameter estimates to be equal to zero. As such it allows for model selection by modifying the amount of penalty. The concept is well established in standard regression and we demonstrate its usage in network data analysis, with the advantage of automatically providing a model selection framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15674v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sergio Buttazzo, G\"oran Kauermann</dc:creator>
    </item>
    <item>
      <title>Directional data analysis using the spherical Cauchy and the Poisson-kernel based distribution</title>
      <link>https://arxiv.org/abs/2409.03292</link>
      <description>arXiv:2409.03292v2 Announce Type: replace 
Abstract: The spherical Cauchy distribution and the Poisson-kernel based distribution were both proposed in 2020, for the analysis of directional data. The paper explores both of them under various frameworks. Alternative parametrizations that offer numerical and estimation advantages, including a straightforward Newton-Raphson algorithm to estimate the parameters are suggested, which further facilitate a more straightforward formulation under the regression setting. A two-sample location test, based on the log-likelihood ratio test is suggested, completing with discriminant analysis. The two distributions are put to the test-bed for all aforementioned cases, through simulation studies and via real data examples comparing and illustrating their performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03292v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michail Tsagris</dc:creator>
    </item>
    <item>
      <title>Consumer Research with Projective Techniques: A Mixed Methods-Focused Review and Empirical Reanalysis</title>
      <link>https://arxiv.org/abs/2409.04995</link>
      <description>arXiv:2409.04995v2 Announce Type: replace 
Abstract: This article gives an integrative review of research using projective methods in the consumer research domain. We give a general historical overview of the use of projective methods, both in psychology and in consumer research applications, and discuss the reliability and validity aspects and measurement for projective techniques. We review the literature on projective techniques in the areas of marketing, hospitality &amp; tourism, and consumer &amp; food science, with a mixed methods research focus on the interplay of qualitative and quantitative techniques. We review the use of several quantitative techniques used for structuring and analyzing projective data and run an empirical reanalysis of previously gathered data. We give recommendations for improved rigor and for potential future work involving mixed methods in projective techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04995v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stephen L. France</dc:creator>
    </item>
    <item>
      <title>Identifiable causal inference with noisy treatment and no side information</title>
      <link>https://arxiv.org/abs/2306.10614</link>
      <description>arXiv:2306.10614v3 Announce Type: replace-cross 
Abstract: In some causal inference scenarios, the treatment variable is measured inaccurately, for instance in epidemiology or econometrics. Failure to correct for the effect of this measurement error can lead to biased causal effect estimates. Previous research has not studied methods that address this issue from a causal viewpoint while allowing for complex nonlinear dependencies and without assuming access to side information. For such a scenario, this study proposes a model that assumes a continuous treatment variable that is inaccurately measured. Building on existing results for measurement error models, we prove that our model's causal effect estimates are identifiable, even without side information and knowledge of the measurement error variance. Our method relies on a deep latent variable model in which Gaussian conditionals are parameterized by neural networks, and we develop an amortized importance-weighted variational objective for training the model. Empirical results demonstrate the method's good performance with unknown measurement error. More broadly, our work extends the range of applications in which reliable causal inference can be conducted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.10614v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antti P\"oll\"anen, Pekka Marttinen</dc:creator>
    </item>
    <item>
      <title>Deep Limit Model-free Prediction in Regression</title>
      <link>https://arxiv.org/abs/2408.09532</link>
      <description>arXiv:2408.09532v3 Announce Type: replace-cross 
Abstract: In this paper, we provide a novel Model-free approach based on Deep Neural Network (DNN) to accomplish point prediction and prediction interval under a general regression setting. Usually, people rely on parametric or non-parametric models to bridge dependent and independent variables (Y and X). However, this classical method relies heavily on the correct model specification. Even for the non-parametric approach, some additive form is often assumed. A newly proposed Model-free prediction principle sheds light on a prediction procedure without any model assumption. Previous work regarding this principle has shown better performance than other standard alternatives. Recently, DNN, one of the machine learning methods, has received increasing attention due to its great performance in practice. Guided by the Model-free prediction idea, we attempt to apply a fully connected forward DNN to map X and some appropriate reference random variable Z to Y. The targeted DNN is trained by minimizing a specially designed loss function so that the randomness of Y conditional on X is outsourced to Z through the trained DNN. Our method is more stable and accurate compared to other DNN-based counterparts, especially for optimal point predictions. With a specific prediction procedure, our prediction interval can capture the estimation variability so that it can render a better coverage rate for finite sample cases. The superior performance of our method is verified by simulation and empirical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09532v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kejin Wu, Dimitris N. Politis</dc:creator>
    </item>
  </channel>
</rss>
