<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 01 Nov 2024 04:01:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Topology optimization of contact-aided compliant mechanisms for tracing multi-kink paths</title>
      <link>https://arxiv.org/abs/2410.23714</link>
      <description>arXiv:2410.23714v1 Announce Type: new 
Abstract: This paper presents a topology optimization approach to design 2D contact-aided compliant mechanisms (CCMs) that can trace the desired output paths with more than one kink while experiencing self and/or external contacts. Such CCMs can be used as mechanical compliant switches. Hexagonal elements are used to parameterize the design domain. Negative circular masks are employed to remove material beneath them and generate rigid contact surfaces. Each mask is assigned five design variables. The first three decide the location and radius of the mask, whereas the last two determine the presence of the contact surface and its radius. To ensure continuity in contacting surfaces' normal, we employ a boundary smoothing scheme. The augmented Lagrange multiplier method is employed to incorporate self and mutual contact. An objective is formulated using the Fourier shape descriptors with the permitted resource constraint. The hill-climber optimization technique is utilized to update the design variables. An in-house code is developed for the entire process. To demonstrate the method's efficacy, a CCM is optimized with a two-kink path. The desired and obtained paths are compared.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23714v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Prabhat Kumar, Roger A Sauer, Anupam Saxena</dc:creator>
    </item>
    <item>
      <title>Self-supervised Learning for Glass Property Screening</title>
      <link>https://arxiv.org/abs/2410.24083</link>
      <description>arXiv:2410.24083v1 Announce Type: new 
Abstract: This paper presents a novel approach to glass composition screening through a self-supervised learning framework, addressing the challenges posed by glass transition temperature (Tg) prediction. Given the critical role of Tg in determining glass performance across various applications, we reformulate the composition screening task as a classification problem, allowing for direct prediction of whether specific compositional samples fall within a designated Tg range. Our model leverages advanced self-supervised learning techniques to optimize for the area under the curve (AUC) metric, mitigating the adverse effects of noise and class imbalances in training data. We introduce a data augmentation method based on the law of large numbers to enhance sample size and improve noise robustness. Additionally, our DeepGlassNet backbone encoder captures intricate second-order and higher-order interactions among components, providing insights into their collective impact on glass properties. We validate our approach using data from the SciGlass database, demonstrating its capability to accurately predict Tg for compositions within the specified range, while also exploring extrapolation to untested samples. This work not only enhances the accuracy of glass composition screening but also offers scalable solutions applicable to material screening across various fields, thereby advancing the development of novel materials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.24083v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Meijing Chen, Bin Liu, Ying Liu, Tianrui Li</dc:creator>
    </item>
    <item>
      <title>Novel Architecture for Distributed Travel Data Integration and Service Provision Using Microservices</title>
      <link>https://arxiv.org/abs/2410.24174</link>
      <description>arXiv:2410.24174v1 Announce Type: new 
Abstract: This paper introduces a microservices architecture for the purpose of enhancing the flexibility and performance of an airline reservation system. The architectural design incorporates Redis cache technologies, two different messaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and PostgreSQL). It also introduces authorization techniques, including secure communication through OAuth2 and JWT which is essential with the management of high-demand travel services. According to selected indicators, the architecture provides an impressive level of data consistency at 99.5% and a latency of data propagation of less than 75 ms allowing rapid and reliable intercommunication between microservices. A system throughput of 1050 events per second was achieved so that the acceptability level was maintained even during peak time. Redis caching reduced a 92% cache hit ratio on the database thereby lowering the burden on the database and increasing the speed of response. Further improvement of the systems scalability was done through the use of Docker and Kubernetes which enabled services to be expanded horizontally to cope with the changes in demand. The error rates were very low, at 0.2% further enhancing the efficiency of the system in handling real-time data integration. This approach is suggested to meet the specific needs of the airline reservation system. It is secure, fast, scalable, all serving to improve the user experience as well as the efficiency of operations. The low latency and high data integration levels and prevaiing efficient usage of the resources demonstrates the architecture ability to offer continued support in the ever growing high demand situations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.24174v1</guid>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <category>cs.DC</category>
      <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Biman Barua, M. Shamim Kaiser</dc:creator>
    </item>
    <item>
      <title>Non-Hydrostatic Model for Simulating Moving Bottom-Generated Waves: A Shallow Water Extension with Quadratic Vertical Pressure Profile</title>
      <link>https://arxiv.org/abs/2410.23707</link>
      <description>arXiv:2410.23707v1 Announce Type: cross 
Abstract: We formulate a depth-averaged non-hydrostatic model to solve wave equations with generation by a moving bottom. This model is built upon the shallow water equations, which are widely used in tsunami wave modelling. An extension leads to two additional unknowns to be solved: vertical momentum and non-hydrostatic pressure. We show that a linear vertical velocity assumption turns out to give us a quadratic pressure relation, which is equivalent to Boussinesq-type equations. However, this extension involves a time derivative of an unknown parameter, rendering the solution by a projection method ambiguous. In this study, we derive an alternative form of the elliptic system of equations to avoid such ambiguity. The new set of equations satisfies the desired solubility property, while also consistently representing the non-flat moving topography wave generation. Validations are performed using several test cases based on previous experiments and a high-fidelity simulation. First, we show the efficiency of our model in solving a vertical movement, which represents an undersea earthquake-generated tsunami. Following that, we demonstrate the accuracy of the model for landslide-generated waves. Finally, we compare the performance of our novel set of equations with the linear and simplified quadratic pressure profiles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23707v1</guid>
      <category>math.NA</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kemal Firdaus, J\"orn Behrens</dc:creator>
    </item>
    <item>
      <title>From PDFs to Structured Data: Utilizing LLM Analysis in Sports Database Management</title>
      <link>https://arxiv.org/abs/2410.17619</link>
      <description>arXiv:2410.17619v2 Announce Type: replace 
Abstract: This study investigates the effectiveness of Large Language Models (LLMs) in processing semi-structured data from PDF documents into structured formats, specifically examining their application in updating the Finnish Sports Clubs Database. Through action research methodology, we developed and evaluated an AI-assisted approach utilizing OpenAI's GPT-4 and Anthropic's Claude 3 Opus models to process data from 72 sports federation membership reports. The system achieved a 90% success rate in automated processing, successfully handling 65 of 72 files without errors and converting over 7,900 rows of data. While the initial development time was comparable to traditional manual processing (three months), the implemented system shows potential for reducing future processing time by approximately 90%. Key challenges included handling multilingual content, processing multi-page datasets, and managing extraneous information. The findings suggest that while LLMs demonstrate significant potential for automating semi-structured data processing tasks, optimal results are achieved through a hybrid approach combining AI automation with selective human oversight. This research contributes to the growing body of literature on practical LLM applications in organizational data management and provides insights into the transformation of traditional data processing workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17619v2</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juhani Merilehto</dc:creator>
    </item>
    <item>
      <title>Linking Across Data Granularity: Fitting Multivariate Hawkes Processes to Partially Interval-Censored Data</title>
      <link>https://arxiv.org/abs/2111.02062</link>
      <description>arXiv:2111.02062v4 Announce Type: replace-cross 
Abstract: The multivariate Hawkes process (MHP) is widely used for analyzing data streams that interact with each other, where events generate new events within their own dimension (via self-excitation) or across different dimensions (via cross-excitation). However, in certain applications, the timestamps of individual events in some dimensions are unobservable, and only event counts within intervals are known, referred to as partially interval-censored data. The MHP is unsuitable for handling such data since its estimation requires event timestamps. In this study, we introduce the Partially Censored Multivariate Hawkes Process (PCMHP), a novel point process which shares parameter equivalence with the MHP and can effectively model both timestamped and interval-censored data. We demonstrate the capabilities of the PCMHP using synthetic and real-world datasets. Firstly, we illustrate that the PCMHP can approximate MHP parameters and recover the spectral radius using synthetic event histories. Next, we assess the performance of the PCMHP in predicting YouTube popularity and find that the PCMHP outperforms the popularity estimation algorithm Hawkes Intensity Process (HIP). Comparing with the fully interval-censored HIP, we show that the PCMHP improves prediction performance by accounting for point process dimensions, particularly when there exist significant cross-dimension interactions. Lastly, we leverage the PCMHP to gain qualitative insights from a dataset comprising daily COVID-19 case counts from multiple countries and COVID-19-related news articles. By clustering the PCMHP-modeled countries, we unveil hidden interaction patterns between occurrences of COVID-19 cases and news reporting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.02062v4</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Pio Calderon, Alexander Soen, Marian-Andrei Rizoiu</dc:creator>
    </item>
    <item>
      <title>Debiasing Alternative Data for Credit Underwriting Using Causal Inference</title>
      <link>https://arxiv.org/abs/2410.22382</link>
      <description>arXiv:2410.22382v2 Announce Type: replace-cross 
Abstract: Alternative data provides valuable insights for lenders to evaluate a borrower's creditworthiness, which could help expand credit access to underserved groups and lower costs for borrowers. But some forms of alternative data have historically been excluded from credit underwriting because it could act as an illegal proxy for a protected class like race or gender, causing redlining. We propose a method for applying causal inference to a supervised machine learning model to debias alternative data so that it might be used for credit underwriting. We demonstrate how our algorithm can be used against a public credit dataset to improve model accuracy across different racial groups, while providing theoretically robust nondiscrimination guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22382v2</guid>
      <category>q-fin.RM</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chris Lam</dc:creator>
    </item>
  </channel>
</rss>
