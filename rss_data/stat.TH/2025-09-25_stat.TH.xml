<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 26 Sep 2025 01:43:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Measuring Partial Exchangeability with Reproducing Kernel Hilbert Spaces</title>
      <link>https://arxiv.org/abs/2509.20221</link>
      <description>arXiv:2509.20221v1 Announce Type: new 
Abstract: In Bayesian multilevel models, the data are structured in interconnected groups, and their posteriors borrow information from one another due to prior dependence between latent parameters. However, little is known about the behaviour of the dependence a posteriori. In this work, we develop a general framework for measuring partial exchangeability for parametric and nonparametric models, both a priori and a posteriori. We define an index that detects exchangeability for common models, is invariant by reparametrization, can be estimated through samples, and, crucially, is well-suited for posteriors. We achieve these properties through the use of Reproducing Kernel Hilbert Spaces, which map any random probability to a random object on a Hilbert space. This leads to many convenient properties and tractable expressions, especially a priori and under mixing. We apply our general framework to i) investigate the dependence a posteriori for the hierarchical Dirichlet process, retrieving a parametric convergence rate under very mild assumptions on the data; ii) eliciting the dependence structure of a parametric model for a principled comparison with a nonparametric alternative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20221v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marta Catalano, Hugo Lavenant, Francesco Mascari</dc:creator>
    </item>
    <item>
      <title>Indirect Statistical Inference with Guaranteed Necessity and Sufficiency</title>
      <link>https://arxiv.org/abs/2509.20249</link>
      <description>arXiv:2509.20249v1 Announce Type: new 
Abstract: This paper develops a new framework for indirect statistical inference with guaranteed necessity and sufficiency, applicable to continuous random variables. We prove that when comparing exponentially transformed order statistics from an assumed distribution with those from simulated unit exponential samples, the ranked quotients exhibit distinct asymptotics: the left segment converges to a non-degenerate distribution, while the middle and right segments degenerate to one. This yields a necessary and sufficient condition in probability for two sequences of continuous random variables to follow the same distribution. Building on this, we propose an optimization criterion based on relative errors between ordered samples. The criterion achieves its minimum if and only if the assumed and true distributions coincide, providing a second necessary and sufficient condition in optimization. These dual NS properties, rare in the literature, establish a fundamentally stronger inference framework than existing methods. Unlike classical approaches based on absolute errors (e.g., Kolmogorov-Smirnov), NSE exploits relative errors to ensure faster convergence, requires only mild approximability of the cumulative distribution function, and provides both point and interval estimates. Simulations and real-data applications confirm NSE's superior performance in preserving distributional assumptions where traditional methods fail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20249v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Z Zhang, X Hu, C Lu, T Liu</dc:creator>
    </item>
    <item>
      <title>Analyzing Uncertainty Quantification in Statistical and Deep Learning Models for Probabilistic Electricity Price Forecasting</title>
      <link>https://arxiv.org/abs/2509.19417</link>
      <description>arXiv:2509.19417v1 Announce Type: cross 
Abstract: Precise probabilistic forecasts are fundamental for energy risk management, and there is a wide range of both statistical and machine learning models for this purpose. Inherent to these probabilistic models is some form of uncertainty quantification. However, most models do not capture the full extent of uncertainty, which arises not only from the data itself but also from model and distributional choices. In this study, we examine uncertainty quantification in state-of-the-art statistical and deep learning probabilistic forecasting models for electricity price forecasting in the German market. In particular, we consider deep distributional neural networks (DDNNs) and augment them with an ensemble approach, Monte Carlo (MC) dropout, and conformal prediction to account for model uncertainty. Additionally, we consider the LASSO-estimated autoregressive (LEAR) approach combined with quantile regression averaging (QRA), generalized autoregressive conditional heteroskedasticity (GARCH), and conformal prediction. Across a range of performance metrics, we find that the LEAR-based models perform well in terms of probabilistic forecasting, irrespective of the uncertainty quantification method. Furthermore, we find that DDNNs benefit from incorporating both data and model uncertainty, improving both point and probabilistic forecasting. Uncertainty itself appears to be best captured by the models using conformal prediction. Overall, our extensive study shows that all models under consideration perform competitively. However, their relative performance depends on the choice of metrics for point and probabilistic forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19417v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Lebedev, Abhinav Das, Sven Pappert, Stephan Schl\"uter</dc:creator>
    </item>
    <item>
      <title>Forecasting High Dimensional Time Series with Dynamic Dimension Reduction</title>
      <link>https://arxiv.org/abs/2509.19418</link>
      <description>arXiv:2509.19418v1 Announce Type: cross 
Abstract: Many dimension reduction techniques have been developed for independent data, and most have also been extended to time series. However, these methods often fail to account for the dynamic dependencies both within and across series. In this work, we propose a general framework for forecasting high-dimensional time series that integrates dynamic dimension reduction with regularization techniques. The effectiveness of the proposed approach is illustrated through a simulated example and a forecasting application using an economic dataset. We show that several specific methods are encompassed within this framework, including Dynamic Principal Components and Reduced Rank Autoregressive Models. Furthermore, time-domain formulations of Dynamic Canonical Correlation and Dynamic Redundancy Analysis are introduced here for the first time as particular instances of the proposed methodology. All of these techniques are analyzed as special cases of a unified procedure, enabling a coherent derivation and interpretation across methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19418v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Pe\~na, Victor J. Yohai</dc:creator>
    </item>
    <item>
      <title>Hierarchical Bayesian Operator-induced Symbolic Regression Trees for Structural Learning of Scientific Expressions</title>
      <link>https://arxiv.org/abs/2509.19710</link>
      <description>arXiv:2509.19710v1 Announce Type: cross 
Abstract: The advent of Scientific Machine Learning has heralded a transformative era in scientific discovery, driving progress across diverse domains. Central to this progress is uncovering scientific laws from experimental data through symbolic regression. However, existing approaches are dominated by heuristic algorithms or data-hungry black-box methods, which often demand low-noise settings and lack principled uncertainty quantification. Motivated by interpretable Statistical Artificial Intelligence, we develop a hierarchical Bayesian framework for symbolic regression that represents scientific laws as ensembles of tree-structured symbolic expressions endowed with a regularized tree prior. This coherent probabilistic formulation enables full posterior inference via an efficient Markov chain Monte Carlo algorithm, yielding a balance between predictive accuracy and structural parsimony. To guide symbolic model selection, we develop a marginal posterior-based criterion adhering to the Occam's window principle and further quantify structural fidelity to ground truth through a tailored expression-distance metric. On the theoretical front, we establish near-minimax rate of Bayesian posterior concentration, providing the first rigorous guarantee in context of symbolic regression. Empirical evaluation demonstrates robust performance of our proposed methodology against state-of-the-art competing modules on a simulated example, a suite of canonical Feynman equations, and single-atom catalysis dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19710v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Somjit Roy, Pritam Dey, Debdeep Pati, Bani K. Mallick</dc:creator>
    </item>
    <item>
      <title>Pure Exploration via Frank-Wolfe Self-Play</title>
      <link>https://arxiv.org/abs/2509.19901</link>
      <description>arXiv:2509.19901v1 Announce Type: cross 
Abstract: We study pure exploration in structured stochastic multi-armed bandits, aiming to efficiently identify the correct hypothesis from a finite set of alternatives. For a broad class of tasks, asymptotic analyses reduce to a maximin optimization that admits a two-player zero-sum game interpretation between an experimenter and a skeptic: the experimenter allocates measurements to rule out alternatives while the skeptic proposes alternatives. We reformulate the game by allowing the skeptic to adopt a mixed strategy, yielding a concave-convex saddle-point problem. This viewpoint leads to Frank-Wolfe Self-Play (FWSP): a projection-free, regularization-free, tuning-free method whose one-hot updates on both sides match the bandit sampling paradigm. However, structural constraints introduce sharp pathologies that complicate algorithm design and analysis: our linear-bandit case study exhibits nonunique optima, optimal designs with zero mass on the best arm, bilinear objectives, and nonsmoothness at the boundary. We address these challenges via a differential-inclusion argument, proving convergence of the game value for best-arm identification in linear bandits. Our analysis proceeds through a continuous-time limit: a differential inclusion with a Lyapunov function that decays exponentially, implying a vanishing duality gap and convergence to the optimal value. Although Lyapunov analysis requires differentiability of the objective, which is not guaranteed on the boundary, we show that along continuous trajectories the algorithm steers away from pathological nonsmooth points and achieves uniform global convergence to the optimal game value. We then embed the discrete-time updates into a perturbed flow and show that the discrete game value also converges. Building on FWSP, we further propose a learning algorithm based on posterior sampling. Numerical experiments demonstrate a vanishing duality gap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19901v1</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyu Liu, Chao Qin, Wei You</dc:creator>
    </item>
    <item>
      <title>Understanding the ratio of the partition sum to its Bethe approximation via double covers</title>
      <link>https://arxiv.org/abs/2509.19910</link>
      <description>arXiv:2509.19910v1 Announce Type: cross 
Abstract: For various classes of graphical models it has been observed that the ratio of the partition sum to its Bethe approximation is often close to being the square of the ratio of the partition sum to its degree-2 Bethe approximation. This is of relevance because the latter ratio can often better be analyzed and/or quantified than the former ratio. In this paper, we give some justifications for the observed relationship between these two ratios and then analyze these ratios for two classes of log-supermodular graphical models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19910v1</guid>
      <category>cs.IT</category>
      <category>math.CO</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pascal O. Vontobel</dc:creator>
    </item>
    <item>
      <title>First-Extinction Law for Resampling Processes</title>
      <link>https://arxiv.org/abs/2509.20101</link>
      <description>arXiv:2509.20101v1 Announce Type: cross 
Abstract: Extinction times in resampling processes are fundamental yet often intractable, as previous formulas scale as $2^M$ with the number of states $M$ present in the initial probability distribution. We solve this by treating multinomial updates as independent square-root diffusions of zero drift, yielding a closed-form law for the first-extinction time. We prove that the mean coincides exactly with the Wright-Fisher result of Baxter et al., thereby replacing exponential-cost evaluations with a linear-cost expression, and we validate this result through extensive simulations. Finally, we demonstrate predictive power for model collapse in a simple self-training setup: the onset of collapse coincides with the resampling-driven first-extinction time computed from the model's initial stationary distribution. These results hint to a unified view of resampling extinction dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20101v1</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>physics.data-an</category>
      <category>q-bio.PE</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Benati, Alessandro Londei, Denise Lanzieri, Vittorio Loreto</dc:creator>
    </item>
    <item>
      <title>Monitoring Violations of Differential Privacy over Time</title>
      <link>https://arxiv.org/abs/2509.20283</link>
      <description>arXiv:2509.20283v1 Announce Type: cross 
Abstract: Auditing differential privacy has emerged as an important area of research that supports the design of privacy-preserving mechanisms. Privacy audits help to obtain empirical estimates of the privacy parameter, to expose flawed implementations of algorithms and to compare practical with theoretical privacy guarantees. In this work, we investigate an unexplored facet of privacy auditing: the sustained auditing of a mechanism that can go through changes during its development or deployment. Monitoring the privacy of algorithms over time comes with specific challenges. Running state-of-the-art (static) auditors repeatedly requires excessive sampling efforts, while the reliability of such methods deteriorates over time without proper adjustments. To overcome these obstacles, we present a new monitoring procedure that extracts information from the entire deployment history of the algorithm. This allows us to reduce sampling efforts, while sustaining reliable outcomes of our auditor. We derive formal guarantees with regard to the soundness of our methods and evaluate their performance for important mechanisms from the literature. Our theoretical findings and experiments demonstrate the efficacy of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20283v1</guid>
      <category>cs.CR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\"Onder Askin, Tim Kutta, Holger Dette</dc:creator>
    </item>
    <item>
      <title>Alignment-Sensitive Minimax Rates for Spectral Algorithms with Learned Kernels</title>
      <link>https://arxiv.org/abs/2509.20294</link>
      <description>arXiv:2509.20294v1 Announce Type: cross 
Abstract: We study spectral algorithms in the setting where kernels are learned from data. We introduce the effective span dimension (ESD), an alignment-sensitive complexity measure that depends jointly on the signal, spectrum, and noise level $\sigma^2$. The ESD is well-defined for arbitrary kernels and signals without requiring eigen-decay conditions or source conditions. We prove that for sequence models whose ESD is at most $K$, the minimax excess risk scales as $\sigma^2 K$. Furthermore, we analyze over-parameterized gradient flow and prove that it can reduce the ESD. This finding establishes a connection between adaptive feature learning and provable improvements in generalization of spectral algorithms. We demonstrate the generality of the ESD framework by extending it to linear models and RKHS regression, and we support the theory with numerical experiments. This framework provides a novel perspective on generalization beyond traditional fixed-kernel theories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20294v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongming Huang, Zhifan Li, Yicheng Li, Qian Lin</dc:creator>
    </item>
    <item>
      <title>The empirical copula process in high dimensions: Stute's representation and applications</title>
      <link>https://arxiv.org/abs/2405.05597</link>
      <description>arXiv:2405.05597v3 Announce Type: replace 
Abstract: The empirical copula process, a fundamental tool for copula inference, is studied in the high dimensional regime where the dimension is allowed to grow to infinity exponentially in the sample size. Under natural, weak smoothness assumptions on the underlying copula, it is shown that Stute's representation is valid in the following sense: all low-dimensional margins of fixed dimension of the empirical copula process can be approximated by a functional of the low-dimensional margins of the standard empirical process, with the almost sure error term being uniform in the margins. The result has numerous potential applications, and is exemplary applied to the problem of testing pairwise stochastic independence in high dimensions, leading to various extensions of recent results in the literature: for certain test statistics based on pairwise association measures, type-I error control is obtained for models beyond mutual independence. Moreover, bootstrap-based critical values are shown to yield strong control of the familywise error rate for a large class of data generating processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05597v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Axel B\"ucher, Cambyse Pakzad</dc:creator>
    </item>
    <item>
      <title>Stein's unbiased risk estimate and Hyv\"arinen's score matching</title>
      <link>https://arxiv.org/abs/2502.20123</link>
      <description>arXiv:2502.20123v2 Announce Type: replace 
Abstract: Given a collection of observed signals corrupted with Gaussian noise, how can we learn to optimally denoise them? This fundamental problem arises in both empirical Bayes and generative modeling. In empirical Bayes, the predominant approach is via nonparametric maximum likelihood estimation (NPMLE), while in generative modeling, score matching (SM) methods have proven very successful. In our setting, Hyv\"arinen's implicit SM is equivalent to another classical idea from statistics -- Stein's Unbiased Risk Estimate (SURE). Revisiting SURE minimization, we establish, for the first time, that SURE achieves nearly parametric rates of convergence of the regret in the classical empirical Bayes setting with homoscedastic noise. We also prove that SURE-training can achieve fast rates of convergence to the oracle denoiser in a commonly studied misspecified model. In contrast, the NPMLE may not even converge to the oracle denoiser under misspecification of the class of signal distributions. We show how to practically implement our method in settings involving heteroscedasticity and side-information, such as in an application to the estimation of economic mobility in the Opportunity Atlas. Our empirical results demonstrate the superior performance of SURE-training over NPMLE under misspecification. Collectively, our findings advance SURE/SM as a strong alternative to the NPMLE for empirical Bayes problems in both theory and practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20123v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sulagna Ghosh, Nikolaos Ignatiadis, Frederic Koehler, Amber Lee</dc:creator>
    </item>
    <item>
      <title>Bayesian Geostatistics Using Predictive Stacking</title>
      <link>https://arxiv.org/abs/2304.12414</link>
      <description>arXiv:2304.12414v4 Announce Type: replace-cross 
Abstract: We develop Bayesian predictive stacking for geostatistical models, where the primary inferential objective is to provide inference on the latent spatial random field and conduct spatial predictions at arbitrary locations. We exploit analytically tractable posterior distributions for regression coefficients of predictors and the realizations of the spatial process conditional upon process parameters. We subsequently combine such inference by stacking these models across the range of values of the hyper-parameters. We devise stacking of means and posterior densities in a manner that is computationally efficient without resorting to iterative algorithms such as Markov chain Monte Carlo (MCMC) and can exploit the benefits of parallel computations. We offer novel theoretical insights into the resulting inference within an infill asymptotic paradigm and through empirical results showing that stacked inference is comparable to full sampling-based Bayesian inference at a significantly lower computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.12414v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lu Zhang, Wenpin Tang, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>Characterization of the asymptotic behavior of $U$-statistics on row-column exchangeable matrices</title>
      <link>https://arxiv.org/abs/2401.07876</link>
      <description>arXiv:2401.07876v4 Announce Type: replace-cross 
Abstract: We consider $U$-statistics on row-column exchangeable matrices, arrays invariant to separate permutations of rows and columns and common in bipartite data. Under the standard dissociation assumption, we develop a graph-indexed analogue of the Hoeffding decomposition tailored to RCE dependence. We present a new decomposition based on orthogonal projections onto probability spaces generated by sets of Aldous-Hoover-Kallenberg variables. These sets are indexed by bipartite graphs, enabling the application of graph-theoretic concepts to describe the decomposition. This framework provides new insights into the characterization of $U$-statistics on row-column exchangeable matrices, particularly their asymptotic behavior, including in degenerate cases. Notably, the limit distribution depends only on specific terms in the decomposition, corresponding to non-zero components indexed by the smallest graphs, namely the principal support graphs. We show that the asymptotic behavior of a $U$-statistic is characterized by the properties of its principal support graphs. The number of nodes in these graphs (the principal degree) dictates the convergence rate to the limit distribution, with degeneracy occurring if and only if this number is strictly greater than 1. Furthermore, when the principal support graphs are connected, the limit distribution is Gaussian, even in degenerate cases. Applications to network analysis illustrate these findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07876v4</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>T\^am Le Minh</dc:creator>
    </item>
    <item>
      <title>Sparse Max-Affine Regression</title>
      <link>https://arxiv.org/abs/2411.02225</link>
      <description>arXiv:2411.02225v2 Announce Type: replace-cross 
Abstract: This paper presents Sparse Gradient Descent as a solution for variable selection in convex piecewise linear regression, where the model is given as the maximum of $k$-affine functions $ x \mapsto \max_{j \in [k]} \langle a_j^\star, x \rangle + b_j^\star$ for $j = 1,\dots,k$. Here, $\{ a_j^\star\}_{j=1}^k$ and $\{b_j^\star\}_{j=1}^k$ denote the ground-truth weight vectors and intercepts. A non-asymptotic local convergence analysis is provided for Sp-GD under sub-Gaussian noise when the covariate distribution satisfies the sub-Gaussianity and anti-concentration properties. When the model order and parameters are fixed, Sp-GD provides an $\epsilon$-accurate estimate given $\mathcal{O}(\max(\epsilon^{-2}\sigma_z^2,1)s\log(d/s))$ observations where $\sigma_z^2$ denotes the noise variance. This also implies the exact parameter recovery by Sp-GD from $\mathcal{O}(s\log(d/s))$ noise-free observations. The proposed initialization scheme uses sparse principal component analysis to estimate the subspace spanned by $\{ a_j^\star\}_{j=1}^k$, then applies an $r$-covering search to estimate the model parameters. A non-asymptotic analysis is presented for this initialization scheme when the covariates and noise samples follow Gaussian distributions. When the model order and parameters are fixed, this initialization scheme provides an $\epsilon$-accurate estimate given $\mathcal{O}(\epsilon^{-2}\max(\sigma_z^4,\sigma_z^2,1)s^2\log^4(d))$ observations. A new transformation named Real Maslov Dequantization (RMD) is proposed to transform sparse generalized polynomials into sparse max-affine models. The error decay rate of RMD is shown to be exponentially small in its temperature parameter. Furthermore, theoretical guarantees for Sp-GD are extended to the bounded noise model induced by RMD. Numerical Monte Carlo results corroborate theoretical findings for Sp-GD and the initialization scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02225v2</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haitham Kanj, Seonho Kim, Kiryung Lee</dc:creator>
    </item>
    <item>
      <title>Wrapped Gaussian on the manifold of Symmetric Positive Definite Matrices</title>
      <link>https://arxiv.org/abs/2502.01512</link>
      <description>arXiv:2502.01512v4 Announce Type: replace-cross 
Abstract: Circular and non-flat data distributions are prevalent across diverse domains of data science, yet their specific geometric structures often remain underutilized in machine learning frameworks. A principled approach to accounting for the underlying geometry of such data is pivotal, particularly when extending statistical models, like the pervasive Gaussian distribution. In this work, we tackle those issue by focusing on the manifold of symmetric positive definite (SPD) matrices, a key focus in information geometry. We introduce a non-isotropic wrapped Gaussian by leveraging the exponential map, we derive theoretical properties of this distribution and propose a maximum likelihood framework for parameter estimation. Furthermore, we reinterpret established classifiers on SPD through a probabilistic lens and introduce new classifiers based on the wrapped Gaussian model. Experiments on synthetic and real-world datasets demonstrate the robustness and flexibility of this geometry-aware distribution, underscoring its potential to advance manifold-based data analysis. This work lays the groundwork for extending classical machine learning and statistical methods to more complex and structured data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01512v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thibault de Surrel, Fabien Lotte, Sylvain Chevallier, Florian Yger</dc:creator>
    </item>
    <item>
      <title>A Scalable Nystr\"om-Based Kernel Two-Sample Test with Permutations</title>
      <link>https://arxiv.org/abs/2502.13570</link>
      <description>arXiv:2502.13570v3 Announce Type: replace-cross 
Abstract: Two-sample hypothesis testing-determining whether two sets of data are drawn from the same distribution-is a fundamental problem in statistics and machine learning with broad scientific applications. In the context of nonparametric testing, maximum mean discrepancy (MMD) has gained popularity as a test statistic due to its flexibility and strong theoretical foundations. However, its use in large-scale scenarios is plagued by high computational costs. In this work, we use a Nystr\"om approximation of the MMD to design a computationally efficient and practical testing algorithm while preserving statistical guarantees. Our main result is a finite-sample bound on the power of the proposed test for distributions that are sufficiently separated with respect to the MMD. The derived separation rate matches the known minimax optimal rate in this setting. We support our findings with a series of numerical experiments, emphasizing applicability to realistic scientific data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13570v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antoine Chatalic, Marco Letizia, Nicolas Schreuder, Lorenzo Rosasco</dc:creator>
    </item>
    <item>
      <title>Detecting Correlation Efficiently in Stochastic Block Models: Breaking Otter's Threshold in the Entire Supercritical Regime</title>
      <link>https://arxiv.org/abs/2503.06464</link>
      <description>arXiv:2503.06464v2 Announce Type: replace-cross 
Abstract: Consider a pair of sparse correlated stochastic block models $\mathcal S(n,\tfrac{\lambda}{n},\epsilon;s)$ subsampled from a common parent stochastic block model with two symmetric communities, average degree $\lambda=O(1)$, divergence parameter $\epsilon\in (0,1)$ and subsampling probability $s$. For all $\epsilon\in(0,1)$ and $\Delta&gt;0$, we construct a statistic based on the combination of two low-degree polynomials and show that there exists a sufficiently small constant $\delta=\delta(\epsilon,\lambda,\Delta)&gt;0$ such that if $\epsilon^2 \lambda s&gt;1+\Delta$ and $s&gt;\sqrt{\alpha}-\delta$ where $\alpha\approx 0.338$ is Otter's constant, this statistic can distinguish this model and a pair of independent stochastic block models $\mathcal S(n,\tfrac{\lambda s}{n},\epsilon)$ with probability $1-o(1)$. We also provide an efficient algorithm that approximates this statistic in polynomial time.
  The crux of our statistic's construction lies in a carefully curated family of multigraphs called \emph{decorated trees}, which enables effective aggregation of the community signal and graph correlation by leveraging the counts of the same decorated tree while suppressing the undesirable correlations among counts of different decorated trees. We believe such construction may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06464v2</guid>
      <category>cs.DS</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guanyi Chen, Jian Ding, Shuyang Gong, Zhangsong Li</dc:creator>
    </item>
    <item>
      <title>Publication Design with Incentives in Mind</title>
      <link>https://arxiv.org/abs/2504.21156</link>
      <description>arXiv:2504.21156v2 Announce Type: replace-cross 
Abstract: The publication process both determines which research receives the most attention, and influences the supply of research through its impact on researchers' private incentives. We introduce a framework to study optimal publication decisions when researchers can choose (i) whether or how to conduct a study and (ii) whether or how to manipulate the research findings (e.g., via selective reporting or data manipulation). When manipulation is not possible, but research entails substantial private costs for the researchers, it may be optimal to incentivize cheaper research designs even if they are less accurate. When manipulation is possible, it is optimal to publish some manipulated results, as well as results that would have not received attention in the absence of manipulability. Even if it is possible to deter manipulation, such as by requiring pre-registered experiments instead of (potentially manipulable) observational studies, it is suboptimal to do so when experiments entail high research costs. We illustrate the implications of our model in an application to medical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21156v2</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ravi Jagadeesan, Davide Viviano</dc:creator>
    </item>
    <item>
      <title>Modeling Innovation Ecosystem Dynamics through Interacting Reinforced Bernoulli Processes</title>
      <link>https://arxiv.org/abs/2505.13364</link>
      <description>arXiv:2505.13364v2 Announce Type: replace-cross 
Abstract: Understanding how capabilities evolve into core capabilities-and how core capabilities may ossify into rigidities-is central to innovation strategy (Leonard-Barton 1992, Teece 2009). A major challenge in formalizing this process lies in the interactive nature of innovation: successes in one domain often reshape others, endogenizing specialization and complicating isolated modeling. This is especially true in ecosystems where firm capabilities and innovation outcomes hinge on managing interdependencies and complementarities (Jacobides, Cennamo and Gawer 2018, 2024).
  To address this, we propose a novel formal model based on interacting reinforced Bernoulli processes. This framework captures how patent successes propagate across technological categories and how these categories co-evolve. The model is able to jointly account for several stylized facts in the empirical innovation literature, including sublinear success growth (successprobability decay), convergence of success shares across fields, and diminishing cross-category correlations over time.
  Empirical validation using GLOBAL PATSTAT (1980-2018) supports the theoretical predictions. We estimate the structural parameters of the interaction matrix and we also propose a statistical procedure to make inference on the intensity of cross-category interactions under the mean-field assumption.
  By endogenizing technological specialization, our model provides a strategic tool for policymakers and managers, supporting decision-making in complex, co-evolving innovation ecosystems-where targeted interventions can produce systemic effects, influencing competitive trajectories and shaping long-term patterns of specialization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13364v2</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giacomo Aletti, Irene Crimaldi, Andrea Ghiglietti, Federico Nutarelli</dc:creator>
    </item>
    <item>
      <title>A Generative Conditional Distribution Equality Testing Framework and Its Minimax Analysis</title>
      <link>https://arxiv.org/abs/2509.17729</link>
      <description>arXiv:2509.17729v2 Announce Type: replace-cross 
Abstract: In this paper, we propose a general framework for testing the equality of the conditional distributions in a two-sample problem. This problem is most relevant to transfer learning under covariate shift. Our framework is built on neural network-based generative methods and sample splitting techniques by transforming the conditional distribution testing problem into an unconditional one. We introduce two special tests: the generative permutation-based conditional distribution equality test and the generative classification accuracy-based conditional distribution equality test. Theoretically, we establish a minimax lower bound for statistical inference in testing the equality of two conditional distributions under certain smoothness conditions. We demonstrate that the generative permutation-based conditional distribution equality test and its modified version can attain this lower bound precisely or up to some iterated logarithmic factor. Moreover, we prove the testing consistency of the generative classification accuracy-based conditional distribution equality test. We also establish the convergence rate for the learned conditional generator by deriving new results related to the recently-developed offset Rademacher complexity and approximation properties using neural networks. Empirically, we conduct numerical studies including synthetic datasets and two real-world datasets, demonstrating the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17729v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Siming Zheng, Meifang Lan, Tong Wang, Yuanyuan Lin</dc:creator>
    </item>
  </channel>
</rss>
