<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 Nov 2024 02:39:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Towards Evaluating Large Language Models for Graph Query Generation</title>
      <link>https://arxiv.org/abs/2411.08449</link>
      <description>arXiv:2411.08449v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are revolutionizing the landscape of Generative Artificial Intelligence (GenAI), with innovative LLM-backed solutions emerging rapidly. However, when applied to database technologies, specifically query generation for graph databases and Knowledge Graphs (KGs), LLMs still face significant challenges. While research on LLM-driven query generation for Structured Query Language (SQL) exists, similar systems for graph databases remain underdeveloped. This paper presents a comparative study addressing the challenge of generating Cypher queries a powerful language for interacting with graph databases using open-access LLMs. We rigorously evaluate several LLM agents (OpenAI ChatGPT 4o, Claude Sonnet 3.5, Google Gemini Pro 1.5, and a locally deployed Llama 3.1 8B) using a designed few-shot learning prompt and Retrieval Augmented Generation (RAG) backed by Chain-of-Thoughts (CoT) reasoning. Our empirical analysis of query generation accuracy reveals that Claude Sonnet 3.5 outperforms its counterparts in this specific domain. Further, we highlight promising future research directions to address the identified limitations and advance LLM-driven query generation for graph databases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08449v1</guid>
      <category>cs.ET</category>
      <category>cs.CL</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Siraj Munir, Alessandro Aldini</dc:creator>
    </item>
    <item>
      <title>Biodynamic Analysis of Alpine Skiing with a Skier-Ski-Snow Interaction Model</title>
      <link>https://arxiv.org/abs/2411.08056</link>
      <description>arXiv:2411.08056v1 Announce Type: cross 
Abstract: This study establishes a skier-ski-snow interaction (SSSI) model that integrates a 3D full-body musculoskeletal model, a flexible ski model, a ski-snow contact model, and an air resistance model. An experimental method is developed to collect kinematic and kinetic data using IMUs, GPS, and plantar pressure measurement insoles, which are cost-effective and capable of capturing motion in large-scale field conditions. The ski-snow interaction parameters are optimized for dynamic alignment with snow conditions and individual turning techniques. Forward-inverse dynamics simulation is performed using only the skier's posture as model input and leaving the translational degrees of freedom (DOFs) between the pelvis and the ground unconstrained. The effectiveness of our model is further verified by comparing the simulated results with the collected GPS and plantar pressure data. The correlation coefficient between the simulated ski-snow contact force and the measured plantar pressure data is 0.964, and the error between the predicted motion trajectory and GPS data is 0.7%. By extracting kinematic and kinetic parameters from skiers of different skill levels, quantitative performance analysis helps quantify ski training. The SSSI model with the parameter optimization algorithm of the ski-snow interaction allows for the description of skiing characteristics across varied snow conditions and different turning techniques, such as carving and skidding. Our research advances the understanding of alpine skiing dynamics, informing the development of training programs and facility designs to enhance athlete performance and safety.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08056v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.ET</category>
      <category>physics.comp-ph</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nan Gao (Department of Engineering Mechanics, Tsinghua University, Beijing, China), Huitong Jin (Department of Engineering Mechanics, Tsinghua University, Beijing, China), Jianqiao Guo (MOE Key Laboratory of Dynamics and Control of Flight Vehicle, School of Aerospace Engineering, Beijing Institute of Technology, Beijing, China), Gexue Ren (Department of Engineering Mechanics, Tsinghua University, Beijing, China), Chun Yang (Department of Engineering Mechanics, Tsinghua University, Beijing, China)</dc:creator>
    </item>
    <item>
      <title>BOSON$^{-1}$: Understanding and Enabling Physically-Robust Photonic Inverse Design with Adaptive Variation-Aware Subspace Optimization</title>
      <link>https://arxiv.org/abs/2411.08210</link>
      <description>arXiv:2411.08210v1 Announce Type: cross 
Abstract: Nanophotonic device design aims to optimize photonic structures to meet specific requirements across various applications. Inverse design has unlocked non-intuitive, high-dimensional design spaces, enabling the discovery of high-performance devices beyond heuristic or analytic methods. The adjoint method, which calculates gradients for all variables using just two simulations, enables efficient navigation of this complex space. However, many inverse-designed structures, while numerically plausible, are difficult to fabricate and sensitive to variations, limiting their practical use. The discrete nature with numerous local-optimal structures also pose significant optimization challenges, often causing gradient-based methods to converge on suboptimal designs. In this work, we formulate inverse design as a fabrication-restricted, discrete, probabilistic optimization problem and introduce BOSON-1, an end-to-end, variation-aware subspace optimization framework to address the challenges of manufacturability, robustness, and optimizability. To overcome optimization difficulty, we propose dense target-enhanced gradient flows to mitigate misleading local optima and introduce a conditional subspace optimization strategy to create high-dimensional tunnels to escape local optima. Furthermore, we significantly reduce the runtime associated with optimizing across exponential variation samples through an adaptive sampling-based robust optimization, ensuring both efficiency and variation robustness. On three representative photonic device benchmarks, our proposed inverse design methodology BOSON^-1 delivers fabricable structures and achieves the best convergence and performance under realistic variations, outperforming prior arts with 74.3% post-fabrication performance. We open-source our codes at https://github.com/ScopeX-ASU/BOSON.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08210v1</guid>
      <category>physics.optics</category>
      <category>cs.ET</category>
      <category>physics.comp-ph</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pingchuan Ma, Zhengqi Gao, Amir Begovic, Meng Zhang, Haoyu Yang, Haoxing Ren, Zhaoran Rena Huang, Duane Boning, Jiaqi Gu</dc:creator>
    </item>
    <item>
      <title>NVCiM-PT: An NVCiM-assisted Prompt Tuning Framework for Edge LLMs</title>
      <link>https://arxiv.org/abs/2411.08244</link>
      <description>arXiv:2411.08244v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) deployed on edge devices, known as edge LLMs, need to continuously fine-tune their model parameters from user-generated data under limited resource constraints. However, most existing learning methods are not applicable for edge LLMs because of their reliance on high resources and low learning capacity. Prompt tuning (PT) has recently emerged as an effective fine-tuning method for edge LLMs by only modifying a small portion of LLM parameters, but it suffers from user domain shifts, resulting in repetitive training and losing resource efficiency. Conventional techniques to address domain shift issues often involve complex neural networks and sophisticated training, which are incompatible for PT for edge LLMs. Therefore, an open research question is how to address domain shift issues for edge LLMs with limited resources. In this paper, we propose a prompt tuning framework for edge LLMs, exploiting the benefits offered by non-volatile computing-in-memory (NVCiM) architectures. We introduce a novel NVCiM-assisted PT framework, where we narrow down the core operations to matrix-matrix multiplication, which can then be accelerated by performing in-situ computation on NVCiM. To the best of our knowledge, this is the first work employing NVCiM to improve the edge LLM PT performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08244v1</guid>
      <category>cs.LG</category>
      <category>cs.ET</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruiyang Qin, Pengyu Ren, Zheyu Yan, Liu Liu, Dancheng Liu, Amir Nassereldine, Jinjun Xiong, Kai Ni, Sharon Hu, Yiyu Shi</dc:creator>
    </item>
    <item>
      <title>Control of Biohybrid Actuators using NeuroEvolution</title>
      <link>https://arxiv.org/abs/2411.08261</link>
      <description>arXiv:2411.08261v1 Announce Type: cross 
Abstract: In medical-related tasks, soft robots can perform better than conventional robots because of their compliant building materials and the movements they are able perform. However, designing soft robot controllers is not an easy task, due to the non-linear properties of their materials. Since human expertise to design such controllers is yet not sufficiently effective, a formal design process is needed. The present research proposes neuroevolution-based algorithms as the core mechanism to automatically generate controllers for biohybrid actuators that can be used on future medical devices, such as a catheter that will deliver drugs. The controllers generated by methodologies based on Neuroevolution of Augmenting Topologies (NEAT) and Hypercube-based NEAT (HyperNEAT) are compared against the ones generated by a standard genetic algorithm (SGA). In specific, the metrics considered are the maximum displacement in upward bending movement and the robustness to control different biohybrid actuator morphologies without redesigning the control strategy. Results indicate that the neuroevolution-based algorithms produce better suited controllers than the SGA. In particular, NEAT designed the best controllers, achieving up to 25% higher displacement when compared with SGA-produced specialised controllers trained over a single morphology and 23% when compared with general purpose controllers trained over a set of morphologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08261v1</guid>
      <category>cs.RO</category>
      <category>cs.ET</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hugo Alcaraz-Herrera, Michail-Antisthenis Tsompanas, Andrew Adamatzky, Igor Balaz</dc:creator>
    </item>
    <item>
      <title>Everything You Wanted to Know About Consumer Light Management in Smart Energy</title>
      <link>https://arxiv.org/abs/2411.08353</link>
      <description>arXiv:2411.08353v1 Announce Type: cross 
Abstract: Consumer lighting plays a significant role in the development of smart cities and smart villages. With the advancement of (IoT) technology, smart lighting solutions have become more prevalent in residential areas as well. These solutions provide consumers with increased energy efficiency, added convenience, and improved security. On the other hand, the growing number of IoT devices has become a global concern due to the carbon footprint and carbon emissions associated with these devices. The overuse of batteries increases maintenance and cost to IoT devices and simultaneously possesses adverse environmental effects, ultimately exacerbating the pace of climate change. Therefore, in tandom with the principles of Industry 4.0, it has become crucial for manufacturing and research industries to prioritize sustainable measures adhering to smart energy as a prevention to the negative impacts. Consequently, it has undoubtedly garnered global interest from scientists, researchers, and industrialists to integrate state-of-the-art technologies in order to solve the current issues in consumer light management systems making it a complete sustainable, and smart solution for consumer lighting application. This manuscript provides a thorough investigation of various methods as well as techniques to design a state-of-the-art IoT-enabled consumer light management system. It critically reviews the existing works done in consumer light management systems, emphasizing the significant limitations and the need for sustainability. The top-down approach of developing sustainable computing frameworks for IoT-enabled consumer light management has been reviewed based on the multidisciplinary technologies involved and state-of-the-art works in the respective domains. Lastly, this article concludes by highlighting possible avenues for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08353v1</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prajnyajit Mohanty, Umesh C. Pati, Kamalakanta Mahapatra, Saraju P. Mohanty</dc:creator>
    </item>
    <item>
      <title>Symbolic-AI-Fusion Deep Learning (SAIF-DL): Encoding Knowledge into Training with Answer Set Programming Loss Penalties by a Novel Loss Function Approach</title>
      <link>https://arxiv.org/abs/2411.08463</link>
      <description>arXiv:2411.08463v1 Announce Type: cross 
Abstract: This paper presents a hybrid methodology that enhances the training process of deep learning (DL) models by embedding domain expert knowledge using ontologies and answer set programming (ASP). By integrating these symbolic AI methods, we encode domain-specific constraints, rules, and logical reasoning directly into the model's learning process, thereby improving both performance and trustworthiness. The proposed approach is flexible and applicable to both regression and classification tasks, demonstrating generalizability across various fields such as healthcare, autonomous systems, engineering, and battery manufacturing applications. Unlike other state-of-the-art methods, the strength of our approach lies in its scalability across different domains. The design allows for the automation of the loss function by simply updating the ASP rules, making the system highly scalable and user-friendly. This facilitates seamless adaptation to new domains without significant redesign, offering a practical solution for integrating expert knowledge into DL models in industrial settings such as battery manufacturing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08463v1</guid>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fadi Al Machot, Martin Thomas Horsch, Habib Ullah</dc:creator>
    </item>
    <item>
      <title>Building Trustworthy AI: Transparent AI Systems via Large Language Models, Ontologies, and Logical Reasoning (TranspNet)</title>
      <link>https://arxiv.org/abs/2411.08469</link>
      <description>arXiv:2411.08469v1 Announce Type: cross 
Abstract: Growing concerns over the lack of transparency in AI, particularly in high-stakes fields like healthcare and finance, drive the need for explainable and trustworthy systems. While Large Language Models (LLMs) perform exceptionally well in generating accurate outputs, their "black box" nature poses significant challenges to transparency and trust. To address this, the paper proposes the TranspNet pipeline, which integrates symbolic AI with LLMs. By leveraging domain expert knowledge, retrieval-augmented generation (RAG), and formal reasoning frameworks like Answer Set Programming (ASP), TranspNet enhances LLM outputs with structured reasoning and verification. This approach ensures that AI systems deliver not only accurate but also explainable and trustworthy results, meeting regulatory demands for transparency and accountability. TranspNet provides a comprehensive solution for developing AI systems that are reliable and interpretable, making it suitable for real-world applications where trust is critical.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08469v1</guid>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fadi Al Machot, Martin Thomas Horsch, Habib Ullah</dc:creator>
    </item>
    <item>
      <title>A System Level Performance Evaluation for Superconducting Digital Systems</title>
      <link>https://arxiv.org/abs/2411.08645</link>
      <description>arXiv:2411.08645v1 Announce Type: cross 
Abstract: Superconducting Digital (SCD) technology offers significant potential for enhancing the performance of next generation large scale compute workloads. By leveraging advanced lithography and a 300 mm platform, SCD devices can reduce energy consumption and boost computational power. This paper presents a cross-layer modeling approach to evaluate the system-level performance benefits of SCD architectures for Large Language Model (LLM) training and inference. Our findings, based on experimental data and Pulse Conserving Logic (PCL) design principles, demonstrate substantial performance gain in both training and inference. We are, thus, able to convincingly show that the SCD technology can address memory and interconnect limitations of present day solutions for next-generation compute systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08645v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>DATE 2025</arxiv:journal_reference>
      <dc:creator>Joyjit Kundu, Debjyoti Bhattacharjee, Nathan Josephsen, Ankit Pokhrel, Udara De Silva, Wenzhe Guo, Steven Van Winckel, Steven Brebels, Manu Perumkunnil, Quentin Herr, Anna Herr</dc:creator>
    </item>
    <item>
      <title>How NOT to Fool the Masses When Giving Performance Results for Quantum Computers</title>
      <link>https://arxiv.org/abs/2411.08860</link>
      <description>arXiv:2411.08860v1 Announce Type: cross 
Abstract: In 1991, David Bailey wrote an article describing techniques for overstating the performance of massively parallel computers. Intended as a lighthearted protest against the practice of inflating benchmark results in order to ``fool the masses" and boost sales, the paper sparked development of procedural standards that help benchmarkers avoid methodological errors leading to unjustified and misleading conclusions.
  Now that quantum computers are starting to realize their potential as viable alternatives to classical computers, we can see the mistakes of three decades ago being repeated by a new batch of researchers who are unfamiliar with this history and these standards.
  Inspired by Bailey's model, this paper presents four suggestions for newcomers to quantum performance benchmarking, about how not to do it. They are: (1) Don't claim superior performance without mentioning runtimes; (2) Don't report optimized results without mentioning the tuning time needed to optimize those results; (3) Don't claim faster runtimes for (or in comparison to) solvers running on imaginary platforms; and (4) No cherry-picking (without justification and qualification). Suggestions for improving current practice appear in the last section.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08860v1</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Catherine McGeoch</dc:creator>
    </item>
    <item>
      <title>Two Sides of the Same Coin: Large-scale Measurements of Builder and Rollup after EIP-4844</title>
      <link>https://arxiv.org/abs/2411.03892</link>
      <description>arXiv:2411.03892v2 Announce Type: replace-cross 
Abstract: Web3 is reshaping decentralized ecosystems through innovations like Ethereum. Recently, EIP-4844 is implemented in Ethereum to support its Layer-2 scaling solutions, which introduces a new 128 KB data structure called blob. This upgrade incorporates type-3 transactions with blobs to verify data availability and reduce gas costs for rollups, significantly affecting the strategies of both builders and rollups. In this paper, we present an in-depth study of emerging strategies in builder and rollup markets after EIP-4844, containing hundred million transactions. We find that the efficiency of builder and rollup strategies is interdependent, akin to two sides of the same coin -- both cannot be optimized simultaneously. That is, when builders operate efficiently, rollups tend to overpay in fees, conversely, when rollups optimize their costs, builders may incur losses in inefficient transaction selection. From the side of builders, our results show that 29.48% of these blocks have been constructed inefficiently, which does not produce sufficient profits for builders. Through our evaluation from the side of rollups, we find that over 72.53% of type-3 transactions pay unnecessary fees, leading to notable economic costs of rollups. Our work provides critical insights into optimizing block construction and transaction strategies, advancing the economic efficiency and data scalability of Web3 infrastructures, yet, much like balancing a seesaw, the efficiency of builders and rollups cannot be optimized concurrently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03892v2</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yue Huang, Shuzheng Wang, Yuming Huang, Jing Tang</dc:creator>
    </item>
  </channel>
</rss>
