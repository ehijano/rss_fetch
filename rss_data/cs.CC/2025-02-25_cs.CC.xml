<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Feb 2025 05:00:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Computing the Polytope Diameter is Even Harder than NP-hard (Already for Perfect Matchings)</title>
      <link>https://arxiv.org/abs/2502.16398</link>
      <description>arXiv:2502.16398v1 Announce Type: new 
Abstract: The diameter of a polytope is a fundamental geometric parameter that plays a crucial role in understanding the efficiency of the simplex method. Despite its central nature, the computational complexity of computing the diameter of a given polytope is poorly understood. Already in 1994, Frieze and Teng [Comp. Compl.] recognized the possibility that this task could potentially be harder than NP-hard, and asked whether the corresponding decision problem is complete for the second stage of the polynomial hierarchy, i.e. $\Pi^p_2$-complete. In the following years, partial results could be obtained. In a cornerstone result, Frieze and Teng themselves proved weak NP-hardness for a family of custom defined polytopes. Sanit\`a [FOCS18] in a break-through result proved that already for the much simpler fractional matching polytope the problem is strongly NP-hard. Very recently, Steiner and N\"obel [SODA25] generalized this result to the even simpler bipartite perfect matching polytope and the circuit diameter. In this paper, we finally show that computing the diameter of the bipartite perfect matching polytope is $\Pi^p_2$-hard. Since the corresponding decision problem is also trivially contained in $\Pi^p_2$, this decidedly answers Frieze and Teng's 30 year old question. Our results also hold when the diameter is replaced by the circuit diameter. As our second main result, we prove that for some $\varepsilon &gt; 0$ the (circuit) diameter of the bipartite perfect matching polytope cannot be approximated by a factor better than $(1 + \varepsilon)$. This answers a recent question by N\"obel and Steiner. It is the first known inapproximability result for the circuit diameter, and extends Sanit\`a's inapproximability result of the diameter to the totally unimodular case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16398v1</guid>
      <category>cs.CC</category>
      <category>cs.CG</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lasse Wulf</dc:creator>
    </item>
    <item>
      <title>Tarski Lower Bounds from Multi-Dimensional Herringbones</title>
      <link>https://arxiv.org/abs/2502.16679</link>
      <description>arXiv:2502.16679v1 Announce Type: new 
Abstract: Tarski's theorem states that every monotone function from a complete lattice to itself has a fixed point. We analyze the query complexity of finding such a fixed point on the $k$-dimensional grid of side length $n$ under the $\leq$ relation. In this setting, there is an unknown monotone function $f: \{0,1,\ldots, n-1\}^k \to \{0,1,\ldots, n-1\}^k$ and an algorithm must query a vertex $v$ to learn $f(v)$. The goal is to find a fixed point of $f$ using as few oracle queries as possible.
  We show that the randomized query complexity of this problem is $\Omega\left( \frac{k \cdot \log^2{n}}{\log{k}} \right)$ for all $n,k \geq 2$. This unifies and improves upon two prior results: a lower bound of $\Omega(\log^2{n})$ from [EPRY 2019] and a lower bound of $\Omega\left( \frac{k \cdot \log{n}}{\log{k}}\right)$ from [BPR 2024], respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16679v1</guid>
      <category>cs.CC</category>
      <category>cs.GT</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simina Br\^anzei, Reed Phillips, Nicholas Recker</dc:creator>
    </item>
    <item>
      <title>When Can We Solve the Weighted Low Rank Approximation Problem in Truly Subquadratic Time?</title>
      <link>https://arxiv.org/abs/2502.16912</link>
      <description>arXiv:2502.16912v1 Announce Type: new 
Abstract: The weighted low-rank approximation problem is a fundamental numerical linear algebra problem and has many applications in machine learning. Given a $n \times n$ weight matrix $W$ and a $n \times n$ matrix $A$, the goal is to find two low-rank matrices $U, V \in \mathbb{R}^{n \times k}$ such that the cost of $\| W \circ (U V^\top - A) \|_F^2$ is minimized. Previous work has to pay $\Omega(n^2)$ time when matrices $A$ and $W$ are dense, e.g., having $\Omega(n^2)$ non-zero entries. In this work, we show that there is a certain regime, even if $A$ and $W$ are dense, we can still hope to solve the weighted low-rank approximation problem in almost linear $n^{1+o(1)}$ time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16912v1</guid>
      <category>cs.CC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenyang Li, Yingyu Liang, Zhenmei Shi, Zhao Song</dc:creator>
    </item>
    <item>
      <title>Compression Barriers for Autoregressive Transformers</title>
      <link>https://arxiv.org/abs/2502.15955</link>
      <description>arXiv:2502.15955v1 Announce Type: cross 
Abstract: A key limitation of autoregressive Transformers is the large memory needed at inference-time to cache all previous key-value (KV) embeddings. Prior works address this by compressing the KV cache, but often assume specific structural properties of the embeddings. This raises the following natural question: Can truly sublinear space utilization be achieved without such assumptions? In this work, we answer this question in the negative. Any algorithm for attention-based token generation must use $\Theta(nd)$ space, where $n$ is the number of tokens generated so far and $d = \Omega(\log n)$ is the dimension of the KV embeddings. Our proof involves a reduction from a classic communication complexity problem and uses a randomized construction that leverages properties of projections in the spirit of the Johnson-Linderstrauss lemma. For the low-dimensional regime $d = o(\log n)$, we show that any algorithm requires $\Omega(d\cdot e^d)$ space and prove, using tight bounds on covering numbers, that SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this bound. Further, we investigate how sparsity assumptions enable token generation in truly sublinear space, presenting impossibility results and proposing a new KV cache compression algorithm for sliding window attention when the value cache outside the window is unmasked. Finally, we analyze token generation's time complexity, using an indistinguishability argument to prove that no non-adaptive algorithm can compute attention online in sublinear time for all tokens.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15955v1</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Themistoklis Haris, Krzysztof Onak</dc:creator>
    </item>
    <item>
      <title>Hunting a rabbit is hard</title>
      <link>https://arxiv.org/abs/2502.15982</link>
      <description>arXiv:2502.15982v1 Announce Type: cross 
Abstract: In the Hunters and Rabbit game, $k$ hunters attempt to shoot an invisible rabbit on a given graph $G$. In each round, the hunters can choose $k$ vertices to shoot at, while the rabbit must move along an edge of $G$. The hunters win if at any point the rabbit is shot. The hunting number of $G$, denoted $h(G)$, is the minimum $k$ for which $k$ hunters can win, regardless of the rabbit's moves. The complexity of computing $h(G)$ has been the longest standing open problem concerning the game and has been posed as an explicit open problem by several authors. The first contribution of this paper resolves this question by establishing that computing $h(G)$ is NP-hard even for bipartite simple graphs. We also prove that the problem remains hard even when $h(G)$ is $O(n^{\epsilon})$ or when $n-h(G)$ is $O(n^{\epsilon})$, where $n$ is the order of $G$. Furthermore, we prove that it is NP-hard to additively approximate $h(G)$ within $O(n^{1-\epsilon})$. Finally, we give a characterization of graphs with loops for which $h(G)=1$ by means of forbidden subgraphs, extending a known characterization for simple graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15982v1</guid>
      <category>math.CO</category>
      <category>cs.CC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Walid Ben-Ameur, Harmender Gahlawat, Alessandro Maddaloni</dc:creator>
    </item>
    <item>
      <title>The Parameterized Landscape of Labeled Graph Contractions</title>
      <link>https://arxiv.org/abs/2502.16096</link>
      <description>arXiv:2502.16096v1 Announce Type: cross 
Abstract: In this work, we study the problem of computing a maximum common contraction of two vertex-labeled graphs, i.e. how to make them identical by contracting as little edges as possible in the two graphs. We study the problem from a parameterized complexity point of view, using parameters such as the maximum degree, the degeneracy, the clique-width or treewidth of the input graphs as well as the number of allowed contractions. We put this complexity in perspective with that of the labeled contractibility problem, i.e determining whether a labeled graph is a contraction of another. Surprisingly, our results indicate very little difference between these problems in terms of parameterized complexity status. We only prove their status to differ when parameterizing by both the degeneracy and the number of allowed contractions, showing W[1]-hardness of the maximum common contraction problem in this case, whereas the contractibility problem is FPT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16096v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Manuel Lafond, Bertrand Marchand</dc:creator>
    </item>
    <item>
      <title>Verifying Quantized Graph Neural Networks is PSPACE-complete</title>
      <link>https://arxiv.org/abs/2502.16244</link>
      <description>arXiv:2502.16244v1 Announce Type: cross 
Abstract: In this paper, we investigate verification of quantized Graph Neural Networks (GNNs), where some fixed-width arithmetic is used to represent numbers. We introduce the linear-constrained validity (LVP) problem for verifying GNNs properties, and provide an efficient translation from LVP instances into a logical language. We show that LVP is in PSPACE, for any reasonable activation functions. We provide a proof system. We also prove PSPACE-hardness, indicating that while reasoning about quantized GNNs is feasible, it remains generally computationally challenging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16244v1</guid>
      <category>cs.LO</category>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Marco S\"alzer, Fran\c{c}ois Schwarzentruber, Nicolas Troquard</dc:creator>
    </item>
    <item>
      <title>Optimality and Renormalization imply Statistical Laws</title>
      <link>https://arxiv.org/abs/2502.16314</link>
      <description>arXiv:2502.16314v1 Announce Type: cross 
Abstract: Benford's Law is an important instance of experimental mathematics that appears to constrain the information-theoretic behavior of numbers. Elias' encoding for integers is a remarkable approach to universality and optimality of codes. In the present analysis we seek to deduce a general law and its particular implications for these two cases from optimality and renormalization as applied to information-theoretical functionals. Both theoretical and experimental results corroborate our conclusions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16314v1</guid>
      <category>cs.IT</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>math-ph</category>
      <category>math.IT</category>
      <category>math.MP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Kolpakov, Aidan Rocke</dc:creator>
    </item>
    <item>
      <title>Monotonicity Testing of High-Dimensional Distributions with Subcube Conditioning</title>
      <link>https://arxiv.org/abs/2502.16355</link>
      <description>arXiv:2502.16355v1 Announce Type: cross 
Abstract: We study monotonicity testing of high-dimensional distributions on $\{-1,1\}^n$ in the model of subcube conditioning, suggested and studied by Canonne, Ron, and Servedio~\cite{CRS15} and Bhattacharyya and Chakraborty~\cite{BC18}. Previous work shows that the \emph{sample complexity} of monotonicity testing must be exponential in $n$ (Rubinfeld, Vasilian~\cite{RV20}, and Aliakbarpour, Gouleakis, Peebles, Rubinfeld, Yodpinyanee~\cite{AGPRY19}). We show that the subcube \emph{query complexity} is $\tilde{\Theta}(n/\varepsilon^2)$, by proving nearly matching upper and lower bounds. Our work is the first to use directed isoperimetric inequalities (developed for function monotonicity testing) for analyzing a distribution testing algorithm. Along the way, we generalize an inequality of Khot, Minzer, and Safra~\cite{KMS18} to real-valued functions on $\{-1,1\}^n$.
  We also study uniformity testing of distributions that are promised to be monotone, a problem introduced by Rubinfeld, Servedio~\cite{RS09} , using subcube conditioning. We show that the query complexity is $\tilde{\Theta}(\sqrt{n}/\varepsilon^2)$. Our work proves the lower bound, which matches (up to poly-logarithmic factors) the uniformity testing upper bound for general distributions (Canonne, Chen, Kamath, Levi, Waingarten~\cite{CCKLW21}). Hence, we show that monotonicity does not help, beyond logarithmic factors, in testing uniformity of distributions with subcube conditional queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16355v1</guid>
      <category>math.ST</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deeparnab Chakrabarty, Xi Chen, Simeon Ristic, C. Seshadhri, Erik Waingarten</dc:creator>
    </item>
    <item>
      <title>On Computational Limits of FlowAR Models: Expressivity and Efficiency</title>
      <link>https://arxiv.org/abs/2502.16490</link>
      <description>arXiv:2502.16490v1 Announce Type: cross 
Abstract: The expressive power and computational complexity of deep visual generative models, such as flow-based and autoregressive (AR) models, have gained considerable interest for their wide-ranging applications in generative tasks. However, the theoretical characterization of their expressiveness through the lens of circuit complexity remains underexplored, particularly for the state-of-the-art architecture like FlowAR proposed by [Ren et al., 2024], which integrates flow-based and autoregressive mechanisms. This gap limits our understanding of their inherent computational limits and practical efficiency. In this study, we address this gap by analyzing the circuit complexity of the FlowAR architecture. We demonstrate that when the largest feature map produced by the FlowAR model has dimensions $n \times n \times c$, the FlowAR model is simulable by a family of threshold circuits $\mathsf{TC}^0$, which have constant depth $O(1)$ and polynomial width $\mathrm{poly}(n)$. This is the first study to rigorously highlight the limitations in the expressive power of FlowAR models. Furthermore, we identify the conditions under which the FlowAR model computations can achieve almost quadratic time. To validate our theoretical findings, we present efficient model variant constructions based on low-rank approximations that align with the derived criteria. Our work provides a foundation for future comparisons with other generative paradigms and guides the development of more efficient and expressive implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16490v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>cs.CV</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chengyue Gong, Yekun Ke, Xiaoyu Li, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song</dc:creator>
    </item>
    <item>
      <title>A Parameterized Complexity Analysis of Bounded Height Depth-first Search Trees</title>
      <link>https://arxiv.org/abs/2502.16723</link>
      <description>arXiv:2502.16723v1 Announce Type: cross 
Abstract: Computing bounded depth decompositions is a bottleneck in many applications of the treedepth parameter. The fastest known algorithm, which is due to Reidl, Rossmanith, S\'{a}nchez Villaamil, and Sikdar [ICALP 2014], runs in $2^{\mathcal{O}(k^2)}\cdot n$ time and it is a big open problem whether the dependency on $k$ can be improved to $2^{o(k^2)}\cdot n^{\mathcal{O}(1)}$. We show that the related problem of finding DFS trees of bounded height can be solved faster in $2^{\mathcal{O}(k \log k)}\cdot n$ time. As DFS trees are treedepth decompositions, this circumvents the above mentioned bottleneck for this subclass of graphs of bounded treedepth. This problem has recently found attention independently under the name Minimum Height Lineal Topology (MinHLT) and our algorithm gives a positive answer to an open problem posed by Golovach [Dagstuhl Reports, 2023]. We complement our main result by studying the complexity of MinHLT and related problems in several other settings. First, we show that it remains NP-complete on chordal graphs, and give an FPT-algorithm on chordal graphs for the dual problem, asking for a DFS tree of height at most $n-k$, parameterized by $k$. The parameterized complexity of Dual MinHLT on general graphs is wide open. Lastly, we show that Dual MinHLT and two other problems concerned with finding DFS trees with few or many leaves are FPT parameterized by $k$ plus the treewidth of the input graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16723v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lars Jaffke, Paloma T. de Lima, Wojciech Nadara, Emmanuel Sam</dc:creator>
    </item>
    <item>
      <title>$\rm P$ has polynomial-time finite-state verifiers</title>
      <link>https://arxiv.org/abs/2306.09542</link>
      <description>arXiv:2306.09542v4 Announce Type: replace 
Abstract: Interactive proof systems whose verifiers are constant-space machines have interesting features that do not have counterparts in the better studied case where the verifiers operate under reasonably large space bounds. The language verification power of finite-state verifiers is known to be sensitive to the difference between private and public randomization. These machines also lack the capability of imposing worst-case superlinear bounds on their own runtime, and long interactions with untrustable provers can involve the risk of being fooled to loop forever. We analyze such verifiers under different bounds on the numbers of private and public random bits that they are allowed to use. This separate accounting for the private and public coin budgets as resource functions of the input length provides interesting characterizations of the collections of the associated languages. When the randomness bound is constant, the verifiable class is $\rm NL$ for private-coin machines, but equals just the regular languages when one uses public coins. Increasing the public coin budget while keeping the number of private coins constant augments the power: We show that the set of languages that are verifiable by such machines in expected polynomial time (with an arbitrarily small positive probability of looping) equals the complexity class $\rm P$. This hints that allowing a minuscule probability of looping may add significant power to polynomial-time finite-state automata, since it is still not known whether those machines can verify all of $\rm P$ when required to halt with probability 1, even with no bound on their private coin usage. We also show that logarithmic-space machines which hide a constant number of their coins are limited to verifying the languages in $\rm P$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.09542v4</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>M. Utkan Gezer, A. C. Cem Say</dc:creator>
    </item>
    <item>
      <title>Hyperplanes Avoiding Problem and Integer Points Counting in Polyhedra</title>
      <link>https://arxiv.org/abs/2411.07030</link>
      <description>arXiv:2411.07030v2 Announce Type: replace 
Abstract: In our work, we consider the problem of computing a vector $x \in Z^n$ of minimum $\|\cdot\|_p$-norm such that $a^\top x \not= a_0$, for any vector $(a,a_0)$ from a given subset of $Z^n$ of size $m$. In other words, we search for a vector of minimum norm that avoids a given finite set of hyperplanes, which is natural to call as the $\textit{Hyperplanes Avoiding Problem}$. This problem naturally appears as a subproblem in Barvinok-type algorithms for counting integer points in polyhedra. We show that:
  1) With respect to $\|\cdot\|_1$, the problem admits a feasible solution $x$ with $\|x\|_1 \leq (m+n)/2$, and show that such solution can be constructed by a deterministic polynomial-time algorithm with $O(n \cdot m)$ operations. Moreover, this inequality is the best possible. This is a significant improvement over the previous randomized algorithm, which computes $x$ with a guaranty $\|x\|_{1} \leq n \cdot m$. The original approach of A.~Barvinok can guarantee only $\|x\|_1 = O\bigl((n \cdot m)^n\bigr)$. To prove this result, we use a newly established algorithmic variant of the Combinatorial Nullstellensatz;
  2) The problem is NP-hard with respect to any norm $\|\cdot\|_p$, for $p \in \bigl(R_{\geq 1} \cup \{\infty\}\bigr)$.
  3) As an application, we show that the problem to count integer points in a polytope $P = \{x \in R^n \colon A x \leq b\}$, for given $A \in Z^{m \times n}$ and $b \in Q^m$, can be solved by an algorithm with $O\bigl(\nu^2 \cdot n^3 \cdot \Delta^3 \bigr)$ operations, where $\nu$ is the maximum size of a normal fan triangulation of $P$, and $\Delta$ is the maximum value of rank-order subdeterminants of $A$. As a further application, it provides a refined complexity bound for the counting problem in polyhedra of bounded codimension. For example, in the polyhedra of the Unbounded Subset-Sum problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07030v2</guid>
      <category>cs.CC</category>
      <category>cs.CG</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Grigorii Dakhno, Dmitry Gribanov, Nikita Kasianov, Anastasiia Kats, Andrey Kupavskii, Nikita Kuz'min</dc:creator>
    </item>
    <item>
      <title>On the Complexity of Hazard-Free Formulas</title>
      <link>https://arxiv.org/abs/2411.09026</link>
      <description>arXiv:2411.09026v2 Announce Type: replace 
Abstract: This paper studies the hazard-free formula complexity of Boolean functions.
  Our first result shows that unate functions are the only Boolean functions for which the monotone formula complexity of the hazard-derivative equals the hazard-free formula complexity of the function itself. Consequently, they are the only functions for which the hazard-derivative approach of Ikenmeyer et al. (J. ACM 2019) yields optimal bounds.
  Our second result proves that the hazard-free formula complexity of random Boolean functions is at most $2^{(1+o(1))n}$. Prior to this, no better upper bound than $O(3^n)$ was known. Notably, unlike in the general case of Boolean circuits and formulas, where the typical complexity is derived from that of the multiplexer function with $n$-bit selector, the hazard-free formula complexity of a random function is smaller than the optimal hazard-free formula for the multiplexer by an exponential factor in $n$.
  We provide two proofs of this fact. The first is direct, bounding the number of prime implicants of a random Boolean function and using this bound to construct a DNF of the claimed size. The second introduces a new and independently interesting result: a weak converse to the hazard-derivative lower bound method, which gives an upper bound on the hazard-free complexity of a function in terms of the monotone complexity of a subset of its hazard-derivatives.
  Additionally, we explore the hazard-free formula complexity of block composition of Boolean functions and obtain a result in the hazard-free setting that is analogous to a result of Karchmer, Raz, and Wigderson (Computational Complexity, 1995) in the monotone setting. We show that our result implies a stronger lower bound on the hazard-free formula depth of the block composition of the set covering function with the multiplexer function than the bound obtained via the hazard-derivative method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09026v2</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leah London Arazi, Amir Shpilka</dc:creator>
    </item>
    <item>
      <title>On the Hardness of the Drone Delivery Problem</title>
      <link>https://arxiv.org/abs/2502.15194</link>
      <description>arXiv:2502.15194v2 Announce Type: replace 
Abstract: Fast shipping and efficient routing are key problems of modern logistics. Building on previous studies that address package delivery from a source node to a destination within a graph using multiple agents (such as vehicles, drones, and ships), we investigate the complexity of this problem in specialized graphs and with restricted agent types, both with and without predefined initial positions. Particularly, in this paper, we aim to minimize the delivery time for delivering a package. To achieve this, we utilize a set of collaborative agents, each capable of traversing a specific subset of the graph and operating at varying speeds. This challenge is encapsulated in the recently introduced Drone Delivery Problem with respect to delivery time (DDT).
  In this work, we show that the DDT with predefined initial positions on a line is NP-hard, even when considering only agents with two distinct speeds. This refines the results presented by Erlebach, et al.[10], who demonstrated the NP-hardness of DDT on a line with agents of arbitrary speeds. Additionally, we examine DDT in grid graphs without predefined initial positions, where each drone can freely choose its starting position. We show that the problem is NP-hard to approximate within a factor of $O(n^{1-\varepsilon}$), where $n$ is the size of the grid, even when all agents are restricted to two different speeds as well as rectangular movement areas. We conclude by providing an easy $O(n)$ approximation algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15194v2</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Bartlmae, Andreas Hene, Kelin Luo</dc:creator>
    </item>
    <item>
      <title>Hamiltonian Property Testing</title>
      <link>https://arxiv.org/abs/2403.02968</link>
      <description>arXiv:2403.02968v3 Announce Type: replace-cross 
Abstract: Locality is a fundamental feature of many physical time evolutions. Assumptions on locality and related structural properties also underlie recently proposed procedures for learning an unknown Hamiltonian from access to the induced time evolution. However, no protocols to rigorously test whether an unknown Hamiltonian is local were known. We investigate Hamiltonian locality testing as a property testing problem, where the task is to determine whether an unknown $n$-qubit Hamiltonian $H$ is $k$-local or $\varepsilon$-far from all $k$-local Hamiltonians, given access to the time evolution along $H$. First, we emphasize the importance of the chosen distance measure: With respect to the operator norm, a worst-case distance measure, incoherent quantum locality testers require $\tilde{\Omega}(2^n)$ many time evolution queries and an expected total evolution time of $\tilde{\Omega}(2^n / \varepsilon)$, and even coherent testers need $\Omega(2^{n/2})$ many queries and $\Omega(2^{n/2}/\varepsilon)$ total evolution time. In contrast, when distances are measured according to the normalized Frobenius norm, corresponding to an average-case distance, we give a sample-, time-, and computationally efficient incoherent Hamiltonian locality testing algorithm based on randomized measurements. In fact, our procedure can be used to simultaneously test a wide class of Hamiltonian properties beyond locality. Finally, we prove that learning a general Hamiltonian remains exponentially hard with this average-case distance, thereby establishing an exponential separation between Hamiltonian testing and learning. Our work initiates the study of property testing for quantum Hamiltonians, demonstrating that a broad class of Hamiltonian properties is efficiently testable even with limited quantum capabilities, and positioning Hamiltonian testing as an independent area of research alongside Hamiltonian learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02968v3</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Bluhm, Matthias C. Caro, Aadil Oufkir</dc:creator>
    </item>
    <item>
      <title>On the MST-ratio: Theoretical Bounds and Complexity of Finding the Maximum</title>
      <link>https://arxiv.org/abs/2409.11079</link>
      <description>arXiv:2409.11079v3 Announce Type: replace-cross 
Abstract: Given a finite set of red and blue points in $\Rspace^d$, the MST-ratio is defined as the total length of the Euclidean minimum spanning trees of the red points and the blue points, divided by the length of the Euclidean minimum spanning tree of their union. The MST-ratio has recently gained attention due to its direct interpretation in topological models for studying point sets with applications in spatial biology. The maximum MST-ratio of a point set is the maximum MST-ratio over all proper colorings of its points by red and blue. We prove that finding the maximum MST-ratio of a given point set is NP-hard when the dimension is part of the input. Moreover, we present a quadratic-time $3$-approximation algorithm for this problem. As part of the proof, we show that, in any metric space, the maximum MST-ratio is smaller than $3$. Additionally, we study the average MST-ratio over all colorings of a set of $n$ points. We show that this average is always at least $\frac{n-2}{n-1}$, and for $n$ random points uniformly distributed in a $d$-dimensional unit cube, the average tends to $\sqrt[d]{2}$ in expectation as $n$ approaches infinity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11079v3</guid>
      <category>cs.CG</category>
      <category>cs.CC</category>
      <category>math.CO</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Afrouz Jabal Ameli, Faezeh Motiei, Morteza Saghafian</dc:creator>
    </item>
    <item>
      <title>From Chinese Postman to Salesman and Beyond I: Approximating Shortest Tours $\delta$-Covering All Points on All Edges</title>
      <link>https://arxiv.org/abs/2410.10613</link>
      <description>arXiv:2410.10613v2 Announce Type: replace-cross 
Abstract: A well-studied continuous model of graphs, introduced by Dearing and Francis [Transportation Science, 1974], considers each edge as a continuous unit-length interval of points. For $\delta \geq 0$, we introduce the problem $\delta$-Tour, where the objective is to find the shortest tour that comes within a distance of $\delta$ of every point on every edge. It can be observed that 0-Tour is essentially equivalent to the Chinese Postman Problem, which is solvable in polynomial time. In contrast, 1/2-Tour is essentially equivalent to the Graphic Traveling Salesman Problem (TSP), which is NP-hard but admits a constant-factor approximation in polynomial time. We investigate $\delta$-Tour for other values of $\delta$, noting that the problem's behavior and the insights required to understand it differ significantly across various $\delta$ regimes. We design polynomial-time approximation algorithms summarized as follows:
  (1) For every fixed $0 &lt; \delta &lt; 3/2$, the problem $\delta$-Tour admits a constant-factor approximation.
  (2) For every fixed $\delta \geq 3/2$, the problem admits an $O(\log{n})$-approximation.
  (3) If $\delta$ is considered to be part of the input, then the problem admits an $O(\log^3{n})$-approximation.
  This is the first of two articles on the $\delta$-Tour problem. In the second one we complement the approximation algorithms presented here with inapproximability results and related to parameterized complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10613v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabian Frei, Ahmed Ghazy, Tim A. Hartmann, Florian H\"orsch, D\'aniel Marx</dc:creator>
    </item>
    <item>
      <title>Complexity of Paired Domination Problems on Circle and $k$-Polygon Graphs</title>
      <link>https://arxiv.org/abs/2411.19473</link>
      <description>arXiv:2411.19473v5 Announce Type: replace-cross 
Abstract: A set $D \subseteq V$ is a dominating set of a graph $G$ if every vertex in $V - D$ is adjacent to at least one vertex in $D$. A dominating set $D$ is a paired-dominating set if the subgraph of $G$ induced by $D$ contains a perfect matching. In this paper, we prove that determining the minimum paired-dominating set in circle graphs is NP-complete. We further present an $O(n(\frac{n}{k^2-k})^{2k^2-2k})$-time algorithm for finding the minimum paired-dominating set in $k$-polygon graphs, a subclass of circle graphs. Additionally, we refine the existing algorithm of Elmallah and Stewart for computing the minimum dominating set in $k$-polygon graphs, reducing its time complexity from $O(n^{4k^2+3})$ to $O(n^{3k-5})$, and further extend it to find the minimum total dominating set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19473v5</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>math.CO</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ta-Yu Mu, Ching-Chi Lin</dc:creator>
    </item>
  </channel>
</rss>
