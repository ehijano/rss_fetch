<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 12 Nov 2025 02:46:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Towards Ecologically Valid LLM Benchmarks: Understanding and Designing Domain-Centered Evaluations for Journalism Practitioners</title>
      <link>https://arxiv.org/abs/2511.05501</link>
      <description>arXiv:2511.05501v1 Announce Type: new 
Abstract: Benchmarks play a significant role in how researchers and the public understand generative AI systems. However, the widespread use of benchmark scores to communicate about model capabilities has led to criticisms of validity, especially whether benchmarks test what they claim to test (i.e. construct validity) and whether benchmark evaluations are representative of how models are used in the wild (i.e. ecological validity). In this work we explore how to create an LLM benchmark that addresses these issues by taking a human-centered approach. We focus on designing a domain-oriented benchmark for journalism practitioners, drawing on insights from a workshop of 23 journalism professionals. Our workshop findings surface specific challenges that inform benchmark design opportunities, which we instantiate in a case study that addresses underlying criticisms and specific domain concerns. Through our findings and design case study, this work provides design guidance for developing benchmarks that are better tuned to specific domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05501v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charlotte Li, Nick Hagar, Sachita Nishal, Jeremy Gilbert, Nick Diakopoulos</dc:creator>
    </item>
    <item>
      <title>Social-Physical Interactions with Virtual Characters: Evaluating the Impact of Physicality through Encountered-Type Haptics</title>
      <link>https://arxiv.org/abs/2511.05683</link>
      <description>arXiv:2511.05683v1 Announce Type: new 
Abstract: This work investigates how robot-mediated physicality influences the perception of social-physical interactions with virtual characters. ETHOS (Encountered-Type Haptics for On-demand Social interaction) is an encountered-type haptic display that integrates a torque-controlled manipulator and interchangeable props with a VR headset to enable three gestures: object handovers, fist bumps, and high fives. We conducted a user study to examine how ETHOS adds physicality to virtual character interactions and how this affects presence, realism, enjoyment, and connection metrics. Each participant experienced one interaction under three conditions: no physicality (NP), static physicality (SP), and dynamic physicality (DP). SP extended the purely virtual baseline (NP) by introducing tangible props for direct contact, while DP further incorporated motion and impact forces to emulate natural touch. Results show presence increased stepwise from NP to SP to DP. Realism, enjoyment, and connection also improved with added physicality, though differences between SP and DP were not significant. Comfort remained consistent across conditions, indicating no added psychological friction. These findings demonstrate the experiential value of ETHOS and motivate the integration of encountered-type haptics into socially meaningful VR experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05683v1</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Godden, Jacquie Groenewegen, Michael Wheeler, Matthew K. X. J. Pan</dc:creator>
    </item>
    <item>
      <title>InsightEdu: Mobile Discord Bot Management and Analytics for Educators</title>
      <link>https://arxiv.org/abs/2511.05685</link>
      <description>arXiv:2511.05685v1 Announce Type: new 
Abstract: Modern educational environments increasingly rely on digital platforms to facilitate interaction between students and educators. Discord has emerged as a popular communication platform in academic settings, offering a combination of messaging and support for chatbot development. However, most existing Discord bots lack specialized educational functionalities and mobile-friendly interfaces, limiting their effectiveness for instructional use. This paper presents InsightEdu, an innovative iOS application that provides a touch-centric interface for managing a custom Discord bot designed for educational contexts. The system enables educators to conduct surveys, collect feedback, and track attendance through an intuitive mobile interface. The architecture combines a SwiftUI-based iOS client application with a Python-based Discord bot server. User evaluation with educators demonstrated significant usability improvements compared to traditional Discord interfaces, with 92% of participants (n = 20) reporting enhanced efficiency in managing educational interactions. This study demonstrates that mobile-first, instructor-friendly design can significantly enhance the utility of existing communication platforms for academic purposes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05685v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mihail Atanasov, Santiago Berrezueta-Guzman</dc:creator>
    </item>
    <item>
      <title>Exploring the Role of Theory of Mind in Human Decision Making: Cognitive, Spatial, and Emotional Influences in the Adversarial Rock-Paper-Scissors Game</title>
      <link>https://arxiv.org/abs/2511.05699</link>
      <description>arXiv:2511.05699v1 Announce Type: new 
Abstract: Understanding how humans attribute beliefs, goals, and intentions to others, known as theory of mind (ToM), is critical in the context of human-computer interaction. Despite various metrics used to assess ToM, the interplay between cognitive, spatial, and emotional factors in influencing human decision making during adversarial interactions remains underexplored. This paper investigates these relationships using the Rock-Paper-Scissors (RPS) game as a testbed. Through established ToM tests, we analyze how cognitive reasoning, spatial awareness, and emotional perceptiveness affect human performance when interacting with bots and human opponents in repeated RPS settings. Our findings reveal significant correlations among certain ToM metrics and highlight humans' ability to detect patterns in opponents' actions. However, most individual ToM metrics proved insufficient for predicting performance variations, with recursive thinking being the only metric moderately associated with decision effectiveness. Through exploratory factor analysis (EFA) and structural equation modeling (SEM), we identified two latent factors influencing decision effectiveness: Factor 1, characterized by recursive thinking, emotional perceptiveness, and spatial reasoning, positively affects decision-making against dynamic bots and human players, while Factor 2, linked to interpersonal skills and rational ability, has a negative impact. These insights lay the groundwork for further research on ToM metrics and for designing more intuitive, adaptive systems that better anticipate and adapt to human behavior, ultimately enhancing human-machine collaboration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05699v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thuy Ngoc Nguyen, Jeffrey Flagg, Cleotilde Gonzalez</dc:creator>
    </item>
    <item>
      <title>AdvisingWise: Supporting Academic Advising in Higher Educations Through a Human-in-the-Loop Multi-Agent Framework</title>
      <link>https://arxiv.org/abs/2511.05706</link>
      <description>arXiv:2511.05706v1 Announce Type: new 
Abstract: Academic advising is critical to student success in higher education, yet high student-to-advisor ratios limit advisors' capacity to provide timely support, particularly during peak periods. Recent advances in Large Language Models (LLMs) present opportunities to enhance the advising process. We present AdvisingWise, a multi-agent system that automates time-consuming tasks, such as information retrieval and response drafting, while preserving human oversight. AdvisingWise leverages authoritative institutional resources and adaptively prompts students about their academic backgrounds to generate reliable, personalized responses. All system responses undergo human advisor validation before delivery to students. We evaluate AdvisingWise through a mixed-methods approach: (1) expert evaluation on responses of 20 sample queries, (2) LLM-as-a-judge evaluation of the information retrieval strategy, and (3) a user study with 8 academic advisors to assess the system's practical utility. Our evaluation shows that AdvisingWise produces accurate, personalized responses. Advisors reported increasingly positive perceptions after using AdvisingWise, as their initial concerns about reliability and personalization diminished. We conclude by discussing the implications of human-AI synergy on the practice of academic advising.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05706v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wendan Jiang, Shiyuan Wang, Hiba Eltigani, Rukhshan Haroon, Abdullah Bin Faisal, Fahad Dogar</dc:creator>
    </item>
    <item>
      <title>Home Environment and Student Creative Thinking: An Educational Data Science Analysis of PISA 2022</title>
      <link>https://arxiv.org/abs/2511.05737</link>
      <description>arXiv:2511.05737v1 Announce Type: new 
Abstract: This study investigates how student exposure to resources in their home environments relates to creative thinking performance, using data from the PISA 2022 Creative Thinking assessment. It focuses on two primary questions: (1) How strongly is exposure to cultural, educational, and digital resources associated with creativity? (2) Do students perform better on divergent thinking tasks when physically engaged or digitally stimulated? Drawing on a sample of 15,425 students from 60 countries, the study applies high-dimensional regression and factor analysis to identify patterns across a wide range of exposure variables. To model the latent structure of home environment variables, we conducted a Confirmatory Factor Analysis. The analysis specified two latent factors: Physical Exposure and Digital Exposure. The model demonstrated excellent fit, with a Comparative Fit Index (CFI) of 0.971 and a Root Mean Square Error of Approximation (RMSEA) of 0.038. When both factors were entered together in the regression, physical and digital exposures each contributed unique explanatory power. There is no indication that one simply proxies the other; rather, they appear to be complementary dimensions of a creative home environment. This study offers compelling international evidence that both physical and digital resources in the home environment play significant, independent, and complementary roles in shaping adolescent creative thinking abilities. These findings have direct implications for efforts to promote creativity and equity in education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05737v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George X. Wang, Yuyang Shen</dc:creator>
    </item>
    <item>
      <title>Adaptive Time Budgets for Safe and Comfortable Vehicle Control Transition in Conditionally Automated Driving</title>
      <link>https://arxiv.org/abs/2511.05744</link>
      <description>arXiv:2511.05744v1 Announce Type: new 
Abstract: Conditionally automated driving requires drivers to resume vehicle control promptly when automation reaches its operational limits. Ensuring smooth vehicle control transitions is critical for the safety and efficiency of mixed-traffic transportation systems, where complex interactions and variable traffic behaviors pose additional challenges. This study addresses this challenge by introducing an adaptive time budget framework that provides drivers with sufficient time to complete takeovers both safely and comfortably across diverse scenarios. We focus in particular on the takeover buffer, that is, the extra time available after drivers consciously resume control to complete evasive maneuvers. A driving simulator experiment is conducted to evaluate the influence of different takeover buffer lengths on safety-related indicators (minimum time-to-collision, maximum deceleration, and steering wheel angle) and subjective assessments (perceived time sufficiency, perceived risk, and performance satisfaction). Results show that (i) takeover buffers of about 5-6 seconds consistently lead to optimal safety and comfort; and (ii) drivers prefer relatively stable takeover buffers across varying traffic densities and n-back tasks. This study introduces an adaptive time budget framework that dynamically allocates transition time by incorporating a predicted takeover time and a preferred takeover buffer (piecewise function). This can serve as an important first step toward providing drivers with sufficient time to resume vehicle control across diverse scenarios, which needs to be validated in more diverse and real-world driving contexts. By aligning the provided time budget with driver needs under specific circumstances, the adaptive framework can improve reliability of control transitions, facilitate human-centered automated driving, reduce crash risk, and maintain overall traffic efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05744v1</guid>
      <category>cs.HC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kexin Liang, Simeon C. Calvert, J. W. C. van Lint</dc:creator>
    </item>
    <item>
      <title>Lived Experience in Dialogue: Co-designing Personalization in Large Language Models to Support Youth Mental Well-being</title>
      <link>https://arxiv.org/abs/2511.05769</link>
      <description>arXiv:2511.05769v1 Announce Type: new 
Abstract: Youth increasingly turn to large language models (LLMs) for mental well-being support, yet current personalization in LLMs can overlook the heterogeneous lived experiences shaping their needs. We conducted a participatory study with youth, parents, and youth care workers (N=38), using co-created youth personas as scaffolds, to elicit community perspectives on how LLMs can facilitate more meaningful personalization to support youth mental well-being. Analysis identified three themes: person-centered contextualization responsive to momentary needs, explicit boundaries around scope and offline referral, and dialogic scaffolding for reflection and autonomy. We mapped these themes to persuasive design features for task suggestions, social facilitation, and system trustworthiness, and created corresponding dialogue extracts to guide LLM fine-tuning. Our findings demonstrate how lived experience can be operationalized to inform design features in LLMs, which can enhance the alignment of LLM-based interventions with the realities of youth and their communities, contributing to more effectively personalized digital well-being tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05769v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kathleen W. Guan, Sarthak Giri, Mohammed Amara, Bernard J. Jansen, Enrico Liscio, Milena Esherick, Mohammed Al Owayyed, Ausrine Ratkute, Gayane Sedrakyan, Mark de Reuver, Joao Fernando Ferreira Goncalves, Caroline A. Figueroa</dc:creator>
    </item>
    <item>
      <title>TalkSketch: Multimodal Generative AI for Real-time Sketch Ideation with Speech</title>
      <link>https://arxiv.org/abs/2511.05817</link>
      <description>arXiv:2511.05817v1 Announce Type: new 
Abstract: Sketching is a widely used medium for generating and exploring early-stage design concepts. While generative AI (GenAI) chatbots are increasingly used for idea generation, designers often struggle to craft effective prompts and find it difficult to express evolving visual concepts through text alone. In the formative study (N=6), we examined how designers use GenAI during ideation, revealing that text-based prompting disrupts creative flow. To address these issues, we developed TalkSketch, an embedded multimodal AI sketching system that integrates freehand drawing with real-time speech input. TalkSketch aims to support a more fluid ideation process through capturing verbal descriptions during sketching and generating context-aware AI responses. Our work highlights the potential of GenAI tools to engage the design process itself rather than focusing on output.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05817v1</guid>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <category>cs.SD</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Weiyan Shi, Sunaya Upadhyay, Geraldine Quek, Kenny Tsu Wei Choo</dc:creator>
    </item>
    <item>
      <title>Towards a Humanized Social-Media Ecosystem: AI-Augmented HCI Design Patterns for Safety, Agency &amp; Well-Being</title>
      <link>https://arxiv.org/abs/2511.05875</link>
      <description>arXiv:2511.05875v1 Announce Type: new 
Abstract: Social platforms connect billions of people, yet their engagement-first algorithms often work on users rather than with them, amplifying stress, misinformation, and a loss of control. We propose Human-Layer AI (HL-AI)--user-owned, explainable intermediaries that sit in the browser between platform logic and the interface. HL-AI gives people practical, moment-to-moment control without requiring platform cooperation. We contribute a working Chrome/Edge prototype implementing five representative pattern frameworks--Context-Aware Post Rewriter, Post Integrity Meter, Granular Feed Curator, Micro-Withdrawal Agent, and Recovery Mode--alongside a unifying mathematical formulation balancing user utility, autonomy costs, and risk thresholds. Evaluation spans technical accuracy, usability, and behavioral outcomes. The result is a suite of humane controls that help users rewrite before harm, read with integrity cues, tune feeds with intention, pause compulsive loops, and seek shelter during harassment, all while preserving agency through explanations and override options. This prototype offers a practical path to retrofit today's feeds with safety, agency, and well-being, inviting rigorous cross-cultural user evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05875v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohd Ruhul Ameen, Akif Islam</dc:creator>
    </item>
    <item>
      <title>Pinching Visuo-haptic Display: Investigating Cross-Modal Effects of Visual Textures on Electrostatic Cloth Tactile Sensations</title>
      <link>https://arxiv.org/abs/2511.05952</link>
      <description>arXiv:2511.05952v1 Announce Type: new 
Abstract: This paper investigates how visual texture presentation influences tactile perception when interacting with electrostatic cloth displays. We propose a visuo-haptic system that allows users to pinch and rub virtual fabrics while feeling realistic frictional sensations modulated by electrostatic actuation. Through a user study, we examined the cross-modal effects between visual roughness and perceived tactile friction. The results demonstrate that visually rough textures amplify the perceived frictional force, even under identical electrostatic stimuli. These findings contribute to the understanding of multimodal texture perception and provide design insights for haptic feedback in virtual material interfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05952v1</guid>
      <category>cs.HC</category>
      <category>cs.CV</category>
      <category>cs.MM</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3716553.3750810</arxiv:DOI>
      <dc:creator>Takekazu Kitagishi, Chun-Wei Ooi, Yuichi Hiroi, Jun Rekimoto</dc:creator>
    </item>
    <item>
      <title>Towards Misinformation Resilience in Pakistan: A Participatory Study with Low-Socioeconomic Status Adults</title>
      <link>https://arxiv.org/abs/2511.06147</link>
      <description>arXiv:2511.06147v1 Announce Type: new 
Abstract: Digital misinformation disproportionately affects low-socioeconomic status (SES) populations. While interventions for the Global South exist, they often report limited success, particularly among marginalized communities. Through a three-phase participatory study with 41 low-SES Pakistani adults, we conducted formative interviews to understand their information practices, followed by co-design sessions that translated these user-identified needs into concrete design requirements. Our findings reveal a sophisticated moral economy of sharing and a layered ecology of trust that prioritizes communal welfare. These insights inform the Scaffolded Support Model, a user-derived framework integrating on-demand assistance with gradual, inoculation-based skill acquisition. We instantiated this model in our prototype, "Pehchaan," and conducted usability testing (N=15), which confirmed its strong acceptance and cultural resonance, validating our culturally grounded approach. Our work contributes a foundational empirical account of non-Western misinformation practices, a replicable participatory methodology for inclusive design, and actionable principles for building information resilience in resource-constrained contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06147v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Abdullah Sohail, Amna Hassan, Shaheer Hammad, Salaar Masood, Suleman Shahid</dc:creator>
    </item>
    <item>
      <title>AI as intermediary in modern-day ritual: An immersive, interactive production of the roller disco musical Xanadu at UCLA</title>
      <link>https://arxiv.org/abs/2511.06195</link>
      <description>arXiv:2511.06195v1 Announce Type: new 
Abstract: Interfaces for contemporary large language, generative media, and perception AI models are often engineered for single user interaction. We investigate ritual as a design scaffold for developing collaborative, multi-user human-AI engagement. We consider the specific case of an immersive staging of the musical Xanadu performed at UCLA in Spring 2025. During a two-week run, over five hundred audience members contributed sketches and jazzercise moves that vision language models translated to virtual scenery elements and from choreographic prompts. This paper discusses four facets of interaction-as-ritual within the show: audience input as offerings that AI transforms into components of the ritual; performers as ritual guides, demonstrating how to interact with technology and sorting audience members into cohorts; AI systems as instruments "played" by the humans, in which sensing, generative components, and stagecraft create systems that can be mastered over time; and reciprocity of interaction, in which the show's AI machinery guides human behavior as well as being guided by humans, completing a human-AI feedback loop that visibly reshapes the virtual world. Ritual served as a frame for integrating linear narrative, character identity, music and interaction. The production explored how AI systems can support group creativity and play, addressing a critical gap in prevailing single user AI design paradigms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06195v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mira Winick, Naisha Agarwal, Chiheb Boussema, Ingrid Lee, Camilo Vargas, Jeff Burke</dc:creator>
    </item>
    <item>
      <title>Decomate: Leveraging Generative Models for Co-Creative SVG Animation</title>
      <link>https://arxiv.org/abs/2511.06297</link>
      <description>arXiv:2511.06297v1 Announce Type: new 
Abstract: Designers often encounter friction when animating static SVG graphics, especially when the visual structure does not match the desired level of motion detail. Existing tools typically depend on predefined groupings or require technical expertise, which limits designers' ability to experiment and iterate independently. We present Decomate, a system that enables intuitive SVG animation through natural language. Decomate leverages a multimodal large language model to restructure raw SVGs into semantically meaningful, animation-ready components. Designers can then specify motions for each component via text prompts, after which the system generates corresponding HTML/CSS/JS animations. By supporting iterative refinement through natural language interaction, Decomate integrates generative AI into creative workflows, allowing animation outcomes to be directly shaped by user intent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06297v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jihyeon Park, Jiyoon Myung, Seone Shin, Jungki Son, Joohyung Han</dc:creator>
    </item>
    <item>
      <title>Personality over Precision: Exploring the Influence of Human-Likeness on ChatGPT Use for Search</title>
      <link>https://arxiv.org/abs/2511.06447</link>
      <description>arXiv:2511.06447v1 Announce Type: new 
Abstract: Conversational search interfaces, like ChatGPT, offer an interactive, personalized, and engaging user experience compared to traditional search. On the downside, they are prone to cause overtrust issues where users rely on their responses even when they are incorrect. What aspects of the conversational interaction paradigm drive people to adopt it, and how it creates personalized experiences that lead to overtrust, is not clear. To understand the factors influencing the adoption of conversational interfaces, we conducted a survey with 173 participants. We examined user perceptions regarding trust, human-likeness (anthropomorphism), and design preferences between ChatGPT and Google. To better understand the overtrust phenomenon, we asked users about their willingness to trade off factuality for constructs like ease of use or human-likeness. Our analysis identified two distinct user groups: those who use both ChatGPT and Google daily (DUB), and those who primarily rely on Google (DUG). The DUB group exhibited higher trust in ChatGPT, perceiving it as more human-like, and expressed greater willingness to trade factual accuracy for enhanced personalization and conversational flow. Conversely, the DUG group showed lower trust toward ChatGPT but still appreciated aspects like ad-free experiences and responsive interactions. Demographic analysis further revealed nuanced patterns, with middle-aged adults using ChatGPT less frequently yet trusting it more, suggesting potential vulnerability to misinformation. Our findings contribute to understanding user segmentation, emphasizing the critical roles of personalization and human-likeness in conversational IR systems, and reveal important implications regarding users' willingness to compromise factual accuracy for more engaging interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06447v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mert Yazan, Frederik Bungaran Ishak Situmeang, Suzan Verberne</dc:creator>
    </item>
    <item>
      <title>Towards Attention-Aware Large Language Models: Integrating Real-Time Eye-Tracking and EEG for Adaptive AI Responses</title>
      <link>https://arxiv.org/abs/2511.06468</link>
      <description>arXiv:2511.06468v1 Announce Type: new 
Abstract: This project proposes an attention-aware LLM that integrates EEG and eye tracking to monitor and measure user attention dynamically. To realize this, the project will integrate real-time EEG and eye-tracking data into an LLM-based interactive system and classify the user's attention state on the fly. The system can identify five attention states: High Attention, Stable Attention, Dropping Attention, Cognitive Overload, and Distraction. It responds accordingly to each state, with a particular focus on adapting to decreased attention, distraction, and cognitive overload to improve user engagement and reduce cognitive load.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06468v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dan Zhang</dc:creator>
    </item>
    <item>
      <title>HugSense: Exploring the Sensing Capabilities of Inflatables</title>
      <link>https://arxiv.org/abs/2511.06532</link>
      <description>arXiv:2511.06532v1 Announce Type: new 
Abstract: What information can we get using inflatables as sensors? While using inflatables as actuators for various interactions has been widely adopted in the HCI community, using the sensing capabilities of inflatables is much less common. Almost all inflatable setups include air pressure sensors as part of the automation when pressurizing or deflating, but the full potential of those sensors is rarely explored. This paper shows how to turn a complete pillow into a force sensor using an inflatable and a simple pneumatics setup including an air pressure sensor. We will show that this setup yields accurate and interesting data that warrants further exploration and elaborate on the potential for practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06532v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Klaus Stephan, Maximilian Eibl, Albrecht Kurze</dc:creator>
    </item>
    <item>
      <title>Accessibility Gaps in U.S. Government Dashboards for Blind and Low-Vision Residents</title>
      <link>https://arxiv.org/abs/2511.06688</link>
      <description>arXiv:2511.06688v1 Announce Type: new 
Abstract: Public dashboards are now a common way for US government agencies to share high stakes information with residents. We audited six live systems at federal, state, and city levels: CDC respiratory illness, HUD homelessness PIT and HIC, California HCD Annual Progress Report, New York City Mayor's Management Report, Houston Permitting, and Chicago public health and budget dashboards. Using a rubric based on screen reader needs and WCAG, we checked five items: (1) discoverability of key metrics by assistive tech, (2) keyboard access without mouse hover, (3) clear semantic labels for axes, series, and categories, (4) short plain language status and trend notes, and (5) machine readable tables or CSVs that mirror what sighted users see. Findings are mixed. Many charts fail basic discoverability or depend on hover, which blocks keyboard and screen reader use. Plain language summaries are common in CDC and Chicago, but rare in HUD and Houston. Machine readable data is strong for NYC, California, and HUD; it is weaker or unclear for Houston. Several sites promise service for the public or for customers yet do not name accessibility in their descriptions. Across systems we also observe urgency inversion: faster, operational dashboards tend to provide fewer accessible affordances than slower accountability dashboards. These patterns matter for equal participation and for ADA Title II compliance that references WCAG 2.1 AA. We propose three steps for any public dashboard: add a brief status and trend text at the same update cadence, publish a matching table or CSV of the visual metrics, and state an explicit accessibility commitment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06688v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.IR</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chadani Acharya</dc:creator>
    </item>
    <item>
      <title>HEDN: A Hard-Easy Dual Network with Task Difficulty Assessment for EEG Emotion Recognition</title>
      <link>https://arxiv.org/abs/2511.06782</link>
      <description>arXiv:2511.06782v1 Announce Type: new 
Abstract: Multi-source domain adaptation represents an effective approach to addressing individual differences in cross-subject EEG emotion recognition. However, existing methods treat all source domains equally, neglecting the varying transfer difficulties between different source domains and the target domain. This oversight can lead to suboptimal adaptation. To address this challenge, we propose a novel Hard-Easy Dual Network (HEDN), which dynamically identifies "Hard Source" and "Easy Source" through a Task Difficulty Assessment (TDA) mechanism and establishes two specialized knowledge adaptation branches. Specifically, the Hard Network is dedicated to handling "Hard Source" with higher transfer difficulty by aligning marginal distribution differences between source and target domains. Conversely, the Easy Network focuses on "Easy Source" with low transfer difficulty, utilizing a prototype classifier to model intra-class clustering structures while generating reliable pseudo-labels for the target domain through a prototype-guided label propagation algorithm. Extensive experiments on two benchmark datasets, SEED and SEED-IV, demonstrate that HEDN achieves state-of-the-art performance in cross-subject EEG emotion recognition, with average accuracies of 93.58\% on SEED and 79.82\% on SEED-IV, respectively. These results confirm the effectiveness and generalizability of HEDN in cross-subject EEG emotion recognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06782v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiang Wang, Liying Yang</dc:creator>
    </item>
    <item>
      <title>AgentSUMO: An Agentic Framework for Interactive Simulation Scenario Generation in SUMO via Large Language Models</title>
      <link>https://arxiv.org/abs/2511.06804</link>
      <description>arXiv:2511.06804v1 Announce Type: new 
Abstract: The growing complexity of urban mobility systems has made traffic simulation indispensable for evidence-based transportation planning and policy evaluation. However, despite the analytical capabilities of platforms such as the Simulation of Urban MObility (SUMO), their application remains largely confined to domain experts. Developing realistic simulation scenarios requires expertise in network construction, origin-destination modeling, and parameter configuration for policy experimentation, creating substantial barriers for non-expert users such as policymakers, urban planners, and city officials. Moreover, the requests expressed by these users are often incomplete and abstract-typically articulated as high-level objectives, which are not well aligned with the imperative, sequential workflows employed in existing language-model-based simulation frameworks. To address these challenges, this study proposes AgentSUMO, an agentic framework for interactive simulation scenario generation via large language models. AgentSUMO departs from imperative, command-driven execution by introducing an adaptive reasoning layer that interprets user intents, assesses task complexity, infers missing parameters, and formulates executable simulation plans. The framework is structured around two complementary components, the Interactive Planning Protocol, which governs reasoning and user interaction, and the Model Context Protocol, which manages standardized communication and orchestration among simulation tools. Through this design, AgentSUMO converts abstract policy objectives into executable simulation scenarios. Experiments on urban networks in Seoul and Manhattan demonstrate that the agentic workflow achieves substantial improvements in traffic flow metrics while maintaining accessibility for non-expert users, successfully bridging the gap between policy goals and executable simulation workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06804v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Minwoo Jeong, Jeeyun Chang, Yoonjin Yoon</dc:creator>
    </item>
    <item>
      <title>A Low-Cost Embedded System for Automated Patient Queue and Health Data Management in Private Medical Chambers</title>
      <link>https://arxiv.org/abs/2511.06914</link>
      <description>arXiv:2511.06914v1 Announce Type: new 
Abstract: This paper presents the design and implementation of a low-cost microcontroller-based system for managing patient queues and preliminary health data collection in private medical chambers. Patient registration, queue management, and the collection of fundamental health metrics such as heart rate and body temperature are automated by the system. The proposed setup integrates an ATmega32 microcontroller, an LM35 temperature sensor, an XD-58C pulse sensor, 4x4 matrix keypads, and 16x2 LCD displays. The system separates patient-side input from doctor-side control, allowing doctors to call patients sequentially with a single button. Experimental evaluation conducted under limited hardware conditions demonstrates that the system reduces manual labor and contact-based data collection, making it feasible for small private practices in developing regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06914v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kawshik Kumar Paul, Mahdi Hasnat Siyam, Khandokar Md. Rahat Hossain</dc:creator>
    </item>
    <item>
      <title>Personalizing Emotion-aware Conversational Agents? Exploring User Traits-driven Conversational Strategies for Enhanced Interaction</title>
      <link>https://arxiv.org/abs/2511.06954</link>
      <description>arXiv:2511.06954v1 Announce Type: new 
Abstract: Conversational agents (CAs) are increasingly embedded in daily life, yet their ability to navigate user emotions efficiently is still evolving. This study investigates how users with varying traits -- gender, personality, and cultural background -- adapt their interaction strategies with emotion-aware CAs in specific emotional scenarios. Using an emotion-aware CA prototype expressing five distinct emotions (neutral, happy, sad, angry, and fear) through male and female voices, we examine how interaction dynamics shift across different voices and emotional contexts through empirical studies. Our findings reveal distinct variations in user engagement and conversational strategies based on individual traits, emphasizing the value of personalized, emotion-sensitive interactions. By analyzing both qualitative and quantitative data, we demonstrate that tailoring CAs to user characteristics can enhance user satisfaction and interaction quality. This work underscores the critical need for ongoing research to design CAs that not only recognize but also adaptively respond to emotional needs, ultimately supporting a diverse user groups more effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06954v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuchong Zhang, Yong Ma, Di Fu, Stephanie Zubicueta Portales, Morten Fjeld, Danica Kragic</dc:creator>
    </item>
    <item>
      <title>Achieving Effective Virtual Reality Interactions via Acoustic Gesture Recognition based on Large Language Models</title>
      <link>https://arxiv.org/abs/2511.07085</link>
      <description>arXiv:2511.07085v1 Announce Type: new 
Abstract: Natural and efficient interaction remains a critical challenge for virtual reality and augmented reality (VR/AR) systems. Vision-based gesture recognition suffers from high computational cost, sensitivity to lighting conditions, and privacy leakage concerns. Acoustic sensing provides an attractive alternative: by emitting inaudible high-frequency signals and capturing their reflections, channel impulse response (CIR) encodes how gestures perturb the acoustic field in a low-cost and user-transparent manner. However, existing CIR-based gesture recognition methods often rely on extensive training of models on large labeled datasets, making them unsuitable for few-shot VR scenarios. In this work, we propose the first framework that leverages large language models (LLMs) for CIR-based gesture recognition in VR/AR systems. Despite LLMs' strengths, it is non-trivial to achieve few-shot and zero-shot learning of CIR gestures due to their inconspicuous features. To tackle this challenge, we collect differential CIR rather than original CIR data. Moreover, we construct a real-world dataset collected from 10 participants performing 15 gestures across three categories (digits, letters, and shapes), with 10 repetitions each. We then conduct extensive experiments on this dataset using an LLM-adopted classifier. Results show that our LLM-based framework achieves accuracy comparable to classical machine learning baselines, while requiring no domain-specific retraining.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07085v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xijie Zhang, Fengliang He, Hong-Ning Dai</dc:creator>
    </item>
    <item>
      <title>NoteEx: Interactive Visual Context Manipulation for LLM-Assisted Exploratory Data Analysis in Computational Notebooks</title>
      <link>https://arxiv.org/abs/2511.07223</link>
      <description>arXiv:2511.07223v1 Announce Type: new 
Abstract: Computational notebooks have become popular for Exploratory Data Analysis (EDA), augmented by LLM-based code generation and result interpretation. Effective LLM assistance hinges on selecting informative context -- the minimal set of cells whose code, data, or outputs suffice to answer a prompt. As notebooks grow long and messy, users can lose track of the mental model of their analysis. They thus fail to curate appropriate contexts for LLM tasks, causing frustration and tedious prompt engineering. We conducted a formative study (n=6) that surfaced challenges in LLM context selection and mental model maintenance. Therefore, we introduce NoteEx, a JupyterLab extension that provides a semantic visualization of the EDA workflow, allowing analysts to externalize their mental model, specify analysis dependencies, and enable interactive selection of task-relevant contexts for LLMs. A user study (n=12) against a baseline shows that NoteEx improved mental model retention and context selection, leading to more accurate and relevant LLM responses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07223v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mohammad Hasan Payandeh, Lin-Ping Yuan, Jian Zhao</dc:creator>
    </item>
    <item>
      <title>Designing Beyond Language: Sociotechnical Barriers in AI Health Technologies for Limited English Proficiency</title>
      <link>https://arxiv.org/abs/2511.07277</link>
      <description>arXiv:2511.07277v1 Announce Type: new 
Abstract: Limited English proficiency (LEP) patients in the U.S. face systemic barriers to healthcare beyond language and interpreter access, encompassing procedural and institutional constraints. AI advances may support communication and care through on-demand translation and visit preparation, but also risk exacerbating existing inequalities. We conducted storyboard-driven interviews with 14 patient navigators to explore how AI could shape care experiences for Spanish-speaking LEP individuals. We identified tensions around linguistic and cultural misunderstandings, privacy concerns, and opportunities and risks for AI to augment care workflows. Participants highlighted structural factors that can undermine trust in AI systems, including sensitive information disclosure, unstable technology access, and low digital literacy. While AI tools can potentially alleviate social barriers and institutional constraints, there are risks of misinformation and uprooting human camaraderie. Our findings contribute design considerations for AI that support LEP patients and care teams via rapport-building, education, and language support, and minimizing disruptions to existing practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07277v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michelle Huang, Violeta J. Rodriguez, Koustuv Saha, Tal August</dc:creator>
    </item>
    <item>
      <title>People Perceive More Phantom Costs From Autonomous Agents When They Make Unreasonably Generous Offers</title>
      <link>https://arxiv.org/abs/2511.07401</link>
      <description>arXiv:2511.07401v1 Announce Type: new 
Abstract: People often reject offers that are too generous due to the perception of hidden drawbacks referred to as "phantom costs." We hypothesized that this perception and the decision-making vary based on the type of agent making the offer (human vs. robot) and the degree to which the agent is perceived to be autonomous or have the capacity for self-interest. To test this conjecture, participants (N = 855) engaged in a car-buying simulation where a human or robot sales agent, described as either autonomous or not, offered either a small (5%) or large (85%) discount. Results revealed that the robot was perceived as less self-interested than the human, which reduced the perception of phantom costs. While larger discounts increased phantom costs, they also increased purchase intentions, suggesting that perceived benefits can outweigh phantom costs. Importantly, phantom costs were not only attributed to the agent participants interacted with, but also to the product and the agent's manager, highlighting at least three sources of suspicion. These findings deepen our understanding of to whom people assign responsibility and how perceptions shape both human-human and human-robot interactions, with implications for ethical AI design and marketing strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07401v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Benjamin Lebrun, Christoph Bartneck, David Kaber, Andrew Vonasch</dc:creator>
    </item>
    <item>
      <title>AgriTrust: a Federated Semantic Governance Framework for Trusted Agricultural Data Sharing</title>
      <link>https://arxiv.org/abs/2511.05572</link>
      <description>arXiv:2511.05572v1 Announce Type: cross 
Abstract: The potential of agricultural data (AgData) to drive efficiency and sustainability is stifled by the "AgData Paradox": a pervasive lack of trust and interoperability that locks data in silos, despite its recognized value. This paper introduces AgriTrust, a federated semantic governance framework designed to resolve this paradox. AgriTrust integrates a multi-stakeholder governance model, built on pillars of Data Sovereignty, Transparent Data Contracts, Equitable Value Sharing, and Regulatory Compliance, with a semantic digital layer. This layer is realized through the AgriTrust Core Ontology, a formal OWL ontology that provides a shared vocabulary for tokenization, traceability, and certification, enabling true semantic interoperability across independent platforms. A key innovation is a blockchain-agnostic, multi-provider architecture that prevents vendor lock-in. The framework's viability is demonstrated through case studies across three critical Brazilian supply chains: coffee (for EUDR compliance), soy (for mass balance), and beef (for animal tracking). The results show that AgriTrust successfully enables verifiable provenance, automates compliance, and creates new revenue streams for data producers, thereby transforming data sharing from a trust-based dilemma into a governed, automated operation. This work provides a foundational blueprint for a more transparent, efficient, and equitable agricultural data economy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05572v1</guid>
      <category>cs.CY</category>
      <category>cs.CE</category>
      <category>cs.CR</category>
      <category>cs.DB</category>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ivan Bergier</dc:creator>
    </item>
    <item>
      <title>Approximating the Mathematical Structure of Psychodynamics</title>
      <link>https://arxiv.org/abs/2511.05580</link>
      <description>arXiv:2511.05580v1 Announce Type: cross 
Abstract: The complexity of human cognition has meant that psychology makes more use of theory and conceptual models than perhaps any other biomedical field. To enable precise quantitative study of the full breadth of phenomena in psychological and psychiatric medicine as well as cognitive aspects of AI safety, there is a need for a mathematical formulation which is both mathematically precise and equally accessible to experts from numerous fields. In this paper we formalize human psychodynamics via the diagrammatic framework of process theory, describe its key properties, and explain the links between a diagrammatic representation and central concepts in analysis of cognitive processes in contexts such as psychotherapy, neurotechnology, AI alignment, AI agent representation of individuals in autonomous negotiations, developing human-like AI systems, and other aspects of AI safety.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05580v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bryce-Allen Bagley, Navin Khoshnan</dc:creator>
    </item>
    <item>
      <title>(Working Paper) Good Faith Design: Religion as a Resource for Technologists</title>
      <link>https://arxiv.org/abs/2511.05819</link>
      <description>arXiv:2511.05819v1 Announce Type: cross 
Abstract: Previous work has found a lack of research in HCI on religion, partly driven by misunderstandings of values and practices between religious and technical communities. To bridge this divide in an empirically rigorous way, we conducted an interview study with 48 religious people and/or experts from 11 faiths, and we document how religious people experience, understand, and imagine technologies. We show that religious stakeholders find non-neutral secular embeddings in technologies and the firms and people that design them, and how these manifest in unintended harms for religious and nonreligious users. Our findings reveal how users navigate technoreligious practices with religiously informed mental models and what they desire from technologies. Informed by this, we distill six design values -- wonder, humility, space, embodiedness, community, and eternity -- to guide technologists in considering and leveraging religion as an additional, valid sociocultural resource when designing for a holistic user. We further spell out directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05819v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nina Lutz, Benjamin Olsen, Weishung Liu, E. Glen Weyl</dc:creator>
    </item>
    <item>
      <title>The Imperfect Learner: Incorporating Developmental Trajectories in Memory-based Student Simulation</title>
      <link>https://arxiv.org/abs/2511.05903</link>
      <description>arXiv:2511.05903v1 Announce Type: cross 
Abstract: User simulation is important for developing and evaluating human-centered AI, yet current student simulation in educational applications has significant limitations. Existing approaches focus on single learning experiences and do not account for students' gradual knowledge construction and evolving skill sets. Moreover, large language models are optimized to produce direct and accurate responses, making it challenging to represent the incomplete understanding and developmental constraints that characterize real learners. In this paper, we introduce a novel framework for memory-based student simulation that incorporates developmental trajectories through a hierarchical memory mechanism with structured knowledge representation. The framework also integrates metacognitive processes and personality traits to enrich the individual learner profiling, through dynamical consolidation of both cognitive development and personal learning characteristics. In practice, we implement a curriculum-aligned simulator grounded on the Next Generation Science Standards. Experimental results show that our approach can effectively reflect the gradual nature of knowledge development and the characteristic difficulties students face, providing a more accurate representation of learning processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05903v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhengyuan Liu, Stella Xin Yin, Bryan Chen Zhengyu Tan, Roy Ka-Wei Lee, Guimei Liu, Dion Hoe-Lian Goh, Wenya Wang, Nancy F. Chen</dc:creator>
    </item>
    <item>
      <title>An Artificial Intelligence-based Assistant for the Visually Impaired</title>
      <link>https://arxiv.org/abs/2511.06080</link>
      <description>arXiv:2511.06080v2 Announce Type: cross 
Abstract: This paper describes an artificial intelligence-based assistant application, AIDEN, developed during 2023 and 2024, aimed at improving the quality of life for visually impaired individuals. Visually impaired individuals face challenges in identifying objects, reading text, and navigating unfamiliar environments, which can limit their independence and reduce their quality of life. Although solutions such as Braille, audio books, and screen readers exist, they may not be effective in all situations. This application leverages state-of-the-art machine learning algorithms to identify and describe objects, read text, and answer questions about the environment. Specifically, it uses You Only Look Once architectures and a Large Language and Vision Assistant. The system incorporates several methods to facilitate the user's interaction with the system and access to textual and visual information in an appropriate manner. AIDEN aims to enhance user autonomy and access to information, contributing to an improved perception of daily usability, as supported by user feedback.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06080v2</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luis Marquez-Carpintero, Francisco Gomez-Donoso, Zuria Bauer, Bessie Dominguez-Dager, Alvaro Belmonte-Baeza, M\'onica Pina-Navarro, Francisco Morillas-Espejo, Felix Escalona, Miguel Cazorla</dc:creator>
    </item>
    <item>
      <title>Scene-Aware Urban Design: A Human-AI Recommendation Framework Using Co-Occurrence Embeddings and Vision-Language Models</title>
      <link>https://arxiv.org/abs/2511.06201</link>
      <description>arXiv:2511.06201v1 Announce Type: cross 
Abstract: This paper introduces a human-in-the-loop computer vision framework that uses generative AI to propose micro-scale design interventions in public space and support more continuous, local participation. Using Grounding DINO and a curated subset of the ADE20K dataset as a proxy for the urban built environment, the system detects urban objects and builds co-occurrence embeddings that reveal common spatial configurations. From this analysis, the user receives five statistically likely complements to a chosen anchor object. A vision language model then reasons over the scene image and the selected pair to suggest a third object that completes a more complex urban tactic. The workflow keeps people in control of selection and refinement and aims to move beyond top-down master planning by grounding choices in everyday patterns and lived experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06201v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rodrigo Gallardo, Oz Fishman, Alexander Htet Kyaw</dc:creator>
    </item>
    <item>
      <title>How AI Fails: An Interactive Pedagogical Tool for Demonstrating Dialectal Bias in Automated Toxicity Models</title>
      <link>https://arxiv.org/abs/2511.06676</link>
      <description>arXiv:2511.06676v1 Announce Type: cross 
Abstract: Now that AI-driven moderation has become pervasive in everyday life, we often hear claims that "the AI is biased". While this is often said jokingly, the light-hearted remark reflects a deeper concern. How can we be certain that an online post flagged as "inappropriate" was not simply the victim of a biased algorithm? This paper investigates this problem using a dual approach. First, I conduct a quantitative benchmark of a widely used toxicity model (unitary/toxic-bert) to measure performance disparity between text in African-American English (AAE) and Standard American English (SAE). The benchmark reveals a clear, systematic bias: on average, the model scores AAE text as 1.8 times more toxic and 8.8 times higher for "identity hate". Second, I introduce an interactive pedagogical tool that makes these abstract biases tangible. The tool's core mechanic, a user-controlled "sensitivity threshold," demonstrates that the biased score itself is not the only harm; instead, the more-concerning harm is the human-set, seemingly neutral policy that ultimately operationalises discrimination. This work provides both statistical evidence of disparate impact and a public-facing tool designed to foster critical AI literacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06676v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Subhojit Ghimire</dc:creator>
    </item>
    <item>
      <title>HCFSLN: Adaptive Hyperbolic Few-Shot Learning for Multimodal Anxiety Detection</title>
      <link>https://arxiv.org/abs/2511.06988</link>
      <description>arXiv:2511.06988v1 Announce Type: cross 
Abstract: Anxiety disorders impact millions globally, yet traditional diagnosis relies on clinical interviews, while machine learning models struggle with overfitting due to limited data. Large-scale data collection remains costly and time-consuming, restricting accessibility. To address this, we introduce the Hyperbolic Curvature Few-Shot Learning Network (HCFSLN), a novel Few-Shot Learning (FSL) framework for multimodal anxiety detection, integrating speech, physiological signals, and video data. HCFSLN enhances feature separability through hyperbolic embeddings, cross-modal attention, and an adaptive gating network, enabling robust classification with minimal data. We collected a multimodal anxiety dataset from 108 participants and benchmarked HCFSLN against six FSL baselines, achieving 88% accuracy, outperforming the best baseline by 14%. These results highlight the effectiveness of hyperbolic space for modeling anxiety-related speech patterns and demonstrate FSL's potential for anxiety classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06988v1</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aditya Sneh, Nilesh Kumar Sahu, Anushka Sanjay Shelke, Arya Adyasha, Haroon R. Lone</dc:creator>
    </item>
    <item>
      <title>Exploring the "Great Unseen" in Medieval Manuscripts: Instance-Level Labeling of Legacy Image Collections with Zero-Shot Models</title>
      <link>https://arxiv.org/abs/2511.07004</link>
      <description>arXiv:2511.07004v1 Announce Type: cross 
Abstract: We aim to theorize the medieval manuscript page and its contents more holistically, using state-of-the-art techniques to segment and describe the entire manuscript folio, for the purpose of creating richer training data for computer vision techniques, namely instance segmentation, and multimodal models for medieval-specific visual content.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07004v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Christofer Meinecke, Estelle Gu\'eville, David Joseph Wrisley</dc:creator>
    </item>
    <item>
      <title>A Picture is Worth a Thousand (Correct) Captions: A Vision-Guided Judge-Corrector System for Multimodal Machine Translation</title>
      <link>https://arxiv.org/abs/2511.07010</link>
      <description>arXiv:2511.07010v1 Announce Type: cross 
Abstract: In this paper, we describe our system under the team name BLEU Monday for the English-to-Indic Multimodal Translation Task at WAT 2025. We participate in the text-only translation tasks for English-Hindi, English-Bengali, English-Malayalam, and English-Odia language pairs. We present a two-stage approach that addresses quality issues in the training data through automated error detection and correction, followed by parameter-efficient model fine-tuning.
  Our methodology introduces a vision-augmented judge-corrector pipeline that leverages multimodal language models to systematically identify and correct translation errors in the training data. The judge component classifies translations into three categories: correct, visually ambiguous (requiring image context), or mistranslated (poor translation quality). Identified errors are routed to specialized correctors: GPT-4o-mini regenerates captions requiring visual disambiguation, while IndicTrans2 retranslates cases with pure translation quality issues. This automated pipeline processes 28,928 training examples across four languages, correcting an average of 17.1% of captions per language.
  We then apply Low-Rank Adaptation (LoRA) to fine-tune the IndicTrans2 en-indic 200M distilled model on both original and corrected datasets. Training on corrected data yields consistent improvements, with BLEU score gains of +1.30 for English-Bengali on the evaluation set (42.00 -&gt; 43.30) and +0.70 on the challenge set (44.90 -&gt; 45.60), +0.60 for English-Odia on the evaluation set (41.00 -&gt; 41.60), and +0.10 for English-Hindi on the challenge set (53.90 -&gt; 54.00).</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07010v1</guid>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siddharth Betala, Kushan Raj, Vipul Betala, Rohan Saswade</dc:creator>
    </item>
    <item>
      <title>Robotic versus Human Teleoperation for Remote Ultrasound</title>
      <link>https://arxiv.org/abs/2511.07275</link>
      <description>arXiv:2511.07275v1 Announce Type: cross 
Abstract: Diagnostic medical ultrasound is widely used, safe, and relatively low cost but requires a high degree of expertise to acquire and interpret the images. Personnel with this expertise are often not available outside of larger cities, leading to difficult, costly travel and long wait times for rural populations. To address this issue, tele-ultrasound techniques are being developed, including robotic teleoperation and recently human teleoperation, in which a novice user is remotely guided in a hand-over-hand manner through mixed reality to perform an ultrasound exam. These methods have not been compared, and their relative strengths are unknown. Human teleoperation may be more practical than robotics for small communities due to its lower cost and complexity, but this is only relevant if the performance is comparable. This paper therefore evaluates the differences between human and robotic teleoperation, examining practical aspects such as setup time and flexibility and experimentally comparing performance metrics such as completion time, position tracking, and force consistency. It is found that human teleoperation does not lead to statistically significant differences in completion time or position accuracy, with mean differences of 1.8% and 0.5%, respectively, and provides more consistent force application despite being substantially more practical and accessible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07275v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Black, Septimiu Salcudean</dc:creator>
    </item>
    <item>
      <title>DigiData: Training and Evaluating General-Purpose Mobile Control Agents</title>
      <link>https://arxiv.org/abs/2511.07413</link>
      <description>arXiv:2511.07413v1 Announce Type: cross 
Abstract: AI agents capable of controlling user interfaces have the potential to transform human interaction with digital devices. To accelerate this transformation, two fundamental building blocks are essential: high-quality datasets that enable agents to achieve complex and human-relevant goals, and robust evaluation methods that allow researchers and practitioners to rapidly enhance agent performance. In this paper, we introduce DigiData, a large-scale, high-quality, diverse, multi-modal dataset designed for training mobile control agents. Unlike existing datasets, which derive goals from unstructured interactions, DigiData is meticulously constructed through comprehensive exploration of app features, resulting in greater diversity and higher goal complexity. Additionally, we present DigiData-Bench, a benchmark for evaluating mobile control agents on real-world complex tasks. We demonstrate that the commonly used step-accuracy metric falls short in reliably assessing mobile control agents and, to address this, we propose dynamic evaluation protocols and AI-powered evaluations as rigorous alternatives for agent assessment. Our contributions aim to significantly advance the development of mobile control agents, paving the way for more intuitive and effective human-device interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07413v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxuan Sun, Manchen Wang, Shengyi Qian, William R. Wong, Eric Gan, Pierluca D'Oro, Alejandro Castillejo Munoz, Sneha Silwal, Pedro Matias, Nitin Kamra, Satwik Kottur, Nick Raines, Xuanyi Zhao, Joy Chen, Joseph Greer, Andrea Madotto, Allen Bolourchi, James Valori, Kevin Carlberg, Karl Ridgeway, Joseph Tighe</dc:creator>
    </item>
    <item>
      <title>Feasibility of Detecting Cognitive Impairment and Psychological Well-being among Older Adults Using Facial, Acoustic, Linguistic, and Cardiovascular Patterns Derived from Remote Conversations</title>
      <link>https://arxiv.org/abs/2412.14194</link>
      <description>arXiv:2412.14194v4 Announce Type: replace 
Abstract: The aging society urgently requires scalable methods to monitor cognitive decline and identify social and psychological factors indicative of dementia risk in older adults. Our machine learning (ML) models captured facial, acoustic, linguistic, and cardiovascular features from 39 older adults with normal cognition or Mild Cognitive Impairment (MCI), derived from remote video conversations and quantified their cognitive status, social isolation, neuroticism, and psychological well-being. Our model could distinguish Clinical Dementia Rating Scale (CDR) of 0.5 (vs. 0) with 0.77 area under the receiver operating characteristic curve (AUC), social isolation with 0.74 AUC, social satisfaction with 0.75 AUC, psychological well-being with 0.72 AUC, and negative affect with 0.74 AUC. Our feature importance analysis showed that speech and language patterns were useful for quantifying cognitive impairment, whereas facial expressions and cardiovascular patterns were useful for quantifying social and psychological well-being. Our bias analysis showed that the best-performing models for quantifying psychological well-being and cognitive states in older adults exhibited significant biases concerning their age, sex, disease condition, and education levels. Our comprehensive analysis shows the feasibility of monitoring the cognitive and psychological health of older adults, as well as the need for collecting largescale interview datasets of older adults to benefit from the latest advances in deep learning technologies to develop generalizable models across older adults with diverse demographic backgrounds and disease conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14194v4</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xiaofan Mu, Merna Bibars, Salman Seyedi, Iris Zheng, Zifan Jiang, Liu Chen, Bolaji Omofojoye, Rachel Hershenberg, Allan I. Levey, Gari D. Clifford, Hiroko H. Dodge, Hyeokhyen Kwon</dc:creator>
    </item>
    <item>
      <title>From Checking to Sensemaking: A Caregiver-in-the-Loop Framework for AI-Assisted Task Verification in Dementia Care</title>
      <link>https://arxiv.org/abs/2508.18267</link>
      <description>arXiv:2508.18267v2 Announce Type: replace 
Abstract: Informal caregivers play a central role in enabling people living with dementia (PLwD) to remain at home, yet they face persistent challenges verifying whether daily tasks have been completed. Existing digital reminder systems prompt actions but rarely confirm outcomes, leaving caregivers to double-check tasks manually. This study explores how generative artificial intelligence (AI) might support caregiver-led task verification without displacing human judgment. We combined qualitative interviews with ten caregivers and one PLwD with a speculative simulation probe using a generative large language model to generate follow-up questions and flag responses for verification. Using template analysis, we identified three interrelated patterns of reasoning: detecting anomalies, constructing trustworthy evidence, and calibrating trust and control. These insights informed the Caregiver-in-the-Loop Task Verification (CLTV) framework, which models verification as a collaborative cycle of anomaly detection, evidence triangulation, AI-assisted summarization, and accountability circulation centered on caregiver oversight. CLTV advances human-AI collaboration theory by situating interpretability, trust, and control within the relational and emotional realities of dementia care and by offering design principles for transparent, adjustable, and context-aware AI support. We contribute a care-centered extension of human-AI collaboration theory, demonstrating how interpretability and trust can be operationalized through caregiver oversight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18267v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joy Lai, Kelly Beaton, David Black, Bing Ye, Alex Mihailidis</dc:creator>
    </item>
    <item>
      <title>PLUTO: A Public Value Assessment Tool</title>
      <link>https://arxiv.org/abs/2509.12773</link>
      <description>arXiv:2509.12773v3 Announce Type: replace 
Abstract: We present PLUTO (Public VaLUe Assessment TOol), a framework for assessing the public value of specific instances of data use. Grounded in the concept of data solidarity, PLUTO aims to empower diverse stakeholders-including regulatory bodies, private enterprises, NGOs, and individuals-to critically engage with data projects through a structured assessment of the risks and benefits of data use, and by encouraging critical reflection. This paper discusses the theoretical foundation, development process, and initial user experiences with PLUTO. Key challenges include translating qualitative assessments of benefits and risks into actionable quantitative metrics while maintaining inclusivity and transparency. Initial feedback highlights PLUTO's potential to foster responsible decision-making and shared accountability in data practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12773v3</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Koesten, P\'eter Ferenc Gyarmati, Connor Hogan, Bernhard Jordan, Seliem El-Sayed, Barbara Prainsack, Torsten M\"oller</dc:creator>
    </item>
    <item>
      <title>Conversational Collective Intelligence (CCI) using Hyperchat AI in a Real-world Forecasting Task</title>
      <link>https://arxiv.org/abs/2511.03732</link>
      <description>arXiv:2511.03732v2 Announce Type: replace 
Abstract: Hyperchat AI is a novel agentic technology that enables thoughtful conversations among networked human groups of potentially unlimited size. It allows large teams to discuss complex issues, brainstorm ideas, surface risks, assess alternatives and efficiently converge on optimized solutions that amplify the group's Collective Intelligence (CI). A formal study was conducted to quantify the forecasting accuracy of human groups using Hyperchat AI to conversationally predict the outcome of Major League Baseball (MLB) games. During an 8-week period, networked groups of approximately 24 sports fans were tasked with collaboratively forecasting the winners of 59 baseball games through real-time conversation facilitated by AI agents. The results showed that when debating the games using Hyperchat AI technology, the groups converged on High Confidence predictions that significantly outperformed Vegas betting markets. Specifically, groups were 78% accurate in their High Confidence picks, a statistically strong result vs the Vegas odds of 57% (p=0.020). Had the groups bet against the spread (ATS) on these games, they would have achieved a 46% ROI against Vegas betting markets. In addition, High Confidence forecasts that were generated through above-average conversation rates were 88% accurate, suggesting that real-time interactive deliberation is central to amplified accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03732v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hans Schumann, Louis Rosenberg, Ganesh Mani, Gregg Willcox</dc:creator>
    </item>
    <item>
      <title>Multi-turn Evaluation of Anthropomorphic Behaviours in Large Language Models</title>
      <link>https://arxiv.org/abs/2502.07077</link>
      <description>arXiv:2502.07077v2 Announce Type: replace-cross 
Abstract: The tendency of users to anthropomorphise large language models (LLMs) is of growing interest to AI developers, researchers, and policy-makers. Here, we present a novel method for empirically evaluating anthropomorphic LLM behaviours in realistic and varied settings. Going beyond single-turn static benchmarks, we contribute three methodological advances in state-of-the-art (SOTA) LLM evaluation. First, we develop a multi-turn evaluation of 14 anthropomorphic behaviours. Second, we present a scalable, automated approach by employing simulations of user interactions. Third, we conduct an interactive, large-scale human subject study (N=1101) to validate that the model behaviours we measure predict real users' anthropomorphic perceptions. We find that all SOTA LLMs evaluated exhibit similar behaviours, characterised by relationship-building (e.g., empathy and validation) and first-person pronoun use, and that the majority of behaviours only first occur after multiple turns. Our work lays an empirical foundation for investigating how design choices influence anthropomorphic model behaviours and for progressing the ethical debate on the desirability of these behaviours. It also showcases the necessity of multi-turn evaluations for complex social phenomena in human-AI interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07077v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lujain Ibrahim, Canfer Akbulut, Rasmi Elasmar, Charvi Rastogi, Minsuk Kahng, Meredith Ringel Morris, Kevin R. McKee, Verena Rieser, Murray Shanahan, Laura Weidinger</dc:creator>
    </item>
    <item>
      <title>From Generation to Attribution: Music AI Agent Architectures for the Post-Streaming Era</title>
      <link>https://arxiv.org/abs/2510.20276</link>
      <description>arXiv:2510.20276v2 Announce Type: replace-cross 
Abstract: Generative AI is reshaping music creation, but its rapid growth exposes structural gaps in attribution, rights management, and economic models. Unlike past media shifts, from live performance to recordings, downloads, and streaming, AI transforms the entire lifecycle of music, collapsing boundaries between creation, distribution, and monetization. However, existing streaming systems, with opaque and concentrated royalty flows, are ill-equipped to handle the scale and complexity of AI-driven production. We propose a content-based Music AI Agent architecture that embeds attribution directly into the creative workflow through block-level retrieval and agentic orchestration. Designed for iterative, session-based interaction, the system organizes music into granular components (Blocks) stored in BlockDB; each use triggers an Attribution Layer event for transparent provenance and real-time settlement. This framework reframes AI from a generative tool into infrastructure for a Fair AI Media Platform. By enabling fine-grained attribution, equitable compensation, and participatory engagement, it points toward a post-streaming paradigm where music functions not as a static catalog but as a collaborative and adaptive ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20276v2</guid>
      <category>cs.IR</category>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <category>cs.SD</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Wonil Kim, Hyeongseok Wi, Seungsoon Park, Taejun Kim, Sangeun Keum, Keunhyoung Kim, Taewan Kim, Jongmin Jung, Taehyoung Kim, Gaetan Guerrero, Mael Le Goff, Julie Po, Dongjoo Moon, Juhan Nam, Jongpil Lee</dc:creator>
    </item>
  </channel>
</rss>
