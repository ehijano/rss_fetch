<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 16 Oct 2024 04:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Comparing Step Counting Algorithms for High-Resolution Wrist Accelerometry Data in NHANES 2011-2014</title>
      <link>https://arxiv.org/abs/2410.11040</link>
      <description>arXiv:2410.11040v1 Announce Type: new 
Abstract: Purpose: To quantify the relative performance of step counting algorithms in studies that collect free-living high-resolution wrist accelerometry data and to highlight the implications of using these algorithms in translational research. Methods: Five step counting algorithms (four open source and one proprietary) were applied to the publicly available, free-living, high-resolution wrist accelerometry data collected by the National Health and Nutrition Examination Survey (NHANES) in 2011-2014. The mean daily total step counts were compared in terms of correlation, predictive performance, and estimated hazard ratios of mortality. Results: The estimated number of steps were highly correlated (median=0.91, range 0.77 to 0.98), had high and comparable predictive performance of mortality (median concordance=0.72, range 0.70 to 0.73). The distributions of the number of steps in the population varied widely (mean step counts range from 2,453 to 12,169). Hazard ratios of mortality associated with a 500-step increase per day varied among step counting algorithms between HR=0.88 and 0.96, corresponding to a 300% difference in mortality risk reduction ([1-0.88]/[1-0.96]=3). Conclusion: Different step counting algorithms provide correlated step estimates and have similar predictive performance that is better than traditional predictors of mortality. However, they provide widely different distributions of step counts and estimated reductions in mortality risk for a 500-step increase.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11040v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lily Koffman, Ciprian Crainiceanu, John Muschelli III</dc:creator>
    </item>
    <item>
      <title>Statistical Analysis of the Impact of FIA Regulations on Safety, Racing Dynamics, and Spectacle in Formula 1</title>
      <link>https://arxiv.org/abs/2410.11375</link>
      <description>arXiv:2410.11375v1 Announce Type: new 
Abstract: This study examines the effects of the F\'ed\'eration Internationale de l'Automobile (FIA) regulations on Formula 1 (F1) from 1990 to 2023, focusing on safety, racing dynamics, and spectacle. By analyzing data on fatalities, overtaking statistics, car weights, and significant regulatory changes, including aerodynamic modifications and the introduction of the Drag Reduction System (DRS), the study assesses whether these regulations have positively influenced safety while negatively impacting racing dynamics and spectacle. The findings reveal that while FIA regulations aim to enhance driver safety, their immediate association with fatalities is not statistically significant due to the rarity of such incidents, delayed safety effects, and methodological constraints. Contrary to initial concerns, regulations have not negatively affected overtaking opportunities. The introduction of DRS has improved overtaking, mitigating some adverse impacts of other regulations on racing excitement. The study concludes that data-driven evaluation and refinement of regulations are necessary to ensure they continue to benefit the sport holistically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11375v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdelghani Belgaid</dc:creator>
    </item>
    <item>
      <title>Capturing Perception to Poverty using Conjoint Analysis &amp; Partial Profile Choice Experiment</title>
      <link>https://arxiv.org/abs/2410.11398</link>
      <description>arXiv:2410.11398v1 Announce Type: new 
Abstract: The objective of this study is applying a utility based analysis to a comparatively efficient design experiment which can capture people's perception towards the various components of a commodity. Here we studied the multi-dimensional poverty index and the relative importance of its components and their two-factor interaction effects. We also discussed how to model a choice based conjoint data for determining the utility of the components and their interactions. Empirical results from survey data shows the nature of coefficients, in terms of utility derived by the individuals, their statistical significance and validity in the present framework. There has been some discrepancies in the results between the bootstrap model and the original model, which can be understood by surveying more people, and ensuring comparative homogeneity in the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11398v1</guid>
      <category>stat.AP</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anushka De, Diganta Mukherjee</dc:creator>
    </item>
    <item>
      <title>An Explainable AI Model for Predicting the Recurrence of Differentiated Thyroid Cancer</title>
      <link>https://arxiv.org/abs/2410.10907</link>
      <description>arXiv:2410.10907v1 Announce Type: cross 
Abstract: Thyroid carcinoma, a significant yet often controllable cancer, has seen a rise in cases, largely due to advancements in diagnostic methods. Differentiated thyroid cancer (DTC), which includes papillary and follicular varieties, is typically associated with a positive prognosis in academic circles. Nevertheless, there are still some individuals who may experience a recurrence. This study employs machine learning, particularly deep learning models, to predict the recurrence of DTC, with the goal of improving patient care through personalized treatment approaches. By analysing a dataset containing clinicopathological features of patients, the model achieved remarkable accuracy rates of 98% during training and 96% during testing. To improve the model's interpretability, we used techniques like LIME and Morris Sensitivity Analysis. These methods gave us valuable insights into how the model makes decisions. The results suggest that combining deep learning models with interpretability techniques can be extremely useful in quickly identifying the recurrence of thyroid cancer in patients. This can help in making informed therapeutic choices and customizing treatment approaches for individual patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10907v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Al-Sayed Ahmad, Jude Haddad</dc:creator>
    </item>
    <item>
      <title>Aproximaci\'on pr\'actica a los m\'etodos de selecci\'on de portafolios de inversi\'on</title>
      <link>https://arxiv.org/abs/2410.11070</link>
      <description>arXiv:2410.11070v1 Announce Type: cross 
Abstract: This paper explores the practical approach to portfolio selection methods for investments. The study delves into portfolio theory, discussing concepts such as expected return, variance, asset correlation, and opportunity sets. It also presents the efficient frontier and its application in the Markowitz model, which employs mean-variance optimization techniques. An alternative approach based on the mean-semivariance model is introduced. This model accounts for the skewness and kurtosis of the asset return distribution, providing a more comprehensive view of risk and return. The study also addresses the practical implementation of these models, including the use of genetic algorithms to optimize portfolio selection. Additionally, transaction costs and integer constraints in portfolio optimization are considered, demonstrating the applicability of the Markowitz model.
  --
  Este documento explorar la aproximaci\'on pr\'actica a los m\'etodos de selecci\'on de portafolios para inversiones. El estudio profundiza en la teor\'ia de los portafolios, discutiendo conceptos como el rendimiento esperado, la varianza, la correlaci\'on entre activos y los conjuntos de oportunidades. Tambi\'en presenta la frontera eficiente y su aplicaci\'on en el modelo de Markowitz, que utiliza t\'ecnicas de optimizaci\'on media-varianza. Se introduce un enfoque alternativo basado en el modelo media-semivarianza. Este modelo tiene en cuenta la asimetr\'ia y la curtosis de la distribuci\'on de retornos de los activos, proporcionando una visi\'on m\'as completa de riesgo y rendimiento. El estudio tambi\'en aborda la implementaci\'on pr\'actica de estos modelos, incluyendo el uso de algoritmos gen\'eticos para optimizar la selecci\'on de portafolios. Adem\'as, se consideran los costos de transacci\'on y las restricciones enteras en la optimizaci\'on del portafolio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11070v1</guid>
      <category>q-fin.PM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Minutti-Martinez</dc:creator>
    </item>
    <item>
      <title>A Two-Stage Federated Learning Approach for Industrial Prognostics Using Large-Scale High-Dimensional Signals</title>
      <link>https://arxiv.org/abs/2410.11101</link>
      <description>arXiv:2410.11101v1 Announce Type: cross 
Abstract: Industrial prognostics aims to develop data-driven methods that leverage high-dimensional degradation signals from assets to predict their failure times. The success of these models largely depends on the availability of substantial historical data for training. However, in practice, individual organizations often lack sufficient data to independently train reliable prognostic models, and privacy concerns prevent data sharing between organizations for collaborative model training. To overcome these challenges, this article proposes a statistical learning-based federated model that enables multiple organizations to jointly train a prognostic model while keeping their data local and secure. The proposed approach involves two key stages: federated dimension reduction and federated (log)-location-scale regression. In the first stage, we develop a federated randomized singular value decomposition algorithm for multivariate functional principal component analysis, which efficiently reduces the dimensionality of degradation signals while maintaining data privacy. The second stage proposes a federated parameter estimation algorithm for (log)-location-scale regression, allowing organizations to collaboratively estimate failure time distributions without sharing raw data. The proposed approach addresses the limitations of existing federated prognostic methods by using statistical learning techniques that perform well with smaller datasets and provide comprehensive failure time distributions. The effectiveness and practicality of the proposed model are validated using simulated data and a dataset from the NASA repository.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11101v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuqi Su, Xiaolei Fang</dc:creator>
    </item>
    <item>
      <title>Real-Time Localization and Bimodal Point Pattern Analysis of Palms Using UAV Imagery</title>
      <link>https://arxiv.org/abs/2410.11124</link>
      <description>arXiv:2410.11124v1 Announce Type: cross 
Abstract: Understanding the spatial distribution of palms within tropical forests is essential for effective ecological monitoring, conservation strategies, and the sustainable integration of natural forest products into local and global supply chains. However, the analysis of remotely sensed data in these environments faces significant challenges, such as overlapping palm and tree crowns, uneven shading across the canopy surface, and the heterogeneous nature of the forest landscapes, which often affect the performance of palm detection and segmentation algorithms. To overcome these issues, we introduce PalmDSNet, a deep learning framework for real-time detection, segmentation, and counting of canopy palms. Additionally, we employ a bimodal reproduction algorithm that simulates palm spatial propagation to further enhance the understanding of these point patterns using PalmDSNet's results. We used UAV-captured imagery to create orthomosaics from 21 sites across western Ecuadorian tropical forests, covering a gradient from the everwet Choc\'o forests near Colombia to the drier forests of southwestern Ecuador. Expert annotations were used to create a comprehensive dataset, including 7,356 bounding boxes on image patches and 7,603 palm centers across five orthomosaics, encompassing a total area of 449 hectares. By combining PalmDSNet with the bimodal reproduction algorithm, which optimizes parameters for both local and global spatial variability, we effectively simulate the spatial distribution of palms in diverse and dense tropical environments, validating its utility for advanced applications in tropical forest monitoring and remote sensing analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11124v1</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kangning Cui, Wei Tang, Rongkun Zhu, Manqi Wang, Gregory D. Larsen, Victor P. Pauca, Sarra Alqahtani, Fan Yang, David Segurado, Paul Fine, Jordan Karubian, Raymond H. Chan, Robert J. Plemmons, Jean-Michel Morel, Miles R. Silman</dc:creator>
    </item>
    <item>
      <title>Discovering the critical number of respondents to validate an item in a questionnaire: The Binomial Cut-level Content Validity proposal</title>
      <link>https://arxiv.org/abs/2410.11151</link>
      <description>arXiv:2410.11151v1 Announce Type: cross 
Abstract: The question that drives this research is: "How to discover the number of respondents that are necessary to validate items of a questionnaire as actually essential to reach the questionnaire's proposal?" Among the efforts in this subject, \cite{Lawshe1975, Wilson2012, Ayre_CVR_2014} approached this issue by proposing and refining the Content Validation Ratio (CVR) that looks to identify items that are actually essentials. Despite their contribution, these studies do not check if an item validated as "essential" should be also validated as "not essential" by the same sample, which should be a paradox. Another issue is the assignment a probability equal a 50\% to a item be randomly checked by a respondent as essential, despite an evaluator has three options to choose. Our proposal faces these issues, making it possible to verify if a paradoxical situation occurs, and being more precise in recommending whether an item should either be retained or discarded from a questionnaire.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11151v1</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Helder Gomes Costa, Eduardo Shimoda, Jos\'e Fabiano da Serra Costa, Aldo Shimoya, Edilvando Pereira Eufrazio</dc:creator>
    </item>
    <item>
      <title>Addressing the Null Paradox in Epidemic Models: Correcting for Collider Bias in Causal Inference</title>
      <link>https://arxiv.org/abs/2410.11743</link>
      <description>arXiv:2410.11743v1 Announce Type: cross 
Abstract: We address the null paradox in epidemic models, where standard methods estimate a non-zero treatment effect despite the true effect being zero. This occurs when epidemic models mis-specify how causal effects propagate over time, especially when covariates act as colliders between past interventions and latent variables, leading to spurious correlations. Standard approaches like maximum likelihood and Bayesian methods can misinterpret these biases, inferring false causal relationships. While semi-parametric models and inverse propensity weighting offer potential solutions, they often limit the ability of domain experts to incorporate epidemic-specific knowledge. To resolve this, we propose an alternative estimating equation that corrects for collider bias while allowing for statistical inference with frequentist guarantees, previously unavailable for complex models like SEIR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11743v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heejong Bong, Val\'erie Ventura, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Do financial regulators act in the public's interest? A Bayesian latent class estimation framework for assessing regulatory responses to banking crises</title>
      <link>https://arxiv.org/abs/2208.03908</link>
      <description>arXiv:2208.03908v2 Announce Type: replace 
Abstract: When banks fail amidst financial crises, the public criticizes regulators for bailing out or liquidating specific banks, especially the ones that gain attention due to their size or dominance. A comprehensive assessment of regulators, however, requires examining all their decisions, and not just specific ones, against the regulator's dual objective of preserving financial stability while discouraging moral hazard. In this article, we develop a Bayesian latent class estimation framework to assess regulators on these competing objectives and evaluate their decisions against resolution rules recommended by theoretical studies of bank behavior designed to contain moral hazard incentives. The proposed estimation framework addresses the unobserved heterogeneity underlying regulator's decisions in resolving failed banks and provides a disciplined statistical approach for inferring if they acted in the public interest. Our results reveal that during the crises of 1980's, the U.S. banking regulator's resolution decisions were consistent with recommended decision rules, while the U.S. savings and loans (S&amp;L) regulator, which ultimately faced insolvency in 1989 at a cost of $132 billion to the taxpayer, had deviated from such recommendations. Timely interventions based on this evaluation could have redressed the S&amp;L regulator's decision structure and prevented losses to taxpayers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.03908v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Padma Sharma, Trambak Banerjee</dc:creator>
    </item>
    <item>
      <title>COBRA: Comparison-Optimal Betting for Risk-limiting Audits</title>
      <link>https://arxiv.org/abs/2304.01010</link>
      <description>arXiv:2304.01010v2 Announce Type: replace 
Abstract: Risk-limiting audits (RLAs) can provide routine, affirmative evidence that reported election outcomes are correct by checking a random sample of cast ballots. An efficient RLA requires checking relatively few ballots. Here we construct highly efficient RLAs by optimizing supermartingale tuning parameters--$\textit{bets}$--for ballot-level comparison audits. The exactly optimal bets depend on the true rate of errors in cast-vote records (CVRs)--digital receipts detailing how machines tabulated each ballot. We evaluate theoretical and simulated workloads for audits of contests with a range of diluted margins and CVR error rates. Compared to bets recommended in past work, using these optimal bets can dramatically reduce expected workloads--by 93% on average over our simulated audits. Because the exactly optimal bets are unknown in practice, we offer some strategies for approximating them. As with the ballot-polling RLAs described in ALPHA and RiLACs, adapting bets to previously sampled data or diversifying them over a range of suspected error rates can lead to substantially more efficient audits than fixing bets to $\textit{a priori}$ values, especially when those values are far from correct. We sketch extensions to other designs and social choice functions, and conclude with some recommendations for real-world comparison audits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.01010v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jacob Spertus</dc:creator>
    </item>
    <item>
      <title>Real-time risk estimation for active road safety: Leveraging Waymo AV sensor data with hierarchical Bayesian extreme value models</title>
      <link>https://arxiv.org/abs/2407.16832</link>
      <description>arXiv:2407.16832v2 Announce Type: replace 
Abstract: This study develops a real-time framework for estimating the risk of near-misses by using high-fidelity two-dimensional (2D) risk indicator time-to-collision (TTC), which is calculated from high-resolution data collected by autonomous vehicles (AVs). The framework utilizes extreme value theory (EVT) to derive near-miss risk based on observed TTC data. Most existing studies employ a generalized extreme value (GEV) distribution for specific sites and conflict types and often overlook individual vehicle dynamics heterogeneity. This framework is versatile across various highway geometries and can encompass vehicle dynamics and fidelity by incorporating covariates such as speed, acceleration, steering angle, and heading. This makes the risk estimation framework suitable for dynamic, real-world traffic environments. The dataset for this study is derived from Waymo perception data, encompassing six sites across three cities: San Francisco, Phoenix, and Los Angeles. Vehicle trajectory data were extracted from the dataset, and near-miss frequencies were calculated using high-fidelity 2D TTC. The crash risk was derived from observed near misses using four hierarchical Bayesian GEV models, explicitly focusing on conflicting pairs as block minima (BM), which revealed that crash risk varies across pairs.The proposed framework is efficient using a hierarchical Bayesian structure random parameter (HBSRP) model, offering superior statistical performance and flexibility by accounting for unobserved heterogeneity across sites. The study identifies and quantifies that the most hazardous conditions involve conflicting vehicle speeds and rapid acceleration and deceleration, significantly increasing crash risk in urban arterials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16832v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Anis, Sixu Li, Srinivas R. Geedipally, Yang Zhou, Dominique Lord</dc:creator>
    </item>
    <item>
      <title>Who's the GOAT? Sports Rankings and Data-Driven Random Walks on the Symmetric Group</title>
      <link>https://arxiv.org/abs/2409.12107</link>
      <description>arXiv:2409.12107v2 Announce Type: replace 
Abstract: Given a collection of historical sports rankings, can one tell which player is the greatest of all time (i.e., the GOAT)? In this work, we design a data-driven random walk on the symmetric group to obtain a stationary distribution over player rankings, spanning across different time periods in sports history. We combine this distribution with a notion of stochastic dominance to obtain a partial order over the players. Compared to existing methods, our approach is distinct in that i) using historical rankings captures the evolution of value systems and facilitates player comparisons when head-to-head data is unavailable, and i) aggregating into a partial order formally comes to terms with the possibility that, while some player comparisons can be established conclusively, others are "too close to call." We implement our methods using publicly available data from the Association of Tennis Professionals (ATP) and the Women's Tennis Association (WTA). Our main findings indicate that Steffi Graf and Serena Williams are the ones that come ahead as the GOATs of the WTA. Likewise, the "Big Three," that is Novak Djokovic, Roger Federer, and Rafael Nadal, are the ones that come ahead as the GOATs of the ATP. As a secondary finding, we note major differences in terms of career and dominance longevity for top players across the associations. While initially motivated by this application in sports analytics, our methods can also be applied to other practical domains where deriving rankings from historical data can inform operational decisions, such as route planning logistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12107v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gian-Gabriel P. Garcia, J. Carlos Mart\'inez Mori</dc:creator>
    </item>
    <item>
      <title>Analysis and Optimization of Seismic Monitoring Networks with Bayesian Optimal Experiment Design</title>
      <link>https://arxiv.org/abs/2410.07215</link>
      <description>arXiv:2410.07215v2 Announce Type: replace 
Abstract: Monitoring networks increasingly aim to assimilate data from a large number of diverse sensors covering many sensing modalities. Bayesian optimal experimental design (OED) seeks to identify data, sensor configurations, or experiments which can optimally reduce uncertainty and hence increase the performance of a monitoring network. Information theory guides OED by formulating the choice of experiment or sensor placement as an optimization problem that maximizes the expected information gain (EIG) about quantities of interest given prior knowledge and models of expected observation data. Therefore, within the context of seismo-acoustic monitoring, we can use Bayesian OED to configure sensor networks by choosing sensor locations, types, and fidelity in order to improve our ability to identify and locate seismic sources. In this work, we develop the framework necessary to use Bayesian OED to optimize a sensor network's ability to locate seismic events from arrival time data of detected seismic phases at the regional-scale. Bayesian OED requires four elements:
  1) A likelihood function that describes the distribution of detection and travel time data from the sensor network,
  2) A Bayesian solver that uses a prior and likelihood to identify the posterior distribution of seismic events given the data,
  3) An algorithm to compute EIG about seismic events over a dataset of hypothetical prior events,
  4) An optimizer that finds a sensor network which maximizes EIG.
  Once we have developed this framework, we explore many relevant questions to monitoring such as: how to trade off sensor fidelity and earth model uncertainty; how sensor types, number, and locations influence uncertainty; and how prior models and constraints influence sensor placement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07215v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>physics.geo-ph</category>
      <category>stat.ML</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jake Callahan, Kevin Monogue, Ruben Villarreal, Tommie Catanach</dc:creator>
    </item>
    <item>
      <title>Ensemble Kalman Inversion for Geothermal Reservoir Modelling</title>
      <link>https://arxiv.org/abs/2410.09017</link>
      <description>arXiv:2410.09017v2 Announce Type: replace 
Abstract: Numerical models of geothermal reservoirs typically depend on hundreds or thousands of unknown parameters, which must be estimated using sparse, noisy data. However, these models capture complex physical processes, which frequently results in long run-times and simulation failures, making the process of estimating the unknown parameters a challenging task. Conventional techniques for parameter estimation and uncertainty quantification, such as Markov chain Monte Carlo (MCMC), can require tens of thousands of simulations to provide accurate results and are therefore challenging to apply in this context. In this paper, we study the ensemble Kalman inversion (EKI) algorithm as an alternative technique for approximate parameter estimation and uncertainty quantification for geothermal reservoir models. EKI possesses several characteristics that make it well-suited to a geothermal setting; it is derivative-free, parallelisable, robust to simulation failures, and requires far fewer simulations than conventional uncertainty quantification techniques such as MCMC. We illustrate the use of EKI in a reservoir modelling context using a combination of synthetic and real-world case studies. Through these case studies, we also demonstrate how EKI can be paired with flexible parametrisation techniques capable of accurately representing prior knowledge of the characteristics of a reservoir and adhering to geological constraints, and how the algorithm can be made robust to simulation failures. Our results demonstrate that EKI provides a reliable and efficient means of obtaining accurate parameter estimates for large-scale, two-phase geothermal reservoir models, with appropriate characterisation of uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09017v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex de Beer, Elvar K Bjarkason, Michael Gravatt, Ruanui Nicholson, John P O'Sullivan, Michael J O'Sullivan, Oliver J Maclaren</dc:creator>
    </item>
    <item>
      <title>Frequentist Inference for Semi-mechanistic Epidemic Models with Interventions</title>
      <link>https://arxiv.org/abs/2309.10792</link>
      <description>arXiv:2309.10792v2 Announce Type: replace-cross 
Abstract: The effect of public health interventions on an epidemic are often estimated by adding the intervention to epidemic models. During the Covid-19 epidemic, numerous papers used such methods for making scenario predictions. The majority of these papers use Bayesian methods to estimate the parameters of the model. In this paper we show how to use frequentist methods for estimating these effects which avoids having to specify prior distributions. We also use model-free shrinkage methods to improve estimation when there are many different geographic regions. This allows us to borrow strength from different regions while still getting confidence intervals with correct coverage and without having to specify a hierarchical model. Throughout, we focus on a semi-mechanistic model which provides a simple, tractable alternative to compartmental methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.10792v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heejong Bong, Val\'erie Ventura, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Fast Revenue Maximization</title>
      <link>https://arxiv.org/abs/2407.07316</link>
      <description>arXiv:2407.07316v2 Announce Type: replace-cross 
Abstract: Problem definition: We study a data-driven pricing problem in which a seller offers a price for a single item based on demand observed at a small number of historical prices. Our goal is to derive precise evaluation procedures of the value of the historical information gathered by the seller, along with prescriptions for more efficient price experimentation. Methodology/results: Our main methodological result is an exact characterization of the maximin ratio (defined as the worst-case revenue garnered by a seller who only relies on past data divided by the optimal revenue achievable with full knowledge of the distribution of values). This result allows to measure the value of any historical data consisting of prices and corresponding conversion rates. We leverage this central reduction to provide new insights about price experimentation. Managerial implications: Motivated by practical constraints that impede the seller from changing prices abruptly, we first illustrate our framework by evaluating the value of local information and show that the mere sign of the gradient of the revenue curve at a single point can provide significant information to the seller. We then showcase how our framework can be used to run efficient price experiments. On the one hand, we develop a method to select the next price experiment that the seller should use to maximize the future robust performance. On the other hand, we demonstrate that our result allows to considerably reduce the number of price experiments needed to reach preset revenue guarantees through dynamic pricing algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07316v2</guid>
      <category>cs.GT</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Achraf Bahamou, Omar Besbes, Omar Mouchtaki</dc:creator>
    </item>
  </channel>
</rss>
