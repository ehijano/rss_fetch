<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 15 Oct 2024 03:31:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>RISC-V V Vector Extension (RVV) with reduced number of vector registers</title>
      <link>https://arxiv.org/abs/2410.08396</link>
      <description>arXiv:2410.08396v1 Announce Type: new 
Abstract: To reduce the area of RISC-V Vector extension (RVV) in small processors, the authors are considering one simple modification: reduce the number of registers in the vector register file. The standard 'V' extension requires 32 vector registers that we propose to reduce to 16 or 8 registers. Other features of RVV are still supported. Reducing the number of vector registers does not generate a completely new programming model: although the resulting core does not have binary code compatibility with standard RVV, compiling for it just requires parameterization of the vector register file size in the compiler. The reduced vector register file allows for still high utilization of vector RVV processor core. Many useful signal processing kernels require few registers, and become efficient at 1:4 chaining ratio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08396v1</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eino Jacobs, Dmitry Utyansky, Muhammad Hassan, Thomas Roecker</dc:creator>
    </item>
    <item>
      <title>MENAGE: Mixed-Signal Event-Driven Neuromorphic Accelerator for Edge Applications</title>
      <link>https://arxiv.org/abs/2410.08403</link>
      <description>arXiv:2410.08403v1 Announce Type: new 
Abstract: This paper presents a mixed-signal neuromorphic accelerator architecture designed for accelerating inference with event-based neural network models. This fully CMOS-compatible accelerator utilizes analog computing to emulate synapse and neuron operations. A C2C ladder structure implements synapses, while operational amplifiers (op-amps) are used to realize neuron functions. To enhance hardware resource utilization and power efficiency, we introduce the concept of a virtual neuron, where a single neuron engine emulates a set of model neurons, leveraging the sparsity inherent in event-based neuromorphic systems. Additionally, we propose a memory-based control technique to manage events in each layer, which improves performance while maintaining the flexibility to support various layer types. We also introduce an integer linear programming (ILP)-based mapping approach for efficiently allocating the model onto the proposed accelerator. The accelerator is a general-purpose neuromorphic platform capable of executing linear and convolutional neural models. The effectiveness of the proposed architecture is evaluated using two specially designed neuromorphic accelerators and two event-based datasets. The results show that the proposed architecture achieves 12.1 TOPS/W energy efficiency when accelerating a model trained on CIFAR10-DVS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08403v1</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Armin Abdollahi, Mehdi Kamal, Massoud Pedram</dc:creator>
    </item>
    <item>
      <title>Neural Architecture Search of Hybrid Models for NPU-CIM Heterogeneous AR/VR Devices</title>
      <link>https://arxiv.org/abs/2410.08326</link>
      <description>arXiv:2410.08326v1 Announce Type: cross 
Abstract: Low-Latency and Low-Power Edge AI is essential for Virtual Reality and Augmented Reality applications. Recent advances show that hybrid models, combining convolution layers (CNN) and transformers (ViT), often achieve superior accuracy/performance tradeoff on various computer vision and machine learning (ML) tasks. However, hybrid ML models can pose system challenges for latency and energy-efficiency due to their diverse nature in dataflow and memory access patterns. In this work, we leverage the architecture heterogeneity from Neural Processing Units (NPU) and Compute-In-Memory (CIM) and perform diverse execution schemas to efficiently execute these hybrid models. We also introduce H4H-NAS, a Neural Architecture Search framework to design efficient hybrid CNN/ViT models for heterogeneous edge systems with both NPU and CIM. Our H4H-NAS approach is powered by a performance estimator built with NPU performance results measured on real silicon, and CIM performance based on industry IPs. H4H-NAS searches hybrid CNN/ViT models with fine granularity and achieves significant (up to 1.34%) top-1 accuracy improvement on ImageNet dataset. Moreover, results from our Algo/HW co-design reveal up to 56.08% overall latency and 41.72% energy improvements by introducing such heterogeneous computing over baseline solutions. The framework guides the design of hybrid network architectures and system architectures of NPU+CIM heterogeneous systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08326v1</guid>
      <category>cs.CV</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yiwei Zhao, Ziyun Li, Win-San Khwa, Xiaoyu Sun, Sai Qian Zhang, Syed Shakib Sarwar, Kleber Hugo Stangherlin, Yi-Lun Lu, Jorge Tomas Gomez, Jae-Sun Seo, Phillip B. Gibbons, Barbara De Salvo, Chiao Liu</dc:creator>
    </item>
    <item>
      <title>Quantum Operating System Support for Quantum Trusted Execution Environments</title>
      <link>https://arxiv.org/abs/2410.08486</link>
      <description>arXiv:2410.08486v1 Announce Type: cross 
Abstract: With the growing reliance on cloud-based quantum computing, ensuring the confidentiality and integrity of quantum computations is paramount. Quantum Trusted Execution Environments (QTEEs) have been proposed to protect users' quantum circuits when they are submitted to remote cloud-based quantum computers. However, deployment of QTEEs necessitates a Quantum Operating Systems (QOS) that can support QTEEs hardware and operation. This work introduces the first architecture for a QOS to support and enable essential steps required for secure quantum task execution on cloud platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08486v1</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <category>cs.CR</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Theodoros Trochatos, Jakub Szefer</dc:creator>
    </item>
    <item>
      <title>Automated Deep Neural Network Inference Partitioning for Distributed Embedded Systems</title>
      <link>https://arxiv.org/abs/2406.19913</link>
      <description>arXiv:2406.19913v2 Announce Type: replace-cross 
Abstract: Distributed systems can be found in various applications, e.g., in robotics or autonomous driving, to achieve higher flexibility and robustness. Thereby, data flow centric applications such as Deep Neural Network (DNN) inference benefit from partitioning the workload over multiple compute nodes in terms of performance and energy-efficiency. However, mapping large models on distributed embedded systems is a complex task, due to low latency and high throughput requirements combined with strict energy and memory constraints. In this paper, we present a novel approach for hardware-aware layer scheduling of DNN inference in distributed embedded systems. Therefore, our proposed framework uses a graph-based algorithm to automatically find beneficial partitioning points in a given DNN. Each of these is evaluated based on several essential system metrics such as accuracy and memory utilization, while considering the respective system constraints. We demonstrate our approach in terms of the impact of inference partitioning on various performance metrics of six different DNNs. As an example, we can achieve a 47.5 % throughput increase for EfficientNet-B0 inference partitioned onto two platforms while observing high energy-efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19913v2</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabian Kre\ss, El Mahdi El Annabi, Tim Hotfilter, Julian Hoefer, Tanja Harbaum, Juergen Becker</dc:creator>
    </item>
    <item>
      <title>Parallax: A Compiler for Neutral Atom Quantum Computers under Hardware Constraints</title>
      <link>https://arxiv.org/abs/2409.04578</link>
      <description>arXiv:2409.04578v2 Announce Type: replace-cross 
Abstract: Among different quantum computing technologies, neutral atom quantum computers have several advantageous features, such as multi-qubit gates, application-specific topologies, movable qubits, homogenous qubits, and long-range interactions. However, existing compilation techniques for neutral atoms fall short of leveraging these advantages in a practical and scalable manner. This paper introduces Parallax, a zero-SWAP, scalable, and parallelizable compilation and atom movement scheduling method tailored for neutral atom systems, which reduces high-error operations by 25% and increases the success rate by 28% on average compared to the state-of-the-art technique.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04578v2</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason Ludmir, Tirthak Patel</dc:creator>
    </item>
  </channel>
</rss>
