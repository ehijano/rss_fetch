<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Jun 2025 04:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Massive parallelization of projection-based depths</title>
      <link>https://arxiv.org/abs/2506.08262</link>
      <description>arXiv:2506.08262v1 Announce Type: new 
Abstract: This article introduces a novel methodology for the massive parallelization of projection-based depths, addressing the computational challenges of data depth in high-dimensional spaces. We propose an algorithmic framework based on Refined Random Search (RRS) and demonstrate significant speedup (up to 7,000 times faster) on GPUs. Empirical results on synthetic data show improved precision and reduced runtime, making the method suitable for large-scale applications. The RRS algorithm (and other depth functions) are available in the Python-library data-depth (https://data-depth.github.io/) with ready-to-use tools to implement and to build upon this work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08262v1</guid>
      <category>stat.CO</category>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Leonardo Leone, Pavlo Mozharovskyi, David Bounie</dc:creator>
    </item>
    <item>
      <title>Fast and light-weight energy statistics using the \textit{R} packages \textsf{Rfast} and \textsf{Rfast2}</title>
      <link>https://arxiv.org/abs/2501.02849</link>
      <description>arXiv:2501.02849v5 Announce Type: replace 
Abstract: Energy statistics, also known as $\mathcal{\varepsilon}$-statistics, are functions of distances between statistical observations. This class of functions has enabled the development of non-linear statistical concepts, such as distance variance, distance covariance, and distance correlation. However, the computational burden associated with $\mathcal{\varepsilon}$-statistics is substantial, particularly when the data reside in multivariate space. To address this challenge, we have developed a method to significantly reduce memory requirements and accelerate computations, thereby facilitating the analysis of large data sets. The following cases are demonstrated: univariate and multivariate distance variance, distance covariance, partial distance correlation, energy distance, and hypothesis testing for the equality of univariate distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02849v5</guid>
      <category>stat.CO</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michail Tsagris, Manos Papadakis</dc:creator>
    </item>
    <item>
      <title>Delayed Acceptance Markov Chain Monte Carlo for Robust Bayesian Analysis</title>
      <link>https://arxiv.org/abs/2504.11761</link>
      <description>arXiv:2504.11761v2 Announce Type: replace 
Abstract: This study introduces a computationally efficient algorithm, delayed acceptance Markov chain Monte Carlo (DA-MCMC), designed to improve posterior simulation in quasi-Bayesian inference. Quasi-Bayesian methods, which do not require fully specifying a probabilistic model, are often computationally expensive owing to the need to evaluate the inverse and determinant of large covariance matrices. DA-MCMC addresses this challenge by employing a two-stage process: In the first stage, proposals are screened using an approximate posterior, whereas a final acceptance or rejection decision is made in the second stage based on the exact target posterior. This reduces the need for costly matrix computations, thereby improving efficiency without sacrificing accuracy. We demonstrate the effectiveness of DA-MCMC through applications to both synthetic and real data. The results demonstrate that, although DA-MCMC slightly reduces the effective sample size per iteration compared with the standard MCMC, it achieves substantial improvement in terms of effective sample size per second, approximately doubling the efficiency. This makes DA-MCMC particularly useful for cases where posterior simulation is computationally intensive. Thus, the DA-MCMC algorithm offers a significant advancement in computational efficiency for quasi-Bayesian inference, making it a valuable tool for robust Bayesian analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11761v2</guid>
      <category>stat.CO</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masahiro Tanaka</dc:creator>
    </item>
    <item>
      <title>EvoPort: An Evolutionary Framework for Portfolio Optimization via Randomized Alpha Discovery and Ensemble-Based Allocation</title>
      <link>https://arxiv.org/abs/2504.21095</link>
      <description>arXiv:2504.21095v2 Announce Type: replace 
Abstract: In this paper, we introduce EvoPort, a novel evolutionary portfolio optimization method that leverages stochastic exploration over a spectrum of investment pipeline depths. From raw equity data, we employ a randomized feature generation framework that hierarchically produces mathematical, logical, time-series, and cross-sectional operators for uncovering latent trading signals. Candidate alphas are then evaluated through a randomized hill-climbing optimization procedure, taking as guidance performance measures such as mean squared error (MSE) or Sharpe ratio. In order to increase robustness and generalizability further, we use a random ensemble model selection process whereby a heterogeneous set of machine learning models (e.g., linear regression, logistic regression, XG-Boost) are randomly drawn and combined to backtest the generated alphas. Finally, we use randomized portfolio weighting schemes based on the Markowitz modern portfolio theory with stochastic optimization techniques such as inverse volatility, risk parity, and variance-constrained approaches to optimally allocate assets. Our empirical results on real equity datasets demonstrate that EvoPort not only discovers rich sets of heterogeneous predictive signals but also constructs very robust and profitable portfolios. Compared to conventional alpha construction and allocation methods, our approach exhibits significant improvement in cumulative returns, Sharpe ratio, and drawdown control. We highlight the interpretability, scalability, and modularity of EvoPort, and speculate on its use as a general-purpose research pipeline for modern quantitative finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21095v2</guid>
      <category>stat.CO</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nguyen Van Thanh, Nguyen Thi Hau</dc:creator>
    </item>
    <item>
      <title>rd2d: Causal Inference in Boundary Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2505.07989</link>
      <description>arXiv:2505.07989v2 Announce Type: replace-cross 
Abstract: Boundary discontinuity designs -- also known as Multi-Score Regression Discontinuity (RD) designs, with Geographic RD designs as a prominent example -- are often used in empirical research to learn about causal treatment effects along a continuous assignment boundary defined by a bivariate score. This article introduces the R package rd2d, which implements and extends the methodological results developed in Cattaneo, Titiunik and Yu (2025) for boundary discontinuity designs. The package employs local polynomial estimation and inference using either the bivariate score or a univariate distance-to-boundary metric. It features novel data-driven bandwidth selection procedures, and offers both pointwise and uniform estimation and inference along the assignment boundary. The numerical performance of the package is demonstrated through a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07989v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Rocio Titiunik, Ruiqi Rae Yu</dc:creator>
    </item>
  </channel>
</rss>
