<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Mar 2025 04:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Tilted sensitivity analysis in matched observational studies</title>
      <link>https://arxiv.org/abs/2503.09736</link>
      <description>arXiv:2503.09736v1 Announce Type: new 
Abstract: We present a new procedure for conducting a sensitivity analysis in matched observational studies. For any candidate test statistic, the approach defines tilted modifications dependent upon the proposed strength of unmeasured confounding. The framework subsumes both (i) existing approaches to sensitivity analysis for sign-score statistics; and (ii) sensitivity analyses using conditional inverse probability weighting, wherein one weights the observed test statistic based upon the worst-case assignment probabilities for a proposed strength of hidden bias. Unlike the prevailing approach to sensitivity analysis after matching, there is a closed form expression for the limiting worst-case distribution when matching with multiple controls. Moreover, the approach admits a closed form for its design sensitivity, a measure used to compare competing test statistics and research designs, for matching with multiple controls, whereas the conventional approach generally only does so for pair matching. The tilted sensitivity analysis improves design sensitivity under a host of generative models. The proposal may also be adaptively combined with the conventional approach to attain a design sensitivity no smaller than the maximum of the individual design sensitivities. Data illustrations indicate that tilting can provide meaningful improvements in the reported robustness of matched observational studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09736v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Colin B. Fogarty</dc:creator>
    </item>
    <item>
      <title>Methods of Selective Inference for Linear Mixed Models: a Review and Empirical Comparison</title>
      <link>https://arxiv.org/abs/2503.09812</link>
      <description>arXiv:2503.09812v1 Announce Type: new 
Abstract: Selective inference aims at providing valid inference after a data-driven selection of models or hypotheses. It is essential to avoid overconfident results and replicability issues. While significant advances have been made in this area for standard regression models, relatively little attention has been given to linear mixed models (LMMs), which are widely used for analyzing clustered or longitudinal data. This paper reviews the existing selective inference approaches developed for LMMs, focusing on selection of fixed effects, where the random effects structure is given. We present these methods in detail and, through comparative simulations, assess their practical performance and computational feasibility under varying data structures. In addition, we apply them to a real-world biological dataset to examine how method choice can impact inference in practice. Our findings highlight an existing trade-off between computational complexity and statistical power and emphasize the scarcity of methods that perform well as the number of variables increases. In such scenarios, basic sample splitting emerges as the most reliable approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09812v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo D'Alessandro, Magne Thoresen</dc:creator>
    </item>
    <item>
      <title>First and Second Moments and Fractional Anisotropy of General von Mises-Fisher and Peanut Distributions</title>
      <link>https://arxiv.org/abs/2503.09851</link>
      <description>arXiv:2503.09851v1 Announce Type: new 
Abstract: Spherical distributions, in particular, the von Mises-Fisher distribution, are often used for problems using or modelling directional data. Since expectation and variance-covariance matrices follow from the first and second moments of the spherical distribution, the moments often need to be approximated numerically by computing trigonometric integrals. Here, we derive the explicit forms of the first and second moments for an n-dimensional von Mises-Fisher and peanut distributions by making use of the divergence theorem in the calculations. The derived formulas can be easily used in simulations, significantly decreasing the computation time. Moreover, we compute the fractional anisotropy formulas for the diffusion tensors derived from the bimodal von Mises-Fisher and peanut distributions, and show that the peanut distribution is limited in the amount of anisotropy it permits, making the von Mises-Fisher distribution a better choice when modelling anisotropy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09851v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandra Shyntar, Thomas Hillen</dc:creator>
    </item>
    <item>
      <title>Towards more reliable public transportation Wi-Fi Origin-Destination matrices: Modeling errors using synthetic noise and optical counts</title>
      <link>https://arxiv.org/abs/2503.10175</link>
      <description>arXiv:2503.10175v1 Announce Type: new 
Abstract: To continuously monitor mobility flows aboard public transportation, low-cost data collection methods based on the passive detection of Wi-Fi signals are promising technological solutions, but they yield uncertain results. We assess the accuracy of these results in light of a three-month experimentation conducted aboard buses equipped with Wi-Fi sensors in a sizable French conurbation. We put forward a method to quantify the error between the stop-to-stop origin-destination (O-D) matrix produced by Wi-Fi data and the ground truth, when the (estimated and real) volumes per boarding and alighting are known. To do so, the error in the estimated matrix is modeled by random noise. Neither additive, nor multiplicative noise replicate the experimental results. Noise models that concentrate on the short O-D trips and/or the central stops better reflect the structure of the error. But only by introducing distinct uncertainties between the boarding stop and the alighting stop can we recover the asymmetry between the alighting and boarding errors, as well as the correct ratios between these aggregate errors and the O-D error. Thus, our findings give insight into the main sources of error in the Wi-Fi based reconstruction of O-D matrices. They also provide analysts with an automatic and reproducible way to control the quality of O-D matrices produced by Wi-Fi data, using (readily available) count data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10175v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'ea Fabre (LAET, LSAF), Caroline Bayart (CHROME, LAET, LSAF), Alexandre Nicolas (ILM, CNRS), Patrick Bonnel (ENTPE, LAET)</dc:creator>
    </item>
    <item>
      <title>Combined P-value Functions for Compatible Effect Estimation and Hypothesis Testing in Drug Regulation</title>
      <link>https://arxiv.org/abs/2503.10246</link>
      <description>arXiv:2503.10246v1 Announce Type: new 
Abstract: The two-trials rule in drug regulation requires statistically significant results from two pivotal trials to demonstrate efficacy. However, it is unclear how the effect estimates from both trials should be combined to quantify the drug effect. Fixed-effect meta-analysis is commonly used but may yield confidence intervals that exclude the value of no effect even when the two-trials rule is not fulfilled. We systematically address this by recasting the two-trials rule and meta-analysis in a unified framework of combined p-value functions, where they are variants of Wilkinson's and Stouffer's combination methods, respectively. This allows us to obtain compatible combined p-values, effect estimates, and confidence intervals, which we derive in closed-form. Additionally, we provide new results for Edgington's, Fisher's, Pearson's, and Tippett's p-value combination methods. When both trials have the same true effect, all methods can consistently estimate it, although some show bias. When true effects differ, the two-trials rule and Pearson's method are conservative (converging to the less extreme effect), Fisher's and Tippett's methods are anti-conservative (converging to the more extreme effect), and Edgington's method and meta-analysis are balanced (converging to a weighted average). Notably, Edgington's confidence intervals asymptotically always include individual trial effects, while meta-analytic confidence intervals shrink to a point at the weighted average effect. We conclude that all of these methods may be appropriate depending on the estimand of interest. We implement combined p-value function inference for two trials in the R package twotrials, allowing researchers to easily perform compatible hypothesis testing and parameter estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10246v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Pawel, Ma{\l}gorzata Roos, Leonhard Held</dc:creator>
    </item>
    <item>
      <title>Comparative analysis and practical applications of cubic transmutations for the Pareto distribution</title>
      <link>https://arxiv.org/abs/2503.10266</link>
      <description>arXiv:2503.10266v1 Announce Type: new 
Abstract: Transmutation is a technique for extending classical probability distributions in order to give them more flexibility. In this paper, we are interested in cubic transmutations of the Pareto distribution. We establish a general formula that unifies existing cubic transmutations of the Pareto distribution and facilitates the derivation of new cubic transmutations that have not yet been explored in the literature. We also derive general formulas for the related mathematical properties. Finally, we perform a comparative analysis of the six transmutations existing in the literature using real-world data. The results obtained confirm the flexibility and effectiveness of cubic transmutations in modeling various types of data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10266v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edoh Katchekpele, Issa Cherif Geraldo, Tchilabalo Abozou Kpanzou</dc:creator>
    </item>
    <item>
      <title>Numerically robust Gaussian state estimation with singular observation noise</title>
      <link>https://arxiv.org/abs/2503.10279</link>
      <description>arXiv:2503.10279v1 Announce Type: new 
Abstract: This article proposes numerically robust algorithms for Gaussian state estimation with singular observation noise. Our approach combines a series of basis changes with Bayes' rule, transforming the singular estimation problem into a nonsingular one with reduced state dimension. In addition to ensuring low runtime and numerical stability, our proposal facilitates marginal-likelihood computations and Gauss-Markov representations of the posterior process. We analyse the proposed method's computational savings and numerical robustness and validate our findings in a series of simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10279v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicholas Kr\"amer, Filip Tronarp</dc:creator>
    </item>
    <item>
      <title>Generalized network autoregressive modelling of longitudinal networks with application to presidential elections in the USA</title>
      <link>https://arxiv.org/abs/2503.10433</link>
      <description>arXiv:2503.10433v1 Announce Type: new 
Abstract: Longitudinal networks are becoming increasingly relevant in the study of dynamic processes characterised by known or inferred community structure. Generalised Network Autoregressive (GNAR) models provide a parsimonious framework for exploiting the underlying network and multivariate time series. We introduce the community-$\alpha$ GNAR model with interactions that exploits prior knowledge or exogenous variables for analysing interactions within and between communities, and can describe serial correlation in longitudinal networks. We derive new explicit finite-sample error bounds that validate analysing high-dimensional longitudinal network data with GNAR models, and provide insights into their attractive properties. We further illustrate our approach by analysing the dynamics of $\textit{Red, Blue}$ and $\textit{Swing}$ states throughout presidential elections in the USA from 1976 to 2020, that is, a time series of length twelve on 51 time series (US states and Washington DC). Our analysis connects network autocorrelation to eight-year long terms, highlights a possible change in the system after the 2016 election, and a difference in behaviour between $\textit{Red}$ and $\textit{Blue}$ states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10433v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Guy Nason, Daniel Salnikov, Mario Cortina-Borja</dc:creator>
    </item>
    <item>
      <title>On the Proportional Principal Stratum Hazards Model</title>
      <link>https://arxiv.org/abs/2503.10481</link>
      <description>arXiv:2503.10481v1 Announce Type: new 
Abstract: In clinical trials involving both mortality and morbidity, an active treatment can influence the observed risk of the first non-fatal event either directly, through its effect on the non-fatal event process, or indirectly, through its effect on the death process, or both. Discerning the direct effect of treatment on the first non-fatal event holds clinical interest. However, with the competing risk of death, the Cox proportional hazards model that treats death as non-informative censoring and evaluates treatment effects on time to the first non-fatal event provides an estimate of the cause-specific hazard ratio, which may not correspond to the direct effect. To obtain the direct effect on the first non-fatal event, within the principal stratification framework, we define the principal stratum hazard and introduce the Proportional Principal Stratum Hazards model. This model estimates the principal stratum hazard ratio, which reflects the direct effect on the first non-fatal event in the presence of death and simplifies to the hazard ratio in the absence of death. The principal stratum membership is identified using the shared frailty model, which assumes independence between the first non-fatal event process and the potential death process from the counterfactual arm, conditional on per-subject random frailty. Simulation studies are conducted to verify the reliability of our estimators. We illustrate the method using the Carvedilol Prospective Randomized Cumulative Survival trial which involves heart-failure events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10481v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiren Sun, Thomas D. Cook</dc:creator>
    </item>
    <item>
      <title>Analysis and sample-size determination for $2^K$ audit experiments with binary response and application to identification of effect of racial discrimination on access to justice</title>
      <link>https://arxiv.org/abs/2503.10591</link>
      <description>arXiv:2503.10591v1 Announce Type: new 
Abstract: Social scientists have increasingly turned to audit experiments to investigate discrimination in the market for jobs, loans, housing and other opportunities. In a typical audit experiment, researchers assign ``signals'' (the treatment) to subjects at random and compare success rates across treatment conditions. In the recent past there has been increased interest in using randomized multifactor designs for audit experiments, popularly called factorial experiments, in which combinations of multiple signals are assigned to subjects. Although social scientists have manipulated multiple factors like race, gender and income, the analyses have been mostly exploratory in nature. In this paper we lay out a comprehensive methodology for design and analysis of $2^K$ factorial designs with binary response using model-free, randomization-based Neymanian inference and demonstrate its application by analyzing the audit experiment reported in Libgober (2020). Specifically, we integrate and extend several sections of the randomization-based, finite-population literature for binary outcomes, including sample size and power calculations, and non-linear factorial estimators, extending results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10591v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicole Pashley, Brian Libgober, Tirthankar Dasgupta</dc:creator>
    </item>
    <item>
      <title>PLRD: Partially Linear Regression Discontinuity Inference</title>
      <link>https://arxiv.org/abs/2503.09907</link>
      <description>arXiv:2503.09907v1 Announce Type: cross 
Abstract: Regression discontinuity designs have become one of the most popular research designs in empirical economics. We argue, however, that widely used approaches to building confidence intervals in regression discontinuity designs exhibit suboptimal behavior in practice: In a simulation study calibrated to high-profile applications of regression discontinuity designs, existing methods either have systematic under-coverage or have wider-than-necessary intervals. We propose a new approach, partially linear regression discontinuity inference (PLRD), and find it to address shortcomings of existing methods: Throughout our experiments, confidence intervals built using PLRD are both valid and short. We also provide large-sample guarantees for PLRD under smoothness assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09907v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Ghosh, Guido Imbens, Stefan Wager</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference of Geometric Brownian Motion: An Extension with Jumps</title>
      <link>https://arxiv.org/abs/2503.09923</link>
      <description>arXiv:2503.09923v1 Announce Type: cross 
Abstract: This analysis derives the maximum likelihood estimator and applies Bayesian inference to model geometric Brownian motion, incorporating jump diffusion to account for sudden market shifts. The Bayesian approach is implemented using Markov Chain Monte Carlo simulations on S\&amp;P 500 stock data from 2009 to 2014, providing a robust framework for analyzing stock dynamics and forecasting future trends. Exact solutions are obtained for both the standard Geometric Brownian Motion (GBM) model and the GBM model with Poisson jumps. Although both models yield reasonable results and fit the data well, the GBM with Poisson jumps exhibits superior performance, significantly enhancing model fit and capturing more complex market dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09923v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifei Yan, Juan Sosa, Carlos Mart\'inez</dc:creator>
    </item>
    <item>
      <title>Explainable Bayesian deep learning through input-skip Latent Binary Bayesian Neural Networks</title>
      <link>https://arxiv.org/abs/2503.10496</link>
      <description>arXiv:2503.10496v1 Announce Type: cross 
Abstract: Modeling natural phenomena with artificial neural networks (ANNs) often provides highly accurate predictions. However, ANNs often suffer from over-parameterization, complicating interpretation and raising uncertainty issues. Bayesian neural networks (BNNs) address the latter by representing weights as probability distributions, allowing for predictive uncertainty evaluation. Latent binary Bayesian neural networks (LBBNNs) further handle structural uncertainty and sparsify models by removing redundant weights. This article advances LBBNNs by enabling covariates to skip to any succeeding layer or be excluded, simplifying networks and clarifying input impacts on predictions. Ultimately, a linear model or even a constant can be found to be optimal for a specific problem at hand. Furthermore, the input-skip LBBNN approach reduces network density significantly compared to standard LBBNNs, achieving over 99% reduction for small networks and over 99.9% for larger ones, while still maintaining high predictive accuracy and uncertainty measurement. For example, on MNIST, we reached 97% accuracy and great calibration with just 935 weights, reaching state-of-the-art for compression of neural networks. Furthermore, the proposed method accurately identifies the true covariates and adjusts for system non-linearity. The main contribution is the introduction of active paths, enhancing directly designed global and local explanations within the LBBNN framework, that have theoretical guarantees and do not require post hoc external tools for explanations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10496v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eirik H{\o}yheim, Lars Skaaret-Lund, Solve S{\ae}b{\o}, Aliaksandr Hubin</dc:creator>
    </item>
    <item>
      <title>Sparse and Low-bias Estimation of High Dimensional Vector Autoregressive Models</title>
      <link>https://arxiv.org/abs/1908.11464</link>
      <description>arXiv:1908.11464v3 Announce Type: replace 
Abstract: Vector autoregressive (VAR) models are widely used for causal discovery and forecasting in multivariate time series analysis. In the high-dimensional setting, which is increasingly common in fields such as neuroscience and econometrics, model parameters are inferred by L1-regularized maximum likelihood (RML). A well-known feature of RML inference is that in general the technique produces a trade-off between sparsity and bias that depends on the choice of the regularization hyperparameter. In the context of multivariate time series analysis, sparse estimates are favorable for causal discovery and low-bias estimates are favorable for forecasting. However, owing to a paucity of research on hyperparameter selection methods, practitioners must rely on ad-hoc methods such as cross-validation (or manual tuning). The particular balance that such approaches achieve between the two goals -- causal discovery and forecasting -- is poorly understood. Our paper investigates this behavior and proposes a method (UoI-VAR) that achieves a better balance between sparsity and bias when the underlying causal influences are in fact sparse. We demonstrate through simulation that RML with a hyperparameter selected by cross-validation tends to overfit, producing relatively dense estimates. We further demonstrate that UoI-VAR much more effectively approximates the correct sparsity pattern with only a minor compromise in model fit, particularly so for larger data dimensions, and that the estimates produced by UoI-VAR exhibit less bias. We conclude that our method achieves improved performance especially well-suited to applications involving simultaneous causal discovery and forecasting in high-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:1908.11464v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 2nd Conference on Learning for Dynamics and Control, in Proceedings of Machine Learning Research 120 (2020) pp. 55-64</arxiv:journal_reference>
      <dc:creator>Trevor D. Ruiz, Sharmodeep Bhattacharyya, Mahesh Balasubramanian, Kristofer E. Bouchard</dc:creator>
    </item>
    <item>
      <title>Similarity-based Random Partition Distribution for Clustering Functional Data</title>
      <link>https://arxiv.org/abs/2308.01704</link>
      <description>arXiv:2308.01704v4 Announce Type: replace 
Abstract: Random partition distribution is a crucial tool for model-based clustering. This study advances the field of random partition in the context of functional spatial data, focusing on the challenges posed by hourly population data across various regions and dates. We propose an extension of the generalized Dirichlet process, named the similarity-based generalized Dirichlet process (SGDP)-type distribution, to address the limitations of simple random partition distributions (e.g., those induced by the Dirichlet process), such as an overabundance of clusters. This model prevents excess cluster production and incorporates pairwise similarity information to ensure accurate and meaningful clustering. The theoretical properties of the SGDP-type distribution are studied. Then, SGDP-type random partition is applied to a real-world dataset of hourly population flow in $500\text{m}^2$ meshes in the central part of Tokyo. In this empirical context, our method excels at detecting meaningful patterns in the data while accounting for spatial nuances. The results underscore the adaptability and utility of the method, showcasing its prowess in revealing intricate spatiotemporal dynamics. The proposed random partition will significantly contribute to urban planning, transportation, and policy-making and will be a helpful tool for understanding population dynamics and their implications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.01704v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoya Wakayama, Shonosuke Sugasawa, Genya Kobayashi</dc:creator>
    </item>
    <item>
      <title>Randomization-Based Inference for Average Treatment Effects in Inexactly Matched Observational Studies</title>
      <link>https://arxiv.org/abs/2308.02005</link>
      <description>arXiv:2308.02005v4 Announce Type: replace 
Abstract: Matching is a widely used causal inference design that aims to approximate a randomized experiment using observational data by forming matched sets of treated and control units based on similarities in their covariates. Ideally, treated units are exactly matched with controls on these covariates, enabling randomization-based inference for treatment effects as in a randomized experiment, under the assumption of no unobserved covariates. However, inexact matching often occurs, leading to residual covariate imbalance after matching. Previous matched studies have typically overlooked this issue and relied on conventional randomization-based inference, assuming that some covariate balance criteria are met. Recent research, however, has shown that this approach can introduce significant bias and proposed methods to correct for bias arising from inexact matching in randomization-based inference. These methods, however, are primarily focused on the constant treatment effect and its extensions (i.e., Fisher's sharp null) and do not apply to average treatment effects (i.e., Neyman's weak null). To address this gap, we introduce a new method -- inverse post-matching probability weighting -- for conducting randomization-based inference for average treatment effects under inexact matching. Our theoretical and simulation results indicate that, compared to conventional randomization-based inference methods, our approach significantly reduces bias and improves coverage rates in the presence of inexact matching. Additionally, we demonstrate how this method can be extended to the instrumental variable setting to simultaneously correct for bias due to inexact matching on observed covariates and bias due to unobserved covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.02005v4</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianan Zhu, Jeffrey Zhang, Zijian Guo, Siyu Heng</dc:creator>
    </item>
    <item>
      <title>A Causal Framework for Evaluating Drivers of Policy Effect Heterogeneity Using Difference-in-Differences</title>
      <link>https://arxiv.org/abs/2408.16670</link>
      <description>arXiv:2408.16670v3 Announce Type: replace 
Abstract: Policymakers and researchers often seek to understand how a policy differentially affects a population and the pathways driving this heterogeneity. For example, when studying an excise tax on sweetened beverages, researchers might assess the roles of cross-border shopping, economic competition, and store-level price changes on beverage sales trends. However, traditional policy evaluation tools, like the difference-in-differences (DiD) approach, primarily target average effects of the observed intervention rather than the underlying drivers of effect heterogeneity. Common approaches to evaluate sources of heterogeneity often lack a causal framework, making it difficult to determine whether observed outcome differences are truly driven by the proposed source of heterogeneity or by other confounding factors. In this paper, we present a framework for evaluating such policy drivers by representing questions of effect heterogeneity under hypothetical interventions and use it to evaluate drivers of the Philadelphia sweetened beverage tax policy effects. Building on recent advancements in estimating causal effect curves under DiD designs, we provide tools to assess policy effect heterogeneity while addressing practical challenges including confounding and neighborhood dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16670v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gary Hettinger, Youjin Lee, Nandita Mitra</dc:creator>
    </item>
    <item>
      <title>Randomization-based Inference for MCP-Mod</title>
      <link>https://arxiv.org/abs/2410.11716</link>
      <description>arXiv:2410.11716v2 Announce Type: replace 
Abstract: Dose selection is critical in pharmaceutical drug development, as it directly impacts therapeutic efficacy and patient safety of a drug. The Generalized Multiple Comparison Procedures and Modeling (MCP-Mod) approach is commonly used in Phase II trials for testing and estimation of dose-response relationships. However, its effectiveness in small sample sizes, particularly with binary endpoints, is hindered by issues like complete separation in logistic regression, leading to non-existence of estimates. Motivated by an actual clinical trial using the MCP-Mod approach, this paper introduces penalized maximum likelihood estimation (MLE) and randomization-based inference techniques to address these challenges. Randomization-based inference allows for exact finite sample inference, while population-based inference for MCP-Mod typically relies on asymptotic approximations. Simulation studies demonstrate that randomization-based tests can enhance statistical power in small to medium-sized samples while maintaining control over type-I error rates, even in the presence of time trends. Our results show that residual-based randomization tests using penalized MLEs not only improve computational efficiency but also outperform standard randomization-based methods, making them an adequate choice for dose-finding analyses within the MCP-Mod framework. Additionally, we apply these methods to pharmacometric settings, demonstrating their effectiveness in such scenarios. The results in this paper underscore the potential of randomization-based inference for the analysis of dose-finding trials, particularly in small sample contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11716v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Pin, Oleksandr Sverdlov, Frank Bretz, Bj\"orn Bornkamp</dc:creator>
    </item>
    <item>
      <title>Cross Validation for Correlated Data in Regression and Classification Models, with Applications to Deep Learning</title>
      <link>https://arxiv.org/abs/2502.14808</link>
      <description>arXiv:2502.14808v2 Announce Type: replace 
Abstract: We present a methodology for model evaluation and selection where the sampling mechanism violates the i.i.d. assumption. Our methodology involves a formulation of the bias between the standard Cross-Validation (CV) estimator and the mean generalization error, denoted by $w_{cv}$, and practical data-based procedures to estimate this term. This concept was introduced in the literature only in the context of a linear model with squared error loss as the criterion for prediction performance. Our proposed bias-corrected CV estimator, $\text{CV}_c=\text{CV}+w_{cv}$, can be applied to any learning model, including deep neural networks, and to a wide class of criteria for prediction performance in regression and classification tasks. We demonstrate the applicability of the proposed methodology in various scenarios where the data contains complex correlation structures (such as clustered and spatial relationships) with synthetic data and real-world datasets, providing evidence that the estimator $\text{CV}_c$ is better than the standard CV estimator. This paper is an expanded version of our published conference paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14808v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oren Yuval, Saharon Rosset</dc:creator>
    </item>
    <item>
      <title>A primer on optimal transport for causal inference with observational data</title>
      <link>https://arxiv.org/abs/2503.07811</link>
      <description>arXiv:2503.07811v2 Announce Type: replace 
Abstract: The theory of optimal transportation has developed into a powerful and elegant framework for comparing probability distributions, with wide-ranging applications in all areas of science. The fundamental idea of analyzing probabilities by comparing their underlying state space naturally aligns with the core idea of causal inference, where understanding and quantifying counterfactual states is paramount. Despite this intuitive connection, explicit research at the intersection of optimal transport and causal inference is only beginning to develop. Yet, many foundational models in causal inference have implicitly relied on optimal transport principles for decades, without recognizing the underlying connection. Therefore, the goal of this review is to offer an introduction to the surprisingly deep existing connections between optimal transport and the identification of causal effects with observational data -- where optimal transport is not just a set of potential tools, but actually builds the foundation of model assumptions. As a result, this review is intended to unify the language and notation between different areas of statistics, mathematics, and econometrics, by pointing out these existing connections, and to explore novel problems and directions for future work in both areas derived from this realization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07811v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>econ.EM</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian F Gunsilius</dc:creator>
    </item>
    <item>
      <title>Is Gibbs sampling faster than Hamiltonian Monte Carlo on GLMs?</title>
      <link>https://arxiv.org/abs/2410.03630</link>
      <description>arXiv:2410.03630v2 Announce Type: replace-cross 
Abstract: The Hamiltonian Monte Carlo (HMC) algorithm is often lauded for its ability to effectively sample from high-dimensional distributions. In this paper we challenge the presumed domination of HMC for the Bayesian analysis of GLMs. By utilizing the structure of the compute graph rather than the graphical model, we show a reduction of the time per sweep of a full-scan Gibbs sampler from $O(d^2)$ to $O(d)$, where $d$ is the number of GLM parameters. A simple change to the implementation of the Gibbs sampler allows us to perform Bayesian inference on high-dimensional GLMs that are practically infeasible with traditional Gibbs sampler implementations. We empirically demonstrate a substantial increase in effective sample size per time when comparing our Gibbs algorithms to state-of-the-art HMC algorithms. While Gibbs is superior in terms of dimension scaling, neither Gibbs nor HMC dominate the other: we provide numerical and theoretical evidence that HMC retains an edge in certain circumstances thanks to its advantageous condition number scaling. Interestingly, for GLMs of fixed data size, we observe that increasing dimensionality can stabilize or even decrease condition number, shedding light on the empirical advantage of our efficient Gibbs sampler.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03630v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Son Luu, Zuheng Xu, Nikola Surjanovic, Miguel Biron-Lattes, Trevor Campbell, Alexandre Bouchard-C\^ot\'e</dc:creator>
    </item>
    <item>
      <title>Transfer Learning Between U.S. Presidential Elections: How Should We Learn From A 2020 Ad Campaign To Inform 2024 Ad Campaigns?</title>
      <link>https://arxiv.org/abs/2411.01100</link>
      <description>arXiv:2411.01100v2 Announce Type: replace-cross 
Abstract: For the 2024 U.S. presidential election, would negative, digital ads against Donald Trump impact voter turnout in Pennsylvania (PA), a key "tipping point'' state? The gold standard to address this question, a randomized experiment where voters get randomized to different ads, yields unbiased estimates of the ad effect, but is very expensive. Instead, we propose a less-than-ideal, but significantly cheaper and faster framework based on transfer learning, where we transfer knowledge from a past ad experiment in 2020 to evaluate ads for 2024. A key component of our framework is a sensitivity analysis that quantifies the unobservable differences between 2020 and 2024 elections, where sensitivity parameters can be calibrated in a data-driven manner. We propose two estimators of the 2024 ad effect: a simple regression estimator with bootstrap, which we recommend for practitioners in this field, and an estimator based on the efficient influence function for broader applications. Using our framework, we estimate the effect of running a negative, digital ad campaign against Trump on voter turnout in PA for the 2024 election. Our findings indicate effect heterogeneity across counties of PA and among important subgroups stratified by gender, urbanicity, and education attainment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01100v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinran Miao, Jiwei Zhao, Hyunseung Kang</dc:creator>
    </item>
    <item>
      <title>Causal Representation Learning from Multimodal Biomedical Observations</title>
      <link>https://arxiv.org/abs/2411.06518</link>
      <description>arXiv:2411.06518v2 Announce Type: replace-cross 
Abstract: Prevalent in biomedical applications (e.g., human phenotype research), multimodal datasets can provide valuable insights into the underlying physiological mechanisms. However, current machine learning (ML) models designed to analyze these datasets often lack interpretability and identifiability guarantees, which are essential for biomedical research. Recent advances in causal representation learning have shown promise in identifying interpretable latent causal variables with formal theoretical guarantees. Unfortunately, most current work on multimodal distributions either relies on restrictive parametric assumptions or yields only coarse identification results, limiting their applicability to biomedical research that favors a detailed understanding of the mechanisms.
  In this work, we aim to develop flexible identification conditions for multimodal data and principled methods to facilitate the understanding of biomedical datasets. Theoretically, we consider a nonparametric latent distribution (c.f., parametric assumptions in previous work) that allows for causal relationships across potentially different modalities. We establish identifiability guarantees for each latent component, extending the subspace identification results from previous work. Our key theoretical contribution is the structural sparsity of causal connections between modalities, which, as we will discuss, is natural for a large collection of biomedical systems. Empirically, we present a practical framework to instantiate our theoretical insights. We demonstrate the effectiveness of our approach through extensive experiments on both numerical and synthetic datasets. Results on a real-world human phenotype dataset are consistent with established biomedical research, validating our theoretical and methodological framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06518v2</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuewen Sun, Lingjing Kong, Guangyi Chen, Loka Li, Gongxu Luo, Zijian Li, Yixuan Zhang, Yujia Zheng, Mengyue Yang, Petar Stojanov, Eran Segal, Eric P. Xing, Kun Zhang</dc:creator>
    </item>
    <item>
      <title>Connections between sequential Bayesian inference and evolutionary dynamics</title>
      <link>https://arxiv.org/abs/2411.16366</link>
      <description>arXiv:2411.16366v2 Announce Type: replace-cross 
Abstract: It has long been posited that there is a connection between the dynamical equations describing evolutionary processes in biology and sequential Bayesian learning methods. This manuscript describes new research in which this precise connection is rigorously established in the continuous time setting. Here we focus on a partial differential equation known as the Kushner-Stratonovich equation describing the evolution of the posterior density in time. Of particular importance is a piecewise smooth approximation of the observation path from which the discrete time filtering equations, which are shown to converge to a Stratonovich interpretation of the Kushner-Stratonovich equation. This smooth formulation will then be used to draw precise connections between nonlinear stochastic filtering and replicator-mutator dynamics. Additionally, gradient flow formulations will be investigated as well as a form of replicator-mutator dynamics which is shown to be beneficial for the misspecified model filtering problem. It is hoped this work will spur further research into exchanges between sequential learning and evolutionary biology and to inspire new algorithms in filtering and sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16366v2</guid>
      <category>math.PR</category>
      <category>q-bio.PE</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sahani Pathiraja, Philipp Wacker</dc:creator>
    </item>
    <item>
      <title>Cosmic Strings-induced CMB anisotropies in light of Weighted Morphology</title>
      <link>https://arxiv.org/abs/2503.00758</link>
      <description>arXiv:2503.00758v2 Announce Type: replace-cross 
Abstract: Motivated by the morphological measures in assessing the geometrical and topological properties of a generic cosmological stochastic field, we propose an extension of the weighted morphological measures, specifically the $n$th conditional moments of derivative (cmd-$n$). This criterion assigns a distinct weight to each excursion set point based on the associated field. We apply the cmd-$n$ on the Cosmic Microwave Background (CMB) to identify the cosmic string networks (CSs) through their unique Gott-Kaiser-Stebbins effect on the temperature anisotropies. We also formulate the perturbative expansion of cmd-$n$ for the weak non-Gaussian regime up to $\mathcal{O}(\sigma_0^3)$. We propose a comprehensive pipeline designed to analyze the morphological properties of string-induced CMB maps within the flat sky approximation. To evaluate the robustness of our proposed criteria, we employ string-induced high-resolution flat-sky CMB simulated patches of $7.2$ deg$^2$ size with a resolution of $0.42$ arc-minutes. Our results demonstrate that the minimum detectable value of cosmic string tension is $G\mu\gtrsim 1.9\times 10^{-7}$ when a noise-free map is analyzed with normalized cmd-$n$. Whereas for the ACT, CMB-S4, and Planck-like experiments at 95.45\% confidence level, the normalized cmd-$n$ can distinguish the CSs network for $G\mu\gtrsim2.9 \times 10^{-7}$, $G\mu\gtrsim 2.4\times 10^{-7}$ and $G\mu\gtrsim 5.8\times 10^{-7}$, respectively. The normalized cmd-$n$ exhibits a significantly enhanced capability in detecting CSs relative to the Minkowski Functionals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00758v2</guid>
      <category>astro-ph.CO</category>
      <category>astro-ph.IM</category>
      <category>physics.comp-ph</category>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adeela Afzal, M. Alakhras, M. H. Jalali Kanafi, S. M. S. Movahed</dc:creator>
    </item>
    <item>
      <title>Bayesian analysis of restricted mean survival time adjusted for covariates using pseudo-observations</title>
      <link>https://arxiv.org/abs/2503.05225</link>
      <description>arXiv:2503.05225v2 Announce Type: replace-cross 
Abstract: The difference in restricted mean survival time (RMST) is a clinically meaningful measure to quantify treatment effect in randomized controlled trials, especially when the proportional hazards assumption does not hold. Several frequentist methods exist to estimate RMST adjusted for covariates based on modeling and integrating the survival function. A more natural approach may be a regression model on RMST using pseudo-observations, which allows for a direct estimation without modeling the survival function. Only a few Bayesian methods exist, and each requires a model of the survival function. We developed a new Bayesian method that combines the use of pseudo-observations with the generalized method of moments. This offers RMST estimation adjusted for covariates without the need to model the survival function, making it more attractive than existing Bayesian methods. A simulation study was conducted with different time-dependent treatment effects (early, delayed, and crossing survival) and covariate effects, showing that our approach provides valid results, aligns with existing methods, and shows improved precision after covariate adjustment. For illustration, we applied our approach to a phase III trial in prostate cancer, providing estimates of the treatment effect on RMST, comparable to existing methods. In addition, our approach provided the effect of other covariates on RMST and determined the posterior probability of the difference in RMST exceeds any given time threshold for any covariate, allowing for nuanced and interpretable results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05225v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'ea Orsini (CESP, U1018), Emmanuel Lesaffre (KU Leuven), Guosheng Yin (DSAS), Caroline Brard (U1018), David Dejardin (U1018), Gw\'ena\"el Le Teuff (U1018)</dc:creator>
    </item>
    <item>
      <title>Representation Retrieval Learning for Heterogeneous Data Integration</title>
      <link>https://arxiv.org/abs/2503.09494</link>
      <description>arXiv:2503.09494v2 Announce Type: replace-cross 
Abstract: In the era of big data, large-scale, multi-modal datasets are increasingly ubiquitous, offering unprecedented opportunities for predictive modeling and scientific discovery. However, these datasets often exhibit complex heterogeneity, such as covariate shift, posterior drift, and missing modalities, that can hinder the accuracy of existing prediction algorithms. To address these challenges, we propose a novel Representation Retrieval ($R^2$) framework, which integrates a representation learning module (the representer) with a sparsity-induced machine learning model (the learner). Moreover, we introduce the notion of "integrativeness" for representers, characterized by the effective data sources used in learning representers, and propose a Selective Integration Penalty (SIP) to explicitly improve the property. Theoretically, we demonstrate that the $R^2$ framework relaxes the conventional full-sharing assumption in multi-task learning, allowing for partially shared structures, and that SIP can improve the convergence rate of the excess risk bound. Extensive simulation studies validate the empirical performance of our framework, and applications to two real-world datasets further confirm its superiority over existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09494v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qi Xu, Annie Qu</dc:creator>
    </item>
  </channel>
</rss>
