<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 01 Apr 2024 04:00:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 01 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Is the edge really necessary for drone computing offloading? An experimental assessment in carrier-grade 5G operator networks</title>
      <link>https://arxiv.org/abs/2403.19729</link>
      <description>arXiv:2403.19729v1 Announce Type: new 
Abstract: In this article, we evaluate the first experience of computation offloading from drones to real fifth-generation (5G) operator systems, including commercial and private carrier-grade 5G networks. A follow-me drone service was implemented as a representative testbed of remote video analytics. In this application, an image of a person from a drone camera is processed at the edge, and image tracking displacements are translated into positioning commands that are sent back to the drone, so that the drone keeps the camera focused on the person at all times. The application is characterised to identify the processing and communication contributions to service delay. Then, we evaluate the latency of the application in a real non standalone 5G operator network, a standalone carrier-grade 5G private network, and, to compare these results with previous research, a Wi-Fi wireless local area network. We considered both multi-access edge computing (MEC) and cloud offloading scenarios. Onboard computing was also evaluated to assess the trade-offs with task offloading. The results determine the network configurations that are feasible for the follow-me application use case depending on the mobility of the end user, and to what extent MEC is advantageous over a state-of-the-art cloud service.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19729v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1002/spe.3161</arxiv:DOI>
      <arxiv:journal_reference>Software: Practice and Experience, vol. 53, no. 3, pp. 579-599, March 2023</arxiv:journal_reference>
      <dc:creator>David Candal-Ventureira, Francisco Javier Gonz\'alez-Casta\~no, Felipe Gil-Casti\~neira, Pablo Fondo-Ferreiro</dc:creator>
    </item>
    <item>
      <title>Latency Reduction in Vehicular Sensing Applications by Dynamic 5G User Plane Function Allocation with Session Continuity</title>
      <link>https://arxiv.org/abs/2403.19730</link>
      <description>arXiv:2403.19730v1 Announce Type: new 
Abstract: Vehicle automation is driving the integration of advanced sensors and new applications that demand high-quality information, such as collaborative sensing for enhanced situational awareness. In this work, we considered a vehicular sensing scenario supported by 5G communications, in which vehicle sensor data need to be sent to edge computing resources with stringent latency constraints. To ensure low latency with the resources available, we propose an optimization framework that deploys User Plane Functions (UPFs) dynamically at the edge to minimize the number of network hops between the vehicles and them. The proposed framework relies on a practical Software-Defined-Networking (SDN)-based mechanism that allows seamless re-assignment of vehicles to UPFs while maintaining session and service continuity. We propose and evaluate different UPF allocation algorithms that reduce communications latency compared to static, random, and centralized deployment baselines. Our results demonstrated that the dynamic allocation of UPFs can support latency-critical applications that would be unfeasible otherwise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19730v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/s21227744</arxiv:DOI>
      <arxiv:journal_reference>Sensors, vol. 21, no 22, p. 7744, 2021</arxiv:journal_reference>
      <dc:creator>Pablo Fondo-Ferreiro, David Candal-Ventureira, Francisco Javier Gonz\'alez-Casta\~no, Felipe Gil-Casti\~neira</dc:creator>
    </item>
    <item>
      <title>Quarantining Malicious IoT Devices in Intelligent Sliced Mobile Networks</title>
      <link>https://arxiv.org/abs/2403.19731</link>
      <description>arXiv:2403.19731v1 Announce Type: new 
Abstract: The unstoppable adoption of the Internet of Things (IoT) is driven by the deployment of new services that require continuous capture of information from huge populations of sensors, or actuating over a myriad of "smart" objects. Accordingly, next generation networks are being designed to support such massive numbers of devices and connections. For example, the 3rd Generation Partnership Project (3GPP) is designing the different 5G releases specifically with IoT in mind. Nevertheless, from a security perspective this scenario is a potential nightmare: the attack surface becomes wider and many IoT nodes do not have enough resources to support advanced security protocols. In fact, security is rarely a priority in their design. Thus, including network-level mechanisms for preventing attacks from malware-infected IoT devices is mandatory to avert further damage. In this paper, we propose a novel Software-Defined Networking (SDN)-based architecture to identify suspicious nodes in 4G or 5G networks and redirect their traffic to a secondary network slice where traffic is analyzed in depth before allowing it reaching its destination. The architecture can be easily integrated in any existing deployment due to its interoperability. By following this approach, we can detect potential threats at an early stage and limit the damage by Distributed Denial of Service (DDoS) attacks originated in IoT devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19731v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/s20185054</arxiv:DOI>
      <arxiv:journal_reference>Sensors, vol. 20, no 18, p. 5054, 2020</arxiv:journal_reference>
      <dc:creator>David Candal-Ventureira, Pablo Fondo-Ferreiro, Felipe Gil-Casti\~neira, Francisco Javier Gonz\'alez-Casta\~no</dc:creator>
    </item>
    <item>
      <title>Performance Evaluation of IEEE 802.11bf Protocol in the sub-7 GHz Band</title>
      <link>https://arxiv.org/abs/2403.19825</link>
      <description>arXiv:2403.19825v1 Announce Type: new 
Abstract: Changes in Wi-Fi signal, using Wi-Fi sensing, have been used to detect movements in the environment and have led to development of many related applications. However, there has not been a standard way to do this until the IEEE 802.11bf standard development activity was taken up recently by the IEEE. Wi-Fi sensing is an overhead to data communication. While the IEEE 802.11bf standard has been designed with careful attention to the overhead and its impact on data communication, there has been no study done to quantify those. Therefore, in this paper, we evaluate performance of IEEE 802.11bf protocol with different system configurations corresponding to different sensing loads and the impact of sensing on data communication in those configurations. We outline some of the key findings from our simulation experiments which may be useful in practical operating configurations of an IEEE 802.11bf network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19825v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anirudha Sahoo, Tanguy Ropitault, Steve Blandino, Nada Golmie</dc:creator>
    </item>
    <item>
      <title>ChatTracer: Large Language Model Powered Real-time Bluetooth Device Tracking System</title>
      <link>https://arxiv.org/abs/2403.19833</link>
      <description>arXiv:2403.19833v1 Announce Type: new 
Abstract: Large language models (LLMs), exemplified by OpenAI ChatGPT and Google Bard, have transformed the way we interact with cyber technologies. In this paper, we study the possibility of connecting LLM with wireless sensor networks (WSN). A successful design will not only extend LLM's knowledge landscape to the physical world but also revolutionize human interaction with WSN. To the end, we present ChatTracer, an LLM-powered real-time Bluetooth device tracking system. ChatTracer comprises three key components: an array of Bluetooth sniffing nodes, a database, and a fine-tuned LLM. ChatTracer was designed based on our experimental observation that commercial Apple/Android devices always broadcast hundreds of BLE packets per minute even in their idle status. Its novelties lie in two aspects: i) a reliable and efficient BLE packet grouping algorithm; and ii) an LLM fine-tuning strategy that combines both supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF). We have built a prototype of ChatTracer with four sniffing nodes. Experimental results show that ChatTracer not only outperforms existing localization approaches, but also provides an intelligent interface for user interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19833v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qijun Wang, Shichen Zhang, Kunzhe Song, Huacheng Zeng</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Evaluation of the Impact of ATM QoS Mechanisms on Network Performance for Multimedia and Data Applications</title>
      <link>https://arxiv.org/abs/2403.19914</link>
      <description>arXiv:2403.19914v1 Announce Type: new 
Abstract: The Asynchronous Transfer Mode (ATM) network is crucial due to its ability to efficiently transmit data, provide reliable connections, and support various service classes with specific Quality of Service (QoS) requirements. In this paper, we utilize the OPNET network simulation software to model an ATM network and analyze the impact of QoS classification on network performance. We investigate the effects of Constant Bit Rate (CBR), Variable Bit Rate (VBR), Available Bit Rate (ABR) and Unspecified Bit Rate (UBR) models on various network traffic types such as voice, video and data. For voice traffic, we examine key QoS parameters including Jitter, Packet Delay Variation and End-to-End Delay. For video traffic, we evaluate Packet Delay Variation and End-to-End Delay. Additionally, we analyze Download Response Time for data traffic to assess the influence of QoS on the ATM network. Our results demonstrate that CBR and VBR are preferred for real-time traffic like voice and video, providing low delay and jitter. The simulation approach enables us to test various configurations and gain insights not possible in hardware tests. Our findings can help network operators determine the optimal QoS settings and tradeoffs when deploying ATM for modern multi-service networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19914v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahdi Manavi</dc:creator>
    </item>
    <item>
      <title>DHNet: A Distributed Network Architecture for Smart Home</title>
      <link>https://arxiv.org/abs/2403.19931</link>
      <description>arXiv:2403.19931v1 Announce Type: new 
Abstract: With the increasing popularity of smart homes, more and more devices need to connect to home networks. Traditional home networks mainly rely on centralized networking, where an excessive number of devices in the centralized topology can increase the pressure on the central router, potentially leading to decreased network performance metrics such as communication latency. To address the latency performance issues brought about by centralized networks, this paper proposes a new network system called DHNet, and designs an algorithm for clustering networking and communication based on vector routing. Communication within clusters in a simulated virtual environment achieves a latency of approximately 0.7 milliseconds. Furthermore, by directly using the first non-"lo" network card address of a device as the protocol's network layer address, the protocol avoids the several tens of milliseconds of access latency caused by DHCP. The integration of service discovery functionality into the network layer protocol is achieved through a combination of "server-initiated service push" and "client request + server reply" methods. Compared to traditional application-layer DNS passive service discovery, the average latency is reduced by over 50%. The PVH protocol is implemented in the user space using the Go programming language, with implementation details drawn from Google's gVisor project. The code has been ported from x86\_64 Linux computers to devices such as OpenWrt routers and Android smartphones. The PVH protocol can communicate through "tunnels" to provide IP compatibility, allowing existing applications based on TCP/IP to communicate using the PVH protocol without requiring modifications to their code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19931v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chaoqi Zhou, Jingpu Duan, YuPeng Xiao, Qing Li, Dingding Chen, Ruobin Zheng, Shaoteng Liu</dc:creator>
    </item>
    <item>
      <title>FaiRTT: An Empirical Approach for Enhanced RTT Fairness and Bottleneck Throughput in BBR</title>
      <link>https://arxiv.org/abs/2403.19973</link>
      <description>arXiv:2403.19973v1 Announce Type: new 
Abstract: In next-generation networks, achieving Round-trip Time (RTT) fairness is essential for ensuring fair bandwidth distribution among diverse flow types, enhancing overall network utilization. The TCP congestion control algorithm -- BBR, was proposed by Google to dynamically adjust sending rates in response to changing network conditions. While BBRv2 was implemented to overcome the unfairness limitation of BBRv1, it still faces intra-protocol fairness challenges in balancing the demands of high-bandwidth, long-RTT elephant flows and more frequent short-RTT mice flows. These issues lead to throughput imbalances and queue buildup, resulting in elephant flow dominance and mice flow starvation. In this paper, we first investigate the limitations of Google's BBR algorithm, specifically in the context of intra-protocol RTT fairness in beyond 5G (B5G) networks. While existing works address this limitation by adjusting the pacing rate, it eventually leads to low throughput. We hence develop the FaiRTT algorithm to resolve the problem by dynamically estimating the Bandwidth Delay Product (BDP) sending rate based on RTT measurements, focusing on equitable bandwidth allocation. By modeling the Inf light dependency on the BDP, bottleneck bandwidth, and packet departure time after every ACK, we can resolve the intra-protocol fairness while not compromising the throughput on the bottleneck link. Through extensive simulations on NS-3 and comprehensive performance evaluations, FaiRTT is shown to significantly improve the fairness index and network throughput, significantly outperforming BBRv2, for diverse flow types. FaiRTT achieves an average throughput ratio of 1.08 between elephant and mice flows, an average fairness index of 0.98, and an average utilization of the bottleneck link of 98.78%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19973v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akshita Abrol, Purnima Murali Mohan, Tram Truong-Huu</dc:creator>
    </item>
    <item>
      <title>Blockchain for Energy Market: A Comprehensive Survey</title>
      <link>https://arxiv.org/abs/2403.20045</link>
      <description>arXiv:2403.20045v1 Announce Type: new 
Abstract: The energy market encompasses the behavior of energy supply and trading within a platform system. By utilizing centralized or distributed trading, energy can be effectively managed and distributed across different regions, thereby achieving market equilibrium and satisfying both producers and consumers. However, recent years have presented unprecedented challenges and difficulties for the development of the energy market. These challenges include regional energy imbalances, volatile energy pricing, high computing costs, and issues related to transaction information disclosure. Researchers widely acknowledge that the security features of blockchain technology can enhance the efficiency of energy transactions and establish the fundamental stability and robustness of the energy market. This type of blockchain-enabled energy market is commonly referred to as an energy blockchain. Currently, there is a burgeoning amount of research in this field, encompassing algorithm design, framework construction, and practical application. It is crucial to organize and compare these research efforts to facilitate the further advancement of energy blockchain. This survey aims to comprehensively review the fundamental characteristics of blockchain and energy markets, highlighting the significant advantages of combining the two. Moreover, based on existing research outcomes, we will categorize and compare the current energy market research supported by blockchain in terms of algorithm design, market framework construction, and the policies and practical applications adopted by different countries. Finally, we will address current issues and propose potential future directions for improvement, to provide guidance for the practical implementation of blockchain in the energy market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20045v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Tianqi Jiang, Haoxiang Luo, Kun Yang, Gang Sun, Hongfang Yu, Qi Huang</dc:creator>
    </item>
    <item>
      <title>Distributed Swarm Learning for Edge Internet of Things</title>
      <link>https://arxiv.org/abs/2403.20188</link>
      <description>arXiv:2403.20188v1 Announce Type: new 
Abstract: The rapid growth of Internet of Things (IoT) has led to the widespread deployment of smart IoT devices at wireless edge for collaborative machine learning tasks, ushering in a new era of edge learning. With a huge number of hardware-constrained IoT devices operating in resource-limited wireless networks, edge learning encounters substantial challenges, including communication and computation bottlenecks, device and data heterogeneity, security risks, privacy leakages, non-convex optimization, and complex wireless environments. To address these issues, this article explores a novel framework known as distributed swarm learning (DSL), which combines artificial intelligence and biological swarm intelligence in a holistic manner. By harnessing advanced signal processing and communications, DSL provides efficient solutions and robust tools for large-scale IoT at the edge of wireless networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20188v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yue Wang, Zhi Tian, FXin Fan, Zhipeng Cai, Cameron Nowzari, Kai Zeng</dc:creator>
    </item>
    <item>
      <title>NeuraLunaDTNet: Feedforward Neural Network-Based Routing Protocol for Delay-Tolerant Lunar Communication Networks</title>
      <link>https://arxiv.org/abs/2403.20199</link>
      <description>arXiv:2403.20199v1 Announce Type: new 
Abstract: Space Communication poses challenges such as severe delays, hard-to-predict routes and communication disruptions. The Delay Tolerant Network architecture, having been specifically designed keeping such scenarios in mind, is suitable to address some challenges. The traditional DTN routing protocols fall short of delivering optimal performance, due to the inherent complexities of space communication. Researchers have aimed at using recent advancements in AI to mitigate some routing challenges [9]. We propose utilising a feedforward neural network to develop a novel protocol NeuraLunaDTNet, which enhances the efficiency of the PRoPHET routing protocol for lunar communication, by learning contact plans in dynamically changing spatio-temporal graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20199v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parth Patel, Milena Radenkovic</dc:creator>
    </item>
    <item>
      <title>Genos: General In-Network Unsupervised Intrusion Detection by Rule Extraction</title>
      <link>https://arxiv.org/abs/2403.19248</link>
      <description>arXiv:2403.19248v1 Announce Type: cross 
Abstract: Anomaly-based network intrusion detection systems (A-NIDS) use unsupervised models to detect unforeseen attacks. However, existing A-NIDS solutions suffer from low throughput, lack of interpretability, and high maintenance costs. Recent in-network intelligence (INI) exploits programmable switches to offer line-rate deployment of NIDS. Nevertheless, current in-network NIDS are either model-specific or only apply to supervised models. In this paper, we propose Genos, a general in-network framework for unsupervised A-NIDS by rule extraction, which consists of a Model Compiler, a Model Interpreter, and a Model Debugger. Specifically, observing benign data are multimodal and usually located in multiple subspaces in the feature space, we utilize a divide-and-conquer approach for model-agnostic rule extraction. In the Model Compiler, we first propose a tree-based clustering algorithm to partition the feature space into subspaces, then design a decision boundary estimation mechanism to approximate the source model in each subspace. The Model Interpreter interprets predictions by important attributes to aid network operators in understanding the predictions. The Model Debugger conducts incremental updating to rectify errors by only fine-tuning rules on affected subspaces, thus reducing maintenance costs. We implement a prototype using physical hardware, and experiments demonstrate its superior performance of 100 Gbps throughput, great interpretability, and trivial updating overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19248v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruoyu Li, Qing Li, Yu Zhang, Dan Zhao, Xi Xiao, Yong Jiang</dc:creator>
    </item>
    <item>
      <title>An Improved Modular Addition Checksum Algorithm</title>
      <link>https://arxiv.org/abs/2304.13496</link>
      <description>arXiv:2304.13496v4 Announce Type: replace-cross 
Abstract: This paper introduces a checksum algorithm that provides a new point in the performance/complexity/effectiveness checksum tradeoff space. It has better fault detection properties than single-sum and dual-sum modular addition checksums. It is also simpler to compute efficiently than a cyclic redundancy check (CRC) due to exploiting commonly available hardware and programming language support for unsigned integer division. The key idea is to compute a single running sum, but introduce a left shift by the size (in bits) of the modulus before performing the modular reduction after each addition step. This approach provides a Hamming Distance of 3 for longer data word lengths than dual-sum approaches such as the Fletcher checksum. Moreover, it provides this capability using a single running sum that is only twice the size of the final computed check value, while providing fault detection capabilities even better than large-block variants of dual-sum approaches that require larger division operations. A variant that includes a parity bit achieves Hamming Distance 4 for the same size check value, approximating the fault detection capabilities of a good CRC for about half the data word length attainable by such a CRC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.13496v4</guid>
      <category>cs.DS</category>
      <category>cs.NI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philip Koopman</dc:creator>
    </item>
    <item>
      <title>Lens: A Foundation Model for Network Traffic in Cybersecurity</title>
      <link>https://arxiv.org/abs/2402.03646</link>
      <description>arXiv:2402.03646v3 Announce Type: replace-cross 
Abstract: Network traffic refers to the amount of data being sent and received over the internet or any system that connects computers. Analyzing and understanding network traffic is vital for improving network security and management. However, the analysis of network traffic is challenging due to the diverse nature of data packets, which often feature heterogeneous headers and encrypted payloads lacking semantics. To capture the latent semantics of traffic, a few studies have adopted pre-training techniques based on the Transformer encoder or decoder to learn the representations from massive traffic data. However, these methods typically excel in traffic understanding (classification) or traffic generation tasks. To address this issue, we develop Lens, a foundation model for network traffic that leverages the T5 architecture to learn the pre-trained representations from large-scale unlabeled data. Harnessing the strength of the encoder-decoder framework, which captures the global information while preserving the generative ability, our model can better learn the representations from raw data. To further enhance pre-training effectiveness, we design a novel loss that combines three distinct tasks: Masked Span Prediction (MSP), Packet Order Prediction (POP), and Homologous Traffic Prediction (HTP). Evaluation results across various benchmark datasets demonstrate that the proposed Lens outperforms the baselines in most downstream tasks related to both traffic understanding and generation. Notably, it also requires much less labeled data for fine-tuning compared to current methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.03646v3</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qineng Wang, Chen Qian, Xiaochang Li, Ziyu Yao, Huajie Shao</dc:creator>
    </item>
  </channel>
</rss>
