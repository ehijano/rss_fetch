<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Oct 2025 01:48:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Bifurcation of spiking oscillations from a center in resonate-and-fire neurons</title>
      <link>https://arxiv.org/abs/2510.13156</link>
      <description>arXiv:2510.13156v1 Announce Type: new 
Abstract: The theta rhythm is important for many cognitive functions including spatial processing, memory encoding, and memory recall. The information processing underlying these functions is thought to rely on consistent, phase-specific spiking throughout a theta oscillation that may fluctuate significantly in baseline (center of oscillations), frequency, or amplitude. Experimental evidence shows that spikes can occur at specific phases even when the baseline membrane potential varies significantly, such that the integrity of phase-locking persists across a large variability in spike threshold. The mechanism of this precise spike timing during the theta rhythm is not yet known and previous mathematical models have not reflected the large variability in threshold potential seen experimentally. Here we introduce a straightforward mathematical neural model capable of demonstrating a phase-locked spiking in the face of significant baseline membrane potential fluctuation during theta rhythm. This novel approach incorporates a degenerate grazing bifurcation of an asymptotically stable oscillation. This model suggests a potential mechanism for how biological neurons can consistently produce spikes near the peak of a variable membrane potential oscillation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13156v1</guid>
      <category>q-bio.NC</category>
      <category>math.DS</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oleg Makarenkov, Marianne Bezaire, Michael Hasselmo</dc:creator>
    </item>
    <item>
      <title>Jacobian-Based Interpretation of Nonlinear Neural Encoding Model</title>
      <link>https://arxiv.org/abs/2510.13688</link>
      <description>arXiv:2510.13688v1 Announce Type: new 
Abstract: In recent years, the alignment between artificial neural network (ANN) embeddings and blood oxygenation level dependent (BOLD) responses in functional magnetic resonance imaging (fMRI) via neural encoding models has significantly advanced research on neural representation mechanisms and interpretability in the brain. However, these approaches remain limited in characterizing the brain's inherently nonlinear response properties. To address this, we propose the Jacobian-based Nonlinearity Evaluation (JNE), an interpretability metric for nonlinear neural encoding models. JNE quantifies nonlinearity by statistically measuring the dispersion of local linear mappings (Jacobians) from model representations to predicted BOLD responses, thereby approximating the nonlinearity of BOLD signals. Centered on proposing JNE as a novel interpretability metric, we validated its effectiveness through controlled simulation experiments on various activation functions and network architectures, and further verified it on real fMRI data, demonstrating a hierarchical progression of nonlinear characteristics from primary to higher-order visual cortices, consistent with established cortical organization. We further extended JNE with Sample-Specificity (JNE-SS), revealing stimulus-selective nonlinear response patterns in functionally specialized brain regions. As the first interpretability metric for quantifying nonlinear responses, JNE provides new insights into brain information processing. Code available at https://github.com/Gaitxh/JNE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13688v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaohui Gao, Haoran Yang, Yue Cheng, Mengfei Zuo, Yiheng Liu, Peiyang Li, Xintao Hu</dc:creator>
    </item>
    <item>
      <title>Data-Driven Reduced Modeling of Recurrent Neural Networks</title>
      <link>https://arxiv.org/abs/2510.13519</link>
      <description>arXiv:2510.13519v1 Announce Type: cross 
Abstract: Artificial Recurrent Neural Networks (RNNs) are widely used in neuroscience to model the collective activity of neurons during behavioral tasks. The high dimensionality of their parameter and activity spaces, however, often make it challenging to infer and interpret the fundamental features of their dynamics.
  In this study, we employ recent nonlinear dynamical system techniques to uncover the core dynamics of several RNNs used in contemporary neuroscience. Specifically, using a data-driven approach, we identify Spectral Submanifolds (SSMs), i.e., low-dimensional attracting invariant manifolds tangent to the eigenspaces of fixed points. The internal dynamics of SSMs serve as nonlinear models that reduce the dimensionality of the full RNNs by orders of magnitude.
  Through low-dimensional, SSM-reduced models, we give mathematically precise definitions of line and ring attractors, which are intuitive concepts commonly used to explain decision-making and working memory. The new level of understanding of RNNs obtained from SSM reduction enables the interpretation of mathematically well-defined and robust structures in neuronal dynamics, leading to novel predictions about the neural computations underlying behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13519v1</guid>
      <category>math.DS</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alice Marraffa, Renate Krause, Valerio Mante, George Haller</dc:creator>
    </item>
    <item>
      <title>Scaling Vision Transformers for Functional MRI with Flat Maps</title>
      <link>https://arxiv.org/abs/2510.13768</link>
      <description>arXiv:2510.13768v1 Announce Type: cross 
Abstract: A key question for adapting modern deep learning architectures to functional MRI (fMRI) is how to represent the data for model input. To bridge the modality gap between fMRI and natural images, we transform the 4D volumetric fMRI data into videos of 2D fMRI activity flat maps. We train Vision Transformers on 2.3K hours of fMRI flat map videos from the Human Connectome Project using the spatiotemporal masked autoencoder (MAE) framework. We observe that masked fMRI modeling performance improves with dataset size according to a strict power scaling law. Downstream classification benchmarks show that our model learns rich representations supporting both fine-grained state decoding across subjects, as well as subject-specific trait decoding across changes in brain state. This work is part of an ongoing open science project to build foundation models for fMRI data. Our code and datasets are available at https://github.com/MedARC-AI/fmri-fm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13768v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Connor Lane, Daniel Z. Kaplan, Tanishq Mathew Abraham, Paul S. Scotti</dc:creator>
    </item>
    <item>
      <title>Attractive and Repulsive Perceptual Biases Naturally Emerge in Generative Adversarial Inference</title>
      <link>https://arxiv.org/abs/2507.19944</link>
      <description>arXiv:2507.19944v2 Announce Type: replace 
Abstract: Perceptual estimates exhibit a reversal in bias depending on uncertainty: they shift toward prior expectations under high stimulus noise, but away from them when sensory noise dominates. The normative framework of a Bayesian observer model can account for this phenomenon, yet most formulations treat it as given rather than explaining its emergence through learning. We introduce a Generative Adversarial Inference (GAI) network that acquires latent representations and inference strategies directly from sensory inputs, without hand-crafted likelihoods or priors. Trained using adversarial learning with reconstruction on Gabor stimuli under varying uncertainty, the network learns to recover underlying stimuli from noisy inputs, and spontaneously reproduces the bias reversal observed in human perception. This emergent behavior arises from network responses that reveal signatures of efficient coding and Bayesian inference. Our findings provide an end-to-end account of perceptual bias that unifies normative theory and deep learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19944v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyun-Jun Jeon, Hansol Choi, Oh-Sang Kwon</dc:creator>
    </item>
    <item>
      <title>Egocentric Visual Navigation through Hippocampal Sequences</title>
      <link>https://arxiv.org/abs/2510.09951</link>
      <description>arXiv:2510.09951v2 Announce Type: replace 
Abstract: Sequential activation of place-tuned neurons in an animal during navigation is typically interpreted as reflecting the sequence of input from adjacent positions along the trajectory. More recent theories about such place cells suggest sequences arise from abstract cognitive objectives like planning. Here, we propose a mechanistic and parsimonious interpretation to complement these ideas: hippocampal sequences arise from intrinsic recurrent circuitry that propagates activity without readily available input, acting as a temporal memory buffer for extremely sparse inputs. We implement a minimal sequence generator inspired by neurobiology and pair it with an actor-critic learner for egocentric visual navigation. Our agent reliably solves a continuous maze without explicit geometric cues, with performance depending on the length of the recurrent sequence. Crucially, the model outperforms LSTM cores under sparse input conditions (16 channels, ~2.5% activity), but not under dense input, revealing a strong interaction between representational sparsity and memory architecture. In contrast to LSTM agents, hidden sequence units develop localized place fields, distance-dependent spatial kernels, and task-dependent remapping, while inputs orthogonalize and spatial information increases across layers. These phenomena align with neurobiological data and are causal to performance. Together, our results show that sparse input synergizes with sequence-generating dynamics, providing both a mechanistic account of place cell sequences in the mammalian hippocampus and a simple inductive bias for reinforcement learning based on sparse egocentric inputs in navigation tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09951v2</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiao-Xiong Lin, Yuk Hoi Yiu, Christian Leibold</dc:creator>
    </item>
    <item>
      <title>Of Mice and Machines: A Comparison of Learning Between Real World Mice and RL Agents</title>
      <link>https://arxiv.org/abs/2505.12204</link>
      <description>arXiv:2505.12204v3 Announce Type: replace-cross 
Abstract: Recent advances in reinforcement learning (RL) have demonstrated impressive capabilities in complex decision-making tasks. This progress raises a natural question: how do these artificial systems compare to biological agents, which have been shaped by millions of years of evolution? To help answer this question, we undertake a comparative study of biological mice and RL agents in a predator-avoidance maze environment. Through this analysis, we identify a striking disparity: RL agents consistently demonstrate a lack of self-preservation instinct, readily risking ``death'' for marginal efficiency gains. These risk-taking strategies are in contrast to biological agents, which exhibit sophisticated risk-assessment and avoidance behaviors. Towards bridging this gap between the biological and artificial, we propose two novel mechanisms that encourage more naturalistic risk-avoidance behaviors in RL agents. Our approach leads to the emergence of naturalistic behaviors, including strategic environment assessment, cautious path planning, and predator avoidance patterns that closely mirror those observed in biological systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12204v3</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuo Han, German Espinosa, Junda Huang, Daniel A. Dombeck, Malcolm A. MacIver, Bradly C. Stadie</dc:creator>
    </item>
    <item>
      <title>Spike-frequency and h-current based adaptation are dynamically equivalent in a Wilson-Cowan field model</title>
      <link>https://arxiv.org/abs/2510.08436</link>
      <description>arXiv:2510.08436v3 Announce Type: replace-cross 
Abstract: During slow-wave sleep, the brain produces traveling waves of slow oscillations (SOs; $\leq 2$ Hz), characterized by the propagation of alternating high- and low-activity states. The question of internal mechanisms that modulate traveling waves of SOs is still unanswered although it is established that it is an adaptation mechanism that mediates them. One mechanism investigated is spike-frequency adaptation, a hyperpolarizing feedback current that is activated during periods of high-activity. An alternative mechanism is based on hyperpolarization-activated currents, which are positive feedback currents that are activated in low-activity states. Both adaptation mechanisms were shown to feature SO-like dynamics in neuronal populations, and the inclusion of a spatial domain seems to enhance observable differences in their effects. To investigate this in detail, we examine a spatially extended two-population Wilson-Cowan model with local spatial coupling and the excitatory populations equipped with either one of the two adaptation mechanisms. We describe them with the same dynamical equation and include the inverse mode of action by changing the signs of adaptation strength and gain. We show that the dynamical systems are mathematically equivalent under a compensatory external input, which depends on the adaptation strength, leading to a shift in state space of the otherwise equivalent bifurcation structure. Strong enough adaptation is required to induce traveling waves. Additionally, adaptation modulates the properties of the spatio-temporal activity patterns, such as temporal and spatial frequencies, and the speed of the traveling waves, all of which increase with increasing strength. Though being dynamically equivalent, our results also explain why location-dependent variations in feedback strength cause differences in the propagation of traveling waves between both adaptation mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08436v3</guid>
      <category>nlin.AO</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ronja Str\"omsd\"orfer, Klaus Obermayer</dc:creator>
    </item>
    <item>
      <title>The Algorithmic Regulator</title>
      <link>https://arxiv.org/abs/2510.10300</link>
      <description>arXiv:2510.10300v3 Announce Type: replace-cross 
Abstract: The regulator theorem states that, under certain conditions, any optimal controller must embody a model of the system it regulates, grounding the idea that controllers embed, explicitly or implicitly, internal models of the controlled. This principle underpins neuroscience and predictive brain theories like the Free-Energy Principle or Kolmogorov/Algorithmic Agent theory. However, the theorem is only proven in limited settings. Here, we treat the deterministic, closed, coupled world-regulator system $(W,R)$ as a single self-delimiting program $p$ via a constant-size wrapper that produces the world output string~$x$ fed to the regulator. We analyze regulation from the viewpoint of the algorithmic complexity of the output, $K(x)$. We define $R$ to be a \emph{good algorithmic regulator} if it \emph{reduces} the algorithmic complexity of the readout relative to a null (unregulated) baseline $\varnothing$, i.e., \[ \Delta = K\big(O_{W,\varnothing}\big) - K\big(O_{W,R}\big) &gt; 0. \] We then prove that the larger $\Delta$ is, the more world-regulator pairs with high mutual algorithmic information are favored. More precisely, a complexity gap $\Delta &gt; 0$ yields \[ \Pr\big((W,R)\mid x\big) \le C\,2^{\,M(W{:}R)}\,2^{-\Delta}, \] making low $M(W{:}R)$ exponentially unlikely as $\Delta$ grows. This is an AIT version of the idea that ``the regulator contains a model of the world.'' The framework is distribution-free, applies to individual sequences, and complements the Internal Model Principle. Beyond this necessity claim, the same coding-theorem calculus singles out a \emph{canonical scalar objective} and implicates a \emph{planner}. On the realized episode, a regulator behaves \emph{as if} it minimized the conditional description length of the readout.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10300v3</guid>
      <category>cs.CC</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.IT</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Giulio Ruffini</dc:creator>
    </item>
  </channel>
</rss>
