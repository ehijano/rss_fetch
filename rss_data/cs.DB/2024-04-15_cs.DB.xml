<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 16 Apr 2024 04:00:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Can LLMs substitute SQL? Comparing Resource Utilization of Querying LLMs versus Traditional Relational Databases</title>
      <link>https://arxiv.org/abs/2404.08727</link>
      <description>arXiv:2404.08727v1 Announce Type: new 
Abstract: Large Language Models (LLMs) can automate or substitute different types of tasks in the software engineering process. This study evaluates the resource utilization and accuracy of LLM in interpreting and executing natural language queries against traditional SQL within relational database management systems. We empirically examine the resource utilization and accuracy of nine LLMs varying from 7 to 34 Billion parameters, including Llama2 7B, Llama2 13B, Mistral, Mixtral, Optimus-7B, SUS-chat-34B, platypus-yi-34b, NeuralHermes-2.5-Mistral-7B and Starling-LM-7B-alpha, using a small transaction dataset. Our findings indicate that using LLMs for database queries incurs significant energy overhead (even small and quantized models), making it an environmentally unfriendly approach. Therefore, we advise against replacing relational databases with LLMs due to their substantial resource utilization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08727v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiang Zhang, Khatoon Khedri, Reza Rawassizadeh</dc:creator>
    </item>
    <item>
      <title>Bullion: A Column Store for Machine Learning</title>
      <link>https://arxiv.org/abs/2404.08901</link>
      <description>arXiv:2404.08901v1 Announce Type: new 
Abstract: The past two decades have witnessed columnar storage revolutionizing data warehousing and analytics. However, the rapid growth of machine learning poses new challenges to this domain. This paper presents Bullion, a columnar storage system tailored for machine learning workloads. Bullion addresses the complexities of data compliance, optimizes the encoding of long sequence sparse features, efficiently manages wide-table projections, and introduces feature quantization in storage. By aligning with the evolving requirements of ML applications, Bullion extends columnar storage to various scenarios, from advertising and recommendation systems to the expanding realm of Generative AI.
  Preliminary experimental results and theoretical analysis demonstrate Bullion's superior performance in handling the unique demands of machine learning workloads compared to existing columnar storage solutions. Bullion significantly reduces I/O costs for deletion compliance, achieves substantial storage savings with its optimized encoding scheme for sparse features, and drastically improves metadata parsing speed for wide-table projections. These advancements position Bullion as a critical component in the future of machine learning infrastructure, enabling organizations to efficiently manage and process the massive volumes of data required for training and inference in modern AI applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08901v1</guid>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gang Liao, Ye Liu, Jianjun Chen, Daniel J. Abadi</dc:creator>
    </item>
    <item>
      <title>Optimizing Disjunctive Queries with Tagged Execution</title>
      <link>https://arxiv.org/abs/2404.09109</link>
      <description>arXiv:2404.09109v1 Announce Type: new 
Abstract: Despite decades of research into query optimization, optimizing queries with disjunctive predicate expressions remains a challenge. Solutions employed by existing systems (if any) are often simplistic and lead to much redundant work being performed by the execution engine. To address these problems, we propose a novel form of query execution called tagged execution. Tagged execution groups tuples into subrelations based on which predicates in the query they satisfy (or don't satisfy) and tags them with that information. These tags then provide additional context for query operators to take advantage of during runtime, allowing them to eliminate much of the redundant work performed by traditional engines and realize predicate pushdown optimizations for disjunctive predicates. However, tagged execution brings its own challenges, and the question of what tags to create is a nontrivial one. Careless creation of tags can lead to an exponential blowup in the tag space, with the overhead outweighing the benefits. To address this issue, we present a technique called tag generalization to minimize the space of tags. We implemented the tagged execution model with tag generalization in our system Basilisk, and our evaluation shows an average 2.7x speedup in runtime over the traditional execution model with up to a 19x speedup in certain situations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09109v1</guid>
      <category>cs.DB</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3654961</arxiv:DOI>
      <dc:creator>Albert Kim, Samuel Madden</dc:creator>
    </item>
    <item>
      <title>climber++: Pivot-Based Approximate Similarity Search over Big Data Series</title>
      <link>https://arxiv.org/abs/2404.09637</link>
      <description>arXiv:2404.09637v1 Announce Type: new 
Abstract: The generation and collection of big data series are becoming an integral part of many emerging applications in sciences, IoT, finance, and web applications among several others. The terabyte-scale of data series has motivated recent efforts to design fully distributed techniques for supporting operations such as approximate kNN similarity search, which is a building block operation in most analytics services on data series. Unfortunately, these techniques are heavily geared towards achieving scalability at the cost of sacrificing the results' accuracy. State-of-the-art systems report accuracy below 10% and 40%, respectively, which is not practical for many real-world applications. In this paper, we investigate the root problems in these existing techniques that limit their ability to achieve better a trade-off between scalability and accuracy. Then, we propose a framework, called CLIMBER, that encompasses a novel feature extraction mechanism, indexing scheme, and query processing algorithms for supporting approximate similarity search in big data series. For CLIMBER, we propose a new loss-resistant dual representation composed of rank-sensitive and ranking-insensitive signatures capturing data series objects. Based on this representation, we devise a distributed two-level index structure supported by an efficient data partitioning scheme. Our similarity metrics tailored for this dual representation enables meaningful comparison and distance evaluation between the rank-sensitive and ranking-insensitive signatures. Finally, we propose two efficient query processing algorithms, CLIMBER-kNN and CLIMBER-kNN-Adaptive, for answering approximate kNN similarity queries. Our experimental study on real-world and benchmark datasets demonstrates that CLIMBER, unlike existing techniques, features results' accuracy above 80% while retaining the desired scalability to terabytes of data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09637v1</guid>
      <category>cs.DB</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>ICDE 2024</arxiv:journal_reference>
      <dc:creator>Liang Zhang, Mohamed Y. Eltabakh, Elke A. Rundensteiner, Khalid Alnuaim</dc:creator>
    </item>
    <item>
      <title>Combining PatternRank with Huffman Coding: A Novel Compression Algorithm</title>
      <link>https://arxiv.org/abs/2404.08669</link>
      <description>arXiv:2404.08669v1 Announce Type: cross 
Abstract: The escalating volume of data involved in Android backup packages necessitates an innovative approach to compression beyond traditional methods like GZIP, which may not fully exploit the redundancy inherent in Android backups, particularly those containing extensive XML data. This paper introduces the PatternRank algorithm, a novel compression strategy specifically designed for Android backups. PatternRank leverages pattern recognition and ranking, combined with Huffman coding, to efficiently compress data by identifying and replacing frequent, longer patterns with shorter codes. We detail two versions of the PatternRank algorithm: the original version focuses on dynamic pattern extraction and ranking, while the second version incorporates a pre-defined dictionary optimized for the common patterns found in Android backups, particularly within XML files. This tailored approach ensures that PatternRank not only outperforms traditional compression methods in terms of compression ratio and speed but also remains highly effective when dealing with the specific challenges posed by Android backup data. Our analysis includes a comparative study of compression performance across GZIP, PatternRank v1, PatternRank v2, and a combined PatternRank-Huffman method, highlighting the superior efficiency and potential of PatternRank in managing the growing data demands of Android backup packages. Through this exploration, we underscore the significance of adopting pattern-based compression algorithms in optimizing data storage and transmission in the mobile domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08669v1</guid>
      <category>cs.IR</category>
      <category>cs.DB</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jasurbek Shukurov</dc:creator>
    </item>
    <item>
      <title>A Circus of Circuits: Connections Between Decision Diagrams, Circuits, and Automata</title>
      <link>https://arxiv.org/abs/2404.09674</link>
      <description>arXiv:2404.09674v1 Announce Type: cross 
Abstract: This document is an introduction to two related formalisms to define Boolean functions: binary decision diagrams, and Boolean circuits. It presents these formalisms and several of their variants studied in the setting of knowledge compilation. Last, it explains how these formalisms can be connected to the notions of automata over words and trees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09674v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <category>cs.FL</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antoine Amarilli, Marcelo Arenas, YooJung Choi, Mika\"el Monet, Guy Van den Broeck, Benjie Wang</dc:creator>
    </item>
    <item>
      <title>Cardinality Estimation of Subgraph Matching: A Filtering-Sampling Approach</title>
      <link>https://arxiv.org/abs/2309.15433</link>
      <description>arXiv:2309.15433v2 Announce Type: replace 
Abstract: Subgraph counting is a fundamental problem in understanding and analyzing graph structured data, yet computationally challenging. This calls for an accurate and efficient algorithm for Subgraph Cardinality Estimation, which is to estimate the number of all isomorphic embeddings of a query graph in a data graph. We present FaSTest, a novel algorithm that combines (1) a powerful filtering technique to significantly reduce the sample space, (2) an adaptive tree sampling algorithm for accurate and efficient estimation, and (3) a worst-case optimal stratified graph sampling algorithm for difficult instances. Extensive experiments on real-world datasets show that FaSTest outperforms state-of-the-art sampling-based methods by up to two orders of magnitude and GNN-based methods by up to three orders of magnitude in terms of accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.15433v2</guid>
      <category>cs.DB</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wonseok Shin, Siwoo Song, Kunsoo Park, Wook-Shin Han</dc:creator>
    </item>
    <item>
      <title>Aleph Filter: To Infinity in Constant Time</title>
      <link>https://arxiv.org/abs/2404.04703</link>
      <description>arXiv:2404.04703v2 Announce Type: replace 
Abstract: Filter data structures are widely used in various areas of computer science to answer approximate set-membership queries. In many applications, the data grows dynamically, requiring their filters to expand along with the data that they represent. However, existing methods for expanding filters cannot maintain stable performance, memory footprint, and false positive rate at the same time. We address this problem with Aleph Filter, which makes the following contributions. (1) It supports all operations (insertions, queries, deletes, etc.) in constant time, no matter how much the data grows. (2) Given any rough estimate of how much the data will ultimately grow, Aleph Filter provides far superior memory vs. false positive rate trade-offs, even if the estimate is off by orders of magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04703v2</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niv Dayan, Ioana Bercea, Rasmus Pagh</dc:creator>
    </item>
    <item>
      <title>A Fair and Memory/Time-efficient Hashmap</title>
      <link>https://arxiv.org/abs/2307.11355</link>
      <description>arXiv:2307.11355v2 Announce Type: replace-cross 
Abstract: Hashmap is a fundamental data structure in computer science. There has been extensive research on constructing hashmaps that minimize the number of collisions leading to efficient lookup query time. Recently, the data-dependant approaches, construct hashmaps tailored for a target data distribution that guarantee to uniformly distribute data across different buckets and hence minimize the collisions. Still, to the best of our knowledge, none of the existing technique guarantees group fairness among different groups of items stored in the hashmap.
  Therefore, in this paper, we introduce FairHash, a data-dependant hashmap that guarantees uniform distribution at the group-level across hash buckets, and hence, satisfies the statistical parity notion of group fairness. We formally define, three notions of fairness and, unlike existing work, FairHash satisfies all three of them simultaneously. We propose three families of algorithms to design fair hashmaps, suitable for different settings. Our ranking-based algorithms reduce the unfairness of data-dependant hashmaps without any memory-overhead. The cut-based algorithms guarantee zero-unfairness in all cases, irrespective of how the data is distributed, but those introduce an extra memory-overhead. Last but not least, the discrepancy-based algorithms enable trading off between various fairness notions. In addition to the theoretical analysis, we perform extensive experiments to evaluate the efficiency and efficacy of our algorithms on real datasets. Our results verify the superiority of FairHash compared to the other baselines on fairness at almost no performance cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.11355v2</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <arxiv:journal_reference>SIGMOD 2024</arxiv:journal_reference>
      <dc:creator>Abolfazl Asudeh, Nima Shahbazi, Stavros Sintos</dc:creator>
    </item>
    <item>
      <title>Systematic Assessment of Tabular Data Synthesis Algorithms</title>
      <link>https://arxiv.org/abs/2402.06806</link>
      <description>arXiv:2402.06806v2 Announce Type: replace-cross 
Abstract: Data synthesis has been advocated as an important approach for utilizing data while protecting data privacy. A large number of tabular data synthesis algorithms (which we call synthesizers) have been proposed. Some synthesizers satisfy Differential Privacy, while others aim to provide privacy in a heuristic fashion. A comprehensive understanding of the strengths and weaknesses of these synthesizers remains elusive due to drawbacks in evaluation metrics and missing head-to-head comparisons of newly developed synthesizers that take advantage of diffusion models and large language models with state-of-the-art marginal-based synthesizers.
  In this paper, we present a systematic evaluation framework for assessing tabular data synthesis algorithms. Specifically, we examine and critique existing evaluation metrics, and introduce a set of new metrics in terms of fidelity, privacy, and utility to address their limitations. Based on the proposed metrics, we also devise a unified objective for tuning, which can consistently improve the quality of synthetic data for all methods. We conducted extensive evaluations of 8 different types of synthesizers on 12 real-world datasets and identified some interesting findings, which offer new directions for privacy-preserving data synthesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06806v2</guid>
      <category>cs.CR</category>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuntao Du, Ninghui Li</dc:creator>
    </item>
  </channel>
</rss>
