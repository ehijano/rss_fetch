<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Jun 2025 04:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Kernel Density Balancing</title>
      <link>https://arxiv.org/abs/2506.12626</link>
      <description>arXiv:2506.12626v1 Announce Type: new 
Abstract: High-throughput chromatin conformation capture (Hi-C) data provide insights into the 3D structure of chromosomes, with normalization being a crucial pre-processing step. A common technique for normalization is matrix balancing, which rescales rows and columns of a Hi-C matrix to equalize their sums. Despite its popularity and convenience, matrix balancing lacks statistical justification. In this paper, we introduce a statistical model to analyze matrix balancing methods and propose a kernel-based estimator that leverages spatial structure. Under mild assumptions, we demonstrate that the kernel-based method is consistent, converges faster, and is more robust to data sparsity compared to existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12626v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John Park, Ning Hao, Yue Selena Niu, Ming Hu</dc:creator>
    </item>
    <item>
      <title>Deep Spatial Neural Net Models with Functional Predictors: Application in Large-Scale Crop Yield Prediction</title>
      <link>https://arxiv.org/abs/2506.13017</link>
      <description>arXiv:2506.13017v1 Announce Type: new 
Abstract: Accurate prediction of crop yield is critical for supporting food security, agricultural planning, and economic decision-making. However, yield forecasting remains a significant challenge due to the complex and nonlinear relationships between weather variables and crop production, as well as spatial heterogeneity across agricultural regions. We propose DSNet, a deep neural network architecture that integrates functional and scalar predictors with spatially varying coefficients and spatial random effects. The method is designed to flexibly model spatially indexed functional data, such as daily temperature curves, and their relationship to variability in the response, while accounting for spatial correlation. DSNet mitigates the curse of dimensionality through a low-rank structure inspired by the spatially varying functional index model (SVFIM). Through comprehensive simulations, we demonstrate that DSNet outperforms state-of-the-art functional regression models for spatial data, when the functional predictors exhibit complex structure and their relationship with the response varies spatially in a potentially nonstationary manner. Application to corn yield data from the U.S. Midwest demonstrates that DSNet achieves superior predictive accuracy compared to both leading machine learning approaches and parametric statistical models. These results highlight the model's robustness and its potential applicability to other weather-sensitive crops.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13017v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yeonjoo Park, Bo Li, Yehua Li</dc:creator>
    </item>
    <item>
      <title>Bayesian Active Learning of (small) Quantile Sets through Expected Estimator Modification</title>
      <link>https://arxiv.org/abs/2506.13211</link>
      <description>arXiv:2506.13211v1 Announce Type: new 
Abstract: Given a multivariate function taking deterministic and uncertain inputs, we consider the problem of estimating a quantile set: a set of deterministic inputs for which the probability that the output belongs to a specific region remains below a given threshold. To solve this problem in the context of expensive-to-evaluate black-box functions, we propose a Bayesian active learning strategy based on Gaussian process modeling. The strategy is driven by a novel sampling criterion, which belongs to a broader principle that we refer to as Expected Estimator Modification (EEM). More specifically, the strategy relies on a novel sampling criterion combined with a sequential Monte Carlo framework that enables the construction of batch-sequential designs for the efficient estimation of small quantile sets. The performance of the strategy is illustrated on several synthetic examples and an industrial application case involving the ROTOR37 compressor model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13211v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Romain Ait Abdelmalek-Lomenech (L2S, RT-UQ), Julien Bect (L2S, RT-UQ), Emmanuel Vazquez (L2S, RT-UQ)</dc:creator>
    </item>
    <item>
      <title>Digging deeper: deep joint species distribution modeling reveals environmental drivers of Earthworm Communities</title>
      <link>https://arxiv.org/abs/2506.13568</link>
      <description>arXiv:2506.13568v1 Announce Type: new 
Abstract: Earthworms are key drivers of soil function, influencing organic matter turnover, nutrient cycling, and soil structure. Understanding the environmental controls on their distribution is essential for predicting the impacts of land use and climate change on soil ecosystems. While local studies have identified abiotic drivers of earthworm communities, broad-scale spatial patterns remain underexplored.
  We developed a multi-species, multi-task deep learning model to jointly predict the distribution of 77 earthworm species across metropolitan France, using historical (1960-1970) and contemporary (1990-2020) records. The model integrates climate, soil, and land cover variables to estimate habitat suitability. We applied SHapley Additive exPlanations (SHAP) to identify key environmental drivers and used species clustering to reveal ecological response groups.
  The joint model achieved high predictive performance (TSS &gt;= 0.7) and improved predictions for rare species compared to traditional species distribution models. Shared feature extraction across species allowed for more robust identification of common and contrasting environmental responses. Precipitation variability, temperature seasonality, and land cover emerged as dominant predictors of earthworm distribution. Species clustering revealed distinct ecological strategies tied to climatic and land use gradients.
  Our study advances both the methodological and ecological understanding of soil biodiversity. We demonstrate the utility of interpretable deep learning approaches for large-scale soil fauna modeling and provide new insights into earthworm habitat specialization. These findings support improved soil biodiversity monitoring and conservation planning in the face of global environmental change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13568v1</guid>
      <category>stat.AP</category>
      <category>q-bio.PE</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sara Si-moussi, Wilfried Thuiller, Esther Galbrun, Thibaud Deca\"ens, Sylvain G\'erard, Daniel F. March\'an, Claire Marsden, Yvan Capowiez, Micka\"el Hedde</dc:creator>
    </item>
    <item>
      <title>The Hammock Plot: Where Categorical and Numerical Data Relax Together</title>
      <link>https://arxiv.org/abs/2506.13630</link>
      <description>arXiv:2506.13630v1 Announce Type: new 
Abstract: Effective methods for visualizing data involving multiple variables, including categorical ones, are limited. The hammock plot (Schonlau, 2003) visualizes both categorical and numerical variables using parallel coordinates. We introduce the Stata implementation hammock. We give numerous examples that explore highlighting, missing values, putting axes on the same scale, and tracing an observation across variables. Further, we introduce parallel univariate plots as an edge case of hammock plots. We also present and make publicly available a new dataset on the 2020 Tour de France.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13630v1</guid>
      <category>stat.AP</category>
      <category>cs.HC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Schonlau, Tiancheng Yang</dc:creator>
    </item>
    <item>
      <title>EUNIS Habitat Maps: Enhancing Thematic and Spatial Resolution for Europe through Machine Learning</title>
      <link>https://arxiv.org/abs/2506.13649</link>
      <description>arXiv:2506.13649v1 Announce Type: new 
Abstract: The EUNIS habitat classification is crucial for categorising European habitats, supporting European policy on nature conservation and implementing the Nature Restoration Law. To meet the growing demand for detailed and accurate habitat information, we provide spatial predictions for 260 EUNIS habitat types at hierarchical level 3, together with independent validation and uncertainty analyses.
  Using ensemble machine learning models, together with high-resolution satellite imagery and ecologically meaningful climatic, topographic and edaphic variables, we produced a European habitat map indicating the most probable EUNIS habitat at 100-m resolution across Europe. Additionally, we provide information on prediction uncertainty and the most probable habitats at level 3 within each EUNIS level 1 formation. This product is particularly useful for both conservation and restoration purposes.
  Predictions were cross-validated at European scale using a spatial block cross-validation and evaluated against independent data from France (forests only), the Netherlands and Austria. The habitat maps obtained strong predictive performances on the validation datasets with distinct trade-offs in terms of recall and precision across habitat formations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13649v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>physics.geo-ph</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sara Si-Moussi, Stephan Hennekens, Sander M\"ucher, Wanda De Keersmaecker, Milan Chytr\'y, Emiliano Agrillo, Fabio Attorre, Idoia Biurrun, Gianmaria Bonari, Andra\v{z} \v{C}arni, Renata \'Cu\v{s}terevska, Tetiana Dziuba, Klaus Ecker, Behl\"ul G\"uler, Ute Jandt, Borja Jim\'enez-Alfaro, Jonathan Lenoir, Jens-Christian Svenning, Grzegorz Swacha, Wilfried Thuiller</dc:creator>
    </item>
    <item>
      <title>Enforcing tail calibration when training probabilistic forecast models</title>
      <link>https://arxiv.org/abs/2506.13687</link>
      <description>arXiv:2506.13687v1 Announce Type: new 
Abstract: Probabilistic forecasts are typically obtained using state-of-the-art statistical and machine learning models, with model parameters estimated by optimizing a proper scoring rule over a set of training data. If the model class is not correctly specified, then the learned model will not necessarily issue forecasts that are calibrated. Calibrated forecasts allow users to appropriately balance risks in decision making, and it is particularly important that forecast models issue calibrated predictions for extreme events, since such outcomes often generate large socio-economic impacts. In this work, we study how the loss function used to train probabilistic forecast models can be adapted to improve the reliability of forecasts made for extreme events. We investigate loss functions based on weighted scoring rules, and additionally propose regularizing loss functions using a measure of tail miscalibration. We apply these approaches to a hierarchy of increasingly flexible forecast models for UK wind speeds, including simple parametric models, distributional regression networks, and conditional generative models. We demonstrate that state-of-the-art models do not issue calibrated forecasts for extreme wind speeds, and that the calibration of forecasts for extreme events can be improved by suitable adaptations to the loss function during model training. This, however, introduces a trade-off between calibrated forecasts for extreme events and calibrated forecasts for more common outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13687v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jakob Benjamin Wessel, Maybritt Schillinger, Frank Kwasniok, Sam Allen</dc:creator>
    </item>
    <item>
      <title>The Maximal Overlap Discrete Wavelet Scattering Transform and Its Application in Classification Tasks</title>
      <link>https://arxiv.org/abs/2506.12039</link>
      <description>arXiv:2506.12039v1 Announce Type: cross 
Abstract: We present the Maximal Overlap Discrete Wavelet Scattering Transform (MODWST), whose construction is inspired by the combination of the Maximal Overlap Discrete Wavelet Transform (MODWT) and the Scattering Wavelet Transform (WST). We also discuss the use of MODWST in classification tasks, evaluating its performance in two applications: stationary signal classification and ECG signal classification. The results demonstrate that MODWST achieved good performance in both applications, positioning itself as a viable alternative to popular methods like Convolutional Neural Networks (CNNs), particularly when the training data set is limited.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12039v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonardo Fonseca Larrubia, Pedro Alberto Morettin, Chang Chiann</dc:creator>
    </item>
    <item>
      <title>A simplified and robust proxy-based approach for overcoming unmeasured confounding in EHR studies</title>
      <link>https://arxiv.org/abs/2506.12177</link>
      <description>arXiv:2506.12177v1 Announce Type: cross 
Abstract: Electronic health records (EHR) are used to study treatment effects in clinical settings, yet unmeasured confounding remains a persistent challenge. Indirect measurements of the unmeasured confounder (proxies) offer a potential solution, but existing approaches -- such as proximal inference or full joint modeling -- can be difficult to implement. We propose a two-stage, proxy-based method that is practical, broadly applicable, and robust. In the first stage, we apply factor analysis to proxy and treatment variables, extracting information on latent factors that serve as a surrogate for the unmeasured confounder. In the second stage, we use this model to build covariates that improve causal effect estimation in a standard outcome regression model. Through simulations, we test the method's performance under assumption violations, including non-normal errors, model misspecification, and scenarios where instruments or confounders are incorrectly treated as proxies. We also apply the method to estimate the effect of hospital admission for older adults presenting to the emergency department with chest pain, a setting where standard analyses may fail to recover plausible effects. Our results show that this simplified strategy recovers more reliable estimates than conventional adjustment methods, offering applied researchers a practical tool for addressing unmeasured confounding with proxy variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12177v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haley Colgate Kottler, Amy Cochran</dc:creator>
    </item>
    <item>
      <title>Temporal cross-validation impacts multivariate time series subsequence anomaly detection evaluation</title>
      <link>https://arxiv.org/abs/2506.12183</link>
      <description>arXiv:2506.12183v1 Announce Type: cross 
Abstract: Evaluating anomaly detection in multivariate time series (MTS) requires careful consideration of temporal dependencies, particularly when detecting subsequence anomalies common in fault detection scenarios. While time series cross-validation (TSCV) techniques aim to preserve temporal ordering during model evaluation, their impact on classifier performance remains underexplored. This study systematically investigates the effect of TSCV strategy on the precision-recall characteristics of classifiers trained to detect fault-like anomalies in MTS datasets. We compare walk-forward (WF) and sliding window (SW) methods across a range of validation partition configurations and classifier types, including shallow learners and deep learning (DL) classifiers. Results show that SW consistently yields higher median AUC-PR scores and reduced fold-to-fold performance variance, particularly for deep architectures sensitive to localized temporal continuity. Furthermore, we find that classifier generalization is sensitive to the number and structure of temporal partitions, with overlapping windows preserving fault signatures more effectively at lower fold counts. A classifier-level stratified analysis reveals that certain algorithms, such as random forests (RF), maintain stable performance across validation schemes, whereas others exhibit marked sensitivity. This study demonstrates that TSCV design in benchmarking anomaly detection models on streaming time series and provide guidance for selecting evaluation strategies in temporally structured learning environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12183v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steven C. Hespeler, Pablo Moriano, Mingyan Li, Samuel C. Hollifield</dc:creator>
    </item>
    <item>
      <title>Statistical Machine Learning for Astronomy -- A Textbook</title>
      <link>https://arxiv.org/abs/2506.12230</link>
      <description>arXiv:2506.12230v1 Announce Type: cross 
Abstract: This textbook provides a systematic treatment of statistical machine learning for astronomical research through the lens of Bayesian inference, developing a unified framework that reveals connections between modern data analysis techniques and traditional statistical methods. We show how these techniques emerge from familiar statistical foundations. The consistently Bayesian perspective prioritizes uncertainty quantification and statistical rigor essential for scientific inference in astronomy. The textbook progresses from probability theory and Bayesian inference through supervised learning including linear regression with measurement uncertainties, logistic regression, and classification. Unsupervised learning topics cover Principal Component Analysis and clustering methods. We then introduce computational techniques through sampling and Markov Chain Monte Carlo, followed by Gaussian Processes as probabilistic nonparametric methods and neural networks within the broader statistical context. Our theory-focused pedagogical approach derives each method from first principles with complete mathematical development, emphasizing statistical insight and complementing with astronomical applications. We prioritize understanding why algorithms work, when they are appropriate, and how they connect to broader statistical principles. The treatment builds toward modern techniques including neural networks through a solid foundation in classical methods and their theoretical underpinnings. This foundation enables thoughtful application of these methods to astronomical research, ensuring proper consideration of assumptions, limitations, and uncertainty propagation essential for advancing astronomical knowledge in the era of large astronomical surveys.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12230v1</guid>
      <category>astro-ph.IM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan-Sen Ting</dc:creator>
    </item>
    <item>
      <title>Generalizable estimation of conditional average treatment effects using Causal Forest in randomized</title>
      <link>https://arxiv.org/abs/2506.12296</link>
      <description>arXiv:2506.12296v1 Announce Type: cross 
Abstract: Generalizing conditional average treatment effects (CATE) estimates in a randomized controlled trial (RCT) to a broader source population can be challenging because of selection bias and high-dimensional covariates. We aim to evaluate CATE estimation approaches using Causal Forest that address selection bias due to trial participation. We propose and compare four CATE estimation approaches using Causal Forest: (1) ignoring selection variables, (2) including selection variables, (3) using inverse probability weighting (IPW) either with (1) or (2). Identifiable condition suggests that including covariates that determine trial selection in CATE-estimating models can yield an unbiased CATE estimate in the source population. However, simulations showed that, in realistic sample sizes in a medical trial, this approach substantially increased variance compared with little gain in bias reduction. IPW-based approaches showed a better performance in most settings by addressing selection bias. Increasing covariates that determine trial participation in Causal Forest estimation can substantially inflate the variance, diminishing benefits of bias reduction. IPW offers a more robust method to adjust for selection bias due to trial participation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12296v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rikuta Hamaya, Etsuji Suzuki, Konan Hara</dc:creator>
    </item>
    <item>
      <title>Truncated Cauchy Combination Test: a Robust and Powerful P-value Combination Method with Arbitrary Correlations</title>
      <link>https://arxiv.org/abs/2506.12489</link>
      <description>arXiv:2506.12489v1 Announce Type: cross 
Abstract: Cauchy combination test has been widely used for combining correlated p-values, but it may fail to work under certain scenarios. We propose a truncated Cauchy combination test (TCCT) which focus on combining p-values with arbitrary correlations, and demonstrate that our proposed test solves the limitations of Cauchy combination test and always has higher power. We prove that the tail probability of our test statistic is asymptotically Cauchy distributed, so it is computationally effective to achieve the combined p-value using our proposed TCCT. We show by simulation that our proposed test has accurate type I error rates, and maintain high power when Cauchy combination test fails to work. We finally perform application studies to illustrate the usefulness of our proposed test on GWAS and microbiome sequencing data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12489v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bo Chen, Wei Xu, Xin Gao</dc:creator>
    </item>
    <item>
      <title>Inferring Grain Size Distributions from Magnetic Hysteresis in M-type Hexaferrites</title>
      <link>https://arxiv.org/abs/2506.12566</link>
      <description>arXiv:2506.12566v1 Announce Type: cross 
Abstract: We develop a stochastic-dynamic framework to infer latent grain size distribution from magnetic hysteresis data in M-type hexaferrite materials, offering an alternative to imaging-based characterization. A stochastic nucleation-growth process yields a Modified Lognormal Power-law grain size distribution. This is combined with Brown's relation to obtain a coercivity probability distribution, which is embedded within a dynamic magnetization model. A key feature is the joint estimation of microstructural parameters, including the critical grain radius, through inverse optimization of full hysteresis loops. Experimental validation on hydrothermally synthesized strontium hexaferrite subjected to nitrogen treatment and recalcination reveals interpretable trajectories of nucleation, growth, and structural memory encoded in the magnetic response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12566v1</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masoud Ataei, Mohammad Jafar Molaei, Abolghasem Ataie</dc:creator>
    </item>
    <item>
      <title>Robust and efficient multiple-unit switchback experimentation</title>
      <link>https://arxiv.org/abs/2506.12654</link>
      <description>arXiv:2506.12654v1 Announce Type: cross 
Abstract: User-randomized A/B testing has emerged as the gold standard for online experimentation. However, when this kind of approach is not feasible due to legal, ethical or practical considerations, experimenters have to consider alternatives like item-randomization. Item-randomization is often met with skepticism due to its poor empirical performance. To fill this gap, in this paper we introduce a novel and rich class of experimental designs, "Regular Balanced Switchback Designs" (RBSDs). At their core, RBSDs work by randomly changing treatment assignments over both time and items. After establishing the properties of our designs in a potential outcomes framework, characterizing assumptions and conditions under which corresponding estimators are resilient to the presence of carryover effects, we show empirically via both realistic simulations and real e-commerce data that RBSDs systematically outperform standard item-randomized and non-balanced switchback approaches by yielding much more accurate estimates of the causal effects of interest without incurring any additional bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12654v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Missault, Lorenzo Masoero, Christian Delb\'e, Thomas Richardson, Guido Imbens</dc:creator>
    </item>
    <item>
      <title>The Mixed-Sparse-Smooth-Model Toolbox (MSSM): Efficient Estimation and Selection of Large Multi-Level Statistical Models</title>
      <link>https://arxiv.org/abs/2506.13132</link>
      <description>arXiv:2506.13132v1 Announce Type: cross 
Abstract: Additive smooth models, such as Generalized additive models (GAMs) of location, scale, and shape (GAMLSS), are a popular choice for modeling experimental data. However, software available to fit such models is usually not tailored specifically to the estimation of mixed models. As a result, estimation can slow down as the number of random effects increases. Additionally, users often have to provide a substantial amount of problem-specific information in case they are interested in more general non-standard smooth models, such as higher-order derivatives of the likelihood. Here we combined and extended recently proposed strategies to reduce memory requirements and matrix infill into a theoretical framework that supports efficient estimation of general mixed sparse smooth models, including GAMs &amp; GAMLSS, based only on the Gradient and Hessian of the log-likelihood. To make non-standard smooth models more accessible, we developed an approximate estimation algorithm (the L-qEFS update) based on limited-memory quasi-Newton methods. This enables estimation of any general smooth model based only on the log-likelihood function. We also considered the problem of model selection for general mixed smooth models. To facilitate practical application we provide a Python implementation of the theoretical framework, algorithms, and model selection strategies presented here: the Mixed-Sparse-Smooth-Model (MSSM) toolbox. MSSM supports estimation and selection of massive additive multi-level models that are impossible to estimate with alternative software, for example of trial level EEG data. Additionally, when the L-qEFS update is used for estimation, implementing a new non-standard smooth model in MSSM is straightforward. Results from multiple simulation studies and real data examples are presented, showing that the framework implemented in MSSM is both efficient and robust to numerical instabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13132v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joshua Krause, Jelmer P. Borst, Jacolien van Rij</dc:creator>
    </item>
    <item>
      <title>Exploring Discrete Factor Analysis with the discFA Package in R</title>
      <link>https://arxiv.org/abs/2506.13309</link>
      <description>arXiv:2506.13309v1 Announce Type: cross 
Abstract: Literature suggested that using the traditional factor analysis for the count data may be inappropriate. With that in mind, discrete factor analysis builds on fitting systems of dependent discrete random variables to data. The data should be in the form of non-negative counts. Data may also be truncated at some positive integer value. The discFA package in R allows for two distributions: Poisson and Negative Binomial, in combination with possible zero inflation and possible truncation, hence, eight different alternatives. A forward search algorithm is employed to find the model optimal factor model with the lowest AIC. Several different illustrative examples from psychology, agriculture, car industry, and a simulated data will be analyzed at the end.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13309v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Reza Arabi Belaghi, Yasin Asar, Rolf Larsson</dc:creator>
    </item>
    <item>
      <title>Calibrated Predictive Lower Bounds on Time-to-Unsafe-Sampling in LLMs</title>
      <link>https://arxiv.org/abs/2506.13593</link>
      <description>arXiv:2506.13593v1 Announce Type: cross 
Abstract: We develop a framework to quantify the time-to-unsafe-sampling - the number of large language model (LLM) generations required to trigger an unsafe (e.g., toxic) response. Estimating this quantity is challenging, since unsafe responses are exceedingly rare in well-aligned LLMs, potentially occurring only once in thousands of generations. As a result, directly estimating time-to-unsafe-sampling would require collecting training data with a prohibitively large number of generations per prompt. However, with realistic sampling budgets, we often cannot generate enough responses to observe an unsafe outcome for every prompt, leaving the time-to-unsafe-sampling unobserved in many cases, making the estimation and evaluation tasks particularly challenging. To address this, we frame this estimation problem as one of survival analysis and develop a provably calibrated lower predictive bound (LPB) on the time-to-unsafe-sampling of a given prompt, leveraging recent advances in conformal prediction. Our key innovation is designing an adaptive, per-prompt sampling strategy, formulated as a convex optimization problem. The objective function guiding this optimized sampling allocation is designed to reduce the variance of the estimators used to construct the LPB, leading to improved statistical efficiency over naive methods that use a fixed sampling budget per prompt. Experiments on both synthetic and real data support our theoretical results and demonstrate the practical utility of our method for safety risk assessment in generative AI models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13593v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hen Davidov, Gilad Freidkin, Shai Feldman, Yaniv Romano</dc:creator>
    </item>
    <item>
      <title>A Hybrid Artificial Intelligence Method for Estimating Flicker in Power Systems</title>
      <link>https://arxiv.org/abs/2506.13611</link>
      <description>arXiv:2506.13611v1 Announce Type: cross 
Abstract: This paper introduces a novel hybrid AI method combining H filtering and an adaptive linear neuron network for flicker component estimation in power distribution systems.The proposed method leverages the robustness of the H filter to extract the voltage envelope under uncertain and noisy conditions followed by the use of ADALINE to accurately identify flicker frequencies embedded in the envelope.This synergy enables efficient time domain estimation with rapid convergence and noise resilience addressing key limitations of existing frequency domain approaches.Unlike conventional techniques this hybrid AI model handles complex power disturbances without prior knowledge of noise characteristics or extensive training.To validate the method performance we conduct simulation studies based on IEC Standard 61000 4 15 supported by statistical analysis Monte Carlo simulations and real world data.Results demonstrate superior accuracy robustness and reduced computational load compared to Fast Fourier Transform and Discrete Wavelet Transform based estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13611v1</guid>
      <category>eess.SY</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Javad Enayati, Pedram Asef, Alexandre Benoit</dc:creator>
    </item>
    <item>
      <title>Probabilistic patient risk profiling with pair-copula constructions</title>
      <link>https://arxiv.org/abs/2506.13731</link>
      <description>arXiv:2506.13731v1 Announce Type: cross 
Abstract: We present, to our knowledge, the first clinical application of vine copula-based classifiers for probabilistic perioperative risk prediction. We obtain full joint probability models for mixed continuous-ordinal variables by fitting a separate vine copula to each outcome class, capturing nonlinear and tail-asymmetric dependence. In a cohort of 767 elective bowel surgeries (81 serious vs. 686 non-serious complications), posterior probabilities from the fitted classification models are used to allocate patients into low-, moderate-, and high-risk groups. Compared to weighted logistic regression and random forests with stratified sampling, the vine copula-based classifiers achieve up to 10% lower class-specific Brier scores and negative log-likelihoods on the out-of-sample. The vine copula-based classifier identifies a larger cohort of true low-risk patients potentially eligible for early discharge. Scenario analyses based on the fitted vine copula models provide interpretable risk profiles, including nonlinear relationships between body mass index, surgery duration, and blood loss, which might remain undetected under linear models. These results demonstrate that vine copula-based classifiers offer a reliable and interpretable framework for individualized, probability-based patient risk profiling. As such, they represent a promising tool for data-driven decision making in perioperative care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13731v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\"Ozge \c{S}ahin</dc:creator>
    </item>
    <item>
      <title>Using iterated local alignment to aggregate trajectory data into a traffic flow map</title>
      <link>https://arxiv.org/abs/2406.17500</link>
      <description>arXiv:2406.17500v5 Announce Type: replace 
Abstract: Vehicle trajectories are a promising GNSS (Global Navigation Satellite System) data source to compute multi-scale traffic flow maps ranging from the city/regional level to the road level. The main obstacle is that trajectory data are prone to measurement noise. While this is negligible for city level, large-scale flow aggregation, it poses substantial difficulties for road level, small-scale aggregation. To overcome these difficulties, we introduce innovative local alignment algorithms, where we infer road segments to serve as local reference segments, and proceed to align nearby road segments to them. We deploy these algorithms in an iterative workflow to compute locally aligned flow maps. By applying this workflow to synthetic and empirical trajectories, we verify that our locally aligned flow maps provide high levels of accuracy and spatial resolution of flow aggregation at multiple scales for static and interactive maps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17500v5</guid>
      <category>stat.AP</category>
      <category>cs.CE</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tarn Duong</dc:creator>
    </item>
    <item>
      <title>A Versatility Measure for Parametric Risk Models</title>
      <link>https://arxiv.org/abs/2407.19218</link>
      <description>arXiv:2407.19218v3 Announce Type: replace 
Abstract: Parametric statistical methods play a central role in analyzing risk through its underlying frequency and severity components. Given the wide availability of numerical algorithms and high-speed computers, researchers and practitioners often model these separate (although possibly statistically dependent) random variables by fitting a large number of parametric probability distributions to historical data and then comparing goodness-of-fit statistics. However, this approach is highly susceptible to problems of overfitting because it gives insufficient weight to fundamental considerations of functional simplicity and adaptability. To address this shortcoming, we propose a formal mathematical measure for assessing the versatility of frequency and severity distributions prior to their application. We then illustrate this approach by computing and comparing values of the versatility measure for a variety of probability distributions commonly used in risk analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19218v3</guid>
      <category>stat.AP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael R. Powers, Jiaxin Xu</dc:creator>
    </item>
    <item>
      <title>Bayesian competing risks survival modeling for assessing the cause of death of patients with heart failure</title>
      <link>https://arxiv.org/abs/2409.16080</link>
      <description>arXiv:2409.16080v2 Announce Type: replace 
Abstract: Competing risk models are survival models with several events of interest acting in competition and whose occurrence is only observed for the event that occurs first in time. This paper presents a Bayesian approach to these models in which the issue of model selection is treated in a special way by proposing generalizations of some of the Bayesian procedures used in univariate survival analysis. This research is motivated by a study on the survival of patients with hearth failure undergoing cardiac resynchronization therapy, a procedure which involves the implant of a device to stabilize the heartbeat. Two different causes of causes of death have been considered: cardiovascular and non-cardiovascular, and a set of baseline covariates are examined in order to better understand their relationship with both causes of death. Model selection procedures and model checking analyses have been implemented and assessed. The posterior distribution of some relevant outputs such as transition probabilities have been computed and discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16080v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jes\'us Guti\'errez-Botella, Carmen Armero, Thomas Kneib, Mar\'ia P. Pata, Javier Garc\'ia-Seara</dc:creator>
    </item>
    <item>
      <title>Structured and sparse partial least squares coherence for multivariate cortico-muscular analysis</title>
      <link>https://arxiv.org/abs/2503.21802</link>
      <description>arXiv:2503.21802v2 Announce Type: replace 
Abstract: Multivariate cortico-muscular analysis has recently emerged as a promising approach for evaluating the corticospinal neural pathway. However, current multivariate approaches encounter challenges such as high dimensionality and limited sample sizes, thus restricting their further applications. In this paper, we propose a structured and sparse partial least squares coherence algorithm (ssPLSC) to extract shared latent space representations related to cortico-muscular interactions. Our approach leverages an embedded optimization framework by integrating a partial least squares (PLS)-based objective function, a sparsity constraint and a connectivity-based structured constraint, addressing the generalizability, interpretability and spatial structure. To solve the optimization problem, we develop an efficient alternating iterative algorithm within a unified framework and prove its convergence experimentally. Extensive experimental results from one synthetic and several real-world datasets have demonstrated that ssPLSC can achieve competitive or better performance over some representative multivariate cortico-muscular fusion methods, particularly in scenarios characterized by limited sample sizes and high noise levels. This study provides a novel multivariate fusion method for cortico-muscular analysis, offering a transformative tool for the evaluation of corticospinal pathway integrity in neurological disorders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21802v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyao Sun, Qilu Zhang, Di Ma, Tianyu Jia, Shijie Jia, Xiaoxue Zhai, Ruimou Xie, Ping-Ju Lin, Zhibin Li, Yu Pan, Linhong Ji, Chong Li</dc:creator>
    </item>
    <item>
      <title>Causal Inference with Outcomes Truncated by Death and Missing Not at Random</title>
      <link>https://arxiv.org/abs/2406.10554</link>
      <description>arXiv:2406.10554v4 Announce Type: replace-cross 
Abstract: In clinical trials, principal stratification analysis is commonly employed to address the issue of truncation by death, where a subject dies before the outcome can be measured. However, in practice, many survivor outcomes may remain uncollected or be missing not at random, posing a challenge to standard principal stratification analyses. In this paper, we explore the identification, estimation, and bounds of the average treatment effect within a subpopulation of individuals who would potentially survive under both treatment and control conditions. We show that the causal parameter of interest can be identified by introducing a proxy variable that affects the outcome only through the principal strata, while requiring that the treatment variable does not directly affect the missingness mechanism. Subsequently, we propose an approach for estimating causal parameters and derive nonparametric bounds in cases where identification assumptions are violated. We illustrate the performance of the proposed method through simulation studies and a real dataset obtained from a Human Immunodeficiency Virus (HIV) study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10554v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Li, Yuan Liu, Shanshan Luo, Zhi Geng</dc:creator>
    </item>
    <item>
      <title>A Bayesian joint model of multiple longitudinal and categorical outcomes with application to multiple myeloma using permutation-based variable importance</title>
      <link>https://arxiv.org/abs/2407.14311</link>
      <description>arXiv:2407.14311v3 Announce Type: replace-cross 
Abstract: Joint models have proven to be an effective approach for uncovering potentially hidden connections between various types of outcomes, mainly continuous, time-to-event, and binary. Typically, longitudinal continuous outcomes are characterized by linear mixed-effects models, survival outcomes are described by proportional hazards models, and the link between outcomes are captured by shared random effects. Other modeling variations include generalized linear mixed-effects models for longitudinal data and logistic regression when a binary outcome is present, rather than time until an event of interest. However, in a clinical research setting, one might be interested in modeling the physician's chosen treatment based on the patient's medical history to identify prognostic factors. In this situation, there are often multiple treatment options, requiring the use of a multiclass classification approach. Inspired by this context, we develop a Bayesian joint model for longitudinal and categorical data. In particular, our motivation comes from a multiple myeloma study, in which biomarkers display nonlinear trajectories that are well captured through bi-exponential submodels, where patient-level information is shared with the categorical submodel. We also present a variable importance strategy to rank prognostic factors. We apply our proposal and a competing model to the multiple myeloma data, compare the variable importance and inferential results for both models, and illustrate patient-level interpretations using our joint model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14311v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Danilo Alvares, Jessica K. Barrett, Fran\c{c}ois Mercier, Jochen Schulze, Sean Yiu, Felipe Castro, Spyros Roumpanis, Yajing Zhu</dc:creator>
    </item>
    <item>
      <title>Hierarchical Latent Class Models for Mortality Surveillance Using Partially Verified Verbal Autopsies</title>
      <link>https://arxiv.org/abs/2410.09274</link>
      <description>arXiv:2410.09274v3 Announce Type: replace-cross 
Abstract: Monitoring cause-of-death data is an important part of understanding disease burdens and effects of public health interventions. Verbal autopsy (VA) is a well-established method for gathering information about deaths outside of hospitals by conducting an interview to caregivers of a deceased person. It is usually the only tool for cause-of-death surveillance in low-resource settings. A critical limitation with current practices of VA analysis is that all algorithms require either domain knowledge about symptom-cause relationships or large labeled datasets for model training. Therefore, they cannot be easily adopted during public health emergencies when new diseases emerge with rapidly evolving epidemiological patterns. In this paper, we consider estimating the fraction of deaths due to an emerging disease. We develop a novel Bayesian framework using hierarchical latent class models to account for the informative cause-of-death verification process. Our model flexibly captures the joint distribution of symptoms and how they change over time in different sub-populations. We also propose structured priors to improve the precision of the cause-specific mortality estimates for small sub-populations. Our model is motivated by mortality surveillance of COVID-19 related deaths in low-resource settings. We apply our method to a dataset that includes suspected COVID-19 related deaths in Brazil in 2021.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09274v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Zhu, Zehang Richard Li</dc:creator>
    </item>
    <item>
      <title>Optimal treatment regimes for the net benefit of a treatment</title>
      <link>https://arxiv.org/abs/2503.22580</link>
      <description>arXiv:2503.22580v2 Announce Type: replace-cross 
Abstract: We develop a mathematical framework to define an optimal individualized treatment rule (ITR) within the context of prioritized outcomes in a randomized controlled trial. Our optimality criterion is based on the framework of generalized pairwise comparisons. We propose two approaches for estimating optimal ITRs on a pairwise basis. The first approach is a variant of the k-nearest neighbors algorithm. The second approach is a meta-learning method based on a randomized bagging scheme, which enables the use of any classification algorithm to construct an ITR. We investigate the theoretical properties of these estimation procedures, evaluate their performance through Monte Carlo simulations, and demonstrate their application to clinical trial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22580v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Petit, G\'erard Biau, Rapha\"el Porcher</dc:creator>
    </item>
    <item>
      <title>A Bayesian Multisource Fusion Model for Spatiotemporal PM2.5 in an Urban Setting</title>
      <link>https://arxiv.org/abs/2506.10688</link>
      <description>arXiv:2506.10688v2 Announce Type: replace-cross 
Abstract: Airborne particulate matter (PM2.5) is a major public health concern in urban environments, where population density and emission sources exacerbate exposure risks. We present a novel Bayesian spatiotemporal fusion model to estimate monthly PM2.5 concentrations over Greater London (2014-2019) at 1km resolution. The model integrates multiple PM2.5 data sources, including outputs from two atmospheric air quality dispersion models and predictive variables, such as vegetation and satellite aerosol optical depth, while explicitly modelling a latent spatiotemporal field. Spatial misalignment of the data is addressed through an upscaling approach to predict across the entire area. Building on stochastic partial differential equations (SPDE) within the integrated nested Laplace approximations (INLA) framework, our method introduces spatially- and temporally-varying coefficients to flexibly calibrate datasets and capture fine-scale variability. Model performance and complexity are balanced using predictive metrics such as the predictive model choice criterion and thorough cross-validation. The best performing model shows excellent fit and solid predictive performance, enabling reliable high-resolution spatiotemporal mapping of PM2.5 concentrations with the associated uncertainty. Furthermore, the model outputs, including full posterior predictive distributions, can be used to map exceedance probabilities of regulatory thresholds, supporting air quality management and targeted interventions in vulnerable urban areas, as well as providing refined exposure estimates of PM2.5 for epidemiological applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10688v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abi I. Riley, Marta Blangiardo, Fr\'ed\'eric B. Piel, Andrew Beddows, Sean Beevers, Gary W. Fuller, Paul Agnew, Monica Pirani</dc:creator>
    </item>
  </channel>
</rss>
