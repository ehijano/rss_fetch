<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 12 Mar 2025 04:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Janus Face of Innovation: Global Disparities and Divergent Options</title>
      <link>https://arxiv.org/abs/2503.07676</link>
      <description>arXiv:2503.07676v1 Announce Type: new 
Abstract: This article examines how unequal access to AI innovation creates systemic challenges for developing countries. Differential access to AI innovation results from the acute competition between domestic and global actors. While developing nations contribute significantly to AI development through data annotation labor, they face limited access to advanced AI technologies and are increasingly caught between divergent regulatory approaches from democratic and authoritarian tendencies. This brief paper analyzes how more affordable AI engagement and Western countries' development cooperation present developing nations with a complex choice between accessibility and governance standards. I argue this challenge entails new institutional mechanisms for technology transfer and regulatory cooperation, while carefully balancing universal standards with local needs. In turn, good practices could help developing countries close the deepening gap of global technological divides, while ensuring responsible AI development in developing countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07676v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>Transatlantic Policy Quarterly, 23(3), 50-60 (2025)</arxiv:journal_reference>
      <dc:creator>Nihat Mugurtay</dc:creator>
    </item>
    <item>
      <title>Ways of Seeing, and Selling, AI Art</title>
      <link>https://arxiv.org/abs/2503.07685</link>
      <description>arXiv:2503.07685v1 Announce Type: new 
Abstract: In early 2025, Augmented Intelligence - Christie's first AI art auction - drew criticism for showcasing a controversial genre. Amid wider legal uncertainty, artists voiced concerns over data mining practices, notably with respect to copyright. The backlash could be viewed as a microcosm of AI's contested position in the creative economy. Touching on the auction's presentation, reception, and results, this paper explores how, among social dissonance, machine learning finds its place in the artworld. Foregrounding responsible innovation, the paper provides a balanced perspective that champions creators' rights and brings nuance to this polarised debate. With a focus on exhibition design, it centres framing, which refers to the way a piece is presented to influence consumer perception. Context plays a central role in shaping our understanding of how good, valuable, and even ethical an artwork is. In this regard, Augmented Intelligence situates AI art within a surprisingly traditional framework, leveraging hallmarks of "high art" to establish the genre's cultural credibility. Generative AI has a clear economic dimension, converging questions of artistic merit with those of monetary worth. Scholarship on ways of seeing, or framing, could substantively inform the interpretation and evaluation of creative outputs, including assessments of their aesthetic and commercial value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07685v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Imke van Heerden</dc:creator>
    </item>
    <item>
      <title>Artificial Intelligence in Deliberation: The AI Penalty and the Emergence of a New Deliberative Divide</title>
      <link>https://arxiv.org/abs/2503.07690</link>
      <description>arXiv:2503.07690v1 Announce Type: new 
Abstract: Digital deliberation has expanded democratic participation, yet challenges remain. This includes processing information at scale, moderating discussions, fact-checking, or attracting people to participate. Recent advances in artificial intelligence (AI) offer potential solutions, but public perceptions of AI's role in deliberation remain underexplored. Beyond efficiency, democratic deliberation is about voice and recognition. If AI is integrated into deliberation, public trust, acceptance, and willingness to participate may be affected. We conducted a preregistered survey experiment with a representative sample in Germany (n=1850) to examine how information about AI-enabled deliberation influences willingness to participate and perceptions of deliberative quality. Respondents were randomly assigned to treatments that provided them information about deliberative tasks facilitated by either AI or humans. Our findings reveal a significant AI-penalty. Participants were less willing to engage in AI-facilitated deliberation and rated its quality lower than human-led formats. These effects were moderated by individual predispositions. Perceptions of AI's societal benefits and anthropomorphization of AI showed positive interaction effects on people's interest to participate in AI-enabled deliberative formats and positive quality assessments, while AI risk assessments showed negative interactions with information about AI-enabled deliberation. These results suggest AI-enabled deliberation faces substantial public skepticism, potentially even introducing a new deliberative divide. Unlike traditional participation gaps based on education or demographics, this divide is shaped by attitudes toward AI. As democratic engagement increasingly moves online, ensuring AI's role in deliberation does not discourage participation or deepen inequalities will be a key challenge for future research and policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07690v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andreas Jungherr, Adrian Rauchfleisch</dc:creator>
    </item>
    <item>
      <title>How Can Video Generative AI Transform K-12 Education? Examining Teachers' Perspectives through TPACK and TAM</title>
      <link>https://arxiv.org/abs/2503.08003</link>
      <description>arXiv:2503.08003v1 Announce Type: new 
Abstract: The rapid advancement of generative AI technology, particularly video generative AI (Video GenAI), has opened new possibilities for K-12 education by enabling the creation of dynamic, customized, and high-quality visual content. Despite its potential, there is limited research on how this emerging technology can be effectively integrated into educational practices. This study explores the perspectives of leading K-12 teachers on the educational applications of Video GenAI, using the TPACK (Technological Pedagogical Content Knowledge) and TAM (Technology Acceptance Model) frameworks as analytical lenses. Through interviews and hands-on experimentation with video generation tools, the research identifies opportunities for enhancing teaching strategies, fostering student engagement, and supporting authentic task design. It also highlights challenges such as technical limitations, ethical considerations, and the need for institutional support. The findings provide actionable insights into how Video GenAI can transform teaching and learning, offering practical implications for policy, teacher training, and the future development of educational technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08003v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Unggi Lee, Yeil Jeong, Seungha Kim, Yoorim Son, Gyuri Byun, Hyeoncheol Kim, Cheolil Lim</dc:creator>
    </item>
    <item>
      <title>Integrating Captive Portal Technology into Computer Science Education: A Modular, Hands-On Approach to Infrastructure</title>
      <link>https://arxiv.org/abs/2503.08426</link>
      <description>arXiv:2503.08426v1 Announce Type: new 
Abstract: In this paper, we present an educational project aimed to introduce students to the technology behind Captive Portals infrastructures. For doing this, we developed a series of modules to emphasize each of the different aspects and features of this technology. The project is based on an open source implementation which is widely used in many computer network courses, making it well-suited and very appealing for instructors and practitioners in this field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08426v1</guid>
      <category>cs.CY</category>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.22369/issn.2153-4136/16/1/8</arxiv:DOI>
      <arxiv:journal_reference>Journal of Computational Science Education, Volume 16, Issue 1 (March 2025), pp.35-42</arxiv:journal_reference>
      <dc:creator>Lianting Wang, Marcelo Ponce</dc:creator>
    </item>
    <item>
      <title>BoundarEase: Fostering Constructive Community Engagement to Inform More Equitable Student Assignment Policies</title>
      <link>https://arxiv.org/abs/2503.08543</link>
      <description>arXiv:2503.08543v1 Announce Type: new 
Abstract: School districts across the United States (US) play a pivotal role in shaping access to quality education through their student assignment policies -- most prominently, school attendance boundaries. Community engagement processes for changing such policies, however, are often opaque, cumbersome, and highly polarizing -- hampering equitable access to quality schools in ways that can perpetuate disparities in future life outcomes. In this paper, we describe a collaboration with a large US public school district serving nearly 150,000 students to design and evaluate a new sociotechnical system, "BoundarEase", for fostering more constructive community engagement around changing school attendance boundaries. Through a formative study with 16 community members, we first identify several frictions in existing community engagement processes, like individualistic over collective thinking; a failure to understand and empathize with the different ways policies might impact other community members; and challenges in understanding the impacts of boundary changes. These frictions inspire the design and development of BoundarEase, a web platform that allows community members to explore and offer feedback on potential boundaries. A user study with 12 community members reveals that BoundarEase prompts reflection among community members on how policies might impact families beyond their own, and increases transparency around the details of policy proposals. Our paper offers education researchers insights into the challenges and opportunities involved in community engagement for designing student assignment policies; human-computer interaction researchers a case study of how new sociotechnical systems might help mitigate polarization in local policymaking; and school districts a practical tool they might use to facilitate community engagement to foster more equitable student assignment policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08543v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cassandra Overney, Cassandra Moe, Alvin Chang, Nabeel Gillani</dc:creator>
    </item>
    <item>
      <title>Exploring Socio-Cultural Challenges and Opportunities in Designing Mental Health Chatbots for Adolescents in India</title>
      <link>https://arxiv.org/abs/2503.08562</link>
      <description>arXiv:2503.08562v1 Announce Type: new 
Abstract: Mental health challenges among Indian adolescents are shaped by unique cultural and systemic barriers, including high social stigma and limited professional support. Through a mixed-methods study involving a survey of 278 adolescents and follow-up interviews with 12 participants, we explore how adolescents perceive mental health challenges and interact with digital tools. Quantitative results highlight low self-stigma but significant social stigma, a preference for text over voice interactions, and low utilization of mental health apps but high smartphone access. Our qualitative findings reveal that while adolescents value privacy, emotional support, and localized content in mental health tools, existing chatbots lack personalization and cultural relevance. These findings inform recommendations for culturally sensitive chatbot design that prioritizes anonymity, tailored support, and localized resources to better meet the needs of adolescents in India. This work advances culturally sensitive chatbot design by centering underrepresented populations, addressing critical gaps in accessibility and support for adolescents in India.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08562v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706599.3720137</arxiv:DOI>
      <arxiv:journal_reference>Extended Abstracts of the CHI Conference on Human Factors in Computing Systems 2025</arxiv:journal_reference>
      <dc:creator>Neil K. R. Sehgal, Hita Kambhamettu, Sai Preethi Matam, Lyle Ungar, Sharath Chandra Guntuku</dc:creator>
    </item>
    <item>
      <title>When Discourse Stalls: Moving Past Five Semantic Stopsigns about Generative AI in Design Research</title>
      <link>https://arxiv.org/abs/2503.08565</link>
      <description>arXiv:2503.08565v1 Announce Type: new 
Abstract: This essay examines how Generative AI (GenAI) is rapidly transforming design practices and how discourse often falls into over-simplified narratives that impede meaningful research and practical progress. We identify and deconstruct five prevalent "semantic stopsigns" -- reductive framings about GenAI in design that halt deeper inquiry and limit productive engagement. Reflecting upon two expert workshops at ACM conferences and semi-structured interviews with design practitioners, we analyze how these stopsigns manifest in research and practice. Our analysis develops mid-level knowledge that bridges theoretical discourse and practical implementation, helping designers and researchers interrogate common assumptions about GenAI in their own contexts. By recasting these stopsigns into more nuanced frameworks, we provide the design research community with practical approaches for thinking about and working with these emerging technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08565v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Willem van der Maden, Vera van der Burg, Brett A. Halperin, Petra J\"a\"askel\"ainen, Joseph Lindley, Derek Lomas, Timothy Merritt</dc:creator>
    </item>
    <item>
      <title>Randomness, Not Representation: The Unreliability of Evaluating Cultural Alignment in LLMs</title>
      <link>https://arxiv.org/abs/2503.08688</link>
      <description>arXiv:2503.08688v1 Announce Type: new 
Abstract: Research on the 'cultural alignment' of Large Language Models (LLMs) has emerged in response to growing interest in understanding representation across diverse stakeholders. Current approaches to evaluating cultural alignment borrow social science methodologies but often overlook systematic robustness checks. Here, we identify and test three assumptions behind current evaluation methods: (1) Stability: that cultural alignment is a property of LLMs rather than an artifact of evaluation design, (2) Extrapolability: that alignment with one culture on a narrow set of issues predicts alignment with that culture on others, and (3) Steerability: that LLMs can be reliably prompted to represent specific cultural perspectives. Through experiments examining both explicit and implicit preferences of leading LLMs, we find a high level of instability across presentation formats, incoherence between evaluated versus held-out cultural dimensions, and erratic behavior under prompt steering. We show that these inconsistencies can cause the results of an evaluation to be very sensitive to minor variations in methodology. Finally, we demonstrate in a case study on evaluation design that narrow experiments and a selective assessment of evidence can be used to paint an incomplete picture of LLMs' cultural alignment properties. Overall, these results highlight significant limitations of current approaches for evaluating the cultural alignment of LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08688v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ariba Khan, Stephen Casper, Dylan Hadfield-Menell</dc:creator>
    </item>
    <item>
      <title>Psychological Counseling Ability of Large Language Models</title>
      <link>https://arxiv.org/abs/2503.07627</link>
      <description>arXiv:2503.07627v1 Announce Type: cross 
Abstract: With the development of science and the continuous progress of artificial intelligence technology, Large Language Models (LLMs) have begun to be widely utilized across various fields. However, in the field of psychological counseling, the ability of LLMs have not been systematically assessed. In this study, we assessed the psychological counseling ability of mainstream LLMs using 1096 psychological counseling skill questions which were selected from the Chinese National Counselor Level 3 Examination, including Knowledge-based, Analytical-based, and Application-based question types. The analysis showed that the correctness rates of the LLMs for Chinese questions, in descending order, were GLM-3 (46.5%), GPT-4 (46.1%), Gemini (45.0%), ERNIE-3.5 (45.7%) and GPT-3.5 (32.9%). The correctness rates of the LLMs for English questions, in descending order, were ERNIE-3.5 (43.9%), GPT-4 (40.6%), Gemini (36.6%), GLM-3 (29.9%) and GPT-3.5 (29.5%). A chi-square test indicated significant differences in the LLMs' performance on Chinese and English questions. Furthermore, we subsequently utilized the Counselor's Guidebook (Level 3) as a reference for ERNIE-3.5, resulting in a new correctness rate of 59.6%, a 13.8% improvement over its initial rate of 45.8%. In conclusion, the study assessed the psychological counseling ability of LLMs for the first time, which may provide insights for future enhancement and improvement of psychological counseling ability of LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07627v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fangyu Peng, Jingxin Nie</dc:creator>
    </item>
    <item>
      <title>Research on Superalignment Should Advance Now with Parallel Optimization of Competence and Conformity</title>
      <link>https://arxiv.org/abs/2503.07660</link>
      <description>arXiv:2503.07660v1 Announce Type: cross 
Abstract: The recent leap in AI capabilities, driven by big generative models, has sparked the possibility of achieving Artificial General Intelligence (AGI) and further triggered discussions on Artificial Superintelligence (ASI), a system surpassing all humans across all domains. This gives rise to the critical research question of: If we realize ASI, how do we align it with human values, ensuring it benefits rather than harms human society, a.k.a., the Superalignment problem. Despite ASI being regarded by many as solely a hypothetical concept, in this paper, we argue that superalignment is achievable and research on it should advance immediately, through simultaneous and alternating optimization of task competence and value conformity. We posit that superalignment is not merely a safeguard for ASI but also necessary for its realization. To support this position, we first provide a formal definition of superalignment rooted in the gap between capability and capacity and elaborate on our argument. Then we review existing paradigms, explore their interconnections and limitations, and illustrate a potential path to superalignment centered on two fundamental principles. We hope this work sheds light on a practical approach for developing the value-aligned next-generation AI, garnering greater benefits and reducing potential harms for humanity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07660v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>HyunJin Kim, Xiaoyuan Yi, Jing Yao, Muhua Huang, JinYeong Bak, James Evans, Xing Xie</dc:creator>
    </item>
    <item>
      <title>The potential role of AI agents in transforming nuclear medicine research and cancer management in India</title>
      <link>https://arxiv.org/abs/2503.07673</link>
      <description>arXiv:2503.07673v1 Announce Type: cross 
Abstract: India faces a significant cancer burden, with an incidence-to-mortality ratio indicating that nearly three out of five individuals diagnosed with cancer succumb to the disease. While the limitations of physical healthcare infrastructure are widely acknowledged as a primary challenge, concerted efforts by government and healthcare agencies are underway to mitigate these constraints. However, given the country's vast geography and high population density, it is imperative to explore alternative soft infrastructure solutions to complement existing frameworks. Artificial Intelligence agents are increasingly transforming problem-solving approaches across various domains, with their application in medicine proving particularly transformative. In this perspective, we examine the potential role of AI agents in advancing nuclear medicine for cancer research, diagnosis, and management in India. We begin with a brief overview of AI agents and their capabilities, followed by a proposed agent-based ecosystem that can address prevailing sustainability challenges in India nuclear medicine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07673v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rajat Vashistha, Arif Gulzar, Parveen Kundu, Punit Sharma, Mark Brunstein, Viktor Vegh</dc:creator>
    </item>
    <item>
      <title>Sublinear Algorithms for Wasserstein and Total Variation Distances: Applications to Fairness and Privacy Auditing</title>
      <link>https://arxiv.org/abs/2503.07775</link>
      <description>arXiv:2503.07775v1 Announce Type: cross 
Abstract: Resource-efficiently computing representations of probability distributions and the distances between them while only having access to the samples is a fundamental and useful problem across mathematical sciences. In this paper, we propose a generic algorithmic framework to estimate the PDF and CDF of any sub-Gaussian distribution while the samples from them arrive in a stream. We compute mergeable summaries of distributions from the stream of samples that require sublinear space w.r.t. the number of observed samples. This allows us to estimate Wasserstein and Total Variation (TV) distances between any two sub-Gaussian distributions while samples arrive in streams and from multiple sources (e.g. federated learning). Our algorithms significantly improves on the existing methods for distance estimation incurring super-linear time and linear space complexities. In addition, we use the proposed estimators of Wasserstein and TV distances to audit the fairness and privacy of the ML algorithms. We empirically demonstrate the efficiency of the algorithms for estimating these distances and auditing using both synthetic and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07775v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.DS</category>
      <category>stat.CO</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Debabrota Basu, Debarshi Chanda</dc:creator>
    </item>
    <item>
      <title>Towards Large Language Models that Benefit for All: Benchmarking Group Fairness in Reward Models</title>
      <link>https://arxiv.org/abs/2503.07806</link>
      <description>arXiv:2503.07806v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) become increasingly powerful and accessible to human users, ensuring fairness across diverse demographic groups, i.e., group fairness, is a critical ethical concern. However, current fairness and bias research in LLMs is limited in two aspects. First, compared to traditional group fairness in machine learning classification, it requires that the non-sensitive attributes, in this case, the prompt questions, be the same across different groups. In many practical scenarios, different groups, however, may prefer different prompt questions and this requirement becomes impractical. Second, it evaluates group fairness only for the LLM's final output without identifying the source of possible bias. Namely, the bias in LLM's output can result from both the pretraining and the finetuning. For finetuning, the bias can result from both the RLHF procedure and the learned reward model. Arguably, evaluating the group fairness of each component in the LLM pipeline could help develop better methods to mitigate the possible bias. Recognizing those two limitations, this work benchmarks the group fairness of learned reward models. By using expert-written text from arXiv, we are able to benchmark the group fairness of reward models without requiring the same prompt questions across different demographic groups. Surprisingly, our results demonstrate that all the evaluated reward models (e.g., Nemotron-4-340B-Reward, ArmoRM-Llama3-8B-v0.1, and GRM-llama3-8B-sftreg) exhibit statistically significant group unfairness. We also observed that top-performing reward models (w.r.t. canonical performance metrics) tend to demonstrate better group fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07806v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kefan Song, Jin Yao, Runnan Jiang, Rohan Chandra, Shangtong Zhang</dc:creator>
    </item>
    <item>
      <title>Entangled responsibility: an analysis of citizen science communication and scientific citizenship</title>
      <link>https://arxiv.org/abs/2503.07840</link>
      <description>arXiv:2503.07840v1 Announce Type: cross 
Abstract: The notion of citizen science is often referred to as the means of engaging public members in scientific research activities that can advance the reach and impact of technoscience. Despite this, few studies have addressed how human-machine collaborations in a citizen science context enable and constrain scientific citizenship and citizens' epistemic agencies and reconfigure science-citizen relations, including the process of citizens' engagement in scientific knowledge production. The following will address this gap by analysing the human and nonhuman material and discursive engagements in the citizen science project The Sound of Denmark. Doing so contributes to new knowledge on designing more responsible forms of citizen science engagement that advance civic agencies. Key findings emphasise that citizen science development can benefit from diverse fields such as participatory design research and feminist technoscience. Finally, the paper contributes to a broader debate on the formation of epistemic subjects, scientific citizenship, and responsible designing and evaluation of citizen science.
  Keywords: scientific citizenship, citizen science communication, epistemic agency, co-design, material-discursive practices, response-ability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07840v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Niels J. Gommesen</dc:creator>
    </item>
    <item>
      <title>Counterfactual Explanations for Model Ensembles Using Entropic Risk Measures</title>
      <link>https://arxiv.org/abs/2503.07934</link>
      <description>arXiv:2503.07934v1 Announce Type: cross 
Abstract: Counterfactual explanations indicate the smallest change in input that can translate to a different outcome for a machine learning model. Counterfactuals have generated immense interest in high-stakes applications such as finance, education, hiring, etc. In several use-cases, the decision-making process often relies on an ensemble of models rather than just one. Despite significant research on counterfactuals for one model, the problem of generating a single counterfactual explanation for an ensemble of models has received limited interest. Each individual model might lead to a different counterfactual, whereas trying to find a counterfactual accepted by all models might significantly increase cost (effort). We propose a novel strategy to find the counterfactual for an ensemble of models using the perspective of entropic risk measure. Entropic risk is a convex risk measure that satisfies several desirable properties. We incorporate our proposed risk measure into a novel constrained optimization to generate counterfactuals for ensembles that stay valid for several models. The main significance of our measure is that it provides a knob that allows for the generation of counterfactuals that stay valid under an adjustable fraction of the models. We also show that a limiting case of our entropic-risk-based strategy yields a counterfactual valid for all models in the ensemble (worst-case min-max approach). We study the trade-off between the cost (effort) for the counterfactual and its validity for an ensemble by varying degrees of risk aversion, as determined by our risk parameter knob. We validate our performance on real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07934v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erfaun Noorani, Pasan Dissanayake, Faisal Hamman, Sanghamitra Dutta</dc:creator>
    </item>
    <item>
      <title>Predicting and Understanding College Student Mental Health with Interpretable Machine Learning</title>
      <link>https://arxiv.org/abs/2503.08002</link>
      <description>arXiv:2503.08002v1 Announce Type: cross 
Abstract: Mental health issues among college students have reached critical levels, significantly impacting academic performance and overall wellbeing. Predicting and understanding mental health status among college students is challenging due to three main factors: the necessity for large-scale longitudinal datasets, the prevalence of black-box machine learning models lacking transparency, and the tendency of existing approaches to provide aggregated insights at the population level rather than individualized understanding.
  To tackle these challenges, this paper presents I-HOPE, the first Interpretable Hierarchical mOdel for Personalized mEntal health prediction. I-HOPE is a two-stage hierarchical model, validated on the College Experience Study, the longest longitudinal mobile sensing dataset. This dataset spans five years and captures data from both pre-pandemic periods and the COVID-19 pandemic. I-HOPE connects raw behavioral features to mental health status through five defined behavioral categories as interaction labels. This approach achieves a prediction accuracy of 91%, significantly surpassing the 60-70% accuracy of baseline methods. In addition, our model distills complex patterns into interpretable and individualized insights, enabling the future development of tailored interventions and improving mental health support. The code is available at https://github.com/roycmeghna/I-HOPE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08002v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3721201.3721372</arxiv:DOI>
      <dc:creator>Meghna Roy Chowdhury, Wei Xuan, Shreyas Sen, Yixue Zhao, Yi Ding</dc:creator>
    </item>
    <item>
      <title>Fact-checking with Generative AI: A Systematic Cross-Topic Examination of LLMs Capacity to Detect Veracity of Political Information</title>
      <link>https://arxiv.org/abs/2503.08404</link>
      <description>arXiv:2503.08404v1 Announce Type: cross 
Abstract: The purpose of this study is to assess how large language models (LLMs) can be used for fact-checking and contribute to the broader debate on the use of automated means for veracity identification. To achieve this purpose, we use AI auditing methodology that systematically evaluates performance of five LLMs (ChatGPT 4, Llama 3 (70B), Llama 3.1 (405B), Claude 3.5 Sonnet, and Google Gemini) using prompts regarding a large set of statements fact-checked by professional journalists (16,513). Specifically, we use topic modeling and regression analysis to investigate which factors (e.g. topic of the prompt or the LLM type) affect evaluations of true, false, and mixed statements. Our findings reveal that while ChatGPT 4 and Google Gemini achieved higher accuracy than other models, overall performance across models remains modest. Notably, the results indicate that models are better at identifying false statements, especially on sensitive topics such as COVID-19, American political controversies, and social issues, suggesting possible guardrails that may enhance accuracy on these topics. The major implication of our findings is that there are significant challenges for using LLMs for factchecking, including significant variation in performance across different LLMs and unequal quality of outputs for specific topics which can be attributed to deficits of training data. Our research highlights the potential and limitations of LLMs in political fact-checking, suggesting potential avenues for further improvements in guardrails as well as fine-tuning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08404v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Elizaveta Kuznetsova, Ilaria Vitulano, Mykola Makhortykh, Martha Stolze, Tomas Nagy, Victoria Vziatysheva</dc:creator>
    </item>
    <item>
      <title>Status and Future Prospects of the Standardization Framework Industry 4.0: A European Perspective</title>
      <link>https://arxiv.org/abs/2503.08460</link>
      <description>arXiv:2503.08460v1 Announce Type: cross 
Abstract: The rapid development of Industry 4.0 technologies requires robust and comprehensive standardization to ensure interoperability, safety and efficiency in the Industry of the Future. This paper examines the fundamental role and functionality of standardization, with a particular focus on its importance in Europe's regulatory framework. Based on this, selected topics in context of standardization activities in context intelligent manufacturing and digital twins are highlighted and, by that, an overview of the Industry 4.0 standards framework is provided. This paper serves both as an informative guide to the existing standards in Industry 4.0 with respect to Artificial Intelligence and Digital Twins, and as a call to action for increased cooperation between standardization bodies and the research community. By fostering such collaboration, we aim to facilitate the continued development and implementation of standards that will drive innovation and progress in the manufacturing sector.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08460v1</guid>
      <category>cs.ET</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olga Meyer, Marvin Boell, Christoph Legat</dc:creator>
    </item>
    <item>
      <title>BiasEdit: Debiasing Stereotyped Language Models via Model Editing</title>
      <link>https://arxiv.org/abs/2503.08588</link>
      <description>arXiv:2503.08588v1 Announce Type: cross 
Abstract: Previous studies have established that language models manifest stereotyped biases. Existing debiasing strategies, such as retraining a model with counterfactual data, representation projection, and prompting often fail to efficiently eliminate bias or directly alter the models' biased internal representations. To address these issues, we propose BiasEdit, an efficient model editing method to remove stereotypical bias from language models through lightweight networks that act as editors to generate parameter updates. BiasEdit employs a debiasing loss guiding editor networks to conduct local edits on partial parameters of a language model for debiasing while preserving the language modeling abilities during editing through a retention loss. Experiments on StereoSet and Crows-Pairs demonstrate the effectiveness, efficiency, and robustness of BiasEdit in eliminating bias compared to tangental debiasing baselines and little to no impact on the language models' general capabilities. In addition, we conduct bias tracing to probe bias in various modules and explore bias editing impacts on different components of language models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08588v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Xu, Wei Xu, Ningyu Zhang, Julian McAuley</dc:creator>
    </item>
    <item>
      <title>Generating Robot Constitutions &amp; Benchmarks for Semantic Safety</title>
      <link>https://arxiv.org/abs/2503.08663</link>
      <description>arXiv:2503.08663v1 Announce Type: cross 
Abstract: Until recently, robotics safety research was predominantly about collision avoidance and hazard reduction in the immediate vicinity of a robot. Since the advent of large vision and language models (VLMs), robots are now also capable of higher-level semantic scene understanding and natural language interactions with humans. Despite their known vulnerabilities (e.g. hallucinations or jail-breaking), VLMs are being handed control of robots capable of physical contact with the real world. This can lead to dangerous behaviors, making semantic safety for robots a matter of immediate concern. Our contributions in this paper are two fold: first, to address these emerging risks, we release the ASIMOV Benchmark, a large-scale and comprehensive collection of datasets for evaluating and improving semantic safety of foundation models serving as robot brains. Our data generation recipe is highly scalable: by leveraging text and image generation techniques, we generate undesirable situations from real-world visual scenes and human injury reports from hospitals. Secondly, we develop a framework to automatically generate robot constitutions from real-world data to steer a robot's behavior using Constitutional AI mechanisms. We propose a novel auto-amending process that is able to introduce nuances in written rules of behavior; this can lead to increased alignment with human preferences on behavior desirability and safety. We explore trade-offs between generality and specificity across a diverse set of constitutions of different lengths, and demonstrate that a robot is able to effectively reject unconstitutional actions. We measure a top alignment rate of 84.3% on the ASIMOV Benchmark using generated constitutions, outperforming no-constitution baselines and human-written constitutions. Data is available at asimov-benchmark.github.io</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08663v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pierre Sermanet, Anirudha Majumdar, Alex Irpan, Dmitry Kalashnikov, Vikas Sindhwani</dc:creator>
    </item>
    <item>
      <title>Investigating Gender Euphoria and Dysphoria on TikTok: Characterization and Comparison</title>
      <link>https://arxiv.org/abs/2305.19552</link>
      <description>arXiv:2305.19552v2 Announce Type: replace 
Abstract: With the emergence of short video-sharing platforms, engagement with social media sites devoted to opinion and knowledge dissemination has rapidly increased. Among these platforms, TikTok is one of the most popular globally and has become the platform of choice for transgender and nonbinary individuals, who have formed a large community to mobilize personal experience and exchange information. The knowledge produced in online spaces can influence the ways in which people understand and experience their own gender and transitions, as they hear about others and weigh experiential and medical knowledge against their own. This paper extends current research and past interview methods on gender euphoria and gender dysphoria to analyze what and how online communities on TikTok discuss these two types of gender experiences. Our findings indicate that gender euphoria and gender dysphoria are differently described in online TikTok spaces. These findings indicate similarities in the words used to describe gender dysphoria as well as gender euphoria in both the comments of videos and content creators' hashtags. Finally, our results show that gender euphoria is described in more similar terms between transfeminine and transmasculine experiences than gender dysphoria, which appears to be more differentiated by gendering experience and transition goals. We hope this paper can provide insights for future research on understanding transgender and nonbinary individuals in online communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.19552v2</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>SJ Dillon, Yueqing Liang, H. Russell Bernard, Kai Shu</dc:creator>
    </item>
    <item>
      <title>The Files are in the Computer: Copyright, Memorization, and Generative AI</title>
      <link>https://arxiv.org/abs/2404.12590</link>
      <description>arXiv:2404.12590v5 Announce Type: replace 
Abstract: The New York Times's copyright lawsuit against OpenAI and Microsoft alleges OpenAI's GPT models have "memorized" NYT articles. Other lawsuits make similar claims. But parties, courts, and scholars disagree on what memorization is, whether it is taking place, and what its copyright implications are. These debates are clouded by ambiguities over the nature of "memorization." We attempt to bring clarity to the conversation. We draw on the technical literature to provide a firm foundation for legal discussions, providing a precise definition of memorization: a model has "memorized" a piece of training data when (1) it is possible to reconstruct from the model (2) a near-exact copy of (3) a substantial portion of (4) that piece of training data. We distinguish memorization from "extraction" (user intentionally causes a model to generate a near-exact copy), from "regurgitation" (model generates a near-exact copy, regardless of user intentions), and from "reconstruction" (the near-exact copy can be obtained from the model by any means). Several consequences follow. (1) Not all learning is memorization. (2) Memorization occurs when a model is trained; regurgitation is a symptom not its cause. (3) A model that has memorized training data is a "copy" of that training data in the sense used by copyright. (4) A model is not like a VCR or other general-purpose copying technology; it is better at generating some types of outputs (possibly regurgitated ones) than others. (5) Memorization is not a phenomenon caused by "adversarial" users bent on extraction; it is latent in the model itself. (6) The amount of training data that a model memorizes is a consequence of choices made in training. (7) Whether or not a model that has memorized actually regurgitates depends on overall system design. In a very real sense, memorized training data is in the model--to quote Zoolander, the files are in the computer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12590v5</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. Feder Cooper, James Grimmelmann</dc:creator>
    </item>
    <item>
      <title>Learning and teaching biological data science in the Bioconductor community</title>
      <link>https://arxiv.org/abs/2410.01351</link>
      <description>arXiv:2410.01351v2 Announce Type: replace 
Abstract: Modern biological research is increasingly data-intensive, leading to a growing demand for effective training in biological data science. In this article, we provide an overview of key resources and best practices available within the Bioconductor project - an open-source software community focused on omics data analysis. This guide serves as a valuable reference for both learners and educators in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01351v2</guid>
      <category>cs.CY</category>
      <category>q-bio.OT</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jenny Drnevich, Frederick J. Tan, Fabricio Almeida-Silva, Robert Castelo, Aedin C. Culhane, Sean Davis, Maria A. Doyle, Ludwig Geistlinger, Andrew R. Ghazi, Susan Holmes, Leo Lahti, Alexandru Mahmoud, Kozo Nishida, Marcel Ramos, Kevin Rue-Albrecht, David J. H. Shih, Laurent Gatto, Charlotte Soneson</dc:creator>
    </item>
    <item>
      <title>The Lazy Student's Dream: ChatGPT Passing an Engineering Course on Its Own</title>
      <link>https://arxiv.org/abs/2503.05760</link>
      <description>arXiv:2503.05760v2 Announce Type: replace 
Abstract: This paper presents a comprehensive investigation into the capability of Large Language Models (LLMs) to successfully complete a semester-long undergraduate control systems course. Through evaluation of 115 course deliverables, we assess LLM performance using ChatGPT under a "minimal effort" protocol that simulates realistic student usage patterns. The investigation employs a rigorous testing methodology across multiple assessment formats, from auto-graded multiple choice questions to complex Python programming tasks and long-form analytical writing. Our analysis provides quantitative insights into AI's strengths and limitations in handling mathematical formulations, coding challenges, and theoretical concepts in control systems engineering. The LLM achieved a B-grade performance (82.24\%), approaching but not exceeding the class average (84.99\%), with strongest results in structured assignments and greatest limitations in open-ended projects. The findings inform discussions about course design adaptation in response to AI advancement, moving beyond simple prohibition towards thoughtful integration of these tools in engineering education. Additional materials including syllabus, examination papers, design projects, and example responses can be found at the project website: https://gradegpt.github.io.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05760v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gokul Puthumanaillam, Melkior Ornik</dc:creator>
    </item>
    <item>
      <title>Unlocking the Potential of AI Researchers in Scientific Discovery: What Is Missing?</title>
      <link>https://arxiv.org/abs/2503.05822</link>
      <description>arXiv:2503.05822v2 Announce Type: replace 
Abstract: The potential of AI researchers in scientific discovery remains largely to be unleashed. Over the past decade, the presence of AI for Science (AI4Science) in the 145 Nature Index journals has increased ninefold, yet nearly 90% of AI4Science research remains predominantly led by experimental scientists. Drawing on the Diffusion of Innovation theory, we project that AI4Science's share of total publications will rise from 3.57% in 2024 to approximately 25% by 2050. Unlocking the potential of AI researchers is essential for driving this shift and fostering deeper integration of AI expertise into the research ecosystem. To this end, we propose structured and actionable workflows, alongside key strategies to position AI researchers at the forefront of scientific discovery. Furthermore, we outline three pivotal pathways: equipping experimental scientists with user-friendly AI tools to amplify the impact of AI researchers, bridging cognitive and methodological gaps to enable more direct participation in scientific discovery, and proactively cultivating a thriving AI-driven scientific ecosystem. By addressing these challenges, this work aims to empower AI researchers as a driving force in shaping the future of scientific discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05822v2</guid>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hengjie Yu, Yaochu Jin</dc:creator>
    </item>
    <item>
      <title>Evaluating Tenant-Landlord Tensions Using Generative AI on Online Tenant Forums</title>
      <link>https://arxiv.org/abs/2404.11681</link>
      <description>arXiv:2404.11681v2 Announce Type: replace-cross 
Abstract: Tenant-landlord relationships exhibit a power asymmetry where landlords' power to evict the tenants at a low-cost results in their dominating status in such relationships. Tenant concerns are thus often unspoken, unresolved, or ignored and this could lead to blatant conflicts as suppressed tenant concerns accumulate. Modern machine learning methods and Large Language Models (LLM) have demonstrated immense abilities to perform language tasks. In this study, we incorporate Latent Dirichlet Allocation (LDA) with GPT-4 to classify Reddit post data scraped from the subreddit r/Tenant, aiming to unveil trends in tenant concerns while exploring the adoption of LLMs and machine learning methods in social science research. We find that tenant concerns in topics like fee dispute and utility issues are consistently dominant in all four states analyzed while each state has other common tenant concerns special to itself. Moreover, we discover temporal trends in tenant concerns that provide important implications regarding the impact of the pandemic and the Eviction Moratorium.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11681v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s42001-025-00378-8</arxiv:DOI>
      <arxiv:journal_reference>J Comput Soc Sc 8, 50 (2025)</arxiv:journal_reference>
      <dc:creator>Xin Chen, Cheng Ren, Timothy A Thomas</dc:creator>
    </item>
    <item>
      <title>To which reference class do you belong? Measuring racial fairness of reference classes with normative modeling</title>
      <link>https://arxiv.org/abs/2407.19114</link>
      <description>arXiv:2407.19114v2 Announce Type: replace-cross 
Abstract: Reference classes in healthcare establish healthy norms, such as pediatric growth charts of height and weight, and are used to chart deviations from these norms which represent potential clinical risk. How the demographics of the reference class influence clinical interpretation of deviations is unknown. Using normative modeling, a method for building reference classes, we evaluate the fairness (racial bias) in reference models of structural brain images that are widely used in psychiatry and neurology. We test whether including race in the model creates fairer models. We predict self-reported race using the deviation scores from three different reference class normative models, to better understand bias in an integrated, multivariate sense. Across all of these tasks, we uncover racial disparities that are not easily addressed with existing data or commonly used modeling techniques. Our work suggests that deviations from the norm could be due to demographic mismatch with the reference class, and assigning clinical meaning to these deviations should be done with caution. Our approach also suggests that acquiring more representative samples is an urgent research priority.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19114v2</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saige Rutherford, Thomas Wolfers, Charlotte Fraza, Nathaniel G. Harnett, Christian F. Beckmann, Henricus G. Ruhe, Andre F. Marquand</dc:creator>
    </item>
    <item>
      <title>Sustainable Visions: Unsupervised Machine Learning Insights on Global Development Goals</title>
      <link>https://arxiv.org/abs/2409.12427</link>
      <description>arXiv:2409.12427v2 Announce Type: replace-cross 
Abstract: The 2030 Agenda for Sustainable Development of the United Nations outlines 17 goals for countries of the world to address global challenges in their development. However, the progress of countries towards these goal has been slower than expected and, consequently, there is a need to investigate the reasons behind this fact. In this study, we have used a novel data-driven methodology to analyze time-series data for over 20 years (2000-2022) from 107 countries using unsupervised machine learning (ML) techniques. Our analysis reveals strong positive and negative correlations between certain SDGs (Sustainable Development Goals). Our findings show that progress toward the SDGs is heavily influenced by geographical, cultural and socioeconomic factors, with no country on track to achieve all the goals by 2030. This highlights the need for a region-specific, systemic approach to sustainable development that acknowledges the complex interdependencies between the goals and the variable capacities of countries to reach them. For this our machine learning based approach provides a robust framework for developing efficient and data-informed strategies to promote cooperative and targeted initiatives for sustainable progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12427v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Garc\'ia-Rodr\'iguez, Matias N\'u\~nez, Miguel Robles P\'erez, Tzipe Govezensky, Rafael A. Barrio, Carlos Gershenson, Kimmo K. Kaski, Julia Tag\"ue\~na</dc:creator>
    </item>
    <item>
      <title>Emotion-Aware Embedding Fusion in LLMs (Flan-T5, LLAMA 2, DeepSeek-R1, and ChatGPT 4) for Intelligent Response Generation</title>
      <link>https://arxiv.org/abs/2410.01306</link>
      <description>arXiv:2410.01306v2 Announce Type: replace-cross 
Abstract: Empathetic and coherent responses are critical in auto-mated chatbot-facilitated psychotherapy. This study addresses the challenge of enhancing the emotional and contextual understanding of large language models (LLMs) in psychiatric applications. We introduce Emotion-Aware Embedding Fusion, a novel framework integrating hierarchical fusion and attention mechanisms to prioritize semantic and emotional features in therapy transcripts. Our approach combines multiple emotion lexicons, including NRC Emotion Lexicon, VADER, WordNet, and SentiWordNet, with state-of-the-art LLMs such as Flan-T5, LLAMA 2, DeepSeek-R1, and ChatGPT 4. Therapy session transcripts, comprising over 2,000 samples are segmented into hierarchical levels (word, sentence, and session) using neural networks, while hierarchical fusion combines these features with pooling techniques to refine emotional representations. Atten-tion mechanisms, including multi-head self-attention and cross-attention, further prioritize emotional and contextual features, enabling temporal modeling of emotion-al shifts across sessions. The processed embeddings, computed using BERT, GPT-3, and RoBERTa are stored in the Facebook AI similarity search vector database, which enables efficient similarity search and clustering across dense vector spaces. Upon user queries, relevant segments are retrieved and provided as context to LLMs, enhancing their ability to generate empathetic and con-textually relevant responses. The proposed framework is evaluated across multiple practical use cases to demonstrate real-world applicability, including AI-driven therapy chatbots. The system can be integrated into existing mental health platforms to generate personalized responses based on retrieved therapy session data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01306v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdur Rasool, Muhammad Irfan Shahzad, Hafsa Aslam, Vincent Chan, Muhammad Ali Arshad</dc:creator>
    </item>
    <item>
      <title>ChatGPT-4 in the Turing Test: A Critical Analysis</title>
      <link>https://arxiv.org/abs/2503.06551</link>
      <description>arXiv:2503.06551v2 Announce Type: replace-cross 
Abstract: This paper critically examines the recent publication "ChatGPT-4 in the Turing Test" by Restrepo Echavarr\'ia (2025), challenging its central claims regarding the absence of minimally serious test implementations and the conclusion that ChatGPT-4 fails the Turing Test. The analysis reveals that the criticisms based on rigid criteria and limited experimental data are not fully justified. More importantly, the paper makes several constructive contributions that enrich our understanding of Turing Test implementations. It demonstrates that two distinct formats--the three-player and two-player tests--are both valid, each with unique methodological implications. The work distinguishes between absolute criteria (reflecting an optimal 50% identification rate in a three-player format) and relative criteria (which measure how closely a machine's performance approximates that of a human), offering a more nuanced evaluation framework. Furthermore, the paper clarifies the probabilistic underpinnings of both test types by modeling them as Bernoulli experiments--correlated in the three-player version and uncorrelated in the two-player version. This formalization allows for a rigorous separation between the theoretical criteria for passing the test, defined in probabilistic terms, and the experimental data that require robust statistical methods for proper interpretation. In doing so, the paper not only refutes key aspects of the criticized study but also lays a solid foundation for future research on objective measures of how closely an AI's behavior aligns with, or deviates from, that of a human being.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06551v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marco Giunti</dc:creator>
    </item>
  </channel>
</rss>
