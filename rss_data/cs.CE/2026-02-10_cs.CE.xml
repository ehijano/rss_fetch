<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Feb 2026 05:00:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Modeling Batch Crystallization under Uncertainty Using Physics-informed Machine Learning</title>
      <link>https://arxiv.org/abs/2602.07184</link>
      <description>arXiv:2602.07184v1 Announce Type: new 
Abstract: The development of robust and reliable modeling approaches for crystallization processes is often challenging because of non-idealities in real data arising from various sources of uncertainty. This study investigated the effectiveness of physics-informed recurrent neural networks (PIRNNs) that integrate the mechanistic population balance model with recurrent neural networks under the presence of systematic and model uncertainties. Such uncertainties are represented by using synthetic data containing controlled noise, solubility shift, and limited sampling. The research demonstrates that PIRNNs achieve strong generalization and physical consistency, maintain stable learning behavior, and accurately recover kinetic parameters despite significant stochastic variations in the training data. In the case of systematic errors in the solubility model, the inclusion of physics regularization improved the test performance by more than an order of magnitude compared to purely data-driven models, whereas excessive weighting of physics increased error arising due to the model mismatch. The results also show that PIRNNs are able to recover model parameters and replicate crystallization dynamics even in the limit of very low sampling resolution. These findings validate the robustness of physics-informed machine learning in handling data imperfections and incomplete domain knowledge, providing a potential pathway toward reliable and practical hybrid modeling of crystallization dynamics and industrial process monitoring and control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07184v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dingqi Nai, Huayu Li, Martha Grover, Andrew Medford</dc:creator>
    </item>
    <item>
      <title>Minimum Carbon Trusses: Constructible Multi-Component Designs with Mixed-Integer Linear Programming</title>
      <link>https://arxiv.org/abs/2602.07185</link>
      <description>arXiv:2602.07185v1 Announce Type: new 
Abstract: Truss optimization is a rich research field receiving renewed interest in limiting the carbon emissions of construction. However, a persistent challenge has been to construct highly optimized and often complex designs. This contribution formulates and solves new mixed-integer linear programs that enable consideration of the interplay between environmental impact and constructability. Specifically, the design engineer is enabled to design with multiple materials and/or structural components, apply separate minimum and maximum cross-sectional area bounds, and constrain the complexity of the structural connections. This is done while explicitly considering compatibility and constitutive laws. The results demonstrate that the lowest embodied carbon designs change significantly when constructability constraints are applied, highlighting the need for an integrated optimization approach. In one example, introducing a lower-carbon material option has almost no effect on the environmental performance, whereas another sees an improvement of nearly 29%. The extensibility of the formulation to design with three component types and additional constraints is demonstrated for a prestressed tensegrity example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07185v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zane Hallowell Schemmer, Josephine Voigt Carstensen</dc:creator>
    </item>
    <item>
      <title>Fin-RATE: A Real-world Financial Analytics and Tracking Evaluation Benchmark for LLMs on SEC Filings</title>
      <link>https://arxiv.org/abs/2602.07294</link>
      <description>arXiv:2602.07294v1 Announce Type: new 
Abstract: With increasing deployment of Large Language Models (LLMs) in the finance domain, LLMs are increasingly expected to parse complex regulatory disclosures. However, existing benchmarks often focus on isolated details, failing to reflect the complexity of professional analysis that requires synthesizing information across multiple documents, reporting periods, and corporate entities. They do not distinguish whether errors stem from retrieval failures, generation flaws, finance-specific reasoning mistakes, or misunderstanding of the query or context. This makes it difficult to pinpoint performance bottlenecks. To bridge these gaps, we introduce Fin-RATE, a benchmark built on U.S. Securities and Exchange Commission (SEC) filings and mirror financial analyst workflows through three pathways: detail-oriented reasoning within individual disclosures, cross-entity comparison under shared topics, and longitudinal tracking of the same firm across reporting periods. We benchmark 17 leading LLMs, spanning open-source, closed-source, and finance-specialized models, under both ground-truth context and retrieval-augmented settings. Results show substantial performance degradation, with accuracy dropping by 18.60% and 14.35% as tasks shift from single-document reasoning to longitudinal and cross-entity analysis. This is driven by rising comparison hallucinations, time and entity mismatches, and mirrored by declines in reasoning and factuality--limitations that prior benchmarks have yet to formally categorize or quantify.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07294v1</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yidong Jiang, Junrong Chen, Eftychia Makri, Jialin Chen, Peiwen Li, Ali Maatouk, Leandros Tassiulas, Eliot Brenner, Bing Xiang, Rex Ying</dc:creator>
    </item>
    <item>
      <title>Probabilistic Modeling of Venture Capital Portfolio Outliers</title>
      <link>https://arxiv.org/abs/2602.07761</link>
      <description>arXiv:2602.07761v1 Announce Type: new 
Abstract: In this paper, we define probabilistic measures for venture portfolio performance based on individual outlier probability for each investment and the dependence across investments. This work is inspired by loan portfolio modeling against default risk used in banking. In mathematical terms, we calculate the probability distribution of the sum of N non-homogeneous Boolean outcomes (investments becoming outliers) that are correlated through common factors such as overall market conditions and sector effects. Specifically, we implemented a latent-factor model in which each investment's success is the exceedance of a Gaussian latent variable composed of idiosyncratic returns and returns from interpretable shared factors (stock markets, industry sector indices, geography and founder type). The formulation follows a simulation approach to preserve heterogeneous deal-level success probabilities and uses empirically estimated correlation matrices. When applied to synthetic portfolios, our model reveals that expected outlier counts alone are insufficient statistics for evaluating venture portfolios. Portfolios with identical expected outcomes can exhibit drastically different levels of reliability and risk when various levels and forms of correlation are embedded. Diversification improves the probability of achieving a minimum number of outliers by reducing exposure to common shocks, but at the cost of lower upside, underscoring a fundamental tradeoff between reliability and magnitude of clustered successes. The framework provides a practical bridge between deal-level outlier probability assessment and objective-aware portfolio construction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07761v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kensei Sakamoto, Hasan Ugur Koyluoglu, Fuat Alican, Yigit Ihlamur</dc:creator>
    </item>
    <item>
      <title>Learning-guided Kansa collocation for forward and inverse PDEs beyond linearity</title>
      <link>https://arxiv.org/abs/2602.07970</link>
      <description>arXiv:2602.07970v1 Announce Type: new 
Abstract: Partial Differential Equations are precise in modelling the physical, biological and graphical phenomena. However, the numerical methods suffer from the curse of dimensionality, high computation costs and domain-specific discretization. We aim to explore pros and cons of different PDE solvers, and apply them to specific scientific simulation problems, including forwarding solution, inverse problems and equations discovery. In particular, we extend the recent CNF (NeurIPS 2023) framework solver to multi-dependent-variable and non-linear settings, together with down-stream applications. The outcomes include implementation of selected methods, self-tuning techniques, evaluation on benchmark problems and a comprehensive survey of neural PDE solvers and scientific simulation applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07970v1</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zheyuan Hu, Weitao Chen, Cengiz \"Oztireli, Chenliang Zhou, Fangcheng Zhong</dc:creator>
    </item>
    <item>
      <title>A Machine Learning Enabled MDO for Bio-Inspired Autonomous Underwater Gliders</title>
      <link>https://arxiv.org/abs/2602.08508</link>
      <description>arXiv:2602.08508v1 Announce Type: new 
Abstract: The preliminary design of AUGs is intrinsically challenging due to the strong coupling between the external hydrodynamic shape, the hydrostatic balance, the structural integrity, and internal packaging constraints. This complexity is further amplified for bio-inspired configurations, whose rich geometric parametrizations lead to high-dimensional design spaces that are difficult to explore using conventional optimization approaches. This work presents a ML-enabled bi-level multidisciplinary design optimization (MDO) framework for the performance-driven design of a manta-ray-inspired AUG. At the upper level, hydrodynamically efficient external geometries are explored in a reduced design space obtained through physics-driven parametric model embedding, which identifies a low-dimensional latent representation directly correlated with the lift, drag, and pressure distributions. At the lower level, a constrained internal sizing problem determines the minimum feasible empty weight by accounting for structural, hydrostatic, geometric, and payload constraints. To render the resulting bi-level problem computationally tractable, a multi-fidelity surrogate-based optimization strategy is adopted, combining low- and high-fidelity hydrodynamic models with stochastic radial basis function surrogates and adaptive Bayesian sampling. The framework enables efficient exploration of the coupled design space while rigorously managing model uncertainty and computational cost. The optimized configurations exhibit a 14.7\% improvement in maximum hydrodynamic efficiency and a 12.8\% reduction in empty weight relative to the baseline design, while satisfying all disciplinary constraints. These results demonstrate that the integration of physics-driven dimensionality reduction and multi-fidelity machine learning enables scalable and physically consistent MDO of complex bio-inspired underwater vehicles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08508v1</guid>
      <category>cs.CE</category>
      <category>math.OC</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Serani, Giorgio Palma, Jeroen Wackers, Matteo Diez</dc:creator>
    </item>
    <item>
      <title>The M-Tensor Format: Optimality in High Dimensional Regression for Nonlinear Models with Scarce Data</title>
      <link>https://arxiv.org/abs/2602.08509</link>
      <description>arXiv:2602.08509v1 Announce Type: new 
Abstract: We present a nonlinear regression framework based on tensor algebra tailored to high dimensional contexts where data is scarce. We exploit algebraic properties of a partial tensor product, namely the m-tensor product, to leverage structured equations with separated variables. The proposed method combines kernel properties along with tensor algebra to prevent the curse of dimensionality and tackle approximations up to hundreds of parameters while avoiding the fixed point strategy. This formalism allows us to provide different regularization techniques fit for low amount of data with a high number of parameters while preserving well-known matrix-based properties. We demonstrate complexity scaling on a general benchmark and dynamical systems to show robustness for engineering problems and ease of implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08509v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R\'emi Cloarec, Sebastian Rodriguez, Xavier Kestelyn, Francisco Chinesta</dc:creator>
    </item>
    <item>
      <title>AbFlow : End-to-end Paratope-Centric Antibody Design by Interaction Enhanced Flow Matching</title>
      <link>https://arxiv.org/abs/2602.07084</link>
      <description>arXiv:2602.07084v1 Announce Type: cross 
Abstract: Antigen-antibody binding is a critical process in the immune response. Although recent progress has advanced antibody design, current methods lack a generative framework for end-to-end modeling of full-atom antibody structures and struggle to fully exploit antigen-specific geometric information for optimizing local binding interfaces and global structures. To overcome these limitations, we introduce AbFlow, a flow-matching framework that leverages optimal transport to design full-atom antibodies end-to-end. AbFlow incorporates an extended velocity field network featuring an equivariant Surface Multi-channel Encoder, which uses surface-level antigen interaction data to refine the antibody structure, particularly the CDR-H3 region. Extensive experiments in paratoep-centric antibody design, multi-CDRs and full-atom antibody design, binding affinity optimization, and complex structure prediction show that AbFlow produces superior antigen-antibody complexes, especially at the contact interface, and markedly improves the binding affinity of generated antibodies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07084v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3770854.3780296</arxiv:DOI>
      <dc:creator>Wenda Wang, Yang Zhang, Zhewei Wei, Wenbing Huang</dc:creator>
    </item>
    <item>
      <title>Systematic Performance Assessment of Deep Material Networks for Multiscale Material Modeling</title>
      <link>https://arxiv.org/abs/2602.07192</link>
      <description>arXiv:2602.07192v1 Announce Type: cross 
Abstract: Deep Material Networks (DMNs) are structure-preserving, mechanistic machine learning models that embed micromechanical principles into their architectures, enabling strong extrapolation capabilities and significant potential to accelerate multiscale modeling of complex microstructures. A key advantage of these models is that they can be trained exclusively on linear elastic data and then generalized to nonlinear inelastic regimes during online prediction. Despite their growing adoption, systematic evaluations of their performance across the full offline-online pipeline remain limited. This work presents a comprehensive comparative assessment of DMNs with respect to prediction accuracy, computational efficiency, and training robustness. We investigate the effects of offline training choices, including initialization, batch size, training data size, and activation regularization on online generalization performance and uncertainty. The results demonstrate that both prediction error and variance decrease with increasing training data size, while initialization and batch size can significantly influence model performance. Moreover, activation regularization is shown to play a critical role in controlling network complexity and therefore generalization performance. Compared with the original DMN, the rotation-free Interaction-based Material Network (IMN) formulation achieves a 3.4x - 4.7x speed-up in offline training, while maintaining comparable online prediction accuracy and computational efficiency. These findings clarify key trade-offs between model expressivity and efficiency in structure-preserving material networks and provide practical guidance for their deployment in multiscale material modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07192v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.comp-ph</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaolong He, Haoyan Wei, Wei Hu, Henan Mao, C. T. Wu</dc:creator>
    </item>
    <item>
      <title>Large language models for spreading dynamics in complex systems</title>
      <link>https://arxiv.org/abs/2602.08085</link>
      <description>arXiv:2602.08085v1 Announce Type: cross 
Abstract: Spreading dynamics is a central topic in the physics of complex systems and network science, providing a unified framework for understanding how information, behaviors, and diseases propagate through interactions among system units. In many propagation contexts, spreading processes are influenced by multiple interacting factors, such as information expression patterns, cultural contexts, living environments, cognitive preferences, and public policies, which are difficult to incorporate directly into classical modeling frameworks. Recently, large language models (LLMs) have exhibited strong capabilities in natural language understanding, reasoning, and generation, enabling explicit perception of semantic content and contextual cues in spreading processes, thereby supporting the analysis of the different influencing factors. Beyond serving as external analytical tools, LLMs can also act as interactive agents embedded in propagation systems, potentially influencing spreading pathways and feedback structures. Consequently, the roles and impacts of LLMs on spreading dynamics have become an active and rapidly growing research area across multiple research disciplines. This review provides a comprehensive overview of recent advances in applying LLMs to the study of spreading dynamics across two representative domains: digital epidemics, such as misinformation and rumors, and biological epidemics, including infectious disease outbreaks. We first examine the foundations of epidemic modeling from a complex-systems perspective and discuss how LLM-based approaches relate to traditional frameworks. We then systematically review recent studies from three key perspectives, which are epidemic modeling, epidemic detection and surveillance, and epidemic prediction and management, to clarify how LLMs enhance these areas. Finally, open challenges and potential research directions are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08085v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shuyu Jiang, Hao Ren, Yichang Gao, Yi-Cheng Zhang, Li Qi, Dayong Xiao, Jie Fan, Rui Tang, Wei Wang</dc:creator>
    </item>
    <item>
      <title>A Machine Learning accelerated geophysical fluid solver</title>
      <link>https://arxiv.org/abs/2602.08670</link>
      <description>arXiv:2602.08670v1 Announce Type: cross 
Abstract: Machine learning methods have been successful in many areas, like image classification and natural language processing. However, it still needs to be determined how to apply ML to areas with mathematical constraints, like solving PDEs. Among various approaches to applying ML techniques to solving PDEs, the data-driven discretization method presents a promising way of accelerating and improving existing PDE solver on structured grids where it predicts the coefficients of quasi-linear stencils for computing values or derivatives of a function at given positions. It can improve the accuracy and stability of low-resolution simulation compared with using traditional finite difference or finite volume schemes. Meanwhile, it can also benefit from traditional numerical schemes like achieving conservation law by adapting finite volume type formulations. In this thesis, we have implemented the shallow water equation and Euler equation classic solver under a different framework. Experiments show that our classic solver performs much better than the Pyclaw solver. Then we propose four different deep neural networks for the ML-based solver. The results indicate that two of these approaches could output satisfactory solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08670v1</guid>
      <category>cs.CV</category>
      <category>cs.CE</category>
      <category>cs.PF</category>
      <category>physics.comp-ph</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yang Bai</dc:creator>
    </item>
    <item>
      <title>Towards Spatio-Temporal Extrapolation of Phase-Field Simulations with Convolution-Only Neural Networks</title>
      <link>https://arxiv.org/abs/2601.04510</link>
      <description>arXiv:2601.04510v2 Announce Type: replace 
Abstract: Phase-field simulations of liquid metal dealloying (LMD) can capture complex microstructural evolutions but can be prohibitively expensive for large domains and long time horizons. In this paper, we introduce a fully convolutional, conditionally parameterized U-Net surrogate designed to extrapolate far beyond its training data in both space and time. The architecture integrates convolutional self-attention, physically informed padding, and a flood-fill corrector method to maintain accuracy under extreme extrapolation, while conditioning on simulation parameters allows for flexible time-step skipping and adaptation to varying alloy compositions. To remove the need for costly solver-based initialization, we couple the surrogate with a conditional diffusion model that generates synthetic, physically consistent initial conditions. We train our surrogate on simulations generated over small domain sizes and short time spans, but, by taking advantage of the convolutional nature of U-Nets, we are able to run and extrapolate surrogate simulations for longer time horizons than what would be achievable with classic numerical solvers. Across multiple alloy compositions, the framework is able to reproduce the LMD physics accurately. It predicts key quantities of interest and spatial statistics with relative errors typically below 5% in the training regime and under 15% during large-scale, long time-horizon extrapolations. Our framework can also deliver speed-ups of up to 36,000 times, bringing the time to run weeks-long simulations down to a few seconds. This work is a first stepping stone towards high-fidelity extrapolation in both space and time of phase-field simulation for LMD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04510v2</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christophe Bonneville, Nathan Bieberdorf, Pieterjan Robbe, Mark Asta, Habib Najm, Laurent Capolungo, Cosmin Safta</dc:creator>
    </item>
    <item>
      <title>Tensor-based multivariate function approximation: methods benchmarking and comparison</title>
      <link>https://arxiv.org/abs/2506.04791</link>
      <description>arXiv:2506.04791v2 Announce Type: replace-cross 
Abstract: We evaluate some methods designed for tensor- (or data-) based multivariate model construction (approximation and compression). To this aim, a collection of multivariate functions and an evaluation methodology are suggested. First, these functions, with varying complexity (e.g., number and degree of the variables) and nature (e.g., rational, irrational, differentiable or not, symmetric, etc.) are used to build $n$-dimensional tensors, each of different dimension and memory size. Second, grounded on this tensor, we evaluate the performances of different methods and implementations leading to different types of surrogate models (e.g., rational functions, networks). The accuracy, the computational time, the parameter tuning impact, etc. are monitored and reported. One objective is to evaluate the different available strategies to guide users on the prospects, advantages, and limits of the various tools. The contributions are twofold: (i) to suggest a comprehensive benchmark collection together with a methodology for tensor approximation with a surrogate model and, in addition, (ii) to provide a digest and additional details of the multivariate Loewner Framework (mLF) approach [Antoulas et al., 2025], as well as detailed examples and code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04791v2</guid>
      <category>math.NA</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>cs.SE</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charles Poussot-Vassal, Ion Victor Gosea, Pierre Vuillemin, Athanasios C. Antoulas</dc:creator>
    </item>
    <item>
      <title>CostNav: A Navigation Benchmark for Real-World Economic-Cost Evaluation of Physical AI Agents</title>
      <link>https://arxiv.org/abs/2511.20216</link>
      <description>arXiv:2511.20216v3 Announce Type: replace-cross 
Abstract: While current navigation benchmarks prioritize task success in simplified settings, they neglect the multidimensional economic constraints essential for the real-world commercialization of autonomous delivery systems. We introduce CostNav, an Economic Navigation Benchmark that evaluates physical AI agents through comprehensive economic cost-revenue analysis aligned with real-world business operations. By integrating industry-standard data - such as SEC filings and AIS injury reports - with Isaac Sim's detailed collision and cargo dynamics, CostNav transcends simple task completion to accurately evaluate business value in complex, real-world scenarios. To our knowledge, CostNav is the first work to quantitatively expose the gap between navigation research metrics and commercial viability, revealing that optimizing for task success on a simplified task fundamentally differs from optimizing for real-world economic deployment. Our evaluation of rule-based Nav2 navigation shows that current approaches are not economically viable: the contribution margin is -22.81/run (AMCL) and -12.87/run (GPS), resulting in no break-even point. We challenge the community to develop navigation policies that achieve economic viability on CostNav. We remain method-agnostic, evaluating success solely on the metric of cost rather than the underlying architecture. All resources are available at https://github.com/worv-ai/CostNav.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20216v3</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haebin Seong, Sungmin Kim, Yongjun Cho, Myunchul Joe, Geunwoo Kim, Yubeen Park, Sunhoo Kim, Yoonshik Kim, Suhwan Choi, Jaeyoon Jung, Jiyong Youn, Jinmyung Kwak, Sunghee Ahn, Jaemin Lee, Younggil Do, Seungyeop Yi, Woojin Cheong, Minhyeok Oh, Minchan Kim, Yoonseok Kang, Seongjae Kang, Samwoo Seong, Youngjae Yu, Yunsung Lee</dc:creator>
    </item>
    <item>
      <title>Investigating Data Pruning for Pretraining Biological Foundation Models at Scale</title>
      <link>https://arxiv.org/abs/2512.12932</link>
      <description>arXiv:2512.12932v2 Announce Type: replace-cross 
Abstract: Biological foundation models (BioFMs), pretrained on large-scale biological sequences, have recently shown strong potential in providing meaningful representations for diverse downstream bioinformatics tasks. However, such models often rely on millions to billions of training sequences and billions of parameters, resulting in prohibitive computational costs and significant barriers to reproducibility and accessibility, particularly for academic labs. To address these challenges, we investigate the feasibility of data pruning for BioFM pretraining and propose a post-hoc influence-guided data pruning framework tailored to biological domains. Our approach introduces a subset-based self-influence formulation that enables efficient estimation of sample importance at low computational cost, and builds upon it two simple yet effective selection strategies, namely Top-k Influence (Top I) and Coverage-Centric Influence (CCI). We empirically validate our method on two representative BioFMs, RNA-FM and ESM-C. For RNA, our framework consistently outperforms random selection baselines under an extreme pruning rate of over 99 percent, demonstrating its effectiveness. Furthermore, we show the generalizability of our framework on protein-related tasks using ESM-C. In particular, our coreset even outperforms random subsets that are ten times larger in both RNA and protein settings, revealing substantial redundancy in biological sequence datasets. These findings underscore the potential of influence-guided data pruning to substantially reduce the computational cost of BioFM pretraining, paving the way for more efficient, accessible, and sustainable biological AI research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12932v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifan Wu, Jiyue Jiang, Xichen Ye, Yiqi Wang, Chang Zhou, Yitao Xu, Jiayang Chen, He Hu, Weizhong Zhang, Cheng Jin, Jiao Yuan, Yu Li</dc:creator>
    </item>
    <item>
      <title>Global Inequalities in Clinical Trials Participation</title>
      <link>https://arxiv.org/abs/2601.04660</link>
      <description>arXiv:2601.04660v2 Announce Type: replace-cross 
Abstract: Clinical trials shape medical evidence and determine who gains access to experimental therapies. Whether participation in these trials reflects the global burden of disease remains unclear. Here we analyze participation inequality across more than 62,000 randomized controlled trials spanning 16 major disease categories from 2000 to 2024. Linking 36.8 million trial participants to country-level disease burden, we show that global inequality in clinical trials participation is overwhelmingly shaped by country rather than disease burden. Country-level factors explain over 90% of variation in participation, whereas disease-specific effects contribute only marginally. Removing entire disease categories-including those traditionally considered underfunded-has little effect on overall inequality. Instead, participation is highly concentrated geographically, with a small group of countries enrolling a disproportionate share of participants across nearly all diseases. These patterns have persisted despite decades of disease-targeted funding and increasing alignment between research attention and disease burden within diseases. Our findings indicate that disease-vertical strategies alone cannot correct participation inequality. Reducing global inequities in clinical research requires horizontal investments in research capacity, health infrastructure, and governance that operate across disease domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04660v2</guid>
      <category>econ.GN</category>
      <category>cs.CE</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wen Lou, Adri\'an A. D\'iaz-Faes, Jiangen He, Zhihao Liu, Vincent Larivi\`ere</dc:creator>
    </item>
    <item>
      <title>A Fast and Generalizable Fourier Neural Operator-Based Surrogate for Melt-Pool Prediction in Laser Processing</title>
      <link>https://arxiv.org/abs/2602.06241</link>
      <description>arXiv:2602.06241v2 Announce Type: replace-cross 
Abstract: High-fidelity simulations of laser welding capture complex thermo-fluid phenomena, including phase change, free-surface deformation, and keyhole dynamics, however their computational cost limits large-scale process exploration and real-time use. In this work we present the Laser Processing Fourier Neural Operator (LP-FNO), a Fourier Neural Operator (FNO) based surrogate model that learns the parametric solution operator of various laser processes from multiphysics simulations generated with FLOW-3D WELD (registered trademark). Through a novel approach of reformulating the transient problem in the moving laser frame and applying temporal averaging, the system results in a quasi-steady state setting suitable for operator learning, even in the keyhole welding regime. The proposed LP-FNO maps process parameters to three-dimensional temperature fields and melt-pool boundaries across a broad process window spanning conduction and keyhole regimes using the non-dimensional normalized enthalpy formulation. The model achieves temperature prediction errors on the order of 1% and intersection-over-union scores for melt-pool segmentation over 0.9. We demonstrate that a LP-FNO model trained on coarse-resolution data can be evaluated on finer grids, yielding accurate super-resolved predictions in mesh-converged conduction regimes, whereas discrepancies in keyhole regimes reflect unresolved dynamics in the coarse-mesh training data. These results indicate that the LP-FNO provides an efficient surrogate modeling framework for laser welding, enabling prediction of full three-dimensional fields and phase interfaces over wide parameter ranges in just tens of milliseconds, up to a hundred thousand times faster than traditional Finite Volume multi-physics software.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.06241v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alix Benoit (EMPA), Toni Ivas (EMPA), Mateusz Papierz (Terra Quantum AG), Asel Sagingalieva (Terra Quantum AG), Alexey Melnikov (Terra Quantum AG), Elia Iseli (EMPA)</dc:creator>
    </item>
  </channel>
</rss>
