<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Aug 2025 01:24:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Perspective: An outlook on fluorescence tracking</title>
      <link>https://arxiv.org/abs/2508.13668</link>
      <description>arXiv:2508.13668v1 Announce Type: cross 
Abstract: Tracking single fluorescent molecules has offered resolution into dynamic molecular processes at the single-molecule level. This perspective traces the evolution of single-molecule tracking, highlighting key developments across various methodological branches within fluorescence microscopy. We compare the strengths and limitations of each approach, ranging from conventional widefield offline tracking to real-time confocal tracking. In the final section, we explore emerging efforts to advance physics-inspired tracking techniques, a possibility for parallelization and artificial intelligence, and discuss challenges and opportunities they present toward achieving higher spatiotemporal resolution and greater computational and data efficiency in next-generation single-molecule studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13668v1</guid>
      <category>physics.bio-ph</category>
      <category>stat.CO</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lance W. Q. Xu, Steve Press\'e</dc:creator>
    </item>
    <item>
      <title>The Traceplot Thickens: Developing All-Purpose Convergence Diagnostics for any Markov Chain Monte Carlo Algorithm</title>
      <link>https://arxiv.org/abs/2408.15392</link>
      <description>arXiv:2408.15392v2 Announce Type: replace 
Abstract: Markov Chain Monte Carlo (MCMC) algorithms are frequently used to perform inference under a Bayesian modeling framework. Convergence diagnostics, such as traceplots, the Gelman-Rubin potential scale reduction factor, and effective sample size, are used to visualize and monitor how well the sampler has explored the parameter space and the mixing of multiple chains. However, these classic diagnostics can be undefined or ineffective when the sample space of the algorithm varies in dimension or has a large number of discrete parameters. In this article, we develop a novel approach to produce convergence diagnostics in these difficult scenarios by mapping the original sample space to the real-line and then evaluating the convergence diagnostics on the mapped values. The effectiveness of our method is demonstrated on a MCMC algorithm sampling from a Dirichlet process mixture model. The proposed diagnostics are also used to evaluate the performance of a Bayesian kernel machine regression model for estimating the health effect of multi-pollutant mixtures in the Study of Environment, Lifestyle, and Fibroids. Based on diagnostics for the latter dataset, we then explain how we modify the MCMC sampler to improve convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15392v2</guid>
      <category>stat.CO</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke Duttweiler, Jonathan Klus, Brent A. Coull, Ruth J. Geller, Birgit Claus Henn, Sally W. Thurston</dc:creator>
    </item>
    <item>
      <title>Robustly estimating heterogeneity in factorial data using Rashomon Partitions</title>
      <link>https://arxiv.org/abs/2404.02141</link>
      <description>arXiv:2404.02141v4 Announce Type: replace-cross 
Abstract: In both observational data and randomized control trials, researchers select statistical models to articulate how the outcome of interest varies with combinations of observable covariates. Choosing a model that is too simple can obfuscate important heterogeneity in outcomes between covariate groups, while too much complexity risks identifying spurious patterns. In this paper, we propose a novel Bayesian framework for model uncertainty called Rashomon Partition Sets (RPSs). The RPS consists of all models that have posterior density close to the maximum a posteriori (MAP) model. We construct the RPS by enumeration, rather than sampling, which ensures that we explore all models models with high evidence in the data, even if they offer dramatically different substantive explanations. We use a l0 prior, which allows the allows us to capture complex heterogeneity without imposing strong assumptions about the associations between effects, showing this prior is minimax optimal from an information-theoretic perspective. We characterize the approximation error of (functions of) parameters computed conditional on being in the RPS relative to the entire posterior. We propose an algorithm to enumerate the RPS from the class of models that are interpretable and unique, then provide bounds on the size of the RPS. We give simulation evidence along with three empirical examples: price effects on charitable giving, heterogeneity in chromosomal structure, and the introduction of microfinance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02141v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aparajithan Venkateswaran, Anirudh Sankar, Arun G. Chandrasekhar, Tyler H. McCormick</dc:creator>
    </item>
    <item>
      <title>LEARNER: A Transfer Learning Method for Low-Rank Matrix Estimation</title>
      <link>https://arxiv.org/abs/2412.20605</link>
      <description>arXiv:2412.20605v2 Announce Type: replace-cross 
Abstract: Low-rank matrix estimation is a fundamental problem in statistics and machine learning with applications across biomedical sciences, including genetics, medical imaging, drug discovery, and electronic health record data analysis. In the context of heterogeneous data generated from diverse sources, a key challenge lies in leveraging data from a source population to enhance the estimation of a low-rank matrix in a target population of interest. We propose an approach that leverages similarity in the latent row and column spaces between the source and target populations to improve estimation in the target population, which we refer to as LatEnt spAce-based tRaNsfer lEaRning (LEARNER). LEARNER is based on performing a low-rank approximation of the target population data which penalizes differences between the latent row and column spaces between the source and target populations. We present a cross-validation approach that allows the method to adapt to the degree of heterogeneity across populations. We conducted extensive simulations which found that LEARNER often outperforms the benchmark approach that only uses the target population data, especially as the signal-to-noise ratio in the source population increases. We also performed an illustrative application and empirical comparison of LEARNER and benchmark approaches in a re-analysis of summary statistics from a genome-wide association study in the BioBank Japan cohort. LEARNER is implemented in the R package learner and the Python package learner-py.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20605v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sean McGrath, Cenhao Zhu, Ryan O'Dea, Min Guo, Rui Duan</dc:creator>
    </item>
    <item>
      <title>Rapid Urban Visibility Hotspots: Quantifying Building Vertex Visibility from Connected Vehicle Trajectories using Spatial Indexing</title>
      <link>https://arxiv.org/abs/2506.03365</link>
      <description>arXiv:2506.03365v2 Announce Type: replace-cross 
Abstract: Effective placement of Out-of-Home advertising and street furniture requires accurate identification of locations offering maximum visual exposure to target audiences, particularly vehicular traffic. Traditional site selection methods often rely on static traffic counts or subjective assessments. This research introduces a data-driven methodology to objectively quantify location visibility by analyzing large-scale connected vehicle trajectory data (sourced from Compass IoT) within urban environments. We model the dynamic driver field-of-view using a forward-projected visibility area for each vehicle position derived from interpolated trajectories. By integrating this with building vertex locations extracted from OpenStreetMap, we quantify the cumulative visual exposure, or ``visibility count'', for thousands of potential points of interest near roadways. The analysis reveals that visibility is highly concentrated, identifying specific ``visual hotspots'' that receive disproportionately high exposure compared to average locations. The core technical contribution involves the construction of a BallTree spatial index over building vertices. This enables highly efficient (O(logN) complexity) radius queries to determine which vertices fall within the viewing circles of millions of trajectory points across numerous trips, significantly outperforming brute-force geometric checks. Analysis reveals two key findings: 1) Visibility is highly concentrated, identifying distinct 'visual hotspots' receiving disproportionately high exposure compared to average locations. 2) The aggregated visibility counts across vertices conform to a Log-Normal distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03365v2</guid>
      <category>eess.SY</category>
      <category>cs.CV</category>
      <category>cs.SY</category>
      <category>stat.CO</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Artur Grigorev, Adriana-Simona Mihaita</dc:creator>
    </item>
  </channel>
</rss>
