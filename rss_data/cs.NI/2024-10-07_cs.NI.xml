<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Oct 2024 04:00:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Trends, Advancements and Challenges in Intelligent Optimization in Satellite Communication</title>
      <link>https://arxiv.org/abs/2410.03674</link>
      <description>arXiv:2410.03674v1 Announce Type: new 
Abstract: Efficient satellite communications play an enormously important role in all of our daily lives. This includes the transmission of data for communication purposes, the operation of IoT applications or the provision of data for ground stations. More and more, AI-based methods are finding their way into these areas. This paper gives an overview of current research in the field of intelligent optimization of satellite communication. For this purpose, a text-mining based literature review was conducted and the identified papers were thematically clustered and analyzed. The identified clusters cover the main topics of routing, resource allocation and, load balancing. Through such a clustering of the literature in overarching topics, a structured analysis of the research papers was enabled, allowing the identification of latest technologies and approaches as well as research needs for intelligent optimization of satellite communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03674v1</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Philippe Krajsic, Viola Suess, Zehong Cao, Ryszard Kowalczyk, Bogdan Franczyk</dc:creator>
    </item>
    <item>
      <title>LLM Agents as 6G Orchestrator: A Paradigm for Task-Oriented Physical-Layer Automation</title>
      <link>https://arxiv.org/abs/2410.03688</link>
      <description>arXiv:2410.03688v1 Announce Type: new 
Abstract: The rapid advancement in generative pre-training models is propelling a paradigm shift in technological progression from basic applications such as chatbots towards more sophisticated agent-based systems. It is with huge potential and necessity that the 6G system be combined with the copilot of large language model (LLM) agents and digital twins (DT) to manage the highly complicated communication system with new emerging features such as native AI service and sensing. With the 6G-oriented agent, the base station could understand the transmission requirements of various dynamic upper-layer tasks, automatically orchestrate the optimal system workflow. Through continuously get feedback from the 6G DT for reinforcement, the agents can finally raise the performance of practical system accordingly. Differing from existing LLM agents designed for general application, the 6G-oriented agent aims to make highly rigorous and precise planning with a vast amount of extra expert knowledge, which inevitably requires a specific system design from model training to implementation. This paper proposes a novel comprehensive approach for building task-oriented 6G LLM agents. We first propose a two-stage continual pre-training and fine-tuning scheme to build the field basic model and diversities of specialized expert models for meeting the requirements of various application scenarios. Further, a novel inference framework based on semantic retrieval for leveraging the existing communication-related functions is proposed. Experiment results of exemplary tasks, such as physical-layer task decomposition, show the proposed paradigm's feasibility and effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03688v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuoran Xiao, Chenhui Ye, Yunbo Hu, Honggang Yuan, Yihang Huang, Yijia Feng, Liyu Cai, Jiang Chang</dc:creator>
    </item>
    <item>
      <title>Floating-floating point: a highly accurate number representation with flexible Counting ranges</title>
      <link>https://arxiv.org/abs/2410.03692</link>
      <description>arXiv:2410.03692v1 Announce Type: new 
Abstract: Efficient number representation is essential for federated learning, natural language processing, and network measurement solutions. Due to timing, area, and power constraints, such applications use narrow bit-width (e.g., 8-bit) number systems. The widely used floating-point systems exhibit a trade-off between the counting range and accuracy. This paper introduces Floating-Floating-Point (F2P) - a floating point number that varies the partition between mantissa and exponent. Such flexibility leads to a large counting range combined with improved accuracy over a selected sub-range. Our evaluation demonstrates that moving to F2P from the state-of-the-art improves network measurement accuracy and federated learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03692v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Itamar Cohen, Gil Einziger</dc:creator>
    </item>
    <item>
      <title>Exploring QUIC Dynamics: A Large-Scale Dataset for Encrypted Traffic Analysis</title>
      <link>https://arxiv.org/abs/2410.03728</link>
      <description>arXiv:2410.03728v1 Announce Type: new 
Abstract: QUIC, a new and increasingly used transport protocol, addresses and resolves the limitations of TCP by offering improved security, performance, and features such as stream multiplexing and connection migration. These features, however, also present challenges for network operators who need to monitor and analyze web traffic. In this paper, we introduce VisQUIC, a labeled dataset comprising over 100,000 QUIC traces from more than 44,000 websites (URLs), collected over a four-month period. These traces provide the foundation for generating more than seven million images, with configurable parameters of window length, pixel resolution, normalization, and labels. These images enable an observer looking at the interactions between a client and a server to analyze and gain insights about QUIC encrypted connections. To illustrate the dataset's potential, we offer a use-case example of an observer estimating the number of HTTP/3 responses/requests pairs in a given QUIC, which can reveal server behavior, client--server interactions, and the load imposed by an observed connection. We formulate the problem as a discrete regression problem, train a machine learning (ML) model for it, and then evaluate it using the proposed dataset on an example use case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03728v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Barak Gahtan, Robert J. Sahala, Alex M. Bronstein, Reuven Cohen</dc:creator>
    </item>
    <item>
      <title>Multi-Scale Convolutional LSTM with Transfer Learning for Anomaly Detection in Cellular Networks</title>
      <link>https://arxiv.org/abs/2410.03732</link>
      <description>arXiv:2410.03732v1 Announce Type: new 
Abstract: The rapid growth in mobile broadband usage and increasing subscribers have made it crucial to ensure reliable network performance. As mobile networks grow more complex, especially during peak hours, manual collection of Key Performance Indicators (KPIs) is time-consuming due to the vast data involved. Detecting network failures and identifying unusual behavior during busy periods is vital to assess network health. Researchers have applied Deep Learning (DL) and Machine Learning (ML) techniques to understand network behavior by predicting throughput, analyzing call records, and detecting outages. However, these methods often require significant computational power, large labeled datasets, and are typically specialized, making retraining for new scenarios costly and time-intensive.
  This study introduces a novel approach Multi-Scale Convolutional LSTM with Transfer Learning (TL) to detect anomalies in cellular networks. The model is initially trained from scratch using a publicly available dataset to learn typical network behavior. Transfer Learning is then employed to fine-tune the model by applying learned weights to different datasets. We compare the performance of the model trained from scratch with that of the fine-tuned model using TL. To address class imbalance and gain deeper insights, Exploratory Data Analysis (EDA) and the Synthetic Minority Over-sampling Technique (SMOTE) are applied. Results demonstrate that the model trained from scratch achieves 99% accuracy after 100 epochs, while the fine-tuned model reaches 95% accuracy on a different dataset after just 20 epochs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03732v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nooruddin Noonari, Daniel Corujo, Rui L. Aguiar, Francisco J. Ferrao</dc:creator>
    </item>
    <item>
      <title>Meta Reinforcement Learning Approach for Adaptive Resource Optimization in O-RAN</title>
      <link>https://arxiv.org/abs/2410.03737</link>
      <description>arXiv:2410.03737v1 Announce Type: new 
Abstract: As wireless networks grow to support more complex applications, the Open Radio Access Network (O-RAN) architecture, with its smart RAN Intelligent Controller (RIC) modules, becomes a crucial solution for real-time network data collection, analysis, and dynamic management of network resources including radio resource blocks and downlink power allocation. Utilizing artificial intelligence (AI) and machine learning (ML), O-RAN addresses the variable demands of modern networks with unprecedented efficiency and adaptability. Despite progress in using ML-based strategies for network optimization, challenges remain, particularly in the dynamic allocation of resources in unpredictable environments. This paper proposes a novel Meta Deep Reinforcement Learning (Meta-DRL) strategy, inspired by Model-Agnostic Meta-Learning (MAML), to advance resource block and downlink power allocation in O-RAN. Our approach leverages O-RAN's disaggregated architecture with virtual distributed units (DUs) and meta-DRL strategies, enabling adaptive and localized decision-making that significantly enhances network efficiency. By integrating meta-learning, our system quickly adapts to new network conditions, optimizing resource allocation in real-time. This results in a 19.8% improvement in network management performance over traditional methods, advancing the capabilities of next-generation wireless networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03737v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fatemeh Lotfi, Fatemeh Afghah</dc:creator>
    </item>
    <item>
      <title>Distributed AI Platform for the 6G RAN</title>
      <link>https://arxiv.org/abs/2410.03747</link>
      <description>arXiv:2410.03747v1 Announce Type: new 
Abstract: Cellular Radio Access Networks (RANs) are rapidly evolving towards 6G, driven by the need to reduce costs and introduce new revenue streams for operators and enterprises. In this context, AI emerges as a key enabler in solving complex RAN problems spanning both the management and application domains. Unfortunately, and despite the undeniable promise of AI, several practical challenges still remain, hindering the widespread adoption of AI applications in the RAN space. This article attempts to shed light to these challenges and argues that existing approaches in addressing them are inadequate for realizing the vision of a truly AI-native 6G network. Motivated by this lack of solutions, it proposes a generic distributed AI platform architecture, tailored to the needs of an AI-native RAN and discusses its alignment with ongoing standardization efforts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03747v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ganesh Ananthanarayanan, Xenofon Foukas, Bozidar Radunovic, Yongguang Zhang</dc:creator>
    </item>
    <item>
      <title>Exploring 5G Network Performance: Comparison of Inner and Outer City Areas in Phetchaburi Province</title>
      <link>https://arxiv.org/abs/2410.04066</link>
      <description>arXiv:2410.04066v1 Announce Type: new 
Abstract: The advancement of 5G technology has transformed various aspects of life, including tourism, by enabling people worldwide to communicate and travel with ease. Traveling to different places and countries is now seamless, removing language barriers and facilitating easy access to information on culture, accommodation, and tourist attractions. Additionally, access to applications that facilitate quicker language translation further enhances the travel experience. Phetchaburi Province holds significant importance as a global tourist destination. UNESCO has recognized Phetchaburi as a member of the UNESCO Creative Cities Network (UCCN), comprising one of 49 cities worldwide acknowledged for their creative city initiatives. Phetchaburi Province stands as the 5th city in Thailand to receive this designation. This research investigated 5G performance in Phetchaburi Province, both the inner and outer city, focusing on download and upload speeds. The results indicate that there is widespread 5G coverage throughout Phetchaburi Province, including urban and rural areas, especially for the 5G network with a good performance provided by one of the mobile network operators. In addition, the statistical analysis reveals differences in 5G performances between the inner city and the outer city of Phetchaburi Province, particularly for download speeds (p-value &lt; 0.001).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04066v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Phisit Pornpongtechavanich, Therdpong Daengsi</dc:creator>
    </item>
    <item>
      <title>Robust Task-Oriented Communication Framework for Real-Time Collaborative Vision Perception</title>
      <link>https://arxiv.org/abs/2410.04168</link>
      <description>arXiv:2410.04168v1 Announce Type: new 
Abstract: Cooperative perception enhances sensing in multi-robot and vehicular networks by aggregating information from multiple agents, improving perception accuracy and range. However, mobility and non-rigid sensor mounts introduce extrinsic calibration errors, necessitating online calibration, which is complicated by limited overlap in sensing regions. Maintaining fresh information is crucial for timely and accurate sensing. To address calibration errors and ensure both perception accuracy and transmission timeliness, we propose a Robust Task-Oriented Communication framework (R-TOCOM) that optimizes calibration and feature transmission in both deployment and streaming phases. First, we formulate an Age of Perceived Targets (AoPT) minimization problem to capture information freshness. Then, in the deployment phase, we introduce a channel-aware self-calibration technique based on re-identification (Re-ID). This technique adaptively compresses key-point features according to channel capacities, effectively addressing calibration issues via spatial and temporal cross-camera correlations. In the streaming phase, we tackle the trade-off between bandwidth and inference accuracy by integrating an Information Bottleneck (IB)-based encoding method that adjusts video compression rates based on task relevance, thereby reducing communication overhead and latency. To mitigate performance degradation from packet loss, we introduce a priority network that filters corrupted features. Extensive studies demonstrate our framework outperforms five baselines, improving multiple object detection accuracy (MODA) by 25.49% and reducing communication costs by 51.36% under severe channel condition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04168v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhengru Fang, Jingjing Wang, Yanan Ma, Yihang Tao, Yiqin Deng, Xianhao Chen, Yuguang Fang</dc:creator>
    </item>
    <item>
      <title>Consistent and Repeatable Testing of O-RAN Distributed Unit (O-DU) across Continents</title>
      <link>https://arxiv.org/abs/2410.04416</link>
      <description>arXiv:2410.04416v1 Announce Type: new 
Abstract: Open Radio Access Networks (O-RAN) are expected to revolutionize the telecommunications industry with benefits like cost reduction, vendor diversity, and improved network performance through AI optimization. Supporting the O-RAN ALLIANCE's mission to achieve more intelligent, open, virtualized and fully interoperable mobile networks, O-RAN Open Testing and Integration Centers (OTICs) play a key role in accelerating the adoption of O-RAN specifications based on rigorous testing and validation. One theme in the recent O-RAN Global PlugFest Spring 2024 focused on demonstrating consistent and repeatable Open Fronthaul testing in multiple labs. To respond to this topic, in this paper, we present a detailed analysis of the testing methodologies and results for O-RAN Distributed Unit (O-DU) in O-RAN across two OTICs. We identify key differences in testing setups, share challenges encountered, and propose best practices for achieving repeatable and consistent testing results. Our findings highlight the impact of different deployment technologies and testing environments on performance and conformance testing outcomes, providing valuable insights for future O-RAN implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04416v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tuan V. Ngo, Mao V. Ngo, Binbin Chen, Gabriele Gemmi, Eduardo Baena, Michele Polese, Tommaso Melodia, William Chien, Tony Quek</dc:creator>
    </item>
    <item>
      <title>Consistent and Repeatable Testing of mMIMO O-RU across labs: A Japan-Singapore Experience</title>
      <link>https://arxiv.org/abs/2410.04427</link>
      <description>arXiv:2410.04427v1 Announce Type: new 
Abstract: Open Radio Access Networks (RAN) aim to bring a paradigm shift to telecommunications industry, by enabling an open, intelligent, virtualized, and multi-vendor interoperable RAN ecosystem. At the center of this movement, O-RAN ALLIANCE defines the O-RAN architecture and standards, so that companies around the globe can use these specifications to create innovative and interoperable solutions. To accelerate the adoption of O-RAN products, rigorous testing of O-RAN Radio Unit (O-RU) and other O-RAN products plays a key role. O-RAN ALLIANCE has approved around 20 Open Testing and Integration Centres (OTICs) globally. OTICs serve as vendor-neutral platforms for providing the testing and integration services, with the vision that an O-RAN product certified in any OTIC is accepted in other parts of the world. To demonstrate the viability of such a certified-once-and-use-everywhere approach, one theme in the O-RAN Global PlugFest Spring 2024 is to demonstrate consistent and repeatable testing for the open fronthaul interface across multiple labs. Towards this, Japan OTIC and Asia Pacific OTIC in Singapore have teamed up together with an O-RU vendor and Keysight Technology. Our international team successfully completed all test cases defined by O-RAN ALLIANCE for O-RU conformance testing. In this paper, we share our journey in achieving this outcome, focusing on the challenges we have overcome and the lessons we have learned through this process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04427v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Thanh-Tam Nguyen, Mao V. Ngo, Binbin Chen, Mitsuhiro Kuchitsu, Serena Wai, Seitaro Kawai, Kenya Suzuki, Eng Wei Koo, Tony Quek</dc:creator>
    </item>
    <item>
      <title>PAMLR: A Passive-Active Multi-Armed Bandit-Based Solution for LoRa Channel Allocation</title>
      <link>https://arxiv.org/abs/2410.05147</link>
      <description>arXiv:2410.05147v1 Announce Type: new 
Abstract: Achieving low duty cycle operation in low-power wireless networks in urban environments is complicated by the complex and variable dynamics of external interference and fading. We explore the use of reinforcement learning for achieving low power consumption for the task of optimal selection of channels. The learning relies on a hybrid of passive channel sampling for dealing with external interference and active channel sampling for dealing with fading. Our solution, Passive-Active Multi-armed bandit for LoRa (PAMLR, pronounced "Pamela"), balances the two types of samples to achieve energy-efficient channel selection: active channel measurements are tuned to an appropriately low level to update noise thresholds, and to compensate passive channel measurements are tuned to an appropriately high level for selecting the top-most channels from channel exploration using the noise thresholds. The rates of both types of samples are adapted in response to channel dynamics. Based on extensive testing in multiple environments in different cities, we validate that PAMLR can maintain excellent communication quality, as demonstrated by a low SNR regret compared to the optimal channel allocation policy, while substantially minimizing the energy cost associated with channel measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05147v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3600100.3623725</arxiv:DOI>
      <arxiv:journal_reference>BuildSys 2023: Proceedings of the 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation, Pages 31 - 40</arxiv:journal_reference>
      <dc:creator>Jihoon Yun, Chengzhang Li, Anish Arora</dc:creator>
    </item>
    <item>
      <title>Smart Jamming Attack and Mitigation on Deep Transfer Reinforcement Learning Enabled Resource Allocation for Network Slicing</title>
      <link>https://arxiv.org/abs/2410.05153</link>
      <description>arXiv:2410.05153v1 Announce Type: new 
Abstract: Network slicing is a pivotal paradigm in wireless networks enabling customized services to users and applications. Yet, intelligent jamming attacks threaten the performance of network slicing. In this paper, we focus on the security aspect of network slicing over a deep transfer reinforcement learning (DTRL) enabled scenario. We first demonstrate how a deep reinforcement learning (DRL)-enabled jamming attack exposes potential risks. In particular, the attacker can intelligently jam resource blocks (RBs) reserved for slices by monitoring transmission signals and perturbing the assigned resources. Then, we propose a DRL-driven mitigation model to mitigate the intelligent attacker. Specifically, the defense mechanism generates interference on unallocated RBs where another antenna is used for transmitting powerful signals. This causes the jammer to consider these RBs as allocated RBs and generate interference for those instead of the allocated RBs. The analysis revealed that the intelligent DRL-enabled jamming attack caused a significant 50% degradation in network throughput and 60% increase in latency in comparison with the no-attack scenario. However, with the implemented mitigation measures, we observed 80% improvement in network throughput and 70% reduction in latency in comparison to the under-attack scenario.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05153v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TMLCN.2024.3470760</arxiv:DOI>
      <dc:creator>Shavbo Salehi, Hao Zhou, Medhat Elsayed, Majid Bavand, Raimundas Gaigalas, Yigit Ozcan, Melike Erol-Kantarci</dc:creator>
    </item>
    <item>
      <title>Post-Quantum Cryptography Anonymous Scheme -- PQCWC: Post-Quantum Cryptography Winternitz-Chen</title>
      <link>https://arxiv.org/abs/2410.03678</link>
      <description>arXiv:2410.03678v1 Announce Type: cross 
Abstract: As quantum computing technology matures, it poses a threat to the security of mainstream asymmetric cryptographic methods. In response, the National Institute of Standards and Technology released the final version of post-quantum cryptographic (PQC) algorithm standards in August 2024. These post-quantum cryptographic algorithms are primarily based on lattice-based and hash-based cryptography. Therefore, this study proposes the Post-Quantum Cryptography Winternitz-Chen (PQCWC) anonymous scheme, aimed at exploring the design of anonymous schemes based on PQC for future applications in privacy protection. The anonymous scheme designed in this study is mainly built on the Winternitz signature scheme, which can prevent the original public key from being exposed in the certificate. Furthermore, the PQCWC anonymous scheme integrates the butterfly key expansion mechanism, proposing the first hash-based butterfly key expansion mechanism in the world, achieving anonymity for both the registration authority and the certificate authority, thereby fully protecting privacy. In the experimental environment, this study compares various hash algorithms, including Secure Hash Algorithm-1 (SHA-1), the SHA-2 series, the SHA-3 series, and the BLAKE series. The results demonstrate that the proposed anonymous scheme can achieve anonymity without increasing key length, signature length, key generation time, signature generation time, or signature verification time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03678v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.NI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abel C. H. Chen</dc:creator>
    </item>
    <item>
      <title>Reparo: Loss-Resilient Generative Codec for Video Conferencing</title>
      <link>https://arxiv.org/abs/2305.14135</link>
      <description>arXiv:2305.14135v3 Announce Type: replace 
Abstract: Packet loss during video conferencing often results in poor quality and video freezing. Retransmitting lost packets is often impractical due to the need for real-time playback, and using Forward Error Correction (FEC) for packet recovery is challenging due to the unpredictable and bursty nature of Internet losses. Excessive redundancy leads to inefficiency and wasted bandwidth, while insufficient redundancy results in undecodable frames, causing video freezes and quality degradation in subsequent frames.
  We introduce Reparo -- a loss-resilient video conferencing framework based on generative deep learning models to address these issues. Our approach generates missing information when a frame or part of a frame is lost. This generation is conditioned on the data received thus far, considering the model's understanding of how people and objects appear and interact within the visual realm. Experimental results, using publicly available video conferencing datasets, demonstrate that Reparo outperforms state-of-the-art FEC-based video conferencing solutions in terms of both video quality (measured through PSNR, SSIM, and LPIPS) and the occurrence of video freezes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.14135v3</guid>
      <category>cs.NI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianhong Li, Vibhaalakshmi Sivaraman, Pantea Karimi, Lijie Fan, Mohammad Alizadeh, Dina Katabi</dc:creator>
    </item>
    <item>
      <title>Bayesian Optimization Framework for Channel Simulation-Based Base Station Placement and Transmission Power Design</title>
      <link>https://arxiv.org/abs/2407.20778</link>
      <description>arXiv:2407.20778v2 Announce Type: replace 
Abstract: This study proposes an adaptive experimental design framework for a channel-simulation-based base station (BS) design that supports the joint optimization of transmission power and placement. We consider a system in which multiple transmitters provide wireless services over a shared frequency band. Our objective is to maximize the average throughput within an area of interest. System operators can design the system configurations prior to deployment by iterating them through channel simulations and updating the parameters. However, accurate channel simulations are computationally expensive; therefore, it is preferable to configure the system using a limited number of simulation iterations. We develop a solver for the problem based on Bayesian optimization (BO), a black-box optimization method. The numerical results demonstrate that our proposed framework can achieve 18-22% higher throughput performance than conventional placement and power optimization strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20778v2</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/LNET.2024.3469175</arxiv:DOI>
      <dc:creator>Koya Sato, Katsuya Suto</dc:creator>
    </item>
    <item>
      <title>Research on Enhancing C-V2X Communication via Danger-Aware Vehicular Networking</title>
      <link>https://arxiv.org/abs/2410.00012</link>
      <description>arXiv:2410.00012v2 Announce Type: replace 
Abstract: This paper presents a protocol that optimizes message dissemination in C-V2X technology, crucial for advancing intelligent transportation systems (ITS) aimed at enhancing road safety. As vehicle density and velocity rise, the volume of data requiring communication significantly increases. By considering the risk levels that vehicles encounter and using inter-vehicle proximity as a key indicator of potential hazards, the proposed protocol prioritizes communication, allowing vehicles facing higher risks to transmit their messages first. Our results show that this prioritization effectively reduces the number of concurrent transmissions, leading to improved performance metrics such as packet delivery ratio, throughput, latency, and lower probabilities of channel congestion and collision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00012v2</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>IEEE ACCESS, 2024</arxiv:journal_reference>
      <dc:creator>Lanre Sadeeq, Qasim Ajao</dc:creator>
    </item>
    <item>
      <title>Performance Analysis of 6TiSCH Networks Using Discrete Events Simulator</title>
      <link>https://arxiv.org/abs/2410.03383</link>
      <description>arXiv:2410.03383v2 Announce Type: replace 
Abstract: The Internet of Things (IoT) empowers small devices to sense, react, and communicate, with applications ranging from smart ordinary household objects to complex industrial processes. To provide access to an increasing number of IoT devices, particularly in long-distance communication scenarios, a robust low-power wide area network (LPWAN) protocol becomes essential. A widely adopted protocol for this purpose is 6TiSCH, which builds upon the IEEE 802.15.4 standard. It introduces time-slotted channel hopping (TSCH) mode as a new medium access control (MAC) layer operating mode, in conjunction with IEEE 802.15.4g, which also defines both MAC and physical layer (PHY) layers and provides IPv6 connectivity for LPWAN. Notably, 6TiSCH has gained adoption in significant standards such as Wireless Intelligent Ubiquitous Networks (Wi-SUN). This study evaluates the scalability of 6TiSCH, with a focus on key parameters such as queue size, the maximum number of single-hop retries, and the slotframe length. Computational simulations were performed using an open-source simulator and obtained the following results: increasing the transmission queue size, along with adjusting the number of retries and slotframe length, leads to a reduction in the packet error rate (PER). Notably, the impact of the number of retries is particularly pronounced. Furthermore, the effect on latency varies based on the specific combination of these parameters as the network scales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03383v2</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guilherme de Santi Peron, Marcos Eduardo Pivaro Monteiro, Jo\~ao Lu\'is Verdegay de Barros, Jamil Farhat, Glauber Brante</dc:creator>
    </item>
    <item>
      <title>Throughput Requirements for RAN Functional Splits in 3D-Networks</title>
      <link>https://arxiv.org/abs/2405.15432</link>
      <description>arXiv:2405.15432v2 Announce Type: replace-cross 
Abstract: The rapid growth of non-terrestrial communication necessitates its integration with existing terrestrial networks, as highlighted in 3GPP Releases 16 and 17. This paper analyses the concept of functional splits in 3D-Networks. To manage this complex structure effectively, the adoption of a Radio Access Network (RAN) architecture with Functional Split (FS) offers advantages in flexibility, scalability, and cost-efficiency. RAN achieves this by disaggregating functionalities into three separate units. Analogous to the terrestrial network approach, 3GPP is extending this concept to non-terrestrial platforms as well. This work presents a general analysis of the requested Fronthaul (FH) data rate on feeder link between a non-terrestrial platform and the ground-station. Each split option is a trade-of between FH data rate and the respected complexity. Since flying nodes face more limitations regarding power consumption and complexity on board in comparison to terrestrial ones, we are investigating the split options between lower and higher physical layer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15432v2</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>MohammadAmin Vakilifard, Tim D\"ue, Mohammad Rihan, Maik R\"oper, Dirk W\"ubben, Carsten Bockelmann, Armin Dekorsy</dc:creator>
    </item>
    <item>
      <title>Differentiable and Learnable Wireless Simulation with Geometric Transformers</title>
      <link>https://arxiv.org/abs/2406.14995</link>
      <description>arXiv:2406.14995v2 Announce Type: replace-cross 
Abstract: Modelling the propagation of electromagnetic wireless signals is critical for designing modern communication systems. Wireless ray tracing simulators model signal propagation based on the 3D geometry and other scene parameters, but their accuracy is fundamentally limited by underlying modelling assumptions and correctness of parameters. In this work, we introduce Wi-GATr, a fully-learnable neural simulation surrogate designed to predict the channel observations based on scene primitives (e.g., surface mesh, antenna position and orientation). Recognizing the inherently geometric nature of these primitives, Wi-GATr leverages an equivariant Geometric Algebra Transformer that operates on a tokenizer specifically tailored for wireless simulation. We evaluate our approach on a range of tasks (i.e., signal strength and delay spread prediction, receiver localization, and geometry reconstruction) and find that Wi-GATr is accurate, fast, sample-efficient, and robust to symmetry-induced transformations. Remarkably, we find our results also translate well to the real world: Wi-GATr demonstrates more than 35% lower error than hybrid techniques, and 70% lower error than a calibrated wireless tracer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14995v2</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Hehn, Markus Peschl, Tribhuvanesh Orekondy, Arash Behboodi, Johann Brehmer</dc:creator>
    </item>
    <item>
      <title>Security Testbed for Preempting Attacks against Supercomputing Infrastructure</title>
      <link>https://arxiv.org/abs/2409.09602</link>
      <description>arXiv:2409.09602v2 Announce Type: replace-cross 
Abstract: Securing HPC has a unique threat model. Untrusted, malicious code exploiting the concentrated computing power may exert an outsized impact on the shared, open-networked environment in HPC, unlike well-isolated VM tenants in public clouds. Therefore, preempting attacks targeting supercomputing systems before damage remains the top security priority. The main challenge is that noisy attack attempts and unreliable alerts often mask \emph{real attacks}, causing permanent damages such as system integrity violations and data breaches. This paper describes a security testbed embedded in live traffic of a supercomputer at the National Center for Supercomputing Applications (NCSA). The objective is to demonstrate attack \textit{preemption}, i.e., stopping system compromise and data breaches at petascale supercomputers. Deployment of our testbed at NCSA enables the following key contributions:
  1) Insights from characterizing unique \textit{attack patterns} found in real security logs of more than 200 security incidents curated in the past two decades at NCSA.
  2) Deployment of an attack visualization tool to illustrate the challenges of identifying real attacks in HPC environments and to support security operators in interactive attack analyses.
  3) Demonstrate the utility of the testbed by running novel models, such as Factor-Graph-based models, to preempt a real-world ransomware family.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09602v2</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Phuong Cao, Zbigniew Kalbarczyk, Ravishankar Iyer</dc:creator>
    </item>
    <item>
      <title>Probabilistic Allocation of Payload Code Rate and Header Copies in LR-FHSS Networks</title>
      <link>https://arxiv.org/abs/2410.03392</link>
      <description>arXiv:2410.03392v2 Announce Type: replace-cross 
Abstract: We evaluate the performance of the LoRaWAN Long-Range Frequency Hopping Spread Spectrum (LR-FHSS) technique using a device-level probabilistic strategy for code rate and header replica allocation. Specifically, we investigate the effects of different header replica and code rate allocations at each end-device, guided by a probability distribution provided by the network server. As a benchmark, we compare the proposed strategy with the standardized LR-FHSS data rates DR8 and DR9. Our numerical results demonstrate that the proposed strategy consistently outperforms the DR8 and DR9 standard data rates across all considered scenarios. Notably, our findings reveal that the optimal distribution rarely includes data rate DR9, while data rate DR8 significantly contributes to the goodput and energy efficiency optimizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03392v2</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jamil de Araujo Farhat, Jean Michel de Souza Sant'Ana, Jo\~ao Luiz Rebelatto, Nurul Huda Mahmood, Gianni Pasolini, Richard Demo Souza</dc:creator>
    </item>
  </channel>
</rss>
