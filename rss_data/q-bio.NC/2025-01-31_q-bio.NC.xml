<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 Jan 2025 05:00:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Statistical Tools for Frequency Response Functions from Posture Control Experiments: Estimation of Probability of a Sample and Comparison Between Groups of Unpaired Samples</title>
      <link>https://arxiv.org/abs/2501.17891</link>
      <description>arXiv:2501.17891v1 Announce Type: cross 
Abstract: The frequency response function (FRF) is an established way to describe the outcome of experiments in posture control literature. The FRF is an empirical transfer function between an input stimulus and the induced body segment sway profile, represented as a vector of complex values associated with a vector of frequencies. Having obtained an FRF from a trial with a subject, it can be useful to quantify the likelihood it belongs to a certain population, e.g., to diagnose a condition or to evaluate the human likeliness of a humanoid robot or a wearable device. In this work, a recently proposed method for FRF statistics based on confidence bands computed with bootstrap will be summarized, and, on its basis, possible ways to quantify the likelihood of FRFs belonging to a given set will be proposed. Furthermore, a statistical test to compare groups of unpaired samples is presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17891v1</guid>
      <category>eess.SP</category>
      <category>q-bio.NC</category>
      <category>stat.ME</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vittorio Lippi</dc:creator>
    </item>
    <item>
      <title>Large Language Models Think Too Fast To Explore Effectively</title>
      <link>https://arxiv.org/abs/2501.18009</link>
      <description>arXiv:2501.18009v1 Announce Type: cross 
Abstract: Large Language Models have emerged many intellectual capacities. While numerous benchmarks assess their intelligence, limited attention has been given to their ability to explore, an essential capacity for discovering new information and adapting to novel environments in both natural and artificial systems. The extent to which LLMs can effectively explore, particularly in open-ended tasks, remains unclear. This study investigates whether LLMs can surpass humans in exploration during an open-ended task, using Little Alchemy 2 as a paradigm, where agents combine elements to discover new ones. Results show most LLMs underperform compared to humans, except for the o1 model, with those traditional LLMs relying primarily on uncertainty driven strategies, unlike humans who balance uncertainty and empowerment. Representational analysis of the models with Sparse Autoencoders revealed that uncertainty and choices are represented at earlier transformer blocks, while empowerment values are processed later, causing LLMs to think too fast and make premature decisions, hindering effective exploration. These findings shed light on the limitations of LLM exploration and suggest directions for improving their adaptability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18009v1</guid>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lan Pan, Hanbo Xie, Robert C. Wilson</dc:creator>
    </item>
    <item>
      <title>A comprehensive numerical investigation of a coupled mathematical model of neuronal excitability</title>
      <link>https://arxiv.org/abs/2501.18013</link>
      <description>arXiv:2501.18013v1 Announce Type: cross 
Abstract: Being an example for a relaxation oscillator, the FitzHugh-Nagumo model has been widely employed for describing the generation of action potentials. In this paper, we begin with a biological interpretation of what the subsequent mathematical and numerical analyses of the model entail. The interaction between action potential variable and recovery variable is then revisited through linear stability analysis around the equilibrium and local stability conditions are determined. Analytical results are compared with numerical simulations. The study aims to show an alternative approach regarding Taylor polynomials and constructed difference scheme which play a key role in the numerical approach for the problem. The robustness of the schemes is investigated in terms of convergency and stability of the techniques. This systematic approach by the combination of numerical techniques provides beneficial results which are uniquely designed for the FitzHugh-Nagumo model. We describe the matrix representations with the collocation points. Then the method is applied in order to acquire a system of nonlinear algebraic equations. On the other hand, we apply finite difference scheme and its stability is also performed. Moreover, the numerical simulations are shown. Consequently, a comprehensive investigation of the related model is examined.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18013v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Burcu G\"urb\"uz, Ayt\"ul G\"ok\c{c}e, Mahmut Modanl{\i}</dc:creator>
    </item>
    <item>
      <title>Perforated Backpropagation: A Neuroscience Inspired Extension to Artificial Neural Networks</title>
      <link>https://arxiv.org/abs/2501.18018</link>
      <description>arXiv:2501.18018v1 Announce Type: cross 
Abstract: The neurons of artificial neural networks were originally invented when much less was known about biological neurons than is known today. Our work explores a modification to the core neuron unit to make it more parallel to a biological neuron. The modification is made with the knowledge that biological dendrites are not simply passive activation funnels, but also compute complex non-linear functions as they transmit activation to the cell body. The paper explores a novel system of "Perforated" backpropagation empowering the artificial neurons of deep neural networks to achieve better performance coding for the same features they coded for in the original architecture. After an initial network training phase, additional "Dendrite Nodes" are added to the network and separately trained with a different objective: to correlate their output with the remaining error of the original neurons. The trained Dendrite Nodes are then frozen, and the original neurons are further trained, now taking into account the additional error signals provided by the Dendrite Nodes. The cycle of training the original neurons and then adding and training Dendrite Nodes can be repeated several times until satisfactory performance is achieved. Our algorithm was successfully added to modern state-of-the-art PyTorch networks across multiple domains, improving upon original accuracies and allowing for significant model compression without a loss in accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18018v1</guid>
      <category>cs.NE</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rorry Brenner, Laurent Itti</dc:creator>
    </item>
    <item>
      <title>ISAM-MTL: Cross-subject multi-task learning model with identifiable spikes and associative memory networks</title>
      <link>https://arxiv.org/abs/2501.18089</link>
      <description>arXiv:2501.18089v1 Announce Type: cross 
Abstract: Cross-subject variability in EEG degrades performance of current deep learning models, limiting the development of brain-computer interface (BCI). This paper proposes ISAM-MTL, which is a multi-task learning (MTL) EEG classification model based on identifiable spiking (IS) representations and associative memory (AM) networks. The proposed model treats EEG classification of each subject as an independent task and leverages cross-subject data training to facilitate feature sharing across subjects. ISAM-MTL consists of a spiking feature extractor that captures shared features across subjects and a subject-specific bidirectional associative memory network that is trained by Hebbian learning for efficient and fast within-subject EEG classification. ISAM-MTL integrates learned spiking neural representations with bidirectional associative memory for cross-subject EEG classification. The model employs label-guided variational inference to construct identifiable spike representations, enhancing classification accuracy. Experimental results on two BCI Competition datasets demonstrate that ISAM-MTL improves the average accuracy of cross-subject EEG classification while reducing performance variability among subjects. The model further exhibits the characteristics of few-shot learning and identifiable neural activity beneath EEG, enabling rapid and interpretable calibration for BCI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18089v1</guid>
      <category>cs.NE</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junyan Li, Bin Hu, Zhi-Hong Guan</dc:creator>
    </item>
    <item>
      <title>Time Makes Space: Emergence of Place Fields in Networks Encoding Temporally Continuous Sensory Experiences</title>
      <link>https://arxiv.org/abs/2408.05798</link>
      <description>arXiv:2408.05798v2 Announce Type: replace 
Abstract: The vertebrate hippocampus is believed to use recurrent connectivity in area CA3 to support episodic memory recall from partial cues. This brain area also contains place cells, whose location-selective firing fields implement maps supporting spatial memory. Here we show that place cells emerge in networks trained to remember temporally continuous sensory episodes. We model CA3 as a recurrent autoencoder that recalls and reconstructs sensory experiences from noisy and partially occluded observations by agents traversing simulated rooms. The agents move in realistic trajectories modeled from rodents and environments are modeled as high-dimensional sensory experience maps. Training our autoencoder to pattern-complete and reconstruct experiences with a constraint on total activity causes spatially localized firing fields, i.e., place cells, to emerge in the encoding layer. The emergent place fields reproduce key aspects of hippocampal phenomenology: a) remapping (maintenance of and reversion to distinct learned maps in different environments), implemented via repositioning of experience manifolds in the network's hidden layer, b) orthogonality of spatial representations in different arenas, c) robust place field emergence in differently shaped rooms, with single units showing multiple place fields in large or complex spaces, and d) slow representational drift of place fields. We argue that these results arise because continuous traversal of space makes sensory experience temporally continuous. We make testable predictions: a) rapidly changing sensory context will disrupt place fields, b) place fields will form even if recurrent connections are blocked, but reversion to previously learned representations upon remapping will be abolished, c) the dimension of temporally smooth experience sets the dimensionality of place fields, including during virtual navigation of abstract spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05798v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhaoze Wang, Ronald W. Di Tullio, Spencer Rooke, Vijay Balasubramanian</dc:creator>
    </item>
    <item>
      <title>Swin fMRI Transformer Predicts Early Neurodevelopmental Outcomes from Neonatal fMRI</title>
      <link>https://arxiv.org/abs/2412.07783</link>
      <description>arXiv:2412.07783v3 Announce Type: replace 
Abstract: Brain development in the first few months of human life is a critical phase characterized by rapid structural growth and functional organization. Accurately predicting developmental outcomes during this time is crucial for identifying delays and enabling timely interventions. This study introduces the SwiFT (Swin 4D fMRI Transformer) model, designed to predict Bayley-III composite scores using neonatal fMRI from the Developing Human Connectome Project (dHCP). To enhance predictive accuracy, we apply dimensionality reduction via group independent component analysis (ICA) and pretrain SwiFT on large adult fMRI datasets to address the challenges of limited neonatal data. Our analysis shows that SwiFT significantly outperforms baseline models in predicting cognitive, motor, and language outcomes, leveraging both single-label and multi-label prediction strategies. The model's attention-based architecture processes spatiotemporal data end-to-end, delivering superior predictive performance. Additionally, we use Integrated Gradients with Smoothgrad sQuare (IG-SQ) to interpret predictions, identifying neural spatial representations linked to early cognitive and behavioral development. These findings underscore the potential of Transformer models to advance neurodevelopmental research and clinical practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07783v3</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Styll, Dowon Kim, Jiook Cha</dc:creator>
    </item>
    <item>
      <title>Stochastic Dynamics and Probability Analysis for a Generalized Epidemic Model with Environmental Noise</title>
      <link>https://arxiv.org/abs/2412.00405</link>
      <description>arXiv:2412.00405v2 Announce Type: replace-cross 
Abstract: In this paper we consider a stochastic SEIQR (susceptible-exposed-infected-quarantined-recovered) epidemic model with a generalized incidence function. Using the Lyapunov method, we establish the existence and uniqueness of a global positive solution to the model, ensuring that it remains well-defined over time. Through the application of Young's inequality and Chebyshev's inequality, we demonstrate the concepts of stochastic ultimate boundedness and stochastic permanence, providing insights into the long-term behavior of the epidemic dynamics under random perturbations. Furthermore, we derive conditions for stochastic extinction, which describe scenarios where the epidemic may eventually die out, and V-geometric ergodicity, which indicates the rate at which the system's state converges to its equilibrium. Finally, we perform numerical simulations to verify our theoretical results and assess the model's behavior under different parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00405v2</guid>
      <category>q-bio.PE</category>
      <category>math.DS</category>
      <category>physics.data-an</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brahim Boukanjime, Mohamed Maama</dc:creator>
    </item>
    <item>
      <title>Unsupervised Learning in Echo State Networks for Input Reconstruction</title>
      <link>https://arxiv.org/abs/2501.11409</link>
      <description>arXiv:2501.11409v2 Announce Type: replace-cross 
Abstract: Conventional echo state networks (ESNs) require supervised learning to train the readout layer, using the desired outputs as training data. In this study, we focus on input reconstruction (IR), which refers to training the readout layer to reproduce the input time series in its output. We reformulate the learning algorithm of the ESN readout layer to perform IR using unsupervised learning (UL). By conducting theoretical analysis and numerical experiments, we demonstrate that IR in ESNs can be effectively implemented under realistic conditions without explicitly using the desired outputs as training data; in this way, UL is enabled. Furthermore, we demonstrate that applications relying on IR, such as dynamical system replication and noise filtering, can be reformulated within the UL framework. Our findings establish a theoretically sound and universally applicable IR formulation, along with its related tasks in ESNs. This work paves the way for novel predictions and highlights unresolved theoretical challenges in ESNs, particularly in the context of time-series processing methods and computational models of the brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11409v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>eess.SP</category>
      <category>nlin.CD</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taiki Yamada, Yuichi Katori, Kantaro Fujiwara</dc:creator>
    </item>
  </channel>
</rss>
