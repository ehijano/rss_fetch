<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.ST</link>
    <description>q-fin.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Dec 2024 05:00:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Comparative Study of DSPy Teleprompter Algorithms for Aligning Large Language Models Evaluation Metrics to Human Evaluation</title>
      <link>https://arxiv.org/abs/2412.15298</link>
      <description>arXiv:2412.15298v1 Announce Type: cross 
Abstract: We argue that the Declarative Self-improving Python (DSPy) optimizers are a way to align the large language model (LLM) prompts and their evaluations to the human annotations. We present a comparative analysis of five teleprompter algorithms, namely, Cooperative Prompt Optimization (COPRO), Multi-Stage Instruction Prompt Optimization (MIPRO), BootstrapFewShot, BootstrapFewShot with Optuna, and K-Nearest Neighbor Few Shot, within the DSPy framework with respect to their ability to align with human evaluations. As a concrete example, we focus on optimizing the prompt to align hallucination detection (using LLM as a judge) to human annotated ground truth labels for a publicly available benchmark dataset. Our experiments demonstrate that optimized prompts can outperform various benchmark methods to detect hallucination, and certain telemprompters outperform the others in at least these experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15298v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bhaskarjit Sarmah, Kriti Dutta, Anna Grigoryan, Sachin Tiwari, Stefano Pasquali, Dhagash Mehta</dc:creator>
    </item>
    <item>
      <title>Battery valuation on electricity intraday markets with liquidity costs</title>
      <link>https://arxiv.org/abs/2412.15959</link>
      <description>arXiv:2412.15959v1 Announce Type: cross 
Abstract: In this paper, we propose a complete modelling framework to value several batteries in the electricity intraday market at the trading session scale. The model consists of a stochastic model for the 24 mid-prices (one price per delivery hour) combined with a deterministic model for the liquidity costs (representing the cost of going deeper in the order book). A stochastic optimisation framework based on dynamic programming is used to calculate the value of the batteries. We carry out a back test for the years 2021, 2022 and 2023 for the German market and for the French market. We show that it is essential to take liquidity into account, especially when the number of batteries is large: it allows much higher profits and avoids high losses using our liquidity model. The use of our stochastic model for the mid-price also significantly improves the results (compared to a deterministic framework where the mid-price forecast is the spot price).</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15959v1</guid>
      <category>q-fin.TR</category>
      <category>q-fin.ST</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Enzo Cogn\'eville, Thomas Deschatre, Xavier Warin</dc:creator>
    </item>
    <item>
      <title>Differentially Private Federated Learning of Diffusion Models for Synthetic Tabular Data Generation</title>
      <link>https://arxiv.org/abs/2412.16083</link>
      <description>arXiv:2412.16083v1 Announce Type: cross 
Abstract: The increasing demand for privacy-preserving data analytics in finance necessitates solutions for synthetic data generation that rigorously uphold privacy standards. We introduce DP-Fed-FinDiff framework, a novel integration of Differential Privacy, Federated Learning and Denoising Diffusion Probabilistic Models designed to generate high-fidelity synthetic tabular data. This framework ensures compliance with stringent privacy regulations while maintaining data utility. We demonstrate the effectiveness of DP-Fed-FinDiff on multiple real-world financial datasets, achieving significant improvements in privacy guarantees without compromising data quality. Our empirical evaluations reveal the optimal trade-offs between privacy budgets, client configurations, and federated optimization strategies. The results affirm the potential of DP-Fed-FinDiff to enable secure data sharing and robust analytics in highly regulated domains, paving the way for further advances in federated learning and privacy-preserving data synthesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16083v1</guid>
      <category>cs.LG</category>
      <category>q-fin.ST</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Timur Sattarov, Marco Schreyer, Damian Borth</dc:creator>
    </item>
  </channel>
</rss>
