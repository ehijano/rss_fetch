<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Nov 2025 05:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>See and Beam: Leveraging LiDAR Sensing and Specular Surfaces for Indoor mmWave Connectivity</title>
      <link>https://arxiv.org/abs/2511.09840</link>
      <description>arXiv:2511.09840v1 Announce Type: new 
Abstract: Millimeter-wave (mmWave) communication enables multi-gigabit-per-second data rates but is highly susceptible to path loss and blockage, especially indoors. Many indoor settings, however, include naturally occurring specular surfaces such as glass, glossy metal panels, and signage, that reflect both light and mmWave signals. Exploiting this dual reflectivity, we propose See and Beam, a low-cost framework that combines LiDAR sensing with passive specular reflectors to enhance mmWave connectivity under non-line-of-sight (NLoS) conditions. In this paper, as a proof of concept, we deploy three types of reflectors, glossy, smooth, and matte (non-specular), to evaluate joint LiDAR/mmWave reflection in an indoor scenario. We demonstrate that using LiDAR-mmWave co-reflective surfaces enables a co-located LiDAR sensor to map the NLoS environment, localize NLoS users, and identify viable communication reflection points. Experimental results at 60 GHz show that LiDAR-guided beam steering with co-reflective surfaces improves the minimum received signal strength by over 20 dB in deep NLoS regions. Moreover, LiDAR-derived angle-of-departure steering achieves performance comparable to exhaustive NLoS beam search. This low cost, and scalable framework serves as an effective alternative to configurable reflecting surfaces and enables robust mmWave connectivity in future 6G and beyond networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09840v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raj Sai Sohel Bandari, Amod Ashtekar, Omar Ibrahim, Mohammed E. Eltayeb</dc:creator>
    </item>
    <item>
      <title>Learning-Based Channel Access in Wi-Fi: A Multi-Armed Bandit Approach</title>
      <link>https://arxiv.org/abs/2511.10143</link>
      <description>arXiv:2511.10143v1 Announce Type: new 
Abstract: Due to its static protocol design, IEEE 802.11 (aka Wi-Fi) channel access lacks adaptability to address dynamic network conditions, resulting in inefficient spectrum utilization, unnecessary contention, and packet collisions. This paper investigates reinforcement learning (RL) solutions to optimize Wi-Fi's medium access control (MAC). In particular, a multi-armed bandit (MAB) framework is proposed for dynamic channel access (including both the primary channel and channel width) and contention window (CW) adjustment. In this setting, we study relevant learning design principles such as adopting joint or factorial action spaces (handled by a single agent (SA) and multiple agents (MA), respectively) and the importance of incorporating contextual information. Our simulation results show that cooperative MA architectures converge faster than their SA counterparts, as agents operate over smaller action spaces. Another key insight is that contextual MAB algorithms consistently outperform non-contextual ones, highlighting the value of leveraging side information in action selection. Moreover, in multi-player settings, results demonstrate that decentralized learners can achieve implicit coordination, although their greediness may degrade coexisting networks' performance and induce policy-chasing dynamics. Overall, these findings demonstrate that (contextual) MAB-based learning offers a practical and adaptive alternative to static IEEE 802.11 protocols, enabling more efficient and intelligent spectrum utilization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10143v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Miguel Casasnovas, Francesc Wilhelmi, Richard Combes, Maksymilian Wojnar, Katarzyna Kosek-Szott, Szymon Szott, Anders Jonsson, Luis Esteve, Boris Bellalta</dc:creator>
    </item>
    <item>
      <title>P4-TAS: P4-Based Time-Aware Shaper for Time-Sensitive Networking</title>
      <link>https://arxiv.org/abs/2511.10249</link>
      <description>arXiv:2511.10249v1 Announce Type: new 
Abstract: Time-Sensitive Networking (TSN) is a set of IEEE standards that extends Ethernet with real-time capabilities. Among its mechanisms, TSN can coordinate transmission times network-wide to minimize queueing, ensuring low latency and bounded delay. This coordination is computed offline and yields a network-wide schedule. The Time-Aware Shaper (TAS), implemented in TSN bridges, protects high-priority scheduled traffic from lower-priority (best-effort) flows by periodically opening and closing priority queues according to this schedule. Deterministic Networking (DetNet), standardized by the IETF, provides similar guarantees at Layer 3 and can leverage TSN mechanisms such as TAS for that purpose. Commercially available TSN-capable switches typically implement TAS in hardware but rarely disclose internal processing delays such as queue opening latency. Such information is essential for precise scheduling but largely unavailable to system designers. In this work, we present P4-TAS, a P4-based implementation of the TAS on a hardware switching ASIC. Our design introduces a novel approach for periodic queue control using a continuous stream of internally generated TAS control frames. We identify and quantify three sources of internal delay on a nanosecond scale which also exist in other implementations that directly affect the precision of executed schedules, providing transparency for future implementations and scheduling algorithms. Moreover, we provide an MPLS/TSN translation layer that enables P4-TAS to operate within DetNet environments, allowing TSN time-based traffic shaping to be carried over high-speed 400 Gb/s forwarding. Finally, we evaluate the scalability of P4-TAS and compare it to available TAS implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10249v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabian Ihle, Moritz Fl\"uchter, Michael Menth</dc:creator>
    </item>
    <item>
      <title>Towards an Agentic Workflow for Internet Measurement Research</title>
      <link>https://arxiv.org/abs/2511.10611</link>
      <description>arXiv:2511.10611v1 Announce Type: new 
Abstract: Internet measurement research faces an accessibility crisis: complex analyses require custom integration of multiple specialized tools that demands specialized domain expertise. When network disruptions occur, operators need rapid diagnostic workflows spanning infrastructure mapping, routing analysis, and dependency modeling. However, developing these workflows requires specialized knowledge and significant manual effort.
  We present ArachNet, the first system demonstrating that LLM agents can independently generate measurement workflows that mimics expert reasoning. Our core insight is that measurement expertise follows predictable compositional patterns that can be systematically automated. ArachNet operates through four specialized agents that mirror expert workflow, from problem decomposition to solution implementation. We validate ArachNet with progressively challenging Internet resilience scenarios. The system independently generates workflows that match expert-level reasoning and produce analytical outputs similar to specialist solutions. Generated workflows handle complex multi-framework integration that traditionally requires days of manual coordination. ArachNet lowers barriers to measurement workflow composition by automating the systematic reasoning process that experts use, enabling broader access to sophisticated measurement capabilities while maintaining the technical rigor required for research-quality analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10611v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alagappan Ramanathan, Eunju Kang, Dongsu Han, Sangeetha Abdu Jyothi</dc:creator>
    </item>
    <item>
      <title>Slice-Aware Spoofing Detection in 5G Networks Using Lightweight Machine Learning</title>
      <link>https://arxiv.org/abs/2511.09610</link>
      <description>arXiv:2511.09610v1 Announce Type: cross 
Abstract: The increasing virtualization of fifth generation (5G) networks expands the attack surface of the user plane, making spoofing a persistent threat to slice integrity and service reliability. This study presents a slice-aware lightweight machine-learning framework for detecting spoofing attacks within 5G network slices. The framework was implemented on a reproducible Open5GS and srsRAN testbed emulating three service classes such as enhanced Mobile Broadband (eMBB), Ultra-Reliable Low-Latency Communication (URLLC), and massive Machine-Type Communication (mMTC) under controlled benign and adversarial traffic. Two efficient classifiers, Logistic Regression and Random Forest, were trained independently for each slice using statistical flow features derived from mirrored user-plane traffic. Slice-aware training improved detection accuracy by up to 5% and achieved F1-scores between 0.93 and 0.96 while maintaining real-time operation on commodity edge hardware. The results demonstrate that aligning security intelligence with slice boundaries enhances detection reliability and preserves operational isolation, enabling practical deployment in 5G network-security environments. Conceptually, the work bridges network-security architecture and adaptive machine learning by showing that isolation-aware intelligence can achieve scalable, privacy-preserving spoofing defense without high computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09610v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniyal Ganiuly, Nurzhau Bolatbek</dc:creator>
    </item>
    <item>
      <title>Privacy-Preserving Explainable AIoT Application via SHAP Entropy Regularization</title>
      <link>https://arxiv.org/abs/2511.09775</link>
      <description>arXiv:2511.09775v1 Announce Type: cross 
Abstract: The widespread integration of Artificial Intelligence of Things (AIoT) in smart home environments has amplified the demand for transparent and interpretable machine learning models. To foster user trust and comply with emerging regulatory frameworks, the Explainable AI (XAI) methods, particularly post-hoc techniques such as SHapley Additive exPlanations (SHAP), and Local Interpretable Model-Agnostic Explanations (LIME), are widely employed to elucidate model behavior. However, recent studies have shown that these explanation methods can inadvertently expose sensitive user attributes and behavioral patterns, thereby introducing new privacy risks. To address these concerns, we propose a novel privacy-preserving approach based on SHAP entropy regularization to mitigate privacy leakage in explainable AIoT applications. Our method incorporates an entropy-based regularization objective that penalizes low-entropy SHAP attribution distributions during training, promoting a more uniform spread of feature contributions. To evaluate the effectiveness of our approach, we developed a suite of SHAP-based privacy attacks that strategically leverage model explanation outputs to infer sensitive information. We validate our method through comparative evaluations using these attacks alongside utility metrics on benchmark smart home energy consumption datasets. Experimental results demonstrate that SHAP entropy regularization substantially reduces privacy leakage compared to baseline models, while maintaining high predictive accuracy and faithful explanation fidelity. This work contributes to the development of privacy-preserving explainable AI techniques for secure and trustworthy AIoT applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09775v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dilli Prasad Sharma, Xiaowei Sun, Liang Xue, Xiaodong Lin, Pulei Xiong</dc:creator>
    </item>
    <item>
      <title>Dynamic Edge Server Selection in Time-Varying Environments: A Reliability-Aware Predictive Approach</title>
      <link>https://arxiv.org/abs/2511.10146</link>
      <description>arXiv:2511.10146v1 Announce Type: cross 
Abstract: Latency-sensitive embedded applications increasingly rely on edge computing, yet dynamic network congestion in multi-server architectures challenges proper edge server selection. This paper proposes a lightweight server-selection method for edge applications that fuses latency prediction with adaptive reliability and hysteresis-based handover. Using passive measurements (arrival rate, utilization, payload size) and an exponentially modulated rational delay model, the proposed Moderate Handover (MO-HAN) method computes a score that balances predicted latency and reliability to ensure handovers occur only when the expected gain is meaningful and maintain reduced end-to-end latency. Results show that MO-HAN consistently outperforms static and fair-distribution baselines by lowering mean and tail latencies, while reducing handovers by nearly 50% compared to pure opportunistic selection. These gains arise without intrusive instrumentation or heavy learning infrastructure, making MO-HAN practical for resource-constrained embedded devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10146v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jaime Sebastian Burbano, Arnova Abdullah, Eldiyar Zhantileuov, Mohan Liyanage, Rolf Schuster</dc:creator>
    </item>
    <item>
      <title>Pk-IOTA: Blockchain empowered Programmable Data Plane to secure OPC UA communications in Industry 4.0</title>
      <link>https://arxiv.org/abs/2511.10248</link>
      <description>arXiv:2511.10248v1 Announce Type: cross 
Abstract: The OPC UA protocol is becoming the de facto standard for Industry 4.0 machine-to-machine communication. It stands out as one of the few industrial protocols that provide robust security features designed to prevent attackers from manipulating and damaging critical infrastructures. However, prior works showed that significant challenges still exists to set up secure OPC UA deployments in practice, mainly caused by the complexity of certificate management in industrial scenarios and the inconsistent implementation of security features across industrial OPC UA devices. In this paper, we present Pk-IOTA, an automated solution designed to secure OPC UA communications by integrating programmable data plane switches for in-network certificate validation and leveraging the IOTA Tangle for decen- tralized certificate distribution. Our evaluation is performed on a physical testbed representing a real-world industrial scenario and shows that Pk-IOTA introduces a minimal overhead while providing a scalable and tamper-proof mechanism for OPC UA certificate management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10248v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rinieri Lorenzo, Gori Giacomo, Melis Andrea, Girau Roberto, Prandini Marco, Callegati Franco</dc:creator>
    </item>
    <item>
      <title>Causal Model-Based Reinforcement Learning for Sample-Efficient IoT Channel Access</title>
      <link>https://arxiv.org/abs/2511.10291</link>
      <description>arXiv:2511.10291v1 Announce Type: cross 
Abstract: Despite the advantages of multi-agent reinforcement learning (MARL) for wireless use case such as medium access control (MAC), their real-world deployment in Internet of Things (IoT) is hindered by their sample inefficiency. To alleviate this challenge, one can leverage model-based reinforcement learning (MBRL) solutions, however, conventional MBRL approaches rely on black-box models that are not interpretable and cannot reason. In contrast, in this paper, a novel causal model-based MARL framework is developed by leveraging tools from causal learn- ing. In particular, the proposed model can explicitly represent causal dependencies between network variables using structural causal models (SCMs) and attention-based inference networks. Interpretable causal models are then developed to capture how MAC control messages influence observations, how transmission actions determine outcomes, and how channel observations affect rewards. Data augmentation techniques are then used to generate synthetic rollouts using the learned causal model for policy optimization via proximal policy optimization (PPO). Analytical results demonstrate exponential sample complexity gains of causal MBRL over black-box approaches. Extensive simulations demonstrate that, on average, the proposed approach can reduce environment interactions by 58%, and yield faster convergence compared to model-free baselines. The proposed approach inherently is also shown to provide interpretable scheduling decisions via attention-based causal attribution, revealing which network conditions drive the policy. The resulting combination of sample efficiency and interpretability establishes causal MBRL as a practical approach for resource-constrained wireless systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10291v1</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aswin Arun, Christo Kurisummoottil Thomas, Rimalpudi Sarvendranath, Walid Saad</dc:creator>
    </item>
    <item>
      <title>Proactive Service Assurance in 5G and B5G Networks: A Closed-Loop Algorithm for End-to-End Network Slicing</title>
      <link>https://arxiv.org/abs/2404.01523</link>
      <description>arXiv:2404.01523v4 Announce Type: replace 
Abstract: The customization of services in Fifth-generation (5G) and Beyond 5G (B5G) networks relies heavily on network slicing, which creates multiple virtual networks on a shared physical infrastructure, tailored to meet specific requirements of distinct applications, using Software Defined Networking (SDN) and Network Function Virtualization (NFV). It is imperative to ensure that network services meet the performance and reliability requirements of various applications and users; thus, service assurance is one of the critical components in network slicing. One of the key functionalities of network slicing is the ability to scale Virtualized Network Functions (VNFs) in response to changing resource demand and to meet Customer Service Level agreements (SLAs). In this paper, we introduce a proactive closed-loop algorithm for end-to-end network orchestration, designed to provide service assurance in 5G and B5G networks. We focus on dynamically scaling resources to meet key performance indicators (KPIs) specific to each network slice and operate in parallel across multiple slices, making it scalable and capable of managing completely automatically real-time service assurance. Through our experiments, we demonstrate that the proposed algorithm effectively fulfills service assurance requirements for different network slice types, thereby minimizing network resource utilization and reducing the over-provisioning of spare resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01523v4</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nguyen Phuc Tran, Oscar Delgado, Brigitte Jaumard</dc:creator>
    </item>
    <item>
      <title>Multi-Level Damage-Aware Graph Learning for Resilient UAV Swarm Networks</title>
      <link>https://arxiv.org/abs/2506.09703</link>
      <description>arXiv:2506.09703v3 Announce Type: replace 
Abstract: Unmanned aerial vehicle (UAV) swarm networks leverage resilient algorithms to restore connectivity from communication network split issues. However, existing graph learning-based approaches face over-aggregation and non-convergence problems caused by uneven and sparse topology under massive damage. In this paper, we propose a novel Multi-Level Damage-Aware (MLDA) Graph Learning algorithm to generate recovery solutions, explicitly utilizing information about destroyed nodes to guide the recovery process. The algorithm first employs a Multi-Branch Damage Attention (MBDA) module as a pre-processing step, focusing attention on the critical relationships between remaining nodes and destroyed nodes in the global topology. By expanding multi-hop neighbor receptive fields of nodes to those damaged areas, it effectively mitigating the initial sparsity and unevenness before graph learning commences. Second, a Dilated Graph Convolution Network (DGCN) is designed to perform convolution on the MBDA-processed bipartite graphs between remaining and destroyed nodes. The DGCN utilizes a specialized bipartite graph convolution operation to aggregate features and incorporates a residual-connected architecture to extend depth, directly generating the target locations for recovery. We theoretically proved the convergence of the proposed algorithm and the computational complexity is acceptable. Simulation results show that the proposed algorithm can guarantee the connectivity restoration with excellent scalability, while significantly expediting the recovery time and improving the topology uniformity after recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09703v3</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huan Lin, Chenguang Zhu, Lianghui Ding, Lin Wang, Feng Yang</dc:creator>
    </item>
    <item>
      <title>On the Power Saving in High-Speed Ethernet-based Networks for Supercomputers and Data Centers</title>
      <link>https://arxiv.org/abs/2510.19783</link>
      <description>arXiv:2510.19783v2 Announce Type: replace 
Abstract: The increase in computation and storage has led to a significant growth in the scale of systems powering applications and services, raising concerns about sustainability and operational costs. In this paper, we explore power-saving techniques in high-performance computing (HPC) and datacenter networks, and their relation with performance degradation. From this premise, we propose leveraging Energy Efficient Ethernet (EEE), with the flexibility to extend to conventional Ethernet or upcoming Ethernet-derived interconnect versions of BXI and Omnipath.
  We analyze the PerfBound proposal, identifying possible improvements and modeling it into a simulation framework. Through different experiments, we examine its impact on performance and determine the most appropriate interconnect. We also study traffic patterns generated by selected HPC and machine learning applications to evaluate the behavior of power-saving techniques.
  From these experiments, we provide an analysis of how applications affect system and network energy consumption. Based on this, we disclose the weakness of dynamic power-down mechanisms and propose an approach that improves energy reduction with minimal or no performance penalty. To our knowledge, this is the first power management proposal tailored to future Ethernet-based HPC architectures, with promising results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19783v2</guid>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Miguel S\'anchez de la Rosa, Francisco J. and\'ujar, Jesus Escudero-Sahuquillo, Jos\'e L. S\'anchez, Francisco J. Alfaro-Cort\'es</dc:creator>
    </item>
    <item>
      <title>Cross-link RTS/CTS for MLO mm-Wave WLANs</title>
      <link>https://arxiv.org/abs/2511.05027</link>
      <description>arXiv:2511.05027v2 Announce Type: replace 
Abstract: The directional RTS/CTS mechanism of mm-wave Wi-Fi hardly resolves the hidden terminal problem perfectly. This paper proposes cross-link RTS/CTS under multi-link operation (MLO) to address this problem and introduces a novel point process, named the generalized RTS/CTS hard-core process (G-HCP), to model the spatial transceiver relationships under the RTS/CTS mechanism, including the directional case and the omnidirectional case. Analytical expressions are derived for the intensity, the mean interference, an approximation of the success probability, and the expected number of hidden nodes for the directional RTS/CTS mechanism. Theoretical and numerical results demonstrate the performance difference between two RTS/CTS mechanisms. The cross-link RTS/CTS mechanism ensures higher link quality at the cost of reduced network throughput. In contrast, the directional RTS/CTS sacrifices the link quality for higher throughput. Our study reveals a fundamental trade-off between link reliability and network throughput, providing critical insights into the selection and optimization of RTS/CTS mechanisms in next-generation WLAN standards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05027v2</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhuoling Chen, Yi Zhong, Martin Haenggi</dc:creator>
    </item>
    <item>
      <title>Learning-based Radio Link Failure Prediction Based on Measurement Dataset in Railway Environments</title>
      <link>https://arxiv.org/abs/2511.08851</link>
      <description>arXiv:2511.08851v2 Announce Type: replace 
Abstract: In this paper, a measurement-driven framework is proposed for early radio link failure (RLF) prediction in 5G non-standalone (NSA) railway environments. Using 10 Hz metro-train traces with serving and neighbor-cell indicators, we benchmark six models, namely CNN, LSTM, XGBoost, Anomaly Transformer, PatchTST, and TimesNet, under varied observation windows and prediction horizons. When the observation window is three seconds, TimesNet attains the highest F1 score with a three-second prediction horizon, while CNN provides a favorable accuracy-latency tradeoff with a two-second horizon, enabling proactive actions such as redundancy and adaptive handovers. The results indicate that deep temporal models can anticipate reliability degradations several seconds in advance using lightweight features available on commercial devices, offering a practical path to early-warning control in 5G-based railway systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08851v2</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Po-Heng Chou, Da-Chih Lin, Hung-Yu Wei, Walid Saad, Yu Tsao</dc:creator>
    </item>
    <item>
      <title>Hierarchical Reinforcement Learning for Integrated Cloud-Fog-Edge Computing in IoT Systems</title>
      <link>https://arxiv.org/abs/2511.09006</link>
      <description>arXiv:2511.09006v2 Announce Type: replace 
Abstract: The Internet of Things (IoT) is transforming industries by connecting billions of devices to collect, process, and share data. However, the massive data volumes and real-time demands of IoT applications strain traditional cloud computing architectures. This paper explores the complementary roles of cloud, fog, and edge computing in enhancing IoT performance, focusing on their ability to reduce latency, improve scalability, and ensure data privacy. We propose a novel framework, the Hierarchical IoT Processing Architecture (HIPA), which dynamically allocates computational tasks across cloud, fog, and edge layers using machine learning. By synthesizing current research and introducing HIPA, this paper highlights how these paradigms can create efficient, secure, and scalable IoT ecosystems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09006v2</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ameneh Zarei, Mahmood Ahmadi, Farhad Mardukhi</dc:creator>
    </item>
  </channel>
</rss>
