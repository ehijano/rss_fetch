<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.ST</link>
    <description>q-fin.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Jun 2025 01:35:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>DELPHYNE: A Pre-Trained Model for General and Financial Time Series</title>
      <link>https://arxiv.org/abs/2506.06288</link>
      <description>arXiv:2506.06288v1 Announce Type: new 
Abstract: Time-series data is a vital modality within data science communities. This is particularly valuable in financial applications, where it helps in detecting patterns, understanding market behavior, and making informed decisions based on historical data. Recent advances in language modeling have led to the rise of time-series pre-trained models that are trained on vast collections of datasets and applied to diverse tasks across financial domains. However, across financial applications, existing time-series pre-trained models have not shown boosts in performance over simple finance benchmarks in both zero-shot and fine-tuning settings. This phenomenon occurs because of a i) lack of financial data within the pre-training stage, and ii) the negative transfer effect due to inherently different time-series patterns across domains. Furthermore, time-series data is continuous, noisy, and can be collected at varying frequencies and with varying lags across different variables, making this data more challenging to model than languages. To address the above problems, we introduce a Pre-trained MoDEL for FINance TimE-series (Delphyne). Delphyne achieves competitive performance to existing foundation and full-shot models with few fine-tuning steps on publicly available datasets, and also shows superior performances on various financial tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06288v1</guid>
      <category>q-fin.ST</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xueying Ding, Aakriti Mittal, Achintya Gopal</dc:creator>
    </item>
    <item>
      <title>A Sinusoidal Hull-White Model for Interest Rate Dynamics: Capturing Long-Term Periodicity in U.S. Treasury Yields</title>
      <link>https://arxiv.org/abs/2506.06317</link>
      <description>arXiv:2506.06317v1 Announce Type: new 
Abstract: This study is motivated by empirical observations of periodic fluctuations in interest rates, notably long-term economic cycles spanning decades, which the conventional Hull-White short-rate model fails to adequately capture. To address this limitation, we propose an extension that incorporates a sinusoidal, time-varying mean reversion speed, allowing the model to reflect cyclic interest rate dynamics more effectively.
  The model is calibrated using a comprehensive dataset of daily U.S. Treasury yield curves obtained from the Federal Reserve Economic Data (FRED) database, covering the period from January 1990 to December 2022. The dataset includes tenors of 1, 2, 3, 5, 7, 10, 20, and 30 years, with the most recent yields ranging from 1.22% (1-year) to 2.36% (30-year).
  Calibration is performed using the Nelder-Mead optimization algorithm, and Monte Carlo simulations with 200 paths and a time step of 0.05 years. The resulting 30-year zero-coupon bond price under the proposed model is 0.43, compared to 0.47 under the standard Hull-White model. This corresponds to root mean squared errors of 0.12% and 0.14%, respectively, indicating a noticeable improvement in fit, particularly for longer maturities.
  These results highlight the model's enhanced capability to capture long-term yield dynamics and suggest significant implications for bond pricing, interest rate risk management, and the valuation of interest rate derivatives. The findings also open avenues for further research into stochastic periodicity and alternative interest rate modeling frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06317v1</guid>
      <category>q-fin.ST</category>
      <category>q-fin.CP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amit Kumar Jha</dc:creator>
    </item>
    <item>
      <title>The Hype Index: an NLP-driven Measure of Market News Attention</title>
      <link>https://arxiv.org/abs/2506.06329</link>
      <description>arXiv:2506.06329v1 Announce Type: new 
Abstract: This paper introduces the Hype Index as a novel metric to quantify media attention toward large-cap equities, leveraging advances in Natural Language Processing (NLP) for extracting predictive signals from financial news. Using the S&amp;P 100 as the focus universe, we first construct a News Count-Based Hype Index, which measures relative media exposure by computing the share of news articles referencing each stock or sector. We then extend it to the Capitalization Adjusted Hype Index, adjusts for economic size by taking the ratio of a stock's or sector's media weight to its market capitalization weight within its industry or sector. We compute both versions of the Hype Index at the stock and sector levels, and evaluate them through multiple lenses: (1) their classification into different hype groups, (2) their associations with returns, volatility, and VIX index at various lags, (3) their signaling power for short-term market movements, and (4) their empirical properties including correlations, samplings, and trends. Our findings suggest that the Hype Index family provides a valuable set of tools for stock volatility analysis, market signaling, and NLP extensions in Finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06329v1</guid>
      <category>q-fin.ST</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zheng Cao, Wanchaloem Wunkaew, Helyette Geman</dc:creator>
    </item>
    <item>
      <title>Explainable-AI powered stock price prediction using time series transformers: A Case Study on BIST100</title>
      <link>https://arxiv.org/abs/2506.06345</link>
      <description>arXiv:2506.06345v1 Announce Type: new 
Abstract: Financial literacy is increasingly dependent on the ability to interpret complex financial data and utilize advanced forecasting tools. In this context, this study proposes a novel approach that combines transformer-based time series models with explainable artificial intelligence (XAI) to enhance the interpretability and accuracy of stock price predictions. The analysis focuses on the daily stock prices of the five highest-volume banks listed in the BIST100 index, along with XBANK and XU100 indices, covering the period from January 2015 to March 2025. Models including DLinear, LTSNet, Vanilla Transformer, and Time Series Transformer are employed, with input features enriched by technical indicators. SHAP and LIME techniques are used to provide transparency into the influence of individual features on model outputs. The results demonstrate the strong predictive capabilities of transformer models and highlight the potential of interpretable machine learning to empower individuals in making informed investment decisions and actively engaging in financial markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06345v1</guid>
      <category>q-fin.ST</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sukru Selim Calik, Andac Akyuz, Zeynep Hilal Kilimci, Kerem Colak</dc:creator>
    </item>
    <item>
      <title>An analysis of capital market through the lens of integral transforms: exploring efficient markets and information asymmetry</title>
      <link>https://arxiv.org/abs/2506.06350</link>
      <description>arXiv:2506.06350v1 Announce Type: new 
Abstract: Post Modigliani and Miller (1958), the concept of usage of arbitrage created a permanent mark on the discourses of financial framework. The arbitrage process is largely based on information dissemination amongst the stakeholders operating in the financial market. The advent of the efficient market Hypothesis draws close to the M&amp;M hypothesis. Giving importance to the arbitrage process, which effects the price discovery in the stock market. This divided the market as random and efficient cohort system. The focus was on which information forms a key factor in deciding the price formation in the market. However, the conventional techniques of analysis do not permit the price cycles to be interpreted beyond its singular wave-like cyclical movement. The apparent cyclic measurement is not coherent as the technical analysis does not give sustained result. Hence adaption of theories and computation from mathematical methods of physics ensures that these cycles are decomposed and the effect of the broken-down cycles is interpreted to understand the overall effect of information on price formation and discovery. In order to break the cycle this paper uses spectrum analysis to decompose and understand the above-said phenomenon in determining the price behavior in National Stock Exchange of India (NSE).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06350v1</guid>
      <category>q-fin.ST</category>
      <category>math.SP</category>
      <category>physics.comp-ph</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kiran Sharma, Abhijit Dutta, Rupak Mukherjee</dc:creator>
    </item>
    <item>
      <title>Towards Competent AI for Fundamental Analysis in Finance: A Benchmark Dataset and Evaluation</title>
      <link>https://arxiv.org/abs/2506.07315</link>
      <description>arXiv:2506.07315v1 Announce Type: new 
Abstract: Generative AI, particularly large language models (LLMs), is beginning to transform the financial industry by automating tasks and helping to make sense of complex financial information. One especially promising use case is the automatic creation of fundamental analysis reports, which are essential for making informed investment decisions, evaluating credit risks, guiding corporate mergers, etc. While LLMs attempt to generate these reports from a single prompt, the risks of inaccuracy are significant. Poor analysis can lead to misguided investments, regulatory issues, and loss of trust. Existing financial benchmarks mainly evaluate how well LLMs answer financial questions but do not reflect performance in real-world tasks like generating financial analysis reports. In this paper, we propose FinAR-Bench, a solid benchmark dataset focusing on financial statement analysis, a core competence of fundamental analysis. To make the evaluation more precise and reliable, we break this task into three measurable steps: extracting key information, calculating financial indicators, and applying logical reasoning. This structured approach allows us to objectively assess how well LLMs perform each step of the process. Our findings offer a clear understanding of LLMs current strengths and limitations in fundamental analysis and provide a more practical way to benchmark their performance in real-world financial settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07315v1</guid>
      <category>q-fin.ST</category>
      <category>cs.AI</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zonghan Wu, Junlin Wang, Congyuan Zou, Chenhan Wang, Yilei Shao</dc:creator>
    </item>
    <item>
      <title>Predicting Realized Variance Out of Sample: Can Anything Beat The Benchmark?</title>
      <link>https://arxiv.org/abs/2506.07928</link>
      <description>arXiv:2506.07928v1 Announce Type: new 
Abstract: The discrepancy between realized volatility and the market's view of volatility has been known to predict individual equity options at the monthly horizon. It is not clear how this predictability depends on a forecast's ability to predict firm-level volatility. We consider this phenomenon at the daily frequency using high-dimensional machine learning models, as well as low-dimensional factor models. We find that marginal improvements to standard forecast error measurements can lead to economically significant gains in portfolio performance. This makes a case for re-imagining the way we train models that are used to construct portfolios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07928v1</guid>
      <category>q-fin.ST</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Austin Pollok</dc:creator>
    </item>
    <item>
      <title>The Subtle Interplay between Square-root Impact, Order Imbalance &amp; Volatility: A Unifying Framework</title>
      <link>https://arxiv.org/abs/2506.07711</link>
      <description>arXiv:2506.07711v2 Announce Type: cross 
Abstract: In this work, we aim to reconcile several apparently contradictory observations in market microstructure: is the famous ''square-root law'' of metaorder impact that decays with time compatible with the random-walk nature of prices and the linear impact of order imbalances? Can one entirely explain the volatility of prices as resulting from the flow of uninformed metaorders that mechanically impact prices? We introduce a new theoretical framework to describe metaorders with different signs, sizes and durations, which all impact prices as a square-root of volume but with a subsequent time decay. We show that, as in the original propagator model, price diffusion is ensured by the long memory of cross-correlations between metaorders. In order to account for the effect of strongly fluctuating volumes $q$ of individual trades, we further introduce two $q$-dependent exponents, which allows us to account for the way the moments of generalized volume imbalance and the correlation between price changes and generalized order flow imbalance scales with $T$. We predict in particular that the corresponding power-laws depend in a non-monotonic fashion on a parameter $a$ that allows one to put the same weight on all child orders or overweight large orders, a behaviour clearly borne out by empirical data. We also predict that the correlation between price changes and volume imbalances should display a maximum as a function of $a$, which again matches observations. Such noteworthy agreement between theory and data suggests that our framework correctly captures the basic mechanism at the heart of price formation, namely the average impact of metaorders. We argue that our results support the ''Order-Driven'' theory of excess volatility, and are at odds with the idea that a ''Fundamental'' component accounts for a large share of the volatility of financial markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07711v2</guid>
      <category>q-fin.TR</category>
      <category>q-fin.ST</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guillaume Maitrier, Jean-Philippe Bouchaud</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Learning in Finance</title>
      <link>https://arxiv.org/abs/2506.03780</link>
      <description>arXiv:2506.03780v2 Announce Type: replace 
Abstract: Recent advances in machine learning have shown promising results for financial prediction using large, over-parameterized models. This paper provides theoretical foundations and empirical validation for understanding when and how these methods achieve predictive success. I examine two key aspects of high-dimensional learning in finance. First, I prove that within-sample standardization in Random Fourier Features implementations fundamentally alters the underlying Gaussian kernel approximation, replacing shift-invariant kernels with training-set dependent alternatives. Second, I establish information-theoretic lower bounds that identify when reliable learning is impossible no matter how sophisticated the estimator. A detailed quantitative calibration of the polynomial lower bound shows that with typical parameter choices, e.g., 12,000 features, 12 monthly observations, and R-square 2-3%, the required sample size to escape the bound exceeds 25-30 years of data--well beyond any rolling-window actually used. Thus, observed out-of-sample success must originate from lower-complexity artefacts rather than from the intended high-dimensional mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03780v2</guid>
      <category>q-fin.ST</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hasan Fallahgoul</dc:creator>
    </item>
  </channel>
</rss>
