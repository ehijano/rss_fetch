<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>eess.IV updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/eess.IV</link>
    <description>eess.IV updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/eess.IV" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 06 Oct 2025 04:00:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Learning a distance measure from the information-estimation geometry of data</title>
      <link>https://arxiv.org/abs/2510.02514</link>
      <description>arXiv:2510.02514v1 Announce Type: new 
Abstract: We introduce the Information-Estimation Metric (IEM), a novel form of distance function derived from an underlying continuous probability density over a domain of signals. The IEM is rooted in a fundamental relationship between information theory and estimation theory, which links the log-probability of a signal with the errors of an optimal denoiser, applied to noisy observations of the signal. In particular, the IEM between a pair of signals is obtained by comparing their denoising error vectors over a range of noise amplitudes. Geometrically, this amounts to comparing the score vector fields of the blurred density around the signals over a range of blur levels. We prove that the IEM is a valid global metric and derive a closed-form expression for its local second-order approximation, which yields a Riemannian metric. For Gaussian-distributed signals, the IEM coincides with the Mahalanobis distance. But for more complex distributions, it adapts, both locally and globally, to the geometry of the distribution. In practice, the IEM can be computed using a learned denoiser (analogous to generative diffusion models) and solving a one-dimensional integral. To demonstrate the value of our framework, we learn an IEM on the ImageNet database. Experiments show that this IEM is competitive with or outperforms state-of-the-art supervised image quality metrics in predicting human perceptual judgments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02514v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guy Ohayon, Pierre-Etienne H. Fiquet, Florentin Guth, Jona Ball\'e, Eero P. Simoncelli</dc:creator>
    </item>
    <item>
      <title>High Pixel Resolution Visible to Extended Shortwave Infrared Single Pixel Imaging with a black Phosphorus-Molybdenum disulfide (bP-MoS2) photodiode</title>
      <link>https://arxiv.org/abs/2510.02673</link>
      <description>arXiv:2510.02673v1 Announce Type: new 
Abstract: High-resolution infrared imagers are currently more expensive than CMOS and CCD cameras, due to costly sensor arrays. Van der Waals (vdWs) materials present an opportunity for low-cost, room temperature infrared photodetectors. Although photodetectors based on vdWs materials show promising performance, creating a megapixel array is yet to be achieved. Imaging with a single vdWs photodetector typically relies on time-consuming mechanical scanning and suffers from low resolution. Single pixel imaging (SPI) offers an affordable alternative to achieve high-resolution imaging, utilizing only one photodetector and a spatial light modulator. Progress in SPI using vdWs material photodetectors has been limited, with only one prior demonstration in the near infrared range (64$\times$64 pixels). In this work, we demonstrate a high-resolution SPI system (1023$\times$768 for visible light and 512$\times$512 for extended shortwave infrared) using a black phosphorus-molybdenum disulfide (bP-MoS$_2$) photodiode, surpassing earlier vdWs material SPI implementations by a factor of 64 in pixel count. We introduce an easy-to-implement edge detection method for rapid feature extraction. We employ compressed sampling and reduce imaging time by a factor of four. Our compressed sampling approach is based on a cyclic S-matrix, which is derived from a Hadamard-based sequence, where each row is a circular shift of the first row. This enables efficient imaging reconstruction via circular convolution and Fourier transforms, allowing fewer measurements while preserving the key image features. Our method for SPI using a vdWs material photodetector presents the opportunity for inexpensive shortwave infrared and midwave infrared cameras, and thus may enable advances in gas detection, biomedical imaging, autonomous driving, security, and surveillance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02673v1</guid>
      <category>eess.IV</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Seyed Saleh Mousavi Khaleghi, Jinyuan Chen, Sivacarendran Balendhran, Alexander Corletto, Shifan Wang, Huan Liu, James Bullock, Kenneth B. Crozier</dc:creator>
    </item>
    <item>
      <title>A UAV-Based VNIR Hyperspectral Benchmark Dataset for Landmine and UXO Detection</title>
      <link>https://arxiv.org/abs/2510.02700</link>
      <description>arXiv:2510.02700v1 Announce Type: new 
Abstract: This paper introduces a novel benchmark dataset of Visible and Near-Infrared (VNIR) hyperspectral imagery acquired via an unmanned aerial vehicle (UAV) platform for landmine and unexploded ordnance (UXO) detection research. The dataset was collected over a controlled test field seeded with 143 realistic surrogate landmine and UXO targets, including surface, partially buried, and fully buried configurations. Data acquisition was performed using a Headwall Nano-Hyperspec sensor mounted on a multi-sensor drone platform, flown at an altitude of approximately 20.6 m, capturing 270 contiguous spectral bands spanning 398-1002 nm. Radiometric calibration, orthorectification, and mosaicking were performed followed by reflectance retrieval using a two-point Empirical Line Method (ELM), with reference spectra acquired using an SVC spectroradiometer. Cross-validation against six reference objects yielded RMSE values below 1.0 and SAM values between 1 and 6 degrees in the 400-900 nm range, demonstrating high spectral fidelity. The dataset is released alongside raw radiance cubes, GCP/AeroPoint data, and reference spectra to support reproducible research. This contribution fills a critical gap in open-access UAV-based hyperspectral data for landmine detection and offers a multi-sensor benchmark when combined with previously published drone-based electromagnetic induction (EMI) data from the same test field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02700v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>eess.SP</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sagar Lekhak, Emmett J. Ientilucci, Jasper Baur, Susmita Ghosh</dc:creator>
    </item>
    <item>
      <title>Image Enhancement Based on Pigment Representation</title>
      <link>https://arxiv.org/abs/2510.02713</link>
      <description>arXiv:2510.02713v1 Announce Type: new 
Abstract: This paper presents a novel and efficient image enhancement method based on pigment representation. Unlike conventional methods where the color transformation is restricted to pre-defined color spaces like RGB, our method dynamically adapts to input content by transforming RGB colors into a high-dimensional feature space referred to as \textit{pigments}. The proposed pigment representation offers adaptability and expressiveness, achieving superior image enhancement performance. The proposed method involves transforming input RGB colors into high-dimensional pigments, which are then reprojected individually and blended to refine and aggregate the information of the colors in pigment spaces. Those pigments are then transformed back into RGB colors to generate an enhanced output image. The transformation and reprojection parameters are derived from the visual encoder which adaptively estimates such parameters based on the content in the input image. Extensive experimental results demonstrate the superior performance of the proposed method over state-of-the-art methods in image enhancement tasks, including image retouching and tone mapping, while maintaining relatively low computational complexity and small model size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02713v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Se-Ho Lee, Keunsoo Ko, Seung-Wook Kim</dc:creator>
    </item>
    <item>
      <title>GCVAMD: A Modified CausalVAE Model for Causal Age-related Macular Degeneration Risk Factor Detection and Prediction</title>
      <link>https://arxiv.org/abs/2510.02781</link>
      <description>arXiv:2510.02781v1 Announce Type: new 
Abstract: Age Related Macular Degeneration(AMD) has been one of the most leading causes of permanent vision impairment in ophthalmology. Though treatments, such as anti VEGF drugs or photodynamic therapies, were developed to slow down the degenerative process of AMD, there is still no specific cure to reverse vision loss caused by AMD. Thus, for AMD, detecting existence of risk factors of AMD or AMD itself within the patient retina in early stages is a crucial task to reduce the possibility of vision impairment. Apart from traditional approaches, deep learning based methods, especially attention mechanism based CNNs and GradCAM based XAI analysis on OCT scans, exhibited successful performance in distinguishing AMD retina from normal retinas, making it possible to use AI driven models to aid medical diagnosis and analysis by ophthalmologists regarding AMD. However, though having significant success, previous works mostly focused on prediction performance itself, not pathologies or underlying causal mechanisms of AMD, which can prohibit intervention analysis on specific factors or even lead to less reliable decisions. Thus, this paper introduces a novel causal AMD analysis model: GCVAMD, which incorporates a modified CausalVAE approach that can extract latent causal factors from only raw OCT images. By considering causality in AMD detection, GCVAMD enables causal inference such as treatment simulation or intervention analysis regarding major risk factors: drusen and neovascularization, while returning informative latent causal features that can enhance downstream tasks. Results show that through GCVAMD, drusen status and neovascularization status can be identified with AMD causal mechanisms in GCVAMD latent spaces, which can in turn be used for various tasks from AMD detection(classification) to intervention analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02781v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Daeyoung Kim</dc:creator>
    </item>
    <item>
      <title>Wave-GMS: Lightweight Multi-Scale Generative Model for Medical Image Segmentation</title>
      <link>https://arxiv.org/abs/2510.03216</link>
      <description>arXiv:2510.03216v1 Announce Type: new 
Abstract: For equitable deployment of AI tools in hospitals and healthcare facilities, we need Deep Segmentation Networks that offer high performance and can be trained on cost-effective GPUs with limited memory and large batch sizes. In this work, we propose Wave-GMS, a lightweight and efficient multi-scale generative model for medical image segmentation. Wave-GMS has a substantially smaller number of trainable parameters, does not require loading memory-intensive pretrained vision foundation models, and supports training with large batch sizes on GPUs with limited memory. We conducted extensive experiments on four publicly available datasets (BUS, BUSI, Kvasir-Instrument, and HAM10000), demonstrating that Wave-GMS achieves state-of-the-art segmentation performance with superior cross-domain generalizability, while requiring only ~2.6M trainable parameters. Code is available at https://github.com/ATPLab-LUMS/Wave-GMS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03216v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Talha Ahmed, Nehal Ahmed Shaikh, Hassan Mohy-ud-Din</dc:creator>
    </item>
    <item>
      <title>Hyperparameters are all you need: Using five-step inference for an original diffusion model to generate images comparable to the latest distillation model</title>
      <link>https://arxiv.org/abs/2510.02390</link>
      <description>arXiv:2510.02390v1 Announce Type: cross 
Abstract: The diffusion model is a state-of-the-art generative model that generates an image by applying a neural network iteratively. Moreover, this generation process is regarded as an algorithm solving an ordinary differential equation or a stochastic differential equation. Based on the analysis of the truncation error of the diffusion ODE and SDE, our study proposes a training-free algorithm that generates high-quality 512 x 512 and 1024 x 1024 images in eight steps, with flexible guidance scales. To the best of my knowledge, our algorithm is the first one that samples a 1024 x 1024 resolution image in 8 steps with an FID performance comparable to that of the latest distillation model, but without additional training. Meanwhile, our algorithm can also generate a 512 x 512 image in 8 steps, and its FID performance is better than the inference result using state-of-the-art ODE solver DPM++ 2m in 20 steps. We validate our eight-step image generation algorithm using the COCO 2014, COCO 2017, and LAION datasets. And our best FID performance is 15.7, 22.35, and 17.52. While the FID performance of DPM++2m is 17.3, 23.75, and 17.33. Further, it also outperforms the state-of-the-art AMED-plugin solver, whose FID performance is 19.07, 25.50, and 18.06. We also apply the algorithm in five-step inference without additional training, for which the best FID performance in the datasets mentioned above is 19.18, 23.24, and 19.61, respectively, and is comparable to the performance of the state-of-the-art AMED Pulgin solver in eight steps, SDXL-turbo in four steps, and the state-of-the-art diffusion distillation model Flash Diffusion in five steps. We also validate our algorithm in synthesizing 1024 * 1024 images within 6 steps, whose FID performance only has a limited distance to the latest distillation algorithm. The code is in repo: https://github.com/TheLovesOfLadyPurple/Hyperparameters-are-all-you-need</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02390v1</guid>
      <category>cs.GR</category>
      <category>cs.AI</category>
      <category>eess.IV</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zilai Li</dc:creator>
    </item>
    <item>
      <title>A Statistical Method for Attack-Agnostic Adversarial Attack Detection with Compressive Sensing Comparison</title>
      <link>https://arxiv.org/abs/2510.02707</link>
      <description>arXiv:2510.02707v1 Announce Type: cross 
Abstract: Adversarial attacks present a significant threat to modern machine learning systems. Yet, existing detection methods often lack the ability to detect unseen attacks or detect different attack types with a high level of accuracy. In this work, we propose a statistical approach that establishes a detection baseline before a neural network's deployment, enabling effective real-time adversarial detection. We generate a metric of adversarial presence by comparing the behavior of a compressed/uncompressed neural network pair. Our method has been tested against state-of-the-art techniques, and it achieves near-perfect detection across a wide range of attack types. Moreover, it significantly reduces false positives, making it both reliable and practical for real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02707v1</guid>
      <category>cs.CR</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chinthana Wimalasuriya, Spyros Tragoudas</dc:creator>
    </item>
    <item>
      <title>WaveNet-SF: A Hybrid Network for Retinal Disease Detection Based on Wavelet Transform in Spatial-Frequency Domain</title>
      <link>https://arxiv.org/abs/2501.11854</link>
      <description>arXiv:2501.11854v3 Announce Type: replace 
Abstract: Retinal diseases are a leading cause of vision impairment and blindness, with timely diagnosis being critical for effective treatment. Optical Coherence Tomography (OCT) has become a standard imaging modality for retinal disease diagnosis, but OCT images often suffer from issues such as speckle noise, complex lesion shapes, and varying lesion sizes, making interpretation challenging. In this paper, we propose a novel framework, WaveNet-SF, to enhance retinal disease detection by integrating the spatial-domain and frequency-domain learning. The framework utilizes wavelet transforms to decompose OCT images into low- and high-frequency components, enabling the model to extract both global structural features and fine-grained details. To improve lesion detection, we introduce a Multi-Scale Wavelet Spatial Attention (MSW-SA) module, which enhances the model's focus on regions of interest at multiple scales. Additionally, a High-Frequency Feature Compensation (HFFC) block is incorporated to recover edge information lost during wavelet decomposition, suppress noise, and preserve fine details crucial for lesion detection. Our approach achieves state-of-the-art (SOTA) classification accuracies of 97.82% and 99.58% on the OCT-C8 and OCT2017 datasets, respectively, surpassing existing methods. These results demonstrate the efficacy of WaveNet-SF in addressing the challenges of OCT image analysis and its potential as a powerful tool for retinal disease diagnosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11854v3</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jilan Cheng, Guoli Long, Zeyu Zhang, Zhenjia Qi, Hanyu Wang, Libin Lu, Shuihua Wang, Yudong Zhang, Jin Hong</dc:creator>
    </item>
    <item>
      <title>CostFilter-AD: Enhancing Anomaly Detection through Matching Cost Filtering</title>
      <link>https://arxiv.org/abs/2505.01476</link>
      <description>arXiv:2505.01476v4 Announce Type: replace 
Abstract: Unsupervised anomaly detection (UAD) seeks to localize the anomaly mask of an input image with respect to normal samples. Either by reconstructing normal counterparts (reconstruction-based) or by learning an image feature embedding space (embedding-based), existing approaches fundamentally rely on image-level or feature-level matching to derive anomaly scores. Often, such a matching process is inaccurate yet overlooked, leading to sub-optimal detection. To address this issue, we introduce the concept of cost filtering, borrowed from classical matching tasks, such as depth and flow estimation, into the UAD problem. We call this approach {\em CostFilter-AD}. Specifically, we first construct a matching cost volume between the input and normal samples, comprising two spatial dimensions and one matching dimension that encodes potential matches. To refine this, we propose a cost volume filtering network, guided by the input observation as an attention query across multiple feature layers, which effectively suppresses matching noise while preserving edge structures and capturing subtle anomalies. Designed as a generic post-processing plug-in, CostFilter-AD can be integrated with either reconstruction-based or embedding-based methods. Extensive experiments on MVTec-AD and VisA benchmarks validate the generic benefits of CostFilter-AD for both single- and multi-class UAD tasks. Code and models will be released at https://github.com/ZHE-SAPI/CostFilter-AD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01476v4</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhe Zhang, Mingxiu Cai, Hanxiao Wang, Gaochang Wu, Tianyou Chai, Xiatian Zhu</dc:creator>
    </item>
    <item>
      <title>Robust Pan-Cancer Mitotic Figure Detection with YOLOv12</title>
      <link>https://arxiv.org/abs/2509.02593</link>
      <description>arXiv:2509.02593v3 Announce Type: replace 
Abstract: Mitotic figures represent a key histoprognostic feature in tumor pathology, providing crucial insights into tumor aggressiveness and proliferation. However, their identification remains challenging, subject to significant inter-observer variability, even among experienced pathologists. To address this issue, the MItosis DOmain Generalization (MIDOG) 2025 challenge marks the third edition of an international competition aiming to develop robust mitosis detection algorithms. In this paper, we present a mitotic figure detection approach based on the state-of-the-art YOLOv12 object detection architecture. Our method achieved an F1-score of 0.801 on the preliminary test set (hotspots only) and ranked second on the final test leaderboard with an F1-score of 0.7216 across complex and heterogeneous whole-slide regions, without relying on external data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02593v3</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rapha\"el Bourgade, Guillaume Balezo, Thomas Walter</dc:creator>
    </item>
    <item>
      <title>Unified Domain Adaptive Semantic Segmentation</title>
      <link>https://arxiv.org/abs/2311.13254</link>
      <description>arXiv:2311.13254v5 Announce Type: replace-cross 
Abstract: Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS) aims to transfer the supervision from a labeled source domain to an unlabeled target domain. The majority of existing UDA-SS works typically consider images whilst recent attempts have extended further to tackle videos by modeling the temporal dimension. Although the two lines of research share the major challenges -- overcoming the underlying domain distribution shift, their studies are largely independent, resulting in fragmented insights, a lack of holistic understanding, and missed opportunities for cross-pollination of ideas. This fragmentation prevents the unification of methods, leading to redundant efforts and suboptimal knowledge transfer across image and video domains. Under this observation, we advocate unifying the study of UDA-SS across video and image scenarios, enabling a more comprehensive understanding, synergistic advancements, and efficient knowledge sharing. To that end, we explore the unified UDA-SS from a general data augmentation perspective, serving as a unifying conceptual framework, enabling improved generalization, and potential for cross-pollination of ideas, ultimately contributing to the overall progress and practical impact of this field of research. Specifically, we propose a Quad-directional Mixup (QuadMix) method, characterized by tackling distinct point attributes and feature inconsistencies through four-directional paths for intra- and inter-domain mixing in a feature space. To deal with temporal shifts with videos, we incorporate optical flow-guided feature aggregation across spatial and temporal dimensions for fine-grained domain alignment. Extensive experiments show that our method outperforms the state-of-the-art works by large margins on four challenging UDA-SS benchmarks. Our source code and models will be released at https://github.com/ZHE-SAPI/UDASS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13254v5</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>eess.IV</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TPAMI.2025.3562999</arxiv:DOI>
      <arxiv:journal_reference>Z. Zhang, G. Wu, J. Zhang, X. Zhu, D. Tao, and T. Chai, Unified Domain Adaptive Semantic Segmentation, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 47, no. 8, pp. 6731-6748, Aug. 2025</arxiv:journal_reference>
      <dc:creator>Zhe Zhang, Gaochang Wu, Jing Zhang, Xiatian Zhu, Dacheng Tao, Tianyou Chai</dc:creator>
    </item>
    <item>
      <title>Ultrasound matrix imaging for 3D transcranial in vivo localization microscopy</title>
      <link>https://arxiv.org/abs/2410.14499</link>
      <description>arXiv:2410.14499v2 Announce Type: replace-cross 
Abstract: Transcranial ultrasound imaging is usually limited by skull-induced attenuation and high-order aberrations. By using contrast agents such as microbubbles in combination with ultrafast imaging, not only can the signal-to-noise ratio be improved, but super-resolution images down to the micrometer scale of the brain vessels can also be obtained. However, ultrasound localization microscopy (ULM) remains affected by wavefront distortions that limit the microbubble detection rate and hamper their localization. In this work, we show how ultrasound matrix imaging, which relies on the prior recording of the reflection matrix, can provide a solution to these fundamental issues. As an experimental proof of concept, an in vivo reconstruction of deep brain microvessels is performed on three anesthetized sheep. The compensation of wave distortions is shown to markedly enhance the contrast and resolution of ULM. This experimental study thus opens up promising perspectives for a transcranial and nonionizing observation of human cerebral microvascular pathologies, such as stroke.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14499v2</guid>
      <category>physics.med-ph</category>
      <category>eess.IV</category>
      <category>physics.app-ph</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1126/sciadv.adt9778</arxiv:DOI>
      <arxiv:journal_reference>Sci. Adv.11, eadt9778, 2025</arxiv:journal_reference>
      <dc:creator>Flavien Bureau, Louise Denis, Antoine Coudert, Mathias Fink, Olivier Couture, Alexandre Aubry</dc:creator>
    </item>
    <item>
      <title>Detection and characterization of targets in complex media using fingerprint matrices</title>
      <link>https://arxiv.org/abs/2502.07052</link>
      <description>arXiv:2502.07052v2 Announce Type: replace-cross 
Abstract: When waves propagate through a complex medium, they undergo several scattering events. This phenomenon is detrimental to imaging, as it causes full blurring of the image. Here we describe a method for detecting, localizing and characterizing any scattering target embedded in a complex medium. We introduce a fingerprint operator that contains the specific signature of the target with respect to its environment. When applied to the recorded reflection matrix, it provides a likelihood index of the target state. This state can be the position of the target for localization purposes, its shape for characterization or any other parameter that influences its response. We demonstrate the versatility of our method by performing proof-of-concept ultrasound experiments on elastic spheres buried inside a strongly scattering granular suspension and on lesion markers, which are commonly used to monitor breast tumours, embedded in a foam mimicking soft tissue. Furthermore, we show how the fingerprint operator can be leveraged to characterize the complex medium itself by mapping the fibre architecture within muscle tissue. Our method is broadly applicable to different types of waves beyond ultrasound for which multi-element technology allows a reflection matrix to be measured.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07052v2</guid>
      <category>physics.app-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>eess.IV</category>
      <category>physics.optics</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1038/s41567-025-03016-2</arxiv:DOI>
      <arxiv:journal_reference>Nature Physics, 2025</arxiv:journal_reference>
      <dc:creator>Arthur Le Ber, Antton Go\"icoechea, Lukas M. Rachbauer, William Lambert, Xiaoping Jia, Mathias Fink, Arnaud Tourin, Stefan Rotter, Alexandre Aubry</dc:creator>
    </item>
    <item>
      <title>Theoretical Model of Microparticle-Assisted Super-Resolution Microscopy</title>
      <link>https://arxiv.org/abs/2504.10268</link>
      <description>arXiv:2504.10268v4 Announce Type: replace-cross 
Abstract: We present the first three-dimensional theoretical model of microparticle-assisted super-resolution imaging, enabling accurate simulation of virtual image formation. The model reveals that accounting for partial spatial coherence of illumination is a fundamental prerequisite for achieving super-resolution. We also propose a novel illumination strategy based on suppressing the normal component of incident light, which enhances image contrast and resolution. The results establish a consistent wave-optical framework that reproduces experimentally observed subwavelength imaging and clarifies the underlying physical mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10268v4</guid>
      <category>physics.optics</category>
      <category>eess.IV</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. R Bekirov</dc:creator>
    </item>
    <item>
      <title>Enhancing Monocular Height Estimation via Sparse LiDAR-Guided Correction</title>
      <link>https://arxiv.org/abs/2505.06905</link>
      <description>arXiv:2505.06905v2 Announce Type: replace-cross 
Abstract: Monocular height estimation (MHE) from very-high-resolution (VHR) remote sensing imagery via deep learning is notoriously challenging due to the lack of sufficient structural information. Conventional digital elevation models (DEMs), typically derived from airborne LiDAR or multi-view stereo, remain costly and geographically limited. Recently, models trained on synthetic data and refined through domain adaptation have shown remarkable performance in MHE, yet it remains unclear how these models make predictions or how reliable they truly are. In this paper, we investigate a state-of-the-art MHE model trained purely on synthetic data to explore where the model looks when making height predictions. Through systematic analyses, we find that the model relies heavily on shadow cues, a factor that can lead to overestimation or underestimation of heights when shadows deviate from expected norms. Furthermore, the inherent difficulty of evaluating regression tasks with the human eye underscores additional limitations of purely synthetic training. To address these issues, we propose a novel correction pipeline that integrates sparse, imperfect global LiDAR measurements (ICESat-2) with deep-learning outputs to improve local accuracy and achieve spatially consistent corrections. Our method comprises two stages: pre-processing raw ICESat-2 data, followed by a random forest-based approach to densely refine height estimates. Experiments in three representative urban regions -- Saint-Omer, Tokyo, and Sao Paulo -- reveal substantial error reductions, with mean absolute error (MAE) decreased by 22.8\%, 6.9\%, and 4.9\%, respectively. These findings highlight the critical role of shadow awareness in synthetic data-driven models and demonstrate how fusing imperfect real-world LiDAR data can bolster the robustness of MHE, paving the way for more reliable and scalable 3D mapping solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06905v2</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jian Song, Hongruixuan Chen, Naoto Yokoya</dc:creator>
    </item>
    <item>
      <title>HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios</title>
      <link>https://arxiv.org/abs/2506.09650</link>
      <description>arXiv:2506.09650v2 Announce Type: replace-cross 
Abstract: Action segmentation is a core challenge in high-level video understanding, aiming to partition untrimmed videos into segments and assign each a label from a predefined action set. Existing methods primarily address single-person activities with fixed action sequences, overlooking multi-person scenarios. In this work, we pioneer textual reference-guided human action segmentation in multi-person settings, where a textual description specifies the target person for segmentation. We introduce the first dataset for Referring Human Action Segmentation, i.e., RHAS133, built from 133 movies and annotated with 137 fine-grained actions with 33h video data, together with textual descriptions for this new task. Benchmarking existing action segmentation methods on RHAS133 using VLM-based feature extractors reveals limited performance and poor aggregation of visual cues for the target person. To address this, we propose a holistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF, leveraging a novel cross-input gate attentional xLSTM to enhance holistic-partial long-range reasoning and a novel Fourier condition to introduce more fine-grained control to improve the action segmentation generation. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse evaluation settings. The dataset and code are available at https://github.com/KPeng9510/HopaDIFF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09650v2</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.MM</category>
      <category>cs.RO</category>
      <category>eess.IV</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kunyu Peng, Junchao Huang, Xiangsheng Huang, Di Wen, Junwei Zheng, Yufan Chen, Kailun Yang, Jiamin Wu, Chongqing Hao, Rainer Stiefelhagen</dc:creator>
    </item>
    <item>
      <title>YOLO-Based Defect Detection for Metal Sheets</title>
      <link>https://arxiv.org/abs/2509.25659</link>
      <description>arXiv:2509.25659v2 Announce Type: replace-cross 
Abstract: In this paper, we propose a YOLO-based deep learning (DL) model for automatic defect detection to solve the time-consuming and labor-intensive tasks in industrial manufacturing. In our experiments, the images of metal sheets are used as the dataset for training the YOLO model to detect the defects on the surfaces and in the holes of metal sheets. However, the lack of metal sheet images significantly degrades the performance of detection accuracy. To address this issue, the ConSinGAN is used to generate a considerable amount of data. Four versions of the YOLO model (i.e., YOLOv3, v4, v7, and v9) are combined with the ConSinGAN for data augmentation. The proposed YOLOv9 model with ConSinGAN outperforms the other YOLO models with an accuracy of 91.3%, and a detection time of 146 ms. The proposed YOLOv9 model is integrated into manufacturing hardware and a supervisory control and data acquisition (SCADA) system to establish a practical automated optical inspection (AOI) system. Additionally, the proposed automated defect detection is easily applied to other components in industrial manufacturing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25659v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/IST63414.2024.10759237</arxiv:DOI>
      <arxiv:journal_reference>Proc. 2024 IEEE Int. Conf. Imaging Systems and Techniques (IST), Tokyo, Japan, Oct. 2024</arxiv:journal_reference>
      <dc:creator>Po-Heng Chou, Chun-Chi Wang, Wei-Lung Mao</dc:creator>
    </item>
  </channel>
</rss>
