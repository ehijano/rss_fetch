<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Sep 2024 04:00:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Categorical Data Analysis</title>
      <link>https://arxiv.org/abs/2409.02942</link>
      <description>arXiv:2409.02942v1 Announce Type: new 
Abstract: Categorical data are common in educational and social science research; however, methods for its analysis are generally not covered in introductory statistics courses. This chapter overviews fundamental concepts and methods in categorical data analysis. It describes and illustrates the analysis of contingency tables given different sampling processes and distributions, estimation of probabilities, hypothesis testing, measures of associations, and tests of no association with nominal variables, as well as the test of linear association with ordinal variables. Three data sets illustrate fatal police shootings in the United States, clinical trials of the Moderna vaccine, and responses to General Social Survey questions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02942v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Dandan Chen, Carolyn Anderson</dc:creator>
    </item>
    <item>
      <title>On Inference of Weitzman Overlapping Coefficient in Two Weibull Distributions</title>
      <link>https://arxiv.org/abs/2409.02950</link>
      <description>arXiv:2409.02950v1 Announce Type: new 
Abstract: Studying overlapping coefficients has recently become of great benefit, especially after its use in goodness-of-fit tests. These coefficients are defined as the amount of similarity between two statistical distributions. This research examines the estimation of one of these overlapping coefficients, which is the Weitzman coefficient {\Delta}, assuming two Weibull distributions and without using any restrictions on the parameters of these distributions. We studied the relative bias and relative mean square error of the resulting estimator by implementing a simulation study. The results show the importance of the resulting estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02950v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omar Eidous, Hala Maqableh</dc:creator>
    </item>
    <item>
      <title>Discussion of "Data fission: splitting a single data point"</title>
      <link>https://arxiv.org/abs/2409.03069</link>
      <description>arXiv:2409.03069v1 Announce Type: new 
Abstract: Leiner et al. [2023] introduce an important generalization of sample splitting, which they call data fission. They consider two cases of data fission: P1 fission and P2 fission. While P1 fission is extremely useful and easy to use, Leiner et al. [2023] provide P1 fission operations only for the Gaussian and the Poisson distributions. They provide little guidance on how to apply P2 fission operations in practice, leaving the reader unsure of how to apply data fission outside of the Gaussian and Poisson settings. In this discussion, we describe how our own work provides P1 fission operations in a wide variety of families and offers insight into when P1 fission is possible. We also provide guidance on how to actually apply P2 fission in practice, with a special focus on logistic regression. Finally, we interpret P2 fission as a remedy for distributional misspecification when carrying out P1 fission operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03069v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Neufeld, Ameer Dharamshi, Lucy L. Gao, Daniela Witten, Jacob Bien</dc:creator>
    </item>
    <item>
      <title>Penalized Subgrouping of Heterogeneous Time Series</title>
      <link>https://arxiv.org/abs/2409.03085</link>
      <description>arXiv:2409.03085v1 Announce Type: new 
Abstract: Interest in the study and analysis of dynamic processes in the social, behavioral, and health sciences has burgeoned in recent years due to the increased availability of intensive longitudinal data. However, how best to model and account for the persistent heterogeneity characterizing such processes remains an open question. The multi-VAR framework, a recent methodological development built on the vector autoregressive model, accommodates heterogeneous dynamics in multiple-subject time series through structured penalization. In the original multi-VAR proposal, individual-level transition matrices are decomposed into common and unique dynamics, allowing for generalizable and person-specific features. The current project extends this framework to allow additionally for the identification and penalized estimation of subgroup-specific dynamics; that is, patterns of dynamics that are shared across subsets of individuals. The performance of the proposed subgrouping extension is evaluated in the context of both a simulation study and empirical application, and results are compared to alternative methods for subgrouping multiple-subject, multivariate time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03085v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher M. Crawford, Jonathan J. Park, Sy-Miin Chow, Anja F. Ernst, Vladas Pipiras, Zachary F. Fisher</dc:creator>
    </item>
    <item>
      <title>Co-Developing Causal Graphs with Domain Experts Guided by Weighted FDR-Adjusted p-values</title>
      <link>https://arxiv.org/abs/2409.03126</link>
      <description>arXiv:2409.03126v1 Announce Type: new 
Abstract: This paper proposes an approach facilitating co-design of causal graphs between subject matter experts and statistical modellers. Modern causal analysis starting with formulation of causal graphs provides benefits for robust analysis and well-grounded decision support. Moreover, this process can enrich the discovery and planning phase of data science projects.
  The key premise is that plotting relevant statistical information on a causal graph structure can facilitate an intuitive discussion between domain experts and modellers. Furthermore, Hand-crafting causality graphs, integrating human expertise with robust statistical methodology, enables ensuring responsible AI practices.
  The paper focuses on using multiplicity-adjusted p-values, controlling for the false discovery rate (FDR), as an aid for co-designing the graph. A family of hypotheses relevant to causal graph construction is identified, including assessing correlation strengths, directions of causal effects, and how well an estimated structural causal model induces the observed covariance structure.
  An iterative flow is described where an initial causal graph is drafted based on expert beliefs about likely causal relationships. The subject matter expert's beliefs, communicated as ranked scores could be incorporated into the control of the measure proposed by Benjamini and Kling, the FDCR (False Discovery Cost Rate). The FDCR-adjusted p-values then provide feedback on which parts of the graph are supported or contradicted by the data. This co-design process continues, adding, removing, or revising arcs in the graph, until the expert and modeller converge on a satisfactory causal structure grounded in both domain knowledge and data evidence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03126v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eli Y. Kling</dc:creator>
    </item>
    <item>
      <title>A New Forward Discriminant Analysis Framework Based On Pillai's Trace and ULDA</title>
      <link>https://arxiv.org/abs/2409.03136</link>
      <description>arXiv:2409.03136v1 Announce Type: new 
Abstract: Linear discriminant analysis (LDA), a traditional classification tool, suffers from limitations such as sensitivity to noise and computational challenges when dealing with non-invertible within-class scatter matrices. Traditional stepwise LDA frameworks, which iteratively select the most informative features, often exacerbate these issues by relying heavily on Wilks' $\Lambda$, potentially causing premature stopping of the selection process. This paper introduces a novel forward discriminant analysis framework that integrates Pillai's trace with Uncorrelated Linear Discriminant Analysis (ULDA) to address these challenges, and offers a unified and stand-alone classifier. Through simulations and real-world datasets, the new framework demonstrates effective control of Type I error rates and improved classification accuracy, particularly in cases involving perfect group separations. The results highlight the potential of this approach as a robust alternative to the traditional stepwise LDA framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03136v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyu Wang</dc:creator>
    </item>
    <item>
      <title>Wrapped Gaussian Process Functional Regression Model for Batch Data on Riemannian Manifolds</title>
      <link>https://arxiv.org/abs/2409.03181</link>
      <description>arXiv:2409.03181v1 Announce Type: new 
Abstract: Regression is an essential and fundamental methodology in statistical analysis. The majority of the literature focuses on linear and nonlinear regression in the context of the Euclidean space. However, regression models in non-Euclidean spaces deserve more attention due to collection of increasing volumes of manifold-valued data. In this context, this paper proposes a concurrent functional regression model for batch data on Riemannian manifolds by estimating both mean structure and covariance structure simultaneously. The response variable is assumed to follow a wrapped Gaussian process distribution. Nonlinear relationships between manifold-valued response variables and multiple Euclidean covariates can be captured by this model in which the covariates can be functional and/or scalar. The performance of our model has been tested on both simulated data and real data, showing it is an effective and efficient tool in conducting functional data regression on Riemannian manifolds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03181v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinzhao Liu, Chao Liu, Jian Qing Shi, Tom Nye</dc:creator>
    </item>
    <item>
      <title>Directional data analysis using the spherical Cauchy and the Poisson-kernel based distribution</title>
      <link>https://arxiv.org/abs/2409.03292</link>
      <description>arXiv:2409.03292v1 Announce Type: new 
Abstract: The spherical Cauchy distribution and the Poisson-kernel based distribution were both proposed in 2020, for the analysis of directional data. The paper explores both of them under various frameworks. Alternative parametrizations that offer numerical and estimation advantages, including a straightforward Newton-Raphson algorithm to estimate the parameters are suggested, which further facilitate a more straightforward formulation under the regression setting. A two-sample location test, based on the log-likelihood ratio test is suggested, completing with discriminant analysis. The two distributions are put to the test-bed for all aforementioned cases, through simulation studies and via real data examples comparing and illustrating their performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03292v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michail Tsagris</dc:creator>
    </item>
    <item>
      <title>An Efficient Two-Dimensional Functional Mixed-Effect Model Framework for Repeatedly Measured Functional Data</title>
      <link>https://arxiv.org/abs/2409.03296</link>
      <description>arXiv:2409.03296v1 Announce Type: new 
Abstract: With the rapid development of wearable device technologies, accelerometers can record minute-by-minute physical activity for consecutive days, which provides important insight into a dynamic association between the intensity of physical activity and mental health outcomes for large-scale population studies. Using Shanghai school adolescent cohort we estimate the effect of health assessment results on physical activity profiles recorded by accelerometers throughout a week, which is recognized as repeatedly measured functional data. To achieve this goal, we propose an innovative two-dimensional functional mixed-effect model (2dFMM) for the specialized data, which smoothly varies over longitudinal day observations with covariate-dependent mean and covariance functions. The modeling framework characterizes the longitudinal and functional structures while incorporating two-dimensional fixed effects for covariates of interest. We also develop a fast three-stage estimation procedure to provide accurate fixed-effect inference for model interpretability and improve computational efficiency when encountering large datasets. We find strong evidence of intraday and interday varying significant associations between physical activity and mental health assessments among our cohort population, which shed light on possible intervention strategies targeting daily physical activity patterns to improve school adolescent mental health. Our method is also used in environmental data to illustrate the wide applicability. Supplementary materials for this article are available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03296v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cheng Cao, Jiguo Cao, Hao Pan, Yunting Zhang, Fan Jiang, Xinyue Li</dc:creator>
    </item>
    <item>
      <title>Testing Whether Reported Treatment Effects are Unduly Dependent on the Specific Outcome Measure Used</title>
      <link>https://arxiv.org/abs/2409.03502</link>
      <description>arXiv:2409.03502v1 Announce Type: new 
Abstract: This paper addresses the situation in which treatment effects are reported using educational or psychological outcome measures comprised of multiple questions or ``items.'' A distinction is made between a treatment effect on the construct being measured, which is referred to as impact, and item-specific treatment effects that are not due to impact, which are referred to as differential item functioning (DIF). By definition, impact generalizes to other measures of the same construct (i.e., measures that use different items), while DIF is dependent upon the specific items that make up the outcome measure. To distinguish these two cases, two estimators of impact are compared: an estimator that naively aggregates over items, and a less efficient one that is highly robust to DIF. The null hypothesis that both are consistent estimators of the true treatment impact leads to a Hausman-like specification test of whether the naive estimate is affected by item-level variation that would not be expected to generalize beyond the specific outcome measure used. The performance of the test is illustrated with simulation studies and a re-analysis of 34 item-level datasets from 22 randomized evaluations of educational interventions. In the empirical example, the dependence of reported effect sizes on the type of outcome measure (researcher-developed or independently developed) was substantially reduced after accounting for DIF. Implications for the ongoing debate about the role of researcher-developed assessments in education sciences are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03502v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Peter Halpin, Joshua Gilbert</dc:creator>
    </item>
    <item>
      <title>Bias correction of posterior means using MCMC outputs</title>
      <link>https://arxiv.org/abs/2409.03513</link>
      <description>arXiv:2409.03513v1 Announce Type: new 
Abstract: We propose algorithms for addressing the bias of the posterior mean when used as an estimator of parameters. These algorithms build upon the recently proposed Bayesian infinitesimal jackknife approximation (Giordano and Broderick (2023)) and can be implemented using the posterior covariance and third-order combined cumulants easily calculated from MCMC outputs. Two algorithms are introduced: The first algorithm utilises the output of a single-run MCMC with the original likelihood and prior to estimate the bias. A notable feature of the algorithm is that its ability to estimate definitional bias (Efron (2015)), which is crucial for Bayesian estimators. The second algorithm is designed for high-dimensional and sparse data settings, where ``quasi-prior'' for bias correction is introduced. The quasi-prior is iteratively refined using the output of the first algorithm as a measure of the residual bias at each step. These algorithms have been successfully implemented and tested for parameter estimation in the Weibull distribution and logistic regression in moderately high-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03513v1</guid>
      <category>stat.ME</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yukito Iba</dc:creator>
    </item>
    <item>
      <title>Extrinsic Principal Component Analysis</title>
      <link>https://arxiv.org/abs/2409.03572</link>
      <description>arXiv:2409.03572v1 Announce Type: new 
Abstract: One develops a fast computational methodology for principal component analysis on manifolds. Instead of estimating intrinsic principal components on an object space with a Riemannian structure, one embeds the object space in a numerical space, and the resulting chord distance is used. This method helps us analyzing high, theoretically even infinite dimensional data, from a new perspective. We define the extrinsic principal sub-manifolds of a random object on a Hilbert manifold embedded in a Hilbert space, and the sample counterparts. The resulting extrinsic principal components are useful for dimension data reduction. For application, one retains a very small number of such extrinsic principal components for a shape of contour data sample, extracted from imaging data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03572v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ka Chun Wong, Vic Patrangenaru, Robert L. Paige, Mihaela Pricop Jeckstadt</dc:creator>
    </item>
    <item>
      <title>Detecting Spatial Dependence in Transcriptomics Data using Vectorised Persistence Diagrams</title>
      <link>https://arxiv.org/abs/2409.03575</link>
      <description>arXiv:2409.03575v1 Announce Type: new 
Abstract: Evaluating spatial patterns in data is an integral task across various domains, including geostatistics, astronomy, and spatial tissue biology. The analysis of transcriptomics data in particular relies on methods for detecting spatially-dependent features that exhibit significant spatial patterns for both explanatory analysis and feature selection. However, given the complex and high-dimensional nature of these data, there is a need for robust, stable, and reliable descriptors of spatial dependence. We leverage the stability and multiscale properties of persistent homology to address this task. To this end, we introduce a novel framework using functional topological summaries, such as Betti curves and persistence landscapes, for identifying and describing non-random patterns in spatial data. In particular, we propose a non-parametric one-sample permutation test for spatial dependence and investigate its utility across both simulated and real spatial omics data. Our vectorised approach outperforms baseline methods at accurately detecting spatial dependence. Further, we find that our method is more robust to outliers than alternative tests using Moran's I.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03575v1</guid>
      <category>stat.ME</category>
      <category>cs.CG</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Katharina Limbeck, Bastian Rieck</dc:creator>
    </item>
    <item>
      <title>On the edge eigenvalues of the precision matrices of nonstationary autoregressive processes</title>
      <link>https://arxiv.org/abs/2109.02204</link>
      <description>arXiv:2109.02204v4 Announce Type: replace 
Abstract: This paper investigates structural changes in the parameters of first-order autoregressive models by analyzing the edge eigenvalues of the precision matrices. Specifically, edge eigenvalues in the precision matrix are observed if and only if there is a structural change in the autoregressive coefficients. We show that these edge eigenvalues correspond to the zeros of a determinantal equation. Additionally, we propose a consistent estimator for detecting outliers within the panel time series framework, supported by numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.02204v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junho Yang</dc:creator>
    </item>
    <item>
      <title>Boosting the Power of Kernel Two-Sample Tests</title>
      <link>https://arxiv.org/abs/2302.10687</link>
      <description>arXiv:2302.10687v2 Announce Type: replace 
Abstract: The kernel two-sample test based on the maximum mean discrepancy (MMD) is one of the most popular methods for detecting differences between two distributions over general metric spaces. In this paper we propose a method to boost the power of the kernel test by combining MMD estimates over multiple kernels using their Mahalanobis distance. We derive the asymptotic null distribution of the proposed test statistic and use a multiplier bootstrap approach to efficiently compute the rejection region. The resulting test is universally consistent and, since it is obtained by aggregating over a collection of kernels/bandwidths, is more powerful in detecting a wide range of alternatives in finite samples. We also derive the distribution of the test statistic for both fixed and local contiguous alternatives. The latter, in particular, implies that the proposed test is statistically efficient, that is, it has non-trivial asymptotic (Pitman) efficiency. The consistency properties of the Mahalanobis and other natural aggregation methods are also explored when the number of kernels is allowed to grow with the sample size. Extensive numerical experiments are performed on both synthetic and real-world datasets to illustrate the efficacy of the proposed method over single kernel tests. The computational complexity of the proposed method is also studied, both theoretically and in simulations. Our asymptotic results rely on deriving the joint distribution of MMD estimates using the framework of multiple stochastic integrals, which is more broadly useful, specifically, in understanding the efficiency properties of recently proposed adaptive MMD tests based on kernel aggregation and also in developing more computationally efficient (linear time) tests that combine multiple kernels. We conclude with an application of the Mahalanobis aggregation method for kernels with diverging scaling parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.10687v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anirban Chatterjee, Bhaswar B. Bhattacharya</dc:creator>
    </item>
    <item>
      <title>LASPATED: a Library for the Analysis of SPAtio-TEmporal Discrete data</title>
      <link>https://arxiv.org/abs/2401.04156</link>
      <description>arXiv:2401.04156v2 Announce Type: replace 
Abstract: We describe methods, tools, and a software library called LASPATED, available on GitHub (at https://github.com/vguigues/) to fit models using spatio-temporal data and space-time discretization. A video tutorial for this library is available on YouTube. We consider two types of methods to estimate a non-homogeneous Poisson process in space and time. The methods approximate the arrival intensity function of the Poisson process by discretizing space and time, and estimating arrival intensity as a function of subregion and time interval. With such methods, it is typical that the dimension of the estimator is large relative to the amount of data, and therefore the performance of the estimator can be improved by using additional data. The first method uses additional data to add a regularization term to the likelihood function for calibrating the intensity of the Poisson process. The second method uses additional data to estimate arrival intensity as a function of covariates. We describe a Python package to perform various types of space and time discretization. We also describe two packages for the calibration of the models, one in Matlab and one in C++. We demonstrate the advantages of our methods compared to basic maximum likelihood estimation with simulated and real data. The experiments with real data calibrate models of the arrival process of emergencies to be handled by the Rio de Janeiro emergency medical service.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.04156v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Guigues, Anton Kleywegt, Giovanni Amorim, Andr\'e Mazal Krauss, Victor Hugo Nascimento</dc:creator>
    </item>
    <item>
      <title>Likelihood estimation for stochastic differential equations with mixed effects</title>
      <link>https://arxiv.org/abs/2408.17257</link>
      <description>arXiv:2408.17257v2 Announce Type: replace 
Abstract: Stochastic differential equations provide a powerful tool for modelling dynamic phenomena affected by random noise. In case of repeated observations of time series for several experimental units, it is often the case that some of the parameters vary between the individual experimental units, which has motivated a considerable interest in stochastic differential equations with mixed effects, where a subset of the parameters are random. These models enable simultaneous representation of randomness in the dynamics and variability between experimental units. When the data are observations at discrete time points, the likelihood function is only rarely explicitly available, so for likelihood-based inference to be feasible, numerical methods are needed. We present Gibbs samplers and stochastic EM-algorithms based on augmented data obtained by the simple method for simulation of diffusion bridges in Bladt and S{\o}rensen (2014). This method is easy to implement and has no tuning parameters. The method is, moreover, computationally efficient at low sampling frequencies because the computing time increases linearly with the time between observations. The algorithms can be extended to models with measurement errors. The Gibbs sampler as well as the EM-algorithm are shown to simplify considerably for exponential families of diffusion processes, including many models used in practice. In a simulation study, the estimation methods are shown to work well for Ornstein-Uhlenbeck processes and t-diffusions with mixed effects. Finally, the Gibbs sampler is applied to neuronal data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17257v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fernando Baltazar-Larios, Mogens Bladt, Michael S{\o}rensen</dc:creator>
    </item>
    <item>
      <title>A causal viewpoint on prediction model performance under changes in case-mix: discrimination and calibration respond differently for prognosis and diagnosis predictions</title>
      <link>https://arxiv.org/abs/2409.01444</link>
      <description>arXiv:2409.01444v2 Announce Type: replace 
Abstract: Prediction models inform important clinical decisions, aiding in diagnosis, prognosis, and treatment planning. The predictive performance of these models is typically assessed through discrimination and calibration. However, changes in the distribution of the data impact model performance. In health-care, a typical change is a shift in case-mix: for example, for cardiovascular risk management, a general practitioner sees a different mix of patients than a specialist in a tertiary hospital.
  This work introduces a novel framework that differentiates the effects of case-mix shifts on discrimination and calibration based on the causal direction of the prediction task. When prediction is in the causal direction (often the case for prognosis predictions), calibration remains stable under case-mix shifts, while discrimination does not. Conversely, when predicting in the anti-causal direction (often with diagnosis predictions), discrimination remains stable, but calibration does not.
  A simulation study and empirical validation using cardiovascular disease prediction models demonstrate the implications of this framework. This framework provides critical insights for evaluating and deploying prediction models across different clinical settings, emphasizing the importance of understanding the causal structure of the prediction task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01444v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wouter A. C. van Amsterdam</dc:creator>
    </item>
  </channel>
</rss>
