<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Jul 2024 04:00:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 25 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Generalised Causal Dantzig</title>
      <link>https://arxiv.org/abs/2407.16786</link>
      <description>arXiv:2407.16786v1 Announce Type: new 
Abstract: Prediction invariance of causal models under heterogeneous settings has been exploited by a number of recent methods for causal discovery, typically focussing on recovering the causal parents of a target variable of interest. When instrumental variables are not available, the causal Dantzig estimator exploits invariance under the more restrictive case of shift interventions. However, also in this case, one requires observational data from a number of sufficiently different environments, which is rarely available. In this paper, we consider a structural equation model where the target variable is described by a generalised additive model conditional on its parents. Besides having finite moments, no modelling assumptions are made on the conditional distributions of the other variables in the system. Under this setting, we characterise the causal model uniquely by means of two key properties: the Pearson residuals are invariant under the causal model and conditional on the causal parents the causal parameters maximise the population likelihood. These two properties form the basis of a computational strategy for searching the causal model among all possible models. Crucially, for generalised linear models with a known dispersion parameter, such as Poisson and logistic regression, the causal model can be identified from a single data environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16786v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alice Polinelli, Veronica Vinciotti, Ernst C. Wit</dc:creator>
    </item>
    <item>
      <title>CoCA: Cooperative Component Analysis</title>
      <link>https://arxiv.org/abs/2407.16870</link>
      <description>arXiv:2407.16870v1 Announce Type: new 
Abstract: We propose Cooperative Component Analysis (CoCA), a new method for unsupervised multi-view analysis: it identifies the component that simultaneously captures significant within-view variance and exhibits strong cross-view correlation. The challenge of integrating multi-view data is particularly important in biology and medicine, where various types of "-omic" data, ranging from genomics to proteomics, are measured on the same set of samples. The goal is to uncover important, shared signals that represent underlying biological mechanisms. CoCA combines an approximation error loss to preserve information within data views and an "agreement penalty" to encourage alignment across data views. By balancing the trade-off between these two key components in the objective, CoCA has the property of interpolating between the commonly-used principal component analysis (PCA) and canonical correlation analysis (CCA) as special cases at the two ends of the solution path. CoCA chooses the degree of agreement in a data-adaptive manner, using a validation set or cross-validation to estimate test error. Furthermore, we propose a sparse variant of CoCA that incorporates the Lasso penalty to yield feature sparsity, facilitating the identification of key features driving the observed patterns. We demonstrate the effectiveness of CoCA on simulated data and two real multiomics studies of COVID-19 and ductal carcinoma in situ of breast. In both real data applications, CoCA successfully integrates multiomics data, extracting components that are not only consistently present across different data views but also more informative and predictive of disease progression. CoCA offers a powerful framework for discovering important shared signals in multi-view data, with the potential to uncover novel insights in an increasingly multi-view data world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16870v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daisy Yi Ding, Alden Green, Min Woo Sun, Robert Tibshirani</dc:creator>
    </item>
    <item>
      <title>Relative local dependence of bivariate copulas</title>
      <link>https://arxiv.org/abs/2407.16948</link>
      <description>arXiv:2407.16948v1 Announce Type: new 
Abstract: For a bivariate probability distribution, local dependence around a single point on the support is often formulated as the second derivative of the logarithm of the probability density function. However, this definition lacks the invariance under marginal distribution transformations, which is often required as a criterion for dependence measures. In this study, we examine the \textit{relative local dependence}, which we define as the ratio of the local dependence to the probability density function, for copulas. By using this notion, we point out that typical copulas can be characterised as the solutions to the corresponding partial differential equations, particularly highlighting that the relative local dependence of the Frank copula remains constant. The estimation and visualization of the relative local dependence are demonstrated using simulation data. Furthermore, we propose a class of copulas where local dependence is proportional to the $k$-th power of the probability density function, and as an example, we demonstrate a newly discovered relationship derived from the density functions of two representative copulas, the Frank copula and the Farlie-Gumbel-Morgenstern (FGM) copula.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16948v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Issey Sukeda, Tomonari Sei</dc:creator>
    </item>
    <item>
      <title>A Bayesian modelling framework for health care resource use and costs in trial-based economic evaluations</title>
      <link>https://arxiv.org/abs/2407.17036</link>
      <description>arXiv:2407.17036v1 Announce Type: new 
Abstract: Individual-level effectiveness and healthcare resource use (HRU) data are routinely collected in trial-based economic evaluations. While effectiveness is often expressed in terms of utility scores derived from some health-related quality of life instruments (e.g.~EQ-5D questionnaires), different types of HRU may be included. Costs are usually generated by applying unit prices to HRU data and statistical methods have been traditionally implemented to analyse costs and utilities or after combining them into aggregated variables (e.g. Quality-Adjusted Life Years). When outcome data are not fully observed, e.g. some patients drop out or only provided partial information, the validity of the results may be hindered both in terms of efficiency and bias. Often, partially-complete HRU data are handled using "ad-hoc" methods, implicitly relying on some assumptions (e.g. fill-in a zero) which are hard to justify beside the practical convenience of increasing the completion rate. We present a general Bayesian framework for the modelling of partially-observed HRUs which allows a flexible model specification to accommodate the typical complexities of the data and to quantify the impact of different types of uncertainty on the results. We show the benefits of using our approach using a motivating example and compare the results to those from traditional analyses focussed on the modelling of cost variables after adopting some ad-hoc imputation strategy for HRU data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17036v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Gabrio</dc:creator>
    </item>
    <item>
      <title>Bayesian non-linear subspace shrinkage using horseshoe priors</title>
      <link>https://arxiv.org/abs/2407.17113</link>
      <description>arXiv:2407.17113v1 Announce Type: new 
Abstract: When modeling biological responses using Bayesian non-parametric regression, prior information may be available on the shape of the response in the form of non-linear function spaces that define the general shape of the response. To incorporate such information into the analysis, we develop a non-linear functional shrinkage (NLFS) approach that uniformly shrinks the non-parametric fitted function into a non-linear function space while allowing for fits outside of this space when the data suggest alternative shapes. This approach extends existing functional shrinkage approaches into linear subspaces to shrinkage into non-linear function spaces using a Taylor series expansion and corresponding updating of non-linear parameters. We demonstrate this general approach on the Hill model, a popular, biologically motivated model, and show that shrinkage into combined function spaces, i.e., where one has two or more non-linear functions a priori, is straightforward. We demonstrate this approach through synthetic and real data. Computational details on the underlying MCMC sampling are provided with data and analysis available in an online supplement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17113v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Julia Christin Duda, Matthew Wheeler</dc:creator>
    </item>
    <item>
      <title>Asymmetry Analysis of Bilateral Shapes</title>
      <link>https://arxiv.org/abs/2407.17225</link>
      <description>arXiv:2407.17225v1 Announce Type: new 
Abstract: Many biological objects possess bilateral symmetry about a midline or midplane, up to a ``noise'' term. This paper uses landmark-based methods to measure departures from bilateral symmetry, especially for the two-group problem where one group is more asymmetric than the other. In this paper, we formulate our work in the framework of size-and-shape analysis including registration via rigid body motion. Our starting point is a vector of elementary asymmetry features defined at the individual landmark coordinates for each object. We introduce two approaches for testing. In the first, the elementary features are combined into a scalar composite asymmetry measure for each object. Then standard univariate tests can be used to compare the two groups. In the second approach, a univariate test statistic is constructed for each elementary feature. The maximum of these statistics lead to an overall test statistic to compare the two groups and we then provide a technique to extract the important features from the landmark data. Our methodology is illustrated on a pre-registered smile dataset collected to assess the success of cleft lip surgery on human subjects. The asymmetry in a group of cleft lip subjects is compared to a group of normal subjects, and statistically significant differences have been found by univariate tests in the first approach. Further, our feature extraction method leads to an anatomically plausible set of landmarks for medical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17225v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kanti V. Mardia, Xiangyu Wu, John T. Kent, Colin R. Goodall, Balvinder S. Khambay</dc:creator>
    </item>
    <item>
      <title>Causal modelling without counterfactuals and individualised effects</title>
      <link>https://arxiv.org/abs/2407.17385</link>
      <description>arXiv:2407.17385v1 Announce Type: new 
Abstract: The most common approach to causal modelling is the potential outcomes framework due to Neyman and Rubin. In this framework, outcomes of counterfactual treatments are assumed to be well-defined. This metaphysical assumption is often thought to be problematic yet indispensable. The conventional approach relies not only on counterfactuals, but also on abstract notions of distributions and assumptions of independence that are not directly testable. In this paper, we construe causal inference as treatment-wise predictions for finite populations where all assumptions are testable; this means that one can not only test predictions themselves (without any fundamental problem), but also investigate sources of error when they fail. The new framework highlights the model-dependence of causal claims as well as the difference between statistical and scientific inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17385v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Benedikt H\"oltgen, Robert C. Williamson</dc:creator>
    </item>
    <item>
      <title>Estimating the hyperuniformity exponent of point processes</title>
      <link>https://arxiv.org/abs/2407.16797</link>
      <description>arXiv:2407.16797v1 Announce Type: cross 
Abstract: We address the challenge of estimating the hyperuniformity exponent $\alpha$ of a spatial point process, given only one realization of it. Assuming that the structure factor $S$ of the point process follows a vanishing power law at the origin (the typical case of a hyperuniform point process), this exponent is defined as the slope near the origin of $\log S$. Our estimator is built upon the (expanding window) asymptotic variance of some wavelet transforms of the point process. By combining several scales and several wavelets, we develop a multi-scale, multi-taper estimator $\widehat{\alpha}$. We analyze its asymptotic behavior, proving its consistency under various settings, and enabling the construction of asymptotic confidence intervals for $\alpha$ when $\alpha &lt; d$ and under Brillinger mixing. This construction is derived from a multivariate central limit theorem where the normalisations are non-standard and vary among the components. We also present a non-asymptotic deviation inequality providing insights into the influence of tapers on the bias-variance trade-off of $\widehat{\alpha}$. Finally, we investigate the performance of $\widehat{\alpha}$ through simulations, and we apply our method to the analysis of hyperuniformity in a real dataset of marine algae.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16797v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel Mastrilli, Bart{\l}omiej B{\l}aszczyszyn, Fr\'ed\'eric Lavancier</dc:creator>
    </item>
    <item>
      <title>On the Parameter Identifiability of Partially Observed Linear Causal Models</title>
      <link>https://arxiv.org/abs/2407.16975</link>
      <description>arXiv:2407.16975v1 Announce Type: cross 
Abstract: Linear causal models are important tools for modeling causal dependencies and yet in practice, only a subset of the variables can be observed. In this paper, we examine the parameter identifiability of these models by investigating whether the edge coefficients can be recovered given the causal structure and partially observed data. Our setting is more general than that of prior research - we allow all variables, including both observed and latent ones, to be flexibly related, and we consider the coefficients of all edges, whereas most existing works focus only on the edges between observed variables. Theoretically, we identify three types of indeterminacy for the parameters in partially observed linear causal models. We then provide graphical conditions that are sufficient for all parameters to be identifiable and show that some of them are provably necessary. Methodologically, we propose a novel likelihood-based parameter estimation method that addresses the variance indeterminacy of latent variables in a specific way and can asymptotically recover the underlying parameters up to trivial indeterminacy. Empirical studies on both synthetic and real-world datasets validate our identifiability theory and the effectiveness of the proposed method in the finite-sample regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16975v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinshuai Dong, Ignavier Ng, Biwei Huang, Yuewen Sun, Songyao Jin, Roberto Legaspi, Peter Spirtes, Kun Zhang</dc:creator>
    </item>
    <item>
      <title>Exploring Covid-19 Spatiotemporal Dynamics: Non-Euclidean Spatially Aware Functional Registration</title>
      <link>https://arxiv.org/abs/2407.17132</link>
      <description>arXiv:2407.17132v1 Announce Type: cross 
Abstract: When it came to Covid-19, timing was everything. This paper considers the spatiotemporal dynamics of the Covid-19 pandemic via a developed methodology of non-Euclidean spatially aware functional registration. In particular, the daily SARS-CoV-2 incidence in each of 380 local authorities in the UK from March to June 2020 is analysed to understand the phase variation of the waves when considered as curves. This is achieved by adapting a traditional registration method (that of local variation analysis) to account for the clear spatial dependencies in the data. This adapted methodology is shown via simulation studies to perform substantially better for the estimation of the registration functions than the non-spatial alternative. Moreover, it is found that the driving time between locations represents the spatial dependency in the Covid-19 data better than geographical distance. However, since driving time is non-Euclidean, the traditional spatial frameworks break down; to solve this, a methodology inspired by multidimensional scaling is developed to approximate the driving times by a Euclidean distance which enables the established theory to be applied. Finally, the resulting estimates of the registration/warping processes are analysed by taking functionals to understand the qualitatively observable earliness/lateness and sharpness/flatness of the Covid-19 waves quantitatively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17132v1</guid>
      <category>stat.AP</category>
      <category>physics.soc-ph</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke A. Barratt (Statistical Laboratory, DPMMS, University of Cambridge, UK), John A. D. Aston (Statistical Laboratory, DPMMS, University of Cambridge, UK)</dc:creator>
    </item>
    <item>
      <title>Generalization Bounds of Surrogate Policies for Combinatorial Optimization Problems</title>
      <link>https://arxiv.org/abs/2407.17200</link>
      <description>arXiv:2407.17200v1 Announce Type: cross 
Abstract: A recent stream of structured learning approaches has improved the practical state of the art for a range of combinatorial optimization problems with complex objectives encountered in operations research. Such approaches train policies that chain a statistical model with a surrogate combinatorial optimization oracle to map any instance of the problem to a feasible solution. The key idea is to exploit the statistical distribution over instances instead of dealing with instances separately. However learning such policies by risk minimization is challenging because the empirical risk is piecewise constant in the parameters, and few theoretical guarantees have been provided so far. In this article, we investigate methods that smooth the risk by perturbing the policy, which eases optimization and improves generalization. Our main contribution is a generalization bound that controls the perturbation bias, the statistical learning error, and the optimization error. Our analysis relies on the introduction of a uniform weak property, which captures and quantifies the interplay of the statistical model and the surrogate combinatorial optimization oracle. This property holds under mild assumptions on the statistical model, the surrogate optimization, and the instance data distribution. We illustrate the result on a range of applications such as stochastic vehicle scheduling. In particular, such policies are relevant for contextual stochastic optimization and our results cover this case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17200v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Pierre-Cyril Aubin-Frankowski, Yohann De Castro, Axel Parmentier, Alessandro Rudi</dc:creator>
    </item>
    <item>
      <title>Low dimensional representation of multi-patient flow cytometry datasets using optimal transport for minimal residual disease detection in leukemia</title>
      <link>https://arxiv.org/abs/2407.17329</link>
      <description>arXiv:2407.17329v1 Announce Type: cross 
Abstract: Representing and quantifying Minimal Residual Disease (MRD) in Acute Myeloid Leukemia (AML), a type of cancer that affects the blood and bone marrow, is essential in the prognosis and follow-up of AML patients. As traditional cytological analysis cannot detect leukemia cells below 5\%, the analysis of flow cytometry dataset is expected to provide more reliable results. In this paper, we explore statistical learning methods based on optimal transport (OT) to achieve a relevant low-dimensional representation of multi-patient flow cytometry measurements (FCM) datasets considered as high-dimensional probability distributions. Using the framework of OT, we justify the use of the K-means algorithm for dimensionality reduction of multiple large-scale point clouds through mean measure quantization by merging all the data into a single point cloud. After this quantization step, the visualization of the intra and inter-patients FCM variability is carried out by embedding low-dimensional quantized probability measures into a linear space using either Wasserstein Principal Component Analysis (PCA) through linearized OT or log-ratio PCA of compositional data. Using a publicly available FCM dataset and a FCM dataset from Bordeaux University Hospital, we demonstrate the benefits of our approach over the popular kernel mean embedding technique for statistical learning from multiple high-dimensional probability distributions. We also highlight the usefulness of our methodology for low-dimensional projection and clustering patient measurements according to their level of MRD in AML from FCM. In particular, our OT-based approach allows a relevant and informative two-dimensional representation of the results of the FlowSom algorithm, a state-of-the-art method for the detection of MRD in AML using multi-patient FCM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17329v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erell Gachon, J\'er\'emie Bigot, Elsa Cazelles, Aguirre Mimoun, Jean-Philippe Vial</dc:creator>
    </item>
    <item>
      <title>Estimation of bid-ask spreads in the presence of serial dependence</title>
      <link>https://arxiv.org/abs/2407.17401</link>
      <description>arXiv:2407.17401v1 Announce Type: cross 
Abstract: Starting from a basic model in which the dynamic of the transaction prices is a geometric Brownian motion disrupted by a microstructure white noise, corresponding to the random alternation of bids and asks, we propose moment-based estimators along with their statistical properties. We then make the model more realistic by considering serial dependence: we assume a geometric fractional Brownian motion for the price, then an Ornstein-Uhlenbeck process for the microstructure noise. In these two cases of serial dependence, we propose again consistent and asymptotically normal estimators. All our estimators are compared on simulated data with existing approaches, such as Roll, Corwin-Schultz, Abdi-Ranaldo, or Ardia-Guidotti-Kroencke estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17401v1</guid>
      <category>q-fin.ST</category>
      <category>q-fin.MF</category>
      <category>q-fin.TR</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xavier Brouty, Matthieu Garcin, Hugo Roccaro</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Time-Varying Coefficient Estimation</title>
      <link>https://arxiv.org/abs/2202.08419</link>
      <description>arXiv:2202.08419v4 Announce Type: replace 
Abstract: In this paper, we develop a novel high-dimensional time-varying coefficient estimation method, based on high-dimensional Ito diffusion processes. To account for high-dimensional time-varying coefficients, we first estimate local (or instantaneous) coefficients using a time-localized Dantzig selection scheme under a sparsity condition, which results in biased local coefficient estimators due to the regularization. To handle the bias, we propose a debiasing scheme, which provides well-performing unbiased local coefficient estimators. With the unbiased local coefficient estimators, we estimate the integrated coefficient, and to further account for the sparsity of the coefficient process, we apply thresholding schemes. We call this Thresholding dEbiased Dantzig (TED). We establish asymptotic properties of the proposed TED estimator. In the empirical analysis, we apply the TED procedure to analyzing high-dimensional factor models using high-frequency data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.08419v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Donggyu Kim, Minseog Oh, Minseok Shin</dc:creator>
    </item>
    <item>
      <title>Causal Identification for Complex Continuous-time Functional Longitudinal Studies</title>
      <link>https://arxiv.org/abs/2206.12525</link>
      <description>arXiv:2206.12525v4 Announce Type: replace 
Abstract: Real-time monitoring in modern medical research introduces functional longitudinal data, characterized by continuous-time measurements of outcomes, treatments, and confounders. This complexity leads to uncountably infinite treatment-confounder feedbacks, which traditional causal inference methodologies cannot handle. Inspired by the coarsened data framework, we adopt stochastic process theory, measure theory, and net convergence to propose a nonparametric causal identification framework. This framework generalizes classical g-computation, inverse probability weighting, and doubly robust formulas, accommodating time-varying outcomes subject to mortality and censoring. Our approach addresses significant gaps in current methodologies, providing a solution for complex, real-time longitudinal data and paving the way for future estimation work in this domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.12525v4</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Ying</dc:creator>
    </item>
    <item>
      <title>Unbalanced Kantorovich-Rubinstein distance, plan, and barycenter on finite spaces: A statistical perspective</title>
      <link>https://arxiv.org/abs/2211.08858</link>
      <description>arXiv:2211.08858v2 Announce Type: replace 
Abstract: We analyze statistical properties of plug-in estimators for unbalanced optimal transport quantities between finitely supported measures in different prototypical sampling models. Specifically, our main results provide non-asymptotic bounds on the expected error of empirical Kantorovich-Rubinstein (KR) distance, plans, and barycenters for mass penalty parameter $C&gt;0$. The impact of the mass penalty parameter $C$ is studied in detail. Based on this analysis, we mathematically justify randomized computational schemes for KR quantities which can be used for fast approximate computations in combination with any exact solver. Using synthetic and real datasets, we empirically analyze the behavior of the expected errors in simulation studies and illustrate the validity of our theoretical bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.08858v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shayan Hundrieser, Florian Heinemann, Marcel Klatt, Marina Struleva, Axel Munk</dc:creator>
    </item>
    <item>
      <title>High-dimensional Covariance Estimation by Pairwise Likelihood Truncation</title>
      <link>https://arxiv.org/abs/2407.07717</link>
      <description>arXiv:2407.07717v2 Announce Type: replace 
Abstract: Pairwise likelihood is a useful approximation to the full likelihood function for covariance estimation in high-dimensional context. It simplifies high-dimensional dependencies by combining marginal bivariate likelihood objects, thus making estimation more manageable. In certain models, including the Gaussian model, both pairwise and full likelihoods are maximized by the same parameter values, thus retaining optimal statistical efficiency, when the number of variables is fixed. Leveraging on this insight, we introduce estimation of sparse high-dimensional covariance matrices by maximizing a truncated version of the pairwise likelihood function, obtained by including pairwise terms corresponding to nonzero covariance elements. To achieve a meaningful truncation, we propose to minimize the $L_2$-distance between pairwise and full likelihood scores plus an $L_1$-penalty discouraging the inclusion of uninformative terms. Differently from other regularization approaches, our method focuses on selecting whole pairwise likelihood objects rather than shrinking individual covariance parameters, thus retaining the inherent unbiasedness of the pairwise likelihood estimating equations. This selection procedure is shown to have the selection consistency property as the covariance dimension increases exponentially fast. Consequently, the implied pairwise likelihood estimator is consistent and converges to the oracle maximum likelihood estimator assuming knowledge of nonzero covariance entries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07717v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alessandro Casa, Davide Ferrari, Zhendong Huang</dc:creator>
    </item>
    <item>
      <title>Effect Heterogeneity with Earth Observation in Randomized Controlled Trials: Exploring the Role of Data, Model, and Evaluation Metric Choice</title>
      <link>https://arxiv.org/abs/2407.11674</link>
      <description>arXiv:2407.11674v2 Announce Type: replace 
Abstract: Many social and environmental phenomena are associated with macroscopic changes in the built environment, captured by satellite imagery on a global scale and with daily temporal resolution. While widely used for prediction, these images and especially image sequences remain underutilized for causal inference, especially in the context of randomized controlled trials (RCTs), where causal identification is established by design. In this paper, we develop and compare a set of general tools for analyzing Conditional Average Treatment Effects (CATEs) from temporal satellite data that can be applied to any RCT where geographical identifiers are available. Through a simulation study, we analyze different modeling strategies for estimating CATE in sequences of satellite images. We find that image sequence representation models with more parameters generally yield a greater ability to detect heterogeneity. To explore the role of model and data choice in practice, we apply the approaches to two influential RCTs -- Banerjee et al. (2015), a poverty study in Cusco, Peru, and Bolsen et al. (2014), a water conservation experiment in Georgia, USA. We benchmark our image sequence models against image-only, tabular-only, and combined image-tabular data sources, summarizing practical implications for investigators in a multivariate analysis. Land cover classifications over satellite images facilitate interpretation of what image features drive heterogeneity. We also show robustness to data and model choice of satellite-based generalization of the RCT results to larger geographical areas outside the original. Overall, this paper shows how satellite sequence data can be incorporated into the analysis of RCTs, and provides evidence about the implications of data, model, and evaluation metric choice for causal analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11674v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Connor T. Jerzak, Ritwik Vashistha, Adel Daoud</dc:creator>
    </item>
    <item>
      <title>Robust and consistent model evaluation criteria in high-dimensional regression</title>
      <link>https://arxiv.org/abs/2407.16116</link>
      <description>arXiv:2407.16116v2 Announce Type: replace 
Abstract: In the last two decades, sparse regularization methods such as the LASSO have been applied in various fields. Most of the regularization methods have one or more regularization parameters, and to select the value of the regularization parameter is essentially equal to select a model, thus we need to determine the regularization parameter adequately. Regarding the determination of the regularization parameter in the linear regression model, we often apply the information criteria like the AIC and BIC, however, it has been pointed out that these criteria are sensitive to outliers and tend not to perform well in high-dimensional settings. Outliers generally have a negative influence on not only estimation but also model selection, consequently, it is important to employ a selection method that is robust against outliers. In addition, when the number of explanatory variables is quite large, most conventional criteria are prone to select unnecessary explanatory variables. In this paper, we propose model evaluation criteria via the statistical divergence with excellence in robustness in both of parametric estimation and model selection. Furthermore, our proposed criteria simultaneously achieve the selection consistency with the robustness even in high-dimensional settings. We also report the results of some numerical examples to verify that the proposed criteria perform robust and consistent variable selection compared with the conventional selection methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16116v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sumito Kurata, Kei Hirose</dc:creator>
    </item>
    <item>
      <title>Logistic regression models for patient-level prediction based on massive observational data: Do we need all data?</title>
      <link>https://arxiv.org/abs/2008.07361</link>
      <description>arXiv:2008.07361v2 Announce Type: replace-cross 
Abstract: Objective: Provide guidance on sample size considerations for developing predictive models by empirically establishing the adequate sample size, which balances the competing objectives of improving model performance and reducing model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on prediction performance and model complexity by generating learning curves for 81 prediction problems (23 outcomes predicted in a depression cohort, 58 outcomes predicted in a hypertension cohort) in three large observational health databases, requiring training of 17,248 prediction models. The adequate sample size was defined as the sample size for which the performance of a model equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001, 0.005, 0.01, and 0.02, respectively. The median reduction of the number of predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction in sample size and model complexity can be estimated for future prediction work. Though, if a researcher is willing to generate a learning curve a much larger reduction of the model complexity may be possible as suggested by a large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the available data was sufficient to produce a model close to the performance of one developed on the full data set, but with a substantially reduced model complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2008.07361v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ijmedinf.2022.104762</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Medical Informatics, Volume 163, July 2022, Article number 104762</arxiv:journal_reference>
      <dc:creator>Luis H. John, Jan A. Kors, Jenna M. Reps, Patrick B. Ryan, Peter R. Rijnbeek</dc:creator>
    </item>
    <item>
      <title>When Does Bottom-up Beat Top-down in Hierarchical Community Detection?</title>
      <link>https://arxiv.org/abs/2306.00833</link>
      <description>arXiv:2306.00833v2 Announce Type: replace-cross 
Abstract: Hierarchical clustering of networks consists in finding a tree of communities, such that lower levels of the hierarchy reveal finer-grained community structures. There are two main classes of algorithms tackling this problem. Divisive ($\textit{top-down}$) algorithms recursively partition the nodes into two communities, until a stopping rule indicates that no further split is needed. In contrast, agglomerative ($\textit{bottom-up}$) algorithms first identify the smallest community structure and then repeatedly merge the communities using a $\textit{linkage}$ method. In this article, we establish theoretical guarantees for the recovery of the hierarchical tree and community structure of a Hierarchical Stochastic Block Model by a bottom-up algorithm. We also establish that this bottom-up algorithm attains the information-theoretic threshold for exact recovery at intermediate levels of the hierarchy. Notably, these recovery conditions are less restrictive compared to those existing for top-down algorithms. This shows that bottom-up algorithms extend the feasible region for achieving exact recovery at intermediate levels. Numerical experiments on both synthetic and real data sets confirm the superiority of bottom-up algorithms over top-down algorithms. We also observe that top-down algorithms can produce dendrograms with inversions. These findings contribute to a better understanding of hierarchical clustering techniques and their applications in network analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.00833v2</guid>
      <category>cs.SI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maximilien Dreveton, Daichi Kuroda, Matthias Grossglauser, Patrick Thiran</dc:creator>
    </item>
    <item>
      <title>Optimal Bias-Correction and Valid Inference in High-Dimensional Ridge Regression: A Closed-Form Solution</title>
      <link>https://arxiv.org/abs/2405.00424</link>
      <description>arXiv:2405.00424v2 Announce Type: replace-cross 
Abstract: Ridge regression is an indispensable tool in big data analysis. Yet its inherent bias poses a significant and longstanding challenge, compromising both statistical efficiency and scalability across various applications. To tackle this critical issue, we introduce an iterative strategy to correct bias effectively when the dimension $p$ is less than the sample size $n$. For $p&gt;n$, our method optimally mitigates the bias such that any remaining bias in the proposed de-biased estimator is unattainable through linear transformations of the response data. To address the remaining bias when $p&gt;n$, we employ a Ridge-Screening (RS) method, producing a reduced model suitable for bias correction. Crucially, under certain conditions, the true model is nested within our selected one, highlighting RS as a novel variable selection approach. Through rigorous analysis, we establish the asymptotic properties and valid inferences of our de-biased ridge estimators for both $p&lt;n$ and $p&gt;n$, where, both $p$ and $n$ may increase towards infinity, along with the number of iterations. We further validate these results using simulated and real-world data examples. Our method offers a transformative solution to the bias challenge in ridge regression inferences across various disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00424v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaoxing Gao, Ruey S. Tsay</dc:creator>
    </item>
    <item>
      <title>Probing the Information Theoretical Roots of Spatial Dependence Measures</title>
      <link>https://arxiv.org/abs/2405.18459</link>
      <description>arXiv:2405.18459v2 Announce Type: replace-cross 
Abstract: Intuitively, there is a relation between measures of spatial dependence and information theoretical measures of entropy. For instance, we can provide an intuition of why spatial data is special by stating that, on average, spatial data samples contain less than expected information. Similarly, spatial data, e.g., remotely sensed imagery, that is easy to compress is also likely to show significant spatial autocorrelation. Formulating our (highly specific) core concepts of spatial information theory in the widely used language of information theory opens new perspectives on their differences and similarities and also fosters cross-disciplinary collaboration, e.g., with the broader AI/ML communities. Interestingly, however, this intuitive relation is challenging to formalize and generalize, leading prior work to rely mostly on experimental results, e.g., for describing landscape patterns. In this work, we will explore the information theoretical roots of spatial autocorrelation, more specifically Moran's I, through the lens of self-information (also known as surprisal) and provide both formal proofs and experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18459v2</guid>
      <category>cs.IT</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhangyu Wang, Krzysztof Janowicz, Gengchen Mai, Ivan Majic</dc:creator>
    </item>
    <item>
      <title>Causal Discovery over High-Dimensional Structured Hypothesis Spaces with Causal Graph Partitioning</title>
      <link>https://arxiv.org/abs/2406.06348</link>
      <description>arXiv:2406.06348v2 Announce Type: replace-cross 
Abstract: The aim in many sciences is to understand the mechanisms that underlie the observed distribution of variables, starting from a set of initial hypotheses. Causal discovery allows us to infer mechanisms as sets of cause and effect relationships in a generalized way -- without necessarily tailoring to a specific domain. Causal discovery algorithms search over a structured hypothesis space, defined by the set of directed acyclic graphs, to find the graph that best explains the data. For high-dimensional problems, however, this search becomes intractable and scalable algorithms for causal discovery are needed to bridge the gap. In this paper, we define a novel causal graph partition that allows for divide-and-conquer causal discovery with theoretical guarantees. We leverage the idea of a superstructure -- a set of learned or existing candidate hypotheses -- to partition the search space. We prove under certain assumptions that learning with a causal graph partition always yields the Markov Equivalence Class of the true causal graph. We show our algorithm achieves comparable accuracy and a faster time to solution for biologically-tuned synthetic networks and networks up to ${10^4}$ variables. This makes our method applicable to gene regulatory network inference and other domains with high-dimensional structured hypothesis spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06348v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashka Shah, Adela DePavia, Nathaniel Hudson, Ian Foster, Rick Stevens</dc:creator>
    </item>
  </channel>
</rss>
