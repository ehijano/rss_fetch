<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Oct 2025 01:49:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Sharp comparisons between sliced and standard $1$-Wasserstein distances</title>
      <link>https://arxiv.org/abs/2510.16465</link>
      <description>arXiv:2510.16465v1 Announce Type: new 
Abstract: Sliced Wasserstein distances are widely used in practice as a computationally efficient alternative to Wasserstein distances in high dimensions. In this paper, motivated by theoretical foundations of this alternative, we prove quantitative estimates between the sliced $1$-Wasserstein distance and the $1$-Wasserstein distance. We construct a concrete example to demonstrate the exponents in the estimate is sharp. We also provide a general analysis for the case where slicing involves projections onto $k$-planes and not just lines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16465v1</guid>
      <category>math.ST</category>
      <category>math.MG</category>
      <category>math.OC</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guillaume Carlier, Alessio Figalli, Quentin M\'erigot, Yi Wang</dc:creator>
    </item>
    <item>
      <title>Estimating location parameters of several exponential distributions with ordered restriction under Linex loss function</title>
      <link>https://arxiv.org/abs/2510.16527</link>
      <description>arXiv:2510.16527v1 Announce Type: new 
Abstract: Some improved estimators of the location parameters of several exponential distributions with ordered restriction are derived and compared numerically using Monte Carlo simulations. Note that the two-parameter exponential distribution is very useful in different areas like survival analysis, reliability engineering and biomedical research, where products have a guaranteed failure-free operating time before failures begin to occur. In the present manuscript, we address the component-wise estimation of location parameters of $k~(\ge 2)$ exponential distributions under an asymmetric Linex loss function. The location parameter represents a minimum guaranteed period before failure. At first, we consider the estimation of the location parameters with ordered scale parameters. Next, we address the estimation of ordered location parameters. For this, we take three different cases into account as follows: $(i)$ scale parameters are known, $(ii)$ scale parameters are unknown but equal, $(iii)$ scale parameters are unknown and unequal. In these cases, we establish general inadmissibility results. Further, using the general result, the inadmissibility of the best affine equivariant estimator is proved. The improved estimators are written in explicit forms. Additionally, we show that the results for several important life-testing schemes namely $(i)$ Type-II censoring, $(ii)$ progressive type-II censoring and $(iii)$ record value data can be obtained using i.i.d sample.Finally, for each case, the Monte Carlo simulation technique is used to compare the performance of the proposed estimators based on their risk values. The numerical results reveal a significant improvement of the proposed estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16527v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Shrajal Bajpai, Lakshmi Kanta Patra, Suchandan Kayal</dc:creator>
    </item>
    <item>
      <title>On Robust hypothesis testing with respect to Hellinger distance</title>
      <link>https://arxiv.org/abs/2510.16750</link>
      <description>arXiv:2510.16750v1 Announce Type: new 
Abstract: We study the hypothesis testing problem where the observed samples need not come from either of the specified hypotheses (distributions). In such a situation, we would like our test to be robust to this misspecification and output the distribution closer in Hellinger distance. If the underlying distribution is close to being equidistant from the hypotheses, then this would not be possible. Our main result is quantifying how close the underlying distribution has to be to either of the hypotheses. We also study the composite testing problem, where each hypothesis is a Hellinger ball around a fixed distribution. A generalized likelihood ratio test is known to work for this problem. We give an alternate test for the same.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16750v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eeshan Modak</dc:creator>
    </item>
    <item>
      <title>Batch learning equals online learning in Bayesian supervised learning</title>
      <link>https://arxiv.org/abs/2510.16892</link>
      <description>arXiv:2510.16892v1 Announce Type: new 
Abstract: Using categorical properties of probabilistic morphisms, we prove that sequential Bayesian inversions in Bayesian supervised learning models for conditionally independent (possibly not identically distributed) data, proposed by L\^e in \cite{Le2025}, coincide with batch Bayesian inversions. Based on this result, we provide a recursive formula for posterior predictive distributions in Bayesian supervised learning. We illustrate our results with Gaussian process regressions. For Polish spaces $\mathcal{Y}$ and arbitrary sets $\mathcal{X}$, we define probability measures on $\mathcal{P} (\mathcal{Y})^{\mathcal X}$, using a projective system generated by $\mathcal{Y}$ and $\mathcal{X}$. This is a generalization of a result by Orbanz \cite{Orbanz2011} for the case $\mathcal{X}$ consisting of one point. We revisit MacEacher's Dependent Dirichlet Processes (DDP) taking values on the space $\mathcal{P} (\mathcal{Y})$ of all probability measures on a measurable subset $\mathcal{Y}$ in $\mathbf{R}^n$, considered by Barrientos-Jara-Quintana \cite{BJQ2012}. We indicate how to compute posterior distributions and posterior predictive distributions of Bayesian supervised learning models with DDP priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16892v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>H\^ong V\^an L\^e</dc:creator>
    </item>
    <item>
      <title>Robust extrapolation problem for stochastic sequences with stationary increments</title>
      <link>https://arxiv.org/abs/2510.16900</link>
      <description>arXiv:2510.16900v1 Announce Type: new 
Abstract: The problem of optimal estimation of functionals $A\xi =\sum\nolimits_{k=0}^{\infty }{}a(k)\xi (k)$ and ${{A}_{N}}\xi =\sum\nolimits_{k=0}^{N}{}a(k)\xi (k)$ which depend on the unknown values of stochastic sequence $\xi (k)$ with stationary $n$th increments is considered. Estimates are based on observations of the sequence $\xi (m)$ at points of time $m=-1,-2,\ldots$. Formulas for calculating the value of the mean square error and the spectral characteristic of the optimal linear estimates of the functionals are derived in the case where spectral density of the sequence is exactly known. Formulas that determine the least favorable spectral densities and minimax (robust) spectral characteristic of the optimal linear estimates of the functionals are proposed in the case where the spectral density of the sequence is not known but a set of admissible spectral densities is given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16900v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maksym Luz, Mykhailo Moklyachuk</dc:creator>
    </item>
    <item>
      <title>On Minimax Estimation Problems for Periodically Correlated Stochastic Processes</title>
      <link>https://arxiv.org/abs/2510.16906</link>
      <description>arXiv:2510.16906v1 Announce Type: new 
Abstract: The aim of this article is to overview the problem of mean square optimal estimation of linear functionals which depend on unknown values of periodically correlated stochastic process. Estimates are based on observations of this process and noise. These problems are investigated under conditions of spectral certainty and spectral uncertainty. Formulas for calculating the main characteristics (spectral characteristic, mean square error) of the optimal linear estimates of the functionals are proposed. The least favorable spectral densities and the minimax-robust spectral characteristics of optimal estimates of the functionals are presented for given sets of admissible spectral densities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16906v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iryna Dubovets'ka, Mykhailo Moklyachuk</dc:creator>
    </item>
    <item>
      <title>Filtering Problem for Functionals of Stationary Processes with Missing Observations</title>
      <link>https://arxiv.org/abs/2510.16908</link>
      <description>arXiv:2510.16908v1 Announce Type: new 
Abstract: The problem of the mean-square optimal linear estimation of the functional $A\xi=\ \int\limits_{R^s}a(t)\xi(-t)dt,$ which depends on the unknown values of stochastic stationary process $\xi(t)$ from observations of the process $\xi(t)+\eta(t)$ at points $t\in\mathbb{R} ^{-} \backslash S $, $S=\bigcup\limits_{l=1}^{s}[-M_{l}-N_{l}, \, \ldots, \, -M_{l} ],$ $R^s=[0,\infty) \backslash S^{+},$ $S^{+}=\bigcup\limits_{l=1}^{s}[ M_{l}, \, \ldots, \, M_{l}+N_{l}]$ is considered. Formulas for calculating the mean-square error and the spectral characteristic of the optimal linear estimate of the functional are proposed under the condition of spectral certainty, where spectral densities of the processes $\xi(t)$ and $\eta(t)$ are exactly known. The minimax (robust) method of estimation is applied in the case where spectral densities are not known exactly, but sets of admissible spectral densities are given. Formulas that determine the least favorable spectral densities and the minimax spectral characteristics are proposed for some special sets of admissible spectral densities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16908v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mykhailo Moklyachuk, Maria Sidei</dc:creator>
    </item>
    <item>
      <title>A Unified Approach to Statistical Estimation Under Nonlinear Observations: Tensor Estimation and Matrix Factorization</title>
      <link>https://arxiv.org/abs/2510.16965</link>
      <description>arXiv:2510.16965v1 Announce Type: new 
Abstract: We consider the estimation of some parameter $\mathbf{x}$ living in a cone from the nonlinear observations of the form $\{y_i=f_i(\langle\mathbf{a}_i,\mathbf{x}\rangle)\}_{i=1}^m$. We develop a unified approach that first constructs a gradient from the data and then establishes the restricted approximate invertibility condition (RAIC), a condition that quantifies how well the gradient aligns with the ideal descent step. We show that RAIC yields linear convergence guarantees for the standard projected gradient descent algorithm, a Riemannian gradient descent algorithm for low Tucker-rank tensor estimation, and a factorized gradient descent algorithm for asymmetric low-rank matrix estimation. Under Gaussian designs, we establish sharp RAIC for the canonical statistical estimation problems of single index models, generalized linear models, noisy phase retrieval, and one-bit compressed sensing. Combining the convergence guarantees and the RAIC, we obtain a set of optimal statistical estimation results, including, to our knowledge, the first minimax-optimal and computationally efficient algorithms for tensor single index models, tensor logistic regression, (local) noisy tensor phase retrieval, and one-bit tensor sensing. Moreover, several other results are new or match the best known guarantees. We also provide simulations and a real-data experiment to illustrate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16965v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junren Chen, Lijun Ding, Dong Xia, Ming Yuan</dc:creator>
    </item>
    <item>
      <title>Composite Lp-quantile regression, near quantile regression and the oracle model selection theory</title>
      <link>https://arxiv.org/abs/2510.17325</link>
      <description>arXiv:2510.17325v1 Announce Type: new 
Abstract: In this paper, we consider high-dimensional Lp-quantile regression which only requires a low order moment of the error and is also a natural generalization of the above methods and Lp-regression as well. The loss function of Lp-quantile regression circumvents the non-differentiability of the absolute loss function and the difficulty of the squares loss function requiring the finiteness of error's variance and thus promises excellent properties of Lp-quantile regression. Specifically, we first develop a new method called composite Lp-quantile regression(CLpQR). We study the oracle model selection theory based on CLpQR (call the estimator CLpQR-oracle) and show in some cases of p CLpQR-oracle behaves better than CQR-oracle (based on composite quantile regression) when error's variance is infinite. Moreover, CLpQR has high efficiency and can be sometimes arbitrarily more efficient than both CQR and the least squares regression. Second, we propose another new regression method,i.e. near quantile regression and prove the asymptotic normality of the estimator when p converges to 1 and the sample size infinity simultaneously. As its applications, a new thought of smoothing quantile objective functions and a new estimation are provided for the asymptotic covariance matrix of quantile regression. Third, we develop a unified efficient algorithm for fitting high-dimensional Lp-quantile regression by combining the cyclic coordinate descent and an augmented proximal gradient algorithm. Remarkably, the algorithm turns out to be a favourable alternative of the commonly used liner programming and interior point algorithm when fitting quantile regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17325v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fuming Lin</dc:creator>
    </item>
    <item>
      <title>Spectral Thresholds in Correlated Spiked Models and Fundamental Limits of Partial Least Squares</title>
      <link>https://arxiv.org/abs/2510.17561</link>
      <description>arXiv:2510.17561v1 Announce Type: new 
Abstract: We provide a rigorous random matrix theory analysis of spiked cross-covariance models where the signals across two high-dimensional data channels are partially aligned. These models are motivated by multi-modal learning and form the standard generative setting underlying Partial Least Squares (PLS), a widely used yet theoretically underdeveloped method. We show that the leading singular values of the sample cross-covariance matrix undergo a Baik-Ben Arous-Peche (BBP)-type phase transition, and we characterize the precise thresholds for the emergence of informative components. Our results yield the first sharp asymptotic description of the signal recovery capabilities of PLS in this setting, revealing a fundamental performance gap between PLS and the Bayes-optimal estimator. In particular, we identify the SNR and correlation regimes where PLS fails to recover any signal, despite detectability being possible in principle. These findings clarify the theoretical limits of PLS and provide guidance for the design of reliable multi-modal inference methods in high dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17561v1</guid>
      <category>math.ST</category>
      <category>cond-mat.dis-nn</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pierre Mergny, Lenka Zdeborov\'a</dc:creator>
    </item>
    <item>
      <title>A robust and scalable framework for high-dimensional volatility estimation</title>
      <link>https://arxiv.org/abs/2510.17578</link>
      <description>arXiv:2510.17578v1 Announce Type: new 
Abstract: This paper introduces a robust and computationally efficient estimation framework for high-dimensional volatility models in the BEKK-ARCH class. The proposed approach employs data truncation to ensure robustness against heavy-tailed distributions and utilizes a regularized least squares method for efficient optimization in high-dimensional settings. This is achieved by leveraging an equivalent VAR representation of the BEKK-ARCH model. Non-asymptotic error bounds are established for the resulting estimators under heavy-tailed regime, and the minimax optimal convergence rate is derived. Moreover, a robust BIC and a Ridge-type estimator are introduced for selecting the model order and the number of BEKK components, respectively, with their selection consistency established under heavy-tailed settings. Simulation studies demonstrate the finite-sample performance of the proposed method, and two empirical applications illustrate its practical utility. The results show that the new framework outperforms existing alternatives in both computational speed and forecasting accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17578v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kejun Chen, Yuchang Lin, Qianqian Zhu</dc:creator>
    </item>
    <item>
      <title>Wild regenerative block bootstrap for Harris recurrent Markov chains</title>
      <link>https://arxiv.org/abs/2510.17648</link>
      <description>arXiv:2510.17648v1 Announce Type: new 
Abstract: We consider Gaussian and bootstrap approximations for the supremum of additive functionals of aperiodic Harris recurrent Markov chains. The supremum is taken over a function class that may depend on the sample size, which allows for non-Donsker settings; that is, the empirical process need not have a weak limit in the space of bounded functions. We first establish a non-asymptotic Gaussian approximation error, which holds at rates comparable to those for sums of high-dimensional independent or one-dependent vectors. Key to our derivation is the Nummelin splitting technique, which enables us to decompose the chain into either independent or one-dependent random blocks. Additionally, building upon the Nummelin splitting, we propose a Gaussian multiplier bootstrap for practical inference and establish its finite-sample guarantees in the strongly aperiodic case. Finally, we apply our bootstrap to construct a uniform confidence band for an invariant density within a certain class of diffusion processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17648v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyuseong Choi, Gabriella Ciolek</dc:creator>
    </item>
    <item>
      <title>Conformal Prediction in The Loop: A Feedback-Based Uncertainty Model for Trajectory Optimization</title>
      <link>https://arxiv.org/abs/2510.16376</link>
      <description>arXiv:2510.16376v1 Announce Type: cross 
Abstract: Conformal Prediction (CP) is a powerful statistical machine learning tool to construct uncertainty sets with coverage guarantees, which has fueled its extensive adoption in generating prediction regions for decision-making tasks, e.g., Trajectory Optimization (TO) in uncertain environments. However, existing methods predominantly employ a sequential scheme, where decisions rely unidirectionally on the prediction regions, and consequently the information from decision-making fails to be fed back to instruct CP. In this paper, we propose a novel Feedback-Based CP (Fb-CP) framework for shrinking-horizon TO with a joint risk constraint over the entire mission time. Specifically, a CP-based posterior risk calculation method is developed by fully leveraging the realized trajectories to adjust the posterior allowable risk, which is then allocated to future times to update prediction regions. In this way, the information in the realized trajectories is continuously fed back to the CP, enabling attractive feedback-based adjustments of the prediction regions and a provable online improvement in trajectory performance. Furthermore, we theoretically prove that such adjustments consistently maintain the coverage guarantees of the prediction regions, thereby ensuring provable safety. Additionally, we develop a decision-focused iterative risk allocation algorithm with theoretical convergence analysis for allocating the posterior allowable risk which closely aligns with Fb-CP. Furthermore, we extend the proposed method to handle distribution shift. The effectiveness and superiority of the proposed method are demonstrated through benchmark experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16376v1</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Han Wang, Chao Ning</dc:creator>
    </item>
    <item>
      <title>Rank-based concordance for zero-inflated data: New representations, estimators, and sharp bounds</title>
      <link>https://arxiv.org/abs/2510.16504</link>
      <description>arXiv:2510.16504v1 Announce Type: cross 
Abstract: Quantifying concordance between two random variables is crucial in applications. Traditional estimation techniques for commonly used concordance measures, such as Gini's gamma or Spearman's rho, often fail when data contain ties. This is particularly problematic for zero-inflated data, characterized by a combination of discrete mass in zero and a continuous component, which frequently appear in insurance, weather forecasting, and biomedical applications. This study provides a new formulation of Gini's gamma and Spearman's footrule, two rank-based concordance measures that incorporate absolute rank differences, tailored to zero-inflated continuous distributions. Along the way, we correct an expression of Spearman's rho for zero-inflated data previously presented in the literature. The best-possible upper and lower bounds for these measures in zero-inflated continuous settings are established, making the estimators useful and interpretable in practice. We pair our theoretical results with simulations and two real-life applications in insurance and weather forecasting, respectively. Our results illustrate the impact of zero inflation on dependence estimation, emphasizing the benefits of appropriately adjusted zero-inflated measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16504v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jasper Arends, Guanjie Lyu, Mhamed Mesfioui, Elisa Perrone, Julien Trufin</dc:creator>
    </item>
    <item>
      <title>Correlation of divergency: c-delta. Being different in a similar way or not</title>
      <link>https://arxiv.org/abs/2510.16717</link>
      <description>arXiv:2510.16717v1 Announce Type: cross 
Abstract: This paper introduces the correlation-of-divergency coefficient, c-delta, a custom statistical measure designed to quantify the similarity of internal divergence patterns between two groups of values. Unlike conventional correlation coefficients such as Pearson or Spearman, which assess the association between paired values, c-delta evaluates whether the way values differ within one group is mirrored in another. The method involves calculating, for each value, its divergence from all other values in its group, and then comparing these patterns across the two groups (e.g., human vs machine intelligence). The coefficient is normalised by the average root mean square divergence within each group, ensuring scale invariance. Potential applications of c-delta span quantum physics, where it can compare the spread of measurement outcomes between quantum systems, as well as fields such as genetics, ecology, psychometrics, manufacturing, machine learning, and social network analysis. The measure is particularly useful for benchmarking, clustering validation, and assessing the similarity of variability structures. While c-delta is not bounded between -1 and 1 and may be sensitive to outliers (but so is PMCC), it offers a new perspective for analysing internal variability and divergence. The article discusses the mathematical formulation, potential adaptations for complex data, and the interpretative considerations relevant to this alternative approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16717v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>quant-ph</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Johan F. Hoorn</dc:creator>
    </item>
    <item>
      <title>Kernel-Based Nonparametric Tests For Shape Constraints</title>
      <link>https://arxiv.org/abs/2510.16745</link>
      <description>arXiv:2510.16745v2 Announce Type: cross 
Abstract: We develop a reproducing kernel Hilbert space (RKHS) framework for nonparametric mean-variance optimization and inference on shape constraints of the optimal rule. We derive statistical properties of the sample estimator and provide rigorous theoretical guarantees, such as asymptotic consistency, a functional central limit theorem, and a finite-sample deviation bound that matches the Monte Carlo rate up to regularization. Building on these findings, we introduce a joint Wald-type statistic to test for shape constraints over finite grids. The approach comes with an efficient computational procedure based on a pivoted Cholesky factorization, facilitating scalability to large datasets. Empirical tests suggest favorably of the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16745v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rohan Sen</dc:creator>
    </item>
    <item>
      <title>Discovering Causal Relationships using Proxy Variables under Unmeasured Confounding</title>
      <link>https://arxiv.org/abs/2510.17167</link>
      <description>arXiv:2510.17167v1 Announce Type: cross 
Abstract: Inferring causal relationships between variable pairs in the observational study is crucial but challenging, due to the presence of unmeasured confounding. While previous methods employed the negative controls to adjust for the confounding bias, they were either restricted to the discrete setting (i.e., all variables are discrete) or relied on strong assumptions for identification. To address these problems, we develop a general nonparametric approach that accommodates both discrete and continuous settings for testing causal hypothesis under unmeasured confounders. By using only a single negative control outcome (NCO), we establish a new identification result based on a newly proposed integral equation that links the outcome and NCO, requiring only the completeness and mild regularity conditions. We then propose a kernel-based testing procedure that is more efficient than existing moment-restriction methods. We derive the asymptotic level and power properties for our tests. Furthermore, we examine cases where our procedure using only NCO fails to achieve identification, and introduce a new procedure that incorporates a negative control exposure (NCE) to restore identifiability. We demonstrate the effectiveness of our approach through extensive simulations and real-world data from the Intensive Care Data and World Values Survey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17167v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yong Wu, Yanwei Fu, Shouyan Wang, Yizhou Wang, Xinwei Sun</dc:creator>
    </item>
    <item>
      <title>The modified odd Burr XII-G family of distributions: Properties and Applications</title>
      <link>https://arxiv.org/abs/2510.17567</link>
      <description>arXiv:2510.17567v1 Announce Type: cross 
Abstract: The modified odd Burr XII-G family is developed, capable of incorporating bimodal and bathtub shapes in its baseline distributions, with properties derived from the exponentiated-G class. A regression model is developed within this family. The parameters are estimated by maximum likelihood, and simulations are performed to verify their consistency. The usefulness of the proposals is demonstrated by means of three real data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17567v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexsandro A. Ferreira, Gauss M. Cordeiro</dc:creator>
    </item>
    <item>
      <title>Non-asymptotic error bounds for probability flow ODEs under weak log-concavity</title>
      <link>https://arxiv.org/abs/2510.17608</link>
      <description>arXiv:2510.17608v1 Announce Type: cross 
Abstract: Score-based generative modeling, implemented through probability flow ODEs, has shown impressive results in numerous practical settings. However, most convergence guarantees rely on restrictive regularity assumptions on the target distribution -- such as strong log-concavity or bounded support. This work establishes non-asymptotic convergence bounds in the 2-Wasserstein distance for a general class of probability flow ODEs under considerably weaker assumptions: weak log-concavity and Lipschitz continuity of the score function. Our framework accommodates non-log-concave distributions, such as Gaussian mixtures, and explicitly accounts for initialization errors, score approximation errors, and effects of discretization via an exponential integrator scheme. Bridging a key theoretical challenge in diffusion-based generative modeling, our results extend convergence theory to more realistic data distributions and practical ODE solvers. We provide concrete guarantees for the efficiency and correctness of the sampling algorithm, complementing the empirical success of diffusion models with rigorous theory. Moreover, from a practical perspective, our explicit rates might be helpful in choosing hyperparameters, such as the step size in the discretization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17608v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gitte Kremling, Francesco Iafrate, Mahsa Taheri, Johannes Lederer</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Pattern Recovery in Penalized and Thresholded Estimation and its Geometry</title>
      <link>https://arxiv.org/abs/2307.10158</link>
      <description>arXiv:2307.10158v5 Announce Type: replace 
Abstract: We consider the framework of penalized estimation where the penalty term is given by a real-valued polyhedral gauge, which encompasses methods such as LASSO, generalized LASSO, SLOPE, OSCAR, PACS and others. Each of these estimators is defined through an optimization problem and can uncover a different structure or ``pattern'' of the unknown parameter vector. We define a novel and general notion of patterns based on subdifferentials and formalize an approach to measure pattern complexity. For pattern recovery, we provide a minimal condition for a particular pattern to be detected by the procedure with positive probability, the so-called accessibility condition. Using our approach, we also introduce the stronger noiseless recovery condition. For the LASSO, it is well known that the irrepresentability condition is necessary for pattern recovery with probability larger than $1/2$ and we show that the noiseless recovery plays exactly the same role in our general framework, thereby unifying and extending the irrepresentability condition to a broad class of penalized estimators. We also show that the noiseless recovery condition can be relaxed when turning to so-called thresholded penalized estimators: we prove that the necessary condition of accessibility is already sufficient for sure pattern recovery by thresholded penalized estimation provided that the noise is small enough. Throughout the article, we demonstrate how our findings can be interpreted through a geometrical lens.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10158v5</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Piotr Graczyk, Ulrike Schneider, Tomasz Skalski, Patrick Tardivel</dc:creator>
    </item>
    <item>
      <title>Online Quantile Regression</title>
      <link>https://arxiv.org/abs/2402.04602</link>
      <description>arXiv:2402.04602v4 Announce Type: replace 
Abstract: This paper addresses the challenge of integrating sequentially arriving data within the quantile regression framework, where the number of features is allowed to grow with the number of observations, the horizon is unknown, and memory is limited. We employ stochastic sub-gradient descent to minimize the empirical check loss and study its statistical properties and regret performance. In our analysis, we unveil the delicate interplay between updating iterates based on individual observations versus batches of observations, revealing distinct regularity properties in each scenario. Our method ensures long-term optimal estimation irrespective of the chosen update strategy. Importantly, our contributions go beyond prior works by achieving exponential-type concentration inequalities and attaining optimal regret and error rates that exhibit only \textsf{ short-term} sensitivity to initial errors. A key insight from our study is the delicate statistical analyses and the revelation that appropriate stepsize schemes significantly mitigate the impact of initial errors on subsequent errors and regrets. This underscores the robustness of stochastic sub-gradient descent in handling initial uncertainties, emphasizing its efficacy in scenarios where the sequential arrival of data introduces uncertainties regarding both the horizon and the total number of observations. Additionally, when the initial error rate is well-controlled, there is a trade-off between short-term error rate and long-term optimality. Due to the lack of delicate statistical analysis for squared loss, we also briefly discuss its properties and proper schemes. Extensive simulations support our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04602v4</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yinan Shen, Dong Xia, Wen-Xin Zhou</dc:creator>
    </item>
    <item>
      <title>Statistical Decision Theory with Counterfactual Loss</title>
      <link>https://arxiv.org/abs/2505.08908</link>
      <description>arXiv:2505.08908v2 Announce Type: replace 
Abstract: Many researchers have applied classical statistical decision theory to evaluate treatment choices and learn optimal policies. However, because this framework is based solely on realized outcomes under chosen decisions and ignores counterfactual outcomes, it cannot assess the quality of a decision relative to feasible alternatives. For example, in bail decisions, a judge must consider not only crime prevention but also the avoidance of unnecessary burdens on arrestees. To address this limitation, we generalize standard decision theory by incorporating counterfactual losses, allowing decisions to be evaluated using all potential outcomes. The central challenge in this counterfactual statistical decision framework is identification: since only one potential outcome is observed for each unit, the associated counterfactual risk is generally not identifiable. We prove that, under the assumption of strong ignorability, the counterfactual risk is identifiable if and only if the counterfactual loss function is additive in the potential outcomes. Moreover, we demonstrate that additive counterfactual losses can yield treatment recommendations, which differ from those based on standard loss functions when the decision problem involves more than two treatment options. One interpretation of this result is that additive counterfactual losses can capture the accuracy and difficulty of a decision, whereas standard losses account for accuracy alone. Finally, we formulate a symbolic linear inverse program that, given a counterfactual loss, determines whether its risk is identifiable, without requiring data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08908v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>econ.TH</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Benedikt Koch, Kosuke Imai</dc:creator>
    </item>
    <item>
      <title>Spacing Test for Fused Lasso</title>
      <link>https://arxiv.org/abs/2509.14229</link>
      <description>arXiv:2509.14229v2 Announce Type: replace 
Abstract: This study addresses the unresolved problem of selecting the regularization parameter in the fused lasso. In particular, we extend the framework of the Spacing Test proposed by Tibshirani et al. to the fused lasso, providing a theoretical foundation for post-selection inference by characterizing the selection event as a polyhedral constraint. Based on the analysis of the solution path of the fused lasso using a LARS-type algorithm, we derive exact conditional $p$-values for the selected change-points. Our method broadens the applicability of the Spacing Test from the standard lasso to fused penalty structures. Furthermore, through numerical experiments comparing the proposed method with sequential versions of AIC and BIC as well as cross-validation, we demonstrate that the proposed approach properly controls the type I error while achieving high detection power. This work offers a theoretically sound and computationally practical solution for parameter selection and post-selection inference in structured signal estimation problems. Keywords: Fused Lasso, Regularization parameter selection, Spacing Test for Lasso, Selective inference, Change-point detection</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14229v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rieko Tasaka, Tatsuya Kimura, Joe Suzuki</dc:creator>
    </item>
    <item>
      <title>Mitigating dimensionality effects with robust graph constructions for testing</title>
      <link>https://arxiv.org/abs/2307.15205</link>
      <description>arXiv:2307.15205v4 Announce Type: replace-cross 
Abstract: Dimensionality effects pose major challenges in high-dimensional and non-Euclidean data analysis. Graph-based two-sample tests and change-point detection are particularly attractive in this context, as they make minimal distributional assumptions and perform well across a wide range of scenarios. These methods rely on similarity graphs constructed from data, with $K$-nearest neighbor graphs and $K$-minimum spanning trees among the most effective and widely used. However, in high-dimensional and non-Euclidean regimes such graphs often produce hubs -- nodes with disproportionately high degrees -- to which graph-based methods are especially sensitive. To mitigate these dimensionality effects, we propose a robust graph construction that is far less prone to hub formation. Incorporating this construction substantially improves the power of graph-based methods across diverse settings. We further establish a theoretical foundation by proving its consistency under fixed alternatives in both low- and high-dimensional regimes. The effectiveness of the approach is demonstrated through real-world applications, including comparisons of correlation matrices for brain regions, gene expression profiles of T cells, and temporal changes in New York City taxi travel patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.15205v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yejiong Zhu, Hao Chen</dc:creator>
    </item>
    <item>
      <title>Permutation groups, partition lattices and block structures</title>
      <link>https://arxiv.org/abs/2409.10461</link>
      <description>arXiv:2409.10461v3 Announce Type: replace-cross 
Abstract: Let $G$ be a transitive permutation group on $\Omega$. The $G$-invariant partitions form a sublattice of the lattice of all partitions of $\Omega$, having the further property that all its elements are uniform (that is, have all parts of the same size). If, in addition, all the equivalence relations defining the partitions commute, then the relations form an \emph{orthogonal block structure}, a concept from statistics; in this case the lattice is modular. If it is distributive, then we have a \emph{poset block structure}, whose automorphism group is a \emph{generalised wreath product}. We examine permutation groups with these properties, which we call the \emph{OB property} and \emph{PB property} respectively, and in particular investigate when direct and wreath products of groups with these properties also have these properties.
  A famous theorem on permutation groups asserts that a transitive imprimitive group $G$ is embeddable in the wreath product of two factors obtained from the group (the group induced on a block by its setwise stabiliser, and the group induced on the set of blocks by~$G$). We extend this theorem to groups with the PB property, embeddng them into generalised wreath products. We show that the map from posets to generalised wreath products preserves intersections and inclusions.
  We have included background and historical material on these concepts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10461v3</guid>
      <category>math.GR</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marina Anagnostopoulou-Merkouri, R. A. Bailey, Peter J. Cameron</dc:creator>
    </item>
    <item>
      <title>Score-based deterministic density sampling</title>
      <link>https://arxiv.org/abs/2504.18130</link>
      <description>arXiv:2504.18130v3 Announce Type: replace-cross 
Abstract: We propose a deterministic sampling framework using Score-Based Transport Modeling for sampling an unnormalized target density $\pi$ given only its score $\nabla \log \pi$. Our method approximates the Wasserstein gradient flow on $\mathrm{KL}(f_t\|\pi)$ by learning the time-varying score $\nabla \log f_t$ on the fly using score matching. While having the same marginal distribution as Langevin dynamics, our method produces smooth deterministic trajectories, resulting in monotone noise-free convergence. We prove that our method dissipates relative entropy at the same rate as the exact gradient flow, provided sufficient training. Numerical experiments validate our theoretical findings: our method converges at the optimal rate, has smooth trajectories, and is often more sample efficient than its stochastic counterpart. Experiments on high-dimensional image data show that our method produces high-quality generations in as few as 15 steps and exhibits natural exploratory behavior. The memory and runtime scale linearly in the sample size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18130v3</guid>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vasily Ilin, Peter Sushko, Jingwei Hu</dc:creator>
    </item>
    <item>
      <title>Squared Linear Models</title>
      <link>https://arxiv.org/abs/2505.19351</link>
      <description>arXiv:2505.19351v2 Announce Type: replace-cross 
Abstract: We study statistical models that are parametrized by squares of linear forms. All critical points of the likelihood function are real and positive. There is one critical point in each region of the projective hyperplane arrangement defined by the linear forms. We examine the ideal and singular locus of the model, and we give a determinantal presentation for its likelihood correspondence. We characterize tropical degenerations of the MLE, we describe the log-normal polytopes, and we explore connections to determinantal point processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19351v2</guid>
      <category>math.AC</category>
      <category>math.AG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hannah Friedman, Bernd Sturmfels, Maximilian Wiesmann</dc:creator>
    </item>
    <item>
      <title>A Pure Hypothesis Test for Inhomogeneous Random Graph Models Based on a Kernelised Stein Discrepancy</title>
      <link>https://arxiv.org/abs/2505.21580</link>
      <description>arXiv:2505.21580v2 Announce Type: replace-cross 
Abstract: Complex data are often represented as a graph, which in turn can often be viewed as a realisation of a random graph, such as an inhomogeneous random graph model (IRG). For general fast goodness-of-fit tests in high dimensions, kernelised Stein discrepancy (KSD) tests are a powerful tool. Here, we develop a KSD-type test for IRG models that can be carried out with a single observation of the network. The test applies to a network of any size, but is particularly interesting for small networks for which asymptotic tests are not warranted. We also provide theoretical guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21580v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anum Fatima, Gesine Reinert</dc:creator>
    </item>
    <item>
      <title>Sufficient digits and density estimation: A Bayesian nonparametric approach using generalized finite P\'olya trees</title>
      <link>https://arxiv.org/abs/2506.09437</link>
      <description>arXiv:2506.09437v2 Announce Type: replace-cross 
Abstract: This paper proposes a novel approach for statistical modelling of a continuous random variable $X$ on $[0, 1)$, based on its digit representation $X=.X_1X_2\ldots$. In general, $X$ can be coupled with a latent random variable $N$ so that $(X_1,\ldots,X_N)$ becomes a sufficient statistics and $.X_{N+1}X_{N+2}\ldots$ is uniformly distributed. In line with this fact, and focusing on binary digits for simplicity, we propose a family of generalized finite P{\'o}lya trees that induces a random density for a sample, which becomes a flexible tool for density estimation. Here, the digit system may be random and learned from the data. We provide a detailed Bayesian analysis, including closed form expression for the posterior distribution which sidesteps the need of MCMC methods for posterior inference. We analyse the frequentist properties as the sample size increases, and provide sufficient conditions for consistency of the posterior distributions of the random density and $N$. We consider an extension to data spanning multiple orders of magnitude, and propose a prior distribution that encodes the so-called extended Newcomb-Benford law. Such a model shows promising results for density estimation of human-activity data. Our methodology is illustrated on several synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09437v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Beraha, Jesper M{\o}ller</dc:creator>
    </item>
    <item>
      <title>A Random Matrix Theory of Pauli Tomography</title>
      <link>https://arxiv.org/abs/2506.12010</link>
      <description>arXiv:2506.12010v3 Announce Type: replace-cross 
Abstract: Quantum state tomography (QST), the process of reconstructing some unknown quantum state $\hat\rho$ from repeated measurements on copies of said state, is a foundationally important task in the context of quantum computation and simulation. For this reason, a detailed characterization of the error $\Delta\hat\rho = \hat\rho-\hat\rho^\prime$ in a QST reconstruction $\hat\rho^\prime$ is of clear importance to quantum theory and experiment. In this work, we develop a fully random matrix theory (RMT) treatment of state tomography in informationally-complete bases; and in doing so we reveal deep connections between QST errors $\Delta\hat\rho$ and the gaussian unitary ensemble (GUE). By exploiting this connection we prove that wide classes of functions of the spectrum of $\Delta\hat\rho$ can be evaluated by substituting samples of an appropriate GUE for realizations of $\Delta\hat\rho$. This powerful and flexible result enables simple analytic treatments of the mean value and variance of the error as quantified by the trace distance $\|\Delta\hat\rho\|_\mathrm{Tr}$ (which we validate numerically for common tomographic protocols), allows us to derive a bound on the QST sample complexity, and subsequently demonstrate that said bound doesn't change under the most widely-used rephysicalization procedure. These results collectively demonstrate the flexibility, strength, and broad applicability of our approach; and lays the foundation for broader studies of RMT treatments of QST in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12010v3</guid>
      <category>quant-ph</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Keenan, John Goold, Alex Nico-Katz</dc:creator>
    </item>
    <item>
      <title>Selecting the Best Arm in One-Shot Multi-Arm RCTs: The Asymptotic Minimax-Regret Decision Framework for the Best-Population Selection Problem</title>
      <link>https://arxiv.org/abs/2509.03796</link>
      <description>arXiv:2509.03796v2 Announce Type: replace-cross 
Abstract: We develop a frequentist decision-theoretic framework for selecting the best arm in one-shot, multi-arm randomized controlled trials (RCTs). Our approach characterizes the minimax-regret (MMR) optimal decision rule for any multivariate location family reward distribution with full support. We show that the MMR rule is deterministic, unique, and computationally tractable. We then specialize to the case of multivariate normal (MVN) rewards with an arbitrary covariance matrix, and establish the local asymptotic minimaxity of a plug-in version of the rule when only estimated means and covariances are available. This asymptotic MMR (AMMR) procedure maps a covariance-matrix estimate directly into decision boundaries, allowing straightforward implementation in practice. Our analysis highlights a sharp contrast between two-arm and multi-arm designs. With two arms, the "pick-the-winner" empirical success rule remains MMR-optimal, regardless of the arm-specific variances. By contrast, with three or more arms and heterogeneous variances, the empirical success rule is no longer optimal: the MMR decision boundaries become nonlinear and systematically penalize high-variance arms, requiring stronger evidence to select them. Our multi-arm AMMR framework offers a rigorous foundation that leads to practical criteria for comparing multiple policies simultaneously.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03796v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joonhwi Joo</dc:creator>
    </item>
    <item>
      <title>Sharp Large Deviations and Gibbs Conditioning for Threshold Models in Portfolio Credit Risk</title>
      <link>https://arxiv.org/abs/2509.19151</link>
      <description>arXiv:2509.19151v2 Announce Type: replace-cross 
Abstract: We obtain sharp large deviation estimates for exceedance probabilities in dependent triangular array threshold models with a diverging number of latent factors. The prefactors quantify how latent-factor dependence and tail geometry enter at leading order, yielding three regimes: Gaussian or exponential-power tails produce polylogarithmic refinements of the Bahadur-Rao $n^{-1/2}$ law; regularly varying tails yield index-driven polynomial scaling; and bounded-support (endpoint) cases lead to an $n^{-3/2}$ prefactor. We derive these results through Laplace-Olver asymptotics for exponential integrals and conditional Bahadur-Rao estimates for the triangular arrays. Using these estimates, we establish a Gibbs conditioning principle in total variation: conditioned on a large exceedance event, the default indicators become asymptotically i.i.d., and the loss-given-default distribution is exponentially tilted (with the boundary case handled by an endpoint analysis). As illustrations, we obtain second-order approximations for Value-at-Risk and Expected Shortfall, clarifying when portfolios operate in the genuine large-deviation regime. The results provide a transferable set of techniques-localization, curvature, and tilt identification-for sharp rare-event analysis in dependent threshold systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19151v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>q-fin.MF</category>
      <category>q-fin.PM</category>
      <category>q-fin.RM</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fengnan Deng, Anand N. Vidyashankar, Jeffrey F. Collamore</dc:creator>
    </item>
    <item>
      <title>Robust Semiparametric Inference for Bayesian Additive Regression Trees</title>
      <link>https://arxiv.org/abs/2509.24634</link>
      <description>arXiv:2509.24634v2 Announce Type: replace-cross 
Abstract: We develop a semiparametric framework for inference on the mean response in missing-data settings using a corrected posterior distribution. Our approach is tailored to Bayesian Additive Regression Trees (BART), which is a powerful predictive method but whose nonsmoothness complicate asymptotic theory with multi-dimensional covariates. When using BART combined with Bayesian bootstrap weights, we establish a new Bernstein-von Mises theorem and show that the limit distribution generally contains a bias term. To address this, we introduce RoBART, a posterior bias-correction that robustifies BART for valid inference on the mean response. Monte Carlo studies support our theory, demonstrating reduced bias and improved coverage relative to existing procedures using BART.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24634v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Breunig, Ruixuan Liu, Zhengfei Yu</dc:creator>
    </item>
    <item>
      <title>Spatial and Temporal Boundaries in Difference-in-Differences: A Framework from Navier-Stokes Equation</title>
      <link>https://arxiv.org/abs/2510.11013</link>
      <description>arXiv:2510.11013v2 Announce Type: replace-cross 
Abstract: This paper develops a unified framework for identifying spatial and temporal boundaries of treatment effects in difference-in-differences designs. Starting from fundamental fluid dynamics equations (Navier-Stokes), we derive conditions under which treatment effects decay exponentially in space and time, enabling researchers to calculate explicit boundaries beyond which effects become undetectable. The framework encompasses both linear (pure diffusion) and nonlinear (advection-diffusion with chemical reactions) regimes, with testable scope conditions based on dimensionless numbers from physics (P\'eclet and Reynolds numbers). We demonstrate the framework's diagnostic capability using air pollution from coal-fired power plants. Analyzing 791 ground-based PM$_{2.5}$ monitors and 189,564 satellite-based NO$_2$ grid cells in the Western United States over 2019-2021, we find striking regional heterogeneity: within 100 km of coal plants, both pollutants show positive spatial decay (PM$_{2.5}$: $\kappa_s = 0.00200$, $d^* = 1,153$ km; NO$_2$: $\kappa_s = 0.00112$, $d^* = 2,062$ km), validating the framework. Beyond 100 km, negative decay parameters correctly signal that urban sources dominate and diffusion assumptions fail. Ground-level PM$_{2.5}$ decays approximately twice as fast as satellite column NO$_2$, consistent with atmospheric transport physics. The framework successfully diagnoses its own validity in four of eight analyzed regions, providing researchers with physics-based tools to assess whether their spatial difference-in-differences setting satisfies diffusion assumptions before applying the estimator. Our results demonstrate that rigorous boundary detection requires both theoretical derivation from first principles and empirical validation of underlying physical assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11013v2</guid>
      <category>econ.EM</category>
      <category>econ.GN</category>
      <category>math.ST</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>On Time-subordinated Brownian Motion Processes for Financial Markets</title>
      <link>https://arxiv.org/abs/2510.14108</link>
      <description>arXiv:2510.14108v2 Announce Type: replace-cross 
Abstract: In the context of time-subordinated Brownian motion models, Fourier theory and methodology are proposed to modelling the stochastic distribution of time increments. Gaussian Variance-Mean mixtures and time-subordinated models are reviewed with a key example being the Variance-Gamma process. A non-parametric characteristic function decomposition of subordinated Brownian motion is presented. The theory requires an extension of the real domain of certain characteristic functions to the complex plane, the validity of which is proven here. This allows one to characterise and study the stochastic time-change directly from the full process. An empirical decomposition of S\&amp;P log-returns is provided to illustrate the methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14108v2</guid>
      <category>q-fin.MF</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rohan Shenoy, Peter Kempthorne</dc:creator>
    </item>
  </channel>
</rss>
