<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Oct 2024 04:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Robust high-dimensional Gaussian and bootstrap approximations for trimmed sample means</title>
      <link>https://arxiv.org/abs/2410.22085</link>
      <description>arXiv:2410.22085v1 Announce Type: new 
Abstract: Most of the modern literature on robust mean estimation focuses on designing estimators which obtain optimal sub-Gaussian concentration bounds under minimal moment assumptions and sometimes also assuming contamination. This work looks at robustness in terms of Gaussian and bootstrap approximations, mainly in the regime where the dimension is exponential on the sample size. We show that trimmed sample means attain - under mild moment assumptions and contamination - Gaussian and bootstrap approximation bounds similar to those attained by the empirical mean under light tails. We apply our results to study the Gaussian approximation of VC-subgraph families and also to the problem of vector mean estimation under general norms, improving the bounds currently available in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22085v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Resende</dc:creator>
    </item>
    <item>
      <title>Refined Risk Bounds for Unbounded Losses via Transductive Priors</title>
      <link>https://arxiv.org/abs/2410.21621</link>
      <description>arXiv:2410.21621v1 Announce Type: cross 
Abstract: We revisit the sequential variants of linear regression with the squared loss, classification problems with hinge loss, and logistic regression, all characterized by unbounded losses in the setup where no assumptions are made on the magnitude of design vectors and the norm of the optimal vector of parameters. The key distinction from existing results lies in our assumption that the set of design vectors is known in advance (though their order is not), a setup sometimes referred to as transductive online learning. While this assumption seems similar to fixed design regression or denoising, we demonstrate that the sequential nature of our algorithms allows us to convert our bounds into statistical ones with random design without making any additional assumptions about the distribution of the design vectors--an impossibility for standard denoising results. Our key tools are based on the exponential weights algorithm with carefully chosen transductive (design-dependent) priors, which exploit the full horizon of the design vectors.
  Our classification regret bounds have a feature that is only attributed to bounded losses in the literature: they depend solely on the dimension of the parameter space and on the number of rounds, independent of the design vectors or the norm of the optimal solution. For linear regression with squared loss, we further extend our analysis to the sparse case, providing sparsity regret bounds that additionally depend on the magnitude of the response variables. We argue that these improved bounds are specific to the transductive setting and unattainable in the worst-case sequential setup. Our algorithms, in several cases, have polynomial time approximations and reduce to sampling with respect to log-concave measures instead of aggregating over hard-to-construct $\varepsilon$-covers of classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21621v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jian Qian, Alexander Rakhlin, Nikita Zhivotovskiy</dc:creator>
    </item>
    <item>
      <title>On the Role of Depth and Looping for In-Context Learning with Task Diversity</title>
      <link>https://arxiv.org/abs/2410.21698</link>
      <description>arXiv:2410.21698v1 Announce Type: cross 
Abstract: The intriguing in-context learning (ICL) abilities of deep Transformer models have lately garnered significant attention. By studying in-context linear regression on unimodal Gaussian data, recent empirical and theoretical works have argued that ICL emerges from Transformers' abilities to simulate learning algorithms like gradient descent. However, these works fail to capture the remarkable ability of Transformers to learn multiple tasks in context. To this end, we study in-context learning for linear regression with diverse tasks, characterized by data covariance matrices with condition numbers ranging from $[1, \kappa]$, and highlight the importance of depth in this setting. More specifically, (a) we show theoretical lower bounds of $\log(\kappa)$ (or $\sqrt{\kappa}$) linear attention layers in the unrestricted (or restricted) attention setting and, (b) we show that multilayer Transformers can indeed solve such tasks with a number of layers that matches the lower bounds. However, we show that this expressivity of multilayer Transformer comes at the price of robustness. In particular, multilayer Transformers are not robust to even distributional shifts as small as $O(e^{-L})$ in Wasserstein distance, where $L$ is the depth of the network. We then demonstrate that Looped Transformers -- a special class of multilayer Transformers with weight-sharing -- not only exhibit similar expressive power but are also provably robust under mild assumptions. Besides out-of-distribution generalization, we also show that Looped Transformers are the only models that exhibit a monotonic behavior of loss with respect to depth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21698v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khashayar Gatmiry, Nikunj Saunshi, Sashank J. Reddi, Stefanie Jegelka, Sanjiv Kumar</dc:creator>
    </item>
    <item>
      <title>On Eigenvector Approximation of Diagonalizable Random Matrices with Random Perturbations: Properties and Applications</title>
      <link>https://arxiv.org/abs/2410.21919</link>
      <description>arXiv:2410.21919v1 Announce Type: cross 
Abstract: We extend the result on the top eigenvalue of the i.i.d.\ matrix with fixed perturbations by Tao to random perturbations. In particular, we consider a setup that $\mathbf{M}=\mathbf{W}+\lambda\mathbf{u}\mathbf{u}^*$ with $\mathbf{W}$ drawn from a Ginibre Orthogonal Ensemble and the perturbation $\mathbf{u}$ drawn uniformly from $\mathcal{S}^{d-1}$. We provide several asymptotic properties about the eigenvalues and the top eigenvector of the random matrix, which can not be obtained trivially from the deterministic perturbation case.
  We also apply our results to extend the work of Max Simchowitz, which provides an optimal lower bound for approximating the eigenspace of a symmetric matrix. We present a \textit{query complexity} lower bound for approximating the eigenvector of any asymmetric but diagonalizable matrix $\mathbf{M}$ corresponding to the largest eigenvalue. We show that for every $\operatorname{gap}\in (0,1/2]$ and large enough dimension $d$, there exists a random matrix $\mathbf{M}$ with $\operatorname{gap}(\mathbf{M})=\Omega(\operatorname{gap})$, such that if a matrix-vector query product algorithm can identity a vector $\hat{\mathbf{v}}$ which satisfies $\left\|\hat{\mathbf{v}}-\mathbf{v}_1(\mathbf{M}) \right\|_2^2\le \operatorname{const}\times \operatorname{gap}$, it needs at least $\mathcal{O}\left(\frac{\log d}{\operatorname{gap}}\right)$ queries of matrix-vector products. In the inverse polynomial accuracy regime where $\epsilon \ge \frac{1}{\operatorname{poly}(d)}$, the complexity matches the upper bounds $\mathcal{O}\left(\frac{\log(d/\epsilon)}{\operatorname{gap}}\right)$, which can be obtained via the power method. As far as we know, it is the first lower bound for computing the eigenvector of an asymmetric matrix, which is far more complicated than in the symmetric case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21919v1</guid>
      <category>cs.IT</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun Chen, Zhihua Zhang</dc:creator>
    </item>
    <item>
      <title>Prior Knowledge Accelerate Variance Computing</title>
      <link>https://arxiv.org/abs/2410.21922</link>
      <description>arXiv:2410.21922v1 Announce Type: cross 
Abstract: Variance is a basic metric to evaluate the degree of data dispersion, and it is also frequently used in the realm of statistics. However, due to the computing variance and the large dataset being time-consuming, there is an urge to accelerate this computing process. The paper suggests a new method to reduce the time of this computation, it assumes a scenario in which we already know the variance of the original dataset, and the whole variance of this merge dataset could be expressed in the form of addition between the original variance and a remainder term. When we want to calculate the total variance after this adds up, the method only needs to calculate the remainder to get the result instead of recalculating the total variance again, which we named this type of method as PKA(Prior Knowledge Acceleration). The paper mathematically proves the technique's effectiveness in PKA of variance, and the conditions for this method to accelerate properly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21922v1</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawen Li</dc:creator>
    </item>
    <item>
      <title>Self-normalized Cram\'er-type Moderate Deviation of Stochastic Gradient Langevin Dynamics</title>
      <link>https://arxiv.org/abs/2410.22047</link>
      <description>arXiv:2410.22047v1 Announce Type: cross 
Abstract: In this paper, we study the self-normalized Cram\'er-type moderate deviation of the empirical measure of the stochastic gradient Langevin dynamics (SGLD). Consequently, we also derive the Berry-Esseen bound for SGLD. Our approach is by constructing a stochastic differential equation (SDE) to approximate the SGLD and then applying Stein's method as developed in [9,19], to decompose the empirical measure into a martingale difference series sum and a negligible remainder term.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22047v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongsheng Dai, Xiequan Fan, Jianya Lu</dc:creator>
    </item>
    <item>
      <title>Model-free Estimation of Latent Structure via Multiscale Nonparametric Maximum Likelihood</title>
      <link>https://arxiv.org/abs/2410.22248</link>
      <description>arXiv:2410.22248v1 Announce Type: cross 
Abstract: Multivariate distributions often carry latent structures that are difficult to identify and estimate, and which better reflect the data generating mechanism than extrinsic structures exhibited simply by the raw data. In this paper, we propose a model-free approach for estimating such latent structures whenever they are present, without assuming they exist a priori. Given an arbitrary density $p_0$, we construct a multiscale representation of the density and propose data-driven methods for selecting representative models that capture meaningful discrete structure. Our approach uses a nonparametric maximum likelihood estimator to estimate the latent structure at different scales and we further characterize their asymptotic limits. By carrying out such a multiscale analysis, we obtain coarseto-fine structures inherent in the original distribution, which are integrated via a model selection procedure to yield an interpretable discrete representation of it. As an application, we design a clustering algorithm based on the proposed procedure and demonstrate its effectiveness in capturing a wide range of latent structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22248v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bryon Aragam, Ruiyi Yang</dc:creator>
    </item>
    <item>
      <title>A compromise criterion for weighted least squares estimates</title>
      <link>https://arxiv.org/abs/2404.00753</link>
      <description>arXiv:2404.00753v3 Announce Type: replace 
Abstract: In the heteroscedastic linear model, the weighted least squares estimate of the model coefficients is more efficient than the ordinary least squares estimate. However, the practical application of weighted least squares is challenging because it requires knowledge of the error variances. Feasible weighted least squares estimates, which use approximations of the variances when they are unknown, may either be more or less efficient than the ordinary least squares estimate depending on the quality of the approximation. A direct comparison between feasible and ordinary least squares has significant implications for the application of regression analysis in varied fields, yet such a comparison remains an unresolved challenge. In this study, we address this challenge by identifying the conditions under which feasible weighted least squares estimates using fixed weights demonstrate greater efficiency than the ordinary least squares estimate. These conditions provide guidance for the design of feasible estimates using random weights. They also shed light on how certain robust regression estimates behave with respect to the linear model with normal errors of unequal variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00753v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jordan Bryan, Haibo Zhou, Didong Li</dc:creator>
    </item>
    <item>
      <title>ROTI-GCV: Generalized Cross-Validation for right-ROTationally Invariant Data</title>
      <link>https://arxiv.org/abs/2406.11666</link>
      <description>arXiv:2406.11666v2 Announce Type: replace 
Abstract: Two key tasks in high-dimensional regularized regression are tuning the regularization strength for accurate predictions and estimating the out-of-sample risk. It is known that the standard approach -- $k$-fold cross-validation -- is inconsistent in modern high-dimensional settings. While leave-one-out and generalized cross-validation remain consistent in some high-dimensional cases, they become inconsistent when samples are dependent or contain heavy-tailed covariates. As a first step towards modeling structured sample dependence and heavy tails, we use right-rotationally invariant covariate distributions -- a crucial concept from compressed sensing. In the proportional asymptotics regime where the number of features and samples grow comparably, which is known to better reflect the empirical behavior in moderately sized datasets, we introduce a new framework, ROTI-GCV, for reliably performing cross-validation under these challenging conditions. Along the way, we propose new estimators for the signal-to-noise ratio and noise variance. We conduct experiments that demonstrate the accuracy of our approach in a variety of synthetic and semi-synthetic settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11666v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kevin Luo, Yufan Li, Pragya Sur</dc:creator>
    </item>
    <item>
      <title>Diffusion Approximations for Thompson Sampling</title>
      <link>https://arxiv.org/abs/2105.09232</link>
      <description>arXiv:2105.09232v3 Announce Type: replace-cross 
Abstract: We study the behavior of Thompson sampling from the perspective of weak convergence. In the regime where the gaps between arm means scale as $1/\sqrt{n}$ with the time horizon $n$, we show that the dynamics of Thompson sampling evolve according to discrete versions of SDE's and stochastic ODE's. As $n \to \infty$, we show that the dynamics converge weakly to solutions of the corresponding SDE's and stochastic ODE's. Our weak convergence theory is developed from first principles using the Continuous Mapping Theorem, and can be easily adapted to analyze other sampling-based bandit algorithms. In this regime, we also show that the weak limits of the dynamics of many sampling-based algorithms -- including Thompson sampling designed for any exponential family of rewards, and algorithms involving bootstrap-based sampling -- coincide with those of Gaussian Thompson sampling. Moreover, in this regime, these algorithms are generally robust to model mis-specification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.09232v3</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Fan, Peter W. Glynn</dc:creator>
    </item>
    <item>
      <title>FLIPHAT: Joint Differential Privacy for High Dimensional Sparse Linear Bandits</title>
      <link>https://arxiv.org/abs/2405.14038</link>
      <description>arXiv:2405.14038v3 Announce Type: replace-cross 
Abstract: High dimensional sparse linear bandits serve as an efficient model for sequential decision-making problems (e.g. personalized medicine), where high dimensional features (e.g. genomic data) on the users are available, but only a small subset of them are relevant. Motivated by data privacy concerns in these applications, we study the joint differentially private high dimensional sparse linear bandits, where both rewards and contexts are considered as private data. First, to quantify the cost of privacy, we derive a lower bound on the regret achievable in this setting. To further address the problem, we design a computationally efficient bandit algorithm, \textbf{F}orgetfu\textbf{L} \textbf{I}terative \textbf{P}rivate \textbf{HA}rd \textbf{T}hresholding (FLIPHAT). Along with doubling of episodes and episodic forgetting, FLIPHAT deploys a variant of Noisy Iterative Hard Thresholding (N-IHT) algorithm as a sparse linear regression oracle to ensure both privacy and regret-optimality. We show that FLIPHAT achieves optimal regret in terms of privacy parameters $\epsilon, \delta$, context dimension $d$, and time horizon $T$ up to a linear factor in model sparsity and logarithmic factor in $d$. We analyze the regret by providing a novel refined analysis of the estimation error of N-IHT, which is of parallel interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14038v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sunrit Chakraborty, Saptarshi Roy, Debabrota Basu</dc:creator>
    </item>
    <item>
      <title>A Statistical Viewpoint on Differential Privacy: Hypothesis Testing, Representation and Blackwell's Theorem</title>
      <link>https://arxiv.org/abs/2409.09558</link>
      <description>arXiv:2409.09558v2 Announce Type: replace-cross 
Abstract: Differential privacy is widely considered the formal privacy for privacy-preserving data analysis due to its robust and rigorous guarantees, with increasingly broad adoption in public services, academia, and industry. Despite originating in the cryptographic context, in this review paper we argue that, fundamentally, differential privacy can be considered a \textit{pure} statistical concept. By leveraging David Blackwell's informativeness theorem, our focus is to demonstrate based on prior work that all definitions of differential privacy can be formally motivated from a hypothesis testing perspective, thereby showing that hypothesis testing is not merely convenient but also the right language for reasoning about differential privacy. This insight leads to the definition of $f$-differential privacy, which extends other differential privacy definitions through a representation theorem. We review techniques that render $f$-differential privacy a unified framework for analyzing privacy bounds in data analysis and machine learning. Applications of this differential privacy definition to private deep learning, private convex optimization, shuffled mechanisms, and U.S.\ Census data are discussed to highlight the benefits of analyzing privacy bounds under this framework compared to existing alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09558v2</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weijie J. Su</dc:creator>
    </item>
    <item>
      <title>Neural Networks Generalize on Low Complexity Data</title>
      <link>https://arxiv.org/abs/2409.12446</link>
      <description>arXiv:2409.12446v2 Announce Type: replace-cross 
Abstract: We show that feedforward neural networks with ReLU activation generalize on low complexity data, suitably defined. Given i.i.d. data generated from a simple programming language, the minimum description length (MDL) feedforward neural network which interpolates the data generalizes with high probability. We define this simple programming language, along with a notion of description length of such networks. We provide several examples on basic computational tasks, such as checking primality of a natural number, and more. For primality testing, our theorem shows the following. Suppose that we draw an i.i.d. sample of $\Theta(N^{\delta}\ln N)$ numbers uniformly at random from $1$ to $N$, where $\delta\in (0,1)$. For each number $x_i$, let $y_i = 1$ if $x_i$ is a prime and $0$ if it is not. Then with high probability, the MDL network fitted to this data accurately answers whether a newly drawn number between $1$ and $N$ is a prime or not, with test error $\leq O(N^{-\delta})$. Note that the network is not designed to detect primes; minimum description learning discovers a network which does so.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12446v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourav Chatterjee, Timothy Sudijono</dc:creator>
    </item>
    <item>
      <title>Sacred and Profane: from the Involutive Theory of MCMC to Helpful Hamiltonian Hacks</title>
      <link>https://arxiv.org/abs/2410.17398</link>
      <description>arXiv:2410.17398v2 Announce Type: replace-cross 
Abstract: In the first edition of this Handbook, two remarkable chapters consider seemingly distinct yet deeply connected subjects ...</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17398v2</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathan E. Glatt-Holtz, Andrew J. Holbrook, Justin A. Krometis, Cecilia F. Mondaini, Ami Sheth</dc:creator>
    </item>
  </channel>
</rss>
