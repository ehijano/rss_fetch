<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Feb 2025 02:50:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Flexible Bayesian Tensor Decomposition for Verbal Autopsy Data</title>
      <link>https://arxiv.org/abs/2502.00171</link>
      <description>arXiv:2502.00171v1 Announce Type: new 
Abstract: Cause-of-death data is fundamental for understanding population health trends and inequalities as well as designing and evaluating public health interventions. A significant proportion of global deaths, particularly in low- and middle-income countries (LMICs), do not have medically certified causes assigned. In such settings, verbal autopsy (VA) is a widely adopted approach to estimate disease burdens by interviewing caregivers of the deceased. Recently, latent class models have been developed to model the joint distribution of symptoms and perform probabilistic cause-of-death assignment. A large number of latent classes are usually needed in order to characterize the complex dependence among symptoms, making the estimated symptom profiles challenging to summarize and interpret. In this paper, we propose a flexible Bayesian tensor decomposition framework that balances the predictive accuracy of the cause-of-death assignment task and the interpretability of the latent structures. The key to our approach is to partition symptoms into groups and model the joint distributions of group-level symptom sub-profiles. The proposed methods achieve better predictive accuracy than existing VA methods and provide a more parsimonious representation of the symptom distributions. We show our methods provide new insights into the clustering patterns of both symptoms and causes using the PHMRC gold-standard VA dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00171v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Zhu, Zehang Richard Li</dc:creator>
    </item>
    <item>
      <title>A Two-Stage Trip Inference Model of Purposes and Socio-Economic Attributes of Regular Public Transit Users</title>
      <link>https://arxiv.org/abs/2502.00644</link>
      <description>arXiv:2502.00644v1 Announce Type: new 
Abstract: Data-driven research is becoming a new paradigm in transportation, but the natural lack of individual socio-economic attributes in transportation data makes research such as activity purpose inference and mobility pattern identification lack convincingness and verifiability. In this paper, a two-stage trip purpose and socio-economic attributes inference model is proposed based on travel resident survey and smart card data. In the first stage, the trip purpose of each trip is inferred by a combination of rule-based and XGBoost models. In the second stage, based on the trip purpose, a machine-learning model is built to inference the socio-economic attributes of individuals. A teacher-student model based on self-training is then applied on the models above to transfer them to smart card data. The impact of independent variables of socio-economic attributes inference model is also investigated. The results show that models for inferring trip purposes and socio-economic attributes have overall accuracies of 92.7% and 76.3%, respectively. Travel time, arrival time, departure time and purpose of the first two trips are most important factors on age and job status, while the land price of jobs-housing are significant to the inference of individual incomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00644v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yitong Chen, Wentao Dong, Chengcheng Yu, Quan Yuan, Chao Yang</dc:creator>
    </item>
    <item>
      <title>Using Individualized Treatment Effects to Assess Treatment Effect Heterogeneity</title>
      <link>https://arxiv.org/abs/2502.00713</link>
      <description>arXiv:2502.00713v1 Announce Type: new 
Abstract: Assessing treatment effect heterogeneity (TEH) in clinical trials is crucial, as it provides insights into the variability of treatment responses among patients, influencing important decisions related to drug development. Furthermore, it can lead to personalized medicine by tailoring treatments to individual patient characteristics. This paper introduces novel methodologies for assessing treatment effects using the individual treatment effect as a basis. To estimate this effect, we use a Double Robust (DR) learner to infer a pseudo-outcome that reflects the causal contrast. This pseudo-outcome is then used to perform three objectives: (1) a global test for heterogeneity, (2) ranking covariates based on their influence on effect modification, and (3) providing estimates of the individualized treatment effect. We compare our DR-learner with various alternatives and competing methods in a simulation study, and also use it to assess heterogeneity in a pooled analysis of five Phase III trials in psoriatic arthritis. By integrating these methods with the recently proposed WATCH workflow (Workflow to Assess Treatment Effect Heterogeneity in Drug Development for Clinical Trial Sponsors), we provide a robust framework for analyzing TEH, offering insights that enable more informed decision-making in this challenging area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00713v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Konstantinos Sechidis, Cong Zhang, Sophie Sun, Yao Chen, Asher Spector, Bj\"orn Bornkamp</dc:creator>
    </item>
    <item>
      <title>Predictive Information Decomposition as a Tool to Quantify Emergent Dynamical Behaviors In Physiological Networks</title>
      <link>https://arxiv.org/abs/2502.00945</link>
      <description>arXiv:2502.00945v1 Announce Type: new 
Abstract: Objective: This work introduces a framework for multivariate time series analysis aimed at detecting and quantifying collective emerging behaviors in the dynamics of physiological networks. Methods: Given a network system mapped by a vector random process, we compute the predictive information (PI) between the present and past network states and dissect it into amounts quantifying the unique, redundant and synergistic information shared by the present of the network and the past of each unit. Emergence is then quantified as the prevalence of the synergistic over the redundant contribution. The framework is implemented in practice using vector autoregressive (VAR) models. Results: Validation in simulated VAR processes documents that emerging behaviors arise in networks where multiple causal interactions coexist with internal dynamics. The application to cardiovascular and respiratory networks mapping the beat-to-beat variability of heart rate, arterial pressure and respiration measured at rest and during postural stress reveals the presence of statistically significant net synergy, as well as its modulation with sympathetic nervous system activation. Conclusion: Causal emergence can be efficiently assessed decomposing the PI of network systems via VAR models applied to multivariate time series. This approach evidences the synergy/redundancy balance as a hallmark of integrated short-term autonomic control in cardiovascular and respiratory networks. Significance: Measures of causal emergence provide a practical tool to quantify the mechanisms of causal influence that determine the dynamic state of cardiovascular and neural network systems across distinct physiopathological conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00945v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Luca Faes, Gorana Mijatovic, Laura Sparacino, Alberto Porta</dc:creator>
    </item>
    <item>
      <title>Bayesian Probit Multi-Study Non-negative Matrix Factorization for Mutational Signatures</title>
      <link>https://arxiv.org/abs/2502.01468</link>
      <description>arXiv:2502.01468v1 Announce Type: new 
Abstract: Mutational signatures are patterns of somatic mutations in tumor genomes that provide insights into underlying mutagenic processes and cancer origin. Developing reliable methods for their estimation is of growing importance in cancer biology. Somatic mutation data are often collected for different cancer types, highlighting the need for multi-study approaches that enable joint analysis in a principled and integrative manner. Despite significant advancements, statistical models tailored for analyzing the genomes of multiple cancer types remain underexplored. In this work, we introduce a Bayesian Multi-Study Non-negative Matrix Factorization (NMF) approach that uses mixture modeling to incorporate sparsity in the exposure weights of each subject to mutational signatures, allowing for individual tumor profiles to be represented by a subset rather than all signatures, and making this subset depend on covariates. This allows for a) more precise ability to identify meaningful contributions of mutational signatures at the individual level; b) estimation of the prevalence of activity of signatures within a cancer type, defined by the proportion of tumor profiles where a certain signature is present; and c) de-novo identification of interpretable patient subtypes based on the mutational signatures present within their mutational profile. We apply our approach to the mutational profiles of tumors from seven different cancer types, demonstrating its ability to accurately estimate mutational signatures while uncovering both individual and tissue-specific differences. An R package implementing our method is available at https://github.com/blhansen/BAPmultiNMF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01468v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Blake Hansen, Isabella N. Grabski, Giovanni Parmigiani, Roberta De Vito</dc:creator>
    </item>
    <item>
      <title>A Statistical Framework for District Energy Long-term Electric Load Forecasting</title>
      <link>https://arxiv.org/abs/2502.01531</link>
      <description>arXiv:2502.01531v1 Announce Type: new 
Abstract: An accurate forecast of electric demand is essential for the optimal design of a generation system. For district installations, the projected lifespan may extend one or two decades. The reliance on a single-year forecast, combined with a fixed load growth rate, is the current industry standard, but does not support a multi-decade investment. Existing work on long-term forecasting focuses on annual growth rate and/or uses time resolution that is coarser than hourly. To address the gap, we propose multiple statistical forecast models, verified over as long as an 11-year horizon. Combining demand data, weather data, and occupancy trends results in a hybrid statistical model, i.e., generalized additive model (GAM) with a seasonal autoregressive integrated moving average (SARIMA) of the GAM residuals, a multiple linear regression (MLR) model, and a GAM with ARIMA errors model. We evaluate accuracy based on: (i) annual growth rates of monthly peak loads; (ii) annual growth rates of overall energy consumption; (iii) preservation of daily, weekly, and month-to-month trends that occur within each year, known as the 'seasonality' of the data; and, (iv) realistic representation of demand for a full range of weather and occupancy conditions. For example, the models yield an 11-year forecast from a one-year training data set with a normalized root mean square error of 9.091%, a six-year forecast from a one-year training data set with a normalized root mean square error of 8.949%, and a one-year forecast from a 1.2-year training data set with a normalized root mean square error of 6.765%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01531v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emily Royal, Soutir Bandyopadhyay, Alexandra Newman, Qiuhua Huang, Paulo Cesar Tabares-Velasco</dc:creator>
    </item>
    <item>
      <title>Statistical enhance learning for modeling and prediction tennis matches at Grand Slam tournaments</title>
      <link>https://arxiv.org/abs/2502.01613</link>
      <description>arXiv:2502.01613v1 Announce Type: new 
Abstract: In this manuscript, we concentrate on a specific type of covariates, which we call statistically enhanced, for modeling tennis matches for men at Grand slam tournaments. Our goal is to assess whether these enhanced covariates have the potential to improve statistical learning approaches, in particular, with regard to the predictive performance. For this purpose, various proposed regression and machine learning model classes are compared with and without such features. To achieve this, we considered three slightly enhanced variables, namely elo rating along with two different player age variables. This concept has already been successfully applied in football, where additional team ability parameters, which were obtained from separate statistical models, were able to improve the predictive performance.
  In addition, different interpretable machine learning (IML) tools are employed to gain insights into the factors influencing the outcomes of tennis matches predicted by complex machine learning models, such as the random forest. Specifically, partial dependence plots (PDP) and individual conditional expectation (ICE) plots are employed to provide better interpretability for the most promising ML model from this work. Furthermore, we conduct a comparison of different regression and machine learning approaches in terms of various predictive performance measures such as classification rate, predictive Bernoulli likelihood, and Brier score. This comparison is carried out on external test data using cross-validation, rolling window, and expanding window strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01613v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nourah Buhamra, Andreas Groll</dc:creator>
    </item>
    <item>
      <title>An Extension of the Iterated Moving Average</title>
      <link>https://arxiv.org/abs/2502.00128</link>
      <description>arXiv:2502.00128v1 Announce Type: cross 
Abstract: This work introduces an extension of the iterated moving average filter, called the Extended Kolmogorov-Zurbenko (EKZ) filter for time series and spatio-temporal analysis. The iterated application of a central simple moving average (SMA) filter, also known as a Kolmogorov-Zurbenko (KZ) filter, is a low-pass filter defined by the length of the moving average window and the number of iterations. These two arguments determine the filter properties such as the energy transfer function and cut-off frequency. However, the existing KZ filter is only defined for positive odd integer widow lengths. Therefore, for any finite time series dataset there is only a relatively small selection of possible window lengths, determined by the length of the dataset, with which to apply a KZ filter. This inflexibility impedes use of KZ filters for a wide variety of applications such as time series component separation, filtration, signal reconstruction, energy transfer function design, modeling, and forecasting. The proposed EKZ filter extends the KZ and SMA filters by permitting a widened range of argument selection for the filter window length providing the choice of an infinite number of filters that may be applied to a dataset, affording enhanced control over the filter characteristics and greater practical application. Simulations and real data application examples are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00128v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edward Valachovic</dc:creator>
    </item>
    <item>
      <title>Prior selection for the precision parameter of Dirichlet Process Mixtures</title>
      <link>https://arxiv.org/abs/2502.00864</link>
      <description>arXiv:2502.00864v1 Announce Type: cross 
Abstract: Consider a Dirichlet process mixture model (DPM) with random precision parameter $\alpha$, inducing $K_n$ clusters over $n$ observations through its latent random partition. Our goal is to specify the prior distribution $p\left(\alpha\mid\boldsymbol{\eta}\right)$, including its fixed parameter vector $\boldsymbol{\eta}$, in a way that is meaningful.
  Existing approaches can be broadly categorised into three groups. Those in the first group rely on the linkage between $p\left(\alpha\mid\boldsymbol{\eta}\right)$ and $p\left(K_n\right)$ to draw conclusions on how to best choose $\boldsymbol{\eta}$ to reflect one's prior knowledge of $K_{n}$; we call them sample-size-dependent. Those in the second and third group consist instead of using quasi-degenerate or improper priors, respectively.
  In this article, we show how all three methods have limitations, especially for large $n$. We enrich the first group by working out and testing Jeffreys' prior in the context of the DPM framework, and by evaluating its behaviour. Then we propose an alternative methodology which does not depend on $K_n$ or on the size of the available sample, but rather on the relationship between the largest stick lengths in the stick-breaking construction of the DPM; and which reflects those prior beliefs in $p\left(\alpha\mid\boldsymbol{\eta}\right)$. We conclude with an example where existing sample-size-dependent approaches fail, while our sample-size-independent approach continues to be feasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00864v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlo Vicentini, Ian Hyla Jermyn</dc:creator>
    </item>
    <item>
      <title>Simulating Application Behavior for Network Monitoring and Security</title>
      <link>https://arxiv.org/abs/2502.01049</link>
      <description>arXiv:2502.01049v1 Announce Type: cross 
Abstract: Existing network simulations often rely on simplistic models that send packets at random intervals, failing to capture the critical role of application-level behaviour. This paper presents a statistical approach that extracts and models application behaviour using probability density functions to generate realistic network simulations. By convolving learned application patterns, the framework produces dynamic, scalable traffic representations that closely mimic real-world networks. The method enables rigorous testing of network monitoring tools and anomaly detection systems by dynamically adjusting application behaviour. It is lightweight, capable of running multiple emulated applications on a single machine, and scalable for analysing large networks where real data collection is impractical. To encourage adoption and further testing, the full code is provided as open-source, allowing researchers and practitioners to replicate and extend the framework for diverse network environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01049v1</guid>
      <category>cs.NI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Murugaraj Odiathevar, Kim Chung Yup</dc:creator>
    </item>
    <item>
      <title>A Bayesian theory for estimation of biodiversity</title>
      <link>https://arxiv.org/abs/2502.01333</link>
      <description>arXiv:2502.01333v1 Announce Type: cross 
Abstract: Statistical inference on biodiversity has a rich history going back to RA Fisher. An influential ecological theory suggests the existence of a fundamental biodiversity number, denoted $\alpha$, which coincides with the precision parameter of a Dirichlet process (DP). In this paper, motivated by this theory, we develop Bayesian nonparametric methods for statistical inference on biodiversity, building on the literature on Gibbs-type priors. We argue that $\sigma$-diversity is the most natural extension of the fundamental biodiversity number and discuss strategies for its estimation. Furthermore, we develop novel theory and methods starting with an Aldous-Pitman (AP) process, which serves as the building block for any Gibbs-type prior with a square-root growth rate. We propose a modeling framework that accommodates the hierarchical structure of Linnean taxonomy, offering a more refined approach to quantifying biodiversity. The analysis of a large and comprehensive dataset on Amazon tree flora provides a motivating application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01333v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tommaso Rigon, Ching-Lung Hsu, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>A Poisson Process AutoDecoder for X-ray Sources</title>
      <link>https://arxiv.org/abs/2502.01627</link>
      <description>arXiv:2502.01627v1 Announce Type: cross 
Abstract: X-ray observing facilities, such as the Chandra X-ray Observatory and the eROSITA, have detected millions of astronomical sources associated with high-energy phenomena. The arrival of photons as a function of time follows a Poisson process and can vary by orders-of-magnitude, presenting obstacles for common tasks such as source classification, physical property derivation, and anomaly detection. Previous work has either failed to directly capture the Poisson nature of the data or only focuses on Poisson rate function reconstruction. In this work, we present Poisson Process AutoDecoder (PPAD). PPAD is a neural field decoder that maps fixed-length latent features to continuous Poisson rate functions across energy band and time via unsupervised learning. PPAD reconstructs the rate function and yields a representation at the same time. We demonstrate the efficacy of PPAD via reconstruction, regression, classification and anomaly detection experiments using the Chandra Source Catalog.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01627v1</guid>
      <category>astro-ph.IM</category>
      <category>astro-ph.HE</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanke Song, Victoria Ashley Villar, Juan Rafael Martinez-Galarza, Steven Dillmann</dc:creator>
    </item>
    <item>
      <title>Measuring sampling plan utility in post-marketing surveillance of medical products</title>
      <link>https://arxiv.org/abs/2312.05678</link>
      <description>arXiv:2312.05678v3 Announce Type: replace 
Abstract: Ensuring product quality is critical to combating the global challenge of substandard and falsified medical products. Post-marketing surveillance is a central quality-assurance activity in which products from consumer-facing locations are collected and tested. Regulators in low-resource settings use post-marketing surveillance to evaluate product quality across locations and determine corrective actions. Part of post-marketing surveillance is developing a sampling plan, which specifies where to test and the number of tests to conduct at a location. With limited resources, it is important to base decisions on the utility of the samples tested. We propose a Bayesian approach to generate a comprehensive utility metric for sampling plans. This sampling plan utility integrates regulatory risk assessments with prior testing data, available supply-chain information, and valuations of regulatory objectives. We develop an efficient method for calculating sampling plan utility. We illustrate the value of the utility metric with a case study based on de-identified post-marketing surveillance data from a low-resource setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.05678v3</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eugene Wickett, Matthew Plumlee, Karen Smilowitz, Souly Phanouvong, Timothy Nwogu</dc:creator>
    </item>
    <item>
      <title>Impacts of Climate Change on Mortality: An extrapolation of temperature effects based on time series data in France</title>
      <link>https://arxiv.org/abs/2406.02054</link>
      <description>arXiv:2406.02054v3 Announce Type: replace 
Abstract: Most contemporary mortality models rely on extrapolating trends or past events. However, population dynamics will be significantly impacted by climate change, notably the influence of temperatures on mortality. In this paper, we introduce a novel approach to incorporate temperature effects on projected mortality using a multi-population mortality model. This method combines a stochastic mortality model with a climate epidemiology model, predicting mortality variations due to daily temperature fluctuations, be it excesses or insufficiencies. The significance of this approach lies in its ability to disrupt mortality projections by utilizing temperature forecasts from climate models and to assess the impact of this unaccounted risk factor in conventional mortality models. We illustrate this proposed mortality model using French data stratified by gender, focusing on past temperatures and mortality. Utilizing climate model predictions across various IPCC scenarios, we investigate gains and loss in life expectancy linked to temperatures and the additional mortality induced by extreme heatwaves, and quantify them by assessing this new risk factor in prediction intervals. Furthermore, we analyze the geographical differences across the Metropolitan France.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02054v3</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Quentin Guibert (CEREMADE), Ga\"elle Pincemin (LSAF), Fr\'ed\'eric Planchet (LSAF)</dc:creator>
    </item>
    <item>
      <title>Analyzing the factors that are involved in length of inpatient stay at the hospital for diabetes patients</title>
      <link>https://arxiv.org/abs/2406.05189</link>
      <description>arXiv:2406.05189v2 Announce Type: replace 
Abstract: The paper investigates the escalating concerns surrounding the surge in diabetes cases, exacerbated by the COVID-19 pandemic, and the subsequent strain on medical resources. The research aims to construct a predictive model quantifying factors influencing inpatient hospital stay durations for diabetes patients, offering insights to hospital administrators for improved patient management strategies. The literature review highlights the increasing prevalence of diabetes, emphasizing the need for continued attention and analysis of urban-rural disparities in healthcare access. International studies underscore the financial implications and healthcare burden associated with diabetes-related hospitalizations and complications, emphasizing the significance of effective management strategies. The methodology involves a quantitative approach, utilizing a dataset comprising 10,000 observations of diabetic inpatient encounters in U.S. hospitals from 1999 to 2008. Predictive modeling techniques, particularly Generalized Linear Models (GLM), are employed to develop a model predicting hospital stay durations based on patient demographics, admission types, medical history, and treatment regimen. The results highlight the influence of age, medical history, and treatment regimen on hospital stay durations for diabetes patients. Despite model limitations, such as heteroscedasticity and deviations from normality in residual analysis, the findings offer valuable insights for hospital administrators in patient management. The paper concludes with recommendations for future research to address model limitations and explore the implications of predictive models on healthcare management strategies, ensuring equitable patient care and resource allocation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05189v2</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jorden Lam, Kunpeng Xu</dc:creator>
    </item>
    <item>
      <title>Data Space Inversion for Efficient Predictions and Uncertainty Quantification for Geothermal Models</title>
      <link>https://arxiv.org/abs/2407.15401</link>
      <description>arXiv:2407.15401v3 Announce Type: replace 
Abstract: The ability to make accurate predictions with quantified uncertainty provides a crucial foundation for the successful management of a geothermal reservoir. Conventional approaches for making predictions using geothermal reservoir models involve estimating unknown model parameters using field data, then propagating the uncertainty in these estimates through to the predictive quantities of interest. However, the unknown parameters are not always of direct interest; instead, the predictions are of primary importance. Data space inversion (DSI) is an alternative methodology that allows for the efficient estimation of predictive quantities of interest, with quantified uncertainty, that avoids the need to estimate model parameters entirely. In this paper, we illustrate the applicability of DSI to geothermal reservoir modelling. We first review the processes of model calibration, prediction and uncertainty quantification from a Bayesian perspective, and introduce data space inversion as a simple, efficient technique for approximating the posterior predictive distribution. We then introduce a modification of the typical DSI algorithm that allows us to sample directly and efficiently from the DSI approximation to the posterior predictive distribution, and apply the algorithm to two model problems in geothermal reservoir modelling. We evaluate the accuracy and efficiency of our DSI algorithm relative to other common methods for uncertainty quantification and study how the number of reservoir model simulations affects the resulting approximation to the posterior predictive distribution. Our results demonstrate that data space inversion is a robust and efficient technique for making predictions with quantified uncertainty using geothermal reservoir models, providing a useful alternative to more conventional approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15401v3</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex de Beer, Andrew Power, Daniel Wong, Ken Dekkers, Michael Gravatt, Elvar K. Bjarkason, John P. O'Sullivan, Michael J. O'Sullivan, Oliver J. Maclaren, Ruanui Nicholson</dc:creator>
    </item>
    <item>
      <title>Covariance Regression for High Dimensional Neural Data via Graph</title>
      <link>https://arxiv.org/abs/2409.19717</link>
      <description>arXiv:2409.19717v2 Announce Type: replace 
Abstract: Modern recording techniques enable neuroscientists to simultaneously study neural activity across large populations of neurons, with capturing predictor-dependent correlations being a fundamental challenge in neuroscience. Moreover, the fact that input covariates often lie in restricted subdomains, according to experimental settings, makes inference even more challenging. To address these challenges, we propose a set of nonparametric mean-covariance regression models for high-dimensional neural activity with restricted inputs. These models reduce the dimensionality of neural responses by employing a lower-dimensional latent factor model, where both factor loadings and latent factors are predictor-dependent, to jointly model mean and covariance across covariates. The smoothness of neural activity across experimental conditions is modeled nonparametrically using two Gaussian processes (GPs), applied to both loading basis and latent factors. Additionally, to account for the covariates lying in restricted subspace, we incorporate graph information into the covariance structure. To flexibly infer the model, we use an MCMC algorithm to sample from posterior distributions. After validating and studying the properties of proposed methods by simulations, we apply them to two neural datasets (local field potential and neural spiking data) to demonstrate the usage of models for continuous and counting observations. Overall, the proposed methods provide a framework to jointly model covariate-dependent mean and covariance in high dimensional neural data, especially when the covariates lie in restricted domains. The framework is general and can be easily adapted to various applications beyond neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19717v2</guid>
      <category>stat.AP</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ganchao Wei</dc:creator>
    </item>
    <item>
      <title>Expected Diverse Utility (EDU): Diverse Bayesian Optimization of Expensive Computer Simulators</title>
      <link>https://arxiv.org/abs/2410.01196</link>
      <description>arXiv:2410.01196v2 Announce Type: replace 
Abstract: The optimization of expensive black-box simulators arises in a myriad of modern scientific and engineering applications. Bayesian optimization provides an appealing solution, by leveraging a fitted surrogate model to guide the selection of subsequent simulator evaluations. In practice, however, the objective is often not to obtain a single good solution, but rather a ``basket'' of good solutions from which users can choose for downstream decision-making. This need arises in our motivating application for real-time control of internal combustion engines for flight propulsion, where a diverse set of control strategies is essential for stable flight control. There has been little work on this front for Bayesian optimization. We thus propose a new Expected Diverse Utility (EDU) method that searches for diverse ``$\epsilon$-optimal'' solutions: locally-optimal solutions within a tolerance level $\epsilon &gt; 0$ from a global optimum. We show that EDU yields a closed-form acquisition function under a Gaussian process surrogate model, which facilitates efficient sequential queries via automatic differentiation. This closed form further reveals a novel exploration-exploitation-diversity trade-off, which incorporates the desired diversity property within the well-known exploration-exploitation trade-off. We demonstrate the improvement of EDU over existing methods in a suite of numerical experiments, then explore the EDU in two applications on rover trajectory optimization and engine control for flight propulsion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01196v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John Joshua Miller, Simon Mak, Benny Sun, Sai Ranjeet Narayanan, Suo Yang, Zongxuan Sun, Kenneth S. Kim, Chol-Bum Mike Kweon</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification in Synthetic Controls with Staggered Treatment Adoption</title>
      <link>https://arxiv.org/abs/2210.05026</link>
      <description>arXiv:2210.05026v5 Announce Type: replace-cross 
Abstract: We propose principled prediction intervals to quantify the uncertainty of a large class of synthetic control predictions (or estimators) in settings with staggered treatment adoption, offering precise non-asymptotic coverage probability guarantees. From a methodological perspective, we provide a detailed discussion of different causal quantities to be predicted, which we call causal predictands, allowing for multiple treated units with treatment adoption at possibly different points in time. From a theoretical perspective, our uncertainty quantification methods improve on prior literature by (i) covering a large class of causal predictands in staggered adoption settings, (ii) allowing for synthetic control methods with possibly nonlinear constraints, (iii) proposing scalable robust conic optimization methods and principled data-driven tuning parameter selection, and (iv) offering valid uniform inference across post-treatment periods. We illustrate our methodology with an empirical application studying the effects of economic liberalization on real GDP per capita for Sub-Saharan African countries. Companion software packages are provided in Python, R, and Stata.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.05026v5</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Yingjie Feng, Filippo Palomba, Rocio Titiunik</dc:creator>
    </item>
    <item>
      <title>The Chained Difference-in-Differences</title>
      <link>https://arxiv.org/abs/2301.01085</link>
      <description>arXiv:2301.01085v4 Announce Type: replace-cross 
Abstract: This paper studies the identification, estimation, and inference of long-term (binary) treatment effect parameters when balanced panel data is not available, or consists of only a subset of the available data. We develop a new estimator: the chained difference-in-differences, which leverages the overlapping structure of many unbalanced panel data sets. This approach consists in aggregating a collection of short-term treatment effects estimated on multiple incomplete panels. Our estimator accommodates (1) multiple time periods, (2) variation in treatment timing, (3) treatment effect heterogeneity, (4) general missing data patterns, and (5) sample selection on observables. We establish the asymptotic properties of the proposed estimator and discuss identification and efficiency gains in comparison to existing methods. Finally, we illustrate its relevance through (i) numerical simulations, and (ii) an application about the effects of an innovation policy in France.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.01085v4</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christophe Bell\'ego, David Benatia, Vincent Dortet-Bernardet</dc:creator>
    </item>
    <item>
      <title>Defining and comparing SICR-events for classifying impaired loans under IFRS 9</title>
      <link>https://arxiv.org/abs/2303.03080</link>
      <description>arXiv:2303.03080v4 Announce Type: replace-cross 
Abstract: The IFRS 9 accounting standard requires the prediction of credit deterioration in financial instruments, i.e., significant increases in credit risk (SICR). However, the definition of such a SICR-event is inherently ambiguous, given its current reliance on evaluating the change in the estimated probability of default (PD) against some arbitrary threshold. We examine the shortcomings of this PD-comparison approach and propose an alternative framework for generating SICR-definitions based on three parameters: delinquency, stickiness, and the outcome period. Having varied these framework parameters, we obtain 27 unique SICR-definitions and fit logistic regression models accordingly using rich South African mortgage and macroeconomic data. For each definition and corresponding model, the resulting SICR-rates are analysed at the portfolio-level on their stability over time and their responsiveness to economic downturns. At the account-level, we compare both the accuracy and dynamicity of the SICR-predictions, and discover several interesting trends and trade-offs. These results can help any bank with appropriately setting the three framework parameters in defining SICR-events for prediction purposes. We demonstrate this process by comparing the best-performing SICR-model to the PD-comparison approach, and show the latter's inferiority as an early-warning system. Our work can therefore guide the formulation, modelling, and testing of any SICR-definition, thereby promoting the timeous recognition of credit losses; the main imperative of IFRS 9.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.03080v4</guid>
      <category>q-fin.RM</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arno Botha, Esmerelda Oberholzer, Janette Larney, Riaan de Jongh</dc:creator>
    </item>
    <item>
      <title>Wasserstein medians: robustness, PDE characterization and numerics</title>
      <link>https://arxiv.org/abs/2307.01765</link>
      <description>arXiv:2307.01765v2 Announce Type: replace-cross 
Abstract: We investigate the notion of Wasserstein median as an alternative to the Wasserstein barycenter, which has become popular but may be sensitive to outliers. In terms of robustness to corrupted data, we indeed show that Wasserstein medians have a breakdown point of approximately $\frac{1}{2}$. We give explicit constructions of Wasserstein medians in dimension one which enable us to obtain $L^p$ estimates (which do not hold in higher dimensions). We also address dual and multimarginal reformulations. In convex subsets of $\mathbb{R}^d$, we connect Wasserstein medians to a minimal (multi) flow problem \`a la Beckmann and a system of PDEs of Monge-Kantorovich-type, for which we propose a $p$-Laplacian approximation. Our analysis eventually leads to a new numerical method to compute Wasserstein medians, which is based on a Douglas-Rachford scheme applied to the minimal flow formulation of the problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.01765v2</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1137/23M1624786</arxiv:DOI>
      <dc:creator>Guillaume Carlier, Enis Chenchene, Katharina Eichinger</dc:creator>
    </item>
    <item>
      <title>Limits of Large Language Models in Debating Humans</title>
      <link>https://arxiv.org/abs/2402.06049</link>
      <description>arXiv:2402.06049v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown remarkable promise in communicating with humans. Their potential use as artificial partners with humans in sociological experiments involving conversation is an exciting prospect. But how viable is it? Here, we rigorously test the limits of agents that debate using LLMs in a preregistered study that runs multiple debate-based opinion consensus games. Each game starts with six humans, six agents, or three humans and three agents. We found that agents can blend in and concentrate on a debate's topic better than humans, improving the productivity of all players. Yet, humans perceive agents as less convincing and confident than other humans, and several behavioral metrics of humans and agents we collected deviate measurably from each other. We observed that agents are already decent debaters, but their behavior generates a pattern distinctly different from the human-generated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06049v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Flamino, Mohammed Shahid Modi, Boleslaw K. Szymanski, Brendan Cross, Colton Mikolajczyk</dc:creator>
    </item>
    <item>
      <title>Exploring Spatial Context: A Comprehensive Bibliography of GWR and MGWR</title>
      <link>https://arxiv.org/abs/2404.16209</link>
      <description>arXiv:2404.16209v3 Announce Type: replace-cross 
Abstract: Local spatial models such as Geographically Weighted Regression (GWR) and Multiscale Geographically Weighted Regression (MGWR) serve as instrumental tools to capture intrinsic contextual effects through the estimates of the local intercepts and behavioral contextual effects through estimates of the local slope parameters. GWR and MGWR provide simple implementation yet powerful frameworks that could be extended to various disciplines that handle spatial data. This bibliography aims to serve as a comprehensive compilation of peer-reviewed papers that have utilized GWR or MGWR as a primary analytical method to conduct spatial analyses and acts as a useful guide to anyone searching the literature for previous examples of local statistical modeling in a wide variety of application fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16209v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. Stewart Fotheringham, Chen-Lun Kao, Hanchen Yu, Sarah Bardin, Taylor Oshan, Ziqi Li, Mehak Sachdeva, Wei Luo</dc:creator>
    </item>
    <item>
      <title>Fundamental properties of linear factor models</title>
      <link>https://arxiv.org/abs/2409.02521</link>
      <description>arXiv:2409.02521v3 Announce Type: replace-cross 
Abstract: We study conditional linear factor models in the context of asset pricing panels. Our analysis focuses on conditional means and covariances to characterize the cross-sectional and inter-temporal properties of returns and factors as well as their interrelationships. We also review the conditions outlined in Kozak and Nagel (2024) and show how the conditional mean-variance efficient portfolio of an unbalanced panel can be spanned by low-dimensional factor portfolios, even without assuming invertibility of the conditional covariance matrices. Our analysis provides a comprehensive foundation for the specification and estimation of conditional linear factor models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02521v3</guid>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Damir Filipovic, Paul Schneider</dc:creator>
    </item>
    <item>
      <title>Learning non-Gaussian spatial distributions via Bayesian transport maps with parametric shrinkage</title>
      <link>https://arxiv.org/abs/2409.19208</link>
      <description>arXiv:2409.19208v2 Announce Type: replace-cross 
Abstract: Many applications, including climate-model analysis and stochastic weather generators, require learning or emulating the distribution of a high-dimensional and non-Gaussian spatial field based on relatively few training samples. To address this challenge, a recently proposed Bayesian transport map (BTM) approach consists of a triangular transport map with nonparametric Gaussian-process (GP) components, which is trained to transform the distribution of interest distribution to a Gaussian reference distribution. To improve the performance of this existing BTM, we propose to shrink the map components toward a ``base'' parametric Gaussian family combined with a Vecchia approximation for scalability. The resulting ShrinkTM approach is more accurate than the existing BTM, especially for small numbers of training samples. It can even outperform the ``base'' family when trained on a single sample of the spatial field. We demonstrate the advantage of ShrinkTM though numerical experiments on simulated data and on climate-model output.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19208v2</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anirban Chakraborty, Matthias Katzfuss</dc:creator>
    </item>
    <item>
      <title>Conformal Prediction for Hierarchical Data</title>
      <link>https://arxiv.org/abs/2411.13479</link>
      <description>arXiv:2411.13479v2 Announce Type: replace-cross 
Abstract: We consider conformal prediction of multivariate data series, which consists of outputting prediction regions based on empirical quantiles of point-estimate errors. We actually consider hierarchical multivariate data series, for which some components are linear combinations of others. The intuition is that the hierarchical structure may be leveraged to improve the prediction regions in terms of their sizes for given coverage levels. We implement this intuition by including a projection step (also called reconciliation step) in the split conformal prediction [SCP] procedure and prove that the resulting prediction regions are indeed globally smaller than without the projection step. The associated strategies and their analyses rely on the literatures of both SCP and forecast reconciliation. We also illustrate the theoretical findings, both on artificial and on real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13479v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guillaume Principato, Gilles Stoltz, Yvenn Amara-Ouali, Yannig Goude, Bachir Hamrouche, Jean-Michel Poggi</dc:creator>
    </item>
    <item>
      <title>Logistic regression models: practical induced prior specification</title>
      <link>https://arxiv.org/abs/2501.18106</link>
      <description>arXiv:2501.18106v2 Announce Type: replace-cross 
Abstract: Bayesian inference for statistical models with a hierarchical structure is often characterized by specification of priors for parameters at different levels of the hierarchy. When higher level parameters are functions of the lower level parameters, specifying a prior on the lower level parameters leads to induced priors on the higher level parameters. However, what are deemed uninformative priors for lower level parameters can induce strikingly non-vague priors for higher level parameters. Depending on the sample size and specific model parameterization, these priors can then have unintended effects on the posterior distribution of the higher level parameters.
  Here we focus on Bayesian inference for the Bernoulli distribution parameter $\theta$ which is modeled as a function of covariates via a logistic regression, where the coefficients are the lower level parameters for which priors are specified. A specific area of interest and application is the modeling of survival probabilities in capture-recapture data and occupancy and detection probabilities in presence-absence data. In particular we propose alternative priors for the coefficients that yield specific induced priors for $\theta$. We address three induced prior cases. The simplest is when the induced prior for $\theta$ is Uniform(0,1). The second case is when the induced prior for $\theta$ is an arbitrary Beta($\alpha$, $\beta$) distribution. The third case is one where the intercept in the logistic model is to be treated distinct from the partial slope coefficients; e.g., $E[\theta]$ equals a specified value on (0,1) when all covariates equal 0. Simulation studies were carried out to evaluate performance of these priors and the methods were applied to a real presence/absence data set and occupancy modelling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18106v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ken B. Newman, Cristiano Villa, Ruth King</dc:creator>
    </item>
  </channel>
</rss>
