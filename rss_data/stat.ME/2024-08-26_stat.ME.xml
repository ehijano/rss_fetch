<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 27 Aug 2024 02:29:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Air-HOLP: Adaptive Regularized Feature Screening for High Dimensional Data</title>
      <link>https://arxiv.org/abs/2408.13000</link>
      <description>arXiv:2408.13000v1 Announce Type: new 
Abstract: Handling high-dimensional datasets presents substantial computational challenges, particularly when the number of features far exceeds the number of observations and when features are highly correlated. A modern approach to mitigate these issues is feature screening. In this work, the High-dimensional Ordinary Least-squares Projection (HOLP) feature screening method is advanced by employing adaptive ridge regularization. The impact of the ridge penalty on the Ridge-HOLP method is examined and Air-HOLP is proposed, a data-adaptive advance to Ridge-HOLP where the ridge-regularization parameter is selected iteratively and optimally for better feature screening performance. The proposed method addresses the challenges of penalty selection in high dimensions by offering a computationally efficient and stable alternative to traditional methods like bootstrapping and cross-validation. Air-HOLP is evaluated using simulated data and a prostate cancer genetic dataset. The empirical results demonstrate that Air-HOLP has improved performance over a large range of simulation settings. We provide R codes implementing the Air-HOLP feature screening method and integrating it into existing feature screening methods that utilize the HOLP formula.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13000v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ibrahim Joudah, Samuel Muller, Houying Zhu</dc:creator>
    </item>
    <item>
      <title>A Restricted Latent Class Model with Polytomous Ordinal Correlated Attributes and Respondent-Level Covariates</title>
      <link>https://arxiv.org/abs/2408.13143</link>
      <description>arXiv:2408.13143v1 Announce Type: new 
Abstract: We present an exploratory restricted latent class model where response data is for a single time point, polytomous, and differing across items, and where latent classes reflect a multi-attribute state where each attribute is ordinal. Our model extends previous work to allow for correlation of the attributes through a multivariate probit and to allow for respondent-specific covariates. We demonstrate that the model recovers parameters well in a variety of realistic scenarios, and apply the model to the analysis of a particular dataset designed to diagnose depression. The application demonstrates the utility of the model in identifying the latent structure of depression beyond single-factor approaches which have been used in the past.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13143v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Alan Wayman, Steven Andrew Culpepper, Jeff Douglas, Jesse Bowers</dc:creator>
    </item>
    <item>
      <title>A latent space model for multivariate count data time series analysis</title>
      <link>https://arxiv.org/abs/2408.13162</link>
      <description>arXiv:2408.13162v1 Announce Type: new 
Abstract: Motivated by a dataset of burglaries in Chicago, USA, we introduce a novel framework to analyze time series of count data combining common multivariate time series models with latent position network models. This novel methodology allows us to gain a new latent variable perspective on the crime dataset that we consider, allowing us to disentangle and explain the complex patterns exhibited by the data, while providing a natural time series framework that can be used to make future predictions. Our model is underpinned by two well known statistical approaches: a log-linear vector autoregressive model, which is prominent in the literature on multivariate count time series, and a latent projection model, which is a popular latent variable model for networks. The role of the projection model is to characterize the interaction parameters of the vector autoregressive model, thus uncovering the underlying network that is associated with the pairwise relationships between the time series. Estimation and inferential procedures are performed using an optimization algorithm and a Hamiltonian Monte Carlo procedure for efficient Bayesian inference. We also include a simulation study to illustrate the merits of our methodology in recovering consistent parameter estimates, and in making accurate future predictions for the time series. As we demonstrate in our application to the crime dataset, this new methodology can provide very meaningful model-based interpretations of the data, and it can be generalized to other time series contexts and applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13162v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hardeep Kaur, Riccardo Rastelli</dc:creator>
    </item>
    <item>
      <title>Estimation of ratios of normalizing constants using stochastic approximation : the SARIS algorithm</title>
      <link>https://arxiv.org/abs/2408.13022</link>
      <description>arXiv:2408.13022v1 Announce Type: cross 
Abstract: Computing ratios of normalizing constants plays an important role in statistical modeling. Two important examples are hypothesis testing in latent variables models, and model comparison in Bayesian statistics. In both examples, the likelihood ratio and the Bayes factor are defined as the ratio of the normalizing constants of posterior distributions. We propose in this article a novel methodology that estimates this ratio using stochastic approximation principle. Our estimator is consistent and asymptotically Gaussian. Its asymptotic variance is smaller than the one of the popular optimal bridge sampling estimator. Furthermore, it is much more robust to little overlap between the two unnormalized distributions considered. Thanks to its online definition, our procedure can be integrated in an estimation process in latent variables model, and therefore reduce the computational effort. The performances of the estimator are illustrated through a simulation study and compared to two other estimators : the ratio importance sampling and the optimal bridge sampling estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13022v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom Gu\'edon, Charlotte Baey, Estelle Kuhn</dc:creator>
    </item>
    <item>
      <title>Non-parametric estimators of scaled cash flows</title>
      <link>https://arxiv.org/abs/2408.13176</link>
      <description>arXiv:2408.13176v1 Announce Type: cross 
Abstract: In multi-state life insurance, incidental policyholder behavior gives rise to expected cash flows that are not easily targeted by classic non-parametric estimators if data is subject to sampling effects. We introduce a scaled version of the classic Aalen--Johansen estimator that overcomes this challenge. Strong uniform consistency and asymptotic normality are established under entirely random right-censoring, subject to lax moment conditions on the multivariate counting process. In a simulation study, the estimator outperforms earlier proposals from the literature. Finally, we showcase the potential of the presented method to other areas of actuarial science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13176v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>T. Bathke, C. Furrer</dc:creator>
    </item>
    <item>
      <title>Augmented Functional Random Forests: Classifier Construction and Unbiased Functional Principal Components Importance through Ad-Hoc Conditional Permutations</title>
      <link>https://arxiv.org/abs/2408.13179</link>
      <description>arXiv:2408.13179v1 Announce Type: cross 
Abstract: This paper introduces a novel supervised classification strategy that integrates functional data analysis (FDA) with tree-based methods, addressing the challenges of high-dimensional data and enhancing the classification performance of existing functional classifiers. Specifically, we propose augmented versions of functional classification trees and functional random forests, incorporating a new tool for assessing the importance of functional principal components. This tool provides an ad-hoc method for determining unbiased permutation feature importance in functional data, particularly when dealing with correlated features derived from successive derivatives. Our study demonstrates that these additional features can significantly enhance the predictive power of functional classifiers. Experimental evaluations on both real-world and simulated datasets showcase the effectiveness of the proposed methodology, yielding promising results compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13179v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabrizio Maturo, Annamaria Porreca</dc:creator>
    </item>
    <item>
      <title>Combining observational and experimental data for causal inference considering data privacy</title>
      <link>https://arxiv.org/abs/2308.02974</link>
      <description>arXiv:2308.02974v2 Announce Type: replace 
Abstract: Combining observational and experimental data for causal inference can improve treatment effect estimation. However, many observational data sets cannot be released due to data privacy considerations, so one researcher may not have access to both experimental and observational data. Nonetheless, a small amount of risk of disclosing sensitive information might be tolerable to organizations that house confidential data. In these cases, organizations can employ data privacy techniques, which decrease disclosure risk, potentially at the expense of data utility. In this paper, we explore disclosure limiting transformations of observational data, which can be combined with experimental data to estimate the sample and population average treatment effects. We consider leveraging observational data to improve generalizability of treatment effect estimates when a randomized experiment (RCT) is not representative of the population of interest, and to increase precision of treatment effect estimates. Through simulation studies, we illustrate the trade-off between privacy and utility when employing different disclosure limiting transformations. We find that leveraging transformed observational data in treatment effect estimation can still improve estimation over only using data from an RCT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.02974v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charlotte Z. Mann, Adam C. Sales, Johann A. Gagnon-Bartsch</dc:creator>
    </item>
    <item>
      <title>Hidden Markov Models for Multivariate Panel Data</title>
      <link>https://arxiv.org/abs/2404.04122</link>
      <description>arXiv:2404.04122v3 Announce Type: replace 
Abstract: While advances continue to be made in model-based clustering, challenges persist in modeling various data types such as panel data. Multivariate panel data present difficulties for clustering algorithms because they are often plagued by missing data and dropouts, presenting issues for estimation algorithms. This research presents a family of hidden Markov models that compensate for the issues that arise in panel data. A modified expectation-maximization algorithm capable of handling missing not at random data and dropout is presented and used to perform model estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04122v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mackenzie R. Neal, Alexa A. Sochaniwsky, Paul D. McNicholas</dc:creator>
    </item>
    <item>
      <title>Exploring the difficulty of estimating win probability: a simulation study</title>
      <link>https://arxiv.org/abs/2406.16171</link>
      <description>arXiv:2406.16171v3 Announce Type: replace 
Abstract: Estimating win probability is one of the classic modeling tasks of sports analytics. Many widely used win probability estimators use machine learning to fit the relationship between a binary win/loss outcome variable and certain game-state variables. To illustrate just how difficult it is to accurately fit such a model from noisy and highly correlated observational data, in this paper we conduct a simulation study. We create a simplified random walk version of football in which true win probability at each game-state is known, and we see how well a model recovers it. We find that the dependence structure of observational play-by-play data substantially inflates the bias and variance of estimators and lowers the effective sample size. This makes it essential to quantify uncertainty in win probability estimates, but typical bootstrapped confidence intervals are too narrow and don't achieve nominal coverage. Hence, we introduce a novel method, the fractional bootstrap, to calibrate these intervals to achieve adequate coverage. Our findings are not unique to the particular application of estimating win probability; they are broadly applicable across sports analytics, as myriad other sports datasets are clustered into groups of observations that share the same outcome.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16171v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan S. Brill, Ronald Yurko, Abraham J. Wyner</dc:creator>
    </item>
    <item>
      <title>Flexible max-stable processes for fast and efficient inference</title>
      <link>https://arxiv.org/abs/2407.13958</link>
      <description>arXiv:2407.13958v4 Announce Type: replace 
Abstract: Max-stable processes serve as the fundamental distributional family in extreme value theory. However, likelihood-based inference methods for max-stable processes still heavily rely on composite likelihoods, rendering them intractable in high dimensions due to their intractable densities. In this paper, we introduce a fast and efficient inference method for max-stable processes based on their angular densities for a class of max-stable processes whose angular densities do not put mass on the boundary space of the simplex. This class can also be used to construct r-Pareto processes. We demonstrate the efficiency of the proposed method through two new max-stable processes: the truncated extremal-t process and the skewed Brown-Resnick process. The skewed Brown-Resnick process contains the popular Brown-Resnick model as a special case and possesses nonstationary extremal dependence structures. The proposed method is shown to be computationally efficient and can be applied to large datasets. We showcase the new max-stable processes on simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13958v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peng Zhong, Scott A. Sisson, Boris Beranger</dc:creator>
    </item>
    <item>
      <title>Marginal Effects for Probit and Tobit with Endogeneity</title>
      <link>https://arxiv.org/abs/2306.14862</link>
      <description>arXiv:2306.14862v3 Announce Type: replace-cross 
Abstract: When evaluating partial effects, it is important to distinguish between structural endogeneity and measurement errors. In contrast to linear models, these two sources of endogeneity affect partial effects differently in nonlinear models. We study this issue focusing on the Instrumental Variable (IV) Probit and Tobit models. We show that even when a valid IV is available, failing to differentiate between the two types of endogeneity can lead to either under- or over-estimation of the partial effects. We develop simple estimators of the bounds on the partial effects and provide easy to implement confidence intervals that correctly account for both types of endogeneity. We illustrate the methods in a Monte Carlo simulation and an empirical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14862v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kirill S. Evdokimov, Ilze Kalnina, Andrei Zeleneev</dc:creator>
    </item>
    <item>
      <title>On the Identifying Power of Monotonicity for Average Treatment Effects</title>
      <link>https://arxiv.org/abs/2405.14104</link>
      <description>arXiv:2405.14104v2 Announce Type: replace-cross 
Abstract: In the context of a binary outcome, treatment, and instrument, Balke and Pearl (1993, 1997) establish that the monotonicity condition of Imbens and Angrist (1994) has no identifying power beyond instrument exogeneity for average potential outcomes and average treatment effects in the sense that adding it to instrument exogeneity does not decrease the identified sets for those parameters whenever those restrictions are consistent with the distribution of the observable data. This paper shows that this phenomenon holds in a broader setting with a multi-valued outcome, treatment, and instrument, under an extension of the monotonicity condition that we refer to as generalized monotonicity. We further show that this phenomenon holds for any restriction on treatment response that is stronger than generalized monotonicity provided that these stronger restrictions do not restrict potential outcomes. Importantly, many models of potential treatments previously considered in the literature imply generalized monotonicity, including the types of monotonicity restrictions considered by Kline and Walters (2016), Kirkeboen et al. (2016), and Heckman and Pinto (2018), and the restriction that treatment selection is determined by particular classes of additive random utility models. We show through a series of examples that restrictions on potential treatments can provide identifying power beyond instrument exogeneity for average potential outcomes and average treatment effects when the restrictions imply that the generalized monotonicity condition is violated. In this way, our results shed light on the types of restrictions required for help in identifying average potential outcomes and average treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14104v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuehao Bai, Shunzhuang Huang, Sarah Moon, Azeem M. Shaikh, Edward J. Vytlacil</dc:creator>
    </item>
    <item>
      <title>End-To-End Causal Effect Estimation from Unstructured Natural Language Data</title>
      <link>https://arxiv.org/abs/2407.07018</link>
      <description>arXiv:2407.07018v2 Announce Type: replace-cross 
Abstract: Knowing the effect of an intervention is critical for human decision-making, but current approaches for causal effect estimation rely on manual data collection and structuring, regardless of the causal assumptions. This increases both the cost and time-to-completion for studies. We show how large, diverse observational text data can be mined with large language models (LLMs) to produce inexpensive causal effect estimates under appropriate causal assumptions. We introduce NATURAL, a novel family of causal effect estimators built with LLMs that operate over datasets of unstructured text. Our estimators use LLM conditional distributions (over variables of interest, given the text data) to assist in the computation of classical estimators of causal effect. We overcome a number of technical challenges to realize this idea, such as automating data curation and using LLMs to impute missing information. We prepare six (two synthetic and four real) observational datasets, paired with corresponding ground truth in the form of randomized trials, which we used to systematically evaluate each step of our pipeline. NATURAL estimators demonstrate remarkable performance, yielding causal effect estimates that fall within 3 percentage points of their ground truth counterparts, including on real-world Phase 3/4 clinical trials. Our results suggest that unstructured text data is a rich source of causal effect information, and NATURAL is a first step towards an automated pipeline to tap this resource.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07018v2</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikita Dhawan, Leonardo Cotta, Karen Ullrich, Rahul G. Krishnan, Chris J. Maddison</dc:creator>
    </item>
    <item>
      <title>On the Robustness of Kernel Goodness-of-Fit Tests</title>
      <link>https://arxiv.org/abs/2408.05854</link>
      <description>arXiv:2408.05854v2 Announce Type: replace-cross 
Abstract: Goodness-of-fit testing is often criticized for its lack of practical relevance; since ``all models are wrong'', the null hypothesis that the data conform to our model is ultimately always rejected when the sample size is large enough. Despite this, probabilistic models are still used extensively, raising the more pertinent question of whether the model is good enough for a specific task. This question can be formalized as a robust goodness-of-fit testing problem by asking whether the data were generated by a distribution corresponding to our model up to some mild perturbation. In this paper, we show that existing kernel goodness-of-fit tests are not robust according to common notions of robustness including qualitative and quantitative robustness. We also show that robust techniques based on tilted kernels from the parameter estimation literature are not sufficient for ensuring both types of robustness in the context of goodness-of-fit testing. We therefore propose the first robust kernel goodness-of-fit test which resolves this open problem using kernel Stein discrepancy balls, which encompass perturbation models such as Huber contamination models and density uncertainty bands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05854v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xing Liu, Fran\c{c}ois-Xavier Briol</dc:creator>
    </item>
  </channel>
</rss>
