<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Mar 2024 04:00:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 26 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Improved Methods of Task Assignment and Resource Allocation with Preemption in Edge Computing Systems</title>
      <link>https://arxiv.org/abs/2403.15665</link>
      <description>arXiv:2403.15665v1 Announce Type: new 
Abstract: Edge computing has become a very popular service that enables mobile devices to run complex tasks with the help of network-based computing resources. However, edge clouds are often resource-constrained, which makes resource allocation a challenging issue. In addition, edge cloud servers must make allocation decisions with only limited information available, since the arrival of future client tasks might be impossible to predict, and the states and behavior of neighboring servers might be obscured. We focus on a distributed resource allocation method in which servers operate independently and do not communicate with each other, but interact with clients (tasks) to make allocation decisions. We follow a two-round bidding approach to assign tasks to edge cloud servers, and servers are allowed to preempt previous tasks to allocate more useful ones. We evaluate the performance of our system using realistic simulations and real-world trace data from a high-performance computing cluster. Results show that our heuristic improves system-wide performance by $20-25\%$ over previous work when accounting for the time taken by each approach. In this way, an ideal trade-off between performance and speed is achieved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15665v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caroline Rublein, Fidan Mehmeti, Mark Mahon, Thomas F. La Porta</dc:creator>
    </item>
    <item>
      <title>Navigating the Landscape of Distributed File Systems: Architectures, Implementations, and Considerations</title>
      <link>https://arxiv.org/abs/2403.15701</link>
      <description>arXiv:2403.15701v1 Announce Type: new 
Abstract: Distributed File Systems (DFS) have emerged as sophisticated solutions for efficient file storage and management across interconnected computer nodes. The main objective of DFS is to achieve flexible, scalable, and resilient file storage management by dispersing file data across multiple interconnected computer nodes, enabling users to seamlessly access and manipulate files distributed across diverse nodes. This article provides an overview of DFS, its architecture, classification methods, design considerations, challenges, and common implementations. Common DFS implementations discussed include NFS, AFS, GFS, HDFS, and CephFS, each tailored to specific use cases and design goals. Understanding the nuances of DFS architecture, classification, and design considerations is crucial for developing efficient, stable, and secure distributed file systems to meet diverse user and application needs in modern computing environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15701v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xueting Pan, Ziqian Luo, Lisang Zhou</dc:creator>
    </item>
    <item>
      <title>Radical-Cylon: A Heterogeneous Data Pipeline for Scientific Computing</title>
      <link>https://arxiv.org/abs/2403.15721</link>
      <description>arXiv:2403.15721v1 Announce Type: new 
Abstract: Managing and preparing complex data for deep learning, a prevalent approach in large-scale data science can be challenging. Data transfer for model training also presents difficulties, impacting scientific fields like genomics, climate modeling, and astronomy. A large-scale solution like Google Pathways with a distributed execution environment for deep learning models exists but is proprietary. Integrating existing open-source, scalable runtime tools and data frameworks on high-performance computing (HPC) platforms are crucial to address these challenges. Our objective is to establish a smooth and unified method of combining data engineering and deep learning frameworks with diverse execution capabilities that can be deployed on various high-performance computing platforms, including cloud and supercomputers. We aim to support heterogeneous systems with accelerators, where Cylon and other data engineering and deep learning frameworks can utilize heterogeneous execution. To achieve this, we propose Radical-Cylon, a heterogeneous runtime system with a parallel and distributed data framework to execute Cylon as a task of Radical Pilot. We thoroughly explain Radical-Cylon's design and development and the execution process of Cylon tasks using Radical Pilot. This approach enables the use of heterogeneous MPI-communicators across multiple nodes. Radical-Cylon achieves better performance than Bare-Metal Cylon with minimal and constant overhead. Radical-Cylon achieves (4~15)% faster execution time than batch execution while performing similar join and sort operations with 35 million and 3.5 billion rows with the same resources. The approach aims to excel in both scientific and engineering research HPC systems while demonstrating robust performance on cloud infrastructures. This dual capability fosters collaboration and innovation within the open-source scientific research community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15721v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arup Kumar Sarker, Aymen Alsaadi, Niranda Perera, Mills Staylor, Gregor von Laszewski, Matteo Turilli, Ozgur Ozan Kilic, Mikhail Titov, Andre Merzky, Shantenu Jha, Geoffrey Fox</dc:creator>
    </item>
    <item>
      <title>Resource-efficient Parallel Split Learning in Heterogeneous Edge Computing</title>
      <link>https://arxiv.org/abs/2403.15815</link>
      <description>arXiv:2403.15815v1 Announce Type: new 
Abstract: Edge AI has been recently proposed to facilitate the training and deployment of Deep Neural Network (DNN) models in proximity to the sources of data. To enable the training of large models on resource-constraint edge devices and protect data privacy, parallel split learning is becoming a practical and popular approach. However, current parallel split learning neglects the resource heterogeneity of edge devices, which may lead to the straggler issue. In this paper, we propose EdgeSplit, a novel parallel split learning framework to better accelerate distributed model training on heterogeneous and resource-constraint edge devices. EdgeSplit enhances the efficiency of model training on less powerful edge devices by adaptively segmenting the model into varying depths. Our approach focuses on reducing total training time by formulating and solving a task scheduling problem, which determines the most efficient model partition points and bandwidth allocation for each device. We employ a straightforward yet effective alternating algorithm for this purpose. Comprehensive tests conducted with a range of DNN models and datasets demonstrate that EdgeSplit not only facilitates the training of large models on resource-restricted edge devices but also surpasses existing baselines in performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15815v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingjin Zhang, Jiannong Cao, Yuvraj Sahni, Xiangchun Chen, Shan Jiang</dc:creator>
    </item>
    <item>
      <title>A Codesign of Scheduling and Parallelization for Large Model Training in Heterogeneous Clusters</title>
      <link>https://arxiv.org/abs/2403.16125</link>
      <description>arXiv:2403.16125v1 Announce Type: new 
Abstract: Joint consideration of scheduling and adaptive parallelism offers great opportunities for improving the training efficiency of large models on heterogeneous GPU clusters. However, integrating adaptive parallelism into a cluster scheduler expands the cluster scheduling space. The new space is the product of the original scheduling space and the parallelism exploration space of adaptive parallelism (also a product of pipeline, data, and tensor parallelism). The exponentially enlarged scheduling space and ever-changing optimal parallelism plan from adaptive parallelism together result in the contradiction between low-overhead and accurate performance data acquisition for efficient cluster scheduling. This paper presents Crius, a training system for efficiently scheduling multiple large models with adaptive parallelism in a heterogeneous cluster. Crius proposes a novel scheduling granularity called Cell. It represents a job with deterministic resources and pipeline stages. The exploration space of Cell is shrunk to the product of only data and tensor parallelism, thus exposing the potential for accurate and low-overhead performance estimation. Crius then accurately estimates Cells and efficiently schedules training jobs. When a Cell is selected as a scheduling choice, its represented job runs with the optimal parallelism plan explored. Experimental results show that Crius reduces job completion time by up to 48.9% and schedules large models with up to 1.49x cluster throughput improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16125v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunyu Xue, Weihao Cui, Han Zhao, Quan Chen, Shulai Zhang, Pengyu Yang, Jing Yang, Shaobo Li, Minyi Guo</dc:creator>
    </item>
    <item>
      <title>MRSch: Multi-Resource Scheduling for HPC</title>
      <link>https://arxiv.org/abs/2403.16298</link>
      <description>arXiv:2403.16298v1 Announce Type: new 
Abstract: Emerging workloads in high-performance computing (HPC) are embracing significant changes, such as having diverse resource requirements instead of being CPU-centric. This advancement forces cluster schedulers to consider multiple schedulable resources during decision-making. Existing scheduling studies rely on heuristic or optimization methods, which are limited by an inability to adapt to new scenarios for ensuring long-term scheduling performance. We present an intelligent scheduling agent named MRSch for multi-resource scheduling in HPC that leverages direct future prediction (DFP), an advanced multi-objective reinforcement learning algorithm. While DFP demonstrated outstanding performance in a gaming competition, it has not been previously explored in the context of HPC scheduling. Several key techniques are developed in this study to tackle the challenges involved in multi-resource scheduling. These techniques enable MRSch to learn an appropriate scheduling policy automatically and dynamically adapt its policy in response to workload changes via dynamic resource prioritizing. We compare MRSch with existing scheduling methods through extensive tracebase simulations. Our results demonstrate that MRSch improves scheduling performance by up to 48% compared to the existing scheduling methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16298v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/CLUSTER51413.2022.00020</arxiv:DOI>
      <dc:creator>Boyang Li, Yuping Fan, Matthew Dearing, Zhiling Lan, Paul Richy, William Allcocky, Michael Papka</dc:creator>
    </item>
    <item>
      <title>Raptor: Distributed Scheduling for Serverless Functions</title>
      <link>https://arxiv.org/abs/2403.16457</link>
      <description>arXiv:2403.16457v1 Announce Type: new 
Abstract: Serverless platforms that poorly schedule function requests inspire developers to implement workarounds to issues like high cold start latencies, poor fault tolerance, and limited support for parallel processing. These solutions litter environments with idle containers and add unnecessary pressure to the already underperforming scheduling services. An effective serverless scheduling policy should encourage developers to write small and reusable snippets of code, and give operators the freedom to administer cluster workloads however necessary in order to meet their operational demands. To this end, we have designed a distributed scheduling service that integrates with existing serverless frameworks. Our service addresses three key issues that affect modern serverless platforms; high cold start latencies, poor fault tolerance, and limited native support for parallel processing patterns like fork-join and map-reduce. We have built a prototype that integrates with the existing OpenWhisk services, and is fully backwards compatible with the existing implementation. The updated architecture improves performance and adds new scheduling and security features. Our empirical results demonstrate that our scheduler reduces cold start execution latencies by up to 80, steady state latencies by up to 10, and does so with negligible time and memory overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16457v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin Exton, Maria Read</dc:creator>
    </item>
    <item>
      <title>ColonyOS -- A Meta-Operating System for Distributed Computing Across Heterogeneous Platform</title>
      <link>https://arxiv.org/abs/2403.16486</link>
      <description>arXiv:2403.16486v1 Announce Type: new 
Abstract: This paper presents ColonyOS, an open-source meta-operating system designed to improve integration and utilization of diverse computing platforms, including IoT, edge, cloud, and HPC. Operating as an overlay, ColonyOS can interface with a wide range of computing environments, fostering creation of so-called compute continuums. This makes it possible to develop AI workflows and applications that can operate across platforms. At its core, ColonyOS consists of distributed executors that integrate with various underlying platforms based on a distributed microservice architecture. These executors collectively form a colony, serving as a unified computing unit. To enable secure integration of various platforms, each colony is provisioned with precisely the resources needed, and all communication is confined within the colony governed by a strict zero-trust security protocol. Interaction with ColonyOS is done by submitting functional meta-descriptions of computational tasks, called function specifications. These are sent to a Colonies server, which acts as intermediary between applications and the executors. Upon assignment, an executor interprets the meta-description and translates it into an executable format, e.g. a Kubernetes deployment description, a Slurm script, or a direct function call within the executor. Furthermore, a built-in meta-file system enables data synchronization directives to be included in meta-descriptions, enabling seamless data management across platforms. Ultimately, ColonyOS paves the way for development of hyper-distributed applications and workflows, which can seamlessly operate in a computing continuum. The paper describes design principles and implementation details of ColonyOS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16486v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johan Kristiansson</dc:creator>
    </item>
    <item>
      <title>Lessons Learned from Building Edge Software System Testbeds</title>
      <link>https://arxiv.org/abs/2403.16869</link>
      <description>arXiv:2403.16869v1 Announce Type: new 
Abstract: Edge computing requires the complex software interaction of geo-distributed, heterogeneous components. The growing research and industry interest in edge computing software systems has necessitated exploring ways of testing and evaluating edge software at scale without relying on physical infrastructure. Beyond simulation, virtual testbeds that emulate edge infrastructure can provide a cost-efficient yet realistic environment to evaluate edge software.
  In this experience paper, we share lessons learned from building a total of five edge software testbeds. We describe pitfalls in architecture and development as well as experiences from having students use our testbed tooling in distributed systems prototyping classes. While we remain confident that building custom testbed tooling is the right approach for edge computing researchers and practitioners alike, we hope this paper allows others to avoid common mistakes and benefit from our experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16869v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tobias Pfandzelter, David Bermbach</dc:creator>
    </item>
    <item>
      <title>Fast real-time arbitrary waveform generation using graphic processing units</title>
      <link>https://arxiv.org/abs/2403.15582</link>
      <description>arXiv:2403.15582v1 Announce Type: cross 
Abstract: Real-time Arbitrary Waveform Generation (AWG) is essential in various engineering and research applications, and often requires complex bespoke hardware and software. This paper introduces an AWG framework using an NVIDIA Graphics Processing Unit (GPU) and a commercially available high-speed Digital-to-Analog Converter (DAC) card, both running on a desktop personal computer (PC). The GPU accelerates the "embarrassingly" data parallel additive waveform synthesis framework for AWG, and the DAC reconstructs the generated waveform in the analog domain at high speed. The AWG framework is programmed using the developer-friendly Compute Unified Device Architecture (CUDA) runtime application programming interface from NVIDIA and is readily customizable, and scalable with additional parallel hardware. We present and characterize two different pathways for computing modulated radio-frequency (rf) waveforms: one pathway offers high-complexity simultaneous chirping of 1000 individual Nyquist-limited single-frequency tones for 35 ms at a sampling rate of 560 MB/s, and the other pathway allows simultaneous continuous chirping of 194 individual Nyquist-limited single-frequency tones at 100 MB/s, or 20 individual tones at 560 MB/s. This AWG framework is designed for fast on-the-fly rearrangement of a large stochastically-loaded optical tweezer array of single atoms or molecules into a defect-free array needed for quantum simulation and quantum computation applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15582v1</guid>
      <category>cond-mat.quant-gas</category>
      <category>cs.DC</category>
      <category>eess.SP</category>
      <category>physics.atom-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juntian Tu, Sarthak Subhankar</dc:creator>
    </item>
    <item>
      <title>An Upload-Efficient Scheme for Transferring Knowledge From a Server-Side Pre-trained Generator to Clients in Heterogeneous Federated Learning</title>
      <link>https://arxiv.org/abs/2403.15760</link>
      <description>arXiv:2403.15760v1 Announce Type: cross 
Abstract: Heterogeneous Federated Learning (HtFL) enables collaborative learning on multiple clients with different model architectures while preserving privacy. Despite recent research progress, knowledge sharing in HtFL is still difficult due to data and model heterogeneity. To tackle this issue, we leverage the knowledge stored in pre-trained generators and propose a new upload-efficient knowledge transfer scheme called Federated Knowledge-Transfer Loop (FedKTL). Our FedKTL can produce client-task-related prototypical image-vector pairs via the generator's inference on the server. With these pairs, each client can transfer pre-existing knowledge from the generator to its local model through an additional supervised local task. We conduct extensive experiments on four datasets under two types of data heterogeneity with 14 kinds of models including CNNs and ViTs. Results show that our upload-efficient FedKTL surpasses seven state-of-the-art methods by up to 7.31% in accuracy. Moreover, our knowledge transfer scheme is applicable in scenarios with only one edge client. Code: https://github.com/TsingZ0/FedKTL</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15760v1</guid>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jianqing Zhang, Yang Liu, Yang Hua, Jian Cao</dc:creator>
    </item>
    <item>
      <title>Initialisation and Topology Effects in Decentralised Federated Learning</title>
      <link>https://arxiv.org/abs/2403.15855</link>
      <description>arXiv:2403.15855v1 Announce Type: cross 
Abstract: Fully decentralised federated learning enables collaborative training of individual machine learning models on distributed devices on a network while keeping the training data localised. This approach enhances data privacy and eliminates both the single point of failure and the necessity for central coordination. Our research highlights that the effectiveness of decentralised federated learning is significantly influenced by the network topology of connected devices. A simplified numerical model for studying the early behaviour of these systems leads us to an improved artificial neural network initialisation strategy, which leverages the distribution of eigenvector centralities of the nodes of the underlying network, leading to a radically improved training efficiency. Additionally, our study explores the scaling behaviour and choice of environmental parameters under our proposed initialisation strategy. This work paves the way for more efficient and scalable artificial neural network training in a distributed and uncoordinated environment, offering a deeper understanding of the intertwining roles of network structure and learning dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15855v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arash Badie-Modiri, Chiara Boldrini, Lorenzo Valerio, J\'anos Kert\'esz, M\'arton Karsai</dc:creator>
    </item>
    <item>
      <title>LOAM: Low-latency Communication, Caching, and Computation Placement in Data-Intensive Computing Networks</title>
      <link>https://arxiv.org/abs/2403.15927</link>
      <description>arXiv:2403.15927v1 Announce Type: cross 
Abstract: Deploying data- and computation-intensive applications such as large-scale AI into heterogeneous dispersed computing networks can significantly enhance application performance by mitigating bottlenecks caused by limited network resources, including bandwidth, storage, and computing power. However, current resource allocation methods in dispersed computing do not provide a comprehensive solution that considers arbitrary topology, elastic resource amount, reuse of computation results, and nonlinear congestion-dependent optimization objectives. In this paper, we propose LOAM, a low-latency joint communication, caching, and computation placement framework with a rigorous analytical foundation that incorporates the above aspects. We tackle the NP-hard aggregated cost minimization problem with two methods: an offline method with a 1/2 approximation and an online adaptive method with a bounded gap from the optimum. Through extensive simulation, the proposed framework outperforms multiple baselines in both synthesis and real-world network scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15927v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jinkun Zhang, Edmund Yeh</dc:creator>
    </item>
    <item>
      <title>Study of Workload Interference with Intelligent Routing on Dragonfly</title>
      <link>https://arxiv.org/abs/2403.16288</link>
      <description>arXiv:2403.16288v1 Announce Type: cross 
Abstract: Dragonfly interconnect is a crucial network technology for supercomputers. To support exascale systems, network resources are shared such that links and routers are not dedicated to any node pair. While link utilization is increased, workload performance is often offset by network contention. Recently, intelligent routing built on reinforcement learning demonstrates higher network throughput with lower packet latency. However, its effectiveness in reducing workload interference is unknown. In this work, we present extensive network simulations to study multi-workload contention under different routing mechanisms, intelligent routing and adaptive routing, on a large-scale Dragonfly system. We develop an enhanced network simulation toolkit, along with a suite of workloads with distinctive communication patterns. We also present two metrics to characterize application communication intensity. Our analysis focuses on examining how different workloads interfere with each other under different routing mechanisms by inspecting both application-level and network-level metrics. Several key insights are made from the analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16288v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/SC41404.2022.00025</arxiv:DOI>
      <dc:creator>Yao Kang, Xin Wang, Zhiling Lan</dc:creator>
    </item>
    <item>
      <title>Interpretable Modeling of Deep Reinforcement Learning Driven Scheduling</title>
      <link>https://arxiv.org/abs/2403.16293</link>
      <description>arXiv:2403.16293v1 Announce Type: cross 
Abstract: In the field of high-performance computing (HPC), there has been recent exploration into the use of deep reinforcement learning for cluster scheduling (DRL scheduling), which has demonstrated promising outcomes. However, a significant challenge arises from the lack of interpretability in deep neural networks (DNN), rendering them as black-box models to system managers. This lack of model interpretability hinders the practical deployment of DRL scheduling. In this work, we present a framework called IRL (Interpretable Reinforcement Learning) to address the issue of interpretability of DRL scheduling. The core idea is to interpret DNN (i.e., the DRL policy) as a decision tree by utilizing imitation learning. Unlike DNN, decision tree models are non-parametric and easily comprehensible to humans. To extract an effective and efficient decision tree, IRL incorporates the Dataset Aggregation (DAgger) algorithm and introduces the notion of critical state to prune the derived decision tree. Through trace-based experiments, we demonstrate that IRL is capable of converting a black-box DNN policy into an interpretable rulebased decision tree while maintaining comparable scheduling performance. Additionally, IRL can contribute to the setting of rewards in DRL scheduling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16293v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/MASCOTS59514.2023.10387651</arxiv:DOI>
      <dc:creator>Boyang Li, Zhiling Lan, Michael E. Papka</dc:creator>
    </item>
    <item>
      <title>Q-adaptive: A Multi-Agent Reinforcement Learning Based Routing on Dragonfly Network</title>
      <link>https://arxiv.org/abs/2403.16301</link>
      <description>arXiv:2403.16301v1 Announce Type: cross 
Abstract: on adaptive routing to balance network traffic for optimum performance. Ideally, adaptive routing attempts to forward packets between minimal and non-minimal paths with the least congestion. In practice, current adaptive routing algorithms estimate routing path congestion based on local information such as output queue occupancy. Using local information to estimate global path congestion is inevitably inaccurate because a router has no precise knowledge of link states a few hops away. This inaccuracy could lead to interconnect congestion. In this study, we present Q-adaptive routing, a multi-agent reinforcement learning routing scheme for Dragonfly systems. Q-adaptive routing enables routers to learn to route autonomously by leveraging advanced reinforcement learning technology. The proposed Q-adaptive routing is highly scalable thanks to its fully distributed nature without using any shared information between routers. Furthermore, a new two-level Q-table is designed for Q-adaptive to make it computational lightly and saves 50% of router memory usage compared with the previous Q-routing. We implement the proposed Q-adaptive routing in SST/Merlin simulator. Our evaluation results show that Q-adaptive routing achieves up to 10.5% system throughput improvement and 5.2x average packet latency reduction compared with adaptive routing algorithms. Remarkably, Q-adaptive can even outperform the optimal VALn non-minimal routing under the ADV+1 adversarial traffic pattern with up to 3% system throughput improvement and 75% average packet latency reduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16301v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3431379.3460650</arxiv:DOI>
      <dc:creator>Yao Kang, Xin Wang, Zhiling Lan</dc:creator>
    </item>
    <item>
      <title>SignSGD with Federated Voting</title>
      <link>https://arxiv.org/abs/2403.16372</link>
      <description>arXiv:2403.16372v1 Announce Type: cross 
Abstract: Distributed learning is commonly used for accelerating model training by harnessing the computational capabilities of multiple-edge devices. However, in practical applications, the communication delay emerges as a bottleneck due to the substantial information exchange required between workers and a central parameter server. SignSGD with majority voting (signSGD-MV) is an effective distributed learning algorithm that can significantly reduce communication costs by one-bit quantization. However, due to heterogeneous computational capabilities, it fails to converge when the mini-batch sizes differ among workers. To overcome this, we propose a novel signSGD optimizer with \textit{federated voting} (signSGD-FV). The idea of federated voting is to exploit learnable weights to perform weighted majority voting. The server learns the weights assigned to the edge devices in an online fashion based on their computational capabilities. Subsequently, these weights are employed to decode the signs of the aggregated local gradients in such a way to minimize the sign decoding error probability. We provide a unified convergence rate analysis framework applicable to scenarios where the estimated weights are known to the parameter server either perfectly or imperfectly. We demonstrate that the proposed signSGD-FV algorithm has a theoretical convergence guarantee even when edge devices use heterogeneous mini-batch sizes. Experimental results show that signSGD-FV outperforms signSGD-MV, exhibiting a faster convergence rate, especially in heterogeneous mini-batch sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16372v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>eess.SP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chanho Park, H. Vincent Poor, Namyoon Lee</dc:creator>
    </item>
    <item>
      <title>FedAC: A Adaptive Clustered Federated Learning Framework for Heterogeneous Data</title>
      <link>https://arxiv.org/abs/2403.16460</link>
      <description>arXiv:2403.16460v1 Announce Type: cross 
Abstract: Clustered federated learning (CFL) is proposed to mitigate the performance deterioration stemming from data heterogeneity in federated learning (FL) by grouping similar clients for cluster-wise model training. However, current CFL methods struggle due to inadequate integration of global and intra-cluster knowledge and the absence of an efficient online model similarity metric, while treating the cluster count as a fixed hyperparameter limits flexibility and robustness. In this paper, we propose an adaptive CFL framework, named FedAC, which (1) efficiently integrates global knowledge into intra-cluster learning by decoupling neural networks and utilizing distinct aggregation methods for each submodule, significantly enhancing performance; (2) includes a costeffective online model similarity metric based on dimensionality reduction; (3) incorporates a cluster number fine-tuning module for improved adaptability and scalability in complex, heterogeneous environments. Extensive experiments show that FedAC achieves superior empirical performance, increasing the test accuracy by around 1.82% and 12.67% on CIFAR-10 and CIFAR-100 datasets, respectively, under different non-IID settings compared to SOTA methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16460v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxin Zhang, Haoyu Chen, Zheng Lin, Zhe Chen, Jin Zhao</dc:creator>
    </item>
    <item>
      <title>Differentially Private Online Federated Learning with Correlated Noise</title>
      <link>https://arxiv.org/abs/2403.16542</link>
      <description>arXiv:2403.16542v1 Announce Type: cross 
Abstract: We propose a novel differentially private algorithm for online federated learning that employs temporally correlated noise to improve the utility while ensuring the privacy of the continuously released models. To address challenges stemming from DP noise and local updates with streaming noniid data, we develop a perturbed iterate analysis to control the impact of the DP noise on the utility. Moreover, we demonstrate how the drift errors from local updates can be effectively managed under a quasi-strong convexity condition. Subject to an $(\epsilon, \delta)$-DP budget, we establish a dynamic regret bound over the entire time horizon that quantifies the impact of key parameters and the intensity of changes in dynamic environments. Numerical experiments validate the efficacy of the proposed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16542v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaojiao Zhang, Linglingzhi Zhu, Mikael Johansson</dc:creator>
    </item>
    <item>
      <title>Accelerating Federated Learning by Selecting Beneficial Herd of Local Gradients</title>
      <link>https://arxiv.org/abs/2403.16557</link>
      <description>arXiv:2403.16557v1 Announce Type: cross 
Abstract: Federated Learning (FL) is a distributed machine learning framework in communication network systems. However, the systems' Non-Independent and Identically Distributed (Non-IID) data negatively affect the convergence efficiency of the global model, since only a subset of these data samples are beneficial for model convergence. In pursuit of this subset, a reliable approach involves determining a measure of validity to rank the samples within the dataset. In this paper, We propose the BHerd strategy which selects a beneficial herd of local gradients to accelerate the convergence of the FL model. Specifically, we map the distribution of the local dataset to the local gradients and use the Herding strategy to obtain a permutation of the set of gradients, where the more advanced gradients in the permutation are closer to the average of the set of gradients. These top portion of the gradients will be selected and sent to the server for global aggregation. We conduct experiments on different datasets, models and scenarios by building a prototype system, and experimental results demonstrate that our BHerd strategy is effective in selecting beneficial local gradients to mitigate the effects brought by the Non-IID dataset, thus accelerating model convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16557v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ping Luo, Xiaoge Deng, Ziqing Wen, Tao Sun, Dongsheng Li</dc:creator>
    </item>
    <item>
      <title>FOOL: Addressing the Downlink Bottleneck in Satellite Computing with Neural Feature Compression</title>
      <link>https://arxiv.org/abs/2403.16677</link>
      <description>arXiv:2403.16677v1 Announce Type: cross 
Abstract: Nanosatellite constellations equipped with sensors capturing large geographic regions provide unprecedented opportunities for Earth observation. As constellation sizes increase, network contention poses a downlink bottleneck. Orbital Edge Computing (OEC) leverages limited onboard compute resources to reduce transfer costs by processing the raw captures at the source. However, current solutions have limited practicability due to reliance on crude filtering methods or over-prioritizing particular downstream tasks.
  This work presents FOOL, an OEC-native and task-agnostic feature compression method that preserves prediction performance. FOOL partitions high-resolution satellite imagery to maximize throughput. Further, it embeds context and leverages inter-tile dependencies to lower transfer costs with negligible overhead. While FOOL is a feature compressor, it can recover images with competitive scores on perceptual quality measures at lower bitrates. We extensively evaluate transfer cost reduction by including the peculiarity of intermittently available network connections in low earth orbit. Lastly, we test the feasibility of our system for standardized nanosatellite form factors. We demonstrate that FOOL permits downlinking over 100x the data volume without relying on prior information on the downstream tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16677v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <category>eess.IV</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alireza Furutanpey, Qiyang Zhang, Philipp Raith, Tobias Pfandzelter, Shangguang Wang, Schahram Dustdar</dc:creator>
    </item>
    <item>
      <title>DISL: Fueling Research with A Large Dataset of Solidity Smart Contracts</title>
      <link>https://arxiv.org/abs/2403.16861</link>
      <description>arXiv:2403.16861v1 Announce Type: cross 
Abstract: The DISL dataset features a collection of $514,506$ unique Solidity files that have been deployed to Ethereum mainnet. It caters to the need for a large and diverse dataset of real-world smart contracts. DISL serves as a resource for developing machine learning systems and for benchmarking software engineering tools designed for smart contracts. By aggregating every verified smart contract from Etherscan up to January 15, 2024, DISL surpasses existing datasets in size and recency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16861v1</guid>
      <category>cs.SE</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriele Morello, Mojtaba Eshghie, Sofia Bobadilla, Martin Monperrus</dc:creator>
    </item>
    <item>
      <title>Towards Secure and Trusted-by-Design Smart Contracts</title>
      <link>https://arxiv.org/abs/2403.16903</link>
      <description>arXiv:2403.16903v1 Announce Type: cross 
Abstract: Distributed immutable ledgers, or blockchains, allow the secure digitization of evidential transactions without relying on a trusted third-party. Evidential transactions involve the exchange of any form of physical evidence, such as money, birth certificate, visas, tickets, etc. Most of the time, evidential transactions occur in the context of complex procedures, called evidential protocols, among physical agents. The blockchain provides the mechanisms to transfer evidence, while smart contracts - programs executing within the blockchain in a decentralized and replicated fashion - allow encoding evidential protocols on top of a blockchain.
  As a smart contract foregoes trusted third-parties and runs on several machines anonymously, it constitutes a highly critical program that has to be secure and trusted-by-design. While most of the current smart contract languages focus on easy programmability, they do not directly address the need of guaranteeing trust and accountability, which becomes a significant issue when evidential protocols are encoded as smart contracts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16903v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zaynah Dargaye, \"Onder G\"urcan, Florent Kirchner, Sara Tucci-Piergiovanni</dc:creator>
    </item>
    <item>
      <title>PICO: Pipeline Inference Framework for Versatile CNNs on Diverse Mobile Devices</title>
      <link>https://arxiv.org/abs/2206.08662</link>
      <description>arXiv:2206.08662v3 Announce Type: replace 
Abstract: Distributing the inference of convolutional neural network (CNN) to multiple mobile devices has been studied in recent years to achieve real-time inference without losing accuracy. However, how to map CNN to devices remains a challenge. On the one hand, scheduling the workload of state-of-the-art CNNs with multiple devices is NP-Hard because the structures of CNNs are directed acyclic graphs (DAG) rather than simple chains. On the other hand, distributing the inference workload suffers from expensive communication and unbalanced computation due to the wireless environment and heterogeneous devices. This paper presents PICO, a pipeline cooperation framework to accelerate the inference of versatile CNNs on diverse mobile devices. At its core, PICO features: (1) a generic graph partition algorithm that considers the characteristics of any given CNN and orchestrates it into a list of model pieces with suitable granularity, and (2) a many-to-many mapping algorithm that produces the best pipeline configuration for heterogeneous devices. In our experiment with 2 ~ 8 Raspberry-Pi devices, the throughput can be improved by 1.8 ~ 6.8x under different CPU frequencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.08662v3</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TMC.2023.3265111</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Mobile Computing, vol. 23, no. 4, pp. 2712-2730, April 2024</arxiv:journal_reference>
      <dc:creator>Xiang Yang, Zikang Xu, Qi Qi, Jingyu Wang, Haifeng Sun, Jianxin Liao, Song Guo</dc:creator>
    </item>
    <item>
      <title>Toward parallel intelligence: an interdisciplinary solution for complex systems</title>
      <link>https://arxiv.org/abs/2311.12838</link>
      <description>arXiv:2311.12838v2 Announce Type: replace 
Abstract: The growing complexity of real-world systems necessitates interdisciplinary solutions to confront myriad challenges in modeling, analysis, management, and control. To meet these demands, the parallel systems method rooted in Artificial systems, Computational experiments, and Parallel execution (ACP) approach has been developed. The method cultivates a cycle, termed parallel intelligence, which iteratively creates data, acquires knowledge, and refines the actual system. Over the past two decades, the parallel systems method has continuously woven advanced knowledge and technologies from various disciplines, offering versatile interdisciplinary solutions for complex systems across diverse fields. This review explores the origins and fundamental concepts of the parallel systems method, showcasing its accomplishments as a diverse array of parallel technologies and applications, while also prognosticating potential challenges. We posit that this method will considerably augment sustainable development while enhancing interdisciplinary communication and cooperation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.12838v2</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.xinn.2023.100521</arxiv:DOI>
      <dc:creator>Yong Zhao, Zhengqiu Zhu, Bin Chen, Sihang Qiu, Jincai Huang, Xin Lu, Weiyi Yang, Chuan Ai, Kuihua Huang, Cheng He, Yucheng Jin, Zhong Liu, Fei-Yue Wang</dc:creator>
    </item>
    <item>
      <title>How Does Stake Distribution Influence Consensus? Analyzing Blockchain Decentralization</title>
      <link>https://arxiv.org/abs/2312.13938</link>
      <description>arXiv:2312.13938v2 Announce Type: replace 
Abstract: In the PoS blockchain landscape, the challenge of achieving full decentralization is often hindered by a disproportionate concentration of staked tokens among a few validators. This study analyses this challenge by first formalizing decentralization metrics for weighted consensus mechanisms. An empirical analysis across ten permissionless blockchains uncovers significant weight concentration among validators, underscoring the need for an equitable approach. To counter this, we introduce the Square Root Stake Weight (SRSW) model, which effectively recalibrates staking weight distribution. Our examination of the SRSW model demonstrates notable improvements in the decentralization metrics: the Gini index improves by 37.16% on average, while Nakamoto coefficients for liveness and safety see mean enhancements of 101.04% and 80.09%, respectively. This research is a pivotal step toward a more fair and equitable distribution of staking weight, advancing the decentralization in blockchain consensus mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13938v2</guid>
      <category>cs.DC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shashank Motepalli, Hans-Arno Jacobsen</dc:creator>
    </item>
    <item>
      <title>ProvDeploy: Provenance-oriented Containerization of High Performance Computing Scientific Workflows</title>
      <link>https://arxiv.org/abs/2403.15324</link>
      <description>arXiv:2403.15324v2 Announce Type: replace 
Abstract: Many existing scientific workflows require High Performance Computing environments to produce results in a timely manner. These workflows have several software library components and use different environments, making the deployment and execution of the software stack not trivial. This complexity increases if the user needs to add provenance data capture services to the workflow. This manuscript introduces ProvDeploy to assist the user in configuring containers for scientific workflows with integrated provenance data capture. ProvDeploy was evaluated with a Scientific Machine Learning workflow, exploring containerization strategies focused on provenance in two distinct HPC environments</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15324v2</guid>
      <category>cs.DC</category>
      <category>cs.DB</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liliane Kunstmann, D\'ebora Pina, Daniel de Oliveira, Marta Mattoso</dc:creator>
    </item>
    <item>
      <title>FrankenSplit: Efficient Neural Feature Compression with Shallow Variational Bottleneck Injection for Mobile Edge Computing</title>
      <link>https://arxiv.org/abs/2302.10681</link>
      <description>arXiv:2302.10681v4 Announce Type: replace-cross 
Abstract: The rise of mobile AI accelerators allows latency-sensitive applications to execute lightweight Deep Neural Networks (DNNs) on the client side. However, critical applications require powerful models that edge devices cannot host and must therefore offload requests, where the high-dimensional data will compete for limited bandwidth. This work proposes shifting away from focusing on executing shallow layers of partitioned DNNs. Instead, it advocates concentrating the local resources on variational compression optimized for machine interpretability. We introduce a novel framework for resource-conscious compression models and extensively evaluate our method in an environment reflecting the asymmetric resource distribution between edge devices and servers. Our method achieves 60% lower bitrate than a state-of-the-art SC method without decreasing accuracy and is up to 16x faster than offloading with existing codec standards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.10681v4</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alireza Furutanpey, Philipp Raith, Schahram Dustdar</dc:creator>
    </item>
    <item>
      <title>Gradient-less Federated Gradient Boosting Trees with Learnable Learning Rates</title>
      <link>https://arxiv.org/abs/2304.07537</link>
      <description>arXiv:2304.07537v3 Announce Type: replace-cross 
Abstract: The privacy-sensitive nature of decentralized datasets and the robustness of eXtreme Gradient Boosting (XGBoost) on tabular data raise the needs to train XGBoost in the context of federated learning (FL). Existing works on federated XGBoost in the horizontal setting rely on the sharing of gradients, which induce per-node level communication frequency and serious privacy concerns. To alleviate these problems, we develop an innovative framework for horizontal federated XGBoost which does not depend on the sharing of gradients and simultaneously boosts privacy and communication efficiency by making the learning rates of the aggregated tree ensembles learnable. We conduct extensive evaluations on various classification and regression datasets, showing our approach achieves performance comparable to the state-of-the-art method and effectively improves communication efficiency by lowering both communication rounds and communication overhead by factors ranging from 25x to 700x. Project Page: https://flower.ai/blog/2023-04-19-xgboost-with-flower/</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.07537v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3578356.3592579</arxiv:DOI>
      <dc:creator>Chenyang Ma, Xinchi Qiu, Daniel J. Beutel, Nicholas D. Lane</dc:creator>
    </item>
    <item>
      <title>A Survey for Federated Learning Evaluations: Goals and Measures</title>
      <link>https://arxiv.org/abs/2308.11841</link>
      <description>arXiv:2308.11841v2 Announce Type: replace-cross 
Abstract: Evaluation is a systematic approach to assessing how well a system achieves its intended purpose. Federated learning (FL) is a novel paradigm for privacy-preserving machine learning that allows multiple parties to collaboratively train models without sharing sensitive data. However, evaluating FL is challenging due to its interdisciplinary nature and diverse goals, such as utility, efficiency, and security. In this survey, we first review the major evaluation goals adopted in the existing studies and then explore the evaluation metrics used for each goal. We also introduce FedEval, an open-source platform that provides a standardized and comprehensive evaluation framework for FL algorithms in terms of their utility, efficiency, and security. Finally, we discuss several challenges and future research directions for FL evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.11841v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Di Chai, Leye Wang, Liu Yang, Junxue Zhang, Kai Chen, Qiang Yang</dc:creator>
    </item>
    <item>
      <title>The Implications of Decentralization in Blockchained Federated Learning: Evaluating the Impact of Model Staleness and Inconsistencies</title>
      <link>https://arxiv.org/abs/2310.07471</link>
      <description>arXiv:2310.07471v2 Announce Type: replace-cross 
Abstract: Blockchain promises to enhance distributed machine learning (ML) approaches such as federated learning (FL) by providing further decentralization, security, immutability, and trust, which are key properties for enabling collaborative intelligence in next-generation applications. Nonetheless, the intrinsic decentralized operation of peer-to-peer (P2P) blockchain nodes leads to an uncharted setting for FL, whereby the concepts of FL round and global model become meaningless, as devices' synchronization is lost without the figure of a central orchestrating server. In this paper, we study the practical implications of outsourcing the orchestration of FL to a democratic setting such as in a blockchain. In particular, we focus on the effects that model staleness and inconsistencies, endorsed by blockchains' modus operandi, have on the training procedure held by FL devices asynchronously. Using simulation, we evaluate the blockchained FL operation by applying two different ML models (ranging from low to high complexity) on the well-known MNIST and CIFAR-10 datasets, respectively, and focus on the accuracy and timeliness of the solutions. Our results show the high impact of model inconsistencies on the accuracy of the models (up to a ~35% decrease in prediction accuracy), which underscores the importance of properly designing blockchain systems based on the characteristics of the underlying FL application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07471v2</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesc Wilhelmi, Nima Afraz, Elia Guerra, Paolo Dini</dc:creator>
    </item>
    <item>
      <title>Multi-Agent Online Graph Exploration on Cycles and Tadpole Graphs</title>
      <link>https://arxiv.org/abs/2402.13845</link>
      <description>arXiv:2402.13845v2 Announce Type: replace-cross 
Abstract: We study the problem of multi-agent online graph exploration, in which a team of k agents has to explore a given graph, starting and ending on the same node. The graph is initially unknown. Whenever a node is visited by an agent, its neighborhood and adjacent edges are revealed. The agents share a global view of the explored parts of the graph. The cost of the exploration has to be minimized, where cost either describes the time needed for the entire exploration (time model), or the length of the longest path traversed by any agent (energy model). We investigate graph exploration on cycles and tadpole graphs for 2-4 agents, providing optimal results on the competitive ratio in the energy model (1-competitive with two agents on cycles and three agents on tadpole graphs), and for tadpole graphs in the time model (1.5-competitive with four agents). We also show competitive upper bounds of 2 for the exploration of tadpole graphs with three agents, and 2.5 for the exploration of tadpole graphs with two agents in the time model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13845v2</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erik van den Akker, Kevin Buchin, Klaus-Tycho Foerster</dc:creator>
    </item>
    <item>
      <title>Plotinus: A Satellite Internet Digital Twin System</title>
      <link>https://arxiv.org/abs/2403.08515</link>
      <description>arXiv:2403.08515v2 Announce Type: replace-cross 
Abstract: The development of an integrated space-air-ground network (SAGIN) requires sophisticated satellite Internet emulation tools that can handle complex, dynamic topologies and offer in-depth analysis. Existing emulation platforms struggle with challenges like the need for detailed implementation across all network layers, real-time response, and scalability. This paper proposes a digital twin system based on microservices for satellite Internet emulation, namely Plotinus, which aims to solve these problems. Plotinus features a modular design, allowing for easy replacement of the physical layer to emulate different aerial vehicles and analyze channel interference. It also enables replacing path computation methods to simplify testing and deploying algorithms. In particular, Plotinus allows for real-time emulation with live network traffic, enhancing practical network models. The evaluation result shows Plotinus's effective emulation of dynamic satellite networks with real-world devices. Its adaptability for various communication models and algorithm testing highlights Plotinus's role as a vital tool for developing and analyzing SAGIN systems, offering a cross-layer, real-time and scalable digital twin system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08515v2</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Gao, Kun Qiu, Zhe Chen, Wenjun Zhu, Qi Zhang, Handong Luo, Quanwei Lin, Ziheng Yang, Wenhao Liu</dc:creator>
    </item>
    <item>
      <title>A learning-based solution approach to the application placement problem in mobile edge computing under uncertainty</title>
      <link>https://arxiv.org/abs/2403.11259</link>
      <description>arXiv:2403.11259v2 Announce Type: replace-cross 
Abstract: Placing applications in mobile edge computing servers presents a complex challenge involving many servers, users, and their requests. Existing algorithms take a long time to solve high-dimensional problems with significant uncertainty scenarios. Therefore, an efficient approach is required to maximize the quality of service while considering all technical constraints. One of these approaches is machine learning, which emulates optimal solutions for application placement in edge servers. Machine learning models are expected to learn how to allocate user requests to servers based on the spatial positions of users and servers. In this study, the problem is formulated as a two-stage stochastic programming. A sufficient amount of training records is generated by varying parameters such as user locations, their request rates, and solving the optimization model. Then, based on the distance features of each user from the available servers and their request rates, machine learning models generate decision variables for the first stage of the stochastic optimization model, which is the user-to-server request allocation, and are employed as independent decision agents that reliably mimic the optimization model. Support Vector Machines (SVM) and Multi-layer Perceptron (MLP) are used in this research to achieve practical decisions from the stochastic optimization models. The performance of each model has shown an execution effectiveness of over 80%. This research aims to provide a more efficient approach for tackling high-dimensional problems and scenarios with uncertainties in mobile edge computing by leveraging machine learning models for optimal decision-making in request allocation to edge servers. These results suggest that machine-learning models can significantly improve solution times compared to conventional approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11259v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>eess.SP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taha-Hossein Hejazi, Zahra Ghadimkhani, Arezoo Borji</dc:creator>
    </item>
    <item>
      <title>FedNMUT -- Federated Noisy Model Update Tracking Convergence Analysis</title>
      <link>https://arxiv.org/abs/2403.13247</link>
      <description>arXiv:2403.13247v2 Announce Type: replace-cross 
Abstract: A novel Decentralized Noisy Model Update Tracking Federated Learning algorithm (FedNMUT) is proposed that is tailored to function efficiently in the presence of noisy communication channels that reflect imperfect information exchange. This algorithm uses gradient tracking to minimize the impact of data heterogeneity while minimizing communication overhead. The proposed algorithm incorporates noise into its parameters to mimic the conditions of noisy communication channels, thereby enabling consensus among clients through a communication graph topology in such challenging environments. FedNMUT prioritizes parameter sharing and noise incorporation to increase the resilience of decentralized learning systems against noisy communications. Theoretical results for the smooth non-convex objective function are provided by us, and it is shown that the $\epsilon-$stationary solution is achieved by our algorithm at the rate of $\mathcal{O}\left(\frac{1}{\sqrt{T}}\right)$, where $T$ is the total number of communication rounds. Additionally, via empirical validation, we demonstrated that the performance of FedNMUT is superior to the existing state-of-the-art methods and conventional parameter-mixing approaches in dealing with imperfect information sharing. This proves the capability of the proposed algorithm to counteract the negative effects of communication noise in a decentralized learning framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13247v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vishnu Pandi Chellapandi, Antesh Upadhyay, Abolfazl Hashemi, Stanislaw H. \.Zak</dc:creator>
    </item>
  </channel>
</rss>
