<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Nov 2025 03:53:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Regularized Reduced Rank Regression for mixed predictor and response variables</title>
      <link>https://arxiv.org/abs/2511.16718</link>
      <description>arXiv:2511.16718v1 Announce Type: new 
Abstract: In this paper, we introduce the Generalized Mixed Regularized Reduced Rank Regression model (GMR4), an extension of the GMR3 model designed to improve performance in high-dimensional settings. GMR3 is a regression method for a mix of numeric, binary and ordinal response variables, while also allowing for mixed-type predictors through optimal scaling. GMR4 extends this approach by incorporating regularization techniques, such as Ridge, Lasso, Group Lasso, or any combination thereof, making the model suitable for datasets with a large number of predictors or collinearity among them. In addition, we propose a cross-validation procedure that enables the estimation of the rank S and the penalty parameter lambda. Through a simulation study, we evaluate the performance of the model under different scenarios, varying the sample size, the number of non-informative predictors and response dimension. The results of the simulation study guide the choice of the penalty parameter lambda in the empirical application ISSP: Health and Healthcare I-II (2023), which includes mixed-type predictors and ordinal responses. In this application, the model results in a sparse and interpretable solution, with a limited set of influential predictors that provide insights into public attitudes toward healthcare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16718v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenza Cotugno, Mark de Rooij, Roberta Siciliano</dc:creator>
    </item>
    <item>
      <title>Correlation Matters! Streamlining the Sample Size Procedure with Composite Time-to-event Endpoints</title>
      <link>https://arxiv.org/abs/2511.16773</link>
      <description>arXiv:2511.16773v1 Announce Type: new 
Abstract: Composite endpoints are widely used in cardiovascular clinical trials to improve statistical efficiency while preserving clinical relevance. The Win Ratio (WR) measure and more general frameworks of Win Statistics have emerged as increasingly popular alternatives to traditional time-to-first-event analyses. Although analytic sample size formulas for WR have been developed, they rely on design parameters that are often not straightforward to specify. Consequently, sample size determination in clinical trials with WR as the primary analysis is most often based on simulations, which can be computationally intensive. Moreover, these simulations commonly assume independence among component endpoints, an assumption that may not hold in practice and can lead to misleading power estimates. To address this challenge, we derive refined formulas to calculate the proportions of wins, losses, and ties for multiple prioritized time-to-event endpoints. These formulas rely on familiar design inputs and become directly applicable when integrated with existing sample size methods. We conduct a comprehensive assessment of how correlation among endpoints affects sample size requirements across varying design features. We further demonstrate the role of correlations through two case studies based on the landmark SPRINT and STICH clinical trials to generate further insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16773v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunhan Mou, Fan Li, Denise Esserman, Yuan Huang</dc:creator>
    </item>
    <item>
      <title>Single-Dataset Meta-Analysis For Many-Analysts And Multiverse Studies</title>
      <link>https://arxiv.org/abs/2511.17064</link>
      <description>arXiv:2511.17064v1 Announce Type: new 
Abstract: Empirical claims often rely on one population, design, and analysis. Many-analysts, multiverse, and robustness studies expose how results can vary across plausible analytic choices. Synthesizing these results, however, is nontrivial as all results are computed from the same dataset. We introduce single-dataset meta-analysis, a weighted-likelihood approach that incorporates the information in the dataset at most once. It prevents overconfident inferences that would arise if a standard meta-analysis was applied to the data. Single-dataset meta-analysis yields meta-analytic point and interval estimates of the average effect across analytic approaches and of between-analyst heterogeneity, and can be supplied by classical and Bayesian hypothesis tests. Both the common-effect and random-effects versions of the model can be estimated by standard meta-analytic software with small input adjustments. We demonstrate the method via application to the many-analysts study on racial bias in soccer, the many-analysts study of marital status and cardiovascular disease, and the multiverse study on technology use and well-being. The results show how single-dataset meta-analysis complements the qualitative evaluation of many-analysts and multiverse studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17064v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Franti\v{s}ek Barto\v{s}, Suzanne Hoogeveen, Alexandra Sarafoglou, Samuel Pawel</dc:creator>
    </item>
    <item>
      <title>Shape Analysis of Euclidean Curves under Frenet-Serret Framework</title>
      <link>https://arxiv.org/abs/2511.17065</link>
      <description>arXiv:2511.17065v1 Announce Type: new 
Abstract: Geometric frameworks for analyzing curves are common in applications as they focus on invariant features and provide visually satisfying solutions to standard problems such as computing invariant distances, averaging curves, or registering curves. We show that for any smooth curve in R^d, d&gt;1, the generalized curvatures associated with the Frenet-Serret equation can be used to define a Riemannian geometry that takes into account all the geometric features of the shape. This geometry is based on a Square Root Curvature Transform that extends the square root-velocity transform for Euclidean curves (in any dimensions) and provides likely geodesics that avoid artefacts encountered by representations using only first-order geometric information. Our analysis is supported by simulated data and is especially relevant for analyzing human motions. We consider trajectories acquired from sign language, and show the interest of considering curvature and also torsion in their analysis, both being physically meaningful.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17065v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>IEEE/CVF International Conference on Computer Vision (ICCV) 2023</arxiv:journal_reference>
      <dc:creator>Perrine Chassat, Juhyun Park, Nicolas Brunel</dc:creator>
    </item>
    <item>
      <title>Flexible unimodal density estimation in hidden Markov models</title>
      <link>https://arxiv.org/abs/2511.17071</link>
      <description>arXiv:2511.17071v1 Announce Type: new 
Abstract: 1. Hidden Markov models (HMMs) are powerful tools for modelling time-series data with underlying state structure. However, selecting appropriate parametric forms for the state-dependent distributions is often challenging and can lead to model misspecification. To address this, P-spline-based nonparametric estimation of state-dependent densities has been proposed. While offering great flexibility, these approaches can result in overly complex densities (e.g. bimodal) that hinder interpretability. 2. We propose a straightforward method that builds on shape-constrained spline theory to enforce unimodality in the estimated state-dependent densities through enforcing unimodality of the spline coefficients. This constraint strikes a practical balance between model flexibility, interpretability, and parsimony. 3. Through two simulation studies and a real-world case study using narwhal (Monodon monoceros) dive data, we demonstrate the proposed approach yields more stable estimates compared to fully flexible, unconstrained models improving model performance and interpretability. 4. Our method bridges a key methodological gap, by providing a parsimonious HMM framework that balances the interpretability of parametric models with the flexibility of nonparametric estimation. This provides ecologists with a powerful tool to derive ecologically meaningful inference from telemetry data while avoiding the pitfalls of overly complex models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17071v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan-Ole Koslik, Fanny Dupont, Marie Auger-M\'eth\'e, Marianne Marcoux, Nancy Heckman</dc:creator>
    </item>
    <item>
      <title>ggskewboxplots: Enhanced Boxplots for Skewed Data in R</title>
      <link>https://arxiv.org/abs/2511.17091</link>
      <description>arXiv:2511.17091v1 Announce Type: new 
Abstract: Traditional boxplots are widely used for summarizing and visualizing the distribution of numerical data, yet they exhibit significant limitations when applied to skewed or heavy-tailed distributions, often leading to misclassification of outliers through swamping -- flagging typical observations as outliers -- or masking -- failing to detect true outliers. This paper addresses these limitations by systematically evaluating several alternative boxplots specifically designed to accommodate distributional asymmetry. We introduce ggskewboxplots, an R package that integrates multiple robust and skewness-aware boxplot variants, providing a unified and user-friendly framework for exploratory data analysis. Using extensive Monte Carlo simulations under controlled skewness and kurtosis conditions, implemented via the mosaic approach based on the Skewed Exponential Power distribution, we assess the sensitivity and specificity of each method. Simulation results indicate that classical Tukey-style boxplots are highly prone to swamping and masking, whereas robust skewness-adjusted variants -- particularly those leveraging quartile-based skewness measures or medcouple-based adjustments -- achieve substantially better performance. These findings offer practical guidance for selecting reliable boxplot methods in applied settings and demonstrate how the ggskewboxplots package facilitates accessible, distribution-aware visualizations within the familiar ggplot2 workflow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17091v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mustafa Cavus</dc:creator>
    </item>
    <item>
      <title>Nonparametric Inference for Extreme CoVaR and CoES</title>
      <link>https://arxiv.org/abs/2511.17180</link>
      <description>arXiv:2511.17180v1 Announce Type: new 
Abstract: Systemic risk measures quantify the potential risk to an individual financial constituent arising from the distress of entire financial system. As a generalization of two widely applied risk measures, Value-at-Risk and Expected Shortfall, the Conditional Value-at-Risk (CoVaR) and Conditional Expected Shortfall (CoES) have recently been receiving growing attention on applications in economics and finance, since they serve as crucial metrics for systemic risk measurement. However, existing approaches confront some challenges in statistical inference and asymptotic theories when estimating CoES, particularly at high risk levels. In this paper, within a framework of upper tail dependence, we propose several extrapolative methods to estimate both extreme CoVaR and CoES nonparametrically via an adjustment factor, which are intimately related to the nonparametric modelling of the tail dependence function. In addition, we study the asymptotic theories of all proposed extrapolative methods based on multivariate extreme value theory. Finally, some simulations and real data analyses are conducted to demonstrate the empirical performances of our methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17180v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qingzhao Zhong, Yanxi Hou</dc:creator>
    </item>
    <item>
      <title>The Experimental Unit Information Index: Balancing Evidentiary Value and Sample Size of Adaptive Designs</title>
      <link>https://arxiv.org/abs/2511.17292</link>
      <description>arXiv:2511.17292v1 Announce Type: new 
Abstract: Reducing the number of experimental units is one of the three pillars of the 3R principles (Replace, Reduce, Refine) in animal research. At the same time, statistical error rates need to be controlled to enable reliable inferences and decisions. This paper proposes a novel measure to quantify the evidentiary value of one experimental unit for a given study design. The experimental unit information index (EUII) is based on power, Type-I error and sample size, and has attractive interpretations both in terms of frequentist error rates and Bayesian posterior odds. We introduce the EUII in simple statistical test settings and show that its asymptotic value depends only on the assumed relative effect size under the alternative. We then extend the definition to adaptive designs where early stopping for efficacy or futility may cause reductions in sample size. Applications to group-sequential designs and a recently proposed adaptive statistical test procedure show the usefulness of the approach when the goal is to maximize the evidentiary value of one experimental unit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17292v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonhard Held, Fadoua Balabdaoui, Samuel Pawel</dc:creator>
    </item>
    <item>
      <title>Covariate Connectivity Combined Clustering for Weighted Networks</title>
      <link>https://arxiv.org/abs/2511.17302</link>
      <description>arXiv:2511.17302v1 Announce Type: new 
Abstract: Community detection is a central task in network analysis, with applications in social, biological, and technological systems. Traditional algorithms rely primarily on network topology, which can fail when community signals are partly encoded in node-specific attributes. Existing covariate-assisted methods often assume the number of clusters is known, involve computationally intensive inference, or are not designed for weighted networks. We propose $\text{C}^4$: Covariate Connectivity Combined Clustering, an adaptive spectral clustering algorithm that integrates network connectivity and node-level covariates into a unified similarity representation. $\text{C}^4$ balances the two sources of information through a data-driven tuning parameter, estimates the number of communities via an eigengap heuristic, and avoids reliance on costly sampling-based procedures. Simulation studies show that $\text{C}^4$ achieves higher accuracy and robustness than competing approaches across diverse scenarios. Application to an airport reachability network demonstrates the method's scalability, interpretability, and practical utility for real-world weighted networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17302v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeyu Hu, Wenrui Li, Jun Yan, Panpan Zhang</dc:creator>
    </item>
    <item>
      <title>U-DESPE: a Bayesian Utility-based methodology for dosing regimen optimization in early-phase oncology trials based on Dose-Exposure, Safety, Pharmacodynamics, Efficacy</title>
      <link>https://arxiv.org/abs/2511.17376</link>
      <description>arXiv:2511.17376v1 Announce Type: new 
Abstract: With the development of novel therapies such as molecularly targeted agents and immunotherapy, the maximum tolerated dose paradigm that "more is better" does not necessarily hold anymore. In this context, doses and schedules of novel therapies may be inadequately characterized and oncology drug dose-finding approaches should be revised. This is increasingly recognized by health authorities, notably through the Optimus project. We developed a Bayesian dose-finding design, called U-DESPE, which allows to either determine the optimal dosing regimen at the end of the dose-escalation phase, or use of dedicated cohorts for randomizing patients to candidate optimal dosing regimens after that safe dosing regimens have been found. U-DESPE design relies on a dose-exposure model built from pharmacokinetic data using non-linear mixed-effect modeling approaches. Then three models are built to assess the relationships between exposure and the probability of selected relevant endpoints on safety, efficacy, and pharmacodynamics. These models are then combined to predict the different endpoints for every candidate dosing regimens. Finally, a utility function is proposed to quantify the trade-off between these endpoints and to determine the optimal dosing regimen. We applied the proposed method on a clinical trial case study and performed an extensive simulation study to evaluate the operating characteristics of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17376v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ana\"is Andrillon, Sandrine Micallef, Moreno Ursino, Pavel Mozgunov, Marie-Karelle Riviere</dc:creator>
    </item>
    <item>
      <title>On treating right-censoring events like treatments</title>
      <link>https://arxiv.org/abs/2511.17379</link>
      <description>arXiv:2511.17379v1 Announce Type: new 
Abstract: In causal inference literature, potential outcomes are often indexed by the "elimination of all right-censoring events," leading to the perception that such a restriction is necessary for defining well-posed causal estimands. In this paper, we clarify that this restriction is not required: a well-defined estimand can be formulated without indexing on the elimination of such events. Achieving this requires a more precise classification of right-censoring events than has historically been considered, as the nature of these events has direct implications for identification of the target estimand. We provide a framework that distinguishes different types of right-censoring events from a causal perspective, and demonstrate how this framework relates to censoring definitions and assumptions in classical survival analysis literature. By bridging these perspectives, we provide a clearer understanding of how to handle right-censoring events and provide guidance for identifying causal estimands when right-censored events are present.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17379v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lan Wen, Aaron L. Sarvet, Jessica G. Young</dc:creator>
    </item>
    <item>
      <title>Bayesian Bridge Gaussian Process Regression</title>
      <link>https://arxiv.org/abs/2511.17415</link>
      <description>arXiv:2511.17415v1 Announce Type: new 
Abstract: The performance of Gaussian Process (GP) regression is often hampered by the curse of dimensionality, which inflates computational cost and reduces predictive power in high-dimensional problems. Variable selection is thus crucial for building efficient and accurate GP models. Inspired by Bayesian bridge regression, we propose the Bayesian Bridge Gaussian Process Regression (B\textsuperscript{2}GPR) model. This framework places $\ell_q$-norm constraints on key GP parameters to automatically induce sparsity and identify active variables. We formulate two distinct versions: one for $q=2$ using conjugate Gaussian priors, and another for $0&lt;q&lt;2$ that employs constrained flat priors, leading to non-standard, norm-constrained posterior distributions. To enable posterior inference, we design a Gibbs sampling algorithm that integrates Spherical Hamiltonian Monte Carlo (SphHMC) to efficiently sample from the constrained posteriors when $0&lt;q&lt;2$. Simulations and a real-data application confirm that B\textsuperscript{2}GPR offers superior variable selection and prediction compared to alternative approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17415v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minshen Xu, Shiwei Lan, Lulu Kang</dc:creator>
    </item>
    <item>
      <title>Iterating marginalized Bayes maps for likelihood maximization with application to nonlinear panel models</title>
      <link>https://arxiv.org/abs/2511.17438</link>
      <description>arXiv:2511.17438v1 Announce Type: new 
Abstract: Complex dynamic systems can be investigated by fitting mechanistic stochastic dynamic models to time series data. In this context, commonly used Monte Carlo inference procedures for model selection and parameter estimation quickly become computationally unfeasible as the system dimension grows. The increasing prevalence of panel data, characterized by multiple related time series, therefore necessitates the development of inference algorithms that are effective for this class of high-dimensional mechanistic models. Nonlinear, non-Gaussian mechanistic models are routinely fitted to time series data but seldom to panel data, despite its widespread availability, suggesting that the practical difficulties for existing procedures are prohibitive. We investigate the use of iterated filtering algorithms for this purpose. We introduce a novel algorithm that contains a marginalization step that mitigates issues arising from particle filtering in high dimensions. Our approach enables likelihood-based inference for models that were previously considered intractable, thus broadening the scope of dynamic models available for panel data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17438v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jesse Wheeler, Aaron J. Abkemeier, Edward L. Ionides</dc:creator>
    </item>
    <item>
      <title>Extending the Accelerated Failure Conditionals Model to Location-Scale Families</title>
      <link>https://arxiv.org/abs/2511.17463</link>
      <description>arXiv:2511.17463v1 Announce Type: new 
Abstract: Arnold and Arvanitis (2020) introduced a novel class of bivariate conditionally specified distributions, in which dependence between two random variables is established by defining the distribution of one variable conditional on the other. This conditioning regime was formulated through survival functions and termed the accelerated failure conditionals model. Subsequently, Lakhani (2025) extended this conditioning framework to encompass distributional families whose marginal densities may exhibit unimodality and skewness, thereby moving beyond families with non-increasing densities. The present study builds on this line of work by proposing a conditional survival specification derived from a location-scale distributional family, where the dependence between $X$ and $Y$ arises not only through the acceleration function but also via a location function. An illustrative example of this new specification is developed using a Weibull marginal for $X$. The resulting models are fully characterized by closed-form expressions for their moments, and simulations are implemented using the Metropolis-Hastings algorithm. Finally, the model is applied to a dataset in which the empirical distribution of $Y$ lies on the real line, demonstrating the models' capacity to accommodate $Y$ marginals defined over $\mathbb{R}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17463v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jared N. Lakhani</dc:creator>
    </item>
    <item>
      <title>Differentially private testing for relevant dependencies in high dimensions</title>
      <link>https://arxiv.org/abs/2511.17167</link>
      <description>arXiv:2511.17167v1 Announce Type: cross 
Abstract: We investigate the problem of detecting dependencies between the components of a high-dimensional vector. Our approach advances the existing literature in two important respects. First, we consider the problem under privacy constraints. Second, instead of testing whether the coordinates are pairwise independent, we are interested in determining whether certain pairwise associations between the components (such as all pairwise Kendall's $\tau$ coefficients) do not exceed a given threshold in absolute value. Considering hypotheses of this form is motivated by the observation that in the high-dimensional regime, it is rare and perhaps impossible to have a null hypothesis that can be modeled exactly by assuming that all pairwise associations are precisely equal to zero.
  The formulation of the null hypothesis as a composite hypothesis makes the problem of constructing tests already non-standard in the non-private setting. Additionally, under privacy constraints, state of the art procedures rely on permutation approaches that are rendered invalid under a composite null. We propose a novel bootstrap based methodology that is especially powerful in sparse settings, develop theoretical guarantees under mild assumptions and show that the proposed method enjoys good finite sample properties even in the high privacy regime. Additionally, we present applications in medical data that showcase the applicability of our methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17167v1</guid>
      <category>math.ST</category>
      <category>cs.CR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick Bastian, Holger Dette, Martin Dunsche</dc:creator>
    </item>
    <item>
      <title>The $\ell$-test: leveraging sparsity in the Gaussian linear model for improved inference</title>
      <link>https://arxiv.org/abs/2406.18390</link>
      <description>arXiv:2406.18390v2 Announce Type: replace 
Abstract: We develop novel LASSO-based methods for coefficient testing and confidence interval construction in the Gaussian linear model with $n\ge d$. Our methods' finite-sample validity is identical to that of their ubiquitous ordinary-least-squares-$t$-test-based analogues, yet have substantially higher power when the true coefficient vector is sparse. In particular, under sparsity our coefficient test, which we call the $\ell$-test, performs like the \emph{one-sided} $t$-test (despite not being given any information about the sign), and $\ell$-test-based confidence intervals are correspondingly shorter than the standard $t$-test-based intervals. The nature of the $\ell$-test directly provides a novel exact adjustment conditional on LASSO selection for post-selection inference, allowing for the construction of post-selection $p$-values and confidence intervals. None of our methods require resampling or Monte Carlo estimation. We perform a variety of simulations and a real data analysis on an HIV drug resistance data set to demonstrate the benefits of the $\ell$-test. We additionally show that the $\ell$-test can be applied to a large class of asymptotically Gaussian estimators, dramatically expanding its applicability beyond linear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18390v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Souhardya Sengupta, Lucas Janson</dc:creator>
    </item>
    <item>
      <title>The construction of augmented designs in square arrays</title>
      <link>https://arxiv.org/abs/2501.08448</link>
      <description>arXiv:2501.08448v4 Announce Type: replace 
Abstract: Augmented designs are typically used in early-stage breeding programs to compare single replicates of test entries by combining them with replicated check varieties. One or two dimensional incomplete blocking can be incorporated in the design to accommodate possible site variation. An augmented design in a square array can be derived from a smaller row-column design (the contraction). In a recent paper Bailey and Haines (2025) investigated the link between an augmented design in a square array and its contraction. Here we formally establish this connection by expressing the average efficiency factor of the augmented design in terms of that of its contraction. A consequence of this is that an optimal contraction can be used to construct an optimal augmented design. The table of cyclic contractions presented by Bailey and Haines (2025) is updated in terms of optimality. Specifically, in cases where a cyclic contraction is not optimal, an augmented design with optimal or near-optimal efficiency can be obtained via computer search.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08448v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>E. R. Williams, H-P. Piepho</dc:creator>
    </item>
    <item>
      <title>Extension of Dynamic Network Biomarker using the propensity score method: Simulation of causal effects on variance and correlation coefficient</title>
      <link>https://arxiv.org/abs/2505.13846</link>
      <description>arXiv:2505.13846v2 Announce Type: replace 
Abstract: In clinical biomarker studies, the Dynamic Network Biomarker (DNB) is sometimes used. DNB is a composite variable derived from the variance and the Pearson correlation coefficient of biological signals. When applying DNB to clinical data, it is important to account for confounding bias. However, little attention has been paid to statistical causal inference methods for variance and correlation coefficients. This study evaluates confounding adjustment using propensity score matching (PSM) through Monte Carlo simulations. Our results support the use of PSM to reduce bias and improve group comparisons when DNB is applied to clinical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13846v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Satoru Shinoda, Hideaki Kawaguchi</dc:creator>
    </item>
    <item>
      <title>Auto-Doubly Robust Estimation of Causal Effects on a Network</title>
      <link>https://arxiv.org/abs/2506.23332</link>
      <description>arXiv:2506.23332v2 Announce Type: replace 
Abstract: This paper develops new methods for causal inference in observational studies on a single large network of interconnected units, addressing two key challenges: long-range dependence among units and the presence of general interference. We introduce a novel network version of Augmented Inverse Propensity Weighted, which combines propensity score and outcome models defined on the network to achieve doubly robust identification and estimation of both direct and spillover causal effects. Under a network version of conditional ignorability, the proposed approach identifies the expected potential outcome for a unit given the treatment assignment vector for its network neighborhood up to a user-specified distance, while marginalizing over treatment assignments for the rest of the network. Under the union of two Markov assumptions--one governing the propensity score model and the other the outcome model--we propose a corresponding semiparametric estimator based on general parametric specifications of nuisance functions. In particular, we suggest a class of parametric auto-regression models motivated by the Markov assumptions (Besag, 1974). By combining a restricted interference assumption with the propensity score model, we establish a new doubly robust identification result for the expected potential outcome under a hypothetical intervention on the treatment assignment vector for the entire network. We formally prove that, under weak network dependence, our proposed estimators are asymptotically normal and we characterize the impact of model misspecification on the asymptotic variance. Extensive simulation studies highlight the practical relevance of our approach. We further demonstrate its application in an empirical analysis of the NNAHRAY study, evaluating the impact of incarceration on individual socioeconomic outcomes in Brooklyn, New York.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23332v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jizhou Liu, Dake Zhang, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Multidimensional constructs and moderated linear and nonlinear factor analysis</title>
      <link>https://arxiv.org/abs/2509.05443</link>
      <description>arXiv:2509.05443v2 Announce Type: replace 
Abstract: Multidimensional factor models with moderations on all model parameters have so far been limited to single-factor and two-factor models. This does not align well with existing psychological measures, which are commonly intended to assess 3-5 dimensions of a latent construct. In this paper, I introduce a multidimensional MNLFA model that permits the moderation of item intercepts, loadings, residual variances, factor means, variances, and correlations across three or more latent factors. I describe efforts to implement the model using Bayesian methods through Stan and penalized maximum likelihood approaches to stabilize estimation and detect partial measurement non-invariance while preserving model interpretability. Closed-form analytic gradients of the likelihood, eliminating the need for costly numerical or MCMC-based approximations. We conclude by discussing the theoretical implications of penalization for measurement invariance, computational considerations, and future directions for extending the framework to categorical indicators, longitudinal data, and applied research contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05443v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R. Noah Padgett</dc:creator>
    </item>
    <item>
      <title>Heterogeneity-Aware Federated Causal Inference Leveraging Effect-Measure Transportability</title>
      <link>https://arxiv.org/abs/2510.16317</link>
      <description>arXiv:2510.16317v2 Announce Type: replace 
Abstract: Federated learning of causal estimands offers a powerful strategy to improve estimation efficiency by leveraging data from multiple study sites while preserving privacy. Existing literature has primarily focused on the average treatment effect using single data source, whereas our work addresses a broader class of causal measures across multiple sources. We derive and compare semiparametrically efficient estimators under two transportability assumptions, which impose different restrictions on the data likelihood and illustrate the efficiency-robustness tradeoff. This estimator also permits the incorporation of flexible machine learning algorithms for nuisance functions while maintaining parametric convergence rates and nominal coverage. To further handle scenarios where some source sites violate transportability, we propose a Post-Federated Weighting Selection (PFWS) framework, which is a two-step procedure that adaptively identifies compatible sites and achieves the semiparametric efficiency bound asymptotically. This framework mitigates the efficiency loss of weighting methods and the instability and computational burden of direct site selection in finite samples. Through extensive simulations and real-data analysis, we demonstrate that our PFWS framework achieves superior variance efficiency compared with the target-only analyses across diverse transportability scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16317v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siqi Cao, Shu Yang</dc:creator>
    </item>
    <item>
      <title>A New Causal Rule Learning Approach to Interpretable Estimation of Heterogeneous Treatment Effect</title>
      <link>https://arxiv.org/abs/2310.06746</link>
      <description>arXiv:2310.06746v3 Announce Type: replace-cross 
Abstract: Interpretability plays a crucial role in the application of statistical learning to estimate heterogeneous treatment effects (HTE) in complex diseases. In this study, we leverage a rule-based workflow, namely causal rule learning (CRL), to estimate and improve our understanding of HTE for atrial septal defect, addressing an overlooked question in the previous literature: what if an individual simultaneously belongs to multiple groups with different average treatment effects? The CRL process consists of three steps: rule discovery, which generates a set of causal rules with corresponding subgroup average treatment effects; rule selection, which identifies a subset of these rules to deconstruct individual-level treatment effects as a linear combination of subgroup-level effects; and rule analysis, which presents a detailed procedure for further analyzing each selected rule from multiple perspectives to identify the most promising rules for validation. Extensive simulation studies and real-world data analysis demonstrate that CRL outperforms other methods in providing interpretable estimates of HTE, especially when dealing with complex ground truth and sufficient sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.06746v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying Wu, Hanzhong Liu, Kai Ren, Shujie Ma, Xiangyu Chang</dc:creator>
    </item>
    <item>
      <title>Estimating Bidirectional Causal Effects with Large Scale Online Kernel Learning</title>
      <link>https://arxiv.org/abs/2511.05050</link>
      <description>arXiv:2511.05050v2 Announce Type: replace-cross 
Abstract: In this study, a scalable online kernel learning framework is proposed for estimating bidirectional causal effects in systems characterized by mutual dependence and heteroskedasticity. Traditional causal inference often focuses on unidirectional effects, overlooking the common bidirectional relationships in real-world phenomena. Building on heteroskedasticity-based identification, the proposed method integrates a quasi-maximum likelihood estimator for simultaneous equation models with large scale online kernel learning. It employs random Fourier feature approximations to flexibly model nonlinear conditional means and variances, while an adaptive online gradient descent algorithm ensures computational efficiency for streaming and high-dimensional data. Results from extensive simulations demonstrate that the proposed method achieves superior accuracy and stability than single equation and polynomial approximation baselines, exhibiting lower bias and root mean squared error across various data-generating processes. These results confirm that the proposed approach effectively captures complex bidirectional causal effects with near-linear computational scaling. By combining econometric identification with modern machine learning techniques, the proposed framework offers a practical, scalable, and theoretically grounded solution for large scale causal inference in natural/social science, policy making, business, and industrial applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05050v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masahiro Tanaka</dc:creator>
    </item>
    <item>
      <title>A unified approach to spatial domain detection and cell-type deconvolution in spot-based spatial transcriptomics</title>
      <link>https://arxiv.org/abs/2511.06204</link>
      <description>arXiv:2511.06204v2 Announce Type: replace-cross 
Abstract: Popular technologies for generating spatially resolved transcriptomic data measure gene expression at the resolution of a "spot", i.e., a small tissue region 55 microns in diameter. Each spot can contain many cells of different types. In typical analyses, researchers are interested in using these data to identify and profile discrete spatial domains in the tissue. In this paper, we propose a new method, DUET, that simultaneously identifies discrete spatial domains and estimates each spot's cell-type proportion. This allows the identified spatial domains to be characterized in terms of the cell type proportions, which affords interpretability and biological insight. DUET utilizes a constrained version of model-based convex clustering, and as such, can accommodate Poisson, negative binomial, normal, and other types of expression data. Through simulation studies and multiple applications, we show that DUET can achieve better clustering and deconvolution performance than existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06204v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hyun Jung Koo, Aaron J. Molstad</dc:creator>
    </item>
  </channel>
</rss>
