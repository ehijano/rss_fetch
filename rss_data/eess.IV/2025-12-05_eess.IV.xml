<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>eess.IV updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/eess.IV</link>
    <description>eess.IV updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/eess.IV" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Dec 2025 05:00:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Structure-Aware Adaptive Kernel MPPCA Denoising for Diffusion MRI</title>
      <link>https://arxiv.org/abs/2512.04586</link>
      <description>arXiv:2512.04586v1 Announce Type: new 
Abstract: Diffusion-weighted MRI (DWI) at high b-values often suffers from low signal-to-noise ratio (SNR), making image quality poor. Marchenko-Pastur PCA (MPPCA) is a popular method to reduce noise, but it uses a fixed patch size across the whole image, which doesn't work well in regions with different structures. To address this, we propose an adaptive kernel MPPCA (ak-MPPCA) that selects the best patch size for each voxel based on its local neighborhood. This improves denoising performance by better handling structural variations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04586v1</guid>
      <category>eess.IV</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ananya Singhal, Dattesh Dayanand Shanbhag, Sudhanya Chatterjee</dc:creator>
    </item>
    <item>
      <title>Multi Task Denoiser Training for Solving Linear Inverse Problems</title>
      <link>https://arxiv.org/abs/2512.04709</link>
      <description>arXiv:2512.04709v1 Announce Type: new 
Abstract: Plug-and-Play Priors (PnP) and Regularisation by Denoising (RED) have established that image denoisers can effectively replace traditional regularisers in linear inverse problem solvers for tasks like super-resolution, demosaicing, and inpainting. It is now well established in the literature that a denoiser's residual links to the gradient of the image log prior (Miyasawa and Tweedie), enabling iterative, gradient ascent-based image generation (e.g., diffusion models), as well as new methods for solving inverse problems. Building on this, we propose enhancing Kadkhodaie and Simoncelli's gradient-based inverse solvers by fine-tuning the denoiser within the iterative solving process itself. Training the denoiser end-to-end across the solver framework and simultaneously across multiple tasks yields a single, versatile denoiser optimised for inverse problems. We demonstrate that even a simple baseline model fine-tuned this way achieves an average PSNR improvement of +1.34 dB across six diverse inverse problems while reducing the required iterations. Furthermore, we analyse the fine-tuned denoiser's properties, finding that its optimisation objective implicitly shifts from minimising standard denoising error (MMSE) towards approximating an ideal prior gradient specifically tailored for guiding inverse recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04709v1</guid>
      <category>eess.IV</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3756863</arxiv:DOI>
      <dc:creator>Cl\'ement Bled, Fran\c{c}ois Piti\'e</dc:creator>
    </item>
    <item>
      <title>Deep infant brain segmentation from multi-contrast MRI</title>
      <link>https://arxiv.org/abs/2512.05114</link>
      <description>arXiv:2512.05114v1 Announce Type: cross 
Abstract: Segmentation of magnetic resonance images (MRI) facilitates analysis of human brain development by delineating anatomical structures. However, in infants and young children, accurate segmentation is challenging due to development and imaging constraints. Pediatric brain MRI is notoriously difficult to acquire, with inconsistent availability of imaging modalities, substantial non-head anatomy in the field of view, and frequent motion artifacts. This has led to specialized segmentation models that are often limited to specific image types or narrow age groups, or that are fragile for more variable images such as those acquired clinically. We address this method fragmentation with BabySeg, a deep learning brain segmentation framework for infants and young children that supports diverse MRI protocols, including repeat scans and image types unavailable during training. Our approach builds on recent domain randomization techniques, which synthesize training images far beyond realistic bounds to promote dataset shift invariance. We also describe a mechanism that enables models to flexibly pool and interact features from any number of input scans. We demonstrate state-of-the-art performance that matches or exceeds the accuracy of several existing methods for various age cohorts and input configurations using a single model, in a fraction of the runtime required by many existing tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05114v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Malte Hoffmann, Lilla Z\"ollei, Adrian V. Dalca</dc:creator>
    </item>
    <item>
      <title>Towards Modality- and Sampling-Universal Learning Strategies for Accelerating Cardiovascular Imaging: Summary of the CMRxRecon2024 Challenge</title>
      <link>https://arxiv.org/abs/2503.03971</link>
      <description>arXiv:2503.03971v3 Announce Type: replace 
Abstract: Cardiovascular health is vital to human well-being, and cardiac magnetic resonance (CMR) imaging is considered the {clinical reference standard} for diagnosing cardiovascular disease. However, its adoption is hindered by long scan times, complex contrasts, and inconsistent quality. While deep learning methods perform well on specific CMR imaging {sequences}, they often fail to generalize across modalities and sampling schemes. The lack of benchmarks for high-quality, fast CMR image reconstruction further limits technology comparison and adoption. The CMRxRecon2024 challenge, attracting over 200 teams from 18 countries, addressed these issues with two tasks: generalization to unseen {modalities} and robustness to diverse undersampling patterns. We introduced the largest public multi-{modality} CMR raw dataset, an open benchmarking platform, and shared code. Analysis of the best-performing solutions revealed that prompt-based adaptation and enhanced physics-driven consistency enabled strong cross-scenario performance. These findings establish principles for generalizable reconstruction models and advance clinically translatable AI in cardiovascular imaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03971v3</guid>
      <category>eess.IV</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fanwen Wang, Zi Wang, Yan Li, Jun Lyu, Chen Qin, Shuo Wang, Kunyuan Guo, Mengting Sun, Mingkai Huang, Haoyu Zhang, Michael T\"anzer, Qirong Li, Xinran Chen, Jiahao Huang, Yinzhe Wu, Haosen Zhang, Kian Anvari Hamedani, Yuntong Lyu, Longyu Sun, Qing Li, Tianxing He, Lizhen Lan, Qiong Yao, Ziqiang Xu, Bingyu Xin, Dimitris N. Metaxas, Narges Razizadeh, Shahabedin Nabavi, George Yiasemis, Jonas Teuwen, Zhenxi Zhang, Sha Wang, Chi Zhang, Daniel B. Ennis, Zhihao Xue, Chenxi Hu, Ruru Xu, Ilkay Oksuz, Donghang Lyu, Yanxin Huang, Xinrui Guo, Ruqian Hao, Jaykumar H. Patel, Guanke Cai, Binghua Chen, Yajing Zhang, Sha Hua, Zhensen Chen, Qi Dou, Xiahai Zhuang, Qian Tao, Wenjia Bai, Jing Qin, He Wang, Claudia Prieto, Michael Markl, Alistair Young, Hao Li, Xihong Hu, Lianming Wu, Xiaobo Qu, Guang Yang, Chengyan Wang</dc:creator>
    </item>
    <item>
      <title>TraceTrans: Translation and Spatial Tracing for Surgical Prediction</title>
      <link>https://arxiv.org/abs/2510.22379</link>
      <description>arXiv:2510.22379v4 Announce Type: replace 
Abstract: Image-to-image translation models have achieved notable success in converting images across visual domains and are increasingly used for medical tasks such as predicting post-operative outcomes and modeling disease progression. However, most existing methods primarily aim to match the target distribution and often neglect spatial correspondences between the source and translated images. This limitation can lead to structural inconsistencies and hallucinations, undermining the reliability and interpretability of the predictions. These challenges are accentuated in clinical applications by the stringent requirement for anatomical accuracy. In this work, we present TraceTrans, a novel deformable image translation model designed for post-operative prediction that generates images aligned with the target distribution while explicitly revealing spatial correspondences with the pre-operative input. The framework employs an encoder for feature extraction and dual decoders for predicting spatial deformations and synthesizing the translated image. The predicted deformation field imposes spatial constraints on the generated output, ensuring anatomical consistency with the source. Extensive experiments on medical cosmetology and brain MRI datasets demonstrate that TraceTrans delivers accurate and interpretable post-operative predictions, highlighting its potential for reliable clinical deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22379v4</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiyu Luo, Haodong Li, Xinxing Cheng, He Zhao, Yang Hu, Xuan Song, Tianyang Zhang</dc:creator>
    </item>
    <item>
      <title>Deep Feature-specific Imaging</title>
      <link>https://arxiv.org/abs/2508.01981</link>
      <description>arXiv:2508.01981v4 Announce Type: replace-cross 
Abstract: Modern photon-counting sensors are increasingly dominated by Poisson noise, yet conventional Feature-Specific Imaging (FSI) is optimized for additive Gaussian noise, leading to suboptimal performance and a loss of its advantages under Poisson noise. To address this, we introduce DeepFSI, a novel end-to-end optical-electronic framework. DeepFSI "unfreezes" traditional FSI masks, enabling a deep neural network to learn globally optimal measurement masks by computing gradients directly under realistic Poisson and additive noise conditions. Our simulations demonstrate DeepFSI's superior feature fidelity and task performance compared to conventional FSI with predefined masks, especially in Poisson-Noise-dominant environments. DeepFSI also exhibits enhanced robustness to design choices and performs well under additive Gaussian noise, representing a significant advance for noise-robust computational imaging in photon-limited applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01981v4</guid>
      <category>physics.optics</category>
      <category>eess.IV</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yizhou Lu, Andreas Velten</dc:creator>
    </item>
    <item>
      <title>BrainPath: A Biologically-Informed AI Framework for Individualized Aging Brain Generation</title>
      <link>https://arxiv.org/abs/2508.16667</link>
      <description>arXiv:2508.16667v3 Announce Type: replace-cross 
Abstract: The global population is aging rapidly, and aging is a major risk factor for various diseases. It is an important task to predict how each individual's brain will age, as the brain supports many human functions. This capability can greatly facilitate healthcare automation by enabling personalized, proactive intervention and efficient healthcare resource allocation. However, this task is extremely challenging because of the brain's complex 3D anatomy. While there have been successes in natural image generation and brain MRI synthesis, existing methods fall short in generating individualized, anatomically faithful aging brain trajectories. To address these gaps, we propose BrainPath, a novel AI model that, given a single structural MRI of an individual, generates synthetic longitudinal MRIs that represent that individual's expected brain anatomy as they age. BrainPath introduces three architectural innovations: an age-aware encoder with biologically grounded supervision, a differential age conditioned decoder for anatomically faithful MRI synthesis, and a swap-learning strategy that implicitly separates stable subject-specific anatomy from aging effects. We further design biologically informed loss functions, including an age calibration loss and an age and structural perceptual loss, to complement the conventional reconstruction loss. This enables the model to capture subtle, temporally meaningful anatomical changes associated with aging. We apply BrainPath to two of the largest public aging datasets and conduct a comprehensive, multifaceted evaluation. Our results demonstrate BrainPath's superior performance in generation accuracy, anatomical fidelity, and cross-dataset generalizability, outperforming competing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16667v3</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Li, Javad Sohankar, Ji Luo, Jing Li, Yi Su</dc:creator>
    </item>
  </channel>
</rss>
