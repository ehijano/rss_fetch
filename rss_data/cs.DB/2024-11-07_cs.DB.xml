<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 08 Nov 2024 02:45:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>{\lambda}-Tune: Harnessing Large Language Models for Automated Database System Tuning</title>
      <link>https://arxiv.org/abs/2411.03500</link>
      <description>arXiv:2411.03500v1 Announce Type: new 
Abstract: We introduce {\lambda}-Tune, a framework that leverages Large Language Models (LLMs) for automated database system tuning. The design of {\lambda}-Tune is motivated by the capabilities of the latest generation of LLMs. Different from prior work, leveraging LLMs to extract tuning hints for single parameters, {\lambda}-Tune generates entire configuration scripts, based on a large input document, describing the tuning context. {\lambda}-Tune generates alternative configurations, using a principled approach to identify the best configuration, out of a small set of candidates. In doing so, it minimizes reconfiguration overheads and ensures that evaluation costs are bounded as a function of the optimal run time. By treating prompt generation as a cost-based optimization problem, {\lambda}-Tune conveys the most relevant context to the LLM while bounding the number of input tokens and, therefore, monetary fees for LLM invocations. We compare {\lambda}-Tune to various baselines, using multiple benchmarks and PostgreSQL and MySQL as target systems for tuning, showing that {\lambda}-Tune is significantly more robust than prior approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03500v1</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Giannankouris, Immanuel Trummer</dc:creator>
    </item>
    <item>
      <title>Instance-Optimal Acyclic Join Processing Without Regret: Engineering the Yannakakis Algorithm in Column Stores</title>
      <link>https://arxiv.org/abs/2411.04042</link>
      <description>arXiv:2411.04042v1 Announce Type: new 
Abstract: Acyclic join queries can be evaluated instance-optimally using Yannakakis' algorithm, which avoids needlessly large intermediate results through semi-join passes. Recent work proposes to address the significant hidden constant factors arising from a naive implementation of Yannakakis by decomposing the hash join operator into two suboperators, called Lookup and Expand. In this paper, we present a novel method for integrating Lookup and Expand plans in interpreted environments, like column stores, formalizing them using Nested Semijoin Algebra (NSA) and implementing them through a shredding approach. We characterize the class of NSA expressions that can be evaluated instance-optimally as those that are 2-phase: no `shrinking' operator is applied after an unnest (i.e., expand). We introduce Shredded Yannakakis (SYA), an evaluation algorithm for acyclic joins that, starting from a binary join plan, transforms it into a 2-phase NSA plan, and then evaluates it through the shredding technique. We show that SYA is provably robust (i.e., never produces large intermediate results) and without regret (i.e., is never worse than the binary join plan under a suitable cost model) on the class of well-behaved binary join plans. Our experiments on a suite of 1,849 queries show that SYA improves performance for 88.7% of the queries with speedups up to 188x, while remaining competitive on the other queries. We hope this approach offers a fresh perspective on Yannakakis' algorithm, helping system engineers better understand its practical benefits and facilitating its adoption into a broader spectrum of query engines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04042v1</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Liese Bekkers, Frank Neven, Stijn Vansummeren, Yisu Remy Wang</dc:creator>
    </item>
    <item>
      <title>Tabular Data Synthesis with Differential Privacy: A Survey</title>
      <link>https://arxiv.org/abs/2411.03351</link>
      <description>arXiv:2411.03351v1 Announce Type: cross 
Abstract: Data sharing is a prerequisite for collaborative innovation, enabling organizations to leverage diverse datasets for deeper insights. In real-world applications like FinTech and Smart Manufacturing, transactional data, often in tabular form, are generated and analyzed for insight generation. However, such datasets typically contain sensitive personal/business information, raising privacy concerns and regulatory risks. Data synthesis tackles this by generating artificial datasets that preserve the statistical characteristics of real data, removing direct links to individuals. However, attackers can still infer sensitive information using background knowledge. Differential privacy offers a solution by providing provable and quantifiable privacy protection. Consequently, differentially private data synthesis has emerged as a promising approach to privacy-aware data sharing. This paper provides a comprehensive overview of existing differentially private tabular data synthesis methods, highlighting the unique challenges of each generation model for generating tabular data under differential privacy constraints. We classify the methods into statistical and deep learning-based approaches based on their generation models, discussing them in both centralized and distributed environments. We evaluate and compare those methods within each category, highlighting their strengths and weaknesses in terms of utility, privacy, and computational complexity. Additionally, we present and discuss various evaluation methods for assessing the quality of the synthesized data, identify research gaps in the field and directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03351v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengmeng Yang, Chi-Hung Chi, Kwok-Yan Lam, Jie Feng, Taolin Guo, Wei Ni</dc:creator>
    </item>
    <item>
      <title>Learning Aggregate Queries Defined by First-Order Logic with Counting</title>
      <link>https://arxiv.org/abs/2411.04003</link>
      <description>arXiv:2411.04003v1 Announce Type: cross 
Abstract: In the logical framework introduced by Grohe and Tur\'an (TOCS 2004) for Boolean classification problems, the instances to classify are tuples from a logical structure, and Boolean classifiers are described by parametric models based on logical formulas. This is a specific scenario for supervised passive learning, where classifiers should be learned based on labelled examples. Existing results in this scenario focus on Boolean classification. This paper presents learnability results beyond Boolean classification. We focus on multiclass classification problems where the task is to assign input tuples to arbitrary integers. To represent such integer-valued classifiers, we use aggregate queries specified by an extension of first-order logic with counting terms called FOC1.
  Our main result shows the following: given a database of polylogarithmic degree, within quasi-linear time, we can build an index structure that makes it possible to learn FOC1-definable integer-valued classifiers in time polylogarithmic in the size of the database and polynomial in the number of training examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04003v1</guid>
      <category>cs.LO</category>
      <category>cs.DB</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steffen van Bergerem, Nicole Schweikardt</dc:creator>
    </item>
    <item>
      <title>Semantic orchestration and exploitation of material data: A dataspace solution demonstrated on steel and copper applications</title>
      <link>https://arxiv.org/abs/2406.19509</link>
      <description>arXiv:2406.19509v3 Announce Type: replace 
Abstract: In materials science and manufacturing, vast amounts of heterogeneous data (e.g., measurement and simulation logs, process data, publications) serve as the bedrock of valuable knowledge for various engineering applications. However, efficiently storing and managing this diverse data poses challenges due to limited standardization and integration across different organizational units. Addressing these challenges is essential to fully unlock the potential of data-driven approaches. This paper introduces novel, comprehensive semantic methodology tailored to materials engineering and realized as a technology stack named Dataspace Management System (DSMS), which powers dataspace solutions that leverage the knowledge encoded in heterogeneous data sources to support data-driven insights and to derive new knowledge. At its core, DSMS offers a distinctive knowledge management approach tuned to meet the specific requirements of the materials science and manufacturing domain, all while adhering to the FAIR principles. DSMS provides functionalities for data integration, linkage, exploration, visualization, processing, data sharing, and services (e.g., consulting) to support engineers in decision-making, design and optimization. We present an architectural overview of DSMS, outlining its core concepts and their technological implementation, as well as demonstrate its applicability to common data-processing tasks through use cases from the StahlDigital and KupferDigital research projects within Germany's MaterialDigital initiative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19509v3</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yoav Nahshon, Lukas Morand, Matthias B\"uschelberger, Dirk Helm, Kiran Kumaraswamy, Paul Zierep, Matthias Weber, Pablo de Andr\'es</dc:creator>
    </item>
    <item>
      <title>Data needs and challenges for quantum dot devices automation</title>
      <link>https://arxiv.org/abs/2312.14322</link>
      <description>arXiv:2312.14322v3 Announce Type: replace-cross 
Abstract: Gate-defined quantum dots are a promising candidate system for realizing scalable, coupled qubit systems and serving as a fundamental building block for quantum computers. However, present-day quantum dot devices suffer from imperfections that must be accounted for, which hinders the characterization, tuning, and operation process. Moreover, with an increasing number of quantum dot qubits, the relevant parameter space grows sufficiently to make heuristic control infeasible. Thus, it is imperative that reliable and scalable autonomous tuning approaches are developed. This meeting report outlines current challenges in automating quantum dot device tuning and operation with a particular focus on datasets, benchmarking, and standardization. We also present insights and ideas put forward by the quantum dot community on how to overcome them. We aim to provide guidance and inspiration to researchers invested in automation efforts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14322v3</guid>
      <category>cond-mat.mes-hall</category>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <category>quant-ph</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1038/s41534-024-00878-x</arxiv:DOI>
      <arxiv:journal_reference>npj Quantum Inf. 10, 105 (2024)</arxiv:journal_reference>
      <dc:creator>Justyna P. Zwolak, Jacob M. Taylor, Reed W. Andrews, Jared Benson, Garnett W. Bryant, Donovan Buterakos, Anasua Chatterjee, Sankar Das Sarma, Mark A. Eriksson, Eli\v{s}ka Greplov\'a, Michael J. Gullans, Fabian Hader, Tyler J. Kovach, Pranav S. Mundada, Mick Ramsey, Torbj{\o}rn Rasmussen, Brandon Severin, Anthony Sigillito, Brennan Undseth, Brian Weber</dc:creator>
    </item>
    <item>
      <title>TableGPT2: A Large Multimodal Model with Tabular Data Integration</title>
      <link>https://arxiv.org/abs/2411.02059</link>
      <description>arXiv:2411.02059v3 Announce Type: replace-cross 
Abstract: The emergence of models like GPTs, Claude, LLaMA, and Qwen has reshaped AI applications, presenting vast new opportunities across industries. Yet, the integration of tabular data remains notably underdeveloped, despite its foundational role in numerous real-world domains.
  This gap is critical for three main reasons. First, database or data warehouse data integration is essential for advanced applications; second, the vast and largely untapped resource of tabular data offers immense potential for analysis; and third, the business intelligence domain specifically demands adaptable, precise solutions that many current LLMs may struggle to provide.
  In response, we introduce TableGPT2, a model rigorously pre-trained and fine-tuned with over 593.8K tables and 2.36M high-quality query-table-output tuples, a scale of table-related data unprecedented in prior research. This extensive training enables TableGPT2 to excel in table-centric tasks while maintaining strong general language and coding abilities.
  One of TableGPT2's key innovations is its novel table encoder, specifically designed to capture schema-level and cell-level information. This encoder strengthens the model's ability to handle ambiguous queries, missing column names, and irregular tables commonly encountered in real-world applications. Similar to visual language models, this pioneering approach integrates with the decoder to form a robust large multimodal model.
  We believe the results are compelling: over 23 benchmarking metrics, TableGPT2 achieves an average performance improvement of 35.20% in the 7B model and 49.32% in the 72B model over prior benchmark-neutral LLMs, with robust general-purpose capabilities intact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02059v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aofeng Su, Aowen Wang, Chao Ye, Chen Zhou, Ga Zhang, Gang Chen, Guangcheng Zhu, Haobo Wang, Haokai Xu, Hao Chen, Haoze Li, Haoxuan Lan, Jiaming Tian, Jing Yuan, Junbo Zhao, Junlin Zhou, Kaizhe Shou, Liangyu Zha, Lin Long, Liyao Li, Pengzuo Wu, Qi Zhang, Qingyi Huang, Saisai Yang, Tao Zhang, Wentao Ye, Wufang Zhu, Xiaomeng Hu, Xijun Gu, Xinjie Sun, Xiang Li, Yuhang Yang, Zhiqing Xiao</dc:creator>
    </item>
  </channel>
</rss>
