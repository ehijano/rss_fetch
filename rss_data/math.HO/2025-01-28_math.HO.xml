<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.HO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.HO</link>
    <description>math.HO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.HO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 Jan 2025 05:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Matrix Calculus (for Machine Learning and Beyond)</title>
      <link>https://arxiv.org/abs/2501.14787</link>
      <description>arXiv:2501.14787v1 Announce Type: new 
Abstract:   This course, intended for undergraduates familiar with elementary calculus and linear algebra, introduces the extension of differential calculus to functions on more general vector spaces, such as functions that take as input a matrix and return a matrix inverse or factorization, derivatives of ODE solutions, and even stochastic derivatives of random functions. It emphasizes practical computational applications, such as large-scale optimization and machine learning, where derivatives must be re-imagined in order to be propagated through complicated calculations. The class also discusses efficiency concerns leading to "adjoint" or "reverse-mode" differentiation (a.k.a. "backpropagation"), and gives a gentle introduction to modern automatic differentiation (AD) techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14787v1</guid>
      <category>math.HO</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Paige Bright, Alan Edelman, Steven G. Johnson</dc:creator>
    </item>
    <item>
      <title>Sophomore's dream function: asymptotics, complex plane behavior and relation to the error function</title>
      <link>https://arxiv.org/abs/2501.10936</link>
      <description>arXiv:2501.10936v2 Announce Type: replace-cross 
Abstract: Sophomore's dream sum $S=\sum_{n=1}^\infty n^{-n}$ is extended to the function $f(t,a)=t\int_{0}^{1}(ax)^{-tx}dx$ with $f(1,1)=S$. Asymptotic behavior for a large $|t|$ is obtained, which is exponential for $t&gt;0$ and $t&lt;0,a&gt;1$, and inverse-logarithmic for $t&lt;0,a&lt;1$. An advanced approximation includes a half-derivative of the exponent and is expressed in terms of the error function. This approach provides excellent interpolation description in the complex plane. The function $f(t,a)$ demonstrates for $a&gt;1$ oscillating behavior along the imaginary axis with slowly increasing amplitude and the period of $2\pi iea$, modulation by high-frequency oscillations being present. Also, $f(t,a)$ has non-trivial zeros in the left complex half-plane with Im$t_n \simeq 2(n-1/8)\pi e/a$ for $a&gt;1$. The results obtained describe analytical integration of the function $x^{tx}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10936v2</guid>
      <category>math.CA</category>
      <category>math.HO</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>V. Yu. Irkhin</dc:creator>
    </item>
  </channel>
</rss>
