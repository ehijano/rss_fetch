<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Feb 2026 05:00:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Personalized Policy Learning through Discrete Experimentation: Theory and Empirical Evidence</title>
      <link>https://arxiv.org/abs/2602.05099</link>
      <description>arXiv:2602.05099v1 Announce Type: new 
Abstract: Randomized Controlled Trials (RCTs), or A/B testing, have become the gold standard for optimizing various operational policies on online platforms. However, RCTs on these platforms typically cover a limited number of discrete treatment levels, while the platforms increasingly face complex operational challenges involving optimizing continuous variables, such as pricing and incentive programs. The current industry practice involves discretizing these continuous decision variables into several treatment levels and selecting the optimal discrete treatment level. This approach, however, often leads to suboptimal decisions as it cannot accurately extrapolate performance for untested treatment levels and fails to account for heterogeneity in treatment effects across user characteristics. This study addresses these limitations by developing a theoretically solid and empirically verified framework to learn personalized continuous policies based on high-dimensional user characteristics, using observations from an RCT with only a discrete set of treatment levels. Specifically, we introduce a deep learning for policy targeting (DLPT) framework that includes both personalized policy value estimation and personalized policy learning. We prove that our policy value estimators are asymptotically unbiased and consistent, and the learned policy achieves a root-n-regret bound. We empirically validate our methods in collaboration with a leading social media platform to optimize incentive levels for content creation. Results demonstrate that our DLPT framework significantly outperforms existing benchmarks, achieving substantial improvements in both evaluating the value of policies for each user group and identifying the optimal personalized policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05099v1</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiqi Zhang, Zhiyu Zeng, Ruohan Zhan, Dennis Zhang</dc:creator>
    </item>
    <item>
      <title>Nested Pseudo-GMM Estimation of Demand for Differentiated Products</title>
      <link>https://arxiv.org/abs/2602.05137</link>
      <description>arXiv:2602.05137v1 Announce Type: new 
Abstract: We propose a fast algorithm for computing the GMM estimator in the BLP demand model (Berry, Levinsohn, and Pakes, 1995). Inspired by nested pseudo-likelihood methods for dynamic discrete choice models, our approach avoids repeatedly solving the inverse demand system by swapping the order of the GMM optimization and the fixed-point computation. We show that, by fixing consumer-level outside-option probabilities, BLP's market-share to mean-utility inversion becomes closed-form and, crucially, separable across products, yielding a nested pseudo-GMM algorithm with analytic gradients. The resulting estimator scales dramatically better with the number of products and is naturally suited for parallel and multithreaded implementation. In the inner loop, outside-option probabilities are treated as fixed objects while a pseudo-GMM criterion is minimized with respect to the structural parameters, substantially reducing computational cost. Monte Carlo simulations and an empirical application show that our method is significantly faster than the fastest existing alternatives, with efficiency gains that grow more than proportionally in the number of products. We provide MATLAB and Julia code to facilitate implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05137v1</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Aguirregabiria, Hui Liu, Yao Luo</dc:creator>
    </item>
    <item>
      <title>Predictive Synthesis under Sporadic Participation: Evidence from Inflation Density Surveys</title>
      <link>https://arxiv.org/abs/2602.05226</link>
      <description>arXiv:2602.05226v1 Announce Type: cross 
Abstract: Central banks rely on density forecasts from professional surveys to assess inflation risks and communicate uncertainty. A central challenge in using these surveys is irregular participation: forecasters enter and exit, skip rounds, and reappear after long gaps. In the European Central Bank's Survey of Professional Forecasters, turnover and missingness vary substantially over time, causing the set of submitted predictions to change from quarter to quarter. Standard aggregation rules -- such as equal-weight pooling, renormalization after dropping missing forecasters, or ad hoc imputation -- can generate artificial jumps in combined predictions driven by panel composition rather than economic information, complicating real-time interpretation and obscuring forecaster performance. We develop coherent Bayesian updating rules for forecast combination under sporadic participation that maintain a well-defined latent predictive state for each forecaster even when their forecast is unobserved. Rather than relying on renormalization or imputation, the combined predictive distribution is updated through the implied conditional structure of the panel. This approach isolates genuine performance differences from mechanical participation effects and yields interpretable dynamics in forecaster influence. In the ECB survey, it improves predictive accuracy relative to equal-weight benchmarks and delivers smoother and better-calibrated inflation density forecasts, particularly during periods of high turnover.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05226v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew C. Johnson, Matteo Luciani, Minzhengxiong Zhang, Kenichiro McAlinn</dc:creator>
    </item>
    <item>
      <title>An invariant modification of the bilinear form test</title>
      <link>https://arxiv.org/abs/2602.05592</link>
      <description>arXiv:2602.05592v1 Announce Type: cross 
Abstract: The invariance properties of certain likelihood-based asymptotic tests as well as their extensions for M-estimation, estimating functions and the generalized method of moments have been well studied. The simulation study reported in Crudu and Osorio [Econ. Lett. 187: 108885, 2020] shows that the bilinear form test is not invariant to one-to-one transformations of the parameter space. This paper provides a set of suitable conditions to establish the invariance property under reparametrization of the bilinear form test for linear or nonlinear hypotheses that arise in extremum estimation which leads to a simple modification of the test statistic. Evidence from a Monte Carlo simulation experiment suggests good performance of the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05592v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Angelo Garate, Felipe Osorio, Federico Crudu</dc:creator>
    </item>
    <item>
      <title>An Improved Inference for IV Regressions</title>
      <link>https://arxiv.org/abs/2506.23816</link>
      <description>arXiv:2506.23816v2 Announce Type: replace 
Abstract: Empirical instrumental variables (IV) studies often report separate results based on low-dimensional instruments and many base instruments. This paper proposes a combination test that integrates these commonly reported statistics. The test linearly combines a cluster-robust Wald statistic based on low-dimensional IVs with leave-one-cluster-out Lagrangian Multiplier (LM) and Anderson-Rubin (AR) statistics constructed from many IVs. Under strong identification of the low-dimensional IVs, we establish joint asymptotic normality and asymptotic optimality of the proposed test. The procedure yields costless efficiency improvements, automatically adapts to weak identification of many instruments, and is accompanied by a practical rule of thumb for assessing efficiency gains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23816v2</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liyu Dou, Pengjin Min, Wenjie Wang, Yichong Zhang</dc:creator>
    </item>
  </channel>
</rss>
