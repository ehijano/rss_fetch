<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Feb 2026 08:34:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Data-driven configuration tuning of glmnet to balance accuracy and computation time</title>
      <link>https://arxiv.org/abs/2602.17922</link>
      <description>arXiv:2602.17922v1 Announce Type: new 
Abstract: glmnet is a widely adopted R package for lasso estimation due to its computational efficiency. Despite its popularity, glmnet sometimes yields solutions that are substantially different from the true ones because of the inappropriate default configuration of the algorithm. The accuracy of the obtained solutions can be improved by appropriately tuning the configuration. However, improving accuracy typically increases computational time, resulting in a trade-off between accuracy and computational efficiency. Therefore, it is essential to establish a systematic approach to determine appropriate configuration. To address this need, we propose a unified data-driven framework specifically designed to optimize the configuration by balancing the trade-off between accuracy and computational efficiency. We generate large-scale simulated datasets and apply glmnet under various configurations to obtain accuracy and computation time. Based on these results, we construct neural networks that predict accuracy and computation time from data characteristics and configuration. Given a new dataset, our framework uses the neural networks to explore the configuration space and derive a Pareto front that represents the trade-off between accuracy and computational cost. This front allows us to automatically identify the configuration that maximize accuracy under a user-specified time constraint. The proposed method is implemented in the R package 'glmnetconf', available at https://github.com/Shuhei-Muroya/glmnetconf.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17922v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuhei Muroya, Kei Hirose</dc:creator>
    </item>
    <item>
      <title>Smoothness and other hyperparameter estimation for inverse problems related to data assimilation</title>
      <link>https://arxiv.org/abs/2602.18328</link>
      <description>arXiv:2602.18328v1 Announce Type: new 
Abstract: We consider Bayesian inverse problems arising in data assimilation for dynamical systems governed by partial and stochastic partial differential equations. The space-time dependent field is inferred jointly with static parameters of the prior and likelihood densities. Particular emphasis is placed on the hyperparameter controlling the prior smoothness and regularity, which is critical in ensuring well-posedness, shaping posterior structure, and determining predictive uncertainty. Commonly it is assumed to be known and fixed a priori; however in this paper we will adopt a hierarchical Bayesian framework in which smoothness and other hyperparameters are treated as unknown and assigned hyperpriors. Posterior inference is performed using Metropolis-within-Gibbs sampling suitable to high dimensions, for which hyperparameter estimation involves little computational overhead. The methodology is demonstrated on inverse problems for the Navier-Stokes equations and the stochastic advection-diffusion equation, under sparse and dense observation regimes, using Gaussian priors with different covariance structure. Numerical results show that jointly estimating the smoothness substantially reduces the errors in uncertainty quantification and parameter estimation induced by smoothness misspecification, by achieving performance comparable to scenarios in which the true smoothness is known.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18328v1</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Baptiste Simandoux, Nikolas Kantas, Dan Crisan</dc:creator>
    </item>
    <item>
      <title>Model Error Embedding with Orthogonal Gaussian Processes</title>
      <link>https://arxiv.org/abs/2602.17923</link>
      <description>arXiv:2602.17923v1 Announce Type: cross 
Abstract: Computational models of complex physical systems often rely on simplifying assumptions which inevitably introduce model error, with consequent predictive errors. Given data on model observables, the estimation of parameterized model-error representations, along with other model parameters, would be ideally done while separating the contributions of each of the two sets of parameters, in order to ensure meaningful stand-alone model predictions. This work builds an embedded model error framework using a weight-space representation of Gaussian processes (GPs) to flexibly capture model-error spatiotemporal correlations and enable inference with GP-embedding in non-linear models. To disambiguate model and model-error/bias parameters, we extend an existing orthogonal GP method to the embedded model-error setting and derive appropriate orthogonality constraints. To address the increased dimensionality introduced by the GP representation, we employ the likelihood-informed subspace method. The construction is demonstrated on linear and non-linear examples, where it effectively corrects model predictions to match data trends. Extrapolation beyond the training data recovers the prior predictive distribution, and the orthogonality constraints lead to meaningful stand-alone model predictions and nearly uncorrelated posteriors between model and model-error parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17923v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mridula Kuppa, Khachik Sargsyan, Marco Panesi, Habib N. Najm</dc:creator>
    </item>
    <item>
      <title>A variational framework for modal estimation</title>
      <link>https://arxiv.org/abs/2602.17956</link>
      <description>arXiv:2602.17956v1 Announce Type: cross 
Abstract: We approach multivariate mode estimation through Gibbs distributions and introduce GERVE (Gibbs-measure Entropy-Regularised Variational Estimation), a likelihood-free framework that approximates Gibbs measures directly from samples by maximizing an entropy-regularised variational objective with natural-gradient updates. GERVE brings together kernel density estimation, mean-shift, variational inference, and annealing in a single platform for mode estimation. It fits Gaussian mixtures that concentrate on high-density regions and yields cluster assignments from responsibilities, with reduced sensitivity to the chosen number of components. We provide theory in two regimes: as the Gibbs temperature approaches zero, mixture components converge to population modes; at fixed temperature, maximisers of the empirical objective exist, are consistent, and are asymptotically normal. We also propose a bootstrap procedure for per-mode confidence ellipses and stability scores. Simulation and real-data studies show accurate mode recovery and emergent clustering, robust to mixture overspecification. GERVE is a practical likelihood-free approach when the number of modes or groups is unknown and full density estimation is impractical.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17956v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>T\^am LeMinh, Julyan Arbel, Florence Forbes, Hien Duy Nguyen</dc:creator>
    </item>
    <item>
      <title>Preconditioned Robust Neural Posterior Estimation for Misspecified Simulators</title>
      <link>https://arxiv.org/abs/2602.18004</link>
      <description>arXiv:2602.18004v1 Announce Type: cross 
Abstract: Simulation-based inference (SBI) enables parameter estimation for complex stochastic models with intractable likelihoods when model simulation is feasible. Neural posterior estimation (NPE) is a popular SBI approach that often achieves accurate inference with far fewer simulations than classical approaches. But in practice, neural approaches can be unreliable for two reasons: incompatible data summaries arising from model misspecification yield unreliable posteriors due to extrapolation, and prior-predictive draws can produce extreme summaries that lead to difficulties in obtaining an accurate posterior for the observed data of interest. Existing preconditioning schemes target well-specified settings, and their behaviour under misspecification remains unexplored. We study preconditioning under misspecification and propose preconditioned robust neural posterior estimation, which computes data-dependent weights that focus training near the observed summaries and fits a robust neural posterior approximation. We also introduce a forest-proximity preconditioning approach that uses tree-based proximity scores to down-weight outlying simulations and concentrate computation around the observed dataset. Across two synthetic examples and one real example with incompatible summaries and extreme prior-predictive behaviour, we demonstrate that preconditioning combined with robust NPE increases stability and improves accuracy, calibration, and posterior-predictive fit over standard baseline methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18004v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan P. Kelly, David T. Frazier, David J. Warne, Christopher C. Drovandi</dc:creator>
    </item>
    <item>
      <title>Adaptive tuning of Hamiltonian Monte Carlo methods</title>
      <link>https://arxiv.org/abs/2506.04082</link>
      <description>arXiv:2506.04082v3 Announce Type: replace 
Abstract: With the recently increased interest in probabilistic models, the efficiency of an underlying sampler becomes a crucial consideration. Hamiltonian Monte Carlo (HMC) is one popular option for models of this kind. Performance of the method, however, strongly relies on a choice of parameters associated with an integration for Hamiltonian equations. Up to date, such a choice remains mainly heuristic or introduces time complexity. We propose a novel computationally inexpensive and flexible approach (we call it Adaptive Tuning or ATune) that, by combining a theoretical analysis of the multivariate Gaussian model with simulation data generated during a burn-in stage of a HMC simulation, detects a system specific splitting integrator with a set of reliable sampler's hyperparameters, including their credible randomization intervals, to be readily used in a production simulation. The method automatically eliminates those values of simulation parameters which could cause undesired extreme scenarios, such as resonance artifacts, low accuracy or poor sampling. The new approach is implemented in the in-house software package HaiCS, with no computational overheads introduced in a production simulation, and can be easily incorporated in any package for Bayesian inference with HMC. The tests on popular statistical models reveal the superiority of adaptively tuned standard and generalized HMC (GHMC) methods in terms of stability, performance and accuracy over conventional HMC tuned heuristically and coupled with the well-established integrators. We also claim that GHMC is preferable for achieving high sampling performance. The efficiency of the new methodology is assessed in comparison with state-of-the-art samplers, e.g. NUTS, in real-world applications, such as endocrine therapy resistance in cancer, modeling of cell-cell adhesion dynamics and influenza A epidemic outbreak.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04082v3</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Akhmatskaya, Lorenzo Nagar, Jose Antonio Carrillo, Leonardo Gavira Balmacz, Hristo Inouzhe, Mart\'in Parga Pazos, Mar\'ia Xos\'e Rodr\'iguez \'Alvarez</dc:creator>
    </item>
    <item>
      <title>StochTree: BART-based modeling in R and Python</title>
      <link>https://arxiv.org/abs/2512.12051</link>
      <description>arXiv:2512.12051v2 Announce Type: replace 
Abstract: stochtree is a C++ library for Bayesian tree ensemble models such as BART and Bayesian Causal Forests (BCF), as well as user-specified variations. Unlike previous BART packages, stochtree provides bindings to both R and Python for full interoperability. stochtree boasts a more comprehensive range of models relative to previous packages, including heteroskedastic forests, random effects, and treed linear models. Additionally, stochtree offers flexible handling of model fits: the ability to save model fits, reinitialize models from existing fits (facilitating improved model initialization heuristics), and pass fits between R and Python. On both platforms, stochtree exposes lower-level functionality, allowing users to specify models incorporating Bayesian tree ensembles without needing to modify C++ code. We illustrate the use of stochtree in three settings: i) straightfoward applications of existing models such as BART and BCF, ii) models that include more sophisticated components like heteroskedasticity and leaf-wise regression models, and iii) as a component of custom MCMC routines to fit nonstandard tree ensemble models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12051v2</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Herren, P. Richard Hahn, Jared Murray, Carlos Carvalho</dc:creator>
    </item>
    <item>
      <title>Learning from geometry-aware near misses to real-time COR: A corridor-wide grouped random parameters GEV framework</title>
      <link>https://arxiv.org/abs/2509.02871</link>
      <description>arXiv:2509.02871v4 Announce Type: replace-cross 
Abstract: Real-time corridor-wide crash-occurrence risk (COR) prediction is challenging because existing near-miss extreme value theory (EVT) models often oversimplify collision geometry, neglect vehicle-infrastructure (V-I) interactions, and inadequately account for spatial heterogeneity in traffic and roadway conditions. This study develops a geometry-aware two-dimensional time-to-collision (2D-TTC) near-miss extraction framework and integrates it with a hierarchical Bayesian grouped random parameter unified generalized extreme value model (HBSGRP-UGEV) to estimate short-term COR in urban corridors. The proposed framework builds on prior grouped EVT formulations while explicitly accommodating both vehicle-vehicle (V-V) and vehicle-infrastructure (V-I) near-miss processes within a unified corridor-wide modeling structure. High-resolution trajectories from the Argoverse-2 dataset were analyzed across 28 sites along Miami's Biscayne Boulevard to extract extreme near-miss events. The model incorporates vehicle dynamics and roadway features as covariates, with partial pooling across segments and intersections to capture corridor-wide heterogeneity. Results indicate that the HBSGRP-UGEV framework outperforms the fixed-parameter HBSFP-UGEV model, reducing the deviance information criterion (DIC) by up to 7.5 percent for V-V interactions and 3.1 percent for V-I interactions. Predictive validation using receiver operating characteristic area under the curve (ROC-AUC) demonstrates strong classification performance, with values of 0.89 for V-V segments, 0.82 for V-V intersections, 0.79 for V-I segments, and 0.75 for V-I intersections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02871v4</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Anis, Yang Zhou, Dominique Lord</dc:creator>
    </item>
    <item>
      <title>Quasi-Monte Carlo methods for uncertainty quantification of tumor growth modeled by a parametric semi-linear parabolic reaction-diffusion equation</title>
      <link>https://arxiv.org/abs/2509.25753</link>
      <description>arXiv:2509.25753v2 Announce Type: replace-cross 
Abstract: We study the application of a quasi-Monte Carlo (QMC) method to a class of semi-linear parabolic reaction-diffusion partial differential equations used to model tumor growth. Mathematical models of tumor growth are largely phenomenological in nature, capturing infiltration of the tumor into surrounding healthy tissue, proliferation of the existing tumor, and patient response to therapies, such as chemotherapy and radiotherapy. Considerable inter-patient variability, inherent heterogeneity of the disease, sparse and noisy data collection, and model inadequacy all contribute to significant uncertainty in the model parameters. It is crucial that these uncertainties can be efficiently propagated through the model to compute quantities of interest (QoIs), which in turn may be used to inform clinical decisions. We show that QMC methods can be successful in computing expectations of meaningful QoIs. Well-posedness results are developed for the model and used to show a theoretical error bound for the case of uniform random fields. The theoretical linear error rate, which is superior to that of standard Monte Carlo, is verified numerically. Encouraging computational results are also provided for lognormal random fields, prompting further theoretical development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25753v2</guid>
      <category>math.NA</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alexander D. Gilbert, Frances Y. Kuo, Dirk Nuyens, Graham Pash, Ian H. Sloan, Karen E. Willcox</dc:creator>
    </item>
  </channel>
</rss>
