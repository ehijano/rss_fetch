<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Nov 2025 05:04:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Handling outcome-dependent missingness with binary responses: A Heckman-like model</title>
      <link>https://arxiv.org/abs/2511.11776</link>
      <description>arXiv:2511.11776v1 Announce Type: new 
Abstract: In regression models with missing outcomes, selection bias can arise when the missingness mechanism depends on the outcome itself. This proposal focuses on an extension of the Heckman model to a setting where the outcome is binary and both the selection process and the outcome are modeled through logistic regression. A correction term analogous to the inverse Mills' ratio is derived based on relative risks. Under given assumptions, such a strategy provides an effective tool for bias correction in the presence of informative missingness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11776v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marco Doretti, Elena Stanghellini, Alessandro Taraborrelli</dc:creator>
    </item>
    <item>
      <title>Approximate Bayesian computation for stochastic hybrid systems with ergodic behaviour</title>
      <link>https://arxiv.org/abs/2511.11782</link>
      <description>arXiv:2511.11782v1 Announce Type: new 
Abstract: Piecewise diffusion Markov processes (PDifMPs) form a versatile class of stochastic hybrid systems that combine continuous diffusion processes with discrete event-driven dynamics, enabling flexible modelling of complex real-world hybrid phenomena. The practical utility of PDifMP models, however, depends critically on accurate estimation of their underlying parameters. In this work, we present a novel framework for parameter inference in PDifMPs based on approximate Bayesian computation (ABC). Our contributions are threefold. First, we provide detailed simulation algorithms for PDifMP sample paths. Second, we extend existing ABC summary statistics for diffusion processes to account for the hybrid nature of PDifMPs, showing particular effectiveness for ergodic systems. Third, we demonstrate our approach on several representative example PDifMPs that empirically exhibit ergodic behaviour. Our results show that the proposed ABC method reliably recovers model parameters across all examples, even in challenging scenarios where only partial information on jumps and diffusion is available or when parameters appear in state-dependent jump rate functions. These findings highlight the potential of ABC as a practical tool for inference in various complex stochastic hybrid systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11782v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sascha Desmettre, Agnes Mallinger, Amira Meddah, Irene Tubikanec</dc:creator>
    </item>
    <item>
      <title>Graphical Model-based Inference on Persistent Homology</title>
      <link>https://arxiv.org/abs/2511.11996</link>
      <description>arXiv:2511.11996v1 Announce Type: new 
Abstract: Persistent homology is a cornerstone of topological data analysis, offering a multiscale summary of topology with robustness to nuisance transformations, such as rotations and small deformations. Persistent homology has seen broad use across domains such as computer vision and neuroscience. Most statistical treatments, however, use homology primarily as a feature extractor, relying on statistical distance-based tests or simple time-to-event models for inferential tasks. While these approaches can detect global differences, they rarely localize the source of those differences. We address this gap by taking a graphical model-based approach: we associate each vertex with a population latent position in a conic space and model each bar's key events (birth and death times) using an exponential distribution, whose rate is a transformation of the latent positions according to an event occurring on the graph. The low-dimensional bars have simple graph-event representations, such as the formation of a minimum spanning tree or the triangulation of a loop, and thus enjoy tractable likelihoods. Taking a Bayesian approach, we infer latent positions and enable model extensions such as hierarchical models that allow borrowing strength across groups. Applications to a neuroimaging study of Alzheimer's disease demonstrate that our method localizes sources of difference and provides interpretable, model-based analyses of topological structure in complex data. The code is provided and maintained at https://github.com/zitianwu/graphPH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11996v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zitian Wu, Arkaprava Roy, Leo L. Duan</dc:creator>
    </item>
    <item>
      <title>MMDCP: A Distribution-free Approach to Outlier Detection and Classification with Coverage Guarantees and SCW-FDR Control</title>
      <link>https://arxiv.org/abs/2511.12016</link>
      <description>arXiv:2511.12016v1 Announce Type: new 
Abstract: We propose the Modified Mahalanobis Distance Conformal Prediction (MMDCP), a unified framework for multi-class classification and outlier detection under label shift, where the training and test distributions may differ. In such settings, many existing methods construct nonconformity scores based on empirical cumulative or density functions combined with data-splitting strategies. However, these approaches are often computationally expensive due to their heavy reliance on resampling procedures and tend to produce overly conservative prediction sets with unstable coverage, especially in small samples. To address these challenges, MMDCP combines class-specific distance measures with full conformal prediction to construct a score function, thereby producing adaptive prediction sets that effectively capture both inlier and outlier structures. Under mild regularity conditions, we establish convergence rates for the resulting sets and provide the first theoretical characterization of the gap between oracle and empirical conformal $p$-values, which ensures valid coverage and effective control of the class-wise false discovery rate (CW-FDR). We further introduce the Summarized Class-Wise FDR (SCW-FDR), a novel global error metric aggregating false discoveries across classes, and show that it can be effectively controlled within the MMDCP framework. Extensive simulations and two real-data applications support our theoretical findings and demonstrate the advantages of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12016v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youwu Lin, Xiaoyu Qian, Jinru Wu, Qi Liu, Pei Wang</dc:creator>
    </item>
    <item>
      <title>Aggregating Conformal Prediction Sets via {\alpha}-Allocation</title>
      <link>https://arxiv.org/abs/2511.12065</link>
      <description>arXiv:2511.12065v1 Announce Type: new 
Abstract: Conformal prediction offers a distribution-free framework for constructing prediction sets with finite-sample coverage. Yet, efficiently leveraging multiple conformity scores to reduce prediction set size remains a major open challenge. Instead of selecting a single best score, this work introduces a principled aggregation strategy, COnfidence-Level Allocation (COLA), that optimally allocates confidence levels across multiple conformal prediction sets to minimize empirical set size while maintaining provable coverage. Two variants are further developed, COLA-s and COLA-f, which guarantee finite-sample marginal coverage via sample splitting and full conformalization, respectively. In addition, we develop COLA-l, an individualized allocation strategy that promotes local size efficiency while achieving asymptotic conditional coverage. Extensive experiments on synthetic and real-world datasets demonstrate that COLA achieves considerably smaller prediction sets than state-of-the-art baselines while maintaining valid coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12065v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Congbin Xu, Yue Yu, Haojie Ren, Zhaojun Wang, Changliang Zou</dc:creator>
    </item>
    <item>
      <title>Determinants of financial and digital inclusion in West and Central Africa: Evidence from binary models with cross-validation</title>
      <link>https://arxiv.org/abs/2511.12179</link>
      <description>arXiv:2511.12179v1 Announce Type: new 
Abstract: This study examines the determinants of financial and digital inclusion in West and Central Africa using the World Bank Findex 2021 data. Unlike prior works that rely solely on traditional logit and probit models, we combine country-by-country analysis with robustness checks including K-fold cross-validation and Vuong test. Three samples were considered : a full sample combin- ing both regions and two separate subsamples for West and Central Africa. The results indicate that gender, educational attainment, income level, and place of residence are significant factors influencing both financial and digital inclusion in the full sample and the West African subsam- ple. In the Central African subsample, gender is not significant; however, age, education, income, and rural residence emerge as key determinants of financial and digital inclusion. Overall, Ghana stands out as the most financially inclusive country, although it trails Senegal in terms of credit access and digital payment use. Nigeria leads in formal account ownership and formal savings but falls considerably behind Ghana in mobile money account ownership and digital payments. Central African countries generally report lower levels of inclusion compared to West Africa, with Cameroon performing comparatively better.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12179v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ismaila A. Jallow, Samya Tajmouati</dc:creator>
    </item>
    <item>
      <title>Bayesian Causal Discovery with Cycles and Latent Confounders</title>
      <link>https://arxiv.org/abs/2511.12333</link>
      <description>arXiv:2511.12333v1 Announce Type: new 
Abstract: Learning causality from observational data has received increasing interest across various scientific fields. However, most existing methods assume the absence of latent confounders and restrict the underlying causal graph to be acyclic, assumptions that are often violated in many real-world applications. In this paper, we address these challenges by proposing a novel framework for causal discovery that accommodates both cycles and latent confounders. By leveraging the identifiability results from noisy independent component analysis and recent advances in factor analysis, we establish the unique causal identifiability under mild conditions. Building on this foundation, we further develop a fully Bayesian approach for causal structure learning, called BayCausal, and evaluate its identifiability, utility, and superior performance against state-of-the-art alternatives through extensive simulation studies. Application to a dataset from the Women's Interagency HIV Study yields interpretable and clinically meaningful insights. To facilitate broader applications, we have implemented BayCausal in an R package, BayCausal, which is the first publicly available software capable of achieving unique causal identification in the presence of both cycles and latent confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12333v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Jin, Lang Lang, Amanda B. Spence, Leah H. Rubin, Yanxun Xu</dc:creator>
    </item>
    <item>
      <title>Regression Analysis After Bipartite Bayesian Record Linkage</title>
      <link>https://arxiv.org/abs/2511.12343</link>
      <description>arXiv:2511.12343v1 Announce Type: new 
Abstract: In many settings, a data curator links records from two files to produce datasets that are shared with secondary analysts. Analysts use the linked files to estimate models of interest, such as regressions. Such two-stage approaches do not necessarily account for uncertainty in model parameters that results from uncertainty in the linkages. Further, they do not leverage the relationships among the study variables in the two files to help determine the linkages. We propose a multiple imputation framework to address these shortcomings. First, we use a bipartite Bayesian record linkage model to generate multiple plausible linked datasets, disregarding the information in the study variables. Second, we presume each linked file has a mixture of true links and false links. We estimate the mixture model using information from the study variables. Through simulation studies under a regression setting, we demonstrate that estimates of the regression model parameters can be more accurate than those based on an analogous two-stage approach. We illustrate the integrated approach using data from the Survey on Household Income and Wealth, examining a regression involving the persistence of income.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12343v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xueyan Hu, Jerome P. Reiter</dc:creator>
    </item>
    <item>
      <title>Group Identification and Variable Selection in Multivariable Mendelian Randomization with Highly-Correlated Exposures</title>
      <link>https://arxiv.org/abs/2511.12375</link>
      <description>arXiv:2511.12375v1 Announce Type: new 
Abstract: Multivariable Mendelian Randomization (MVMR) estimates the direct causal effects of multiple risk factors on an outcome using genetic variants as instruments. The growing availability of summary-level genetic data has created opportunities to apply MVMR in high-dimensional settings with many strongly correlated candidate risk factors. However, existing methods face three major limitations: weak instrument bias, limited interpretability, and the absence of valid post-selection inference. Here we introduce MVMR-PACS, a method that identifies signal-groups -- sets of causal risk factors with high genetic correlation or indistinguishable causal effects -- and estimates the direct effect of each group. MVMR-PACS minimizes a debiased objective function that reduces weak instrument bias while yielding interpretable estimates with theoretical guarantees for variable selection. We adapt a data-thinning strategy to summary-data MVMR to enable valid post-selection inference. In simulations, MVMR-PACS outperforms existing approaches in both estimation accuracy and variable selection. When applied to 27 lipoprotein subfraction traits and coronary artery disease risk, MVMR-PACS identifies biologically meaningful and robust signal-groups with interpretable direct causal effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12375v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinxiang Wu, Neil M. Davies, Ting Ye</dc:creator>
    </item>
    <item>
      <title>Transfer learning for high-dimensional Factor-augmented sparse model</title>
      <link>https://arxiv.org/abs/2511.12435</link>
      <description>arXiv:2511.12435v1 Announce Type: new 
Abstract: In this paper, we study transfer learning for high-dimensional factor-augmented sparse linear models, motivated by applications in economics and finance where strongly correlated predictors and latent factor structures pose major challenges for reliable estimation. Our framework simultaneously mitigates the impact of high correlation and removes the additional contributions of latent factors, thereby reducing potential model misspecification in conventional linear modeling. In such settings, the target dataset is often limited, but multiple heterogeneous auxiliary sources may provide additional information. We develop transfer learning procedures that effectively leverage these auxiliary datasets to improve estimation accuracy, and establish non-asymptotic $\ell_1$- and $\ell_2$-error bounds for the proposed estimators. To prevent negative transfer, we introduce a data-driven source detection algorithm capable of identifying informative auxiliary datasets and prove its consistency. In addition, we provide a hypothesis testing framework for assessing the adequacy of the factor model, together with a procedure for constructing simultaneous confidence intervals for the regression coefficients of interest. Numerical studies demonstrate that our methods achieve substantial gains in estimation accuracy and remain robust under heterogeneity across datasets. Overall, our framework offers a theoretical foundation and a practically scalable solution for incorporating heterogeneous auxiliary information in settings with highly correlated features and latent factor structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12435v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bo Fu, Dandan Jiang</dc:creator>
    </item>
    <item>
      <title>The Probabilistic Foundations of Surveillance Failure: From False Alerts to Structural Bias</title>
      <link>https://arxiv.org/abs/2511.12459</link>
      <description>arXiv:2511.12459v1 Announce Type: new 
Abstract: For decades, forensic statisticians have debated whether searching large DNA databases undermines the evidential value of a match. Modern surveillance faces an exponentially harder problem: screening populations across thousands of attributes using threshold rules rather than exact matching. Intuition suggests that requiring many coincidental matches should make false alerts astronomically unlikely. This intuition fails.
  Consider a system that monitors 1,000 attributes, each with a 0.5 percent innocent match rate. Matching 15 pre-specified attributes has probability \(10^{-35}\), one in 30 decillion, effectively impossible. But operational systems require no such specificity. They might flag anyone who matches \emph{any} 15 of the 1,000. In a city of one million innocent people, this produces about 226 false alerts. A seemingly impossible event becomes all but guaranteed. This is not an implementation flaw but a mathematical consequence of high-dimensional screening.
  We identify fundamental probabilistic limits on screening reliability. Systems undergo sharp transitions from reliable to unreliable with small increases in data scale, a fragility worsened by data growth and correlations. As data accumulate and correlation collapses effective dimensionality, systems enter regimes where alerts lose evidential value even when individual coincidences remain vanishingly rare. This framework reframes the DNA database controversy as a shift between operational regimes. Unequal surveillance exposures magnify failure, making ``structural bias'' mathematically inevitable. These limits are structural: beyond a critical scale, failure cannot be prevented through threshold adjustment or algorithmic refinement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12459v1</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <category>math.PR</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Pollanen</dc:creator>
    </item>
    <item>
      <title>Determining the K in K-fold cross-validation</title>
      <link>https://arxiv.org/abs/2511.12698</link>
      <description>arXiv:2511.12698v1 Announce Type: new 
Abstract: Cross-validation is a standard technique used across science to test how well a model predicts new data. Data are split into $K$ "folds," where one fold (i.e., hold-out set) is used to evaluate a model's predictive ability. Researchers typically rely on conventions when choosing $K$, commonly $K=5$, or $80{:}20$ split, even though the choice of $K$ can affect inference and model evaluation. Principally, this $K$ should be determined by balancing the predictive accuracy (bias) and the uncertainty of this accuracy (variance), which forms a tradeoff based on the size of the hold-out set. More training data means more accurate models, but fewer testing data lead to uncertain evaluation, and vice versa. The challenge is that this evaluation uncertainty cannot be estimated directly from data. We propose a procedure to determine the optimal $K$ by deriving a finite-sample upper bound on the evaluation uncertainty and adopting a utility-based approach to make this tradeoff explicit. Analyses of real-world datasets using linear regression and random forest demonstrate this procedure in practice, providing insight into implicit assumptions, robustness, and model performance. Critically, the results show that the optimal $K$ depends on both the data and the model, and that conventional choices implicitly make assumptions about the fundamental characteristics of the data. Our framework makes these assumptions explicit and provides a principled, transparent way to select $K$ based on the data and model rather than convention. By replacing a one-size-fits-all choice with context-specific reasoning, it enables more reliable comparisons of predictive performance across scientific domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12698v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenichiro McAlinn, Kosaku Takanashi</dc:creator>
    </item>
    <item>
      <title>Scalable and Communication-Efficient Varying Coefficient Mixed Effect Models: Methodology, Theory, and Applications</title>
      <link>https://arxiv.org/abs/2511.12732</link>
      <description>arXiv:2511.12732v1 Announce Type: new 
Abstract: Human migration exhibits complex spatiotemporal dependence driven by environmental and socioeconomic forces. Modeling such patterns at scale-often across multiple administrative or institutional boundaries-requires statistically efficient methods that remain robust under limited communication, i.e., when transmitting raw data or large design matrices across distributed nodes is costly or restricted. This paper develops a communication-efficient inference framework for Varying Coefficient Mixed Models (VCMMs) that accommodates many input variables in the mean structure and rich correlation induced by numerous random effects in hierarchical migration data. We show that a penalized spline estimator admit a Bayesian hierarchical representation, which in turn yields sufficient statistics that preserve the full likelihood contribution of each node when communication is unconstrained; aggregating these summaries reproduces the centralized estimator exactly. Under communication constraints, the same summaries define a surrogate likelihood enabling one-step estimation with first-order statistical efficiency. The framework also incorporates an SVD-enhanced implementation to ensure numerical stability and scalability, extending applicability to settings with many random effects, with or without communication limits. Statistical and theoretical guarantees are provided. Extensive simulations confirm the accuracy and robustness of the method. An application to U.S. migration flow data demonstrates its ability to efficiently and precisely uncover dynamic spatial patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12732v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lida Chalangar Jalili Dehkharghani, Li-Hsiang Lin</dc:creator>
    </item>
    <item>
      <title>SIMBA: Scalable Image Modeling using a Bayesian Approach, A Consistent Framework for Including Spatial Dependencies in fMRI Studies</title>
      <link>https://arxiv.org/abs/2511.12825</link>
      <description>arXiv:2511.12825v1 Announce Type: new 
Abstract: Bayesian spatial modeling provides a flexible framework for whole-brain fMRI analysis by explicitly incorporating spatial dependencies, overcoming the limitations of traditional massive univariate approaches that lead to information waste. In this work, we introduce SIMBA, a Scalable Image Modeling using a Bayesian Approach, for group-level fMRI analysis, which places Gaussian process (GP) priors on spatially varying functions to capture smooth and interpretable spatial association patterns across the brain volume. To address the significant computational challenges of GP inference in high-dimensional neuroimaging data, we employ a low-rank kernel approximation that enables projection into a reduced-dimensional subspace. This allows for efficient posterior computation without sacrificing spatial resolution, and we have developed efficient algorithms for this implemented in Python that achieve fully Bayesian inference either within minutes using the Gibbs sampler or within seconds using mean-field variational inference (VI). Through extensive simulation studies, we first show that SIMBA outperforms competing methods in estimation accuracy, activation detection sensitivity, and uncertainty quantification, especially in low signal-to-noise settings. We further demonstrate the scalability and interpretability of SIMBA in large-scale task-based fMRI applications, analyzing both volumetric and cortical surface data from the NARPS and ABCD studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12825v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuan Zhong, Gang Chen, Paul A. Taylor, Jian Kang</dc:creator>
    </item>
    <item>
      <title>Bayesian Variable Selection on Small Sample Trial Data via Adaptive Posterior-Informed Shrinkage Prior</title>
      <link>https://arxiv.org/abs/2511.13000</link>
      <description>arXiv:2511.13000v1 Announce Type: new 
Abstract: Identifying variables associated with clinical endpoints is of much interest in clinical trials. With the rapid growth of cell and gene therapy (CGT) and therapeutics for ultra-rare diseases, there is an urgent need for statistical methods that can detect meaningful associations under severe sample-size constraints. Motivated by data-borrowing strategies for historical controls, we propose the Adaptive Posterior-Informed Shrinkage Prior (APSP), a Bayesian approach that adaptively borrows information from external sources to improve variable-selection efficiency while preserving robustness across plausible scenarios. APSP builds upon existing Bayesian data borrowing frameworks, incorporating data-driven adaptive information selection, structure of mixture shrinkage informative priors and decision making with empirical null to enhance variable selection performances under small sample size. Extensive simulations show that APSP attains better efficiency relative to traditional and popular data-borrowing and Bayesian variable-selection methods while maintaining robustness under linear relationships. We further applied APSP to identify variables associated with peak C-peptide at Day 75 from the Clinical Islet Transplantation (CIT) Consortium study CIT06 by borrowing information from the study CIT07.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13000v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lingxuan Kong, Yumin Zhang, Chenkun Wang, Yaoyuan Vincent Tan</dc:creator>
    </item>
    <item>
      <title>Modeling group heterogeneity in spatio-temporal data via physics-informed semiparametric regression</title>
      <link>https://arxiv.org/abs/2511.13203</link>
      <description>arXiv:2511.13203v1 Announce Type: new 
Abstract: In this work we propose a novel approach for modeling spatio-temporal data characterized by group structures. In particular, we extend classical mixed effect regression models by introducing a space-time nonparametric component, regularized through a partial differential equation, to embed the physical dynamics of the underlying process, while random effects capture latent variability associated with the group structure present in the data. We propose a two-step procedure to estimate the fixed and random components of the model, relying on a functional version of the Iterative Reweighted Least Squares algorithm. We investigate the asymptotic properties of both fixed and random components, and we assess the performance of the proposed model through a simulation study, comparing it with state-of-the-art alternatives from the literature. The proposed methodology is finally applied to the study of hourly nitrogen dioxide concentration data in Lombardy (Italy), using random effects to account for measurement heterogeneity across monitoring stations equipped with different sensor technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13203v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Marco F. De Sanctis, Eleonora Arnone, Francesca Ieva, Laura M. Sangalli</dc:creator>
    </item>
    <item>
      <title>Transformation-free linear simplicial-simplicial regression via constrained iterative reweighted least squares</title>
      <link>https://arxiv.org/abs/2511.13296</link>
      <description>arXiv:2511.13296v1 Announce Type: new 
Abstract: Simplicial-simplicial regression refers to the regression setting where both the responses and predictor variables lie within the simplex space, i.e. they are compositional. \cite{fiksel2022} proposed a transformation-free lienar regression model, that minimizes the Kullback-Leibler divergence from the observed to the fitted compositions was recently proposed. To effectively estimate the regression coefficients the EM algorithm was employed. We formulate the model as a constrained logistic regression, in the spirit of \cite{tsagris2025}, and we estimate the regression coefficients using constrained iteratively reweighted least squares. This approach makes the estimation procedure significantly faster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13296v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michail Tsagris</dc:creator>
    </item>
    <item>
      <title>Novel Tau-Informed Initialization for Maximum Likelihood Estimation of Copulas with Discrete Margins</title>
      <link>https://arxiv.org/abs/2511.13337</link>
      <description>arXiv:2511.13337v1 Announce Type: new 
Abstract: We study Gaussian-copula models with discrete margins, with primary emphasis on low-count (Poisson) data. Our goal is exact yet computationally efficient maximum likelihood (ML) estimation in regimes where many observations contain small counts, which imperils both identifiability and numerical stability. We develop three novel Kendall's tau-based approaches for initialization tailored to discrete margins in the low-count regime and embed it within an inference functions for margins (IFM) inspired start. We present three practical initializers (exact, low-intensity approximation, and a transformation-based approach) that substantially reduce the number of ML iterations and improve convergence. For the ML stage, we use an unconstrained reparameterization of the model's parameters using the log and spherical-Cholesky and compute exact rectangle probabilities. Analytical score functions are supplied throughout to stabilize Newton-type optimization. A simulation study across dimensions, dependence levels, and intensity regimes shows that the proposed initialization combined with exact ML achieves lower root-mean-squared error, lower bias and faster computation times than the alternative procedures. The methodology provides a pragmatic path to retain the statistical guarantees of ML (consistency, asymptotic normality, efficiency under correct specification) while remaining tractable for moderate- to high-dimensional discrete data. We conclude with guidance on initializer choice and discuss extensions to alternative correlation structures and different margins.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13337v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna van Es, Eva Cantoni</dc:creator>
    </item>
    <item>
      <title>A Gentle Introduction to Conformal Time Series Forecasting</title>
      <link>https://arxiv.org/abs/2511.13608</link>
      <description>arXiv:2511.13608v1 Announce Type: new 
Abstract: Conformal prediction is a powerful post-hoc framework for uncertainty quantification that provides distribution-free coverage guarantees. However, these guarantees crucially rely on the assumption of exchangeability. This assumption is fundamentally violated in time series data, where temporal dependence and distributional shifts are pervasive. As a result, classical split-conformal methods may yield prediction intervals that fail to maintain nominal validity. This review unifies recent advances in conformal forecasting methods specifically designed to address nonexchangeable data. We first present a theoretical foundation, deriving finite-sample guarantees for split-conformal prediction under mild weak-dependence conditions. We then survey and classify state-of-the-art approaches that mitigate serial dependence by reweighting calibration data, dynamically updating residual distributions, or adaptively tuning target coverage levels in real time. Finally, we present a comprehensive simulation study that compares these techniques in terms of empirical coverage, interval width, and computational cost, highlighting practical trade-offs and open research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13608v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. Stocker, W. Ma{\l}gorzewicz, M. Fontana, S. Ben Taieb</dc:creator>
    </item>
    <item>
      <title>The Bottom-Up Approach for Powerful Testing with FWER Control</title>
      <link>https://arxiv.org/abs/2511.13624</link>
      <description>arXiv:2511.13624v1 Announce Type: new 
Abstract: We seek to design novel multiple testing procedures, which take into account a relevant notion of ''power'' or true discovery on the one hand, and allow computationally efficient test design and application on the other. Towards this end we characterize the optimal procedures that strongly control the family-wise error rate, for a range of power objectives measuring the success of multiple testing procedures in making true individual discoveries, and under a reasonable set of assumptions. While we cannot generally find these optimal solutions in practice, we propose the bottom-up approach, which constructs consonant closed testing procedures, while taking into account the overall power objective in designing the tests on every level of the closed testing hierarchy. This leads to a general recipe, yielding novel procedures which are computationally practical and demonstrate substantially improved power in both simulations and a real data study, compared to existing procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13624v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajesh Karmakar, Ruth Heller, Saharon Rosset</dc:creator>
    </item>
    <item>
      <title>Shortest fixed-width confidence intervals for a bounded parameter: The Push algorithm</title>
      <link>https://arxiv.org/abs/2511.13694</link>
      <description>arXiv:2511.13694v1 Announce Type: new 
Abstract: We present a method for computing optimal fixed-width confidence intervals for a single, bounded parameter, extending a method for the binomial due to Asparaouhov and Lorden, who called it the Push algorithm. The method produces the shortest possible non-decreasing confidence interval for a given confidence level, and if the Push interval does not exist for a given width and level, then no such interval exists. The method applies to any bounded parameter that is discrete, or is continuous and has the monotone likelihood ratio property. We demonstrate the method on the binomial, hypergeometric, and normal distributions with our available R package. In each of these distributions the proposed method outperforms the standard ones, and in the latter case even improves upon the $z$-interval. We apply the proposed method to World Health Organization (WHO) data on tobacco use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13694v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jay Bartroff, Asmit Chakraborty</dc:creator>
    </item>
    <item>
      <title>Simulating Keystroke and Computing the Theoretical Probability of Infinite Monkey Theorem with Markov Process</title>
      <link>https://arxiv.org/abs/2511.11760</link>
      <description>arXiv:2511.11760v1 Announce Type: cross 
Abstract: The Infinite Monkey Theorem states that if one monkey randomly hits the keys in front of a typewriter keyboard during an infinite amount of time, any works written by William Shakespeare will almost surely be typed out at the end of the total text. Due to the seemingly low chance of typing the exact literature works, our group are motivated to find out the expected time the Hamlet, our target text, being typed out by simulated random typing on a standard keyboard. For finding the answer, 30 users randomly typed characters into a file. Then, the frequency of each characters occurred following the previous character is calculated. This conditional probability is used to build the Markov matrix by considering all 128 times 128 cases. Finally, the expected time we estimated is about 10 to the power of 34 (min), which is surprisingly lower than the theoretical computation, and not achievable at all even in the cosmic time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11760v1</guid>
      <category>physics.soc-ph</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juncheng Yi, Kaiwen Zhou, James Jiang</dc:creator>
    </item>
    <item>
      <title>Socrates-Mol: Self-Oriented Cognitive Reasoning through Autonomous Trial-and-Error with Empirical-Bayesian Screening for Molecules</title>
      <link>https://arxiv.org/abs/2511.11769</link>
      <description>arXiv:2511.11769v1 Announce Type: cross 
Abstract: Molecular property prediction is fundamental to chemical engineering applications such as solvent screening. We present Socrates-Mol, a framework that transforms language models into empirical Bayesian reasoners through context engineering, addressing cold start problems without model fine-tuning. The system implements a reflective-prediction cycle where initial outputs serve as priors, retrieved molecular cases provide evidence, and refined predictions form posteriors, extracting reusable chemical rules from sparse data. We introduce ranking tasks aligned with industrial screening priorities and employ cross-model self-consistency across five language models to reduce variance. Experiments on amine solvent LogP prediction reveal task-dependent patterns: regression achieves 72% MAE reduction and 112% R-squared improvement through self-consistency, while ranking tasks show limited gains due to systematic multi-model biases. The framework reduces deployment costs by over 70% compared to full fine-tuning, providing a scalable solution for molecular property prediction while elucidating the task-adaptive nature of self-consistency mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11769v1</guid>
      <category>physics.chem-ph</category>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiangru Wang, Zekun Jiang, Heng Yang, Cheng Tan, Xingying Lan, Chunming Xu, Tianhang Zhou</dc:creator>
    </item>
    <item>
      <title>Compound Selection Decisions: An Almost SURE Approach</title>
      <link>https://arxiv.org/abs/2511.11862</link>
      <description>arXiv:2511.11862v1 Announce Type: cross 
Abstract: This paper proposes methods for producing compound selection decisions in a Gaussian sequence model. Given unknown, fixed parameters $\mu_ {1:n}$ and known $\sigma_{1:n}$ with observations $Y_i \sim \textsf{N}(\mu_i, \sigma_i^2)$, the decision maker would like to select a subset of indices $S$ so as to maximize utility $\frac{1}{n}\sum_{i\in S} (\mu_i - K_i)$, for known costs $K_i$. Inspired by Stein's unbiased risk estimate (SURE), we introduce an almost unbiased estimator, called ASSURE, for the expected utility of a proposed decision rule. ASSURE allows a user to choose a welfare-maximizing rule from a pre-specified class by optimizing the estimated welfare, thereby producing selection decisions that borrow strength across noisy estimates. We show that ASSURE produces decision rules that are asymptotically no worse than the optimal but infeasible decision rule in the pre-specified class. We apply ASSURE to the selection of Census tracts for economic opportunity, the identification of discriminating firms, and the analysis of $p$-value decision procedures in A/B testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11862v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiafeng Chen, Lihua Lei, Timothy Sudijono, Liyang Sun, Tian Xie</dc:creator>
    </item>
    <item>
      <title>A Code Smell Refactoring Approach using GNNs</title>
      <link>https://arxiv.org/abs/2511.12069</link>
      <description>arXiv:2511.12069v1 Announce Type: cross 
Abstract: Code smell is a great challenge in software refactoring, which indicates latent design or implementation flaws that may degrade the software maintainability and evolution. Over the past decades, a variety of refactoring approaches have been proposed, which can be broadly classified into metrics-based, rule-based, and machine learning-based approaches. Recent years, deep learning-based approaches have also attracted widespread attention. However, existing techniques exhibit various limitations. Metrics- and rule-based approaches rely heavily on manually defined heuristics and thresholds, whereas deep learning-based approaches are often constrained by dataset availability and model design. In this study, we proposed a graph-based deep learning approach for code smell refactoring. Specifically, we designed two types of input graphs (class-level and method-level) and employed both graph classification and node classification tasks to address the refactoring of three representative code smells: long method, large class, and feature envy. In our experiment, we propose a semi-automated dataset generation approach that could generate a large-scale dataset with minimal manual effort. We implemented the proposed approach with three classical GNN (graph neural network) architectures: GCN, GraphSAGE, and GAT, and evaluated its performance against both traditional and state-of-the-art deep learning approaches. The results demonstrate that proposed approach achieves superior refactoring performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12069v1</guid>
      <category>cs.SE</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>HanYu Zhang, Tomoji Kishi</dc:creator>
    </item>
    <item>
      <title>SCoRES: An R Package for Simultaneous Confidence Region Estimates</title>
      <link>https://arxiv.org/abs/2511.12242</link>
      <description>arXiv:2511.12242v1 Announce Type: cross 
Abstract: The identification of domain sets whose outcomes belong to predefined subsets can address fundamental risk assessment challenges in climatology and medicine. Existing approaches for inverse domain estimates require restrictive assumptions, including domain density and continuity of function near thresholds, and large-sample guarantees, which limit the applicability. Besides, the estimation and coverage depend on setting a fixed threshold level, which is difficult to determine. Recently, Ren et al. (2024) proved that confidence sets of multiple levels can be simultaneously constructed with the desired confidence non-asymptotically through inverting simultaneous confidence bands. Here, we present the SCoRES R package, which implements Ren's approach for both the estimation of the inverse region and the corresponding simultaneous outer and inner confidence regions, along with visualization tools. Besides, the package also provides functions that help construct SCBs for regression data, functional data and geographical data. To illustrate its broad applicability, we present three rigorous examples that demonstrate the SCoRES workflow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12242v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuoran Yu, Armin Schwartzman, Junting Ren, Julia Wrobel</dc:creator>
    </item>
    <item>
      <title>Multiscale Comparison of Nonparametric Trending Coefficients</title>
      <link>https://arxiv.org/abs/2511.12600</link>
      <description>arXiv:2511.12600v1 Announce Type: cross 
Abstract: This paper proposes a novel framework to test for slope heterogeneity between time-varying coefficients in panel data models. Our test not only allows us to detect whether the coefficient functions are the same across all units or not, but also determines which of them are different and where these differences are located. We establish the asymptotic validity of our multiscale test. As an extension of the proposed procedure, we show how to use the results to uncover latent group structures in the model. We apply our methods to test for heterogeneity in the effect of U.S. monetary shocks on 49 foreign economies and itself. We find evidence that such heterogeneity indeed exists and we discuss the clustering results for two groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12600v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marina Khismatullina, Bernhard van der Sluis</dc:creator>
    </item>
    <item>
      <title>Change-Point Detection Utilizing Normalized Entropy as a Fundamental Metric</title>
      <link>https://arxiv.org/abs/2511.12703</link>
      <description>arXiv:2511.12703v1 Announce Type: cross 
Abstract: This paper introduces a concept for change-point detection based on normalized entropy as a fundamental metric, aiming to overcome the dependence of traditional entropy methods on assumptions about data distribution and absolute scales. Normalized entropy maps entropy values to the [0,1] interval through standardization, accurately capturing relative changes in data complexity. By utilizing a sliding window to compute normalized entropy, this approach transforms the challenge of detecting change points in complex time series, arising from variations in scale, distribution, and diversity, into the task of identifying significant features within the normalized entropy sequence, thereby avoiding interference from parametric assumptions and effectively highlighting distributional shifts. Experimental results show that normalized entropy exhibits significant numerical fluctuation characteristics and patterns near change points across various distributions and parameter combinations. The average deviation between fluctuation moments and actual change points is only 2.4% of the sliding window size, demonstrating strong adaptability. This paper provides theoretical support for change-point detection in complex data environments and lays a methodological foundation for precise and automated detection based on normalized entropy as a fundamental metric.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12703v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 17th International Conference on Pattern Recognition and Information Processing (PRIP 2025), pp. 288-291, 2025</arxiv:journal_reference>
      <dc:creator>Qingqing Song, Shaoliang Xia</dc:creator>
    </item>
    <item>
      <title>Identification-aware Markov chain Monte Carlo</title>
      <link>https://arxiv.org/abs/2511.12847</link>
      <description>arXiv:2511.12847v1 Announce Type: cross 
Abstract: Leaving posterior sensitivity concerns aside, non-identifiability of the parameters does not raise a difficulty for Bayesian inference as far as the posterior is proper, but multi-modality or flat regions of the posterior induced by the lack of identification leaves a challenge for modern Bayesian computation. Sampling methods often struggle with slow or non-convergence when dealing with multiple modes or flat regions of the target distributions. This paper develops a novel Markov chain Monte Carlo (MCMC) approach for non-identified models, leveraging the knowledge of observationally equivalent sets of parameters, and highlights an important role that identification plays in modern Bayesian analysis. We show that our proposal overcomes the issues of being trapped in a local mode and achieves a faster rate of convergence than the existing MCMC techniques including random walk Metropolis-Hastings and Hamiltonian Monte Carlo. The gain in the speed of convergence is more significant as the dimension or cardinality of the identified sets increases. Simulation studies show its superior performance compared to other popular computational methods including Hamiltonian Monte Carlo and sequential Monte Carlo. We also demonstrate that our method uncovers non-trivial modes in the target distribution in a structural vector moving-average (SVMA) application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12847v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Toru Kitagawa, Yizhou Kuang</dc:creator>
    </item>
    <item>
      <title>On the Hierarchical Bayes justification of Empirical Bayes Confidence Intervals</title>
      <link>https://arxiv.org/abs/2511.13037</link>
      <description>arXiv:2511.13037v1 Announce Type: cross 
Abstract: Multi-level normal hierarchical models, also interpreted as mixed effects models, play an important role in developing statistical theory in multi-parameter estimation for a wide range of applications. In this article, we propose a novel reconciliation framework of the empirical Bayes (EB) and hierarchical Bayes approaches for interval estimation of random effects under a two-level normal model. Our framework shows that a second-order efficient empirical Bayes confidence interval, with EB coverage error of order $O(m^{-3/2})$, $m$ being the number of areas in the area-level model, can also be viewed as a credible interval whose posterior coverage is close to the nominal level, provided a carefully chosen prior - referred to as a 'matching prior' - is placed on the hyperparameters. While existing literature has examined matching priors that reconcile frequentist and Bayesian inference in various settings, this paper is the first to study matching priors with the goal of interval estimation of random effects in a two-level model. We obtain an area-dependent matching prior on the variance component that achieves a proper posterior under mild regularity conditions. The theoretical results in the paper are corroborated through a Monte Carlo simulation study and a real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13037v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aditi Sen, Masayo Y. Hirose, Partha Lahiri</dc:creator>
    </item>
    <item>
      <title>Variable importance measures for heterogeneous treatment effects</title>
      <link>https://arxiv.org/abs/2204.06030</link>
      <description>arXiv:2204.06030v4 Announce Type: replace 
Abstract: Motivated by applications in precision medicine and treatment effect heterogeneity, recent research has focused on estimating conditional average treatment effects (CATEs) using machine learning (ML). CATE estimates may represent complicated functions that provide little insight into the key drivers of heterogeneity. Therefore, we introduce nonparametric treatment effect variable importance measures (TE-VIMs), based on the mean-squared error (MSE) in predicting the individual treatment effect. More precisely, TE-VIMs represent the increase in MSE when variables are removed from the CATE conditioning set. We derive efficient TE-VIM estimators which can be used with any CATE estimation strategy and are amenable to ML estimation. We propose several strategies to calculate these VIMs (e.g. leave-one out, or keep-one in), using popular meta-learners for the CATE. We study the finite sample performance through a simulation study and illustrate their application using clinical trial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.06030v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oliver J. Hines, Karla Diaz-Ordaz, Stijn Vansteelandt</dc:creator>
    </item>
    <item>
      <title>A spatial interference approach to account for mobility in air pollution studies with multivariate continuous treatments</title>
      <link>https://arxiv.org/abs/2305.14194</link>
      <description>arXiv:2305.14194v3 Announce Type: replace 
Abstract: We develop new methodology to improve our understanding of the causal effects of multivariate air pollution exposures on public health. Typically, exposure to air pollution for an individual is measured at their home geographic region, though people travel to different regions with potentially different levels of air pollution. To account for this, we incorporate estimates of the mobility of individuals from cell phone mobility data to get an improved estimate of their exposure to air pollution. We treat this as an interference problem, where individuals in one geographic region can be affected by exposures in other regions due to mobility into those areas. We propose policy-relevant estimands and derive expressions showing the extent of bias one would obtain by ignoring this mobility. We additionally highlight the benefits of the proposed interference framework relative to a measurement error framework for accounting for mobility. We develop novel estimation strategies to estimate causal effects that account for this spatial spillover utilizing flexible Bayesian methodology. Lastly, we use the proposed methodology to study the health effects of ambient air pollution on mortality among Medicare enrollees in the United States.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.14194v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heejun Shin, Danielle Braun, Kezia Irene, Michelle Audirac, Joseph Antonelli</dc:creator>
    </item>
    <item>
      <title>Regularised Canonical Correlation Analysis: graphical lasso, biplots and beyond</title>
      <link>https://arxiv.org/abs/2403.02979</link>
      <description>arXiv:2403.02979v2 Announce Type: replace 
Abstract: Recent developments in regularized Canonical Correlation Analysis (CCA) promise powerful methods for high-dimensional, multiview data analysis. However, justifying the structural assumptions behind many popular approaches remains a challenge, and features of realistic biological datasets pose practical difficulties that are seldom discussed. We propose a novel CCA estimator rooted in an assumption of conditional independencies and based on the Graphical Lasso. Our method has desirable theoretical guarantees and good empirical performance, demonstrated through extensive simulations and real-world biological datasets. Recognizing the difficulties of model selection in high dimensions and other practical challenges of applying CCA in real-world settings, we introduce a novel framework for evaluating and interpreting regularized CCA models in the context of Exploratory Data Analysis (EDA), which we hope will empower researchers and pave the way for wider adoption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02979v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lennie Wells, Kumar Thurimella, Sergio Bacallado</dc:creator>
    </item>
    <item>
      <title>Prediction-Powered Inference with Imputed Covariates and Nonuniform Sampling</title>
      <link>https://arxiv.org/abs/2501.18577</link>
      <description>arXiv:2501.18577v3 Announce Type: replace 
Abstract: Machine learning models are increasingly used to produce predictions that serve as input data in subsequent statistical analyses. For example, computer vision predictions of economic and environmental indicators based on satellite imagery are used in downstream regressions; similarly, language models are widely used to approximate human ratings and opinions in social science research. However, failure to properly account for errors in the machine learning predictions renders standard statistical procedures invalid. Prior work uses what we call the Predict-Then-Debias estimator to give valid confidence intervals when machine learning algorithms impute missing variables, assuming a small complete sample from the population of interest. We expand the scope by introducing bootstrap confidence intervals that apply when the complete data is a nonuniform (i.e., weighted, stratified, or clustered) sample and to settings where an arbitrary subset of features is imputed. Importantly, the method can be applied to many settings without requiring additional calculations. We prove that these confidence intervals are valid under no assumptions on the quality of the machine learning model and are no wider than the intervals obtained by methods that do not use machine learning predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18577v3</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dan M. Kluger, Kerri Lu, Tijana Zrnic, Sherrie Wang, Stephen Bates</dc:creator>
    </item>
    <item>
      <title>On Anticipation Effect in Stepped Wedge Cluster Randomized Trials</title>
      <link>https://arxiv.org/abs/2504.08158</link>
      <description>arXiv:2504.08158v2 Announce Type: replace 
Abstract: In stepped wedge cluster randomized trials (SW-CRTs), the intervention is rolled out to clusters over multiple periods. A standard approach for analyzing SW-CRTs utilizes the linear mixed model, where the treatment effect is only present after the treatment adoption, under the assumption of no anticipation. This assumption, however, may not always hold in practice because stakeholders, providers, or individuals who are aware of the treatment adoption timing (especially when blinding is challenging or infeasible) can inadvertently change their behaviors in anticipation of the forthcoming intervention. We provide an analytical framework to address the anticipation effect in SW-CRTs and study its impact. We derive expectations of the estimators based on a collection of linear mixed models and demonstrate that when the anticipation effect is ignored, these estimators give biased estimates of the treatment effect. We also provide updated sample size formulas that explicitly account for anticipation effects, exposure-time heterogeneity, or both in SW-CRTs and illustrate their impact on study power. Through simulation studies and empirical analyses, we compare the treatment effect estimators with and without adjusting for anticipation, and provide some practical considerations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08158v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Wang, Xinyuan Chen, Katherine R. Courtright, Scott D. Halpern, Michael O. Harhay, Monica Taljaard, Fan Li</dc:creator>
    </item>
    <item>
      <title>Extreme Conformal Prediction: Reliable Intervals for High-Impact Events</title>
      <link>https://arxiv.org/abs/2505.08578</link>
      <description>arXiv:2505.08578v2 Announce Type: replace 
Abstract: Conformal prediction is a popular method to construct prediction intervals for black-box machine learning models with marginal coverage guarantees. In applications with potentially high-impact events, such as flooding or financial crises, regulators often require very high confidence for such intervals. However, if the desired level of confidence is too large relative to the amount of data used for calibration, then classical conformal methods provide infinitely wide, thus, uninformative prediction intervals. In this paper, we propose a new method to overcome this limitation. We bridge extreme value statistics and conformal prediction to provide reliable and informative prediction intervals with high-confidence coverage, which can be constructed using any black-box extreme quantile regression method. A weighted version of our approach can account for nonstationary data. The advantages of our extreme conformal prediction method are illustrated in a simulation study and in an application to flood risk forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08578v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olivier C. Pasche, Henry Lam, Sebastian Engelke</dc:creator>
    </item>
    <item>
      <title>A practical identifiability criterion leveraging weak-form parameter estimation</title>
      <link>https://arxiv.org/abs/2506.17373</link>
      <description>arXiv:2506.17373v3 Announce Type: replace 
Abstract: In this work, we define a practical identifiability criterion, (e, q)-identifiability, based on a parameter e, reflecting the noise in observed variables, and a parameter q, reflecting the mean-square error of the parameter estimator. This criterion is better able to encompass changes in the quality of the parameter estimate due to increased noise in the data (compared to existing criteria based solely on average relative errors). Furthermore, we leverage a weak-form equation error-based method of parameter estimation for systems with unobserved variables to assess practical identifiability far more quickly in comparison to output error-based parameter estimation. We do so by generating weak-form input-output equations using differential algebra techniques, as previously proposed by Boulier et al [1], and then applying Weak form Estimation of Nonlinear Dynamics (WENDy) to obtain parameter estimates. This method is computationally efficient and robust to noise, as demonstrated through two classical biological modelling examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17373v3</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nora Heitzman-Breen, Vanja Dukic, David M. Bortz</dc:creator>
    </item>
    <item>
      <title>A cautionary tale of model misspecification and identifiability</title>
      <link>https://arxiv.org/abs/2507.04894</link>
      <description>arXiv:2507.04894v3 Announce Type: replace 
Abstract: Mathematical models are routinely applied to interpret biological data, with common goals that include both prediction and parameter estimation. A challenge in mathematical biology, in particular, is that models are often complex and non-identifiable, while data are limited. Rectifying identifiability through simplification can seemingly yield more precise parameter estimates, albeit, as we explore in this perspective, at the potentially catastrophic cost of introducing model misspecification and poor accuracy. We demonstrate how uncertainty in model structure can be propagated through to uncertainty in parameter estimates using a semi-parametric Gaussian process approach that delineates parameters of interest from uncertainty in model terms. Specifically, we study generalised logistic growth with an unknown crowding function, and a spatially resolved process described by a partial differential equation with a time-dependent diffusivity parameter. Allowing for structural model uncertainty yields more robust and accurate parameter estimates, and a better quantification of remaining uncertainty. We conclude our perspective by discussing the connections between identifiability and model misspecification, and alternative approaches to dealing with model misspecification in mathematical biology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04894v3</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander P Browning, Jennifer A Flegg, Ryan J Murphy</dc:creator>
    </item>
    <item>
      <title>A nonparametric approach to practical identifiability of nonlinear mixed effects models</title>
      <link>https://arxiv.org/abs/2507.20288</link>
      <description>arXiv:2507.20288v2 Announce Type: replace 
Abstract: Mathematical modelling is a widely used approach to understand and interpret clinical trial data. This modelling typically involves fitting mechanistic mathematical models to data from individual trial participants. Despite the widespread adoption of this individual-based fitting, it is becoming increasingly common to take a hierarchical approach to parameter estimation, where modellers characterize the population parameter distributions, rather than considering each individual independently. This hierarchical parameter estimation is standard in pharmacometric modelling. However, many of the existing techniques for parameter identifiability do not immediately translate from the individual-based fitting to the hierarchical setting. Here, we propose a nonparametric approach to study practical identifiability within a hierarchical parameter estimation framework. We focus on the commonly used nonlinear mixed effects framework and investigate two well-studied examples from the pharmacometrics and viral dynamics literature to illustrate the potential utility of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20288v2</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tyler Cassidy, Stuart T. Johnston, Michael Plank, Imke Botha, Jennifer A. Flegg, Ryan J. Murphy, Sara Hamis</dc:creator>
    </item>
    <item>
      <title>Random-effects meta-analysis via generalized linear mixed models: A Bartlett-corrected approach for few studies</title>
      <link>https://arxiv.org/abs/2508.08758</link>
      <description>arXiv:2508.08758v2 Announce Type: replace 
Abstract: Random-effects models are central to meta-analysis, yet the between-study variance is often underestimated when the number of studies is small. In such settings, confidence intervals become unduly narrow and fail to attain the nominal coverage probability. Although several small-sample corrections, including the Bartlett correction, have been developed under the normal-normal model, corresponding methodology for generalized linear mixed models (GLMMs) remains limited. This study proposes a unified framework for random-effects meta-analysis within the GLMM that relies exclusively on aggregate data and accommodates outcomes that follow any distribution in the exponential family, including the binomial, Poisson, and gamma distributions. To improve interval estimation with few studies, we develop a profile likelihood method with a simplified Bartlett correction (PLSBC), which refines the chi-squared approximation of the profile likelihood ratio statistic without requiring higher-order derivatives. We show theoretically that the proposed estimators preserve the consistency and asymptotic normality of the maximum likelihood estimators. Simulation studies demonstrate that the PLSBC yields nearly unbiased estimates and maintains nominal coverage across a variety of outcome types. Applications to three published meta-analyses with binomial, Poisson, and gamma outcomes indicate that the proposed approach provides robust and interpretable inference with few studies. The PLSBC therefore offers a practical and broadly applicable framework for random-effects meta-analysis when the number of studies is limited.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08758v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keisuke Hanada, Tomoyuki Sugimoto</dc:creator>
    </item>
    <item>
      <title>Spatial disaggregation of time series</title>
      <link>https://arxiv.org/abs/2509.04065</link>
      <description>arXiv:2509.04065v2 Announce Type: replace 
Abstract: Spatiotemporal modeling of economic aggregates is increasingly relevant in regional science due to the presence of both spatial spillovers and temporal dynamics. Traditional temporal disaggregation methods, such as Chow-Lin, often ignore spatial dependence, potentially losing important regional information. We propose a novel methodology for spatiotemporal disaggregation, integrating spatial autoregressive models, benchmarking restrictions, and auxiliary covariates. The approach accommodates partially observed regional data through an anchoring mechanism, ensuring consistency with known aggregates while reducing prediction variance. We establish identifiability and asymptotic normality of the estimator under general conditions, including non-Gaussian and heteroskedastic residuals. Extensive simulations confirm the method's robustness across a wide range of spatial autocorrelations and covariate informativeness. The methodology is illustrated by disaggregating Spanish GDP into 17 autonomous communities from 2002 to 2023, using auxiliary indicators and principal component analysis for dimensionality reduction. This framework extends classical temporal disaggregation to the spatial domain, providing accurate regional estimates while accounting for spatial spillovers and irregular data availability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04065v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. Tobar, A. Mir, R. Alberich, I. Garcia Mosquera, M. Mir\'o, NA. Cruz</dc:creator>
    </item>
    <item>
      <title>Conformalized Polynomial Chaos Expansion for Uncertainty-aware Surrogate Modeling</title>
      <link>https://arxiv.org/abs/2510.22375</link>
      <description>arXiv:2510.22375v2 Announce Type: replace 
Abstract: This work introduces a method to equip data-driven polynomial chaos expansion surrogate models with intervals that quantify the predictive uncertainty of the surrogate. To that end, jackknife-based conformal prediction is integrated into regression-based polynomial chaos expansions. The jackknife algorithm uses leave-one-out residuals to generate predictive intervals around the predictions of the polynomial chaos surrogate. The jackknife+ extension additionally requires leave-one-out model predictions. Both methods allow to use the entire dataset for model training and do not require a hold-out dataset for prediction interval calibration. The key to efficient implementation is to leverage the linearity of the polynomial chaos regression model, so that leave-one-out residuals and, if necessary, leave-one-out model predictions can be computed with analytical, closed-form expressions. This eliminates the need for repeated model re-training. The conformalized polynomial chaos expansion method is first validated on four benchmark models and then applied to two electrical engineering design use-cases. The method produces predictive intervals that provide the target coverages, even for low-accuracy models trained with small datasets. At the same time, training data availability plays a crucial role in improving the empirical coverage and narrowing the predictive interval, as well as in reducing their variability over different training datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22375v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dimitrios Loukrezis, Dimitris G. Giovanis</dc:creator>
    </item>
    <item>
      <title>Is Representational Similarity Analysis Reliable? A Comparison with Regression</title>
      <link>https://arxiv.org/abs/2511.00395</link>
      <description>arXiv:2511.00395v2 Announce Type: replace 
Abstract: Representational Similarity Analysis (RSA) is a popular method for analyzing neuroimaging and behavioral data. Here we evaluate the accuracy and reliability of RSA in the context of model selection, and compare it to that of regression. Although RSA offers flexibility in handling high-dimensional, cross-modal, and cross-species data, its reliance on a transformation of raw data into similarity structures may result in the loss of critical stimulus-response information. Across extensive simulation studies and empirical analyses, we show that RSA leads to lower model selection accuracy, regardless of sample size, noise level, feature dimensionality, or multicollinearity, relative to regression. While principal component analysis and feature reweighting mitigate RSA's deficits driven by multicollinearity, regression remains superior in accurately distinguishing between models. Empirical data and a follow-up fMRI simulation further support these conclusions. Our findings suggest that researchers should carefully consider which approach to use: RSA is less effective than linear regression for model selection and fitting when direct stimulus-response mappings are available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00395v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chuanji Gao, Gang Chen, Svetlana V. Shinkareva, Rutvik H. Desai</dc:creator>
    </item>
    <item>
      <title>A Simple and Effective Random Forest Modelling for Nonlinear Time Series Data</title>
      <link>https://arxiv.org/abs/2511.06544</link>
      <description>arXiv:2511.06544v2 Announce Type: replace 
Abstract: In this paper, we propose Random Forests by Random Weights (RF-RW), a theoretically grounded and practically effective alternative RF modelling for nonlinear time series data, where existing RF-based approaches struggle to adequately capture temporal dependence. RF-RW reconciles the strengths of classic RF with the temporal dependence inherent in time series forecasting. Specifically, it avoids the bootstrap resampling procedure, therefore preserves the serial dependence structure, whilst incorporates independent random weights to reduce correlations among trees. We establish non-asymptotic concentration bounds and asymptotic uniform consistency guarantees, for both fixed- and high-dimensional feature spaces, which extend beyond existing theoretical analyses of RF. Extensive simulation studies demonstrate that RF-RW outperforms existing RF-based approaches and other benchmarks such as SVM and LSTM. It also achieves the lowest error among competitors in our real-data example of predicting UK COVID-19 daily cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06544v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shihao Zhang, Zudi Lu, Chao Zheng</dc:creator>
    </item>
    <item>
      <title>Achieving Fairness with a Simple Ridge Penalty</title>
      <link>https://arxiv.org/abs/2105.13817</link>
      <description>arXiv:2105.13817v4 Announce Type: replace-cross 
Abstract: In this paper we present a general framework for estimating regression models subject to a user-defined level of fairness. We enforce fairness as a model selection step in which we choose the value of a ridge penalty to control the effect of sensitive attributes. We then estimate the parameters of the model conditional on the chosen penalty value. Our proposal is mathematically simple, with a solution that is partly in closed form, and produces estimates of the regression coefficients that are intuitive to interpret as a function of the level of fairness. Furthermore, it is easily extended to generalised linear models, kernelised regression models and other penalties; and it can accommodate multiple definitions of fairness.
  We compare our approach with the regression model from Komiyama et al. (2018), which implements a provably-optimal linear regression model; and with the fair models from Zafar et al. (2019). We evaluate these approaches empirically on six different data sets, and we find that our proposal provides better goodness of fit and better predictive accuracy for the same level of fairness. In addition, we highlight a source of bias in the original experimental evaluation in Komiyama et al. (2018).</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.13817v4</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Statistics and Computing (2022), 32, 77</arxiv:journal_reference>
      <dc:creator>Marco Scutari, Francesca Panero, Manuel Proissl</dc:creator>
    </item>
    <item>
      <title>Foundations of Structural Causal Models with Latent Selection</title>
      <link>https://arxiv.org/abs/2401.06925</link>
      <description>arXiv:2401.06925v3 Announce Type: replace-cross 
Abstract: Three distinct phenomena complicate statistical causal analysis: latent common causes, causal cycles, and latent selection. Foundational works on Structural Causal Models (SCMs), e.g., Bongers et al. (2021, Ann. Stat., 49(5): 2885-2915), treat cycles and latent variables, while an analogous account of latent selection is missing. The goal of this article is to develop a theoretical foundation for modeling latent selection with SCMs. To achieve that, we introduce a conditioning operation for SCMs: it maps an SCM with explicit selection mechanisms to one without them while preserving the causal semantics of the selected subpopulation. Graphically, in Directed Mixed Graphs we extend bidirected edge--beyond latent common cause--to also encode latent selection. We prove that the conditioning operation preserves simplicity, acyclicity, and linearity of SCMs, and interacts well with marginalization, conditioning, and interventions. These properties make those three operations valuable tools for causal modeling, reasoning, and learning after abstracting away latent details (latent common causes and selection). Examples show how this abstraction streamlines analysis and clarifies when standard tools (e.g., adjustment, causal calculus, instrumental variables) remain valid under selection bias. We hope that these results deepen the SCM-based understanding of selection bias and become part of the standard causal modeling toolbox to build more reliable causal analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06925v3</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leihao Chen, Onno Zoeter, Joris M. Mooij</dc:creator>
    </item>
    <item>
      <title>Causality Pursuit from Heterogeneous Environments via Neural Adversarial Invariance Learning</title>
      <link>https://arxiv.org/abs/2405.04715</link>
      <description>arXiv:2405.04715v5 Announce Type: replace-cross 
Abstract: Pursuing causality from data is a fundamental problem in scientific discovery, treatment intervention, and transfer learning. This paper introduces a novel algorithmic method for addressing nonparametric invariance and causality learning in regression models across multiple environments, where the joint distribution of response variables and covariates varies, but the conditional expectations of outcome given an unknown set of quasi-causal variables are invariant. The challenge of finding such an unknown set of quasi-causal or invariant variables is compounded by the presence of endogenous variables that have heterogeneous effects across different environments. The proposed Focused Adversarial Invariant Regularization (FAIR) framework utilizes an innovative minimax optimization approach that drives regression models toward prediction-invariant solutions through adversarial testing. Leveraging the representation power of neural networks, FAIR neural networks (FAIR-NN) are introduced for causality pursuit. It is shown that FAIR-NN can find the invariant variables and quasi-causal variables under a minimal identification condition and that the resulting procedure is adaptive to low-dimensional composition structures in a non-asymptotic analysis. Under a structural causal model, variables identified by FAIR-NN represent pragmatic causality and provably align with exact causal mechanisms under conditions of sufficient heterogeneity. Computationally, FAIR-NN employs a novel Gumbel approximation with decreased temperature and a stochastic gradient descent ascent algorithm. The procedures are demonstrated using simulated and real-data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04715v5</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Annals of Statistics, 53(5): 2230-2257, 2025</arxiv:journal_reference>
      <dc:creator>Yihong Gu, Cong Fang, Peter B\"uhlmann, Jianqing Fan</dc:creator>
    </item>
    <item>
      <title>Adaptive Estimation of Multivariate Binary Distributions under Sparse Generalized Correlation Structures</title>
      <link>https://arxiv.org/abs/2410.15166</link>
      <description>arXiv:2410.15166v5 Announce Type: replace-cross 
Abstract: We consider the problem of estimating the joint distribution of an $M$-dimensional binary vector, which involves exponentially many parameters without additional assumptions. Using the representation from \citet{bahadur1959representation}, we relate the sparsity of its parameters to conditional independence among components. The maximum likelihood estimator is computationally infeasible and prone to overfitting. {We reformulate the problem as estimating a high-dimensional vector of generalized correlation coefficients, quantifying interaction effects among all component subsets, together with low or moderate-dimensional nuisance parameters corresponding to the marginal probabilities.} Since the marginal probabilities can be consistently estimated, we first propose a two-stage procedure that first estimates the marginal probabilities and then applies an $\ell_1$-regularized estimator for the generalized correlations, exploiting sparsity arising from potential independence structures. While computationally efficient, this estimator is not rate-optimal. We therefore further develop a regularized adversarial estimator that attains the optimal rate under standard regularity conditions while remaining tractable. The framework naturally extends to settings with covariates. We apply the proposed estimators to causal inference with multiple binary treatments and demonstrate substantial finite-sample improvements over non-adaptive estimators that estimate all probabilities directly. Simulation studies corroborate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15166v5</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandre Belloni, Yan Chen, Matthew Harding</dc:creator>
    </item>
    <item>
      <title>Linear Regressions with Combined Data</title>
      <link>https://arxiv.org/abs/2412.04816</link>
      <description>arXiv:2412.04816v2 Announce Type: replace-cross 
Abstract: We study linear regressions in a context where the outcome of interest and some of the covariates are observed in two different datasets that cannot be matched. Traditional approaches obtain point identification by relying, often implicitly, on exclusion restrictions. We show that without such restrictions, coefficients of interest can still be partially identified, with the sharp bounds taking a simple form. We obtain tighter bounds when variables observed in both datasets, but not included in the regression of interest, are available, even if these variables are not subject to specific restrictions. We develop computationally simple and asymptotically normal estimators of the bounds. Finally, we apply our methodology to estimate racial disparities in patent approval rates and to evaluate the effect of patience and risk-taking on educational performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04816v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xavier D'Haultfoeuille, Christophe Gaillac, Arnaud Maurel</dc:creator>
    </item>
    <item>
      <title>CAD-VAE: Leveraging Correlation-Aware Latents for Comprehensive Fair Disentanglement</title>
      <link>https://arxiv.org/abs/2503.07938</link>
      <description>arXiv:2503.07938v2 Announce Type: replace-cross 
Abstract: While deep generative models have significantly advanced representation learning, they may inherit or amplify biases and fairness issues by encoding sensitive attributes alongside predictive features. Enforcing strict independence in disentanglement is often unrealistic when target and sensitive factors are naturally correlated. To address this challenge, we propose \textbf{CAD-VAE} (\textbf{C}orrelation-\textbf{A}ware \textbf{D}isentangled \textbf{VAE}), which introduces a correlated latent code to capture the information shared between the target and sensitive attributes. Given this correlated latent, our method effectively separates overlapping factors without extra domain knowledge by directly minimizing the conditional mutual information between target and sensitive codes. A relevance-driven optimization strategy refines the correlated code by efficiently capturing essential correlated features and eliminating redundancy. Extensive experiments on benchmark datasets demonstrate that CAD-VAE produces fairer representations, realistic counterfactuals, and improved fairness-aware image editing. Source code is available : https://github.com/merry7cherry/CAD-VAE</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07938v2</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenrui Ma, Xi Xiao, Tianyang Wang, Xiao Wang, Yanning Shen</dc:creator>
    </item>
    <item>
      <title>Surrogate Modeling and Explainable Artificial Intelligence for Complex Systems: A Workflow for Automated Simulation Exploration</title>
      <link>https://arxiv.org/abs/2510.16742</link>
      <description>arXiv:2510.16742v2 Announce Type: replace-cross 
Abstract: Complex systems are increasingly explored through simulation-driven engineering workflows that combine physics-based and empirical models with optimization and analytics. Despite their power, these workflows face two central obstacles: (1) high computational cost, since accurate exploration requires many expensive simulator runs; and (2) limited transparency and reliability when decisions rely on opaque blackbox components. We propose a workflow that addresses both challenges by training lightweight emulators on compact designs of experiments that (i) provide fast, low-latency approximations of expensive simulators, (ii) enable rigorous uncertainty quantification, and (iii) are adapted for global and local Explainable Artificial Intelligence (XAI) analyses. This workflow unifies every simulation-based complex-system analysis tool, ranging from engineering design to agent-based models for socio-environmental understanding. In this paper, we proposea comparative methodology and practical recommendations for using surrogate-based explainability tools within the proposed workflow. The methodology supports continuous and categorical inputs, combines global-effect and uncertainty analyses with local attribution, and evaluates the consistency of explanations across surrogate models, thereby diagnosing surrogate adequacy and guiding further data collection or model refinement. We demonstrate the approach on two contrasting case studies: a multidisciplinary design analysis of a hybrid-electric aircraft and an agent-based model of urban segregation. Results show that the surrogate model and XAI coupling enables large-scale exploration in seconds, uncovers nonlinear interactions and emergent behaviors, identifies key design and policy levers, and signals regions where surrogates require more data or alternative architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16742v2</guid>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Saves, Pramudita Satria Palar, Muhammad Daffa Robani, Nicolas Verstaevel, Moncef Garouani, Julien Aligon, Benoit Gaudou, Koji Shimoyama, Joseph Morlier</dc:creator>
    </item>
    <item>
      <title>Bridging Constraints and Stochasticity: A Fully First-Order Method for Stochastic Bilevel Optimization with Linear Constraints</title>
      <link>https://arxiv.org/abs/2511.09845</link>
      <description>arXiv:2511.09845v2 Announce Type: replace-cross 
Abstract: This work provides the first finite-time convergence guarantees for linearly constrained stochastic bilevel optimization using only first-order methods, requiring solely gradient information without any Hessian computations or second-order derivatives. We address the unprecedented challenge of simultaneously handling linear constraints, stochastic noise, and finite-time analysis in bilevel optimization, a combination that has remained theoretically intractable until now. While existing approaches either require second-order information, handle only unconstrained stochastic problems, or provide merely asymptotic convergence results, our method achieves finite-time guarantees using gradient-based techniques alone. We develop a novel framework that constructs hypergradient approximations via smoothed penalty functions, using approximate primal and dual solutions to overcome the fundamental challenges posed by the interaction between linear constraints and stochastic noise. Our theoretical analysis provides explicit finite-time bounds on the bias and variance of the hypergradient estimator, demonstrating how approximation errors interact with stochastic perturbations. We prove that our first-order algorithm converges to $(\delta, \epsilon)$-Goldstein stationary points using $\Theta(\delta^{-1}\epsilon^{-5})$ stochastic gradient evaluations, establishing the first finite-time complexity result for this challenging problem class and representing a significant theoretical breakthrough in constrained stochastic bilevel optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09845v2</guid>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cac Phan, Kai Wang</dc:creator>
    </item>
    <item>
      <title>Online Price Competition under Generalized Linear Demands</title>
      <link>https://arxiv.org/abs/2511.10718</link>
      <description>arXiv:2511.10718v2 Announce Type: replace-cross 
Abstract: We study sequential price competition among $N$ sellers, each influenced by the pricing decisions of their rivals. Specifically, the demand function for each seller $i$ follows the single index model $\lambda_i(\mathbf{p}) = \mu_i(\langle \boldsymbol{\theta}_{i,0}, \mathbf{p} \rangle)$, with known increasing link $\mu_i$ and unknown parameter $\boldsymbol{\theta}_{i,0}$, where the vector $\mathbf{p}$ denotes the vector of prices offered by all the sellers simultaneously at a given instant. Each seller observes only their own realized demand -- unobservable to competitors -- and the prices set by rivals. Our framework generalizes existing approaches that focus solely on linear demand models. We propose a novel decentralized policy, PML-GLUCB, that combines penalized MLE with an upper-confidence pricing rule, removing the need for coordinated exploration phases across sellers -- which is integral to previous linear models -- and accommodating both binary and real-valued demand observations. Relative to a dynamic benchmark policy, each seller achieves $O(N^{2}\sqrt{T}\log(T))$ regret, which essentially matches the optimal rate known in the linear setting. A significant technical contribution of our work is the development of a variant of the elliptical potential lemma -- typically applied in single-agent systems -- adapted to our competitive multi-agent environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10718v2</guid>
      <category>cs.GT</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Bracale, Moulinath Banerjee, Cong Shi, Yuekai Sun</dc:creator>
    </item>
  </channel>
</rss>
