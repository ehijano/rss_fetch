<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 04 Mar 2024 05:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 04 Mar 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Warped Kernel Estimator for I.I.D. Paths of Diffusion Processes</title>
      <link>https://arxiv.org/abs/2403.00186</link>
      <description>arXiv:2403.00186v1 Announce Type: new 
Abstract: This paper deals with a nonparametric warped kernel estimator $\widehat b$ of the drift function computed from independent continuous observations of a diffusion process. A risk bound on $\widehat b$ is established. The paper also deals with an extension of the PCO bandwidth selection method for $\widehat b$. Finally, some numerical experiments are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00186v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Marie, Am\'elie Rosier</dc:creator>
    </item>
    <item>
      <title>Substitute adjustment via recovery of latent variables</title>
      <link>https://arxiv.org/abs/2403.00202</link>
      <description>arXiv:2403.00202v1 Announce Type: new 
Abstract: The deconfounder was proposed as a method for estimating causal parameters in a context with multiple causes and unobserved confounding. It is based on recovery of a latent variable from the observed causes. We disentangle the causal interpretation from the statistical estimation problem and show that the deconfounder in general estimates adjusted regression target parameters. It does so by outcome regression adjusted for the recovered latent variable termed the substitute. We refer to the general algorithm, stripped of causal assumptions, as substitute adjustment. We give theoretical results to support that substitute adjustment estimates adjusted regression parameters when the regressors are conditionally independent given the latent variable. We also introduce a variant of our substitute adjustment algorithm that estimates an assumption-lean target parameter with minimal model assumptions. We then give finite sample bounds and asymptotic results supporting substitute adjustment estimation in the case where the latent variable takes values in a finite set. A simulation study illustrates finite sample properties of substitute adjustment. Our results support that when the latent variable model of the regressors hold, substitute adjustment is a viable method for adjusted regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00202v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeffrey Adams, Niels Richard Hansen</dc:creator>
    </item>
    <item>
      <title>Inferences for Random Graphs Evolved by Clustering Attachment</title>
      <link>https://arxiv.org/abs/2403.00551</link>
      <description>arXiv:2403.00551v1 Announce Type: new 
Abstract: The evolution of random undirected graphs by the clustering attachment (CA) both without node and edge deletion and with uniform node or edge deletion is investigated. Theoretical results are obtained for the CA without node and edge deletion when a newly appended node is connected to two existing nodes of the graph at each evolution step. Theoretical results concern to (1) the sequence of increments of the consecutive mean clustering coefficients tends to zero; (2) the sequences of node degrees and triangle counts of any fixed node which are proved to be submartingales. These results were obtained for any initial graph. The simulation study is provided for the CA with uniform node or edge deletion and without any deletion. It is shown that (1) the CA leads to light-tailed distributed node degrees and triangle counts; (2) the average clustering coefficient tends to a constant over time; (3) the mean node degree and the mean triangle count increase over time with the rate depending on the parameters of the CA. The exposition is accompanied by a real data study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00551v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Natalia Markovich, Maksim Ryzhov, Marijus Vai\v{c}iulis</dc:creator>
    </item>
    <item>
      <title>On the signature of an image</title>
      <link>https://arxiv.org/abs/2403.00130</link>
      <description>arXiv:2403.00130v1 Announce Type: cross 
Abstract: Over the past decade, the importance of the 1D signature which can be seen as a functional defined along a path, has been pivotal in both path-wise stochastic calculus and the analysis of time series data. By considering an image as a two-parameter function that takes values in a $d$-dimensional space, we introduce an extension of the path signature to images. We address numerous challenges associated with this extension and demonstrate that the 2D signature satisfies a version of Chen's relation in addition to a shuffle-type product. Furthermore, we show that specific variations of the 2D signature can be recursively defined, thereby satisfying an integral-type equation. We analyze the properties of the proposed signature, such as continuity, invariance to stretching, translation and rotation of the underlying image. Additionally, we establish that the proposed 2D signature over an image satisfies a universal approximation property.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00130v1</guid>
      <category>math.CA</category>
      <category>math.AP</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joscha Diehl, Kurusch Ebrahimi-Fard, Fabian Harang, Samy Tindel</dc:creator>
    </item>
    <item>
      <title>Estimating the linear relation between variables that are never jointly observed: an application in in vivo experiments</title>
      <link>https://arxiv.org/abs/2403.00140</link>
      <description>arXiv:2403.00140v1 Announce Type: cross 
Abstract: This work is motivated by in vivo experiments in which measurement are destructive so that the variables of interest can never be observed simultaneously when the aim is to estimate the regression coefficients of a linear regression. Assuming that the global experiment can be decomposed into sub experiments (corresponding for example to different doses) with distinct first moments, we propose different estimators of the linear regression which take account of that additional information. We consider estimators based on moments as well as estimators based optimal transport theory. These estimators are proved to be consistent as well as asymptotically Gaussian under weak hypotheses. The asymptotic variance has no explicit expression, except in some particular cases, and specific bootstrap approaches are developed to build confidence intervals for the estimated parameter. A Monte Carlo study is conducted to assess and compare the finite sample performances of the different approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00140v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Polina Arsenteva, Mohamed Amine Benadjaoud, Herv\'e Cardot</dc:creator>
    </item>
    <item>
      <title>Shifted Interpolation for Differential Privacy</title>
      <link>https://arxiv.org/abs/2403.00278</link>
      <description>arXiv:2403.00278v1 Announce Type: cross 
Abstract: Noisy gradient descent and its variants are the predominant algorithms for differentially private machine learning. It is a fundamental question to quantify their privacy leakage, yet tight characterizations remain open even in the foundational setting of convex losses. This paper improves over previous analyses by establishing (and refining) the "privacy amplification by iteration" phenomenon in the unifying framework of $f$-differential privacy--which tightly captures all aspects of the privacy loss and immediately implies tighter privacy accounting in other notions of differential privacy, e.g., $(\varepsilon,\delta)$-DP and Renyi DP. Our key technical insight is the construction of shifted interpolated processes that unravel the popular shifted-divergences argument, enabling generalizations beyond divergence-based relaxations of DP. Notably, this leads to the first exact privacy analysis in the foundational setting of strongly convex optimization. Our techniques extend to many settings: convex/strongly convex, constrained/unconstrained, full/cyclic/stochastic batches, and all combinations thereof. As an immediate corollary, we recover the $f$-DP characterization of the exponential mechanism for strongly convex optimization in Gopi et al. (2022), and moreover extend this result to more general settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00278v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinho Bok, Weijie Su, Jason M. Altschuler</dc:creator>
    </item>
    <item>
      <title>Wavelet Based Periodic Autoregressive Moving Average Models</title>
      <link>https://arxiv.org/abs/2403.00281</link>
      <description>arXiv:2403.00281v1 Announce Type: cross 
Abstract: This paper proposes a wavelet-based method for analysing periodic autoregressive moving average (PARMA) time series. Even though Fourier analysis provides an effective method for analysing periodic time series, it requires the estimation of a large number of Fourier parameters when the PARMA parameters do not vary smoothly. The wavelet-based analysis helps us to obtain a parsimonious model with a reduced number of parameters. We have illustrated this with simulated and actual data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00281v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rhea Davis, N. Balakrishna</dc:creator>
    </item>
    <item>
      <title>Coherent forecasting of NoGeAR(1) model</title>
      <link>https://arxiv.org/abs/2403.00304</link>
      <description>arXiv:2403.00304v1 Announce Type: cross 
Abstract: This article focuses on the coherent forecasting of the recently introduced novel geometric AR(1) (NoGeAR(1)) model - an INAR model based on inflated - parameter binomial thinning approach. Various techniques are available to achieve h - step ahead coherent forecasts of count time series, like median and mode forecasting. However, there needs to be more body of literature addressing coherent forecasting in the context of overdispersed count time series. Here, we study the forecasting distribution corresponding to NoGeAR(1) process using the Monte Carlo (MC) approximation method. Accordingly, several forecasting measures are employed in the simulation study to facilitate a thorough comparison of the forecasting capability of NoGeAR(1) with other models. The methodology is also demonstrated using real-life data, specifically the data on CW{\ss} TeXpert downloads and Barbados COVID-19 data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00304v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Divya Kuttenchalil Andrews, N. Balakrishna</dc:creator>
    </item>
    <item>
      <title>Structurally Aware Robust Model Selection for Mixtures</title>
      <link>https://arxiv.org/abs/2403.00687</link>
      <description>arXiv:2403.00687v1 Announce Type: cross 
Abstract: Mixture models are often used to identify meaningful subpopulations (i.e., clusters) in observed data such that the subpopulations have a real-world interpretation (e.g., as cell types). However, when used for subpopulation discovery, mixture model inference is usually ill-defined a priori because the assumed observation model is only an approximation to the true data-generating process. Thus, as the number of observations increases, rather than obtaining better inferences, the opposite occurs: the data is explained by adding spurious subpopulations that compensate for the shortcomings of the observation model. However, there are two important sources of prior knowledge that we can exploit to obtain well-defined results no matter the dataset size: known causal structure (e.g., knowing that the latent subpopulations cause the observed signal but not vice-versa) and a rough sense of how wrong the observation model is (e.g., based on small amounts of expert-labeled data or some understanding of the data-generating process). We propose a new model selection criteria that, while model-based, uses this available knowledge to obtain mixture model inferences that are robust to misspecification of the observation model. We provide theoretical support for our approach by proving a first-of-its-kind consistency result under intuitive assumptions. Simulation studies and an application to flow cytometry data demonstrate our model selection criteria consistently finds the correct number of subpopulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00687v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawei Li, Jonathan H. Huggins</dc:creator>
    </item>
    <item>
      <title>Shrinkage estimators in zero-inflated Bell regression model with application</title>
      <link>https://arxiv.org/abs/2403.00749</link>
      <description>arXiv:2403.00749v1 Announce Type: cross 
Abstract: We propose Stein-type estimators for zero-inflated Bell regression models by incorporating information on model parameters. These estimators combine the advantages of unrestricted and restricted estimators. We derive the asymptotic distributional properties, including bias and mean squared error, for the proposed shrinkage estimators. Monte Carlo simulations demonstrate the superior performance of our shrinkage estimators across various scenarios. Furthermore, we apply the proposed estimators to analyze a real dataset, showcasing their practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00749v1</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Solmaz Seifollahi, Hossein Bevrani, Zakariya Yahya Algamal</dc:creator>
    </item>
    <item>
      <title>Malliavin calculus for the optimal estimation of the invariant density of discretely observed diffusions in intermediate regime</title>
      <link>https://arxiv.org/abs/2208.03253</link>
      <description>arXiv:2208.03253v3 Announce Type: replace 
Abstract: Let $(X_t)_{t \ge 0}$ be solution of a one-dimensional stochastic differential equation. Our aim is to study the convergence rate for the estimation of the invariant density in intermediate regime, assuming that a discrete observation of the process $(X_t)_{t \in [0, T]}$ is available, when $T$ tends to $\infty$. We find the convergence rates associated to the kernel density estimator we proposed and a condition on the discretization step $\Delta_n$ which plays the role of threshold between the intermediate regime and the continuous case. In intermediate regime the convergence rate is $n^{- \frac{2 \beta}{2 \beta + 1}}$, where $\beta$ is the smoothness of the invariant density. After that, we complement the upper bounds previously found with a lower bound over the set of all the possible estimator, which provides the same convergence rate: it means it is not possible to propose a different estimator which achieves better convergence rates. This is obtained by the two hypotheses method; the most challenging part consists in bounding the Hellinger distance between the laws of the two models. The key point is a Malliavin representation for a score function, which allows us to bound the Hellinger distance through a quantity depending on the Malliavin weight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.03253v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Amorino, Arnaud Gloter</dc:creator>
    </item>
    <item>
      <title>Testing mean and variance by e-processes</title>
      <link>https://arxiv.org/abs/2301.12480</link>
      <description>arXiv:2301.12480v4 Announce Type: replace 
Abstract: We address the problem of testing conditional mean and conditional variance for non-stationary data. We build e-values and p-values for four types of non-parametric composite hypotheses with specified mean and variance as well as other conditions on the shape of the data-generating distribution. These shape conditions include symmetry, unimodality, and their combination. Using the obtained e-values and p-values, we construct tests via e-processes, also known as testing by betting, as well as some tests based on combining p-values for comparison. Although we mainly focus on one-sided tests, the two-sided test for the mean is also studied. Simulation and empirical studies are conducted under a few settings, and they illustrate features of the methods based on e-processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.12480v4</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yixuan Fan, Zhanyi Jiao, Ruodu Wang</dc:creator>
    </item>
    <item>
      <title>Exact variable selection in sparse nonparametric models</title>
      <link>https://arxiv.org/abs/2310.07677</link>
      <description>arXiv:2310.07677v2 Announce Type: replace 
Abstract: We study the problem of adaptive variable selection in a Gaussian white noise model of intensity $\varepsilon$ under certain sparsity and regularity conditions on an unknown regression function $f$. The $d$-variate regression function $f$ is assumed to be a sum of functions each depending on a smaller number $k$ of variables ($1 \leq k \leq d$). These functions are unknown to us and only few of them are nonzero. We assume that $d=d_\varepsilon \to \infty$ as $\varepsilon \to 0$ and consider the cases when $k$ is fixed and when $k=k_\varepsilon \to \infty$, $k=o(d)$ as $\varepsilon \to 0$. In this work, we introduce an adaptive selection procedure that, under some model assumptions, identifies exactly all nonzero $k$-variate components of $f$. In addition, we establish conditions under which exact identification of the nonzero components is impossible. These conditions ensure that the proposed selection procedure is the best possible in the asymptotically minimax sense with respect to the Hamming risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07677v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Natalia Stepanova, Marie Turcicova</dc:creator>
    </item>
    <item>
      <title>Spectral Ranking Inferences based on General Multiway Comparisons</title>
      <link>https://arxiv.org/abs/2308.02918</link>
      <description>arXiv:2308.02918v3 Announce Type: replace-cross 
Abstract: This paper studies the performance of the spectral method in the estimation and uncertainty quantification of the unobserved preference scores of compared entities in a general and more realistic setup. Specifically, the comparison graph consists of hyper-edges of possible heterogeneous sizes, and the number of comparisons can be as low as one for a given hyper-edge. Such a setting is pervasive in real applications, circumventing the need to specify the graph randomness and the restrictive homogeneous sampling assumption imposed in the commonly used Bradley-Terry-Luce (BTL) or Plackett-Luce (PL) models. Furthermore, in scenarios where the BTL or PL models are appropriate, we unravel the relationship between the spectral estimator and the Maximum Likelihood Estimator (MLE). We discover that a two-step spectral method, where we apply the optimal weighting estimated from the equal weighting vanilla spectral method, can achieve the same asymptotic efficiency as the MLE. Given the asymptotic distributions of the estimated preference scores, we also introduce a comprehensive framework to carry out both one-sample and two-sample ranking inferences, applicable to both fixed and random graph settings. It is noteworthy that this is the first time effective two-sample rank testing methods have been proposed. Finally, we substantiate our findings via comprehensive numerical simulations and subsequently apply our developed methodologies to perform statistical inferences for statistical journals and movie rankings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.02918v3</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianqing Fan, Zhipeng Lou, Weichen Wang, Mengxin Yu</dc:creator>
    </item>
    <item>
      <title>On Rate-Optimal Partitioning Classification from Observable and from Privatised Data</title>
      <link>https://arxiv.org/abs/2312.14889</link>
      <description>arXiv:2312.14889v2 Announce Type: replace-cross 
Abstract: In this paper we revisit the classical method of partitioning classification and study its convergence rate under relaxed conditions, both for observable (non-privatised) and for privatised data. Let the feature vector $X$ take values in $\mathbb{R}^d$ and denote its label by $Y$. Previous results on the partitioning classifier worked with the strong density assumption, which is restrictive, as we demonstrate through simple examples. We assume that the distribution of $X$ is a mixture of an absolutely continuous and a discrete distribution, such that the absolutely continuous component is concentrated to a $d_a$ dimensional subspace. Here, we study the problem under much milder assumptions: in addition to the standard Lipschitz and margin conditions, a novel characteristic of the absolutely continuous component is introduced, by which the exact convergence rate of the classification error probability is calculated, both for the binary and for the multi-label cases. Interestingly, this rate of convergence depends only on the intrinsic dimension $d_a$.
  The privacy constraints mean that the data $(X_1,Y_1), \dots ,(X_n,Y_n)$ cannot be directly observed, and the classifiers are functions of the randomised outcome of a suitable local differential privacy mechanism. The statistician is free to choose the form of this privacy mechanism, and here we add Laplace distributed noises to the discontinuations of all possible locations of the feature vector $X_i$ and to its label $Y_i$. Again, tight upper bounds on the rate of convergence of the classification error probability are derived, without the strong density assumption, such that this rate depends on $2\,d_a$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14889v2</guid>
      <category>stat.ML</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bal\'azs Csan\'ad Cs\'aji, L\'aszl\'o Gy\"orfi, Ambrus Tam\'as, Harro Walk</dc:creator>
    </item>
  </channel>
</rss>
