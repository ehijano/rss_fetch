<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 07 Nov 2024 05:04:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Near-Optimal and Tractable Estimation under Shift-Invariance</title>
      <link>https://arxiv.org/abs/2411.03383</link>
      <description>arXiv:2411.03383v1 Announce Type: new 
Abstract: How hard is it to estimate a discrete-time signal $(x_{1}, ..., x_{n}) \in \mathbb{C}^n$ satisfying an unknown linear recurrence relation of order $s$ and observed in i.i.d. complex Gaussian noise? The class of all such signals is parametric but extremely rich: it contains all exponential polynomials over $\mathbb{C}$ with total degree $s$, including harmonic oscillations with $s$ arbitrary frequencies. Geometrically, this class corresponds to the projection onto $\mathbb{C}^{n}$ of the union of all shift-invariant subspaces of $\mathbb{C}^\mathbb{Z}$ of dimension $s$. We show that the statistical complexity of this class, as measured by the squared minimax radius of the $(1-\delta)$-confidence $\ell_2$-ball, is nearly the same as for the class of $s$-sparse signals, namely $O\left(s\log(en) + \log(\delta^{-1})\right) \cdot \log^2(es) \cdot \log(en/s).$ Moreover, the corresponding near-minimax estimator is tractable, and it can be used to build a test statistic with a near-minimax detection threshold in the associated detection problem. These statistical results rest upon an approximation-theoretic one: we show that finite-dimensional shift-invariant subspaces admit compactly supported reproducing kernels whose Fourier spectra have nearly the smallest possible $\ell_p$-norms, for all $p \in [1,+\infty]$ at once.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03383v1</guid>
      <category>math.ST</category>
      <category>math.CA</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dmitrii M. Ostrovskii</dc:creator>
    </item>
    <item>
      <title>Asymptotic analysis of estimators of ergodic stochastic differential equations</title>
      <link>https://arxiv.org/abs/2411.03623</link>
      <description>arXiv:2411.03623v1 Announce Type: new 
Abstract: The paper studies asymptotic properties of estimators of multidimensional stochastic differential equations driven by Brownian motions from high-frequency discrete data. Consistency and central limit properties of a class of estimators of the diffusion parameter and an approximate maximum likelihood estimator of the drift parameter based on a discretized likelihood function have been established in a suitable scaling regime involving the time-gap between the observations and the overall time span. Our framework is more general than that typically considered in the literature and, thus, has the potential to be applicable to a wider range of stochastic models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03623v1</guid>
      <category>math.ST</category>
      <category>math.DS</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arnab Ganguly</dc:creator>
    </item>
    <item>
      <title>Stochastic orders and shape properties for a new distorted proportional odds model</title>
      <link>https://arxiv.org/abs/2411.03828</link>
      <description>arXiv:2411.03828v1 Announce Type: new 
Abstract: Building on recent developments in models focused on the shape properties of odds ratios, this paper introduces two new models that expand the class of available distributions while preserving specific shape characteristics of an underlying baseline distribution. The first model offers enhanced control over odds and logodds functions, facilitating adjustments to skewness, tail behavior, and hazard rates. The second model, with even greater flexibility, describes odds ratios as quantile distortions. This approach leads to an enlarged log-logistic family capable of capturing these quantile transformations and diverse hazard behaviors, including non-monotonic and bathtub-shaped rates. Central to our study are the shape relations described through stochastic orders; we establish conditions that ensure stochastic ordering both within each family and across models under various ordering concepts, such as hazard rate, likelihood ratio, and convex transform orders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03828v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Idir Arab, Milto Hadjikyriakou, Paulo Eduardo Oliveira</dc:creator>
    </item>
    <item>
      <title>Beyond Regularity: Simple versus Optimal Mechanisms, Revisited</title>
      <link>https://arxiv.org/abs/2411.03583</link>
      <description>arXiv:2411.03583v1 Announce Type: cross 
Abstract: A large proportion of the Bayesian mechanism design literature is restricted to the family of regular distributions $\mathbb{F}_{\tt reg}$ [Mye81] or the family of monotone hazard rate (MHR) distributions $\mathbb{F}_{\tt MHR}$ [BMP63], which overshadows this beautiful and well-developed theory. We (re-)introduce two generalizations, the family of quasi-regular distributions $\mathbb{F}_{\tt Q-reg}$ and the family of quasi-MHR distributions $\mathbb{F}_{\tt Q-MHR}$. All four families together form the following hierarchy: $\mathbb{F}_{\tt MHR} \subsetneq (\mathbb{F}_{\tt reg} \cap \mathbb{F}_{\tt Q-MHR}) \subsetneq \mathbb{F}_{\tt Q-reg}$ and $\mathbb{F}_{\tt Q-MHR} \subsetneq (\mathbb{F}_{\tt reg} \cup \mathbb{F}_{\tt Q-MHR}) \subsetneq \mathbb{F}_{\tt Q-reg}$.
  The significance of our new families is manifold. First, their defining conditions are immediate relaxations of the regularity/MHR conditions (i.e., monotonicity of the virtual value functions and/or the hazard rate functions), which reflect economic intuition. Second, they satisfy natural mathematical properties (about order statistics) that are violated by both original families $\mathbb{F}_{\tt reg}$ and $\mathbb{F}_{\tt MHR}$. Third but foremost, numerous results [BK96, HR09a, CD15, DRY15, HR14, AHN+19, JLTX20, JLQ+19b, FLR19, GHZ19b, JLX23, LM24] established before for regular/MHR distributions now can be generalized, with or even without quantitative losses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03583v1</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiding Feng, Yaonan Jin</dc:creator>
    </item>
    <item>
      <title>Asymptotically optimal Wasserstein couplings for the small-time stable domain of attraction</title>
      <link>https://arxiv.org/abs/2411.03609</link>
      <description>arXiv:2411.03609v1 Announce Type: cross 
Abstract: We develop two novel couplings between general pure-jump L\'evy processes in $\R^d$ and apply them to obtain upper bounds on the rate of convergence in an appropriate Wasserstein distance on the path space for a wide class of L\'evy processes attracted to a multidimensional stable process in the small-time regime. We also establish general lower bounds based on certain universal properties of slowly varying functions and the relationship between the Wasserstein and Toscani--Fourier distances of the marginals. Our upper and lower bounds typically have matching rates. In particular, the rate of convergence is polynomial for the domain of normal attraction and slower than a slowly varying function for the domain of non-normal attraction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03609v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jorge Gonz\'alez C\'azares, David Kramer-Bang, Aleksandar Mijatovi\'c</dc:creator>
    </item>
    <item>
      <title>The Optimality of Blocking Designs in Equally and Unequally Allocated Randomized Experiments with General Response</title>
      <link>https://arxiv.org/abs/2212.01887</link>
      <description>arXiv:2212.01887v3 Announce Type: replace 
Abstract: We consider the performance of the difference-in-means estimator in a two-arm randomized experiment under common experimental endpoints such as continuous (regression), incidence, proportion and survival. We examine performance under both equal and unequal allocation to treatment groups and we consider both the Neyman randomization model and the population model. We show that in the Neyman model, where the only source of randomness is the treatment manipulation, there is no free lunch: complete randomization is minimax for the estimator's mean squared error. In the population model, where each subject experiences response noise with zero mean, the optimal design is the deterministic perfect-balance allocation. However, this allocation is generally NP-hard to compute and moreover, depends on unknown response parameters. When considering the tail criterion of Kapelner et al. (2021), we show the optimal design is less random than complete randomization and more random than the deterministic perfect-balance allocation. We prove that Fisher's blocking design provides the asymptotically optimal degree of experimental randomness. Theoretical results are supported by simulations in all considered experimental settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.01887v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Azriel, Abba M. Krieger, Adam Kapelner</dc:creator>
    </item>
    <item>
      <title>Nonparametric Evaluation of Noisy ICA Solutions</title>
      <link>https://arxiv.org/abs/2401.08468</link>
      <description>arXiv:2401.08468v3 Announce Type: replace 
Abstract: Independent Component Analysis (ICA) was introduced in the 1980's as a model for Blind Source Separation (BSS), which refers to the process of recovering the sources underlying a mixture of signals, with little knowledge about the source signals or the mixing process. While there are many sophisticated algorithms for estimation, different methods have different shortcomings. In this paper, we develop a nonparametric score to adaptively pick the right algorithm for ICA with arbitrary Gaussian noise. The novelty of this score stems from the fact that it just assumes a finite second moment of the data and uses the characteristic function to evaluate the quality of the estimated mixing matrix without any knowledge of the parameters of the noise distribution. In addition, we propose some new contrast functions and algorithms that enjoy the same fast computability as existing algorithms like FASTICA and JADE but work in domains where the former may fail. While these also may have weaknesses, our proposed diagnostic, as shown by our simulations, can remedy them. Finally, we propose a theoretical framework to analyze the local and global convergence properties of our algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.08468v3</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Syamantak Kumar, Purnamrita Sarkar, Peter Bickel, Derek Bean</dc:creator>
    </item>
    <item>
      <title>Series ridge regression for spatial data on $\mathbb{R}^d$</title>
      <link>https://arxiv.org/abs/2402.02773</link>
      <description>arXiv:2402.02773v5 Announce Type: replace 
Abstract: This paper develops a general asymptotic theory of series estimators for spatial data collected at irregularly spaced locations within a sampling region $R_n \subset \mathbb{R}^d$. We employ a stochastic sampling design that can flexibly generate irregularly spaced sampling sites, encompassing both pure increasing and mixed increasing domain frameworks. Specifically, we focus on a spatial trend regression model and a nonparametric regression model with spatially dependent covariates. For these models, we investigate $L^2$-penalized series estimation of the trend and regression functions. We establish uniform and $L^2$ convergence rates and multivariate central limit theorems for general series estimators as main results. Additionally, we show that spline and wavelet series estimators achieve optimal uniform and $L^2$ convergence rates and propose methods for constructing confidence intervals for these estimators. Finally, we demonstrate that our dependence structure conditions on the underlying spatial processes include a broad class of random fields, including L\'evy-driven continuous autoregressive and moving average random fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02773v5</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daisuke Kurisu, Yasumasa Matsuda</dc:creator>
    </item>
    <item>
      <title>Oja's Algorithm for Streaming Sparse PCA</title>
      <link>https://arxiv.org/abs/2402.07240</link>
      <description>arXiv:2402.07240v4 Announce Type: replace 
Abstract: Oja's algorithm for Streaming Principal Component Analysis (PCA) for $n$ data-points in a $d$ dimensional space achieves the same sin-squared error $O(r_{\mathsf{eff}}/n)$ as the offline algorithm in $O(d)$ space and $O(nd)$ time and a single pass through the datapoints. Here $r_{\mathsf{eff}}$ is the effective rank (ratio of the trace and the principal eigenvalue of the population covariance matrix $\Sigma$). Under this computational budget, we consider the problem of sparse PCA, where the principal eigenvector of $\Sigma$ is $s$-sparse, and $r_{\mathsf{eff}}$ can be large. In this setting, to our knowledge, \textit{there are no known single-pass algorithms} that achieve the minimax error bound in $O(d)$ space and $O(nd)$ time without either requiring strong initialization conditions or assuming further structure (e.g., spiked) of the covariance matrix. We show that a simple single-pass procedure that thresholds the output of Oja's algorithm (the Oja vector) can achieve the minimax error bound under some regularity conditions in $O(d)$ space and $O(nd)$ time. We present a nontrivial and novel analysis of the entries of the unnormalized Oja vector, which involves the projection of a product of independent random matrices on a random initial vector. This is completely different from previous analyses of Oja's algorithm and matrix products, which have been done when the $r_{\mathsf{eff}}$ is bounded.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07240v4</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Syamantak Kumar, Purnamrita Sarkar</dc:creator>
    </item>
    <item>
      <title>The German Tank Problem with Multiple Factories</title>
      <link>https://arxiv.org/abs/2403.14881</link>
      <description>arXiv:2403.14881v3 Announce Type: replace 
Abstract: During the Second World War, estimates of the number of tanks deployed by Germany were critically needed. The Allies adopted a successful statistical approach to estimate this information: assuming that the tanks are sequentially numbered starting from 1, if we observe $k$ tanks from an unknown total of $N$, then the best linear unbiased estimator for $N$ is $M(1+1/k)-1$ where $M$ is the maximum observed serial number. However, in many situations, the original German Tank Problem is insufficient, since typically there are $l&gt;1$ factories, and tanks produced by different factories may have serial numbers in disjoint ranges that are often far separated.
  Clark, Gonye and Miller presented an unbiased estimator for $N$ when the minimum serial number is unknown. Provided one identifies which samples correspond to which factory, one can then estimate each factory's range and summing the sizes of these ranges yields an estimate for the rival's total productivity. We construct an efficient procedure to estimate the total productivity and prove that it is effective when $\log l/\log k$ is sufficiently small. In the final section, we show that given information about the gaps, we can make an estimator that performs orders of magnitude better when we have a small number of samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14881v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Steven J. Miller, Kishan Sharma, Andrew K. Yang</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Cell Type Deconvolution</title>
      <link>https://arxiv.org/abs/2202.06420</link>
      <description>arXiv:2202.06420v4 Announce Type: replace-cross 
Abstract: Integrating data from different platforms, such as bulk and single-cell RNA sequencing, is crucial for improving the accuracy and interpretability of complex biological analyses like cell type deconvolution. However, this task is complicated by measurement and biological heterogeneity between target and reference datasets. For the problem of cell type deconvolution, existing methods often neglect the correlation and uncertainty in cell type proportion estimates, possibly leading to an additional concern of false positives in downstream comparisons across multiple individuals. We introduce MEAD, a comprehensive statistical framework that not only estimates cell type proportions but also provides asymptotically valid statistical inference on the estimates. One of our key contributions is the identifiability result, which rigorously establishes the conditions under which cell type proportions are identifiable despite arbitrary heterogeneity of measurement biases between platforms. MEAD also supports the comparison of cell type proportions across individuals after deconvolution, accounting for gene-gene correlations and biological variability. Through simulations and real-data analysis, MEAD demonstrates superior reliability for inferring cell type compositions in complex biological systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.06420v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongyue Xie, Jingshu Wang</dc:creator>
    </item>
    <item>
      <title>Mind the spikes: Benign overfitting of kernels and neural networks in fixed dimension</title>
      <link>https://arxiv.org/abs/2305.14077</link>
      <description>arXiv:2305.14077v3 Announce Type: replace-cross 
Abstract: The success of over-parameterized neural networks trained to near-zero training error has caused great interest in the phenomenon of benign overfitting, where estimators are statistically consistent even though they interpolate noisy training data. While benign overfitting in fixed dimension has been established for some learning methods, current literature suggests that for regression with typical kernel methods and wide neural networks, benign overfitting requires a high-dimensional setting where the dimension grows with the sample size. In this paper, we show that the smoothness of the estimators, and not the dimension, is the key: benign overfitting is possible if and only if the estimator's derivatives are large enough. We generalize existing inconsistency results to non-interpolating models and more kernels to show that benign overfitting with moderate derivatives is impossible in fixed dimension. Conversely, we show that rate-optimal benign overfitting is possible for regression with a sequence of spiky-smooth kernels with large derivatives. Using neural tangent kernels, we translate our results to wide neural networks. We prove that while infinite-width networks do not overfit benignly with the ReLU activation, this can be fixed by adding small high-frequency fluctuations to the activation function. Our experiments verify that such neural networks, while overfitting, can indeed generalize well even on low-dimensional data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.14077v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moritz Haas, David Holzm\"uller, Ulrike von Luxburg, Ingo Steinwart</dc:creator>
    </item>
  </channel>
</rss>
