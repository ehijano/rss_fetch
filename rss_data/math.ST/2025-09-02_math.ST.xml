<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 03 Sep 2025 04:00:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A note on the asymptotic distribution of the Likelihood Ratio Test statistic under boundary conditions</title>
      <link>https://arxiv.org/abs/2509.00223</link>
      <description>arXiv:2509.00223v1 Announce Type: new 
Abstract: In the context of likelihood ratio testing with parameters on the boundary, we revisit two situations for which there are some discrepancies in the literature: the case of two parameters of interest on the boundary, with all other parameters in the interior, and the case where one of the two parameters on the boundary is a nuisance parameter. For the former case, we clarify that two seemingly conflicting results are consistent upon closer examination. For the latter, we clarify the source of the discrepancy and explain the different findings. As for this case the closed-form expression is valid only under positive correlation, we further propose a heuristic modification to the asymptotic distribution of the likelihood ratio test that extends its applicability to cases involving negative correlation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00223v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Clara Bertinelli Salucci, Anders Kvellestad, Riccardo De Bin</dc:creator>
    </item>
    <item>
      <title>Signal Lasso with Non-Convex Penalties for Efficient Network Reconstruction and Topology Inference</title>
      <link>https://arxiv.org/abs/2509.00342</link>
      <description>arXiv:2509.00342v1 Announce Type: new 
Abstract: Inferring network structures remains an interesting question for its importance on the understanding and controlling collective dynamics of complex systems. The existing shrinking methods such as Lasso-type estimation can not suitably reveal such property. A new method recently suggested, called by {\it signal lasso} (or its updating version: adaptive signal lasso) was proposed to solve the network reconstruction problem, where the signal parameter can be shrunk to either 0 or 1 in two different directions. The signal lasso or adaptive signal lasso employed the additive penalty of signal and non-signal terms which is a convex function and easily to complementation in computation. However their methods need tuning the one or two parameters to find an optimal solution, which is time cost for large size network. In this paper we propose new signal lasso method based on two penalty functions to estimate the signal parameter and uncovering network topology in complex network with a small amount of observations. The penalty functions we introduced are non-convex function, thus coordinate descent algorithms are suggested. We find in this method the tuning parameter can be set to a large enough values such that the signal parameter can be completely shrunk either 0 or 1. The extensive simulations are conducted in linear regression models with different assumptions, the evolutionary-game-based dynamic model and Kuramoto model of synchronization problem. The advantage and disadvantage of each method are fully discussed in various conditions. Finally a real example comes from behavioral experiment is used for illustration. Our results show that signal lasso with non-convex penalties is effective and fast in estimating signal parameters in linear regression model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00342v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lei Shi, Jie Hu, Huaiyu Tan, Libin Jin, Wei Zhong, Chen Shen</dc:creator>
    </item>
    <item>
      <title>Logarithmic Accuracy in Importance Sampling via Large Deviations</title>
      <link>https://arxiv.org/abs/2509.00343</link>
      <description>arXiv:2509.00343v1 Announce Type: new 
Abstract: Importance sampling (IS) is a widely used simulation method for estimating rare event probabilities. In IS, the relative variance of an estimator is the most common measure of estimator accuracy, and the focus of existing literature is on constructing an importance measure under which the relative variance of the estimator grows sub-exponentially as the parameter increases. In practice, constructing such an estimator is not easy. In this work, we study the behavior of IS estimators under an importance measure which is not necessarily optimal using large deviations theory. This provides new insights into asymptotic efficiency of IS estimators and the required sample size. Based on the study, we also propose new diagnostics of IS for rare event simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00343v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julie Choi, Peter Glynn</dc:creator>
    </item>
    <item>
      <title>Computing expectiles via fixed point iterations</title>
      <link>https://arxiv.org/abs/2509.00408</link>
      <description>arXiv:2509.00408v1 Announce Type: new 
Abstract: Expectiles are statistical parameters which also provide a class of sublinear risk measures in finance. They are solutions of continuous optimization problems. The corresponding first order condition provides two different fixed point characterizations for expectiles, both of which can be utilized for computing them. Although especially the so-called two-sided version is already implemented and widely used, a general convergence proof appears to be new.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00408v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thi Khanh Linh Ha, Andreas Heinrich Hamel, Daniel Kostner</dc:creator>
    </item>
    <item>
      <title>On a transform of the Vincze-statistic and its exact and asymptotic distribution</title>
      <link>https://arxiv.org/abs/2509.00452</link>
      <description>arXiv:2509.00452v1 Announce Type: new 
Abstract: We motivate a new nonparametric test for the one-sided two-sample problem, which is based on a transform T of the Vincze-statistic (R,D). The exact and asymptotic distribution of T is derived. The fundamental idea can also be applied to the two-sided problem. Here too, a limit theorem can be proved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00452v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dietmar Ferger</dc:creator>
    </item>
    <item>
      <title>Minimax testing in a statistical inverse problem with unknown operator</title>
      <link>https://arxiv.org/abs/2509.01567</link>
      <description>arXiv:2509.01567v1 Announce Type: new 
Abstract: We study minimax testing in a statistical inverse problem when the associated operator is unknown. In particular, we consider observations from an inverse Gaussian regression model where the associated operator is unknown but contained in a given dictionary B of finite cardinality. Using the non-asymptotic framework for minimax testing (that is, for any fixed value of the noise level), we provide optimal separation conditions for the goodness-of-fit testing problem. We restrict our attention to the specific case where the dictionary contains only two members. As we will demonstrate, even this simple case is quite intrigued and reveals an interesting phase transition phenomenon. The general case is even more involved, requires different strategies, and it is only briefly discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01567v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cl\'ement Marteau (PSPM, ICJ), Theofanis Sapatinas (UCY)</dc:creator>
    </item>
    <item>
      <title>Covariate balance under Bayesian decision theory</title>
      <link>https://arxiv.org/abs/2509.01828</link>
      <description>arXiv:2509.01828v1 Announce Type: new 
Abstract: We study optimal sample allocation between treatment and control groups under Bayesian linear models. We derive an analytic expression for the Bayes risk, which depends jointly on sample size and covariate mean balance across groups. Under a flat conditional prior, the covariate mean balance term simplifies to the Mahalanobis distance. Our results reveal that the optimal allocation does not always correspond to equal sample sizes, and we provide sufficient conditions under which equal allocation is optimal. Finally, we extend the analysis to sequential settings with groups of patients arriving over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01828v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andr\'e A. F. Fumis, Victor Fossaluza, Rafael B. Stern</dc:creator>
    </item>
    <item>
      <title>Comment on "Worst-case Nonparametric Bounds for the Student T-statistic", arXiv:2508.13226</title>
      <link>https://arxiv.org/abs/2509.02410</link>
      <description>arXiv:2509.02410v1 Announce Type: new 
Abstract: The main result in "Worst-case Nonparametric Bounds for the Student T-statistic'', arXiv:2508.13226 is incorrect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02410v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Iosif Pinelis</dc:creator>
    </item>
    <item>
      <title>G-HIVE: Parameter Estimation and Approximate Inference for Multivariate Response Generalized Linear Models with Hidden Variables</title>
      <link>https://arxiv.org/abs/2509.00196</link>
      <description>arXiv:2509.00196v1 Announce Type: cross 
Abstract: In practice, there often exist unobserved variables, also termed hidden variables, associated with both the response and covariates. Existing works in the literature mostly focus on linear regression with hidden variables. However, when the regression model is non-linear, the presence of hidden variables leads to new challenges in parameter identification, estimation, and statistical inference. This paper studies multivariate response generalized linear models (GLMs) with hidden variables. We propose a unified framework for parameter estimation and statistical inference called G-HIVE, short for 'G'eneralized - 'HI'dden 'V'ariable adjusted 'E'stimation. Specifically, based on factor model assumptions, we propose a modified quasi-likelihood approach to estimate an intermediate parameter, defined through a set of reweighted estimating equations. The key of our approach is to construct the proper weight, so that the first-order asymptotic bias of the estimator can be removed by orthogonal projection. Moreover, we propose an approximate inference framework for uncertainty quantification. Theoretically, we establish the first-order and second-order asymptotic bias and the convergence rate of our estimator. In addition, we characterize the accuracy of the Gaussian approximation of our estimator via the Berry-Esseen bound, which justifies the validity of the proposed approximate inference approach. Extensive simulations and real data analysis results show that G-HIVE is feasibly implementable and can outperform the baseline method that ignores hidden variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00196v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Inbeom Lee, Yang Ning</dc:creator>
    </item>
    <item>
      <title>Partial Functional Dynamic Backdoor Diffusion-based Causal Model</title>
      <link>https://arxiv.org/abs/2509.00472</link>
      <description>arXiv:2509.00472v1 Announce Type: cross 
Abstract: We introduce a Partial Functional Dynamic Backdoor Diffusion-based Causal Model (PFD-BDCM), specifically designed for causal inference in the presence of unmeasured confounders with spatial heterogeneity and temporal dependency. The proposed PFD-BDCM framework addresses the restrictions of the existing approaches by uniquely integrating models for complex spatio-temporal dynamics with the analysis of multi-resolution variables. Specifically, the framework systematically mitigates confounding bias by integrating valid backdoor adjustment sets into a diffusion-based sampling mechanism. Moreover, it accounts for the intricate dynamics of unmeasured confounders through the deployment of region-specific structural equations and conditional autoregressive processes, and accommodates variables observed at heterogeneous resolutions via basis expansions for functional data. Our theoretical analysis establishes error bounds for counterfactual estimates of PFD-BDCM, formally linking reconstruction accuracy to counterfactual fidelity under monotonicity assumptions of structural equation and invertibility assumptions of encoding function. Empirical evaluations on synthetic datasets and real-world air pollution data demonstrate PFD-BDCM's superiority over existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00472v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinwen Liu, Lei Qian, Song Xi Chen, Niansheng Tang</dc:creator>
    </item>
    <item>
      <title>SANVI: A Fast Spectral-Assisted Network Variational Inference Method with an Extended Surrogate Likelihood Function</title>
      <link>https://arxiv.org/abs/2509.00562</link>
      <description>arXiv:2509.00562v1 Announce Type: cross 
Abstract: Bayesian inference has been broadly applied to statistical network analysis, but suffers from the expensive computational costs due to the nature of Markov chain Monte Carlo sampling algorithms. This paper proposes a novel and computationally efficient Spectral-Assisted Network Variational Inference (SANVI) method within the framework of the generalized random dot product graph. The key idea is a cleverly designed extended surrogate likelihood function that enjoys two convenient features. Firstly, it decouples the generalized inner product of latent positions in the random graph model. Secondly, it relaxes the complicated domain of the original likelihood function to the entire Euclidean space. Leveraging these features, we design a computationally efficient Gaussian variational inference algorithm via stochastic gradient descent. Furthermore, we show the asymptotic efficiency of the maximum extended surrogate likelihood estimator and the Bernstein-von Mises limit of the variational posterior distribution. Through extensive numerical studies, we demonstrate the usefulness of the proposed SANVI algorithm compared to the classical Markov chain Monte Carlo algorithm, including comparable estimation accuracy for the latent positions and less computational costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00562v1</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dingbo Wu, Fangzheng Xie</dc:creator>
    </item>
    <item>
      <title>Bootstrap Diagnostic Tests</title>
      <link>https://arxiv.org/abs/2509.01351</link>
      <description>arXiv:2509.01351v1 Announce Type: cross 
Abstract: Violation of the assumptions underlying classical (Gaussian) limit theory frequently leads to unreliable statistical inference. This paper shows the novel result that the bootstrap can detect such violation by means of simple and powerful tests which (a) induce no pre-testing bias, (b) can be performed using the same critical values in a broad range of applications, and (c) are consistent against deviations from asymptotic normality. By focusing on the discrepancy between the conditional distribution of a bootstrap statistic and the (limiting) Gaussian distribution which obtains under valid specification, we show how to assess whether this discrepancy is large enough to indicate specification invalidity. The method, which is computationally straightforward, only requires to measure the discrepancy between the bootstrap and the Gaussian distributions based on a sample of i.i.d. draws of the bootstrap statistic. We derive sufficient conditions for the randomness in the data to mix with the randomness in the bootstrap repetitions in a way such that (a), (b) and (c) above hold. To demonstrate the practical relevance and broad applicability of our diagnostic procedure, we discuss five scenarios where the asymptotic Gaussian approximation may fail: (i) weak instruments in instrumental variable regression; (ii) non-stationarity in autoregressive time series; (iii) parameters near or at the boundary of the parameter space; (iv) infinite variance innovations in a location model for i.i.d. data; (v) invalidity of the delta method due to (near-)rank deficiency in the implied Jacobian matrix. An illustration drawn from the empirical macroeconomic literature concludes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01351v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giuseppe Cavaliere, Luca Fanelli, Iliyan Georgiev</dc:creator>
    </item>
    <item>
      <title>A James-Stein Estimator based Generalized OMP Algorithm for Robust Signal Recovery using Sparse Representation</title>
      <link>https://arxiv.org/abs/2509.01410</link>
      <description>arXiv:2509.01410v1 Announce Type: cross 
Abstract: In this paper, we introduce a novel algorithm named JS-gOMP, which enhances the generalized Orthogonal Matching Pursuit (gOMP) algorithm for improved noise robustness in sparse signal processing. The JS-gOMP algorithm uniquely incorporates the James-Stein estimator, optimizing the trade-off between signal recovery and noise suppression. This modification addresses the challenges posed by noise in the dictionary, a common issue in sparse representation scenarios. Comparative analyses demonstrate that JS-gOMP outperforms traditional gOMP, especially in noisy environments, offering a more effective solution for signal and image processing applications where noise presence is significant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01410v1</guid>
      <category>eess.SP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Debraj Banerjee, Amitava Chatterjee</dc:creator>
    </item>
    <item>
      <title>The Price of Sparsity: Sufficient Conditions for Sparse Recovery using Sparse and Sparsified Measurements</title>
      <link>https://arxiv.org/abs/2509.01809</link>
      <description>arXiv:2509.01809v1 Announce Type: cross 
Abstract: We consider the problem of recovering the support of a sparse signal using noisy projections. While extensive work has been done on the dense measurement matrix setting, the sparse setting remains less explored. In this work, we establish sufficient conditions on the sample size for successful sparse recovery using sparse measurement matrices. Bringing together our result with previously known necessary conditions, we discover that, in the regime where $ds/p \rightarrow +\infty$, sparse recovery in the sparse setting exhibits a phase transition at an information-theoretic threshold of $n_{\text{INF}}^{\text{SP}} = \Theta\left(s\log\left(p/s\right)/\log\left(ds/p\right)\right)$, where $p$ denotes the signal dimension, $s$ the number of non-zero components of the signal, and $d$ the expected number of non-zero components per row of measurement. This expression makes the price of sparsity explicit: restricting each measurement to $d$ non-zeros inflates the required sample size by a factor of $\log{s}/\log\left(ds/p\right)$, revealing a precise trade-off between sampling complexity and measurement sparsity. Additionally, we examine the effect of sparsifying an originally dense measurement matrix on sparse signal recovery. We prove in the regime of $s = \alpha p$ and $d = \psi p$ with $\alpha, \psi \in \left(0,1\right)$ and $\psi$ small that a sample of size $n^{\text{Sp-ified}}_{\text{INF}} = \Theta\left(p / \psi^2\right)$ is sufficient for recovery, subject to a certain uniform integrability conjecture, the proof of which is work in progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01809v1</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youssef Chaabouni, David Gamarnik</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification for Ranking with Heterogeneous Preferences</title>
      <link>https://arxiv.org/abs/2509.01847</link>
      <description>arXiv:2509.01847v1 Announce Type: cross 
Abstract: This paper studies human preference learning based on partially revealed choice behavior and formulates the problem as a generalized Bradley-Terry-Luce (BTL) ranking model that accounts for heterogeneous preferences. Specifically, we assume that each user is associated with a nonparametric preference function, and each item is characterized by a low-dimensional latent feature vector - their interaction defines the underlying low-rank score matrix. In this formulation, we propose an indirect regularization method for collaboratively learning the score matrix, which ensures entrywise $\ell_\infty$-norm error control - a novel contribution to the heterogeneous preference learning literature. This technique is based on sieve approximation and can be extended to a broader class of binary choice models where a smooth link function is adopted. In addition, by applying a single step of the Newton-Raphson method, we debias the regularized estimator and establish uncertainty quantification for item scores and rankings of items, both for the aggregated and individual preferences. Extensive simulation results from synthetic and real datasets corroborate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01847v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianqing Fan, Hyukjun Kwon, Xiaonan Zhu</dc:creator>
    </item>
    <item>
      <title>On the Kolmogorov-Feller weak law of large numbers for Frechet mean on non-compact symmetric spaces</title>
      <link>https://arxiv.org/abs/2509.02074</link>
      <description>arXiv:2509.02074v1 Announce Type: cross 
Abstract: We establish the Kolmogorov-Feller weak law of large numbers for Frechet mean on non-compact symmetric spaces under certain regularity conditions. Our results accommodate non-identically distributed random variables and are accompanied by an illustrative example that demonstrates their applicability to the space of symmetric positive-definite matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02074v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jongmin Lee, Sungkyu Jung</dc:creator>
    </item>
    <item>
      <title>Canonical Forms as Dual Volumes</title>
      <link>https://arxiv.org/abs/2509.02239</link>
      <description>arXiv:2509.02239v1 Announce Type: cross 
Abstract: We study dual volume representations of canonical forms for positive geometries in projective spaces, expressing their rational canonical functions as Laplace transforms of measures supported on the convex dual of the semialgebraic set. When the measure is non-negative, we term the geometry completely monotone, reflecting the property of its canonical function. We identify a class of positive geometries whose canonical functions admit such dual volume representations, characterized by the algebraic boundary cut out by a hyperbolic polynomial, for which the geometry is a hyperbolicity region. In particular, simplex-like minimal spectrahedra are completely monotone, with representing measures related to the Wishart distribution, capturing volumes of spectrahedra or their boundaries. We explicitly compute these measures for positive geometries in the projective plane bounded by lines and conics or by a nodal cubic, revealing periods evaluating to transcendental functions. This dual volume perspective reinterprets positive geometries by replacing logarithmic differential forms with probability measures on the dual, forging new connections to partial differential equations, hyperbolicity, convexity, positivity, algebraic statistics, and convex optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02239v1</guid>
      <category>hep-th</category>
      <category>math.AG</category>
      <category>math.AP</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elia Mazzucchelli, Prashanth Raman</dc:creator>
    </item>
    <item>
      <title>A nonparametric Bayesian analysis of independent and identically distributed observations of covariate-driven Poisson processes</title>
      <link>https://arxiv.org/abs/2509.02299</link>
      <description>arXiv:2509.02299v1 Announce Type: cross 
Abstract: An important task in the statistical analysis of inhomogeneous point processes is to investigate the influence of a set of covariates on the point-generating mechanism. In this article, we consider the nonparametric Bayesian approach to this problem, assuming that $n$ independent and identically distributed realizations of the point pattern and the covariate random field are available. In many applications, different covariates are often vastly diverse in physical nature, resulting in anisotropic intensity functions whose variations along distinct directions occur at different smoothness levels. To model this scenario, we employ hierarchical prior distributions based on multi-bandwidth Gaussian processes. We prove that the resulting posterior distributions concentrate around the ground truth at optimal rate as $n\to\infty$, and achieve automatic adaptation to the anisotropic smoothness. Posterior inference is concretely implemented via a Metropolis-within-Gibbs Markov chain Monte Carlo algorithm that incorporates a dimension-robust sampling scheme to handle the functional component of the proposed nonparametric Bayesian model. Our theoretical results are supported by extensive numerical simulation studies. Further, we present an application to the analysis of a Canadian wildfire dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02299v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patric Dolmeta, Matteo Giordano</dc:creator>
    </item>
    <item>
      <title>Distribution estimation via Flow Matching with Lipschitz guarantees</title>
      <link>https://arxiv.org/abs/2509.02337</link>
      <description>arXiv:2509.02337v1 Announce Type: cross 
Abstract: Flow Matching, a promising approach in generative modeling, has recently gained popularity. Relying on ordinary differential equations, it offers a simple and flexible alternative to diffusion models, which are currently the state-of-the-art. Despite its empirical success, the mathematical understanding of its statistical power so far is very limited. This is largely due to the sensitivity of theoretical bounds to the Lipschitz constant of the vector field which drives the ODE. In this work, we study the assumptions that lead to controlling this dependency. Based on these results, we derive a convergence rate for the Wasserstein $1$ distance between the estimated distribution and the target distribution which improves previous results in high dimensional setting. This rate applies to certain classes of unbounded distributions and particularly does not require $\log$-concavity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02337v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lea Kunkel</dc:creator>
    </item>
    <item>
      <title>Resampling-based multi-resolution false discovery exceedance control</title>
      <link>https://arxiv.org/abs/2509.02376</link>
      <description>arXiv:2509.02376v1 Announce Type: cross 
Abstract: MaxT is a highly popular resampling-based multiple testing procedure, which controls the familywise error rate (FWER) and is powerful under dependence. This paper generalizes maxT to what we term ``multi-resolution'' False Discovery eXceedance (FDX) control. FDX control means ensuring that the FDP -- the proportion of false discoveries among all rejections -- is at most $\gamma$ with probability at least $1-\alpha$. Here $\gamma$ and $\alpha$ are prespecified, small values between 0 and 1. If we take $\gamma=0$, FDX control is the same as FWER control. When $\gamma=0$, the new procedure reduces to maxT. For $\gamma&gt;0$, our method has much higher power. Our method is then strongly connected to maxT as well. For example, if our method rejects fewer than $\gamma^{-1}$ hypotheses, then maxT rejects the same. Further, the implied global tests of the two methods are the same, although the implied closed testing procedures differ. Finally, our method provides an easy-to-use simultaneous, multi-resolution guarantee: the procedure outputs a single rejection threshold $q$, but ensures that with probability $1-\alpha$, simultaneously over all stricter thresholds, the corresponding FDPs are also below $\gamma$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02376v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesse Hemerik</dc:creator>
    </item>
    <item>
      <title>Bringing Closure to False Discovery Rate Control: A General Principle for Multiple Testing</title>
      <link>https://arxiv.org/abs/2509.02517</link>
      <description>arXiv:2509.02517v1 Announce Type: cross 
Abstract: We present a novel necessary and sufficient principle for multiple testing methods controlling an expected loss. This principle asserts that every such multiple testing method is a special case of a general closed testing procedure based on e-values. It generalizes the Closure Principle, known to underlie all methods controlling familywise error and tail probabilities of false discovery proportions, to a large class of error rates -- in particular to the false discovery rate (FDR). By writing existing methods as special cases of this procedure, we can achieve uniform improvements existing multiple testing methods such as the e-Benjamini-Hochberg and the Benjamini-Yekutieli procedures, and the self-consistent method of Su (2018). We also show that methods derived using the closure principle have several valuable properties. For example, they generally control their error rate not just for one rejected set, but simultaneously over many, allowing post hoc flexibility for the researcher. Moreover, we show that because all multiple testing methods for all error metrics are derived from the same procedure, researchers may even choose the error metric post hoc. Under certain conditions, this flexibility even extends to post hoc choice of the nominal error rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02517v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyu Xu, Aldo Solari, Lasse Fischer, Rianne de Heide, Aaditya Ramdas, Jelle Goeman</dc:creator>
    </item>
    <item>
      <title>Is RL fine-tuning harder than regression? A PDE learning approach for diffusion models</title>
      <link>https://arxiv.org/abs/2509.02528</link>
      <description>arXiv:2509.02528v1 Announce Type: cross 
Abstract: We study the problem of learning the optimal control policy for fine-tuning a given diffusion process, using general value function approximation. We develop a new class of algorithms by solving a variational inequality problem based on the Hamilton-Jacobi-Bellman (HJB) equations. We prove sharp statistical rates for the learned value function and control policy, depending on the complexity and approximation errors of the function class. In contrast to generic reinforcement learning problems, our approach shows that fine-tuning can be achieved via supervised regression, with faster statistical rate guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02528v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenlong Mou</dc:creator>
    </item>
    <item>
      <title>Large-scale Multiple Testing: Fundamental Limits of False Discovery Rate Control and Compound Oracle</title>
      <link>https://arxiv.org/abs/2302.06809</link>
      <description>arXiv:2302.06809v3 Announce Type: replace 
Abstract: The false discovery rate (FDR) and the false non-discovery rate (FNR), defined as the expected false discovery proportion (FDP) and the false non-discovery proportion (FNP), are the most popular benchmarks for multiple testing. Despite the theoretical and algorithmic advances in recent years, the optimal tradeoff between the FDR and the FNR has been largely unknown except for certain restricted classes of decision rules, e.g., separable rules, or for other performance metrics, e.g., the marginal FDR and the marginal FNR (mFDR and mFNR). In this paper, we determine the asymptotically optimal FDR-FNR tradeoff under the two-group random mixture model when the number of hypotheses tends to infinity. Distinct from the optimal mFDR-mFNR tradeoff, which is achieved by separable decision rules, the optimal FDR-FNR tradeoff requires compound rules even in the large-sample limit and for models as simple as the Gaussian location model. This suboptimality of separable rules also holds for other objectives, such as maximizing the expected number of true discoveries. Finally, to address the limitation of the FDR which only controls the expectation but not the fluctuation of the FDP, we also determine the optimal tradeoff when the FDP is controlled with high probability and show it coincides with that of the mFDR and the mFNR. Extensions to models with a fixed non-null proportion are also obtained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.06809v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yutong Nie, Yihong Wu</dc:creator>
    </item>
    <item>
      <title>Cardinality Sparsity: Applications in Matrix-Matrix Multiplications and Machine Learning</title>
      <link>https://arxiv.org/abs/2302.08235</link>
      <description>arXiv:2302.08235v3 Announce Type: replace 
Abstract: High-dimensional data has become ubiquitous across the sciences but presents computational and statistical challenges. A common approach to addressing these challenges is through sparsity. In this paper, we introduce a new concept of sparsity, called cardinality sparsity. Broadly speaking, we define a tensor as sparse if it contains only a small number of unique values. We demonstrate that cardinality sparsity can improve deep learning and tensor regression both statistically and computationally. Along the way, we generalize recent statistical theories in these fields. Most importantly, we show that cardinality sparsity has a strikingly powerful application beyond high-dimensional data analysis: it can significantly speed up matrix-matrix multiplications. For instance, we demonstrate that cardinality sparsity leads to algorithms for binary-matrix multiplication that outperform state-of-the-art algorithms by a substantial margin. Additionally, another crucial aspect of this sparsity is minimizing memory usage. By executing matrix multiplication in the compressed domain, we can significantly lower the amount of memory needed to store the input data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.08235v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ali Mohades, Johannes Lederer</dc:creator>
    </item>
    <item>
      <title>Optimal rates for estimating the covariance kernel from synchronously sampled functional data</title>
      <link>https://arxiv.org/abs/2407.13641</link>
      <description>arXiv:2407.13641v2 Announce Type: replace 
Abstract: We obtain minimax-optimal convergence rates in the supremum norm, including information-theoretic lower bounds, for estimating the covariance kernel of a stochastic process which is repeatedly observed at discrete, synchronous design points. We focus on the supremum norm instead of the simpler $L_2$ norm, since it corresponds to the visualization of the estimation error and forms the basis for the construction of uniform confidence bands. For dense design, assuming H\"older-smooth sample paths we obtain the $\sqrt n$-rate of convergence in the supremum norm without additional logarithmic factors which typically occur in the results in the literature. Surprisingly, in the transition from dense to sparse design the rates do not reflect the two-dimensional nature of the covariance kernel but correspond to those for univariate mean function estimation. Our estimation method can make use of higher-order smoothness of the covariance kernel away from the diagonal, and does not require the same smoothness on the diagonal itself. Hence, our results cover covariance kernels of processes with rough, non-differentiable sample paths. Moreover, the estimator does not use mean function estimation to form residuals, and no smoothness assumptions on the mean have to be imposed. In the dense case we also obtain a central limit theorem in the supremum norm, which can be used as the basis for the construction of uniform confidence sets. Extensions to estimating partial derivatives as well as to asynchronous designs are also discussed. Simulations and real-data applications illustrate the practical usefulness of the methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13641v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Berger, Hajo Holzmann</dc:creator>
    </item>
    <item>
      <title>WeSpeR: Computing non-linear shrinkage formulas for the weighted sample covariance</title>
      <link>https://arxiv.org/abs/2410.14413</link>
      <description>arXiv:2410.14413v2 Announce Type: replace 
Abstract: We address the issue of computing the non-linear shrinkage formulas for the weighted sample covariance in high dimension. We use theoretical properties of the asymptotic sample spectrum in order to derive the \textit{WeSpeR} algorithm and significantly speed up non-linear shrinkage in dimension higher than $1000$. Empirical tests confirm the good properties of the \textit{WeSpeR} algorithm. We provide the implementation in PyTorch for it.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14413v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Benoit Oriol</dc:creator>
    </item>
    <item>
      <title>Centrality and shape-related comparisons in a tree-structured Markov random field</title>
      <link>https://arxiv.org/abs/2410.20240</link>
      <description>arXiv:2410.20240v2 Announce Type: replace 
Abstract: Understanding the effects of the choice of the tree on the joint distribution of a tree-structured Markov random field (MRF) is crucial for fully exploiting the intelligibility of such probabilistic graphical models. Tools must be developed in this regard: this is the overarching objective of this paper. Our discussion is two-fold. First, we examine concepts specific to network centrality theory. We put forth a new conception of centrality for MRFs that not only accounts for the tree topology, but also the underlying stochastic dynamics. In this vein, we compare synecdochic pairs, random vectors comprising a MRF's component and its sum, using stochastic orders. The resulting orderings are transferred to risk-allocation quantities, which therefore serve as new centrality indices tailored to our stochastic framework. Second, we shed light on the influence of the tree's shapes, by establishing convex orderings for MRFs encrypted on trees of different shapes. This results in the design of a new partial order on tree shapes. This analysis is done within the framework of a propagation-based family of tree-structured MRFs with the uncommon property of having fixed Poisson marginal distributions unaffected by the dependence scheme. This work is a first step into the analysis of MRFs' trees' shapes and a stepping stone to extending this analysis to a broader framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20240v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin C\^ot\'e, H\'el\`ene Cossette, Etienne Marceau</dc:creator>
    </item>
    <item>
      <title>Convergence of empirical Gromov-Wasserstein distance</title>
      <link>https://arxiv.org/abs/2508.03985</link>
      <description>arXiv:2508.03985v2 Announce Type: replace 
Abstract: We study rates of convergence for estimation of the Gromov-Wasserstein (GW) distance. For two marginals supported on compact subsets of $\R^{d_x}$ and $\R^{d_y}$, respectively, with $\min \{ d_x,d_y \} &gt; 4$, prior work established the rate $n^{-\frac{2}{\min\{d_x,d_y\}}}$ in $L^1$ for the plug-in empirical estimator based on $n$ i.i.d. samples. We extend this fundamental result to marginals with unbounded supports, assuming only finite polynomial moments. Our proof techniques for the upper bounds can be adapted to obtain sample complexity results for penalized Wasserstein alignment that encompasses the GW distance and Wasserstein Procrustes. Furthermore, we establish matching minimax lower bounds (up to logarithmic factors) for estimating the GW distance. Finally, we establish deviation inequalities for the error of empirical GW in cases where two marginals have compact supports, exponential tails, or finite polynomial moments. The deviation inequalities yield that the same rate $n^{-\frac{2}{\min\{d_x,d_y\}}}$ holds for empirical GW also with high probability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03985v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kengo Kato, Boyu Wang</dc:creator>
    </item>
    <item>
      <title>Worst-case Nonparametric Bounds for the Student T-statistic</title>
      <link>https://arxiv.org/abs/2508.13226</link>
      <description>arXiv:2508.13226v2 Announce Type: replace 
Abstract: We address the problem of finding worst-case nonparametric bounds for T-statistic by considering the extremal problem of maximising the mid-quantile (a special case of 'smoothed quantile' as discussed in \cite{St77} and \cite{W11}) $\tilde Q(S(w);\alpha)$ over nonnegative weight vectors $w\in\RR^n$ with $\|w\|_2=1$, where $S(w)=\sum_{i=1}^n w_i \varepsilon_i$ and $\varepsilon_i$ are independent Rademacher variables. While classical results of Hoeffding [1] and Chernoff [2] may be used to provide sub-Gaussian upper bounds, and optimal-order inequalities were later obtained by the author [3,4], the associated extremal problem has remained unsolved. We resolve this problem exactly (for the Mid-Quantile and, trivially, the Continuous case): for each $\alpha&lt;{1\over 2}$ and each $n$, we determine the maximal value and characterise all maximising weights. The maximisers are $k$-sparse equal-weight vectors with weights $1/\sqrt{k}$, and the optimal support size $k$ is found by a finite search over at most $n$ candidates. This yields an explicit envelope $M_n(\alpha)$ and its universal limit as $n$ grows. Our results provide exact solutions to problems that have been studied through bounds and approximations for over sixty years, with applications to nonparametric inference, self-standardised statistics, and robust hypothesis testing under symmetry assumptions, including a conjecture by Edelman\cite{edelman1990}, albeit for continuous distributions only (which he did not specify, which has been found to not always hold otherwise)</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13226v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Edelman</dc:creator>
    </item>
    <item>
      <title>Simple and Sharp Generalization Bounds via Lifting</title>
      <link>https://arxiv.org/abs/2508.18682</link>
      <description>arXiv:2508.18682v2 Announce Type: replace 
Abstract: We develop an information-theoretic framework for bounding the expected supremum and tail probabilities of stochastic processes, offering a simpler and sharper alternative to classical chaining and slicing arguments for generalization bounds. The key idea is a lifting argument that produces information-theoretic analogues of empirical process bounds, such as Dudley's entropy integral. The lifting introduces symmetry, yielding sharp bounds even when the classical Dudley integral is loose. As a by-product, we obtain a concise proof of the majorizing measure theorem, providing explicit constants. The information-theoretic approach provides a soft version of classical localized complexity bounds in generalization theory, but is more concise and does not require the slicing argument. We apply this approach to empirical risk minimization over Sobolev ellipsoids and weak $\ell_q$ balls, obtaining sharper convergence rates or extensions to settings not covered by classical methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18682v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Jingbo Liu</dc:creator>
    </item>
    <item>
      <title>Distance and Kernel-Based Measures for Global and Local Two-Sample Conditional Distribution Testing</title>
      <link>https://arxiv.org/abs/2210.08149</link>
      <description>arXiv:2210.08149v3 Announce Type: replace-cross 
Abstract: Testing the equality of two conditional distributions is crucial in various modern applications, including transfer learning and causal inference. Despite its importance, this fundamental problem has received surprisingly little attention in the literature, with existing works focusing exclusively on global two-sample conditional distribution testing. Based on distance and kernel methods, this paper presents the first unified framework for both global and local two-sample conditional distribution testing. To this end, we introduce distance and kernel-based measures that characterize the homogeneity of two conditional distributions. Drawing from the concept of conditional U-statistics, we propose consistent estimators for these measures. Theoretically, we derive the convergence rates and the asymptotic distributions of the estimators under both the null and alternative hypotheses. Utilizing these measures, along with a local bootstrap approach, we develop global and local tests that can detect discrepancies between two conditional distributions at global and local levels, respectively. Our tests demonstrate reliable performance through simulations and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.08149v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jian Yan, Zhuoxi Li, Xianyang Zhang</dc:creator>
    </item>
    <item>
      <title>Euler Characteristics and Homotopy Types of Definable Sublevel Sets, with Applications to Topological Data Analysis</title>
      <link>https://arxiv.org/abs/2309.03142</link>
      <description>arXiv:2309.03142v3 Announce Type: replace-cross 
Abstract: Given a definable function $f: S \to \mathbb{R}$ on a definable set $S$, we study sublevel sets of the form $S^f_t = \{x \in S: f(x) \leq t\}$ for all $t \in \mathbb{R}$. Using o-minimal structures, we prove that the Euler characteristic of $S^f_t$ is right continuous with respect to $t$. Furthermore, when $S$ is compact, we show that $S^f_{t+\delta}$ deformation retracts to $S^f_t$ for all sufficiently small $\delta &gt; 0$. Applying these results, we also characterize the connections between the following concepts in topological data analysis: the Euler characteristic transform (ECT), smooth ECT, Euler-Radon transform (ERT), and smooth ERT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.03142v3</guid>
      <category>math.AT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mattie Ji, Kun Meng</dc:creator>
    </item>
    <item>
      <title>Combining Evidence Across Filtrations</title>
      <link>https://arxiv.org/abs/2402.09698</link>
      <description>arXiv:2402.09698v4 Announce Type: replace-cross 
Abstract: In sequential anytime-valid inference, any admissible procedure must be based on e-processes: generalizations of test martingales that quantify the accumulated evidence against a composite null hypothesis at any stopping time. This paper proposes a method for combining e-processes constructed in different filtrations but for the same null. Although e-processes in the same filtration can be combined effortlessly (by averaging), e-processes in different filtrations cannot because their validity in a coarser filtration does not translate to a finer filtration. This issue arises in sequential tests of randomness and independence, as well as in the evaluation of sequential forecasters. We establish that a class of functions called adjusters can lift arbitrary e-processes across filtrations. The result yields a generally applicable "adjust-then-combine" procedure, which we demonstrate on the problem of testing randomness in real-world financial data. Furthermore, we prove a characterization theorem for adjusters that formalizes a sense in which using adjusters is necessary. There are two major implications. First, if we have a powerful e-process in a coarsened filtration, then we readily have a powerful e-process in the original filtration. Second, when we coarsen the filtration to construct an e-process, there is a logarithmic cost to recovering validity in the original filtration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09698v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yo Joong Choe, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Jump detection in high-frequency order prices</title>
      <link>https://arxiv.org/abs/2403.00819</link>
      <description>arXiv:2403.00819v2 Announce Type: replace-cross 
Abstract: We propose methods to infer jumps of a semi-martingale, which describes long-term price dynamics, based on discrete, noisy, high-frequency observations. Different to the classical model of additive, centered market microstructure noise, we consider one-sided microstructure noise for order prices in a limit order book.
  We develop methods to estimate, locate and test for jumps using local minima of best ask quotes. We provide a local jump test and show that we can consistently estimate jump sizes and jump times. One main contribution is a global test for jumps. We establish the asymptotic properties and optimality of this test. We derive the asymptotic distribution of a maximum statistic under the null hypothesis of no jumps based on extreme value theory. We prove consistency under the alternative hypothesis. The rate of convergence for local alternatives is determined and shown to be much faster than optimal rates for the standard market microstructure noise model. This allows the identification of smaller jumps. In the process, we establish uniform consistency for spot volatility estimation under one-sided noise. Online jump detection based on the new approach is shown to achieve a speed advantage compared to standard methods applied to mid quotes.
  A simulation study sheds light on the finite-sample implementation and properties of the new approach and draws a comparison to a popular method for market microstructure noise. We showcase how our new approach helps to improve jump detection in an empirical analysis of intra-daily limit order book data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00819v2</guid>
      <category>q-fin.ST</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Markus Bibinger, Nikolaus Hautsch, Alexander Ristig</dc:creator>
    </item>
    <item>
      <title>Two-Sided Nearest Neighbors: An adaptive and minimax optimal procedure for matrix completion</title>
      <link>https://arxiv.org/abs/2411.12965</link>
      <description>arXiv:2411.12965v2 Announce Type: replace-cross 
Abstract: Nearest neighbor (NN) algorithms have been extensively used for missing data problems in recommender systems and sequential decision-making systems. Prior theoretical analysis has established favorable guarantees for NN when the underlying data is sufficiently smooth and the missingness probabilities are lower bounded. Here we analyze NN with non-smooth non-linear functions with vast amounts of missingness. In particular, we consider matrix completion settings where the entries of the underlying matrix follow a latent non-linear factor model, with the non-linearity belonging to a \Holder function class that is less smooth than Lipschitz. Our results establish following favorable properties for a suitable two-sided NN: (1) The mean squared error (MSE) of NN adapts to the smoothness of the non-linearity, (2) under certain regularity conditions, the NN error rate matches the rate obtained by an oracle equipped with the knowledge of both the row and column latent factors, and finally (3) NN's MSE is non-trivial for a wide range of settings even when several matrix entries might be missing deterministically. We support our theoretical findings via extensive numerical simulations and a case study with data from a mobile health study, HeartSteps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12965v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tathagata Sadhukhan, Manit Paul, Raaz Dwivedi</dc:creator>
    </item>
    <item>
      <title>Hard edge asymptotics of correlation functions between singular values and eigenvalues</title>
      <link>https://arxiv.org/abs/2501.15765</link>
      <description>arXiv:2501.15765v3 Announce Type: replace-cross 
Abstract: Any square complex matrix of size $n\times n$ can be partially characterized by its $n$ eigenvalues and/or $n$ singular values. While no one-to-one correspondence exists between those two kinds of values on a deterministic level, for random complex matrices drawn from a bi-unitarily invariant ensemble, a bijection exists between the underlying singular value ensemble and the corresponding eigenvalue ensemble. This enabled the recent finding of an explicit formula for the joint probability density between $1$ eigenvalue and $k$ singular values, coined $1,k$-point function. We derive here the large $n$ asymptotic of the $1,k$-point function around the origin (hard edge) for a large subclass of bi-unitarily invariant ensembles called polynomial ensembles and its subclass P\'olya ensembles. This latter subclass contains all Meijer-G ensembles and, in particular, Muttalib-Borodin ensembles and the classical Wishart-Laguerre (complex Ginibre), Jacobi (truncated unitary), Cauchy-Lorentz ensembles. We show that the latter three ensembles share the same asymptotic of the $1,k$-point function around the origin. In the case of Jacobi ensembles, there exists another hard edge for the singular values, namely the upper edge of their support, which corresponds to a soft edge for the eigenvalue (soft-hard edge). We give the explicit large $n$ asymptotic of the $1,k$-point function around this soft-hard edge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15765v3</guid>
      <category>math.PR</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1088/1751-8121/add9f3</arxiv:DOI>
      <dc:creator>Matthias Allard</dc:creator>
    </item>
    <item>
      <title>An Iterative Block Matrix Inversion (IBMI) Algorithm for Symmetric Positive Definite Matrices with Applications to Covariance Matrices</title>
      <link>https://arxiv.org/abs/2502.06377</link>
      <description>arXiv:2502.06377v2 Announce Type: replace-cross 
Abstract: Obtaining the inverse of a large symmetric positive definite matrix $\mathcal{A}\in\mathbb{R}^{p\times p}$ is a continual challenge across many mathematical disciplines. The computational complexity associated with direct methods can be prohibitively expensive, making it infeasible to compute the inverse. In this paper, we present a novel iterative algorithm (IBMI), which is designed to approximate the inverse of a large, dense, symmetric positive definite matrix. The matrix is first partitioned into blocks, and an iterative process using block matrix inversion is repeated until the matrix approximation reaches a satisfactory level of accuracy. We demonstrate that the two-block, non-overlapping approach converges for any positive definite matrix, while numerical results provide strong evidence that the multi-block, overlapping approach also converges for such matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06377v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ann Paterson, Jennifer Pestana, Victorita Dolean</dc:creator>
    </item>
    <item>
      <title>G{\'e}n{\'e}ration de Matrices de Corr{\'e}lation avec des Structures de Graphe par Optimisation Convexe</title>
      <link>https://arxiv.org/abs/2503.21298</link>
      <description>arXiv:2503.21298v2 Announce Type: replace-cross 
Abstract: This work deals with the generation of theoretical correlation matrices with specific sparsity patterns, associated to graph structures. We present a novel approach based on convex optimization, offering greater flexibility compared to existing techniques, notably by controlling the mean of the entry distribution in the generated correlation matrices. This allows for the generation of correlation matrices that better represent realistic data and can be used to benchmark statistical methods for graph inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21298v2</guid>
      <category>eess.SP</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ali Fahkar (STATIFY), K\'evin Polisano (SVH), Ir\`ene Gannaz (G-SCOP\_GROG, G-SCOP), Sophie Achard (STATIFY)</dc:creator>
    </item>
    <item>
      <title>Singular values of sparse random rectangular matrices: Emergence of outliers at criticality</title>
      <link>https://arxiv.org/abs/2508.01456</link>
      <description>arXiv:2508.01456v2 Announce Type: replace-cross 
Abstract: Consider the random bipartite Erd\H{o}s-R\'{e}nyi graph $\mathbb{G}(n, m, p)$, where each edge with one vertex in $V_{1}=[n]$ and the other vertex in $V_{2} =[m]$ is connected with probability $p$, and $n=\lfloor \gamma m\rfloor$ for a constant aspect ratio $\gamma \geq 1$. It is well known that the empirical spectral measure of its centered and normalized adjacency matrix converges to the Mar\v{c}enko-Pastur (MP) distribution. However, largest and smallest singular values may not converge to the right and left edges, respectively, especially when $p = o(1)$. Notably, it was proved by Dumitriu and Zhu (2024) that there are almost surely no singular value outside the compact support of the MP law when $np = \omega(\log(n))$. In this paper, we consider the critical sparsity regime where $p = b\log(n)/\sqrt{mn}$ for some constant $b&gt;0$. We quantitatively characterize the emergence of outlier singular values as follows. For explicit $b_{*}$ and $b^{*}$ as functions of $\gamma$, we prove that when $b &gt; b_{*}$, there is no outlier outside the bulk; when $b^{*}&lt; b &lt; b_{*}$, outliers are present only outside the right edge of the MP law; and when $b &lt; b^{*}$, outliers are present on both sides, all with high probability. Moreover, the locations of those outliers are precisely characterized by a function depending on the largest and smallest degree vertices of the random graph. We estimate the number of outliers as well. Our results follow the path forged by Alt, Ducatez and Knowles (2021), and can be extended to sparse random rectangular matrices with bounded entries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01456v2</guid>
      <category>math.PR</category>
      <category>cs.NA</category>
      <category>math.CO</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ioana Dumitriu, Hai-Xiao Wang, Zhichao Wang, Yizhe Zhu</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Subgraph Frequencies of Exchangeable Hyperedge Models</title>
      <link>https://arxiv.org/abs/2508.13258</link>
      <description>arXiv:2508.13258v2 Announce Type: replace-cross 
Abstract: In statistical network analysis, models for binary adjacency matrices satisfying vertex exchangeability are commonly used. However, such models may fail to capture key features of the data-generating process when interactions, rather than nodes, are fundamental units. We study statistical inference for subgraph counts under an exchangeable hyperedge model. We introduce several classes of subgraph statistics for hypergraphs and develop inferential tools for subgraph frequencies that account for edge multiplicity. We show that a subclass of these subgraph statistics is robust to the deletion of low-degree nodes, enabling inference in settings where low-degree nodes are more likely to be missing. We also examine a more traditional notion of subgraph frequency that ignores multiplicity, showing that while inference based on limiting distributions is feasible in some cases, a non-degenerate limiting distribution may not exist in others. Empirically, we assess our methods through simulations and newly collected real-world hypergraph data on academic and movie collaborations, where our inferential tools outperform traditional approaches based on binary adjacency matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13258v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ayoushman Bhattacharya, Nilanjan Chakraborty, Robert Lunde</dc:creator>
    </item>
  </channel>
</rss>
