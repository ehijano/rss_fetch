<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 29 May 2025 04:02:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Causal Inference for Experiments with Latent Outcomes: Key Results and Their Implications for Design and Analysis</title>
      <link>https://arxiv.org/abs/2505.21909</link>
      <description>arXiv:2505.21909v1 Announce Type: new 
Abstract: How should we analyze randomized experiments in which the main outcome is measured in multiple ways and each measure contains some degree of error? Since Costner (1971) and Bagozzi (1977), methodological discussions of experiments with latent outcomes have reviewed the modeling assumptions that are invoked when the quantity of interest is the average treatment effect (ATE) of a randomized intervention on a latent outcome that is measured with error. Many authors have proposed methods to estimate this ATE when multiple measures of an outcome are available. Despite this extensive literature, social scientists rarely use these modeling approaches when analyzing experimental data, perhaps because the surge of interest in experiments coincides with increased skepticism about the modeling assumptions that these methods invoke. The present paper takes a fresh look at the use of latent variable models to analyze experiments. Like the skeptics, we seek to minimize reliance on ad hoc assumptions that are not rooted in the experimental design and measurement strategy. At the same time, we think that some of the misgivings that are frequently expressed about latent variable models can be addressed by modifying the research design in ways that make the underlying assumptions defensible or testable. We describe modeling approaches that enable researchers to identify and estimate key parameters of interest, suggest ways that experimental designs can be augmented so as to make the modeling requirements more credible, and discuss empirical tests of key modeling assumptions. Simulations and an empirical application illustrate the gains in terms of precision and robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21909v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawei Fu, Donald P. Green</dc:creator>
    </item>
    <item>
      <title>A Synthetic Business Cycle Approach to Counterfactual Analysis with Nonstationary Macroeconomic Data</title>
      <link>https://arxiv.org/abs/2505.22388</link>
      <description>arXiv:2505.22388v1 Announce Type: new 
Abstract: This paper investigates the use of synthetic control methods for causal inference in macroeconomic settings when dealing with possibly nonstationary data. While the synthetic control approach has gained popularity for estimating counterfactual outcomes, we caution researchers against assuming a common nonstationary trend factor across units for macroeconomic outcomes, as doing so may result in misleading causal estimation-a pitfall we refer to as the spurious synthetic control problem. To address this issue, we propose a synthetic business cycle framework that explicitly separates trend and cyclical components. By leveraging the treated unit's historical data to forecast its trend and using control units only for cyclical fluctuations, our divide-and-conquer strategy eliminates spurious correlations and improves the robustness of counterfactual prediction in macroeconomic applications. As empirical illustrations, we examine the cases of German reunification and the handover of Hong Kong, demonstrating the advantages of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22388v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhentao Shi, Jin Xi, Haitian Xie</dc:creator>
    </item>
    <item>
      <title>Regularizing Fairness in Optimal Policy Learning with Distributional Targets</title>
      <link>https://arxiv.org/abs/2401.17909</link>
      <description>arXiv:2401.17909v2 Announce Type: replace 
Abstract: A decision maker typically (i) incorporates training data to learn about the relative effectiveness of treatments, and (ii) chooses an implementation mechanism that implies an ``optimal'' predicted outcome distribution according to some target functional. Nevertheless, a fairness-aware decision maker may not be satisfied achieving said optimality at the cost of being ``unfair" against a subgroup of the population, in the sense that the outcome distribution in that subgroup deviates too strongly from the overall optimal outcome distribution. We study a framework that allows the decision maker to regularize such deviations, while allowing for a wide range of target functionals and fairness measures to be employed. We establish regret and consistency guarantees for empirical success policies with (possibly) data-driven preference parameters, and provide numerical results. Furthermore, we briefly illustrate the methods in two empirical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.17909v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anders Bredahl Kock, David Preinerstorfer</dc:creator>
    </item>
    <item>
      <title>Improving precision of A/B experiments using trigger intensity</title>
      <link>https://arxiv.org/abs/2411.03530</link>
      <description>arXiv:2411.03530v2 Announce Type: replace 
Abstract: In industry, online randomized controlled experiment (a.k.a. A/B experiment) is a standard approach to measure the impact of a causal change. These experiments have small treatment effect to reduce the potential blast radius. As a result, these experiments often lack statistical significance due to low signal-to-noise ratio. A standard approach for improving the precision (or reducing the standard error) focuses only on the trigger observations, where the output of the treatment and the control model are different. Although evaluation with full information about trigger observations (full knowledge) improves the precision, detecting all such trigger observations is a costly affair. In this paper, we propose a sampling based evaluation method (partial knowledge) to reduce this cost. The randomness of sampling introduces bias in the estimated outcome. We theoretically analyze this bias and show that the bias is inversely proportional to the number of observations used for sampling. We also compare the proposed evaluation methods using simulation and empirical data. In simulation, bias in evaluation with partial knowledge effectively reduces to zero when a limited number of observations (&lt;= 0.1%) are sampled for trigger estimation. In empirical setup, evaluation with partial knowledge reduces the standard error by 36.48%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03530v2</guid>
      <category>econ.EM</category>
      <category>cs.CE</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tanmoy Das, Dohyeon Lee, Arnab Sinha</dc:creator>
    </item>
    <item>
      <title>Utilization and Profitability of Tractor Services for Maize Farming in Ejura-Sekyedumase Municipality, Ghana</title>
      <link>https://arxiv.org/abs/2411.15797</link>
      <description>arXiv:2411.15797v2 Announce Type: replace 
Abstract: Maize farming is a major livelihood activity for many farmers in Ghana. Unfortunately, farmers usually do not obtain the expected returns on their investment due to reliance on rudimentary, labor-intensive, and inefficient methods of production. Using cross-sectional data from 359 maize farmers, this study investigates the profitability and determinants of the use of tractor services for maize production in Ejura-Sekyedumase, Ashanti Region of Ghana. Results from descriptive and profitability analyses reveal that tractor services such as ploughing and shelling are widely used, but their profitability varies significantly among farmers. Key factors influencing profitability include farm size, fertilizer quantity applied, and farmer experience. Results from a multivariate probit analysis also showed that farming experience, fertilizer quantity, and profit per acre have a positive influence on tractor service use for shelling, while household size, farm size, and FBO have a negative influence. Farming experience, fertilizer quantity, and profit per acre positively influence tractor service use for ploughing, while farm size has a negative influence. A t-test result reveals a statistically significant difference in profit between farmers who use tractor services and those who do not. Specifically, farmers who utilize tractor services on their maize farm had a return to cost of 9 percent more than those who do not (p-value &lt; 0.05). The Kendall's result showed a moderate agreement among the maize farmers (first ranked being financial issues) in their ability to access/utilize tractor services on their farm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15797v2</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fred Nimoh, Innocent Yao Yevu, Attah-Nyame Essampong, Asante Emmanuel Addo, Addai Kevin</dc:creator>
    </item>
    <item>
      <title>Using Experiments to Correct for Selection in Observational Studies</title>
      <link>https://arxiv.org/abs/2006.09676</link>
      <description>arXiv:2006.09676v2 Announce Type: replace-cross 
Abstract: Researchers increasingly have access to two types of data: (i) large observational datasets where treatment (e.g., class size) is not randomized but several primary outcomes (e.g., graduation rates) and secondary outcomes (e.g., test scores) are observed and (ii) experimental data in which treatment is randomized but only secondary outcomes are observed. We develop a new method to estimate treatment effects on primary outcomes in such settings. We use the difference between the secondary outcome and its predicted value based on the experimental treatment effect to measure selection bias in the observational data. Controlling for this estimate of selection bias yields an unbiased estimate of the treatment effect on the primary outcome under a new assumption that we term latent unconfoundedness, which requires that the same confounders affect the primary and secondary outcomes. Latent unconfoundedness weakens the assumptions underlying commonly used surrogate estimators. We apply our estimator to identify the effect of third grade class size on students outcomes. Estimated impacts on test scores using OLS regressions in observational school district data have the opposite sign of estimates from the Tennessee STAR experiment. In contrast, selection-corrected estimates in the observational data replicate the experimental estimates. Our estimator reveals that reducing class sizes by 25% increases high school graduation rates by 0.7 percentage points. Controlling for observables does not change the OLS estimates, demonstrating that experimental selection correction can remove biases that cannot be addressed with standard controls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2006.09676v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Susan Athey, Raj Chetty, Guido Imbens</dc:creator>
    </item>
    <item>
      <title>PUATE: Efficient Average Treatment Effect Estimation from Treated (Positive) and Unlabeled Units</title>
      <link>https://arxiv.org/abs/2501.19345</link>
      <description>arXiv:2501.19345v2 Announce Type: replace-cross 
Abstract: The estimation of average treatment effects (ATEs), defined as the difference in expected outcomes between treatment and control groups, is a central topic in causal inference. This study develops semiparametric efficient estimators for ATE in a setting where only a treatment group and an unlabeled group, consisting of units whose treatment status is unknown, are observed. This scenario constitutes a variant of learning from positive and unlabeled data (PU learning) and can be viewed as a special case of ATE estimation with missing data. For this setting, we derive the semiparametric efficiency bounds, which characterize the lowest achievable asymptotic variance for regular estimators. We then construct semiparametric efficient ATE estimators that attain these bounds. Our results contribute to the literature on causal inference with missing data and weakly supervised learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19345v2</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato, Fumiaki Kozai, Ryo Inokuchi</dc:creator>
    </item>
  </channel>
</rss>
