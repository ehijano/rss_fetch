<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 10 Nov 2025 05:01:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Dynamic Allocation of Public Goods with Approximate Core Equilibria</title>
      <link>https://arxiv.org/abs/2511.04817</link>
      <description>arXiv:2511.04817v1 Announce Type: new 
Abstract: We consider the problem of repeatedly allocating multiple shareable public goods that have limited availability in an online setting without the use of money. In our setting, agents have additive values, and the value each agent receives from getting access to the goods in each period is drawn i.i.d. from some joint distribution $\mathcal{D}$ (that can be arbitrarily correlated between agents). The principal also has global constraints on the set of goods they can select over the horizon, which is represented via a submodular allocation-cost function. Our goal is to select the periods to allocate the good to ensure high value for each group of agents.
  We develop mechanisms for this problem using an artificial currency, where we give each agent a budget proportional to their (exogenous) fair share. The correlated value distribution makes this an especially challenging problem, as agents may attempt to free-ride by declaring low valuations for the good when they know other agents have high values-hoping those agents will bear a larger share of the cost of the resource. We offer a black-box reduction from monetary mechanisms for the allocation of a costly excludable public good. We focus on pacing strategies, the natural strategies when using AI agents, where agents report a scaled version of their value to the mechanism. Our main results show that when using a truthful monetary mechanism as our building block, the resulting online mechanism has a focal equilibrium in which each agent plays a pacing strategy whose outcome results in an allocation that is a $(\mathcal{H}_n-1)$-approximation of the core, where $\mathcal{H}_n$ is the Harmonic number, and $n$ is the number of agents. Remarkably, we are able to achieve an approximate core solution as a Nash outcome without explicit collaboration or coordination between the agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04817v1</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chido Onyeze, David X. Lin, Siddhartha Banerjee, \'Eva Tardos</dc:creator>
    </item>
    <item>
      <title>Persuading Stable Matching</title>
      <link>https://arxiv.org/abs/2511.04846</link>
      <description>arXiv:2511.04846v1 Announce Type: new 
Abstract: In bipartite matching problems, agents on two sides of a graph want to be paired according to their preferences. The stability of a matching depends on these preferences, which in uncertain environments also reflect agents' beliefs about the underlying state of the world. We investigate how a principal -- who observes the true state of the world -- can strategically shape these beliefs through Bayesian persuasion to induce stable matching that maximizes a desired utility. Due to the general intractability of the underlying matching optimization problem as well as the multi-receiver persuasion problem, our main considerations are two important special cases: (1) when agents can be categorized into a small number of types based on their value functions, and (2) when the number of possible world states is small. For each case, we study both public and private signaling settings. Our results draw a complete complexity landscape: we show that private persuasion remains intractable even when the number of worlds is small, while all other settings admit polynomial-time algorithms. We present efficient algorithms for each tractable case and prove NP-hardness for the intractable ones. These results illuminate the algorithmic frontier of stable matching under information design and clarify when optimal persuasion is computationally feasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04846v1</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Shaki, Jiarui Gan, Sarit Kraus</dc:creator>
    </item>
    <item>
      <title>Optimal Selection Using Algorithmic Rankings with Side Information</title>
      <link>https://arxiv.org/abs/2511.04867</link>
      <description>arXiv:2511.04867v1 Announce Type: new 
Abstract: Motivated by online platforms such as job markets, we study an agent choosing from a list of candidates, each with a hidden quality that determines match value. The agent observes only a noisy ranking of the candidates plus a binary signal that indicates whether each candidate is "free" or "busy." Being busy is positively correlated with higher quality, but can also reduce value due to decreased availability. We study the agent's optimal selection problem in the presence of ranking noise and free-busy signals and ask how the accuracy of the ranking tool impacts outcomes. In a setting with one high-valued candidate and an arbitrary number of low-valued candidates, we show that increased accuracy of the ranking tool can result in reduced social welfare. This can occur for two reasons: agents may be more likely to make offers to busy candidates, and (paradoxically) may be more likely to select lower-ranked candidates when rankings are more indicative of quality. We further discuss conditions under which these results extend to more general settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04867v1</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kate Donahue, Nicole Immorlica, Brendan Lucier</dc:creator>
    </item>
    <item>
      <title>Fair Division with Indivisible Goods, Chores, and Cake</title>
      <link>https://arxiv.org/abs/2511.04891</link>
      <description>arXiv:2511.04891v1 Announce Type: new 
Abstract: We study the problem of fairly allocating indivisible items and a desirable heterogeneous divisible good (i.e., cake) to agents with additive utilities. In our paper, each indivisible item can be a good that yields non-negative utilities to some agents and a chore that yields negative utilities to the other agents. Given a fixed set of divisible and indivisible resources, we investigate almost envy-free allocations, captured by the natural fairness concept of envy-freeness for mixed resources (EFM). It requires that an agent $i$ does not envy another agent $j$ if agent $j$'s bundle contains any piece of cake yielding positive utility to agent $i$ (i.e., envy-freeness), and agent $i$ is envy-free up to one item (EF1) towards agent $j$ otherwise. We prove that with indivisible items and a cake, an EFM allocation always exists for any number of agents with additive utilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04891v1</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haris Aziz, Xinhang Lu, Simon Mackenzie, Mashbat Suzuki</dc:creator>
    </item>
    <item>
      <title>On the Coordination of Value-Maximizing Bidders</title>
      <link>https://arxiv.org/abs/2511.04993</link>
      <description>arXiv:2511.04993v1 Announce Type: new 
Abstract: While the auto-bidding literature predominantly considers independent bidding, we investigate the coordination problem among multiple auto-bidders in online advertising platforms. Two motivating scenarios are: collaborative bidding among multiple distinct bidders managed by a third-party bidding agent, and strategic bid selection for multiple ad campaigns managed by a single advertiser. We formalize this coordination problem as a theoretical model and demonstrate that a straightforward coordination mechanism, where only the highest-value bidder competes with outside bids, strictly dominates independent bidding, improving both Return-on-Spend (RoS) compliance and the total value accrued for each participating auto-bidder or ad campaign. Additionally, our simulations on synthetic and real-world datasets support the theoretical result that coordinated mechanism outperforms independent bidding. These findings highlight both the theoretical potential and the practical robustness of coordination in auto-bidding in online auctions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04993v1</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanru Guan, Jiahao Zhang, Zhe Feng, Tao Lin</dc:creator>
    </item>
    <item>
      <title>Cooperation Under Network-Constrained Communication</title>
      <link>https://arxiv.org/abs/2511.05290</link>
      <description>arXiv:2511.05290v1 Announce Type: new 
Abstract: In this paper, we study cooperation in distributed games under network-constrained communication. Building on the framework of Monderer and Tennenholtz (1999), we derive a sufficient condition for cooperative equilibrium in settings where communication between agents is delayed by the underlying network topology. Each player deploys an agent at every location, and local interactions follow a Prisoner's Dilemma structure. We derive a sufficient condition that depends on the network diameter and the number of locations, and analyze extreme cases of instantaneous, delayed, and proportionally delayed communication. We also discuss the asymptotic case of scale-free communication networks, in which the network diameter grows sub-linearly in the number of locations. These insights clarify how communication latency and network design jointly determine the emergence of distributed cooperation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05290v1</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>cs.SI</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tommy Mordo, Omer Madmon, Moshe Tennenholtz</dc:creator>
    </item>
    <item>
      <title>An Overview of Some Extensions of Mean Field Games beyond Perfect Homogeneity and Anonymity</title>
      <link>https://arxiv.org/abs/2511.04929</link>
      <description>arXiv:2511.04929v1 Announce Type: cross 
Abstract: The mean field games (MFG) paradigm was introduced to provide tractable approximations of games involving very large populations. The theory typically rests on two key assumptions: homogeneity, meaning that all players share the same dynamics and cost functions, and anonymity, meaning that each player interacts with others only through their empirical distribution. While these assumptions simplify the analysis, they can be restrictive for many applications. Fortunately, several extensions of the standard MFG framework that relax these assumptions have been developed in the literature. The purpose of these notes is to offer a pedagogical introduction to such models. In particular, we discuss multi-population MFGs, graphon MFGs, major-minor MFGs, and Stackelberg MFGs, as well as variants involving cooperative players.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04929v1</guid>
      <category>math.OC</category>
      <category>cs.GT</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s11432-025-4658-5</arxiv:DOI>
      <dc:creator>Mathieu Lauri\`ere</dc:creator>
    </item>
    <item>
      <title>Data-Driven Mechanism Design using Multi-Agent Revealed Preferences</title>
      <link>https://arxiv.org/abs/2404.15391</link>
      <description>arXiv:2404.15391v3 Announce Type: replace 
Abstract: We study a sequence of independent one-shot non-cooperative games where agents play equilibria determined by a tunable mechanism. Observing only equilibrium decisions, without parametric or distributional knowledge of utilities, we aim to steer equilibria towards social optimality, and to certify when this is impossible due to the game's structure. We develop an adaptive RL framework for this mechanism design objective. First, we derive a multi-agent revealed-preference test for Pareto optimality that gives necessary and sufficient conditions for the existence of utilities under which the empirically observed mixed-strategy Nash equilibria are socially optimal. The conditions form a tractable linear program. Using this, we build an IRL step that computes the Pareto gap, the distance of observed strategies from Pareto optimality, and couple it with a policy-gradient update. We prove convergence to a mechanism that globally minimizes the Pareto gap. This yields a principled achievability test: if social optimality is attainable for the given game and observed equilibria, Algorithm 1 attains it; otherwise, the algorithm certifies unachievability while converging to the mechanism closest to social optimality. We also show a tight link between our loss and robust revealed-preference metrics, allowing algorithmic suboptimality to be interpreted through established microeconomic notions. Finally, when only finitely many i.i.d. samples from mixed strategies (partial strategy specifications) are available, we derive concentration bounds for convergence and design a distributionally robust RL procedure that attains the mechanism-design objective for the fully specified strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15391v3</guid>
      <category>cs.GT</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke Snow, Vikram Krishnamurthy</dc:creator>
    </item>
    <item>
      <title>Compatibility of Fairness and Nash Welfare under Subadditive Valuations</title>
      <link>https://arxiv.org/abs/2407.12461</link>
      <description>arXiv:2407.12461v4 Announce Type: replace 
Abstract: We establish a compatibility between fairness and efficiency, captured via Nash Social Welfare (NSW), under the broad class of subadditive valuations. We prove that, for subadditive valuations, there always exists a partial allocation that is envy-free up to the removal of any good (EFx) and has NSW at least half of the optimal; here, optimality is considered across all allocations, fair or otherwise. We also prove, for subadditive valuations, the universal existence of complete allocations that are envy-free up to one good (EF1) and also achieve a factor $1/2$ approximation to the optimal NSW. Our EF1 result resolves an open question posed by Garg, Husic, Li, V\'{e}gh, and Vondr\'{a}k (STOC 2023).
  In addition, we develop a polynomial-time algorithm which, given an arbitrary allocation $\widetilde{A}$ as input, returns an EF1 allocation with NSW at least $\frac{1}{e^{2/e}}\approx \frac{1}{2.08}$ times that of $\widetilde{A}$. Therefore, our results imply that the EF1 criterion can be attained simultaneously with a constant-factor approximation to optimal NSW in polynomial time (with demand queries), for subadditive valuations. The previously best-known approximation factor for optimal NSW, under EF1 and among $n$ agents, was $O(n)$ -- we improve this bound to $O(1)$.
  It is known that EF1 and exact Pareto efficiency (PO) are incompatible with subadditive valuations. Complementary to this negative result, the current work shows that we regain compatibility by just considering a factor $1/2$ approximation: EF1 can be achieved in conjunction with $\frac{1}{2}$-PO under subadditive valuations. As such, our results serve as a general tool that can be used as a black box to convert any efficient outcome into a fair one, with only a marginal decrease in efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12461v4</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siddharth Barman, Mashbat Suzuki</dc:creator>
    </item>
    <item>
      <title>Quantifying Inefficiency</title>
      <link>https://arxiv.org/abs/2412.11984</link>
      <description>arXiv:2412.11984v2 Announce Type: replace-cross 
Abstract: We axiomatically define a cardinal social inefficiency function, which, given a set of alternatives and individuals' vNM preferences over the alternatives, assigns a unique number -- the social inefficiency -- to each alternative. These numbers -- and not only their order -- are uniquely defined by our axioms despite no exogenously given interpersonal comparison, outside option, or disagreement point. We interpret these numbers as per-capita losses in endogenously normalized utility. We apply our social inefficiency function to a setting in which interpersonal comparison is notoriously hard to justify -- object allocation without money -- leveraging techniques from computer science to prove an approximate-efficiency result for the Random Serial Dictatorship mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11984v2</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yannai A. Gonczarowski, Ella Segev</dc:creator>
    </item>
    <item>
      <title>Instant Runoff Voting and the Reinforcement Paradox</title>
      <link>https://arxiv.org/abs/2502.05185</link>
      <description>arXiv:2502.05185v3 Announce Type: replace-cross 
Abstract: We analyze the susceptibility of instant runoff voting (IRV) to a lesser-studied paradox known as a \emph{reinforcement paradox}, which occurs when candidate $X$ wins under IRV in two distinct elections but $X$ loses in the combined election formed by merging the ballots from the two elections. For three-candidate IRV elections we provide necessary and sufficient conditions under which there exists a partition of the ballot set into two sets of ballots such that a given losing candidate wins each of the sub-elections. Applying these conditions, we use Monte Carlo simulations to estimate the frequency with which such partitions exist under various models of voter behavior. We also analyze the frequency with which the paradox occurs in a large dataset of real-world ranked-choice elections to provide empirical probabilities. Our general finding is that IRV is highly susceptible to this paradox in three-candidate elections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05185v3</guid>
      <category>physics.soc-ph</category>
      <category>cs.GT</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David McCune, Jennifer Wilson</dc:creator>
    </item>
    <item>
      <title>Optimism Without Regularization: Constant Regret in Zero-Sum Games</title>
      <link>https://arxiv.org/abs/2506.16736</link>
      <description>arXiv:2506.16736v2 Announce Type: replace-cross 
Abstract: This paper studies the optimistic variant of Fictitious Play for learning in two-player zero-sum games. While it is known that Optimistic FTRL -- a regularized algorithm with a bounded stepsize parameter -- obtains constant regret in this setting, we show for the first time that similar, optimal rates are also achievable without regularization: we prove for two-strategy games that Optimistic Fictitious Play (using any tiebreaking rule) obtains only constant regret, providing surprising new evidence on the ability of non-no-regret algorithms for fast learning in games. Our proof technique leverages a geometric view of Optimistic Fictitious Play in the dual space of payoff vectors, where we show a certain energy function of the iterates remains bounded over time. Additionally, we also prove a regret lower bound of $\Omega(\sqrt{T})$ for Alternating Fictitious Play. In the unregularized regime, this separates the ability of optimism and alternation in achieving $o(\sqrt{T})$ regret.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16736v2</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John Lazarsfeld, Georgios Piliouras, Ryann Sim, Stratis Skoulakis</dc:creator>
    </item>
    <item>
      <title>Learning from Delayed Feedback in Games via Extra Prediction</title>
      <link>https://arxiv.org/abs/2509.22426</link>
      <description>arXiv:2509.22426v2 Announce Type: replace-cross 
Abstract: This study raises and addresses the problem of time-delayed feedback in learning in games. Because learning in games assumes that multiple agents independently learn their strategies, a discrepancy in optimization often emerges among the agents. To overcome this discrepancy, the prediction of the future reward is incorporated into algorithms, typically known as Optimistic Follow-the-Regularized-Leader (OFTRL). However, the time delay in observing the past rewards hinders the prediction. Indeed, this study firstly proves that even a single-step delay worsens the performance of OFTRL from the aspects of social regret and convergence. This study proposes the weighted OFTRL (WOFTRL), where the prediction vector of the next reward in OFTRL is weighted $n$ times. We further capture an intuition that the optimistic weight cancels out this time delay. We prove that when the optimistic weight exceeds the time delay, our WOFTRL recovers the good performances that social regret is constant in general-sum normal-form games, and the strategies last-iterate converge to the Nash equilibrium in poly-matrix zero-sum games. The theoretical results are supported and strengthened by our experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22426v2</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>math.OC</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuma Fujimoto, Kenshi Abe, Kaito Ariu</dc:creator>
    </item>
  </channel>
</rss>
