<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Feb 2026 02:47:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Approximately Partitioning Vertices into Short Paths</title>
      <link>https://arxiv.org/abs/2602.03991</link>
      <description>arXiv:2602.03991v1 Announce Type: new 
Abstract: Given a fixed positive integer $k$ and a simple undirected graph $G = (V, E)$, the {\em $k^-$-path partition} problem, denoted by $k$PP for short, aims to find a minimum collection $\cal{P}$ of vertex-disjoint paths in $G$ such that each path in $\cal{P}$ has at most $k$ vertices and each vertex of $G$ appears in one path in $\cal{P}$. In this paper, we present a $\frac {k+4}5$-approximation algorithm for $k$PP when $k\in\{9,10\}$ and an improved $(\frac{\sqrt{11}-2}7 k + \frac {9-\sqrt{11}}7)$-approximation algorithm when $k \ge 11$. Our algorithms achieve the current best approximation ratios for $k \in \{ 9, 10, \ldots, 18 \}$.
  Our algorithms start with a maximum triangle-free path-cycle cover $\cal{F}$, which may not be feasible because of the existence of cycles or paths with more than $k$ vertices. We connect as many cycles in $\cal{F}$ with $4$ or $5$ vertices as possible by computing another maximum-weight path-cycle cover in a suitably constructed graph so that $\cal{F}$ can be transformed into a $k^-$-path partition of $G$ without losing too many edges.
  Keywords: $k^-$-path partition; Triangle-free path-cycle cover; $[f, g]$-factor; Approximation algorithm</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03991v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mingyang Gong, Zhi-Zhong Chen, Brendan Mumey</dc:creator>
    </item>
    <item>
      <title>Minimizing Makespan in Sublinear Time via Weighted Random Sampling</title>
      <link>https://arxiv.org/abs/2602.04059</link>
      <description>arXiv:2602.04059v1 Announce Type: new 
Abstract: We consider the classical makespan minimization scheduling problem where $n$ jobs must be scheduled on $m$ identical machines. Using weighted random sampling, we developed two sublinear time approximation schemes: one for the case where $n$ is known and the other for the case where $n$ is unknown. Both algorithms not only give a $(1+3\epsilon)$-approximation to the optimal makespan but also generate a sketch schedule.
  Our first algorithm, which targets the case where $n$ is known and draws samples in a single round under weighted random sampling, has a running time of $\tilde{O}(\tfrac{m^5}{\epsilon^4} \sqrt{n}+A(\ceiling{m\over \epsilon}, {\epsilon} ))$, where
  $A(\mathcal{N}, \alpha)$ is the time complexity of any $(1+\alpha)$-approximation scheme for the makespan minimization of $\mathcal{N}$ jobs.
  The second algorithm addresses the case where $n$ is unknown. It uses adaptive weighted random sampling, %\textit{that is}, it draws samples in several rounds, adjusting the number of samples after each round,
  and runs in sublinear time $\tilde{O}\left( \tfrac{m^5} {\epsilon^4} \sqrt{n} +
  A(\ceiling{m\over \epsilon}, {\epsilon} )\right)$. We also provide an implementation that generates a weighted random sample using $O(\log n)$ uniform random samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04059v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bin Fu, Yumei Huo, Hairong Zhao</dc:creator>
    </item>
    <item>
      <title>QuadRank: Engineering a High Throughput Rank</title>
      <link>https://arxiv.org/abs/2602.04103</link>
      <description>arXiv:2602.04103v1 Announce Type: new 
Abstract: Given a text, a query $\mathsf{rank}(q, c)$ counts the number of occurrences of character $c$ among the first $q$ characters of the text. Space-efficient methods to answer these rank queries form an important building block in many succinct data structures. For example, the FM-index is a widely used data structure that uses rank queries to locate all occurrences of a pattern in a text.
  In bioinformatics applications, the goal is usually to process a given input as fast as possible. Thus, data structures should have high throughput when used with many threads.
  Contributions. For the binary alphabet, we develop BiRank with 3.28% space overhead. It merges the central ideas of two recent papers: (1) we interleave (inline) offsets in each cache line of the underlying bit vector [Laws et al., 2024], reducing cache-misses, and (2) these offsets are to the middle of each block so that only half of them need popcounting [Gottlieb and Reinert, 2025]. In QuadRank (14.4% space overhead), we extend these techniques to the $\sigma=4$ (DNA) alphabet.
  Both data structures require only a single cache miss per query, making them highly suitable for high-throughput and memory-bound settings. To enable efficient batch-processing, we support prefetching the cache lines required to answer upcoming queries.
  Results. BiRank and QuadRank are around $1.5\times$ and $2\times$ faster than similar-overhead methods that do not use inlining. Prefetching gives an additional $2\times$ speedup, at which point the dual-channel DDR4 RAM bandwidth becomes a hard limit on the total throughput. With prefetching, both methods outperform all other methods apart from SPIDER [Laws et al., 2024] by $2\times$.
  When using QuadRank with prefetching in a toy count-only FM-index, QuadFm, this results in a smaller size and up to $4\times$ speedup over Genedex, a state-of-the-art batching FM-index implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04103v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R. Groot Koerkamp</dc:creator>
    </item>
    <item>
      <title>Improved Sparse Recovery for Approximate Matrix Multiplication</title>
      <link>https://arxiv.org/abs/2602.04386</link>
      <description>arXiv:2602.04386v1 Announce Type: new 
Abstract: We present a simple randomized algorithm for approximate matrix multiplication (AMM) whose error scales with the *output* norm $\|AB\|_F$. Given any $n\times n$ matrices $A,B$ and a runtime parameter $r\leq n$, the algorithm produces in $O(n^2(r+\log n))$ time, a matrix $C$ with total squared error $\mathbb{E}[\|C-AB\|_F^2]\le (1-\frac{r}{n})\|AB\|_F^2$, per-entry variance $\|AB\|_F^2/n^2$ and bias $\mathbb{E}[C]=\frac{r}{n}AB$. Alternatively, the algorithm can compute an *unbiased* estimation with expected total squared error $\frac{n}{r}\|{AB}\|_{F}^2$, recovering the state-of-art AMM error obtained by Pagh's TensorSketch algorithm (Pagh, 2013). Our algorithm is a log-factor faster.
  The key insight in the algorithm is a new variation of pseudo-random rotation of the input matrices (a Fast Hadamard Transform with asymmetric diagonal scaling), which redistributes the Frobenius norm of the *output* $AB$ uniformly across its entries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04386v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yahel Uffenheimer, Omri Weinstein</dc:creator>
    </item>
    <item>
      <title>Simple 2-approximations for bad triangle transversals and some hardness results for related problems</title>
      <link>https://arxiv.org/abs/2602.04463</link>
      <description>arXiv:2602.04463v1 Announce Type: new 
Abstract: Given a signed graph, the bad triangle transversal (BTT) problem asks to find the smallest number of edges that need to be removed such that the remaining graph does not have a triangle with exactly one negative edge (a bad triangle). We propose novel 2-approximations for this problem, which are much simpler and faster than a folklore adaptation of the 2-approximation by Krivelevich for finding a minimum triangle transversal in unsigned graphs. One of our algorithms also works for weighted BTT and for approximately optimal feasible solutions to the bad triangle cover LP. Using a recent result on approximating the bad triangle cover LP, we obtain a $(2+\epsilon)$ approximation in time almost equal to the time needed to find a maximal set of edge-disjoint bad triangles (which would give a standard 3-approximation). Additionally, several inapproximability results are provided. For complete signed graphs, we show that BTT is NP-hard to approximate with factor better than $\frac{2137}{2136}$. Our reduction also implies the same hardness result for related problems such as correlation clustering (cluster editing), cluster deletion and the min. strong triadic closure problem. On complete signed graphs, BTT is closely related to correlation clustering. We show that the correlation clustering optimum is at most $3/2$ times the BTT optimum, by describing a pivot procedure that transforms BTT solutions into clusters. This improves a result by Veldt, which states that their ratio is at most two.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04463v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian Adriaens, Nikolaj tatti</dc:creator>
    </item>
    <item>
      <title>Incongruity-sensitive access to highly compressed strings</title>
      <link>https://arxiv.org/abs/2602.04523</link>
      <description>arXiv:2602.04523v1 Announce Type: new 
Abstract: Random access to highly compressed strings -- represented by straight-line programs or Lempel-Ziv parses, for example -- is a well-studied topic. Random access to such strings in strongly sublogarithmic time is impossible in the worst case, but previous authors have shown how to support faster access to specific characters and their neighbourhoods. In this paper we explore whether, since better compression can impede access, we can support faster access to relatively incompressible substrings of highly compressed strings. We first show how, given a run-length compressed straight-line program (RLSLP) of size $g_{rl}$ or a block tree of size $L$, we can build an $O (g_{rl})$-space or an $O (L)$-space data structure, respectively, that supports access to any character in time logarithmic in the length of the longest repeated substring containing that character. That is, the more incongruous a character is with respect to the characters around it in a certain sense, the faster we can support access to it. We then prove a similar but more powerful and sophisticated result for parsings in which phrases' sources do not overlap much larger phrases, with the query time depending also on the number of phrases we must copy from their sources to obtain the queried character.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04523v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ferdinando Cicalese, Zsuzsanna Lipt\'ak, Travis Gagie, Gonzalo Navarro, Nicola Prezza, Cristian Urbina</dc:creator>
    </item>
    <item>
      <title>The matrix-vector complexity of $Ax=b$</title>
      <link>https://arxiv.org/abs/2602.04842</link>
      <description>arXiv:2602.04842v1 Announce Type: new 
Abstract: Matrix-vector algorithms, particularly Krylov subspace methods, are widely viewed as the most effective algorithms for solving large systems of linear equations. This paper establishes lower bounds on the worst-case number of matrix-vector products needed by such an algorithm to approximately solve a general linear system. The first main result is that, for a matrix-vector algorithm which can perform products with both a matrix and its transpose, $\Omega(\kappa \log(1/\varepsilon))$ matrix-vector products are necessary to solve a linear system with condition number $\kappa$ to accuracy $\varepsilon$, matching an upper bound for conjugate gradient on the normal equations. The second main result is that one-sided algorithms, which lack access to the transpose, must use $n$ matrix-vector products to solve an $n \times n$ linear system, even when the problem is perfectly conditioned. Both main results include explicit constants that match known upper bounds up to a factor of four. These results rigorously demonstrate the limitations of matrix-vector algorithms and confirm the optimality of widely used Krylov subspace algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04842v1</guid>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Micha{\l} Derezi\'nski, Ethan N. Epperly, Raphael A. Meyer</dc:creator>
    </item>
    <item>
      <title>Functional Stochastic Localization</title>
      <link>https://arxiv.org/abs/2602.03999</link>
      <description>arXiv:2602.03999v1 Announce Type: cross 
Abstract: Eldan's stochastic localization is a probabilistic construction that has proved instrumental to modern breakthroughs in high-dimensional geometry and the design of sampling algorithms. Motivated by sampling under non-Euclidean geometries and the mirror descent algorithm in optimization, we develop a functional generalization of Eldan's process that replaces Gaussian regularization with regularization by any positive integer multiple of a log-Laplace transform. We further give a mixing time bound on the Markov chain induced by our localization process, which holds if our target distribution satisfies a functional Poincar\'e inequality. Finally, we apply our framework to differentially private convex optimization in $\ell_p$ norms for $p \in [1, 2)$, where we improve state-of-the-art query complexities in a zeroth-order model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03999v1</guid>
      <category>math.PR</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anming Gu, Bobby Shi, Kevin Tian</dc:creator>
    </item>
    <item>
      <title>The Needle is a Thread: Finding Planted Paths in Noisy Process Trees</title>
      <link>https://arxiv.org/abs/2602.04694</link>
      <description>arXiv:2602.04694v1 Announce Type: cross 
Abstract: Motivated by applications in cybersecurity such as finding meaningful sequences of malware-related events buried inside large amounts of computer log data, we introduce the "planted path" problem and propose an algorithm to find fuzzy matchings between two trees. This algorithm can be used as a "building block" for more complicated workflows. We demonstrate usefulness of a few of such workflows in mining synthetically generated data as well as real-world ACME cybersecurity datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04694v1</guid>
      <category>cs.SI</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maya Le, Pawe{\l} Pra{\l}at, Aaron Smith, Fran\c{c}ois Th\'eberge</dc:creator>
    </item>
    <item>
      <title>A Decomposition Theorem for Dynamic Flows</title>
      <link>https://arxiv.org/abs/2407.04761</link>
      <description>arXiv:2407.04761v2 Announce Type: replace 
Abstract: The famous flow decomposition theorem of Gallai (1985) states that any static edge $s$,$d$-flow in a directed graph can be decomposed into a nonnegative linear combination of incidence vectors of paths and cycles. In this paper, we study the decomposition problem for the setting of dynamic edge $s$,$d$-flows assuming a quite general dynamic flow propagation model. We prove the following decomposition theorem: For any integrable dynamic edge $s$,$d$-flow, there exists a decomposition into a nonnegative linear combination of $s$,$d$-walk inflows and cycles of zero transit time. We show that a variant of the classical algorithmic approach of iteratively subtracting walk inflows from the current dynamic edge flow converges to a dynamic circulation and that every such circulation can be induced by inflows into cycles of zero transit time. The algorithm terminates in finite time, if there is a lower bound on the minimum edge travel times and the flow is finitely supported. We further characterize those dynamic edge flows which can be decomposed purely into nonnegative linear combinations of $s$,$d$-walk inflows.
  The proofs rely on the new concept of autonomous network loadings which allows us to describe how particles of a different walk flow would hypothetically propagate throughout the network under the fixed travel times induced by the given edge flow. We show several technical properties of this type of network loading and, as a byproduct, we also derive some general results on dynamic flows which could be of interest outside the context of this paper as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04761v2</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas Graf, Tobias Harks, Julian Schwarz</dc:creator>
    </item>
    <item>
      <title>Color Refinement for Relational Structures</title>
      <link>https://arxiv.org/abs/2407.16022</link>
      <description>arXiv:2407.16022v3 Announce Type: replace 
Abstract: Color Refinement, also known as Naive Vertex Classification, is a classical method to distinguish graphs by iteratively computing a coloring of their vertices. While it is mainly used as an imperfect way to test for isomorphism, the algorithm permeated many other, seemingly unrelated, areas of computer science. The method is algorithmically simple, and it has a well-understood distinguishing power: It is logically characterized by Cai, F\"urer and Immerman (1992), who showed that it distinguishes precisely those graphs that can be distinguished by a sentence of first-order logic with counting quantifiers and only two variables. A combinatorial characterization is given by Dvo\v{r}\'ak (2010), who shows that it distinguishes precisely those graphs that can be distinguished by the number of homomorphisms from some tree.
  In this paper, we introduce Relational Color Refinement (RCR, for short), a generalization of the Color Refinement method from graphs to arbitrary relational structures, whose distinguishing power admits the equivalent combinatorial and logical characterizations as Color Refinement has on graphs: We show that RCR distinguishes precisely those structures that can be distinguished by the number of homomorphisms from an acyclic relational structure. Further, we show that RCR distinguishes precisely those structures that can be distinguished by a sentence of the guarded fragment of first-order logic with counting quantifiers.
  Additionally, we show that for every fixed finite relational signature, RCR can be implemented to run on structures of that signature in time $O(N\cdot \log N)$, where $N$ denotes the number of tuples present in the structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16022v3</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>cs.LO</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Scheidt, Nicole Schweikardt</dc:creator>
    </item>
    <item>
      <title>What Can Be Computed Locally Revisited: First-Order Logic on Sparse Graphs in Distributed Computing</title>
      <link>https://arxiv.org/abs/2411.14825</link>
      <description>arXiv:2411.14825v3 Announce Type: replace 
Abstract: The question of 'what can be computed locally?' lies at the heart of distributed computing in networks. As established in Naor and Stockmeyer's seminal paper (STOC 1993), this question is undecidable, even for graph problems whose solutions can be checked locally. In this paper, we adopt a novel perspective on the question, by asking for which classes $\Pi$ of problems, and for which classes $\mathcal{G}$ of graphs, all problems in $\Pi$ can be solved efficiently in a distributed manner in all graphs of $\mathcal{G}$. This paper focuses on two natural candidates for such an approach, namely the class of problems expressible in first-order logic (FO), because of their intrinsic form of locality thanks to Gaifman's theorem, and the class of graphs with bounded expansion, because they form a large class of graphs encompassing, e.g., planar, bounded-treewidth, and bounded-degree graphs.
  The starting point of our work is the decade-old open question of Ne\v{s}et\v{r}il and Ossona de Mendez (Distributed Computing 2016) on the distributed complexity of local FO formula on graphs of bounded expansion, in the standard CONGEST model of distributed computing. A formula $\varphi(x)$ is local if the satisfaction of $\varphi(x)$ depends only on the $r$-neighborhood of its free variable $x$, for some fixed $r$. For instance, the formula '$x$ belongs to a triangle' is local. We resolve the open problem positively by showing that, for every local FO formula $\varphi(x)$, and for every graph class $\mathcal{G}$ of bounded expansion, there exists a deterministic algorithm that identifies, for every $n$-vertex graph $G\in \mathcal{G}$, all vertices $v$ of $G$ such that $G\models \varphi(v)$, in $O(\log n)$ rounds. When dropping the locality condition, we show that $O(D+\log n)$ rounds are sufficient for deciding any FO formula $\varphi$ on graphs of bounded expansion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14825v3</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>L\'elia Blin, Fedor V. Fomin, Pierre Fraigniaud, Sylvain Gay, Petr A. Golovach, Pedro Montealegre, Ivan Rapaport, Ioan Todinca</dc:creator>
    </item>
    <item>
      <title>Sequential Diversification with Provable Guarantees</title>
      <link>https://arxiv.org/abs/2412.10944</link>
      <description>arXiv:2412.10944v3 Announce Type: replace 
Abstract: Diversification is a useful tool for exploring large collections of information items. It has been used to reduce redundancy and cover multiple perspectives in information-search settings. Diversification finds applications in many different domains, including presenting search results of information-retrieval systems and selecting suggestions for recommender systems.
  Interestingly, existing measures of diversity are defined over \emph{sets} of items, rather than evaluating \emph{sequences} of items. This design choice comes in contrast with commonly-used relevance measures, which are distinctly defined over sequences of items, taking into account the ranking of items. The importance of employing sequential measures is that information items are almost always presented in a sequential manner, and during their information-exploration activity users tend to prioritize items with higher~ranking.
  In this paper, we study the problem of \emph{maximizing sequential diversity}. This is a new measure of \emph{diversity}, which accounts for the \emph{ranking} of the items, and incorporates \emph{item relevance} and \emph{user behavior}. The overarching framework can be instantiated with different diversity measures, and here we consider the measures of \emph{sum~diversity} and \emph{coverage~diversity}. The problem was recently proposed by Coppolillo et al.~\citep{coppolillo2024relevance}, where they introduce empirical methods that work well in practice. Our paper is a theoretical treatment of the problem: we establish the problem hardness and present algorithms with constant approximation guarantees for both diversity measures we consider. Experimentally, we demonstrate that our methods are competitive against strong baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10944v3</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Honglian Wang, Sijing Tu, Aristides Gionis</dc:creator>
    </item>
    <item>
      <title>Fast Order Statistics with Group Inequality Testing</title>
      <link>https://arxiv.org/abs/2507.12634</link>
      <description>arXiv:2507.12634v2 Announce Type: replace 
Abstract: Suppose that a group test operation is available for checking order relations in a set, can this speed up problems like finding the minimum/maximum element, determining the rank of element, and computing order statistics? We consider a one-sided group inequality test to be available, where queries are of the form $u \le_Q V$ or $V \le_Q u$, and the answer is `yes' if and only if there is some $v \in V$ such that $u \le v$ or $v \le u$, respectively. We restrict attention to total orders and focus on query-complexity; for min or max finding, we give a Las Vegas algorithm that makes $\mathcal{O}(\log^2 n)$ expected queries. We observe that rank determination can be solved with existing ``defect-counting'' algorithms, but also give a simple Monte Carlo approximation algorithm with expected query complexity $\tilde{\mathcal{O}}(\frac{1}{\delta^2} \log \frac{1}{\epsilon})$, where $1-\epsilon$ is the probability that the algorithm succeeds and we allow a relative error of $1 \pm \delta$ for $\delta &gt; 0$ in the estimated rank. We then give a Monte Carlo algorithm for approximate selection that has expected query complexity $\tilde{\mathcal{O}}(\frac{1}{\delta^4}\log \frac{1}{\epsilon \delta^2} )$; it has probability at least $\frac{1}{2}$ to output an element $x$, and if so, $x$ has the desired approximate rank with probability $1-\epsilon$. Keywords: Order statistics, Group inequality testing, Randomized algorithms</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12634v2</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Adiesha Liyanage, Brendan Mumey, Braeden Sopp</dc:creator>
    </item>
    <item>
      <title>On the Parallel Complexity of Identifying Groups and Quasigroups via Decompositions</title>
      <link>https://arxiv.org/abs/2508.06478</link>
      <description>arXiv:2508.06478v2 Announce Type: replace 
Abstract: In this paper, we investigate the computational complexity of isomorphism testing for finite groups and quasigroups, given by their multiplication tables. We crucially take advantage of their various decompositions to show the following:
  - We first consider the class of groups that admit direct product decompositions, where each indecompsable factor is $O(1)$-generated, and either perfect or centerless. We show any group in this class is identified by the $O(1)$-dimensional count-free Weisfeiler--Leman (WL) algorithm with $O(\log \log n)$ rounds, and the $O(1)$-dimensional counting WL algorithm with $O(1)$ rounds. Consequently, the isomorphism problem for this class is in $\textsf{L}$. This improves upon the previous upper bound of $\textsf{TC}^{1}$, which was obtained using $O(\log n)$ rounds of the $O(1)$-dimensional counting WL (Grochow and Levet; FCT 2023, \textit{J. Comput. Syst. Sci.} 2026).
  - We next consider more generally, the class of groups where each indecomposable factor is $O(1)$-generated. We exhibit an $\textsf{AC}^{3}$ canonical labeling procedure for this class. Here, we accomplish this by showing that in the multiplication table model, the direct product decomposition can be computed in $\textsf{AC}^{3}$, parallelizing the work of Kayal and Nezhmetdinov (ICALP 2009).
  - Isomorphism testing between a central quasigroup $G$ and an arbitrary quasigroup $H$ is in $\textsf{NC}$. Here, we take advantage of the fact that central quasigroups admit an affine decomposition in terms of an underlying Abelian group. Only the trivial bound of $n^{\log(n)+O(1)}$-time was previously known for isomorphism testing of central quasigroups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06478v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>math.GR</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dan Johnson, Michael Levet, Petr Vojt\v{e}chovsk\'y, Brett Widholm</dc:creator>
    </item>
    <item>
      <title>Fast-Convergent Proximity Graphs for Approximate Nearest Neighbor Search</title>
      <link>https://arxiv.org/abs/2510.05975</link>
      <description>arXiv:2510.05975v5 Announce Type: replace 
Abstract: Approximate nearest neighbor (ANN) search in high-dimensional metric spaces is a fundamental problem with many applications. Over the past decade, proximity graph (PG)-based indexes have demonstrated superior empirical performance over alternatives. However, these methods often lack theoretical guarantees regarding the quality of query results, especially in the worst-case scenarios. In this paper, we introduce the {\alpha}-convergent graph ({\alpha}-CG), a new PG structure that employs a carefully designed edge pruning rule. This rule eliminates candidate neighbors for each data point p by applying the shifted-scaled triangle inequalities among p, its existing out-neighbors, and new candidates. If the distance between the query point q and its exact nearest neighbor v* is at most {\tau} for some constant {\tau} &gt; 0, our {\alpha}-CG finds the exact nearest neighbor in poly-logarithmic time, assuming bounded intrinsic dimensionality for the dataset; otherwise, it can find an ANN in the same time. To enhance scalability, we develop the {\alpha}-convergent neighborhood graph ({\alpha}-CNG), a practical variant that applies the pruning rule locally within each point's neighbors. We also introduce optimizations to reduce the index construction time. Experimental results show that our {\alpha}-CNG outperforms existing PGs on real-world datasets. For most datasets, {\alpha}-CNG can reduce the number of distance computations and search steps by over 15% and 45%, respectively, when compared with the best-performing baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05975v5</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Binhong Li, Xiao Yan, Shangqi Lu</dc:creator>
    </item>
    <item>
      <title>Approximate minimization of interpretations in fuzzy description logics under the G\"odel semantics</title>
      <link>https://arxiv.org/abs/2510.21423</link>
      <description>arXiv:2510.21423v2 Announce Type: replace 
Abstract: The problem of minimizing fuzzy interpretations in fuzzy description logics (FDLs) is important both theoretically and practically. For instance, fuzzy or weighted social networks can be modeled as fuzzy interpretations, where individuals represent actors and roles capture interactions. Minimizing such interpretations yields more compact representations, which can significantly improve the efficiency of reasoning and analysis tasks in knowledge-based systems. We present the first algorithm that minimizes a finite fuzzy interpretation while preserving fuzzy concept assertions in FDLs without the Baaz projection operator and the universal role, under the G\"odel semantics. The considered class of FDLs ranges from the sublogic of $f\!\mathcal{ALC}$ without the union operator and universal restriction to the FDL that extends $f\!\mathcal{ALC}_{reg}$ with inverse roles and nominals. Our algorithm is given in an extended form that supports approximate preservation: it minimizes a finite fuzzy interpretation $\mathcal{I}$ while preserving fuzzy concept assertions up to a degree $\gamma \in (0,1]$. Its time complexity is $O((m\log{l} + n)\log{n})$, where $n$ is the size of the domain of $\mathcal{I}$, $m$ is the number of nonzero instances of atomic roles in $\mathcal{I}$, and $l$ is the number of distinct fuzzy values used in such instances plus 2. Methodologically, our approach fundamentally differs from existing ones, as it avoids quotient constructions traditionally employed for minimizing fuzzy interpretations and fuzzy automata.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21423v2</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linh Anh Nguyen</dc:creator>
    </item>
    <item>
      <title>Refining the Complexity Landscape of Speed Scaling: Hardness and Algorithms</title>
      <link>https://arxiv.org/abs/2512.17663</link>
      <description>arXiv:2512.17663v2 Announce Type: replace 
Abstract: We study the computational complexity of scheduling jobs on a single speed-scalable processor with the objective of capturing the trade-off between the (weighted) flow time and the energy consumption. This trade-off has been extensively explored in the literature through a number of problem formulations that differ in the specific job characteristics and the precise objective function. Nevertheless, the computational complexity of four important problem variants has remained unresolved and was explicitly identified as an open question in prior work. In this paper, we settle the complexity of these variants.
  More specifically, we prove that the problem of minimizing the objective of total (weighted) flow time plus energy is NP-hard for the cases of (i) unit-weight jobs with arbitrary sizes, and (ii)~arbitrary-weight jobs with unit sizes. These results extend to the objective of minimizing the total (weighted) flow time subject to an energy budget and hold even when the schedule is required to adhere to a given priority ordering.
  In contrast, we show that when a completion-time ordering is provided, the same problem variants become polynomial-time solvable. The latter result highlights the subtle differences between priority and completion orderings for the problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17663v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonios Antoniadis, Denise Graafsma, Ruben Hoeksma, Maria Vlasiou</dc:creator>
    </item>
    <item>
      <title>On the Complexity of Maximal/Closed Frequent Tree Mining for Bounded Height Trees</title>
      <link>https://arxiv.org/abs/2602.03436</link>
      <description>arXiv:2602.03436v2 Announce Type: replace 
Abstract: In this paper, we address the problem of enumerating all frequent maximal/closed trees. This is a classical and central problem in data mining. Although many practical algorithms have been developed for this problem, its complexity under ``realistic assumptions'' on tree height has not been clarified. More specifically, while it was known that the mining problem becomes hard when the tree height is at least 60, the complexity for cases where the tree height is smaller has not yet been clarified. We resolve this gap by establishing results for these tree mining problems under several settings, including ordered and unordered trees, as well as maximal and closed variants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03436v2</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kenta Komoto, Kazuhiro Kurita, Hirotaka Ono</dc:creator>
    </item>
    <item>
      <title>The Lov\'asz Theta Function for Recovering Planted Clique Covers and Graph Colorings</title>
      <link>https://arxiv.org/abs/2310.00257</link>
      <description>arXiv:2310.00257v2 Announce Type: replace-cross 
Abstract: The problems of computing graph colorings and clique covers are central challenges in combinatorial optimization. Both of these are known to be NP-hard, and thus computationally intractable in the worst-case instance. A prominent approach for computing approximate solutions to these problems is the celebrated Lov\'asz theta function $\vartheta(G)$, which is specified as the solution of a semidefinite program (SDP), and hence tractable to compute. In this work, we move beyond the worst-case analysis and set out to understand whether the Lov\'asz theta function recovers clique covers for random instances that have a latent clique cover structure, possibly obscured by noise. We answer this question in the affirmative and show that for graphs generated from the planted clique model we introduce in this work, the SDP formulation of $\vartheta(G)$ has a unique solution that reveals the underlying clique-cover structure with high-probability. The main technical step is an intermediate result where we prove a deterministic condition of recovery based on an appropriate notion of sparsity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.00257v2</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.CO</category>
      <category>math.IT</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaxin Hou, Yong Sheng Soh, Antonios Varvitsiotis</dc:creator>
    </item>
    <item>
      <title>Sequential Selection with Expirations</title>
      <link>https://arxiv.org/abs/2406.15691</link>
      <description>arXiv:2406.15691v2 Announce Type: replace-cross 
Abstract: Motivated by applications where impatience is pervasive and evaluation times are uncertain, we study a selection model where options may expire at an unknown point in time and evaluation times are stochastic. Initially, the decision-maker (DM) has access to $n$ options with known non-negative values: these options have unknown stochastic evaluation and expiration times with known distributional information, which we assume to be independent. When the DM is free, we can select an available option that occupies the DM for an unknown amount of time and collect its value. The objective is to maximize the expected total value obtained from options selected by the DM. Natural formulations of this problem suffer from the curse of dimensionality. In fact, this problem is NP-hard even in the deterministic case. Hence, we focus on efficiently computable approximation algorithms that can provide high expected reward compared to the optimal expected value. Towards this end, we first provide a compact linear programming (LP) relaxation that gives an upper bound on the expected value obtained by the optimal policy. Then we design a polynomial-time algorithm that is nearly a $(1/2)\cdot (1-1/e)$-approximation to the optimal LP value (so also to the optimal expected value). We next shift our focus to the case of independent and identically distributed (i.i.d.) evaluation times. In this case, we show that the greedy policy that always selects the highest-valued option whenever the DM is free obtains a $1/2$-approximation to the optimal expected value. Our approaches extend effortlessly, and we demonstrate their flexibility by providing approximations to natural extensions of our problem. Finally, we evaluate our LP-based policies and the greedy policy empirically on synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15691v2</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yihua Xu, Rohan Ghuge, Sebastian Perez-Salazar</dc:creator>
    </item>
    <item>
      <title>Differentially Private Sampling via Reveal-or-Obscure</title>
      <link>https://arxiv.org/abs/2504.14696</link>
      <description>arXiv:2504.14696v2 Announce Type: replace-cross 
Abstract: We introduce a differentially private (DP) algorithm called Reveal-or-Obscure (ROO) to generate a single representative sample from a dataset of n i.i.d. observations from an unknown distribution. Unlike methods that add explicit noise to the estimated empirical distribution, ROO achieves $\epsilon$-differential privacy by choosing whether to "reveal" or "obscure" the empirical distribution with a fixed probability $q$. While our proposed mechanism is structurally identical to an algorithm proposed by Cheu and Nayak, we prove a strictly better bound on the sampling complexity than that established in their theorem. Building on this framework, we propose a novel generalized sampler called Data-Specific ROO (DS-ROO), where the obscuring probability $q$ is a function of the empirical distribution. We show that when the dataset contains enough samples from every element of the alphabet, DS-ROO can achieve $\epsilon$-DP while obscuring much less. In addition, we provide tight upper bounds on the utility of DS-ROO in terms of total variation distance. Our results show that under the same privacy budget, DS-ROO can achieve better utility than state-of-the-art private samplers and vanilla ROO, with total variation distance decaying exponentially in dataset size $n$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14696v2</guid>
      <category>cs.IT</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naima Tasnim, Atefeh Gilani, Lalitha Sankar, Oliver Kosut</dc:creator>
    </item>
    <item>
      <title>Universal Solvability for Robot Motion Planning on Graphs</title>
      <link>https://arxiv.org/abs/2506.18755</link>
      <description>arXiv:2506.18755v2 Announce Type: replace-cross 
Abstract: We study the Universal Solvability of Robot Motion Planning on Graphs (USolR) problem: given an undirected graph G = (V, E) and p robots, determine whether any arbitrary configuration of the robots can be transformed into any other arbitrary configuration via a sequence of valid, collision-free moves. We design a canonical accumulation procedure that maps arbitrary configurations to configurations that occupy a fixed subset of vertices, enabling us to analyze configuration reachability in terms of equivalence classes. We prove that in instances that are not universally solvable, at least half of all configurations are unreachable from a given one, and leverage this to design an efficient randomized algorithm with one-sided error, which can be derandomized with a blow-up in the running time by a factor of p. Further, we optimize our deterministic algorithm by using the structure of the input graph G = (V, E), achieving a running time of O(p * (|V| + |E|)) in sparse graphs and O(|V| + |E|) in dense graphs. Finally, we consider the Graph Edge Augmentation for Universal Solvability (EAUS) problem, where given a connected graph G that is not universally solvable for p robots, the question is to check if for a given budget b, at most b edges can be added to G to make it universally solvable for p robots. We provide an upper bound of p - 2 on b for general graphs. On the other hand, we also provide examples of graphs that require Theta(p) edges to be added. We further study the Graph Vertex and Edge Augmentation for Universal Solvability (VEAUS) problem, where a vertices and b edges can be added, and we provide lower bounds on a and b.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18755v2</guid>
      <category>cs.CC</category>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anubhav Dhar, Ashlesha Hota, Sudeshna Kolay, Pranav Nyati, Tanishq Prasad</dc:creator>
    </item>
    <item>
      <title>Hallucination is a Consequence of Space-Optimality: A Rate-Distortion Theorem for Membership Testing</title>
      <link>https://arxiv.org/abs/2602.00906</link>
      <description>arXiv:2602.00906v4 Announce Type: replace-cross 
Abstract: Large language models often hallucinate with high confidence on "random facts" that lack inferable patterns. We formalize the memorization of such facts as a membership testing problem, unifying the discrete error metrics of Bloom filters with the continuous log-loss of LLMs. By analyzing this problem in the regime where facts are sparse in the universe of plausible claims, we establish a rate-distortion theorem: the optimal memory efficiency is characterized by the minimum KL divergence between score distributions on facts and non-facts. This theoretical framework provides a distinctive explanation for hallucination: even with optimal training, perfect data, and a simplified "closed world" setting, the information-theoretically optimal strategy under limited capacity is not to abstain or forget, but to assign high confidence to some non-facts, resulting in hallucination. We validate this theory empirically on synthetic data, showing that hallucinations persist as a natural consequence of lossy compression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00906v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anxin Guo, Jingwei Li</dc:creator>
    </item>
    <item>
      <title>Totally $\Delta$-Modular Tree Decompositions of Graphic Matrices for Integer Programming</title>
      <link>https://arxiv.org/abs/2602.01499</link>
      <description>arXiv:2602.01499v2 Announce Type: replace-cross 
Abstract: We introduce the tree-decomposition-based parameter totally $\Delta$-modular treewidth (TDM-treewidth) for matrices with two nonzero entries per row. We show how to solve integer programs whose matrices have bounded TDM-treewidth when variables are bounded. This extends previous graph-based decomposition parameters for matrices with at most two nonzero entries per row to include matrices with entries outside of $\{-1,0,1\}$. We also give an analogue of the Grid Theorem of Robertson and Seymour for matrices of bounded TDM-treewidth in the language of rooted signed graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01499v2</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Caleb McFarland</dc:creator>
    </item>
  </channel>
</rss>
