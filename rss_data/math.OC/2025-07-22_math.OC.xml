<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 23 Jul 2025 04:00:56 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Intrinsic Riemannian Proximal Gradient Method for Convex Optimization</title>
      <link>https://arxiv.org/abs/2507.16055</link>
      <description>arXiv:2507.16055v1 Announce Type: new 
Abstract: We consider a class of (possibly strongly) geodesically convex optimization problems on Hadamard manifolds, where the objective function splits into the sum of a smooth and a possibly nonsmooth function. We introduce an intrinsic convex Riemannian proximal gradient (CRPG) method that employs the manifold proximal map for the nonsmooth step, without operating in the embedding or tangent space. A sublinear convergence rate for convex problems and a linear convergence rate for strongly convex problems is established, and we derive fundamental proximal gradient inequalities that generalize the Euclidean case. Our numerical experiments on hyperbolic spaces and manifolds of symmetric positive definite matrices demonstrate substantial computational advantages over existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16055v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.DG</category>
      <category>math.NA</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ronny Bergmann, Hajg Jasa, Paula John, Max Pfeffer</dc:creator>
    </item>
    <item>
      <title>Conservative fusion of unbiased partial state estimates: CI is optimal</title>
      <link>https://arxiv.org/abs/2507.16216</link>
      <description>arXiv:2507.16216v1 Announce Type: new 
Abstract: We show that Covariance Intersection (CI) is optimal amongst all conservative unbiased linear fusion rules also in the general case of information fusion of two unbiased partial state estimates, significantly generalizing the known optimality result for fusion of full state estimates. In fact, we prove the much stronger result that three different optimization problems are equivalent, namely the abstract optimal conservative unbiased linear information fusion problem with respect to a strictly isotone cost function, the scalar Covariance Intersection (CI) problem, and a simple semi-definite program (SDP). We provide a general solvability condition for these problems as well as equations characterizing the optimal solutions for the matrix determinant and matrix trace cost functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16216v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jochen Trumpf, Behzad Zamani, Chris Manzie</dc:creator>
    </item>
    <item>
      <title>Physics-aware Truck and Drone Delivery Planning Using Optimization &amp; Machine Learning</title>
      <link>https://arxiv.org/abs/2507.16259</link>
      <description>arXiv:2507.16259v1 Announce Type: new 
Abstract: Combining an energy-efficient drone with a high-capacity truck for last-mile package delivery can benefit operators and customers by reducing delivery times and environmental impact. However, directly integrating drone flight dynamics into the combinatorially hard truck route planning problem is challenging. Simplified models that ignore drone flight physics can lead to suboptimal delivery plans. We propose an integrated formulation for the joint problem of truck route and drone trajectory planning and a new end-to-end solution approach that combines optimization and machine learning to generate high-quality solutions in practical online runtimes. Our solution method trains neural network predictors based on offline solutions to the drone trajectory optimization problem instances to approximate drone flight times, and uses these approximations to optimize the overall truck-and-drone delivery plan by augmenting an existing order-first-split-second heuristic. Our method explicitly incorporates key kinematics and energy equations in drone trajectory optimization, and thereby outperforms state-of-the-art benchmarks that ignore drone flight physics. Extensive experimentation using synthetic datasets and real-world case studies shows that the integration of drone trajectories into package delivery planning substantially improves system performance in terms of tour duration and drone energy consumption. Our modeling and computational framework can help delivery planners achieve annual savings worth millions of dollars while also benefiting the environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16259v1</guid>
      <category>math.OC</category>
      <category>cs.RO</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yineng Sun, Armin F\"ugenschuh, Vikrant Vaze</dc:creator>
    </item>
    <item>
      <title>Learning Acceleration Algorithms for Fast Parametric Convex Optimization with Certified Robustness</title>
      <link>https://arxiv.org/abs/2507.16264</link>
      <description>arXiv:2507.16264v1 Announce Type: new 
Abstract: We develop a machine-learning framework to learn hyperparameter sequences for accelerated first-order methods (e.g., the step size and momentum sequences in accelerated gradient descent) to quickly solve parametric convex optimization problems with certified robustness. We obtain a strong form of robustness guarantee -- certification of worst-case performance over all parameters within a set after a given number of iterations -- through regularization-based training. The regularization term is derived from the performance estimation problem (PEP) framework based on semidefinite programming, in which the hyperparameters appear as problem data. We show how to use gradient-based training to learn the hyperparameters for several first-order methods: accelerated versions of gradient descent, proximal gradient descent, and alternating direction method of multipliers. Through various numerical examples from signal processing, control, and statistics, we demonstrate that the quality of the solution can be dramatically improved within a budget of iterations, while also maintaining strong robustness guarantees. Notably, our approach is highly data-efficient in that we only use ten training instances in all of the numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16264v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Rajiv Sambharya, Jinho Bok, Nikolai Matni, George Pappas</dc:creator>
    </item>
    <item>
      <title>Spectral Methods for Polynomial Optimization</title>
      <link>https://arxiv.org/abs/2507.16272</link>
      <description>arXiv:2507.16272v1 Announce Type: new 
Abstract: We present a hierarchy of tractable relaxations to obtain lower bounds on the minimum value of a polynomial over a constraint set defined by polynomial equations. In contrast to previous convex relaxation techniques for this problem, our method is based on computing the smallest generalized eigenvalue of a pair of matrices derived from the problem data, which can be accomplished for large problem instances using off-the-shelf software. We characterize the algebraic structure in a problem that facilitates the application of our framework, and we observe that our method is applicable for all polynomial optimization problems with bounded constraint sets. Our construction also yields a nested sequence of structured convex outer approximations of a bounded algebraic variety with the property that linear optimization over each approximation reduces to an eigenvalue computation. Finally, we present numerical experiments on representative problems in which we demonstrate the scalability of our approach compared to convex relaxation methods derived from sums-of-squares certificates of nonnegativity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16272v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elvira Moreno, Venkat Chandrasekaran</dc:creator>
    </item>
    <item>
      <title>A Distributional View of High Dimensional Optimization</title>
      <link>https://arxiv.org/abs/2507.16315</link>
      <description>arXiv:2507.16315v1 Announce Type: new 
Abstract: This PhD thesis presents a distributional view of optimization in place of a worst-case perspective. We motivate this view with an investigation of the failure point of classical optimization. Subsequently we consider the optimization of a randomly drawn objective function. This is the setting of Bayesian Optimization. After a review of Bayesian optimization we outline how such a distributional view may explain predictable progress of optimization in high dimension. It further turns out that this distributional view provides insights into optimal step size control of gradient descent. To enable these results, we develop mathematical tools to deal with random input to random functions and a characterization of non-stationary isotropic covariance kernels. Finally, we outline how assumptions about the data, specifically exchangability, can lead to random objective functions in machine learning and analyze their landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16315v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felix Benning</dc:creator>
    </item>
    <item>
      <title>Discrete-Time LQ Stochastic Two Person Nonzero Sum Difference Games With Random Coefficients:~Closed-Loop Nash Equilibrium</title>
      <link>https://arxiv.org/abs/2507.16412</link>
      <description>arXiv:2507.16412v1 Announce Type: new 
Abstract: This paper investigates closed-loop Nash equilibria for discrete-time linear-quadratic (LQ) stochastic nonzero-sum difference games with random coefficients. Unlike existing works, we consider randomness in both state dynamics and cost functionals, leading to a complex structure of fully coupled cross-coupled stochastic Riccati equations (CCREs). The key contributions lie in characterizing the equilibrium via state-feedback strategies derived by decoupling stochastic Hamiltonian systems governed by two symmetric CCREs-these random coefficients induce a higher-order nonlinear backward stochastic difference equation (BS$\triangle$E) system, fundamentally differing from deterministic counterparts. Under minimal regularity conditions, we establish necessary and sufficient conditions for closed-loop Nash equilibrium existence, contingent on the regular solvability of CCREs without requiring strong assumptions. Solutions are constructed using a dynamic programming principle (DPP), linking equilibrium strategies to coupled Lyapunov-type equations. Our analysis resolves critical challenges in modeling inherent randomness and provides a unified framework for dynamic decision-making under uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16412v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Qingxin Meng, Yiwei Wu</dc:creator>
    </item>
    <item>
      <title>Inexact Levenberg-Marquardt methods under H\"{o}lder metric subregularity</title>
      <link>https://arxiv.org/abs/2507.16461</link>
      <description>arXiv:2507.16461v1 Announce Type: new 
Abstract: This paper investigates two inexact Levenberg-Marquardt (LM) methods for solving systems of nonlinear equations. Both approaches compute approximate search directions by solving the LM linear system inexactly, subject to specific residual-based conditions. The first method uses an adaptive scheme to update the LM parameter, and we establish its local superlinear convergence under H\"older metric subregularity and local H\"older continuity of the gradient. The second method combines an inexact LM step with a nonmonotone quadratic regularization strategy. For this variant, we prove global convergence under the assumption of Lipschitz continuous gradients and derive a worst-case global complexity bound, showing that an approximate stationary point can be found in $\mathcal{O}(\epsilon^{-2})$ function and gradient evaluations. Finally, we justify the use of the LSQR algorithm for efficiently solving the linear systems involved, which is used in our numerical experiment on several nonlinear systems, including those appearing in real-world biochemical reaction networks, monotone and nonlinear equations, and image deblurring problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16461v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bas Symoens, Morteza Rahimi, Masoud Ahookhosh</dc:creator>
    </item>
    <item>
      <title>The Sweet Spot of Bound Tightening for Topology Optimization</title>
      <link>https://arxiv.org/abs/2507.16496</link>
      <description>arXiv:2507.16496v1 Announce Type: new 
Abstract: Topology optimization has emerged as a powerful and increasingly relevant strategy for enhancing the flexibility and efficiency of power system operations. However, solving these problems is computationally demanding due to their combinatorial nature and the use of big-M formulations. Optimization-based bound tightening (OBBT) is a well-known strategy to improve the solution of mixed-integer linear programs (MILPs) by computing tighter bounds for continuous variables. Yet, existing OBBT approaches in topology optimization typically relax all switching decisions in the bounding subproblems, leading to excessively loose feasible regions and limited bound improvements. In this work, we propose a topology-aware bound tightening method that uses network structure to determine which switching variables to relax. Through extensive computational experiments on the IEEE 118-bus system, we find that keeping a small subset of switching variables as binary, while relaxing the rest, strikes a sweet spot between the computational effort required to solve the bounding problems and the tightness of the resulting bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16496v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Salvador Pineda, Juan Miguel Morales</dc:creator>
    </item>
    <item>
      <title>A robust and stable phase field method for structural topology optimization</title>
      <link>https://arxiv.org/abs/2507.16519</link>
      <description>arXiv:2507.16519v1 Announce Type: new 
Abstract: This paper presents a novel phase-field-based methodology for solving minimum compliance problems in topology optimization under fixed external loads and body forces. The proposed framework characterizes the optimal structure through an order parameter function, analogous to phase-field models in materials science, where the design domain and its boundary are intrinsically represented by the order parameter function. The topology optimization problem is reformulated as a constrained minimization problem with respect to this order parameter, requiring simultaneous satisfaction of three critical properties: bound preservation, volume conservation, and monotonic objective functional decay throughout the optimization process. The principal mathematical challenge arises from handling domain-dependent body forces, which necessitates the development of a constrained optimization framework. To address this, we develop an operator-splitting algorithm incorporating Lagrange multipliers, enhanced by a novel limiter mechanism. This hybrid approach guarantees strict bound preservation, exact volume conservation, and correct objective functional decaying rate. Numerical implementation demonstrates the scheme's robustness through comprehensive 2D and 3D benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16519v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huangxin Chen, Piaopiao Dong, Dong Wang, Xiao-Ping Wang</dc:creator>
    </item>
    <item>
      <title>Study on Control Problem of a Impulsive Neutral Integro-Differential Equations with Fading Memory</title>
      <link>https://arxiv.org/abs/2507.16560</link>
      <description>arXiv:2507.16560v1 Announce Type: new 
Abstract: This article addresses control problems for semilinear impulsive neutral integro-differential equations with memory in a Banach space. It investigates the approximate controllability of linear and semilinear systems and proves the establishment of mild solutions in the semilinear setting. The approach involves constructing a resolvent family for the corresponding integro-differential equation of linear type without memory. The results for the linear system are established first, then extended to the semilinear scenario, followed by a detailed example to illustrate the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16560v1</guid>
      <category>math.OC</category>
      <category>math.DS</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Garima Gupta, Jaydev Dabas</dc:creator>
    </item>
    <item>
      <title>Mean-Field Stochastic Linear-Quadratic Optimal Controls: Roles of Expectation and Conditional Expectation Operators</title>
      <link>https://arxiv.org/abs/2507.16582</link>
      <description>arXiv:2507.16582v1 Announce Type: new 
Abstract: This paper investigates a mean-field linear-quadratic optimal control problem where the state dynamics and cost functional incorporate both expectation and conditional expectation terms. We explicitly derive the pre-committed, na\"{\i}ve, and equilibrium solutions and establish the well-posedness of the associated Riccati equations. This reveals how the expectation and conditional expectation operators influence time-consistency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16582v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanxiao Wang, Jiongmin Yong</dc:creator>
    </item>
    <item>
      <title>An inertial iteratively regularized extragradient method for bilevel variational inequality problems</title>
      <link>https://arxiv.org/abs/2507.16640</link>
      <description>arXiv:2507.16640v1 Announce Type: new 
Abstract: We study a bilevel variational inequality problem where the feasible set is itself the solution set of another variational inequality. Motivated by the difficulty of computing projections onto such sets, we consider a regularized extragradient method, as proposed by Samadi and Yousefian (2025), which operates over a simpler constraint set. Building on this framework, we introduce an inertial variant (called IneIREG) that incorporates momentum through extrapolation steps. We establish iteration-complexity bounds for the general (non-strongly monotone) case under both constant and diminishing regularization, and derive improved results under strong monotonicity assumptions. Our analysis extends and refines the results of the previous work by capturing both inertial and regularization effects within a unified framework. Preliminary numerical experiments are also presented to illustrate the behavior of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16640v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. Marques Alves, Kangming Chen, Ellen H. Fukuda</dc:creator>
    </item>
    <item>
      <title>On the Worst-Case Analysis of Cyclic Block Coordinate Descent type Algorithms</title>
      <link>https://arxiv.org/abs/2507.16675</link>
      <description>arXiv:2507.16675v1 Announce Type: new 
Abstract: We study the worst-case behavior of Block Coordinate Descent (BCD) type algorithms for unconstrained minimization of coordinate-wise smooth convex functions. This behavior is indeed not completely understood, and the practical success of these algorithms is not fully explained by current convergence analyses. We extend the recently proposed Performance Estimation Problem (PEP) approach to convex coordinate-wise smooth functions by proposing necessary interpolation conditions. We then exploit this to obtain improved numerical upper bounds on the worst-case convergence rate of three different BCD algorithms, namely Cyclic Coordinate Descent (CCD), Alternating Minimization (AM), and a Cyclic version of the Random Accelerated Coordinate Descent introduced in Fercoq and Richt\'arik (2015) (CACD), substantially outperforming the best current bounds in some situations. In addition, we show the convergence of the CCD algorithm with more natural assumptions in the context of convex optimization than those typically made in the literature. Our methodology uncovers a number of phenomena, some of which can be formally established. These include a scale-invariance property of the worst case of CCD with respect to the coordinate-wise smoothness constants and a lower bound on the worst-case performance of CCD which is equal to the number of blocks times the worst-case of full gradient descent over the class of smooth convex functions. We also adapt our framework to the analysis of random BCD algorithms, and present numerical results showing that the standard acceleration scheme in Fercoq and Richt\'arik (2015) appears to be inefficient for deterministic algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16675v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yassine Kamri, Fran\c{c}ois Glineur, Julien M. Hendrickx, Ion Necoara</dc:creator>
    </item>
    <item>
      <title>Enhancing Stability of Physics-Informed Neural Network Training Through Saddle-Point Reformulation</title>
      <link>https://arxiv.org/abs/2507.16008</link>
      <description>arXiv:2507.16008v1 Announce Type: cross 
Abstract: Physics-informed neural networks (PINNs) have gained prominence in recent years and are now effectively used in a number of applications. However, their performance remains unstable due to the complex landscape of the loss function. To address this issue, we reformulate PINN training as a nonconvex-strongly concave saddle-point problem. After establishing the theoretical foundation for this approach, we conduct an extensive experimental study, evaluating its effectiveness across various tasks and architectures. Our results demonstrate that the proposed method outperforms the current state-of-the-art techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16008v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dmitry Bylinkin, Mikhail Aleksandrov, Savelii Chezhegov, Aleksandr Beznosikov</dc:creator>
    </item>
    <item>
      <title>Pontryagin Maximum Principle for McKean-Vlasov Reaction-Diffusion Equations</title>
      <link>https://arxiv.org/abs/2507.16288</link>
      <description>arXiv:2507.16288v1 Announce Type: cross 
Abstract: We consider the stochastic control of a semi-linear stochastic partial differential equations (SPDE) of McKean-Vlasov type. Based on a recent novel approach to the Lions derivative for Banach space valued functions, we prove the Gateaux differentiability of the control to state map and, using adjoint calculus, we derive explicit representations of the gradient of the cost functional and a Pontryagin maximum principle. On the way, we also prove a novel existence and uniqueness result for linear McKean-Vlasov backward SPDE. Furthermore, for deterministic controls, we prove the existence of optimal controls using a martingale approach and a novel compactness method. This result is complemented in the appendix with a rigorous proof of folklore results on the compactness method in the variational approach to SPDE. Our setting uses the variational approach to SPDE with monotone coefficients, allowing for a polynomial perturbation and allowing the drift and diffusion coefficients to depend on the state, the distribution of the state and the control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16288v1</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johan Benedikt Spille, Wilhelm Stannat</dc:creator>
    </item>
    <item>
      <title>Graphical Analysis of Nonlinear Multivariable Feedback Systems</title>
      <link>https://arxiv.org/abs/2507.16513</link>
      <description>arXiv:2507.16513v1 Announce Type: cross 
Abstract: Scaled Relative Graphs (SRGs) provide a novel graphical frequency-domain method for the analysis of nonlinear systems. There have been recent efforts to generalize SRG analysis to Multiple-Input Multiple-Output (MIMO) systems. However, these attempts yielded only results for square systems, and in some cases, only methods applicable for Linear Time-Invariant (LTI) systems. In this paper, we develop a complete SRG framework for the analysis of MIMO systems, which may be nonlinear and non-square. The key element is the embedding of operators to a space of operators acting on a common Hilbert space, while restricting the input space to the original input dimension. We develop interconnection rules that use restricted input spaces and stability theorems to guarantee causality, well-posedness and (incremental) $L_2$-gain bounds for the overall interconnection. We show utilization of the proposed theoretical concepts on the analysis of nonlinear systems in a linear fractional representation form, which is a rather general class of systems with a representation form directly utilizable for control. Moreover, we provide formulas for the computation of MIMO SRGs of stable LTI operators and diagonal static nonlinear operators. Finally, we demonstrate the capabilities of our proposed approach on several examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16513v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julius P. J. Krebbekx, Roland T\'oth, Amritam Das</dc:creator>
    </item>
    <item>
      <title>Integral action for bilinear systems with application to counter current heat exchanger</title>
      <link>https://arxiv.org/abs/2507.16553</link>
      <description>arXiv:2507.16553v1 Announce Type: cross 
Abstract: In this study, we propose a robust control strategy for a counter-current heat exchanger. The primary objective is to regulate the outlet temperature of one fluid stream by manipulating the flow rate of the second counter-current fluid stream. By leveraging the energy balance equations, we develop a structured bilinear system model derived by using a uniform spatial discretization of each stream into a cascade of homogeneous volumes and by considering the heat transfer and convective phenomena within the exchanger. We introduce three control strategies: (i) an enhanced forwarding-based controller, (ii) an output feedback controller incorporating a state observer, and (iii) a purely integral control law. The effectiveness of the proposed control strategy is validated through real experiments on a real heat exchanger.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16553v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Francesco Ripa, Daniele Astolfi, Boussad Hamroun, Diego Regruto</dc:creator>
    </item>
    <item>
      <title>Port-Hamiltonian Realizations of Nonminimal Linear Time Invariant Systems</title>
      <link>https://arxiv.org/abs/2201.05355</link>
      <description>arXiv:2201.05355v3 Announce Type: replace 
Abstract: Numerical methods for developing port-Hamiltonian representations of general linear time-invariant systems are studied. The approach extends previous port-Hamiltonian characterizations to include the general non-minimal case and the case where the feedthrough term fails to have an invertible symmetric part. The resulting construction is able to identify infeasibility when the system fails to be port-Hamiltonian, and allows for the incorporation of perturbations in order to arrive at a nearby port-Hamiltonian system. Results are illustrated via numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.05355v3</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher Beattie, Volker Mehrmann, Hongguo Xu</dc:creator>
    </item>
    <item>
      <title>Viscosity Solutions for HJB Equations on the Process Space</title>
      <link>https://arxiv.org/abs/2401.04920</link>
      <description>arXiv:2401.04920v2 Announce Type: replace 
Abstract: In this paper we investigate a path dependent optimal control problem on the process space with both drift and volatility controls, with possibly degenerate volatility. The dynamic value function is characterized by a fully nonlinear second order path dependent HJB equation on the process space, which is by nature infinite dimensional. In particular, our model covers mean field control problems with common noise as a special case. We shall introduce a new notion of viscosity solutions and establish both the existence and the comparison principle, under merely Lipschitz/Holder continuity assumptions. The main feature of our notion is that, besides the standard smooth part, the test function consists of an extra singular component which allows us to handle the second order derivatives of the smooth test functions without invoking the Crandall-Ishii lemma. We shall use the doubling variable arguments, combined with the Ekeland-Borwein-Preiss variational principle in order to overcome the noncompactness of the state space. A smooth gauge-type function on the path space is crucial for our estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.04920v2</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianjun Zhou, Nizar Touzi, Jianfeng Zhang</dc:creator>
    </item>
    <item>
      <title>On the Complexity of p-Order Cone Programs</title>
      <link>https://arxiv.org/abs/2501.09828</link>
      <description>arXiv:2501.09828v2 Announce Type: replace 
Abstract: This manuscript explores novel complexity results for the feasibility problem over $p$-order cones, extending the foundational work of Porkolab and Khachiyan. By leveraging the intrinsic structure of $p$-order cones, we derive refined complexity bounds that surpass those obtained via standard semidefinite programming reformulations. Our analysis not only improves theoretical bounds but also provides practical insights into the computational efficiency of solving such problems. In addition to establishing complexity results, we derive explicit bounds for solutions when the feasibility problem admits one. For infeasible instances, we analyze their discrepancy quantifying the degree of infeasibility. Finally, we examine specific cases of interest, highlighting scenarios where the geometry of $p$-order cones or problem structure yields further computational simplifications. These findings contribute to both the theoretical understanding and practical tractability of optimization problems involving $p$-order cones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09828v2</guid>
      <category>math.OC</category>
      <category>cs.CC</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>V\'ictor Blanco, Victor Magron, Miguel Mart\'inez-Ant\'on</dc:creator>
    </item>
    <item>
      <title>A new nonlocal fractional differential quasi-variational inequality in Hilbert spaces with applications</title>
      <link>https://arxiv.org/abs/2503.02669</link>
      <description>arXiv:2503.02669v3 Announce Type: replace 
Abstract: This paper considers a new nonlocal fractional differential quasi-variational inequality (NFDQVI) comprising a fractional differential equation with a nonlocal condition and a time-dependent quasi-variational inequality in Hilbert spaces. Qualitative properties of the solution for the time-dependent parameterized quasi-variational inequality are investigated, which improve some known results in the literature. Moreover, the unique existence of the solution and Hyers-Ulam stability are obtained for such a novel NFDQVI under mild conditions. Finally, the obtained abstract results for NFDQVI are applied to analyze the unique solvability and stability addressing a time-dependent multi-agent optimization problem and a time-dependent price control problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02669v3</guid>
      <category>math.OC</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeng-bao Wu, Quan-guo Zhang, Tao Chen, Yue Zeng, Nan-jing Huang, Yi-bin Xiao</dc:creator>
    </item>
    <item>
      <title>Extending Parametric Model Embedding with Physical Information for Design-space Dimensionality Reduction in Shape Optimization</title>
      <link>https://arxiv.org/abs/2504.05863</link>
      <description>arXiv:2504.05863v2 Announce Type: replace 
Abstract: Design-space dimensionality reduction is essential to mitigate the cost of high-fidelity simulation-based optimization, especially when dealing with high-dimensional geometric parameterizations. Traditional linear techniques, such as principal component analysis, are widely used but often neglect the physical response of the system and lack invertibility to the design space, i.e., the ability to reconstruct the original design parameters from a reduced representation. This work introduces two physics-aware extensions of the parametric model embedding (PME) framework, aimed at generating reduced representations that incorporate physical information while maintaining analytical backmapping. The first, physics-informed PME (PI-PME), combines geometric and physical variability; the second, physics-driven PME (PD-PME), relies solely on physical responses. The proposed methods enable the construction of interpretable and physically relevant reduced spaces that can be used for design-space exploration, surrogate modeling, and optimization. The approach is demonstrated on multiple engineering configurations, including airfoils, propellers, gliders, and hulls, showing its ability to capture performance-relevant directions and preserve parametric consistency. The methodology is offline and non-intrusive, compatible with low-fidelity simulations, and requires only a modest number of samples to ensure variance convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05863v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Serani, Giorgio Palma, Jeroen Wackers, Domenico Quagliarella, Stefano Gaggero, Matteo Diez</dc:creator>
    </item>
    <item>
      <title>Block Coordinate Descent Network Simplex Methods for Optimal Transport</title>
      <link>https://arxiv.org/abs/2506.21231</link>
      <description>arXiv:2506.21231v2 Announce Type: replace 
Abstract: We propose the Block Coordinate Descent Network Simplex (BCDNS) method for solving large-scale discrete Optimal Transport (OT) problems. BCDNS integrates the Network Simplex (NS) algorithm with a block coordinate descent (BCD) strategy, decomposing the full problem into smaller subproblems per iteration and reusing basis variables to ensure feasibility. We prove that BCDNS terminates in a finite number of iterations with an exact optimal solution, and we characterize its per-iteration complexity as O(s N), where s is a user-defined parameter in (0,1) and N is the total number of variables. Numerical experiments demonstrate that BCDNS matches the classical NS method in solution accuracy, reduces memory footprint compared to the Sinkhorn algorithm, achieves speed-ups of up to tens of times over the classical NS method, and exhibits runtime comparable to a high-precision Sinkhorn implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21231v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lingrui Li, Nobuo Yamashita</dc:creator>
    </item>
    <item>
      <title>Interpretable Gradient Descent for Kalman Gain</title>
      <link>https://arxiv.org/abs/2507.14354</link>
      <description>arXiv:2507.14354v2 Announce Type: replace 
Abstract: We derive a decomposition for the gradient of the innovation loss with respect to the filter gain in a linear time-invariant system, decomposing as a product of an observability Gramian and a term quantifying the ``non-orthogonality" between the estimation error and the innovation. We leverage this decomposition to give a convergence proof of gradient descent to the optimal Kalman gain, specifically identifying how recovery of the Kalman gain depends on a non-standard observability condition, and obtaining an interpretable geometric convergence rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14354v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. A. Belabbas, A. Olshevsky</dc:creator>
    </item>
    <item>
      <title>On exploration of an interior mirror descent flow for stochastic nonconvex constrained problem</title>
      <link>https://arxiv.org/abs/2507.15264</link>
      <description>arXiv:2507.15264v2 Announce Type: replace 
Abstract: We study a nonsmooth nonconvex optimization problem defined over nonconvex constraints, where the feasible set is given by the intersection of the closure of an open set and a smooth manifold. By endowing the open set with a Riemannian metric induced by a barrier function, we obtain a Riemannian subgradient flow formulated as a differential inclusion, which remains strictly within the interior of the feasible set. This continuous dynamical system unifies two classes of iterative optimization methods, namely the Hessian barrier method and mirror descent scheme, by revealing that these methods can be interpreted as discrete approximations of the continuous flow. We explore the long-term behavior of the trajectories generated by this dynamical system and show that the existing deficient convergence properties of the Hessian barrier and mirror descent scheme can be unifily and more insightfully interpreted through these of the continuous trajectory. For instance, the notorious spurious stationary points \cite{chen2024spurious} observed in Hessian barrier method and mirror descent scheme are interpreted as stable equilibria of the dynamical system that do not correspond to real stationary points of the original optimization problem. We provide two sufficient condition such that these spurious stationary points can be avoided if the strict complementarity conditions holds. In the absence of these regularity condition, we propose a random perturbation strategy that ensures the trajectory converges (subsequentially) to an approximate stationary point. Building on these insights, we introduce two iterative Riemannian subgradient methods, form of interior point methods, that generalizes the existing Hessian barrier method and mirror descent scheme for solving nonsmooth nonconvex optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15264v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kuangyu Ding, Kim-Chuan Toh</dc:creator>
    </item>
    <item>
      <title>How averaged is the projection?</title>
      <link>https://arxiv.org/abs/2312.15421</link>
      <description>arXiv:2312.15421v2 Announce Type: replace-cross 
Abstract: Projection operators are important in Analysis, Optimization and Algorithm. It is well known that these operators are firmly nonexpansive. In this paper, we provide an exact result that sharpens this well-known result. We develop the theory of averaged operators and provide a lower bound. We give a result on the avergedness of operator compositions. We also provide some nonlinear examples to illustrate our results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15421v2</guid>
      <category>math.FA</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuang Song</dc:creator>
    </item>
    <item>
      <title>Edge of Stochastic Stability: Revisiting the Edge of Stability for SGD</title>
      <link>https://arxiv.org/abs/2412.20553</link>
      <description>arXiv:2412.20553v4 Announce Type: replace-cross 
Abstract: Recent findings by Cohen et al., 2021, demonstrate that when training neural networks with full-batch gradient descent with a step size of $\eta$, the largest eigenvalue $\lambda_{\max}$ of the full-batch Hessian consistently stabilizes at $\lambda_{\max} = 2/\eta$. These results have significant implications for convergence and generalization. This, however, is not the case of mini-batch stochastic gradient descent (SGD), limiting the broader applicability of its consequences. We show that SGD trains in a different regime we term Edge of Stochastic Stability (EoSS). In this regime, what stabilizes at $2/\eta$ is *Batch Sharpness*: the expected directional curvature of mini-batch Hessians along their corresponding stochastic gradients. As a consequence $\lambda_{\max}$ -- which is generally smaller than Batch Sharpness -- is suppressed, aligning with the long-standing empirical observation that smaller batches and larger step sizes favor flatter minima. We further discuss implications for mathematical modeling of SGD trajectories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20553v4</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arseniy Andreyev, Pierfrancesco Beneventano</dc:creator>
    </item>
    <item>
      <title>Global Convergence and Rich Feature Learning in $L$-Layer Infinite-Width Neural Networks under $\mu$P Parametrization</title>
      <link>https://arxiv.org/abs/2503.09565</link>
      <description>arXiv:2503.09565v2 Announce Type: replace-cross 
Abstract: Despite deep neural networks' powerful representation learning capabilities, theoretical understanding of how networks can simultaneously achieve meaningful feature learning and global convergence remains elusive. Existing approaches like the neural tangent kernel (NTK) are limited because features stay close to their initialization in this parametrization, leaving open questions about feature properties during substantial evolution. In this paper, we investigate the training dynamics of infinitely wide, $L$-layer neural networks using the tensor program (TP) framework. Specifically, we show that, when trained with stochastic gradient descent (SGD) under the Maximal Update parametrization ($\mu$P) and mild conditions on the activation function, SGD enables these networks to learn linearly independent features that substantially deviate from their initial values. This rich feature space captures relevant data information and ensures that any convergent point of the training process is a global minimum. Our analysis leverages both the interactions among features across layers and the properties of Gaussian random variables, providing new insights into deep representation learning. We further validate our theoretical findings through experiments on real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09565v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zixiang Chen, Greg Yang, Qingyue Zhao, Quanquan Gu</dc:creator>
    </item>
  </channel>
</rss>
