<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 21 Oct 2024 04:02:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Generalization for Least Squares Regression With Simple Spiked Covariances</title>
      <link>https://arxiv.org/abs/2410.13991</link>
      <description>arXiv:2410.13991v1 Announce Type: new 
Abstract: Random matrix theory has proven to be a valuable tool in analyzing the generalization of linear models. However, the generalization properties of even two-layer neural networks trained by gradient descent remain poorly understood. To understand the generalization performance of such networks, it is crucial to characterize the spectrum of the feature matrix at the hidden layer. Recent work has made progress in this direction by describing the spectrum after a single gradient step, revealing a spiked covariance structure. Yet, the generalization error for linear models with spiked covariances has not been previously determined. This paper addresses this gap by examining two simple models exhibiting spiked covariances. We derive their generalization error in the asymptotic proportional regime. Our analysis demonstrates that the eigenvector and eigenvalue corresponding to the spike significantly influence the generalization error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13991v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiping Li, Rishi Sonthalia</dc:creator>
    </item>
    <item>
      <title>General linear hypothesis testing of high-dimensional mean vectors with unequal covariance matrices based on random integration</title>
      <link>https://arxiv.org/abs/2410.14120</link>
      <description>arXiv:2410.14120v1 Announce Type: new 
Abstract: This paper is devoted to the study of the general linear hypothesis testing (GLHT) problem of multi-sample high-dimensional mean vectors. For the GLHT problem, we introduce a test statistic based on $L^2$-norm and random integration method, and deduce the asymptotic distribution of the statistic under given conditions. Finally, the potential advantages of our test statistics are verified by numerical simulation studies and examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14120v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingxiang Cao, Yelong Qiu</dc:creator>
    </item>
    <item>
      <title>Asymptotically efficient estimation under local constraint in Wicksell's problem</title>
      <link>https://arxiv.org/abs/2410.14263</link>
      <description>arXiv:2410.14263v1 Announce Type: new 
Abstract: We consider nonparametric estimation of the distribution function $F$ of squared sphere radii in the classical Wicksell problem. Under smoothness conditions on $F$ in a neighborhood of $x$, in \cite{21} it is shown that the Isotonic Inverse Estimator (IIE) is asymptotically efficient and attains rate of convergence $\sqrt{n / \log n}$. If $F$ is constant on an interval containing $x$, the optimal rate of convergence increases to $\sqrt{n}$ and the IIE attains this rate adaptively, i.e.\ without explicitly using the knowledge of local constancy. However, in this case, the asymptotic distribution is not normal. In this paper, we introduce three \textit{informed} projection-type estimators of $F$, which use knowledge on the interval of constancy and show these are all asymptotically equivalent and normal. Furthermore, we establish a local asymptotic minimax lower bound in this setting, proving that the three \textit{informed} estimators are asymptotically efficient and a convolution result showing that the IIE is not efficient. We also derive the asymptotic distribution of the difference of the IIE with the efficient estimators, demonstrating that the IIE is \textit{not} asymptotically equivalent to the \textit{informed} estimators. Through a simulation study, we provide evidence that the performance of the IIE closely resembles that of its competitors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14263v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Francesco Gili, Geurt Jongbloed, Aad van der Vaart</dc:creator>
    </item>
    <item>
      <title>An Optimal Linear Fusion Estimation Algorithm of Reduced Dimension for T-Proper Systems with Multiple Packet Dropouts</title>
      <link>https://arxiv.org/abs/2410.14378</link>
      <description>arXiv:2410.14378v1 Announce Type: new 
Abstract: This paper analyses the centralized fusion linear estimation problem in multi-sensor systems with multiple packet dropouts and correlated noises. Packet dropouts are modeled by independent Bernoulli distributed random variables. This problem is addressed in the tessarine domain under conditions of T1 and T2-properness, which entails a reduction in the dimension of the problem and, consequently, computational savings. The methodology proposed enables us to provide an optimal (in the least-mean-squares sense) linear fusion filtering algorithm for estimating the tessarine state with a lower computational cost than the conventional one devised in the real field. Simulation results illustrate the performance and advantages of the solution proposed in different settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14378v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/s23084047</arxiv:DOI>
      <arxiv:journal_reference>Sensors 2023, 23(8), 4047</arxiv:journal_reference>
      <dc:creator>Rosa M. Fern\'andez-Alcal\'a, Jos\'e D. Jim\'enez-L\'opez, Nicolas Le Bihan, Clive Cheong Took</dc:creator>
    </item>
    <item>
      <title>Asymptotic spectrum of weighted sample covariance: a Marcenko-Pastur generalization</title>
      <link>https://arxiv.org/abs/2410.14408</link>
      <description>arXiv:2410.14408v1 Announce Type: new 
Abstract: We propose an extension of the high dimensional spectrum analysis of sample covariance in the setting of the weighted sample covariance. We derive an asymptotic equation characterizing the limit density of the weighted sample eigenvalues generalizing for weighted sample covariance matrices the Marcenko-Pastur theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14408v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Benoit Oriol</dc:creator>
    </item>
    <item>
      <title>WeSpeR: Population spectrum retrieval and spectral density estimation of weighted sample covariance</title>
      <link>https://arxiv.org/abs/2410.14413</link>
      <description>arXiv:2410.14413v1 Announce Type: new 
Abstract: The spectrum of the weighted sample covariance shows a asymptotic non random behavior when the dimension grows with the number of samples. In this setting, we prove that the asymptotic spectral distribution $F$ of the weighted sample covariance has a continuous density on $\mathbb{R}^*$. We address then the practical problem of numerically finding this density. We propose a procedure to compute it, to determine the support of $F$ and define an efficient grid on it. We use this procedure to design the $\textit{WeSpeR}$ algorithm, which estimates the spectral density and retrieves the true spectral covariance spectrum. Empirical tests confirm the good properties of the $\textit{WeSpeR}$ algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14413v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Benoit Oriol</dc:creator>
    </item>
    <item>
      <title>Asymptotic non-linear shrinkage formulas for weighted sample covariance</title>
      <link>https://arxiv.org/abs/2410.14420</link>
      <description>arXiv:2410.14420v1 Announce Type: new 
Abstract: We compute asymptotic non-linear shrinkage formulas for covariance and precision matrix estimators for weighted sample covariances, in the spirit of Ledoit and P\'ech\'e. We detail explicitly the formulas for exponentially-weighted sample covariances. Those new tools pave a way for applying non-linear shrinkage methods on weighted sample covariance. We show experimentally the performance of the asymptotic shrinkage formulas. Finally, we test the robustness of the theory to a heavy-tailed distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14420v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Benoit Oriol</dc:creator>
    </item>
    <item>
      <title>Matrix normal distribution and elliptic distribution</title>
      <link>https://arxiv.org/abs/2410.14490</link>
      <description>arXiv:2410.14490v1 Announce Type: new 
Abstract: In this paper, we introduce the matrix normal distribution according to the tensor decomposition of its covariance. Based on the canonical diagonal form, the moment generating function of sample covariance matrix and the distribution of latent roots are explicitly calculated. We also discuss the connections between matrix normal distributions, elliptic distributions, and their relevance to multivariate analysis and matrix variate distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14490v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Haoming Wang</dc:creator>
    </item>
    <item>
      <title>The Bernoulli structure of discrete distributions</title>
      <link>https://arxiv.org/abs/2410.13920</link>
      <description>arXiv:2410.13920v1 Announce Type: cross 
Abstract: Any discrete distribution with support on $\{0,\ldots, d\}$ can be constructed as the distribution of sums of Bernoulli variables. We prove that the class of $d$-dimensional Bernoulli variables $\boldsymbol{X}=(X_1,\ldots, X_d)$ whose sums $\sum_{i=1}^dX_i$ have the same distribution $p$ is a convex polytope $\mathcal{P}(p)$ and we analytically find its extremal points. Our main result is to prove that the Hausdorff measure of the polytopes $\mathcal{P}(p), p\in \mathcal{D}_d,$ is a continuous function $l(p)$ over $\mathcal{D}_d$ and it is the density of a finite measure $\mu_s$ on $\mathcal{D}_d$ that is Hausdorff absolutely continuous. We also prove that the measure $\mu_s$ normalized over the simplex $\mathcal{D}$ belongs to the class of Dirichlet distributions. We observe that the symmetric binomial distribution is the mean of the Dirichlet distribution on $\mathcal{D}$ and that when $d$ increases it converges to the mode.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13920v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roberto Fontana, Patrizia Semeraro</dc:creator>
    </item>
    <item>
      <title>Modelling 1/f Noise in TRNGs via Fractional Brownian Motion</title>
      <link>https://arxiv.org/abs/2410.14205</link>
      <description>arXiv:2410.14205v1 Announce Type: cross 
Abstract: Building upon the foundational work of atomic clock physicists Barnes and Allan, this paper presents a highly scalable and numerically exact framework for modeling \(1/f\) noise in oscillatory True Random Number Generators (TRNGs) and assessing their cryptographic security. By employing Fractional Brownian Motion, the framework constructs Gaussian non-stationary processes that represent these noise spectra accurately and in a mathematically sound way. Furthermore, it establishes several critical properties, including optimal bounds on the achievable generation rate of cryptographically secure bits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14205v1</guid>
      <category>cs.CR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maciej Skorski</dc:creator>
    </item>
    <item>
      <title>Adaptive L-statistics for high dimensional test problem</title>
      <link>https://arxiv.org/abs/2410.14308</link>
      <description>arXiv:2410.14308v1 Announce Type: cross 
Abstract: In this study, we focus on applying L-statistics to the high-dimensional one-sample location test problem. Intuitively, an L-statistic with $k$ parameters tends to perform optimally when the sparsity level of the alternative hypothesis matches $k$. We begin by deriving the limiting distributions for both L-statistics with fixed parameters and those with diverging parameters. To ensure robustness across varying sparsity levels of alternative hypotheses, we first establish the asymptotic independence between L-statistics with fixed and diverging parameters. Building on this, we propose a Cauchy combination test that integrates L-statistics with different parameters. Both simulation results and real-data applications highlight the advantages of our proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14308v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huifang Ma, Long Feng, Zhaojun Wang</dc:creator>
    </item>
    <item>
      <title>Laplace Transform Based Low-Complexity Learning of Continuous Markov Semigroups</title>
      <link>https://arxiv.org/abs/2410.14477</link>
      <description>arXiv:2410.14477v1 Announce Type: cross 
Abstract: Markov processes serve as a universal model for many real-world random processes. This paper presents a data-driven approach for learning these models through the spectral decomposition of the infinitesimal generator (IG) of the Markov semigroup. The unbounded nature of IGs complicates traditional methods such as vector-valued regression and Hilbert-Schmidt operator analysis. Existing techniques, including physics-informed kernel regression, are computationally expensive and limited in scope, with no recovery guarantees for transfer operator methods when the time-lag is small. We propose a novel method that leverages the IG's resolvent, characterized by the Laplace transform of transfer operators. This approach is robust to time-lag variations, ensuring accurate eigenvalue learning even for small time-lags. Our statistical analysis applies to a broader class of Markov processes than current methods while reducing computational complexity from quadratic to linear in the state dimension. Finally, we illustrate the behaviour of our method in two experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14477v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vladimir R. Kostic, Karim Lounici, H\'el\`ene Halconruy, Timoth\'ee Devergne, Pietro Novelli, Massimiliano Pontil</dc:creator>
    </item>
    <item>
      <title>A novel statistical approach to analyze image classification</title>
      <link>https://arxiv.org/abs/2206.02151</link>
      <description>arXiv:2206.02151v2 Announce Type: replace 
Abstract: The recent statistical theory of neural networks focuses on nonparametric denoising problems that treat randomness as additive noise. Variability in image classification datasets does, however, not originate from additive noise but from variation of the shape and other characteristics of the same object across different images. To address this problem, we introduce a tractable model for supervised image classification. While from the function estimation point of view, every pixel in an image is a variable, and large images lead to high-dimensional function recovery tasks suffering from the curse of dimensionality, increasing the number of pixels in the proposed image deformation model enhances the image resolution and makes the object classification problem easier. We introduce and theoretically analyze three approaches. Two methods combine image alignment with a one-nearest neighbor classifier. Under a minimal separation condition, it is shown that perfect classification is possible. The third method fits a convolutional neural network (CNN) to the data. We derive a rate for the misclassification error that depends on the sample size and the complexity of the deformation class. A small empirical study corroborates the theoretical findings on images generated from the MNIST handwritten digit database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.02151v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juntong Chen, Sophie Langer, Johannes Schmidt-Hieber</dc:creator>
    </item>
    <item>
      <title>Lower Complexity Adaptation for Empirical Entropic Optimal Transport</title>
      <link>https://arxiv.org/abs/2306.13580</link>
      <description>arXiv:2306.13580v4 Announce Type: replace 
Abstract: Entropic optimal transport (EOT) presents an effective and computationally viable alternative to unregularized optimal transport (OT), offering diverse applications for large-scale data analysis. In this work, we derive novel statistical bounds for empirical plug-in estimators of the EOT cost and show that their statistical performance in the entropy regularization parameter $\epsilon$ and the sample size $n$ only depends on the simpler of the two probability measures. For instance, under sufficiently smooth costs this yields the parametric rate $n^{-1/2}$ with factor $\epsilon^{-d/2}$, where $d$ is the minimum dimension of the two population measures. This confirms that empirical EOT also adheres to the lower complexity adaptation principle, a hallmark feature only recently identified for unregularized OT. As a consequence of our theory, we show that the empirical entropic Gromov-Wasserstein distance and its unregularized version for measures on Euclidean spaces also obey this principle. Additionally, we comment on computational aspects and complement our findings with Monte Carlo simulations. Our techniques employ empirical process theory and rely on a dual formulation of EOT over a single function class. Crucial to our analysis is the observation that the entropic cost-transformation of a function class does not increase its uniform metric entropy by much.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.13580v4</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michel Groppe, Shayan Hundrieser</dc:creator>
    </item>
    <item>
      <title>Identifiability of Sparse Causal Effects using Instrumental Variables</title>
      <link>https://arxiv.org/abs/2203.09380</link>
      <description>arXiv:2203.09380v4 Announce Type: replace-cross 
Abstract: Exogenous heterogeneity, for example, in the form of instrumental variables can help us learn a system's underlying causal structure and predict the outcome of unseen intervention experiments. In this paper, we consider linear models in which the causal effect from covariates $X$ on a response $Y$ is sparse. We provide conditions under which the causal coefficient becomes identifiable from the observed distribution. These conditions can be satisfied even if the number of instruments is as small as the number of causal parents. We also develop graphical criteria under which identifiability holds with probability one if the edge coefficients are sampled randomly from a distribution that is absolutely continuous with respect to Lebesgue measure and $Y$ is childless. As an estimator, we propose spaceIV and prove that it consistently estimates the causal effect if the model is identifiable and evaluate its performance on simulated data. If identifiability does not hold, we show that it may still be possible to recover a subset of the causal parents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.09380v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niklas Pfister, Jonas Peters</dc:creator>
    </item>
    <item>
      <title>On the convergence of dynamic implementations of Hamiltonian Monte Carlo and No U-Turn Samplers</title>
      <link>https://arxiv.org/abs/2307.03460</link>
      <description>arXiv:2307.03460v2 Announce Type: replace-cross 
Abstract: There is substantial empirical evidence about the success of dynamic implementations of Hamiltonian Monte Carlo (HMC), such as the No U-Turn Sampler (NUTS), in many challenging inference problems but theoretical results about their behavior are scarce. The aim of this paper is to fill this gap. More precisely, we consider a general class of MCMC algorithms we call dynamic HMC. We show that this general framework encompasses NUTS as a particular case, implying the invariance of the target distribution as a by-product. Second, we establish conditions under which NUTS is irreducible and aperiodic and as a corrolary ergodic. Under conditions similar to the ones existing for HMC, we also show that NUTS is geometrically ergodic. Finally, we improve existing convergence results for HMC showing that this method is ergodic without any boundedness condition on the stepsize and the number of leapfrog steps, in the case where the target is a perturbation of a Gaussian distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.03460v2</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alain Durmus, Samuel Gruffaz, Miika Kailas, Eero Saksman, Matti Vihola</dc:creator>
    </item>
    <item>
      <title>Inferring Change Points in High-Dimensional Regression via Approximate Message Passing</title>
      <link>https://arxiv.org/abs/2404.07864</link>
      <description>arXiv:2404.07864v2 Announce Type: replace-cross 
Abstract: We consider the problem of localizing change points in a generalized linear model (GLM), a model that covers many widely studied problems in statistical learning including linear, logistic, and rectified linear regression. We propose a novel and computationally efficient Approximate Message Passing (AMP) algorithm for estimating both the signals and the change point locations, and rigorously characterize its performance in the high-dimensional limit where the number of parameters $p$ is proportional to the number of samples $n$. This characterization is in terms of a state evolution recursion, which allows us to precisely compute performance measures such as the asymptotic Hausdorff error of our change point estimates, and allows us to tailor the algorithm to take advantage of any prior structural information on the signals and change points. Moreover, we show how our AMP iterates can be used to efficiently compute a Bayesian posterior distribution over the change point locations in the high-dimensional limit. We validate our theory via numerical experiments, and demonstrate the favorable performance of our estimators on both synthetic and real data in the settings of linear, logistic, and rectified linear regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07864v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel Arpino, Xiaoqi Liu, Julia Gontarek, Ramji Venkataramanan</dc:creator>
    </item>
    <item>
      <title>ScoreFusion: fusing score-based generative models via Kullback-Leibler barycenters</title>
      <link>https://arxiv.org/abs/2406.19619</link>
      <description>arXiv:2406.19619v2 Announce Type: replace-cross 
Abstract: We introduce ScoreFusion, a theoretically grounded method for fusing multiple pre-trained diffusion models that are assumed to generate from auxiliary populations. ScoreFusion is particularly useful for enhancing the generative modeling of a target population with limited observed data. Our starting point considers the family of KL barycenters of the auxiliary populations, which is proven to be an optimal parametric class in the KL sense, but difficult to learn. Nevertheless, by recasting the learning problem as score matching in denoising diffusion, we obtain a tractable way of computing the optimal KL barycenter weights. We prove a dimension-free sample complexity bound in total variation distance, provided that the auxiliary models are well fitted for their own task and the auxiliary tasks combined capture the target well. We also explain a connection of the practice of checkpoint merging in AI art creation to an approximation of our KL-barycenter-based fusion approach. However, our fusion method differs in key aspects, allowing generation of new populations, as we illustrate in experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19619v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Liu, Junze Tony Ye, Jose Blanchet, Nian Si</dc:creator>
    </item>
    <item>
      <title>Granger Causality in Extremes</title>
      <link>https://arxiv.org/abs/2407.09632</link>
      <description>arXiv:2407.09632v2 Announce Type: replace-cross 
Abstract: We introduce a rigorous mathematical framework for Granger causality in extremes, designed to identify causal links from extreme events in time series. Granger causality plays a pivotal role in uncovering directional relationships among time-varying variables. While this notion gains heightened importance during extreme and highly volatile periods, state-of-the-art methods primarily focus on causality within the body of the distribution, often overlooking causal mechanisms that manifest only during extreme events. Our framework is designed to infer causality mainly from extreme events by leveraging the causal tail coefficient. We establish equivalences between causality in extremes and other causal concepts, including (classical) Granger causality, Sims causality, and structural causality. We prove other key properties of Granger causality in extremes and show that the framework is especially helpful under the presence of hidden confounders. We also propose a novel inference method for detecting the presence of Granger causality in extremes from data. Our method is model-free, can handle non-linear and high-dimensional time series, outperforms current state-of-the-art methods in all considered setups, both in performance and speed, and was found to uncover coherent effects when applied to financial and extreme weather observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09632v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juraj Bodik, Olivier C. Pasche</dc:creator>
    </item>
    <item>
      <title>Kernel Density Estimators in Large Dimensions</title>
      <link>https://arxiv.org/abs/2408.05807</link>
      <description>arXiv:2408.05807v3 Announce Type: replace-cross 
Abstract: This paper studies Kernel Density Estimation for a high-dimensional distribution $\rho(x)$. Traditional approaches have focused on the limit of large number of data points $n$ and fixed dimension $d$. We analyze instead the regime where both the number $n$ of data points $y_i$ and their dimensionality $d$ grow with a fixed ratio $\alpha=(\log n)/d$. Our study reveals three distinct statistical regimes for the kernel-based estimate of the density $\hat \rho_h^{\mathcal {D}}(x)=\frac{1}{n h^d}\sum_{i=1}^n K\left(\frac{x-y_i}{h}\right)$, depending on the bandwidth $h$: a classical regime for large bandwidth where the Central Limit Theorem (CLT) holds, which is akin to the one found in traditional approaches. Below a certain value of the bandwidth, $h_{CLT}(\alpha)$, we find that the CLT breaks down. The statistics of $\hat\rho_h^{\mathcal {D}}(x)$ for a fixed $x$ drawn from $\rho(x)$ is given by a heavy-tailed distribution (an alpha-stable distribution). In particular below a value $h_G(\alpha)$, we find that $\hat\rho_h^{\mathcal {D}}(x)$ is governed by extreme value statistics: only a few points in the database matter and give the dominant contribution to the density estimator. We provide a detailed analysis for high-dimensional multivariate Gaussian data. We show that the optimal bandwidth threshold based on Kullback-Leibler divergence lies in the new statistical regime identified in this paper. As known by practitioners, when decreasing the bandwidth a Kernel-estimated estimated changes from a smooth curve to a collections of peaks centred on the data points. Our findings reveal that this general phenomenon is related to sharp transitions between phases characterized by different statistical properties, and offer new insights for Kernel density estimation in high-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05807v3</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giulio Biroli, Marc M\'ezard</dc:creator>
    </item>
    <item>
      <title>Incremental effects for continuous exposures</title>
      <link>https://arxiv.org/abs/2409.11967</link>
      <description>arXiv:2409.11967v2 Announce Type: replace-cross 
Abstract: Causal inference problems often involve continuous treatments, such as dose, duration, or frequency. However, identifying and estimating standard dose-response estimands requires that everyone has some chance of receiving any level of the exposure (i.e., positivity). To avoid this assumption, we consider stochastic interventions based on exponentially tilting the treatment distribution by some parameter $\delta$ (i.e. an incremental effect); this increases or decreases the likelihood a unit receives a given treatment level. We derive the efficient influence function and semiparametric efficiency bound for these incremental effects under continuous exposures. We then show estimation depends on the size of the tilt, as measured by $\delta$. In particular, we derive new minimax lower bounds illustrating how the best possible root mean squared error scales with an effective sample size of $n / \delta$, instead of $n$. Further, we establish new convergence rates and bounds on the bias of double machine learning-style estimators. Our novel analysis gives a better dependence on $\delta$ compared to standard analyses by using mixed supremum and $L_2$ norms. Finally, we show that taking $\delta \to \infty$ gives a new estimator of the dose-response curve at the edge of the support, and give a detailed study of convergence rates in this regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11967v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyle Schindl, Shuying Shen, Edward H. Kennedy</dc:creator>
    </item>
    <item>
      <title>Models for spatiotemporal data with some missing locations and application to emergency calls models calibration</title>
      <link>https://arxiv.org/abs/2410.11103</link>
      <description>arXiv:2410.11103v2 Announce Type: replace-cross 
Abstract: We consider two classes of models for spatiotemporal data: one without covariates and one with covariates. If $\mathcal{T}$ is a partition of time and $\mathcal{I}$ a partition of the studied area into zones and if $\mathcal{C}$ is the set of arrival types, we assume that the process of arrivals for time interval $t \in \mathcal{T}$, zone $i \in \mathcal{I}$, and arrival type $c \in \mathcal{C}$ is Poisson with some intensity $\lambda_{c,i,t}$. We discussed the calibration and implementation of such models in \cite{laspatedpaper, laspatedmanual} with corresponding software LASPATED (Library for the Analysis of SPAtioTEmporal Discrete data) available on GitHub at https://github.com/vguigues/LASPATED. In this paper, we discuss the extension of these models when some of the locations are missing in the historical data. We propose three models to deal with missing locations and implemented them both in Matlab and C++. The corresponding code is available on GitHub as an extension of LASPATED at https://github.com/vguigues/LASPATED/Missing_Data. We tested our implementation using the process of emergency calls to an Emergency Health Service where many calls come with missing locations and show the importance and benefit of using models that consider missing locations, rather than discarding the calls with missing locations for the calibration of statistical models for such calls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11103v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Guigues, Anton Kleywegt, Victor Hugo Nascimento, Lucas Lucas Rafael de Andrade</dc:creator>
    </item>
  </channel>
</rss>
