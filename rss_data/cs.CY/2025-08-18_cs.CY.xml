<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 19 Aug 2025 02:09:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Knowledge Graph Informing Soil Carbon Modeling</title>
      <link>https://arxiv.org/abs/2508.10965</link>
      <description>arXiv:2508.10965v1 Announce Type: new 
Abstract: Soil organic carbon is crucial for climate change mitigation and agricultural sustainability. However, understanding its dynamics requires integrating complex, heterogeneous data from multiple sources. This paper introduces the Soil Organic Carbon Knowledge Graph (SOCKG), a semantic infrastructure designed to transform agricultural research data into a queryable knowledge representation. SOCKG features a robust ontological model of agricultural experimental data, enabling precise mapping of datasets from the Agricultural Collaborative Research Outcomes System. It is semantically aligned with the National Agricultural Library Thesaurus for consistent terminology and improved interoperability. The knowledge graph, constructed in GraphDB and Neo4j, provides advanced querying capabilities and RDF access. A user-friendly dashboard allows easy exploration of the knowledge graph and ontology. SOCKG supports advanced analyses, such as comparing soil organic carbon changes across fields and treatments, advancing soil carbon research, and enabling more effective agricultural strategies to mitigate climate change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10965v1</guid>
      <category>cs.CY</category>
      <category>cs.SC</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nasim Shirvani-Mahdavi, Devin Wingfield, Juan Guajardo Gutierrez, Mai Tran, Zhengyuan Zhu, Zeyu Zhang, Haiqi Zhang, Abhishek Divakar Goudar, Chengkai Li, Virginia Jin, Timothy Propst, Dan Roberts, Catherine Stewart, Jianzhong Su, Jennifer Woodward-Greene</dc:creator>
    </item>
    <item>
      <title>JobPulse: A Big Data Approach to Real-Time Engineering Workforce Analysis and National Industrial Policy</title>
      <link>https://arxiv.org/abs/2508.11014</link>
      <description>arXiv:2508.11014v1 Announce Type: new 
Abstract: Employment on a societal scale contributes heavily to national and global affairs; consequently, job openings and unemployment estimates provide important information to financial markets and governments alike. However, such reports often describe only the supply (employee job seeker) side of the job market, and skill mismatches are poorly understood. Job postings aggregated on recruiting platforms illuminate marketplace demand, but to date have primarily focused on candidate skills described in their personal profiles. In this paper, we report on a big data approach to estimating job market mismatches by focusing on demand, as represented in publicly available job postings. We use commercially available web scraping tools and a new data processing scheme to build a job posting data set for the semiconductor industry, a strategically critical sector of the United States economy; we focus on Southern California as a central hub of advanced technologies. We report on the employer base and relative needs of various job functions. Our work contributes on three fronts: First, we provide nearly real-time insight into workforce demand; second, we discuss disambiguation and semantic challenges in analysis of employer data bases at scale; and third, we report on the Southern California semiconductor engineering ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11014v1</guid>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Karen S. Markel, Mihir Tale, Andrea Belz</dc:creator>
    </item>
    <item>
      <title>Bias is a Math Problem, AI Bias is a Technical Problem: 10-year Literature Review of AI/LLM Bias Research Reveals Narrow [Gender-Centric] Conceptions of 'Bias', and Academia-Industry Gap</title>
      <link>https://arxiv.org/abs/2508.11067</link>
      <description>arXiv:2508.11067v1 Announce Type: new 
Abstract: The rapid development of AI tools and implementation of LLMs within downstream tasks has been paralleled by a surge in research exploring how the outputs of such AI/LLM systems embed biases, a research topic which was already being extensively explored before the era of ChatGPT. Given the high volume of research around the biases within the outputs of AI systems and LLMs, it is imperative to conduct systematic literature reviews to document throughlines within such research. In this paper, we conduct such a review of research covering AI/LLM bias in four premier venues/organizations -- *ACL, FAccT, NeurIPS, and AAAI -- published over the past 10 years. Through a coverage of 189 papers, we uncover patterns of bias research and along what axes of human identity they commonly focus. The first emergent pattern within the corpus was that 82% (155/189) papers did not establish a working definition of "bias" for their purposes, opting instead to simply state that biases and stereotypes exist that can have harmful downstream effects while establishing only mathematical and technical definition of bias. 94 of these 155 papers have been published in the past 5 years, after Blodgett et al. (2020)'s literature review with a similar finding about NLP research and recommendation to consider how such researchers should conceptualize bias, going beyond strictly technical definitions. Furthermore, we find that a large majority of papers -- 79.9% or 151/189 papers -- focus on gender bias (mostly, gender and occupation bias) within the outputs of AI systems and LLMs. By demonstrating a strong focus within the field on gender, race/ethnicity (30.2%; 57/189), age (20.6%; 39/189), religion (19.1%; 36/189) and nationality (13.2%; 25/189) bias, we document how researchers adopt a fairly narrow conception of AI bias by overlooking several non-Western communities in fairness research, as we advocate for a stronger coverage of such populations. Finally, we note that while our corpus contains several examples of innovative debiasing methods across the aforementioned aspects of human identity, only 10.6% (20/189) include recommendations for how to implement their findings or contributions in real-world AI systems or design processes. This indicates a concerning academia-industry gap, especially since many of the biases that our corpus contains several successful mitigation methods that still persist within the outputs of AI systems and LLMs commonly used today. We conclude with recommendations towards future AI/LLM fairness research, with stronger focus on diverse marginalized populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11067v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourojit Ghosh, Kyra Wilson</dc:creator>
    </item>
    <item>
      <title>CLMIR: A Textual Dataset for Rumor Identification and Marking</title>
      <link>https://arxiv.org/abs/2508.11138</link>
      <description>arXiv:2508.11138v1 Announce Type: new 
Abstract: With the rise of social media, rumor detection has drawn increasing attention. Although numerous methods have been proposed with the development of rumor classification datasets, they focus on identifying whether a post is a rumor, lacking the ability to mark the specific rumor content. This limitation largely stems from the lack of fine-grained marks in existing datasets. Constructing a rumor dataset with rumor content information marking is of great importance for fine-grained rumor identification. Such a dataset can facilitate practical applications, including rumor tracing, content moderation, and emergency response. Beyond being utilized for overall performance evaluation, this dataset enables the training of rumor detection algorithms to learn content marking, and thus improves their interpretability and reasoning ability, enabling systems to effectively address specific rumor segments. This paper constructs a dataset for rumor detection with fine-grained markings, named CLMIR (Content-Level Marking Dataset for Identifying Rumors). In addition to determining whether a post is a rumor, this dataset further marks the specific content upon which the rumor is based.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11138v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bin Ma, Yifei Zhang, Yongjin Xian, Qi Li, Linna Zhou, Gongxun Miao</dc:creator>
    </item>
    <item>
      <title>Intergenerational Support for Deepfake Scams Targeting Older Adults</title>
      <link>https://arxiv.org/abs/2508.11579</link>
      <description>arXiv:2508.11579v1 Announce Type: new 
Abstract: AI-enhanced scams now employ deepfake technology to produce convincing audio and visual impersonations of trusted family members, often grandchildren, in real time. These attacks fabricate urgent scenarios, such as legal or medical emergencies, to socially engineer older adults into transferring money. The realism of these AI-generated impersonations undermines traditional cues used to detect fraud, making them a powerful tool for financial exploitation. In this study, we explore older adults' perceptions of these emerging threats and their responses, with a particular focus on the role of youth, who may also be impacted by having their identities exploited, in supporting older family members' online safety. We conducted focus groups with 37 older adults (ages 65+) to examine their understanding of deepfake impersonation scams and the value of intergenerational technology support. Findings suggest that older adults frequently rely on trusted relationships to detect scams and develop protective practices. Based on this, we identify opportunities to engage youth as active partners in enhancing resilience across generations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11579v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Karina LaRubbio, Alyssa Lanter, Seihyun Lee, Mahima Ramesh, Diana Freed</dc:creator>
    </item>
    <item>
      <title>How do Data Journalists Design Maps to Tell Stories?</title>
      <link>https://arxiv.org/abs/2508.10903</link>
      <description>arXiv:2508.10903v1 Announce Type: cross 
Abstract: Maps are essential to news media as they provide a familiar way to convey spatial context and present engaging narratives. However, the design of journalistic maps may be challenging, as editorial teams need to balance multiple aspects, such as aesthetics, the audience's expected data literacy, tight publication deadlines, and the team's technical skills. Data journalists often come from multiple areas and lack a cartography, data visualization, and data science background, limiting their competence in creating maps. While previous studies have examined spatial visualizations in data stories, this research seeks to gain a deeper understanding of the map design process employed by news outlets. To achieve this, we strive to answer two specific research questions: what is the design space of journalistic maps? and how do editorial teams produce journalistic map articles? To answer the first one, we collected and analyzed a large corpus of 462 journalistic maps used in news articles from five major news outlets published over three months. As a result, we created a design space comprised of eight dimensions that involved both properties describing the articles' aspects and the visual/interactive features of maps. We approach the second research question via semi-structured interviews with four data journalists who create data-driven articles daily. Through these interviews, we identified the most common design rationales made by editorial teams and potential gaps in current practices. We also collected the practitioners' feedback on our design space to externally validate it. With these results, we aim to provide researchers and journalists with empirical data to design and study journalistic maps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10903v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arlindo Gomes, Emilly Brito, Luis Morais, Nivan Ferreira</dc:creator>
    </item>
    <item>
      <title>Designing for Engaging Communication Between Parents and Young Adult Children Through Shared Music Experiences</title>
      <link>https://arxiv.org/abs/2508.10907</link>
      <description>arXiv:2508.10907v1 Announce Type: cross 
Abstract: This paper aims to foster social interaction between parents and young adult children living apart via music. Our approach transforms their music-listening moment into an opportunity to listen to the other's favorite songs and enrich interaction in their daily lives. To this end, we explore the current practice and needs of parent-child communication and the experience and perception of music-mediated interaction. Based on the findings, we developed DJ-Fam, a mobile application that enables parents and children to listen to their favorite songs and use them as conversation starters to foster parent-child interaction. From our deployment study with seven families over four weeks in South Korea, we show the potential of DJ-Fam to influence parent-child interaction and their mutual understanding and relationship positively. Specifically, DJ-Fam considerably increases the frequency of communication and diversifies the communication channels and topics, all of which are satisfactory to the participants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10907v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Euihyeok Lee, Souneil Park, Jin Yu, Seungchul Lee, Seungwoo Kang</dc:creator>
    </item>
    <item>
      <title>Uncovering Latent Connections in Indigenous Heritage: Semantic Pipelines for Cultural Preservation in Brazil</title>
      <link>https://arxiv.org/abs/2508.10911</link>
      <description>arXiv:2508.10911v1 Announce Type: cross 
Abstract: Indigenous communities face ongoing challenges in preserving their cultural heritage, particularly in the face of systemic marginalization and urban development. In Brazil, the Museu Nacional dos Povos Indigenas through the Tainacan platform hosts the country's largest online collection of Indigenous objects and iconographies, providing a critical resource for cultural engagement. Using publicly available data from this repository, we present a data-driven initiative that applies artificial intelligence to enhance accessibility, interpretation, and exploration. We develop two semantic pipelines: a visual pipeline that models image-based similarity and a textual pipeline that captures semantic relationships from item descriptions. These embedding spaces are projected into two dimensions and integrated into an interactive visualization tool we also developed. In addition to similarity-based navigation, users can explore the collection through temporal and geographic lenses, enabling both semantic and contextualized perspectives. The system supports curatorial tasks, aids public engagement, and reveals latent connections within the collection. This work demonstrates how AI can ethically contribute to cultural preservation practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10911v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Luis Vitor Zerkowski, Nina S. T. Hirata</dc:creator>
    </item>
    <item>
      <title>Multimodal Quantitative Measures for Multiparty Behaviour Evaluation</title>
      <link>https://arxiv.org/abs/2508.10916</link>
      <description>arXiv:2508.10916v1 Announce Type: cross 
Abstract: Digital humans are emerging as autonomous agents in multiparty interactions, yet existing evaluation metrics largely ignore contextual coordination dynamics. We introduce a unified, intervention-driven framework for objective assessment of multiparty social behaviour in skeletal motion data, spanning three complementary dimensions: (1) synchrony via Cross-Recurrence Quantification Analysis, (2) temporal alignment via Multiscale Empirical Mode Decompositionbased Beat Consistency, and (3) structural similarity via Soft Dynamic Time Warping. We validate metric sensitivity through three theory-driven perturbations -- gesture kinematic dampening, uniform speech-gesture delays, and prosodic pitch-variance reduction-applied to $\approx 145$ 30-second thin slices of group interactions from the DnD dataset. Mixed-effects analyses reveal predictable, joint-independent shifts: dampening increases CRQA determinism and reduces beat consistency, delays weaken cross-participant coupling, and pitch flattening elevates F0 Soft-DTW costs. A complementary perception study ($N=27$) compares judgments of full-video and skeleton-only renderings to quantify representation effects. Our three measures deliver orthogonal insights into spatial structure, timing alignment, and behavioural variability. Thereby forming a robust toolkit for evaluating and refining socially intelligent agents. Code available on \href{https://github.com/tapri-lab/gig-interveners}{GitHub}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10916v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3716553.3750752</arxiv:DOI>
      <dc:creator>Ojas Shirekar, Wim Pouw, Chenxu Hao, Vrushank Phadnis, Thabo Beeler, Chirag Raman</dc:creator>
    </item>
    <item>
      <title>Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing</title>
      <link>https://arxiv.org/abs/2508.11258</link>
      <description>arXiv:2508.11258v1 Announce Type: cross 
Abstract: Instruction fine-tuned large language models (LLMs) enable a simple zero-shot or few-shot prompting paradigm, also known as in-context learning, for building prediction models. This convenience, combined with continued advances in LLM capability, has the potential to drive their adoption across a broad range of domains, including high-stakes applications where group fairness -- preventing disparate impacts across demographic groups -- is essential. The majority of existing approaches to enforcing group fairness on LLM-based classifiers rely on traditional fair algorithms applied via model fine-tuning or head-tuning on final-layer embeddings, but they are no longer applicable to closed-weight LLMs under the in-context learning setting, which include some of the most capable commercial models today, such as GPT-4, Gemini, and Claude. In this paper, we propose a framework for deriving fair classifiers from closed-weight LLMs via prompting: the LLM is treated as a feature extractor, and features are elicited from its probabilistic predictions (e.g., token log probabilities) using prompts strategically designed for the specified fairness criterion to obtain sufficient statistics for fair classification; a fair algorithm is then applied to these features to train a lightweight fair classifier in a post-hoc manner. Experiments on five datasets, including three tabular ones, demonstrate strong accuracy-fairness tradeoffs for the classifiers derived by our framework from both open-weight and closed-weight LLMs; in particular, our framework is data-efficient and outperforms fair classifiers trained on LLM embeddings (i.e., head-tuning) or from scratch on raw tabular features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11258v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruicheng Xian, Yuxuan Wan, Han Zhao</dc:creator>
    </item>
    <item>
      <title>Element and Everything Tokens: Two-Tier Architecture for Mobilizing Alternative Assets</title>
      <link>https://arxiv.org/abs/2508.11266</link>
      <description>arXiv:2508.11266v1 Announce Type: cross 
Abstract: Alternative assets such as mines, power plants, or infrastructure projects are often large, heterogeneous bundles of resources, rights, and outputs whose value is difficult to trade or fractionalize under traditional frameworks. This paper proposes a novel two-tier tokenization architecture to enhance the liquidity and transparency of such complex assets. We introduce the concepts of Element Tokens and Everything Tokens: elemental tokens represent standardized, fully collateralized components of an asset (e.g., outputs, rights, or credits), while an everything token represents the entire asset as a fixed combination of those elements. The architecture enables both fine-grained partial ownership and integrated whole-asset ownership through a system of two-way convertibility. We detail the design and mechanics of this system, including an arbitrage mechanism that keeps the price of the composite token aligned with the net asset value of its constituents. Through illustrative examples in the energy and industrial sectors, we demonstrate that our approach allows previously illiquid, high-value projects to be fractionalized and traded akin to stocks or exchange-traded funds (ETFs). We discuss the benefits for investors and asset owners, such as lower entry barriers, improved price discovery, and flexible financing, as well as the considerations for implementation and regulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11266v1</guid>
      <category>cs.DC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ailiya Borjigin, Cong He, Charles CC Lee, Wei Zhou</dc:creator>
    </item>
    <item>
      <title>ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection</title>
      <link>https://arxiv.org/abs/2508.11281</link>
      <description>arXiv:2508.11281v1 Announce Type: cross 
Abstract: Detecting toxic content using language models is crucial yet challenging. While substantial progress has been made in English, toxicity detection in French remains underdeveloped, primarily due to the lack of culturally relevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new public benchmark of 53,622 French online comments, constructed via a semi-automated annotation pipeline that reduces manual labeling to only 10% through high-confidence LLM-based pre-annotation and human verification. Then, we benchmark a broad range of models and uncover a counterintuitive insight: Small Language Models (SLMs) outperform many larger models in robustness and generalization under the toxicity detection task. Motivated by this finding, we propose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic weighted loss that progressively emphasizes the model's final decision, significantly improving faithfulness. Our fine-tuned 4B model achieves state-of-the-art performance, improving its F1 score by 13% over its baseline and outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a cross-lingual toxicity benchmark demonstrates strong multilingual ability, suggesting that our methodology can be effectively extended to other languages and safety-critical classification tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11281v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Axel Delaval, Shujian Yang, Haicheng Wang, Han Qiu, Jialiang Lu</dc:creator>
    </item>
    <item>
      <title>Retrieval-augmented reasoning with lean language models</title>
      <link>https://arxiv.org/abs/2508.11386</link>
      <description>arXiv:2508.11386v1 Announce Type: cross 
Abstract: This technical report details a novel approach to combining reasoning and retrieval augmented generation (RAG) within a single, lean language model architecture. While existing RAG systems typically rely on large-scale models and external APIs, our work addresses the increasing demand for performant and privacy-preserving solutions deployable in resource-constrained or secure environments. Building on recent developments in test-time scaling and small-scale reasoning models, we develop a retrieval augmented conversational agent capable of interpreting complex, domain-specific queries using a lightweight backbone model. Our system integrates a dense retriever with fine-tuned Qwen2.5-Instruct models, using synthetic query generation and reasoning traces derived from frontier models (e.g., DeepSeek-R1) over a curated corpus, in this case, the NHS A-to-Z condition pages. We explore the impact of summarisation-based document compression, synthetic data design, and reasoning-aware fine-tuning on model performance. Evaluation against both non-reasoning and general-purpose lean models demonstrates that our domain-specific fine-tuning approach yields substantial gains in answer accuracy and consistency, approaching frontier-level performance while remaining feasible for local deployment. All implementation details and code are publicly released to support reproducibility and adaptation across domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11386v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.16408412</arxiv:DOI>
      <dc:creator>Ryan Sze-Yin Chan, Federico Nanni, Tomas Lazauskas, Rosie Wood, Penelope Yong, Lionel Tarassenko, Mark Girolami, James Geddes, Andrew Duncan</dc:creator>
    </item>
    <item>
      <title>Banking 2.0: The Stablecoin Banking Revolution -- How Digital Assets Are Reshaping Global Finance</title>
      <link>https://arxiv.org/abs/2508.11395</link>
      <description>arXiv:2508.11395v1 Announce Type: cross 
Abstract: The global financial system stands at an inflection point. Stablecoins represent the most significant evolution in banking since the abandonment of the gold standard, positioned to enable "Banking 2.0" by seamlessly integrating cryptocurrency innovation with traditional finance infrastructure. This transformation rivals artificial intelligence as the next major disruptor in the financial sector. Modern fiat currencies derive value entirely from institutional trust rather than physical backing, creating vulnerabilities that stablecoins address through enhanced stability, reduced fraud risk, and unified global transactions that transcend national boundaries. Recent developments demonstrate accelerating institutional adoption: landmark U.S. legislation including the GENIUS Act of 2025, strategic industry pivots from major players like JPMorgan's crypto-backed loan initiatives, and PayPal's comprehensive "Pay with Crypto" service. Widespread stablecoin implementation addresses critical macroeconomic imbalances, particularly the inflation-productivity gap plaguing modern monetary systems, through more robust and diversified backing mechanisms. Furthermore, stablecoins facilitate deregulation and efficiency gains, paving the way for a more interconnected international financial system. This whitepaper comprehensively explores how stablecoins are poised to reshape banking, supported by real-world examples, current market data, and analysis of their transformative potential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11395v1</guid>
      <category>cs.ET</category>
      <category>cs.CE</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin McNamara, Rhea Pritham Marpu</dc:creator>
    </item>
    <item>
      <title>Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political Discourse</title>
      <link>https://arxiv.org/abs/2508.11434</link>
      <description>arXiv:2508.11434v1 Announce Type: cross 
Abstract: Anti-sexist speech, i.e., public expressions that challenge or resist gendered abuse and sexism, plays a vital role in shaping democratic debate online. Yet automated content moderation systems, increasingly powered by large language models (LLMs), may struggle to distinguish such resistance from the sexism it opposes. This study examines how five LLMs classify sexist, anti-sexist, and neutral political tweets from the UK, focusing on high-salience trigger events involving female Members of Parliament in the year 2022. Our analysis show that models frequently misclassify anti-sexist speech as harmful, particularly during politically charged events where rhetorical styles of harm and resistance converge. These errors risk silencing those who challenge sexism, with disproportionate consequences for marginalised voices. We argue that moderation design must move beyond binary harmful/not-harmful schemas, integrate human-in-the-loop review during sensitive events, and explicitly include counter-speech in training data. By linking feminist scholarship, event-based analysis, and model evaluation, this work highlights the sociotechnical challenges of safeguarding resistance speech in digital political spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11434v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditi Dutta, Susan Banducci</dc:creator>
    </item>
    <item>
      <title>Predicting and Explaining Traffic Crash Severity Through Crash Feature Selection</title>
      <link>https://arxiv.org/abs/2508.11504</link>
      <description>arXiv:2508.11504v1 Announce Type: cross 
Abstract: Motor vehicle crashes remain a leading cause of injury and death worldwide, necessitating data-driven approaches to understand and mitigate crash severity. This study introduces a curated dataset of more than 3 million people involved in accidents in Ohio over six years (2017-2022), aggregated to more than 2.3 million vehicle-level records for predictive analysis. The primary contribution is a transparent and reproducible methodology that combines Automated Machine Learning (AutoML) and explainable artificial intelligence (AI) to identify and interpret key risk factors associated with severe crashes. Using the JADBio AutoML platform, predictive models were constructed to distinguish between severe and non-severe crash outcomes. The models underwent rigorous feature selection across stratified training subsets, and their outputs were interpreted using SHapley Additive exPlanations (SHAP) to quantify the contribution of individual features. A final Ridge Logistic Regression model achieved an AUC-ROC of 85.6% on the training set and 84.9% on a hold-out test set, with 17 features consistently identified as the most influential predictors. Key features spanned demographic, environmental, vehicle, human, and operational categories, including location type, posted speed, minimum occupant age, and pre-crash action. Notably, certain traditionally emphasized factors, such as alcohol or drug impairment, were less influential in the final model compared to environmental and contextual variables. Emphasizing methodological rigor and interpretability over mere predictive performance, this study offers a scalable framework to support Vision Zero with aligned interventions and advanced data-informed traffic safety policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11504v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Castellani, Zacharias Papadovasilakis, Giorgos Papoutsoglou, Mary Cole, Brian Bautsch, Tobias Rodemann, Ioannis Tsamardinos, Angela Harden</dc:creator>
    </item>
    <item>
      <title>Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models</title>
      <link>https://arxiv.org/abs/2508.11534</link>
      <description>arXiv:2508.11534v1 Announce Type: cross 
Abstract: As large language models (LLMs) become more widely deployed, it is crucial to examine their ethical tendencies. Building on research on fairness and discrimination in AI, we investigate whether LLMs exhibit speciesist bias -- discrimination based on species membership -- and how they value non-human animals. We systematically examine this issue across three paradigms: (1) SpeciesismBench, a 1,003-item benchmark assessing recognition and moral evaluation of speciesist statements; (2) established psychological measures comparing model responses with those of human participants; (3) text-generation tasks probing elaboration on, or resistance to, speciesist rationalizations. In our benchmark, LLMs reliably detected speciesist statements but rarely condemned them, often treating speciesist attitudes as morally acceptable. On psychological measures, results were mixed: LLMs expressed slightly lower explicit speciesism than people, yet in direct trade-offs they more often chose to save one human over multiple animals. A tentative interpretation is that LLMs may weight cognitive capacity rather than species per se: when capacities were equal, they showed no species preference, and when an animal was described as more capable, they tended to prioritize it over a less capable human. In open-ended text generation tasks, LLMs frequently normalized or rationalized harm toward farmed animals while refusing to do so for non-farmed animals. These findings suggest that while LLMs reflect a mixture of progressive and mainstream human views, they nonetheless reproduce entrenched cultural norms around animal exploitation. We argue that expanding AI fairness and alignment frameworks to explicitly include non-human moral patients is essential for reducing these biases and preventing the entrenchment of speciesist attitudes in AI systems and the societies they influence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11534v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Monika Jotautait\.e, Lucius Caviola, David A. Brewster, Thilo Hagendorff</dc:creator>
    </item>
    <item>
      <title>Google's Chrome Antitrust Paradox</title>
      <link>https://arxiv.org/abs/2406.11856</link>
      <description>arXiv:2406.11856v3 Announce Type: replace 
Abstract: This Article examines Google's dominance of the browser market, highlighting how Google's Chrome browser plays a critical role in reinforcing Google's dominance in other markets. While Google portrays Chrome as a neutral platform built on open-source technologies, this Article shows that Chrome is instrumental in Google's strategy to reinforce its dominance in the online advertising, publishing, and browser markets. The examination of Google's strategic acquisitions, anticompetitive practices, and implementation of so-called "privacy controls" underlines that Chrome is far from a neutral gateway to the web. Rather, it serves as a key tool for Google to maintain and extend its market power, often to the detriment of competition and innovation in the digital economy.
  This Article illustrates how Chrome not only bolsters Google's position in online advertising and publishing through practices such as coercion and self-preferencing, but also leverages its advertising clout to engage in a "pay-to-play" paradigm--the cornerstone of Google's larger strategy of market control. It also outlines potential regulatory interventions and remedies by drawing on historical antitrust precedents. Lastly, this Article proposes a triad of solutions motivated by an analysis of Google's abuse of Chrome, including behavioral remedies targeting specific anticompetitive practices, structural remedies involving an internal separation of Google's divisions, and divestiture of Chrome from Google into an independent organization.
  (Abstract abridged for arXiv. Full abstract available in published version.)</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11856v3</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Vanderbilt Journal of Entertainment &amp; Technology Law, 27(3), 419-521, 2025</arxiv:journal_reference>
      <dc:creator>Shaoor Munir, Konrad Kollnig, Anastasia Shuba, Zubair Shafiq</dc:creator>
    </item>
    <item>
      <title>Audit Cards: Contextualizing AI Evaluations</title>
      <link>https://arxiv.org/abs/2504.13839</link>
      <description>arXiv:2504.13839v2 Announce Type: replace 
Abstract: AI governance frameworks increasingly rely on audits, yet the results of their underlying evaluations require interpretation and context to be meaningfully informative. Even technically rigorous evaluations can offer little useful insight if reported selectively or obscurely. Current literature focuses primarily on technical best practices, but evaluations are an inherently sociotechnical process, and there is little guidance on reporting procedures and context. Through literature review, stakeholder interviews, and analysis of governance frameworks, we propose "audit cards" to make this context explicit. We identify six key types of contextual features to report and justify in audit cards: auditor identity, evaluation scope, methodology, resource access, process integrity, and review mechanisms. Through analysis of existing evaluation reports, we find significant variation in reporting practices, with most reports omitting crucial contextual information such as auditors' backgrounds, conflicts of interest, and the level and type of access to models. We also find that most existing regulations and frameworks lack guidance on rigorous reporting. In response to these shortcomings, we argue that audit cards can provide a structured format for reporting key claims alongside their justifications, enhancing transparency, facilitating proper interpretation, and establishing trust in reporting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13839v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leon Staufer, Mick Yang, Anka Reuel, Stephen Casper</dc:creator>
    </item>
    <item>
      <title>From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility Systems</title>
      <link>https://arxiv.org/abs/2507.04996</link>
      <description>arXiv:2507.04996v3 Announce Type: replace 
Abstract: Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity to operate according to internal rules without external control. Accordingly, autonomous vehicles (AuVs) are viewed as vehicular systems capable of perceiving their environment and executing pre-programmed tasks independently of external input. However, both research and real-world deployments increasingly showcase vehicles that demonstrate behaviors beyond this definition (including the SAE levels 0 to 5); Examples of this outpace include the interaction with humans with natural language, goal adaptation, contextual reasoning, external tool use, and unseen ethical dilemma handling, largely empowered by multi-modal large language models (LLMs). These developments reveal a conceptual gap between technical autonomy and the broader cognitive and social capabilities needed for future human-centered mobility systems. To address this gap, this paper introduces the concept of agentic vehicles (AgVs), referring to vehicles that integrate agentic AI systems to reason, adapt, and interact within complex environments. This paper proposes the term AgVs and their distinguishing characteristics from conventional AuVs. It synthesizes relevant advances in integrating LLMs and AuVs and highlights how AgVs might transform future mobility systems and ensure the systems are human-centered. The paper concludes by identifying key challenges in the development and governance of AgVs, and how they can play a significant role in future agentic transportation systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04996v3</guid>
      <category>cs.CY</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiangbo Yu</dc:creator>
    </item>
    <item>
      <title>Dead Zone of Accountability: Why Social Claims in Machine Learning Research Should Be Articulated and Defended</title>
      <link>https://arxiv.org/abs/2508.08739</link>
      <description>arXiv:2508.08739v3 Announce Type: replace 
Abstract: Many Machine Learning research studies use language that describes potential social benefits or technical affordances of new methods and technologies. Such language, which we call "social claims", can help garner substantial resources and influence for those involved in ML research and technology production. However, there exists a gap between social claims and reality (the claim-reality gap): ML methods often fail to deliver the claimed functionality or social impacts. This paper investigates the claim-reality gap and makes a normative argument for developing accountability mechanisms for it. In making the argument, we make three contributions. First, we show why the symptom - absence of social claim accountability - is problematic. Second, we coin dead zone of accountability - a lens that scholars and practitioners can use to identify opportunities for new forms of accountability. We apply this lens to the claim-reality gap and provide a diagnosis by identifying cognitive and structural resistances to accountability in the claim-reality gap. Finally, we offer a prescription - two potential collaborative research agendas that can help create the condition for social claim accountability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08739v3</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianqi Kou, Dana Calacci, Cindy Lin</dc:creator>
    </item>
  </channel>
</rss>
