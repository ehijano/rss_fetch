<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Sep 2025 04:02:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Local Projections Bootstrap Inference</title>
      <link>https://arxiv.org/abs/2509.17949</link>
      <description>arXiv:2509.17949v1 Announce Type: new 
Abstract: Bootstrap procedures for local projections typically rely on assuming that the data generating process (DGP) is a finite order vector autoregression (VAR), often taken to be that implied by the local projection at horizon 1. Although convenient, it is well documented that a VAR can be a poor approximation to impulse dynamics at horizons beyond its lag length. In this paper we assume instead that the precise form of the parametric model generating the data is not known. If one is willing to assume that the DGP is perhaps an infinite order process, a larger class of models can be accommodated and more tailored bootstrap procedures can be constructed. Using the moving average representation of the data, we construct appropriate bootstrap procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17949v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mar\'ia Dolores Gadea, \`Oscar Jord\`a</dc:creator>
    </item>
    <item>
      <title>Regularizing Extrapolation in Causal Inference</title>
      <link>https://arxiv.org/abs/2509.17180</link>
      <description>arXiv:2509.17180v1 Announce Type: cross 
Abstract: Many common estimators in machine learning and causal inference are linear smoothers, where the prediction is a weighted average of the training outcomes. Some estimators, such as ordinary least squares and kernel ridge regression, allow for arbitrarily negative weights, which improve feature imbalance but often at the cost of increased dependence on parametric modeling assumptions and higher variance. By contrast, estimators like importance weighting and random forests (sometimes implicitly) restrict weights to be non-negative, reducing dependence on parametric modeling and variance at the cost of worse imbalance. In this paper, we propose a unified framework that directly penalizes the level of extrapolation, replacing the current practice of a hard non-negativity constraint with a soft constraint and corresponding hyperparameter. We derive a worst-case extrapolation error bound and introduce a novel "bias-bias-variance" tradeoff, encompassing biases due to feature imbalance, model misspecification, and estimator variance; this tradeoff is especially pronounced in high dimensions, particularly when positivity is poor. We then develop an optimization procedure that regularizes this bound while minimizing imbalance and outline how to use this approach as a sensitivity analysis for dependence on parametric modeling assumptions. We demonstrate the effectiveness of our approach through synthetic experiments and a real-world application, involving the generalization of randomized controlled trial estimates to a target population of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17180v1</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>David Arbour, Harsh Parikh, Bijan Niknam, Elizabeth Stuart, Kara Rudolph, Avi Feller</dc:creator>
    </item>
    <item>
      <title>Bayesian Semi-supervised Inference via a Debiased Modeling Approach</title>
      <link>https://arxiv.org/abs/2509.17385</link>
      <description>arXiv:2509.17385v1 Announce Type: cross 
Abstract: Inference in semi-supervised (SS) settings has gained substantial attention in recent years due to increased relevance in modern big-data problems. In a typical SS setting, there is a much larger-sized unlabeled data, containing only observations of predictors, and a moderately sized labeled data containing observations for both an outcome and the set of predictors. Such data naturally arises when the outcome, unlike the predictors, is costly or difficult to obtain. One of the primary statistical objectives in SS settings is to explore whether parameter estimation can be improved by exploiting the unlabeled data. We propose a novel Bayesian method for estimating the population mean in SS settings. The approach yields estimators that are both efficient and optimal for estimation and inference. The method itself has several interesting artifacts. The central idea behind the method is to model certain summary statistics of the data in a targeted manner, rather than the entire raw data itself, along with a novel Bayesian notion of debiasing. Specifying appropriate summary statistics crucially relies on a debiased representation of the population mean that incorporates unlabeled data through a flexible nuisance function while also learning its estimation bias. Combined with careful usage of sample splitting, this debiasing approach mitigates the effect of bias due to slow rates or misspecification of the nuisance parameter from the posterior of the final parameter of interest, ensuring its robustness and efficiency. Concrete theoretical results, via Bernstein--von Mises theorems, are established, validating all claims, and are further supported through extensive numerical studies. To our knowledge, this is possibly the first work on Bayesian inference in SS settings, and its central ideas also apply more broadly to other Bayesian semi-parametric inference problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17385v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ecosta.2025.05.001</arxiv:DOI>
      <arxiv:journal_reference>Econometrics and Statistics (2025)</arxiv:journal_reference>
      <dc:creator>G\"ozde Sert, Abhishek Chakrabortty, Anirban Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Functional effects models: Accounting for preference heterogeneity in panel data with machine learning</title>
      <link>https://arxiv.org/abs/2509.18047</link>
      <description>arXiv:2509.18047v1 Announce Type: cross 
Abstract: In this paper, we present a general specification for Functional Effects Models, which use Machine Learning (ML) methodologies to learn individual-specific preference parameters from socio-demographic characteristics, therefore accounting for inter-individual heterogeneity in panel choice data. We identify three specific advantages of the Functional Effects Model over traditional fixed, and random/mixed effects models: (i) by mapping individual-specific effects as a function of socio-demographic variables, we can account for these effects when forecasting choices of previously unobserved individuals (ii) the (approximate) maximum-likelihood estimation of functional effects avoids the incidental parameters problem of the fixed effects model, even when the number of observed choices per individual is small; and (iii) we do not rely on the strong distributional assumptions of the random effects model, which may not match reality. We learn functional intercept and functional slopes with powerful non-linear machine learning regressors for tabular data, namely gradient boosting decision trees and deep neural networks. We validate our proposed methodology on a synthetic experiment and three real-world panel case studies, demonstrating that the Functional Effects Model: (i) can identify the true values of individual-specific effects when the data generation process is known; (ii) outperforms both state-of-the-art ML choice modelling techniques that omit individual heterogeneity in terms of predictive performance, as well as traditional static panel choice models in terms of learning inter-individual heterogeneity. The results indicate that the FI-RUMBoost model, which combines the individual-specific constants of the Functional Effects Model with the complex, non-linear utilities of RUMBoost, performs marginally best on large-scale revealed preference panel data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18047v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Salvad\'e, Tim Hillel</dc:creator>
    </item>
    <item>
      <title>Distributional Effects with Two-Sided Measurement Error: An Application to Intergenerational Income Mobility</title>
      <link>https://arxiv.org/abs/2107.09235</link>
      <description>arXiv:2107.09235v4 Announce Type: replace 
Abstract: This paper considers identification and estimation of distributional effect parameters that depend on the joint distribution of an outcome and another variable of interest ("treatment") in a setting with "two-sided" measurement error -- that is, where both variables are possibly measured with error. Examples of these parameters in the context of intergenerational income mobility include transition matrices, rank-rank correlations, and the poverty rate of children as a function of their parents' income, among others. Building on recent work on quantile regression (QR) with measurement error in the outcome (particularly, Hausman, Liu, Luo, and Palmer (2021)), we show that, given (i) two linear QR models separately for the outcome and treatment conditional on other observed covariates and (ii) assumptions about the measurement error for each variable, one can recover the joint distribution of the outcome and the treatment. Besides these conditions, our approach does not require an instrument, repeated measurements, or distributional assumptions about the measurement error. Using recent data from the 1997 National Longitudinal Study of Youth, we find that accounting for measurement error notably reduces several estimates of intergenerational mobility parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.09235v4</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brantly Callaway, Tong Li, Irina Murtazashvili, Emmanuel Tsyawo</dc:creator>
    </item>
    <item>
      <title>Don't (fully) exclude me, it's not necessary! Causal inference with semi-IVs</title>
      <link>https://arxiv.org/abs/2303.12667</link>
      <description>arXiv:2303.12667v5 Announce Type: replace 
Abstract: This paper proposes semi-instrumental variables (semi-IVs) as an alternative to instrumental variables (IVs) to identify the causal effect of a binary (or discrete) endogenous treatment. A semi-IV is a less restrictive form of instrument: it affects the selection into treatment but is excluded only from one, not necessarily both, potential outcomes. Having two continuously distributed semi-IVs, one excluded from the potential outcome under treatment and the other from the potential outcome under control, is sufficient to nonparametrically point identify marginal treatment effect (MTE) and local average treatment effect (LATE) parameters. In practice, semi-IVs provide a solution to the challenge of finding valid IVs because they are often easier to find: many selection-specific shocks, policies, prices, costs, or benefits are valid semi-IVs. As an application, I estimate the returns to working in the manufacturing sector on earnings using sector-specific characteristics as semi-IVs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.12667v5</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christophe Bruneel-Zupanc</dc:creator>
    </item>
    <item>
      <title>Kernel Choice Matters for Local Polynomial Density Estimators at Boundaries</title>
      <link>https://arxiv.org/abs/2306.07619</link>
      <description>arXiv:2306.07619v3 Announce Type: replace 
Abstract: This paper examines kernel selection for local polynomial density (LPD) estimators at boundary points. Contrary to conventional wisdom, we demonstrate that the choice of kernel has a substantial impact on the efficiency of LPD estimators. In particular, we provide theoretical results and present simulation and empirical evidence showing that commonly used kernels, such as the triangular kernel, suffer from several efficiency issues: They yield a larger mean squared error than our preferred Laplace kernel. For inference, the efficiency loss is even more pronounced, with confidence intervals based on popular kernels being wide, whereas those based on the Laplace kernel are markedly tighter. Furthermore, the variance of the LPD estimator with such popular kernels explodes as the sample size decreases, reflecting the fact -- formally proven here -- that its finite-sample variance is infinite. This small-sample problem, however, can be avoided by employing kernels with unbounded support. Taken together, both asymptotic and finite-sample analyses justify the use of the Laplace kernel: Simply changing the kernel function improves the reliability of LPD estimation and inference, and its effect is numerically significant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.07619v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shunsuke Imai, Yuta Okamoto</dc:creator>
    </item>
    <item>
      <title>Covariate Balancing and the Equivalence of Weighting and Doubly Robust Estimators of Average Treatment Effects</title>
      <link>https://arxiv.org/abs/2310.18563</link>
      <description>arXiv:2310.18563v2 Announce Type: replace 
Abstract: How should researchers adjust for covariates? We show that if the propensity score is estimated using a specific covariate balancing approach, inverse probability weighting (IPW), augmented inverse probability weighting (AIPW), and inverse probability weighted regression adjustment (IPWRA) estimators are numerically equivalent for the average treatment effect (ATE), and likewise for the average treatment effect on the treated (ATT). The resulting weights are inherently normalized, making normalized and unnormalized IPW and AIPW identical. We discuss implications for instrumental variables and difference-in-differences estimators and illustrate with two applications how these numerical equivalences simplify analysis and interpretation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18563v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tymon S{\l}oczy\'nski, S. Derya Uysal, Jeffrey M. Wooldridge</dc:creator>
    </item>
    <item>
      <title>Testing for Underpowered Literatures</title>
      <link>https://arxiv.org/abs/2406.13122</link>
      <description>arXiv:2406.13122v3 Announce Type: replace 
Abstract: How many experimental studies would have come to different conclusions had they been run on larger samples? I show how to estimate the expected number of statistically significant results that a set of experiments would have reported had their sample sizes all been counterfactually increased. The proposed deconvolution estimator is asymptotically normal and adjusts for publication bias. Unlike related methods, this approach requires no assumptions of any kind about the distribution of true intervention treatment effects and allows for point masses. Simulations find good coverage even when the t-score is only approximately normal. An application to randomized trials (RCTs) published in economics journals finds that doubling every sample would increase the power of t-tests by 7.2 percentage points on average. This effect is smaller than for non-RCTs and comparable to systematic replications in laboratory psychology where previous studies enabled more accurate power calculations. This suggests that RCTs are on average relatively insensitive to sample size increases. Research funders who wish to raise power should generally consider sponsoring better-measured and higher quality experiments -- rather than only larger ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13122v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefan Faridani</dc:creator>
    </item>
    <item>
      <title>Partially Identified Rankings from Pairwise Interactions</title>
      <link>https://arxiv.org/abs/2410.18272</link>
      <description>arXiv:2410.18272v2 Announce Type: replace 
Abstract: This paper considers the problem of ranking objects based on their latent merits using data from pairwise interactions. We allow for incomplete observation of these interactions and study what can be inferred about rankings in such settings. First, we show that identification of the ranking depends on a trade-off between the tournament graph and the interaction function: in parametric models, such as the Bradley-Terry-Luce, rankings are point identified even with sparse graphs, whereas nonparametric models require dense graphs. Second, moving beyond point identification, we characterize the identified set in the nonparametric model under any tournament structure and represent it through moment inequalities. Finally, we propose a likelihood-based statistic to test whether a ranking belongs to the identified set. We study two testing procedures: one is finite-sample valid but computationally intensive; the other is easy to implement and valid asymptotically. We illustrate our results using Brazilian employer-employee data to study how workers rank firms when moving across jobs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18272v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Federico Crippa, Danil Fedchenko</dc:creator>
    </item>
    <item>
      <title>Uniform Confidence Band for Marginal Treatment Effect Function</title>
      <link>https://arxiv.org/abs/2501.17455</link>
      <description>arXiv:2501.17455v2 Announce Type: replace 
Abstract: This paper presents a method for constructing uniform confidence bands for the marginal treatment effect (MTE) function. The shape of the MTE function offers insight into how the unobserved propensity to receive treatment is related to the treatment effect. Our approach visualizes the statistical uncertainty of an estimated function, facilitating inferences about the function's shape. The proposed method is computationally inexpensive and requires only minimal information: sample size, standard errors, kernel function, and bandwidth. This minimal data requirement enables applications to both new analyses and published results without access to original data. We derive a Gaussian approximation for a local quadratic estimator and consider the approximation of the distribution of its supremum in polynomial order. Monte Carlo simulations demonstrate that our bands provide the desired coverage and are less conservative than those based on the Gumbel approximation. An empirical illustration regarding the returns to education is included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17455v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Toshiki Tsuda, Yanchun Jin, Ryo Okui</dc:creator>
    </item>
    <item>
      <title>On the Identification of Diagnostic Expectations: Econometric Insights from DSGE Models</title>
      <link>https://arxiv.org/abs/2509.08472</link>
      <description>arXiv:2509.08472v2 Announce Type: replace 
Abstract: This paper provides the first econometric evidence for diagnostic expectations (DE) in DSGE models. Using the identification framework of Qu and Tkachenko (2017), I show that DE generate dynamics unreplicable under rational expectations (RE), with no RE parameterization capable of matching the autocovariance implied by DE. Consequently, DE are not observationally equivalent to RE and constitute an endogenous source of macroeconomic fluctuations, distinct from both structural frictions and exogenous shocks. From an econometric perspective, DE preserve overall model identification but weaken the identification of shock variances. To ensure robust conclusions across estimation methods and equilibrium conditions, I extend Bayesian estimation with Sequential Monte Carlo sampling to the indeterminacy domain. These findings advance the econometric study of expectations and highlight the macroeconomic relevance of diagnostic beliefs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08472v2</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinting Guo</dc:creator>
    </item>
    <item>
      <title>Semiparametrics via parametrics and contiguity</title>
      <link>https://arxiv.org/abs/2501.09483</link>
      <description>arXiv:2501.09483v2 Announce Type: replace-cross 
Abstract: Inference on the parametric part of a semiparametric model is no trivial task. If one approximates the infinite dimensional part of the semiparametric model by a parametric function, one obtains a parametric model that is in some sense close to the semiparametric model and inference may proceed by the method of maximum likelihood. Under regularity conditions, the ensuing maximum likelihood estimator is asymptotically normal and efficient in the approximating parametric model. Thus one obtains a sequence of asymptotically normal and efficient estimators in a sequence of growing parametric models that approximate the semiparametric model and, intuitively, the limiting 'semiparametric' estimator should be asymptotically normal and efficient as well. In this paper we make this intuition rigorous: we move much of the semiparametric analysis back into classical parametric terrain, and then translate our parametric results back to the semiparametric world by way of contiguity. Our approach departs from the conventional sieve literature by being more specific about the approximating parametric models, by working not only with but also under these when treating the parametric models, and by taking full advantage of the mutual contiguity that we require between the parametric and semiparametric models. We illustrate our theory with two canonical examples of semiparametric models, namely the partially linear regression model and the Cox regression model. An upshot of our theory is a new, relatively simple, and rather parametric proof of the efficiency of the Cox partial likelihood estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09483v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam Lee, Emil A. Stoltenberg, Per A. Mykland</dc:creator>
    </item>
    <item>
      <title>Refining the Notion of No Anticipation in Difference-in-Differences Studies</title>
      <link>https://arxiv.org/abs/2507.12891</link>
      <description>arXiv:2507.12891v2 Announce Type: replace-cross 
Abstract: We address an ambiguity in identification strategies using difference-in-differences, which are widely applied in empirical research, particularly in economics. The assumption commonly referred to as the "no-anticipation assumption" states that treatment has no effect on outcomes before its implementation. However, because standard causal models rely on a temporal structure in which causes precede effects, such an assumption seems to be inherently satisfied. This raises the question of whether the assumption is repeatedly stated out of redundancy or because the formal statements fail to capture the intended subject-matter interpretation. We argue that confusion surrounding the no-anticipation assumption arises from ambiguity in the intervention considered and that current formulations of the assumption are ambiguous. Therefore, new definitions and identification results are proposed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12891v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Piccininni, Eric J. Tchetgen Tchetgen, Mats J. Stensrud</dc:creator>
    </item>
  </channel>
</rss>
