<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Feb 2026 05:00:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Differentially private graph coloring</title>
      <link>https://arxiv.org/abs/2602.13460</link>
      <description>arXiv:2602.13460v1 Announce Type: new 
Abstract: Differential Privacy is the gold standard in privacy-preserving data analysis. This paper addresses the challenge of producing a differentially edge-private vertex coloring. In this paper, we present two novel algorithms to approach this problem. Both algorithms initially randomly colors each vertex from a fixed size palette, then applies the exponential mechanism to locally resample colors for either all or a chosen subset of the vertices.
  Any non-trivial differentially edge private coloring of graph needs to be defective. A coloring of a graph is k defective if all vertices of the graph share it's assigned color with at most k of its neighbors. This is the metric by which we will measure the utility of our algorithms. Our first algorithm applies to d-inductive graphs. Assume we have a d-inductive graph with n vertices and max degree $\Delta$. We show that our algorithm provides a \(3\epsilon\)-differentially private coloring with \(O(\frac{\log n}{\epsilon}+d)\) max defectiveness, given a palette of size $\Theta(\frac{\Delta}{\log n}+\frac{1}{\epsilon})$ Furthermore, we show that this algorithm can generalize to $O(\frac{\Delta}{c\epsilon}+d)$ defectiveness, where c is the size of the palette and $c=O(\frac{\Delta}{\log n})$. Our second algorithm utilizes noisy thresholding to guarantee \(O(\frac{\log n}{\epsilon})\) max defectiveness, given a palette of size $\Theta(\frac{\Delta}{\log n}+\frac{1}{\epsilon})$, generalizing to all graphs rather than just d-inductive ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13460v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Michael Xie, Jiayi Wu, Dung Nguyen, Aravind Srinivasan</dc:creator>
    </item>
    <item>
      <title>Optimal-Time Mapping in Run-Length Compressed PBWT</title>
      <link>https://arxiv.org/abs/2602.13461</link>
      <description>arXiv:2602.13461v1 Announce Type: new 
Abstract: The Positional Burrows--Wheeler Transform (PBWT) is a data structure designed for efficiently representing and querying large collections of sequences, such as haplotype panels in genomics. Forward and backward stepping operations -- analogues to LF- and FL-mapping in the traditional BWT -- are fundamental to the PBWT, underpinning many algorithms based on the PBWT for haplotype matching and related analyses. Although the run-length encoded variant of the PBWT (also known as the $\mu$-PBWT) achieves $O(\newR)$-word space usage, where $\newR$ is the total number of runs, no data structure supporting both forward and backward stepping in constant time within this space bound was previously known. In this paper, we consider the multi-allelic PBWT that is extended from its original binary form to a general ordered alphabet $\{0, \dots, \sigma-1\}$. We first establish bounds on the size $\newR$ and then introduce a new $O(\newR)$-word data structure built over a list of haplotypes $\{S_1, \dots, S_\height\}$, each of length $\width$, that supports constant-time forward and backward stepping.
  We further revisit two key applications -- haplotype retrieval and prefix search -- leveraging our efficient forward stepping technique. Specifically, we design an $O(\newR)$-word space data structure that supports haplotype retrieval in $O(\log \log_{\word} h + \width)$ time. For prefix search, we present an $O(\height + \newR)$-word data structure that answers queries in $O(m' \log\log_{\word} \sigma + \occ)$ time, where $m'$ denotes the length of the longest common prefix returned and $\occ$ denotes the number of haplotypes prefixed the longest prefix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13461v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paola Bonizzoni, Davide Cozzi, Younan Gao</dc:creator>
    </item>
    <item>
      <title>How to Train Your Filter: Should You Learn, Stack or Adapt?</title>
      <link>https://arxiv.org/abs/2602.13484</link>
      <description>arXiv:2602.13484v1 Announce Type: new 
Abstract: Filters are ubiquitous in computer science, enabling space-efficient approximate membership testing. Since Bloom filters were introduced in 1970, decades of work improved their space efficiency and performance. Recently, three new paradigms have emerged offering orders-of-magnitude improvements in false positive rates (FPRs) by using information beyond the input set: (1) learned filters train a model to distinguish (non)members, (2) stacked filters use negative workload samples to build cascading layers, and (3) adaptive filters update internal representation in response to false positive feedback. Yet each paradigm targets specific use cases, introduces complex configuration tuning, and has been evaluated in isolation. This results in unclear trade-offs and a gap in understanding how these approaches compare and when each is most appropriate. This paper presents the first comprehensive evaluation of learned, stacked, and adaptive filters across real-world datasets and query workloads. Our results reveal critical trade-offs: (1) Learned filters achieve up to 10^2 times lower FPRs but exhibit high variance and lack robustness under skewed or dynamic workloads. Critically, model inference overhead leads to up to 10^4 times slower query latencies than stacked or adaptive filters. (2) Stacked filters reliably achieve up to 10^3 times lower FPRs on skewed workloads but require workload knowledge. (3) Adaptive filters are robust across settings, achieving up to 10^3 times lower FPRs under adversarial queries without workload assumptions. Based on our analysis, learned filters suit stable workloads where input features enable effective model training and space constraints are paramount, stacked filters excel when reliable query distributions are known, and adaptive filters are most generalizable, providing robust theoretically bound guarantees even in dynamic or adversarial environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13484v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Diandre Miguel Sabale, Wolfgang Gatterbauer, Prashant Pandey</dc:creator>
    </item>
    <item>
      <title>Probabilistic RNA Designability via Interpretable Ensemble Approximation and Dynamic Decomposition</title>
      <link>https://arxiv.org/abs/2602.13610</link>
      <description>arXiv:2602.13610v1 Announce Type: new 
Abstract: Motivation: RNA design aims to find RNA sequences that fold into a given target secondary structure, a problem also known as RNA inverse folding. However, not all target structures are designable. Recent advances in RNA designability have focused primarily on minimum free energy (MFE)-based criteria, while ensemble-based notions of designability remain largely underexplored. To address this gap, we introduce a theory of ensemble approximation and a probability decomposition framework for bounding the folding probabilities of RNA structures in an explainable way. We further develop a linear-time dynamic programming algorithm that efficiently searches over exponentially many decompositions and identifies the optimal one that yields the tightest probabilistic bound for a given structure. Results: Applying our methods to both native and artificial RNA structures in the ArchiveII and Eterna100 benchmarks, we obtained probability bounds that are much tighter than prior approaches. In addition, our methods further provide anatomical tools for analyzing RNA structures and understanding the sources of design difficulty at the motif level. Availability: Source code and data are available at https://github.com/shanry/RNA-Undesign. Supplementary information: Supplementary text and data are available in a separate PDF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13610v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianshuo Zhou, David H. Mathews, Liang Huang</dc:creator>
    </item>
    <item>
      <title>Compressed Index with Construction in Compressed Space</title>
      <link>https://arxiv.org/abs/2602.13735</link>
      <description>arXiv:2602.13735v1 Announce Type: new 
Abstract: Suppose that we are given a string $s$ of length $n$ over an alphabet $\{0,1,\ldots,n^{O(1)}\}$ and $\delta$ is a compression measure for $s$ called string complexity. We describe an index on $s$ with $O(\delta\log\frac{n}{\delta})$ space, measured in $O(\log n)$-bit machine words, that can search in $s$ any string of length $m$ in $O(m + (\mathrm{occ} + 1)\log^\epsilon n)$ time, where $\mathrm{occ}$ is the number of found occurrences and $\epsilon &gt; 0$ is any fixed constant (the big-O in the space bound hides factor $\frac{1}{\epsilon}$). Crucially, the index can be built within this space in $O(n\log n)$ expected time by one left-to-right pass on the string $s$ in a streaming fashion. The index does not use the Karp--Rabin fingerprints, and the randomization in the construction time can be eliminated by using deterministic dictionaries instead of hash tables (with a slowdown). The search time matches currently best results and the space is almost optimal (the known optimum is $O(\delta\log\frac{n}{\delta\alpha})$, where $\alpha = \log_\sigma n$ and $\sigma$ is the alphabet size, and it coincides with $O(\delta\log\frac{n}{\delta})$ when $\delta = O(n / \alpha^2)$). This is the first index that can be constructed within such space and with such time guarantees. To avoid uninteresting marginal cases, all above bounds are stated for $\delta \ge \Omega(\log\log n)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13735v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dmitry Kosolobov</dc:creator>
    </item>
    <item>
      <title>Spanning tree congestion of proper interval graphs</title>
      <link>https://arxiv.org/abs/2602.13756</link>
      <description>arXiv:2602.13756v1 Announce Type: new 
Abstract: We show that the spanning tree congestion problem is NP-complete even for proper interval graphs of linear clique-width at most 4.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13756v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yota Otachi</dc:creator>
    </item>
    <item>
      <title>Min-Max Connected Multiway Cut</title>
      <link>https://arxiv.org/abs/2602.13861</link>
      <description>arXiv:2602.13861v1 Announce Type: new 
Abstract: We introduce a variant of the multiway cut that we call the min-max connected multiway cut. Given a graph $G=(V,E)$ and a set $\Gamma\subseteq V$ of $t$ terminals, partition $V$ into $t$ parts such that each part is connected and contains exactly one terminal; the objective is to minimize the maximum weight of the edges leaving any part of the partition. This problem is a natural modification of the standard multiway cut problem and it differs from it in two ways: first, the cost of a partition is defined to be the maximum size of the boundary of any part, as opposed to the sum of all boundaries, and second, the subgraph induced by each part is required to be connected. Although the modified objective function has been considered before in the literature under the name min-max multiway cut, the requirement on each component to be connected has not been studied as far as we know.
  We show various hardness results for this problem, including a proof of weak NP-hardness of the weighted version of the problem on graphs with tree-width two, and provide a pseudopolynomial time algorithm as well as an FPTAS for the weighted problem on trees. As a consequence of our investigation we also show that the (unconstrained) min-max multiway cut problem is NP-hard even for three terminals, strengthening the known results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13861v1</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <category>math.OC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hans Raj Tiwary, Petr Kolman</dc:creator>
    </item>
    <item>
      <title>Faster Parameterized Vertex Multicut</title>
      <link>https://arxiv.org/abs/2602.13981</link>
      <description>arXiv:2602.13981v1 Announce Type: new 
Abstract: In the {\sc Vertex Multicut} problem the input consists of a graph $G$, integer $k$, and a set $\mathbf{T} = \{(s_1, t_1), \ldots, (s_p, t_p)\}$ of pairs of vertices of $G$. The task is to find a set $X$ of at most $k$ vertices such that, for every $(s_i, t_i) \in \mathbf{T}$, there is no path from $s_i$ to $t_i$ in $G - X$. Marx and Razgon [STOC 2011 and SICOMP 2014] and Bousquet, Daligault, and Thomass\'{e} [STOC 2011 and SICOMP 2018] independently and simultaneously gave the first algorithms for {\sc Vertex Multicut} with running time $f(k)n^{O(1)}$. The running time of their algorithms is $2^{O(k^3)}n^{O(1)}$ and $2^{O(k^{O(1)})}n^{O(1)}$, respectively. As part of their result, Marx and Razgon introduce the {\em shadow removal} technique, which was subsequently applied in algorithms for several parameterized cut and separation problems. The shadow removal step is the only step of the algorithm of Marx and Razgon which requires $2^{O(k^3)}n^{O(1)}$ time. Chitnis et al. [TALG 2015] gave an improved version of the shadow removal step, which, among other results, led to a $k^{O(k^2)}n^{O(1)}$ time algorithm for {\sc Vertex Multicut}.
  We give a faster algorithm for the {\sc Vertex Multicut} problem with running time $k^{O(k)}n^{O(1)}$. Our main technical contribution is a refined shadow removal step for vertex separation problems that only introduces an overhead of $k^{O(k)}\log n$ time. The new shadow removal step implies a $k^{O(k^2)}n^{O(1)}$ time algorithm for {\sc Directed Subset Feedback Vertex Set} and a $k^{O(k)}n^{O(1)}$ time algorithm for {\sc Directed Multiway Cut}, improving over the previously best known algorithms of Chitnis et al. [TALG 2015].</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13981v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huairui Chu, Yuxi Liu, Daniel Lokshtanov, Junqiang Peng, Kangyi Tian, Mingyu Xiao</dc:creator>
    </item>
    <item>
      <title>Counting Balanced Triangles on Social Networks With Uncertain Edge Signs</title>
      <link>https://arxiv.org/abs/2602.14084</link>
      <description>arXiv:2602.14084v1 Announce Type: new 
Abstract: On signed social networks, balanced and unbalanced triangles are a critical motif due to their role as the foundations of Structural Balance Theory. The uses for these motifs have been extensively explored in networks with known edge signs, however in the real-world graphs with ground-truth signs are near non-existent, particularly on a large-scale. In reality, edge signs are inferred via various techniques with differing levels of confidence, meaning the edge signs on these graphs should be modelled with a probability value. In this work, we adapt balanced and unbalanced triangles to a setting with uncertain edge signs and explore the problems of triangle counting and enumeration. We provide a baseline and improved method (leveraging the inherent information provided by the edge probabilities in order to reduce the search space) for fast exact counting and enumeration. We also explore approximate solutions for counting via different sampling approaches, including leveraging insights from our improved exact solution to significantly reduce the runtime of each sample resulting in upwards of two magnitudes more queries executed per second. We evaluate the efficiency of all our solutions as well as examine the effectiveness of our sampling approaches on real-world topological networks with a variety of probability distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14084v1</guid>
      <category>cs.DS</category>
      <category>cs.SI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Zhou, Haoyang Li, Anxin Tian, Zhiyuan Li, Yue Wang</dc:creator>
    </item>
    <item>
      <title>Catalytic Tree Evaluation From Matching Vectors</title>
      <link>https://arxiv.org/abs/2602.14320</link>
      <description>arXiv:2602.14320v1 Announce Type: new 
Abstract: We give new algorithms for tree evaluation (S. Cook et al. TOCT 2012) in the catalytic-computing model (Buhrman et al. STOC 2014). Two existing approaches aim to solve tree evaluation (TreeEval) in low space: on the one hand, J. Cook and Mertz (STOC 2024) give an algorithm for TreeEval running in super-logarithmic space $O(\log n\log\log n)$ and super-polynomial time $n^{O(\log\log n)}$. On the other hand, a simple reduction from TreeEval to circuit evaluation, combined with the result of Buhrman et al. (STOC 2014), gives a catalytic algorithm for TreeEval running in logarithmic $O(\log n)$ free space and polynomial time, but with polynomial catalytic space.
  We show that the latter result can be improved. We give a catalytic algorithm for TreeEval with logarithmic $O(\log n)$ free space, polynomial runtime, and subpolynomial $2^{\log^\epsilon n}$ catalytic space (for any $\epsilon &gt; 0$). Our result gives the first natural problem known to be solvable with logarithmic free space and even $n^{1-\epsilon}$ catalytic space, that is not known to be in standard logspace even under assumptions. Our result immediately implies an improved simulation of time by catalytic space, by the reduction of Williams (STOC 2025).
  Our catalytic TreeEval algorithm is inspired by a connection to matching vector families and private information retrieval, and improved constructions of (uniform) matching vector families would imply improvements to our algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14320v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandra Henzinger, Edward Pyne, Seyoon Ragavan</dc:creator>
    </item>
    <item>
      <title>Sublinear-Time Lower Bounds for Approximating Matching Size using Non-Adaptive Queries</title>
      <link>https://arxiv.org/abs/2602.14326</link>
      <description>arXiv:2602.14326v1 Announce Type: new 
Abstract: We study the problem of estimating the size of the maximum matching in the sublinear-time setting. This problem has been extensively studied, with several known upper and lower bounds. A notable result by Behnezhad (FOCS 2021) established a 2-approximation in ~O(n) time.
  However, all known upper and lower bounds are in the adaptive query model, where each query can depend on previous answers. In contrast, non-adaptive query models-where the distribution over all queries must be fixed in advance-are widely studied in property testing, often revealing fundamental gaps between adaptive and non-adaptive complexities. This raises the natural question: is adaptivity also necessary for approximating the maximum matching size in sublinear time? This motivates the goal of achieving a constant or even a polylogarithmic approximation using ~O(n) non-adaptive adjacency list queries, similar to what was done by Behnezhad using adaptive queries.
  We show that this is not possible by proving that any randomized non-adaptive algorithm achieving an n^{1/3 - gamma}-approximation, for any constant gamma &gt; 0, with probability at least 2/3, must make Omega(n^{1 + eps}) adjacency list queries, for some constant eps &gt; 0 depending on gamma. This result highlights the necessity of adaptivity in achieving strong approximations. However, non-trivial upper bounds are still achievable: we present a simple randomized algorithm that achieves an n^{1/2}-approximation in O(n log^2 n) queries.
  Moreover, our lower bound also extends to the newly defined variant of the non-adaptive model, where queries are issued according to a fixed query tree, introduced by Azarmehr, Behnezhad, Ghafari, and Sudan (FOCS 2025) in the context of Local Computation Algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14326v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1137/1.9781611978971.182</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 2026 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), SIAM, 2026, pp. 5027-5065</arxiv:journal_reference>
      <dc:creator>Vihan Shah</dc:creator>
    </item>
    <item>
      <title>Sensitivity of Repetitiveness Measures to String Reversal</title>
      <link>https://arxiv.org/abs/2602.14385</link>
      <description>arXiv:2602.14385v1 Announce Type: new 
Abstract: We study the impact that string reversal can have on several repetitiveness measures. First, we exhibit an infinite family of strings where the number, $r$, of runs in the run-length encoding of the Burrows--Wheeler transform (BWT) can increase additively by $\Theta(n)$ when reversing the string. This substantially improves the known $\Omega(\log n)$ lower-bound for the additive sensitivity of $r$ and it is asymptotically tight. We generalize our result to other variants of the BWT, including the variant with an appended end-of-string symbol and the bijective BWT. We show that an analogous result holds for the size $z$ of the Lempel--Ziv 77 (LZ) parsing of the text, and also for some of its variants, including the non-overlapping LZ parsing, and the LZ-end parsing. Moreover, we describe a family of strings for which the ratio $z(w^R)/z(w)$ approaches $3$ from below as $|w|\rightarrow \infty$. We also show an asymptotically tight lower-bound of $\Theta(n)$ for the additive sensitivity of the size $v$ of the smallest lexicographic parsing to string reversal. Finally, we show that the multiplicative sensitivity of $v$ to reversing the string is $\Theta(\log n)$, and this lower-bound is also tight. Overall, our results expose the limitations of repetitiveness measures that are widely used in practice, against string reversal -- a simple and natural data transformation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14385v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hideo Bannai, Yuto Fujie, Peaker Guo, Shunsuke Inenaga, Yuto Nakashima, Simon J. Puglisi, Cristian Urbina</dc:creator>
    </item>
    <item>
      <title>Faster Pseudo-Deterministic Minimum Cut</title>
      <link>https://arxiv.org/abs/2602.14550</link>
      <description>arXiv:2602.14550v1 Announce Type: new 
Abstract: Pseudo-deterministic algorithms are randomized algorithms that, with high constant probability, output a fixed canonical solution. The study of pseudo-deterministic algorithms for the global minimum cut problem was recently initiated by Agarwala and Varma [ITCS'26], who gave a black-box reduction incurring an $O(\log n \log \log n)$ overhead. We introduce a natural graph-theoretic tie-breaking mechanism that uniquely selects a canonical minimum cut. Using this mechanism, we obtain: (i) A pseudo-deterministic minimum cut algorithm for weighted graphs running in $O(m\log^2 n)$ time, eliminating the $O(\log n \log \log n)$ overhead of prior work and matching existing randomized algorithms. (ii) The first pseudo-deterministic algorithm for maintaining a canonical minimum cut in a fully-dynamic unweighted graph, with $\mathrm{polylog}(n)$ update time and $\tilde{O}(n)$ query time. (iii) Improved pseudo-deterministic algorithms for unweighted graphs in the dynamic streaming and cut-query models of computation, matching the best randomized algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14550v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yotam Kenneth-Mordoch</dc:creator>
    </item>
    <item>
      <title>Near-Linear Time Computation of Welzl Orders on Graphs with Linear Neighborhood Complexity</title>
      <link>https://arxiv.org/abs/2602.14625</link>
      <description>arXiv:2602.14625v1 Announce Type: new 
Abstract: Orders with low crossing number, introduced by Welzl, are a fundamental tool in range searching and computational geometry. Recently, they have found important applications in structural graph theory: set systems with linear shatter functions correspond to graph classes with linear neighborhood complexity. For such systems, Welzl's theorem guarantees the existence of orders with only $\mathcal{O}(\log^2 n)$ crossings. A series of works has progressively improved the runtime for computing such orders, from Chazelle and Welzl's original $\mathcal{O}(|U|^3 |\mathcal{F}|)$ bound, through Har-Peled's $\mathcal{O}(|U|^2|\mathcal{F}|)$, to the recent sampling-based methods of Csik\'os and Mustafa.
  We present a randomized algorithm that computes Welzl orders for set systems with linear primal and dual shatter functions in time $\mathcal{O}(\|S\| \log \|S\|)$, where $\|S\| = |U| + \sum_{X \in \mathcal{F}} |X|$ is the size of the canonical input representation. As an application, we compute compact neighborhood covers in graph classes with (near-)linear neighborhood complexity in time \(\mathcal{O}(n \log n)\) and improve the runtime of first-order model checking on monadically stable graph classes from $\mathcal{O}(n^{5+\varepsilon})$ to $\mathcal{O}(n^{3+\varepsilon})$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14625v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Dreier, Clemens Kuske</dc:creator>
    </item>
    <item>
      <title>On the Parameterized Tractability of Packing Vertex-Disjoint A-Paths with Length Constraints</title>
      <link>https://arxiv.org/abs/2602.14768</link>
      <description>arXiv:2602.14768v1 Announce Type: new 
Abstract: Given an undirected graph G and a set A \subseteq V(G), an A-path is a path in G that starts and ends at two distinct vertices of A with intermediate vertices in V(G) \setminus A. An A-path is called an (A,\ell)-path if the length of the path is exactly \ell. In the {\sc (A, \ell)-Path Packing} problem (ALPP), we seek to determine whether there exist k vertex-disjoint (A, \ell)-paths in G or not. We pursue this problem with respect to structural parameters. We prove that ALPP is W[1]-hard when it is parameterized by the combined parameter distance to path (dtp) and |A|. In addition, we consider the combined parameters distance to cluster (cvd) + |A| and distance to cluster (cvd) + \ell. For both these combined parameters, we provide FPT algorithms. Finally, we consider the vertex cover number (vc) as the parameter and provide a kernel with O(vc^2) vertices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14768v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Susobhan Bandopadhyay, Aritra Banik, Diptapriyo Majumdar, Abhishek Sahu</dc:creator>
    </item>
    <item>
      <title>Expander Decomposition with Almost Optimal Overhead</title>
      <link>https://arxiv.org/abs/2602.15015</link>
      <description>arXiv:2602.15015v1 Announce Type: new 
Abstract: We present the first polynomial-time algorithm for computing a near-optimal \emph{flow}-expander decomposition. Given a graph $G$ and a parameter $\phi$, our algorithm removes at most a $\phi\log^{1+o(1)}n$ fraction of edges so that every remaining connected component is a $\phi$-\emph{flow}-expander (a stronger guarantee than being a $\phi$-\emph{cut}-expander). This achieves overhead $\log^{1+o(1)}n$, nearly matching the $\Omega(\log n)$ graph-theoretic lower bound that already holds for cut-expander decompositions, up to a $\log^{o(1)}n$ factor. Prior polynomial-time algorithms required removing $O(\phi\log^{1.5}n)$ and $O(\phi\log^{2}n)$ fractions of edges to guarantee $\phi$-cut-expander and $\phi$-flow-expander components, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15015v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikhil Bansal, Arun Jambulapati, Thatchaphol Saranurak</dc:creator>
    </item>
    <item>
      <title>Quantum Speedups for Group Relaxations of Integer Linear Programs</title>
      <link>https://arxiv.org/abs/2602.13494</link>
      <description>arXiv:2602.13494v1 Announce Type: cross 
Abstract: Integer Linear Programs (ILPs) are a flexible and ubiquitous model for discrete optimization problems. Solving ILPs is \textsf{NP-Hard} yet of great practical importance. Super-quadratic quantum speedups for ILPs have been difficult to obtain because classical algorithms for many-constraint ILPs are global and exhaustive, whereas quantum frameworks that offer super-quadratic speedup exploit local structure of the objective and feasible set. We address this via quantum algorithms for Gomory's group relaxation. The group relaxation of an ILP is obtained by dropping nonnegativity on variables that are positive in the optimal solution of the linear programming (LP) relaxation, while retaining integrality of the decision variables. We present a competitive feasibility-preserving classical local-search algorithm for the group relaxation, and a corresponding quantum algorithm that, under reasonable technical conditions, achieves a super-quadratic speedup. When the group relaxation satisfies a nondegeneracy condition analogous to, but stronger than, LP non-degeneracy, our approach yields the optimal solution to the original ILP. Otherwise, the group relaxation tightens bounds on the optimal objective value of the ILP, and can improve downstream branch-and-cut by reducing the integrality gap; we numerically observe this on several practically relevant ILPs. To achieve these results, we derive efficiently constructible constraint-preserving mixers for the group relaxation with favorable spectral properties, which are of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13494v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brandon Augustino, Dylan Herman, Guneykan Ozgul, Jacob Watkins, Atithi Acharya, Enrico Fontana, Junhyung Lyle Kim, Shouvanik Chakrabarti</dc:creator>
    </item>
    <item>
      <title>High-accuracy log-concave sampling with stochastic queries</title>
      <link>https://arxiv.org/abs/2602.14342</link>
      <description>arXiv:2602.14342v1 Announce Type: cross 
Abstract: We show that high-accuracy guarantees for log-concave sampling -- that is, iteration and query complexities which scale as $\mathrm{poly}\log(1/\delta)$, where $\delta$ is the desired target accuracy -- are achievable using stochastic gradients with subexponential tails. Notably, this exhibits a separation with the problem of convex optimization, where stochasticity (even additive Gaussian noise) in the gradient oracle incurs $\mathrm{poly}(1/\delta)$ queries. We also give an information-theoretic argument that light-tailed stochastic gradients are necessary for high accuracy: for example, in the bounded variance case, we show that the minimax-optimal query complexity scales as $\Theta(1/\delta)$. Our framework also provides similar high accuracy guarantees under stochastic zeroth order (value) queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14342v1</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fan Chen, Sinho Chewi, Constantinos Daskalakis, Alexander Rakhlin</dc:creator>
    </item>
    <item>
      <title>Fine-Grained Complexity for Quantum Problems from Size-Preserving Circuit-to-Hamiltonian Constructions</title>
      <link>https://arxiv.org/abs/2602.14379</link>
      <description>arXiv:2602.14379v1 Announce Type: cross 
Abstract: The local Hamiltonian (LH) problem is the canonical $\mathsf{QMA}$-complete problem introduced by Kitaev. In this paper, we show its hardness in a very strong sense: we show that the 3-local Hamiltonian problem on $n$ qubits cannot be solved classically in time $O(2^{(1-\varepsilon)n})$ for any $\varepsilon&gt;0$ under the Strong Exponential-Time Hypothesis (SETH), and cannot be solved quantumly in time $O(2^{(1-\varepsilon)n/2})$ for any $\varepsilon&gt;0$ under the Quantum Strong Exponential-Time Hypothesis (QSETH). These lower bounds give evidence that the currently known classical and quantum algorithms for LH cannot be significantly improved.
  Furthermore, we are able to demonstrate fine-grained complexity lower bounds for approximating the quantum partition function (QPF) with an arbitrary constant relative error. Approximating QPF with relative error is known to be equivalent to approximately counting the dimension of the solution subspace of $\mathsf{QMA}$ problems. We show the SETH and QSETH hardness to estimate QPF with constant relative error. We then provide a quantum algorithm that runs in $O(\sqrt{2^n})$ time for an arbitrary $1/\mathrm{poly}(n)$ relative error, matching our lower bounds and improving the state-of-the-art algorithm by Bravyi, Chowdhury, Gosset, and Wocjan (Nature Physics 2022) in the low-temperature regime.
  To prove our fine-grained lower bounds, we introduce the first size-preserving circuit-to-Hamiltonian construction that encodes the computation of a $T$-time quantum circuit acting on $N$ qubits into a $(d+1)$-local Hamiltonian acting on $N+O(T^{1/d})$ qubits. This improves the standard construction based on the unary clock, which uses $N+O(T)$ qubits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14379v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Nai-Hui Chia, Atsuya Hasegawa, Fran\c{c}ois Le Gall, Yu-Ching Shen</dc:creator>
    </item>
    <item>
      <title>Constrained and Composite Sampling via Proximal Sampler</title>
      <link>https://arxiv.org/abs/2602.14478</link>
      <description>arXiv:2602.14478v1 Announce Type: cross 
Abstract: We study two log-concave sampling problems: constrained sampling and composite sampling. First, we consider sampling from a target distribution with density proportional to $\exp(-f(x))$ supported on a convex set $K \subset \mathbb{R}^d$, where $f$ is convex. The main challenge is enforcing feasibility without degrading mixing. Using an epigraph transformation, we reduce this task to sampling from a nearly uniform distribution over a lifted convex set in $\mathbb{R}^{d+1}$. We then solve the lifted problem using a proximal sampler. Assuming only a separation oracle for $K$ and a subgradient oracle for $f$, we develop an implementation of the proximal sampler based on the cutting-plane method and rejection sampling. Unlike existing constrained samplers that rely on projection, reflection, barrier functions, or mirror maps, our approach enforces feasibility using only minimal oracle access, resulting in a practical and unbiased sampler without knowing the geometry of the constraint set.
  Second, we study composite sampling, where the target is proportional to $\exp(-f(x)-h(x))$ with closed and convex $f$ and $h$. This composite structure is standard in Bayesian inference with $f$ modeling data fidelity and $h$ encoding prior information. We reduce composite sampling via an epigraph lifting of $h$ to constrained sampling in $\mathbb{R}^{d+1}$, which allows direct application of the constrained sampling algorithm developed in the first part. This reduction results in a double epigraph lifting formulation in $\mathbb{R}^{d+2}$, on which we apply a proximal sampler. By keeping $f$ and $h$ separate, we further demonstrate how different combinations of oracle access (such as subgradient and proximal) can be leveraged to construct separation oracles for the lifted problem. For both sampling problems, we establish mixing time bounds measured in R\'enyi and $\chi^2$ divergences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14478v1</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thanh Dang, Jiaming Liang</dc:creator>
    </item>
    <item>
      <title>FO and MSO Model Checking on Temporal Graphs</title>
      <link>https://arxiv.org/abs/2602.14592</link>
      <description>arXiv:2602.14592v1 Announce Type: cross 
Abstract: Algorithmic meta-theorems provide an important tool for showing tractability of graph problems on graph classes defined by structural restrictions. While such results are well established for static graphs, corresponding frameworks for temporal graphs are comparatively limited.
  In this work, we revisit past applications of logical meta-theorems to temporal graphs and develop an extended unifying logical framework. Our first contribution is the introduction of logical encodings for the parameters vertex-interval-membership (VIM) width and tree-interval-membership (TIM) width, parameters which capture the signature of vertex and component activity over time. Building on this, we extend existing monadic second-order (MSO) meta-theorems for bounded lifetime and temporal degree to the parameters VIM and TIM width, and establish novel first-order (FO) meta-theorems for all four parameters.
  Finally, we signpost a modular lexicon of reusable FO and MSO formulas for a broad range of temporal graph problems, and give an example. This lexicon allows new problems to be expressed compositionally and directly yields fixed-parameter tractability results across the four parameters we consider.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14592v1</guid>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michelle D\"oring, Jessica Enright, Laura Larios-Jones, George Skretas</dc:creator>
    </item>
    <item>
      <title>Evaluation of Dynamic Vector Bin Packing for Virtual Machine Placement</title>
      <link>https://arxiv.org/abs/2602.14704</link>
      <description>arXiv:2602.14704v1 Announce Type: cross 
Abstract: Virtual machine placement is a crucial challenge in cloud computing for efficiently utilizing physical machine resources in data centers. Virtual machine placement can be formulated as a MinUsageTime Dynamic Vector Bin Packing (DVBP) problem, aiming to minimize the total usage time of the physical machines. This paper evaluates state-of-the-art MinUsageTime DVBP algorithms in non-clairvoyant, clairvoyant and learning-augmented online settings, where item durations (virtual machine lifetimes) are unknown, known and predicted, respectively. Besides the algorithms taken from the literature, we also develop several new algorithms or enhancements. Empirical experimentation is carried out with real-world datasets of Microsoft Azure. The insights from the experimental results are discussed to explore the structures of algorithms and promising design elements that work well in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14704v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zong Yu Lee, Xueyan Tang</dc:creator>
    </item>
    <item>
      <title>Constant-Time Dynamic Enumeration of Word Infixes in a Regular Language</title>
      <link>https://arxiv.org/abs/2602.14748</link>
      <description>arXiv:2602.14748v1 Announce Type: cross 
Abstract: For a fixed regular language $L$, the enumeration of $L$-infixes is the following task: we are given an input word $w = a_1 \cdots a_n$ and we must enumerate the infixes of $w$ that belong to $L$, i.e., the pairs $i \leq j$ such that $a_i \cdots a_j \in L$. We are interested in dynamic enumeration of $L$-infixes, where we must additionally support letter substitution updates on $w$ (e.g., "replace the $i$-th letter of $w$ by a letter $a$"). Each update changes the set of infixes to enumerate, and resets the enumeration state.
  We study for which regular languages $L$ we can perform dynamic enumeration of $L$-infixes in constant delay (i.e., the next infix is always produced in constant time) and constant additional memory throughout the enumeration, while supporting each update in constant time.
  We show that, for languages $L$ with a neutral letter, if the language $L$ belongs to the class ZG and is extensible (i.e., if $u \in L$ and $u$ is a factor of $v$ then $v \in L$), then dynamic enumeration of $L$-infixes can be achieved with a simple algorithm that ensures constant-time updates and constant delay, but not constant additional memory. Our main contribution is then to show an algorithm that additionally uses only constant additional memory, and applies to a more general class of semi-extensible ZG languages for which we give several equivalent characterizations. We further discuss whether our results can be generalized to larger language classes and show some (conditional) lower bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14748v1</guid>
      <category>cs.FL</category>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antoine Amarilli, Sven Dziadek, Luc Segoufin</dc:creator>
    </item>
    <item>
      <title>Robust Value Maximization in Challenge the Champ Tournaments with Probabilistic Outcomes</title>
      <link>https://arxiv.org/abs/2602.14966</link>
      <description>arXiv:2602.14966v1 Announce Type: cross 
Abstract: Challenge the Champ is a simple tournament format, where an ordering of the players -- called a seeding -- is decided. The first player in this order is the initial champ, and faces the next player. The outcome of each match decides the current champion, who faces the next player in the order. Each player also has a popularity, and the value of each match is the popularity of the winner. Value maximization in tournaments has been previously studied when each match has a deterministic outcome. However, match outcomes are often probabilistic, rather than deterministic. We study robust value maximization in Challenge the Champ tournaments, when the winner of a match may be probabilistic. That is, we seek to maximize the total value that is obtained, irrespective of the outcome of probabilistic matches. We show that even in simple binary settings, for non-adaptive algorithms, the optimal robust value -- which we term the \textsc{VnaR}, or the value not at risk -- is hard to approximate. However, if we allow adaptive algorithms that determine the order of challengers based on the outcomes of previous matches, or restrict the matches with probabilistic outcomes, we can obtain good approximations to the optimal \textsc{VnaR}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14966v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Umang Bhaskar, Juhi Chaudhary, Sushmita Gupta, Pallavi Jain, Sanjay Seetharaman</dc:creator>
    </item>
    <item>
      <title>Information Theoretic Limits of Cardinality Estimation: Fisher Meets Shannon</title>
      <link>https://arxiv.org/abs/2007.08051</link>
      <description>arXiv:2007.08051v3 Announce Type: replace 
Abstract: Estimating the cardinality (number of distinct elements) of a large multiset is a classic problem in streaming and sketching. In this paper we study the intrinsic tradeoff between the space complexity of the sketch and its estimation error. We define a new measure of efficiency for data sketches called the Fisher-Shannon (FiSh) number $\mathcal{H}/\mathcal{I}$. It captures the tension between the limiting Shannon entropy ($\mathcal{H}$) of the sketch and its normalized Fisher information ($\mathcal{I}$) that characterizes the variance of a statistically efficient, asymptotically unbiased estimator. Our aim in introducing the FiSh-number is to build the mathematical machinery necessary to argue for precise optimality, rather than asymptotic optimality, up to large constant factors. Our results are as follows. [1] We prove that all base-$q$ variants of Flajolet and Martin's PCSA sketch have FiSh-number $H_0/I_0 \approx 1.98016$ and that every base-$q$ variant of HyperLogLog has FiSh-number worse than $H_0/I_0$, but that they tend to $H_0/I_0$ in the limit as $q\rightarrow \infty$. Here $H_0,I_0$ are precisely defined constants. [2] We describe a sketch called Fishmonger that is based on a smoothed, entropy-compressed variant of PCSA with a different estimator function. Fishmonger processes a multiset of $[U]$ such that at all times, w.h.p., its space is $(1+o(1))(H_0/I_0)m \approx 1.98m$ bits and its standard error is $1/\sqrt{m}$. For example, to achieve a 1% standard error, one needs a little more than 19,800 bits, or $\approx 2.42$ kilobytes. [3] Finally, we give circumstantial evidence that $H_0/I_0$ is the optimum FiSh-number of mergeable sketches for Cardinality Estimation. We define a natural subset of mergeable sketches called linearizable sketches and prove that no member of this class can beat $H_0/I_0$. The popular mergeable sketches are, in fact, also linearizable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2007.08051v3</guid>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seth Pettie, Dingyu Wang</dc:creator>
    </item>
    <item>
      <title>Improved Bounds for Rectangular Monotone Min-Plus Product and Applications</title>
      <link>https://arxiv.org/abs/2208.02862</link>
      <description>arXiv:2208.02862v4 Announce Type: replace 
Abstract: In a recent breakthrough paper, Chi et al. (STOC'22) introduce an $\tilde{O}(n^{\frac{3 + \omega}{2}})$ time algorithm to compute Monotone Min-Plus Product between two square matrices of dimensions $n \times n$ and entries bounded by $O(n)$. This greatly improves upon the previous $\tilde O(n^{\frac{12 + \omega}{5}})$ time algorithm and as a consequence improves bounds for its applications. Several other applications involve Monotone Min-Plus Product between rectangular matrices, and even if Chi et al.'s algorithm seems applicable for the rectangular case, the generalization is not straightforward.
  In this paper we present a generalization of the algorithm of Chi et al. to solve Monotone Min-Plus Product for rectangular matrices with polynomial bounded values. We next use this faster algorithm to improve running times for the following applications of Rectangular Monotone Min-Plus Product: $M$-bounded Single Source Replacement Path, Batch Range Mode, $k$-Dyck Edit Distance and 2-approximation of All Pairs Shortest Path. We also improve the running time for Unweighted Tree Edit Distance using the algorithm by Chi et al.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.02862v4</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anita D\"urr</dc:creator>
    </item>
    <item>
      <title>Parameterized Critical Node Cut Revisited</title>
      <link>https://arxiv.org/abs/2506.23363</link>
      <description>arXiv:2506.23363v2 Announce Type: replace 
Abstract: We study how to sparsify connectivity in graphs under a tight deletion budget. Given a graph $G$ and integers $k,x \ge 0$, Critical Node Cut (CNC) asks whether we can delete at most $k$ vertices so that the number of remaining unordered pairs of connected vertices is at most $x$. CNC generalizes Vertex Cover (the case $x=0$) and models tasks in network design, epidemiology, and social network analysis. We comprehensively map the structural parameterized complexity landscape for Critical Node Cut. First, we prove W[1]-hardness for the combined parameter $k + \mathrm{fes} + \Delta + \mathrm{pw}$, where $\mathrm{fes}$ is the feedback edge set number, $\Delta$ the maximum degree, and $\mathrm{pw}$ the pathwidth of the input graph respectively. This significantly improves over the known W[1]-hardness for $k+\mathrm{tw}$, where $\mathrm{tw}$ denotes the treewidth, and is tight in that tree-depth together with maximum degree trivially yields FPT. Second, we give new positive results. Specifically, we identify three structural parameters--max-leaf number, vertex integrity, and modular-width--that render the problem fixed-parameter tractable, and develop a polynomial-time algorithm for graphs of constant clique-width. Third, leveraging a technique introduced by Lampis~[ICALP '14], we develop an FPT approximation scheme that, for any $\varepsilon &gt; 0$, computes a $(1+\varepsilon)$-approximate solution in time $(\mathrm{tw} / \varepsilon)^{\mathcal{O}(\mathrm{tw})} n^{\mathcal{O}(1)}$, where $\mathrm{tw}$ denotes the treewidth of the input graph. Finally, we show that CNC admits no polynomial kernel when parameterized by vertex cover number, unless standard assumptions fail. Together, these results substantially sharpen the known complexity landscape for CNC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23363v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Du\v{s}an Knop, Nikolaos Melissinos, Manolis Vasilakis</dc:creator>
    </item>
    <item>
      <title>Additive, Near-Additive, and Multiplicative Approximations for APSP in Weighted Undirected Graphs: Trade-offs and Algorithms</title>
      <link>https://arxiv.org/abs/2509.04640</link>
      <description>arXiv:2509.04640v2 Announce Type: replace 
Abstract: We present a $+2\sum_{i=1}^{k+1}{W_i}$-APASP algorithm for dense weighted graphs with runtime $\tilde O\left(n^{2+\frac{1}{3k+2}}\right)$, where $W_{i}$ is the weight of an $i^{th}$ heaviest edge on a shortest path. Dor, Halperin and Zwick [FOCS'96, SICOMP'00] had two algorithms for the commensurate unweighted $+2\cdot\left( k+1\right)$-APASP: $\tilde O\left(n^{2-\frac{1}{k+2}}m^{\frac{1}{k+2}}\right)$ runtime for sparse graphs and $\tilde O\left(n^{2+\frac{1}{3k+2}}\right)$ runtime for dense graphs. Cohen and Zwick [SODA'97, JALG'01] adapted the sparse variant to weighted graphs: $+2\sum_{i=1}^{k+1}{W_i}$-APASP algorithm in the same runtime. We show an algorithm for dense weighted graphs.
  For nearly additive APASP, we present a $\left(1+\varepsilon,\min{\left\{2W_1,4W_{2}\right\}}\right)$-APASP algorithm with $\tilde O\left(\left(\frac{1}{\varepsilon}\right)^{O\left(1\right)}\cdot n^{2.15135313}\cdot\log W\right)$ runtime. This improves the $\left(1+\varepsilon,2W_1\right)$-APASP of Saha and Ye [SODA'24].
  For multiplicative APASP, we show a framework of $\left(\frac{3\ell +4}{\ell + 2}+\varepsilon\right)$-APASP algorithms, reducing the runtime of Akav and Roditty [ESA'21] for dense graphs and generalizing the $\left(2+\varepsilon\right)$-APASP algorithm of Dory et al [SODA'24]. Our base case is a $\left(\frac{7}{3}+\varepsilon\right)$-APASP in $\tilde O\left(\left(\frac{1}{\varepsilon}\right)^{O\left(1\right)}\cdot n^{2.15135313}\cdot \log W\right)$ runtime, improving the $\frac{7}{3}$-APASP algorithm of Baswana and Kavitha [FOCS'06, SICOMP'10] for dense graphs.
  Finally, we "bypass" an $\tilde \Omega \left(n^\omega\right)$ conditional lower bound by Dor, Halperin, and Zwick for $\alpha$-APASP with $\alpha &lt; 2$, by allowing an additive term (e.g. $\left(\frac{6k+3}{3k+2},\sum_{i=1}^{k+1}W_{i}\right)$-APASP in $\tilde O\left(n^{2+\frac{1}{3k+2}}\right)$ runtime).</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04640v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liam Roditty, Ariel Sapir</dc:creator>
    </item>
    <item>
      <title>A Predictive Framework for Base-n Radix Sort Optimization</title>
      <link>https://arxiv.org/abs/2509.19021</link>
      <description>arXiv:2509.19021v2 Announce Type: replace 
Abstract: Sorting is a foundational primitive of computer science and optimizations in sorting subroutines can cascade into significant performance gains for high-throughput systems. In this paper, we analyze the inefficiencies of a non-comparison sorting algorithm, namely, Base-n Radix Sort (BNRS), specifically the `zero padding' problem in skewed datasets. We develop an execution model, called, Stable Partitioning - Least Significant Digit Radix Sort (shortly, SP-LSD), an iterative least significant digit based pruning model designed to address this inefficiency. Based on this development, we derive the Radix Crossover Framework(RCF), an analytic three-point decision framework. The framework is established on the precondition of non-negative integers, which enables the derivation of three critical boundaries. First, the Asymptotic Crossover ($k&lt;n^{\log_2 n}$) defines when BNRS and SP-LSD can theoretically outperform the comparison sorting algorithms where k is the maximum value and n is the input size. Second, the Round-feasibility Crossover ($k&gt;n^2$) defines when overhead cost of implemented model SP-LSD is amortized. Third, we derive Pruning Crossover parameterized by the ratio of random-access sorting cost to sequential partitioning cost. This model demonstrates that SP-LSD yields a net gain on skewed and uniform distributions over standard BNRS. The experimental results are consistent with the crossover boundaries, providing a deterministic roadmap for adaptive algorithm selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19021v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Atharv Pandey, Lakshmanan Kuppusamy</dc:creator>
    </item>
    <item>
      <title>Oracle-based Uniform Sampling from Convex Bodies</title>
      <link>https://arxiv.org/abs/2510.02983</link>
      <description>arXiv:2510.02983v2 Announce Type: replace 
Abstract: We propose new Markov chain Monte Carlo algorithms to sample a uniform distribution on a convex body $K$. Our algorithms are based on the proximal sampler, which uses Gibbs sampling on an augmented distribution and assumes access to the so-called restricted Gaussian oracle (RGO). The key contribution of this work is an efficient implementation of the RGO for uniform sampling on convex $K$ that goes beyond the membership-oracle model used in many classical and modern uniform samplers, and instead leverages richer oracle access commonly assumed in convex optimization. We implement the RGO via rejection sampling and access to either a projection oracle or a separation oracle on $K$. In both oracle models, we provide non-asymptotic complexity guarantees for obtaining unbiased samples, with accuracy quantified in R\'enyi divergence and $\chi^2$-divergence, and we support these theoretical guarantees with numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02983v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thanh Dang, Jiaming Liang</dc:creator>
    </item>
    <item>
      <title>Online computation of normalized substring complexity</title>
      <link>https://arxiv.org/abs/2510.16454</link>
      <description>arXiv:2510.16454v2 Announce Type: replace 
Abstract: The normalized substring complexity $\delta$ of a string is defined as $\max_k \{c[k]/k\}$, where $c[k]$ is the number of \textit{distinct} substrings of length $k$. This simply defined measure has recently attracted attention due to its established relationship to popular string compression algorithms. We consider the problem of computing $\delta$ online, when the string is provided from a stream. We present two algorithms solving the problem: one working in $O(\log n)$ amortized time per character, and the other in $O(\log^3 n)$ worst-case time per character. To our knowledge, this is the first polylog-time online solution to this problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16454v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gregory Kucherov, Yakov Nekrich</dc:creator>
    </item>
    <item>
      <title>Improved Tree Sparsifiers in Near-Linear Time</title>
      <link>https://arxiv.org/abs/2511.06574</link>
      <description>arXiv:2511.06574v2 Announce Type: replace 
Abstract: A \emph{tree cut-sparsifier} $T$ of quality $\alpha$ of a graph $G$ is a single tree that preserves the capacities of all cuts in the graph up to a factor of $\alpha$. A \emph{tree flow-sparsifier} $T$ of quality $\alpha$ guarantees that every demand that can be routed in $T$ can also be routed in $G$ with congestion at most $\alpha$.
  We present a near-linear time algorithm that, for any undirected capacitated graph $G=(V,E,c)$, constructs a tree cut-sparsifier $T$ of quality $O(\log^{2} n \log\log n)$, where $n=|V|$. This nearly matches the quality of the best known polynomial construction of a tree cut-sparsifier, of quality $O(\log^{1.5} n \log\log n)$ [R\"acke and Shah, ESA~2014]. By the flow-cut gap, our result yields a tree flow-sparsifier (and congestion-approximator) of quality $O(\log^{3} n \log\log n)$. This improves on the celebrated result of [R\"acke, Shah, and T\"aubig, SODA~2014] (RST) that gave a near-linear time construction of a tree flow-sparsifier of quality $O(\log^{4} n)$.
  Our algorithm builds on a recent \emph{expander decomposition} algorithm by [Agassy, Dorfman, and Kaplan, ICALP~2023], which we use as a black box to obtain a clean and modular foundation for tree cut-sparsifiers. This yields an improved and simplified version of the RST construction for cut-sparsifiers with quality $O(\log^{3} n)$. We then introduce a near-linear time \emph{refinement phase} that controls the load accumulated on boundary edges of the sub-clusters across the levels of the tree. Combining the improved framework with this refinement phase leads to our final $O(\log^{2} n \log\log n)$ tree cut-sparsifier.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06574v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Agassy, Dani Dorfman, Haim Kaplan</dc:creator>
    </item>
    <item>
      <title>Robust Algorithms for Finding Cliques in Random Intersection Graphs via Sum-of-Squares</title>
      <link>https://arxiv.org/abs/2511.20376</link>
      <description>arXiv:2511.20376v3 Announce Type: replace 
Abstract: We study efficient algorithms for recovering cliques in dense random intersection graphs (RIGs). In this model, $d = n^{\Omega(1)}$ cliques of size approximately $k$ are randomly planted by choosing the vertices to participate in each clique independently with probability $\delta$. While there has been extensive work on recovering one, or multiple disjointly planted cliques in random graphs, the natural extension of this question to recovering overlapping cliques has been, surprisingly, largely unexplored. Moreover, because every vertex can be part of polynomially many cliques, this task is significantly more challenging than in case of disjointly planted cliques (as recently studied by Kothari, Vempala, Wein and Xu [COLT'23]).
  In this work we obtain the first efficient algorithms for recovering the community structure of RIGs both from the perspective of exact and approximate recovery. Our algorithms are further robust to noise, monotone adversaries, and a certain, optimal number of edge corruptions. They work whenever $k \gg \sqrt{n \log(n)}$. Our techniques follow the proofs-to-algorithms framework utilizing the sum-of-squares hierarchy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20376v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Andreas G\"obel, Janosch Ruff, Leon Schiller</dc:creator>
    </item>
    <item>
      <title>Publishing Below-Threshold Triangle Counts under Local Weight Differential Privacy</title>
      <link>https://arxiv.org/abs/2601.01710</link>
      <description>arXiv:2601.01710v2 Announce Type: replace 
Abstract: We propose an algorithm for counting below-threshold triangles in weighted graphs under local weight differential privacy. While prior work has largely focused on unweighted graphs, edge weights are intrinsic to many real-world networks. We consider the setting in which the graph topology is publicly known and privacy is required only for the contribution of an individual to incident edge weights, capturing practical scenarios such as road and telecommunication networks. Our method uses two rounds of communication. In the first round, each node releases privatized information about its incident edge weights under local weight differential privacy. In the second round, nodes locally count below-threshold triangles using this privatized information; we introduce both biased and unbiased variants of the estimator. We further develop two refinements: (i) a pre-computation step that reduces covariance and thus lowers expected error, and (ii) an efficient procedure for computing smooth sensitivity, which substantially reduces running time relative to a straightforward implementation. Finally, we present experimental results that quantify the trade-offs between the biased and unbiased variants and demonstrate the effectiveness of the proposed improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01710v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kevin Pfisterer, Quentin Hillebrand, Vorapong Suppakitpaisarn</dc:creator>
    </item>
    <item>
      <title>Learning the Inverse Temperature of Ising Models under Hard Constraints using One Sample</title>
      <link>https://arxiv.org/abs/2509.20993</link>
      <description>arXiv:2509.20993v2 Announce Type: replace-cross 
Abstract: We consider the problem of estimating inverse temperature parameter $\beta$ of an $n$-dimensional truncated Ising model using a single sample. Given a graph $G = (V,E)$ with $n$ vertices, a truncated Ising model is a probability distribution over the $n$-dimensional hypercube $\{-1,1\}^n$ where each configuration $\mathbf{\sigma}$ is constrained to lie in a truncation set $S \subseteq \{-1,1\}^n$ and has probability $\Pr(\mathbf{\sigma}) \propto \exp(\beta\mathbf{\sigma}^\top A\mathbf{\sigma})$ with $A$ being the adjacency matrix of $G$. We adopt the recent setting of [Galanis et al. SODA'24], where the truncation set $S$ can be expressed as the set of satisfying assignments of a $k$-SAT formula. Given a single sample $\mathbf{\sigma}$ from a truncated Ising model, with inverse parameter $\beta^*$, underlying graph $G$ of bounded degree $\Delta$ and $S$ being expressed as the set of satisfying assignments of a $k$-SAT formula, we design in nearly $O(n)$ time an estimator $\hat{\beta}$ that is $O(\Delta^3/\sqrt{n})$-consistent with the true parameter $\beta^*$ for $k \gtrsim \log(d^2k)\Delta^3.$
  Our estimator is based on the maximization of the pseudolikelihood, a notion that has received extensive analysis for various probabilistic models without [Chatterjee, Annals of Statistics '07] or with truncation [Galanis et al. SODA '24]. Our approach generalizes recent techniques from [Daskalakis et al. STOC '19, Galanis et al. SODA '24], to confront the more challenging setting of the truncated Ising model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20993v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rohan Chauhan, Ioannis Panageas</dc:creator>
    </item>
    <item>
      <title>The Contiguous Art Gallery Problem is in {\Theta}(n log n)</title>
      <link>https://arxiv.org/abs/2511.02960</link>
      <description>arXiv:2511.02960v3 Announce Type: replace-cross 
Abstract: Recently, a natural variant of the Art Gallery problem, known as the \emph{Contiguous Art Gallery problem} was proposed. Given a simple polygon $P$, the goal is to partition its boundary $\partial P$ into the smallest number of contiguous segments such that each segment is completely visible from some point in $P$. Unlike the classical Art Gallery problem, which is NP-hard, this variant is polynomial-time solvable. At SoCG~2025, three independent works presented algorithms for this problem, each achieving a running time of $O(k n^5 \log n)$ (or $O(n^6\log n)$), where $k$ is the size of an optimal solution. Interestingly, these results were obtained using entirely different approaches, yet all led to roughly the same asymptotic complexity, suggesting that such a running time might be inherent to the problem.
  We show that this is not the case. In the real RAM-model, the prevalent model in computational geometry, we present an $O(n \log n)$-time algorithm, achieving an $O(k n^4)$ factor speed-up over the previous state-of-the-art. We also give a straightforward sorting-based lower bound by reducing from the set intersection problem. We thus show that the Contiguous Art Gallery problem is in $\Theta(n \log n)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02960v3</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarita de Berg, Jacobus Conradi, Ivor van der Hoog, Eva Rotenberg</dc:creator>
    </item>
    <item>
      <title>Graded Projection Recursion (GPR): A Framework for Controlling Bit-Complexity of Algebraic Packing</title>
      <link>https://arxiv.org/abs/2511.11988</link>
      <description>arXiv:2511.11988v3 Announce Type: replace-cross 
Abstract: We present Graded Projection Recursion (GPR), a framework for converting certain blocked recursive algorithms into model-honest (i.e., reflects full bit complexity) near-quadratic procedures under bounded intermediate budgets. Part I gives a proof-complete {\em integral specification} of a three-band packing identity and a two-round middle-band extractor, and shows how per-node centering and sqrt-free dyadic l2 normalization (recursive invariant amplification) gives a sufficient packing base that grows linearly with the scaling depth (i.e., logarithmic bit-growth) in exact arithmetic.
  On fixed-width hardware, exact evaluation of the packed recursion generally requires either an extended-precision path or digit-band (slice) staging} that emulates b(n) bits of mantissa precision using w-bit words, incurring only polylogarithmic overhead; This leads to a soft-quadratic bit cost O(n^2) when b(n)=\Theta(\log n) in the basic 2x2 recursion). Part II introduces execution-format comparators (e.g., IEEE-754), a drift ledger, and a decision-invariance theorem that supports commensurate-accuracy claims in floating arithmetic (and that cleanly accounts for any staged/truncated auxiliary drift). Part III provides case-study reductions (LUP/solve/det/inv, LDL^T, blocked QR, SOI/SPD functions, GSEVP, dense LP/SDP IPM kernels, Gaussian process regression, and representative semiring problems) showing how to export the kernel advantage without reintroducing uncontrolled intermediate growth. Part IV abstracts admissible packings and extractors via a master condition and an easily checkable BWBM sufficient condition, and sketches extensions to multilinear/multigraded kernels and non-rounding extractors (e.g., CRT and semiring bucket projections).</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11988v3</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeffrey Uhlmann</dc:creator>
    </item>
    <item>
      <title>Trellis codes with a good distance profile constructed from expander graphs</title>
      <link>https://arxiv.org/abs/2602.08718</link>
      <description>arXiv:2602.08718v2 Announce Type: replace-cross 
Abstract: We derive Singleton-type bounds on the free distance and column distances of trellis codes. Our results show that, at a given time instant, the maximum attainable column distance of trellis codes can exceed that of convolutional codes. Moreover, using expander graphs, we construct trellis codes over constant-size alphabets that achieve a rate-distance trade-off arbitrarily close to that of convolutional codes with a maximum distance profile. By comparison, all known constructions of convolutional codes with a maximum distance profile require working over alphabets whose size grows at least exponentially with the number of output symbols per time instant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08718v2</guid>
      <category>cs.IT</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <category>math.IT</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yubin Zhu, Zitan Chen</dc:creator>
    </item>
  </channel>
</rss>
