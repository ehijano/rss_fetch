<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.BM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.BM</link>
    <description>q-bio.BM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.BM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 20 May 2024 04:01:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 20 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>UniCorn: A Unified Contrastive Learning Approach for Multi-view Molecular Representation Learning</title>
      <link>https://arxiv.org/abs/2405.10343</link>
      <description>arXiv:2405.10343v1 Announce Type: new 
Abstract: Recently, a noticeable trend has emerged in developing pre-trained foundation models in the domains of CV and NLP. However, for molecular pre-training, there lacks a universal model capable of effectively applying to various categories of molecular tasks, since existing prevalent pre-training methods exhibit effectiveness for specific types of downstream tasks. Furthermore, the lack of profound understanding of existing pre-training methods, including 2D graph masking, 2D-3D contrastive learning, and 3D denoising, hampers the advancement of molecular foundation models. In this work, we provide a unified comprehension of existing pre-training methods through the lens of contrastive learning. Thus their distinctions lie in clustering different views of molecules, which is shown beneficial to specific downstream tasks. To achieve a complete and general-purpose molecular representation, we propose a novel pre-training framework, named UniCorn, that inherits the merits of the three methods, depicting molecular views in three different levels. SOTA performance across quantum, physicochemical, and biological tasks, along with comprehensive ablation study, validate the universality and effectiveness of UniCorn.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10343v1</guid>
      <category>q-bio.BM</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shikun Feng, Yuyan Ni, Minghao Li, Yanwen Huang, Zhi-Ming Ma, Wei-Ying Ma, Yanyan Lan</dc:creator>
    </item>
    <item>
      <title>Lysine-Cysteine-Serine-Tryptophan Inserted into the DNA-Binding Domain of Human Mineralocorticoid Receptor Increases Transcriptional Activation by Aldosterone</title>
      <link>https://arxiv.org/abs/2405.10432</link>
      <description>arXiv:2405.10432v1 Announce Type: new 
Abstract: Due to alternative splicing in an ancestral DNA-binding domain (DBD) of the mineralocorticoid receptor (MR), humans contain two almost identical MR transcripts with either 984 amino acids (MR-984) or 988 amino acids (MR-988), in which their DBDs differ by only four amino acids, Lys,Cys,Ser,Trp (KCSW). Human MRs also contain mutations at two sites, codons 180 and 241, in the amino terminal domain (NTD). Together, there are five distinct full-length human MR genes in GenBank. Human MR-984, which was cloned in 1987, has been extensively studied. Human MR-988, cloned in 1995, contains KCSW in its DBD. Neither this human MR-988 nor the other human MR-988 genes have been studied for their response to aldosterone and other corticosteroids. Here, we report that transcriptional activation of human MR-988 by aldosterone is increased by about 50% compared to activation of human MR-984 in HEK293 cells transfected with the TAT3 promoter, while the half-maximal response (EC50) is similar for aldosterone activation of MR-984 and MR-988. Transcriptional activation of human MR also depends on the amino acids at codons 180 and 241. Interestingly, in HEK293 cells transfected with the MMTV promoter, transcriptional activation by aldosterone of human MR-988 is similar to activation of human MR-984, indicating that the promoter has a role in the regulation of the response of human MR-988 to aldosterone. The physiological responses to aldosterone and other corticosteroids in humans with MR genes containing KCSW and with differences at codons 180 and 241 in the NTD warrant investigation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10432v1</guid>
      <category>q-bio.BM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yoshinao Katsu, Jiawen Zhang, Michael E. Baker</dc:creator>
    </item>
    <item>
      <title>Specialising and Analysing Instruction-Tuned and Byte-Level Language Models for Organic Reaction Prediction</title>
      <link>https://arxiv.org/abs/2405.10625</link>
      <description>arXiv:2405.10625v1 Announce Type: cross 
Abstract: Transformer-based encoder-decoder models have demonstrated impressive results in chemical reaction prediction tasks. However, these models typically rely on pretraining using tens of millions of unlabelled molecules, which can be time-consuming and GPU-intensive. One of the central questions we aim to answer in this work is: Can FlanT5 and ByT5, the encode-decoder models pretrained solely on language data, be effectively specialised for organic reaction prediction through task-specific fine-tuning? We conduct a systematic empirical study on several key issues of the process, including tokenisation, the impact of (SMILES-oriented) pretraining, fine-tuning sample efficiency, and decoding algorithms at inference. Our key findings indicate that although being pretrained only on language tasks, FlanT5 and ByT5 provide a solid foundation to fine-tune for reaction prediction, and thus become `chemistry domain compatible' in the process. This suggests that GPU-intensive and expensive pretraining on a large dataset of unlabelled molecules may be useful yet not essential to leverage the power of language models for chemistry. All our models achieve comparable Top-1 and Top-5 accuracy although some variation across different models does exist. Notably, tokenisation and vocabulary trimming slightly affect final performance but can speed up training and inference; The most efficient greedy decoding strategy is very competitive while only marginal gains can be achieved from more sophisticated decoding algorithms. In summary, we evaluate FlanT5 and ByT5 across several dimensions and benchmark their impact on organic reaction prediction, which may guide more effective use of these state-of-the-art language models for chemistry-related tasks in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10625v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.BM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiayun Pang, Ivan Vuli\'c</dc:creator>
    </item>
    <item>
      <title>HelixFold-Multimer: Elevating Protein Complex Structure Prediction to New Heights</title>
      <link>https://arxiv.org/abs/2404.10260</link>
      <description>arXiv:2404.10260v2 Announce Type: replace 
Abstract: While monomer protein structure prediction tools boast impressive accuracy, the prediction of protein complex structures remains a daunting challenge in the field. This challenge is particularly pronounced in scenarios involving complexes with protein chains from different species, such as antigen-antibody interactions, where accuracy often falls short. Limited by the accuracy of complex prediction, tasks based on precise protein-protein interaction analysis also face obstacles. In this report, we highlight the ongoing advancements of our protein complex structure prediction model, HelixFold-Multimer, underscoring its enhanced performance. HelixFold-Multimer provides precise predictions for diverse protein complex structures, especially in therapeutic protein interactions. Notably, HelixFold-Multimer achieves remarkable success in antigen-antibody and peptide-protein structure prediction, greatly surpassing AlphaFold 3. HelixFold-Multimer is now available for public use on the PaddleHelix platform, offering both a general version and an antigen-antibody version. Researchers can conveniently access and utilize this service for their development needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10260v2</guid>
      <category>q-bio.BM</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiaomin Fang, Jie Gao, Jing Hu, Lihang Liu, Yang Xue, Xiaonan Zhang, Kunrui Zhu</dc:creator>
    </item>
  </channel>
</rss>
