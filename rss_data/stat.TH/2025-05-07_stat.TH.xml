<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 07 May 2025 04:00:58 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Depth based trimmed means</title>
      <link>https://arxiv.org/abs/2505.03523</link>
      <description>arXiv:2505.03523v1 Announce Type: new 
Abstract: Robust estimation of location is a fundamental problem in statistics, particularly in scenarios where data contamination by outliers or model misspecification is a concern. In univariate settings, methods such as the sample median and trimmed means balance robustness and efficiency by mitigating the influence of extreme observations. This paper extends these robust techniques to the multivariate context through the use of data depth functions, which provide a natural means to order and rank multidimensional data. We review several depth measures and discuss their role in generalizing trimmed mean estimators beyond one dimension.
  Our main contributions are twofold: first, we prove the almost sure consistency of the multivariate trimmed mean estimator under mixing conditions; second, we establish a general limit distribution theorem for a broad family of depth-based estimators, encompassing popular examples such as Tukey's and projection depth. These theoretical advancements not only enhance the understanding of robust location estimation in high-dimensional settings but also offer practical guidelines for applications in areas such as machine learning, economic analysis, and financial risk assessment.
  A small example with simulated data is performed, varying the depth measure used and the percentage of trimmed data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03523v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alejandro Cholaquidis, Ricardo Fraiman, Leonardo Moreno, Gonzalo Perera</dc:creator>
    </item>
    <item>
      <title>Information-theoretic reduction of deep neural networks to linear models in the overparametrized proportional regime</title>
      <link>https://arxiv.org/abs/2505.03577</link>
      <description>arXiv:2505.03577v1 Announce Type: new 
Abstract: We rigorously analyse fully-trained neural networks of arbitrary depth in the Bayesian optimal setting in the so-called proportional scaling regime where the number of training samples and width of the input and all inner layers diverge proportionally. We prove an information-theoretic equivalence between the Bayesian deep neural network model trained from data generated by a teacher with matching architecture, and a simpler model of optimal inference in a generalized linear model. This equivalence enables us to compute the optimal generalization error for deep neural networks in this regime. We thus prove the "deep Gaussian equivalence principle" conjectured in Cui et al. (2023) (arXiv:2302.00375). Our result highlights that in order to escape this "trivialisation" of deep neural networks (in the sense of reduction to a linear model) happening in the strongly overparametrized proportional regime, models trained from much more data have to be considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03577v1</guid>
      <category>math.ST</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.IT</category>
      <category>math-ph</category>
      <category>math.IT</category>
      <category>math.MP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Camilli, Daria Tieplova, Eleonora Bergamin, Jean Barbier</dc:creator>
    </item>
    <item>
      <title>Maximum likelihood estimation for the $\lambda$-exponential family</title>
      <link>https://arxiv.org/abs/2505.03582</link>
      <description>arXiv:2505.03582v1 Announce Type: new 
Abstract: The $\lambda$-exponential family generalizes the standard exponential family via a generalized convex duality motivated by optimal transport. It is the constant-curvature analogue of the exponential family from the information-geometric point of view, but the development of computational methodologies is still in an early stage. In this paper, we propose a fixed point iteration for maximum likelihood estimation under i.i.d.~sampling, and prove using the duality that the likelihood is monotone along the iterations. We illustrate the algorithm with the $q$-Gaussian distribution and the Dirichlet perturbation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03582v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiwei Tian, Ting-Kam Leonard Wong, Jiaowen Yang, Jun Zhang</dc:creator>
    </item>
    <item>
      <title>New affine invariant ensemble samplers and their dimensional scaling</title>
      <link>https://arxiv.org/abs/2505.02987</link>
      <description>arXiv:2505.02987v1 Announce Type: cross 
Abstract: We introduce some new affine invariant ensemble samplers that are easy to construct and improve upon existing widely used algorithms, especially for high-dimensional problems. Specifically, we propose a derivative-free ensemble side move sampler that performs favorably compared to popular samplers in the \texttt{emcee} package. Additionally, we develop a class of derivative-based ensemble Hamiltonian Monte Carlo (HMC) samplers with affine invariance, which outperform standard HMC without affine invariance when sampling highly skewed distributions. We provide asymptotic scaling analysis for high-dimensional Gaussian targets to further elucidate the properties of these affine invariant ensemble samplers. In particular, with derivative information, the affine invariant ensemble HMC can scale much better with dimension compared to derivative-free ensemble samplers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02987v1</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Chen</dc:creator>
    </item>
    <item>
      <title>Transformers for Learning on Noisy and Task-Level Manifolds: Approximation and Generalization Insights</title>
      <link>https://arxiv.org/abs/2505.03205</link>
      <description>arXiv:2505.03205v1 Announce Type: cross 
Abstract: Transformers serve as the foundational architecture for large language and video generation models, such as GPT, BERT, SORA and their successors. Empirical studies have demonstrated that real-world data and learning tasks exhibit low-dimensional structures, along with some noise or measurement error. The performance of transformers tends to depend on the intrinsic dimension of the data/tasks, though theoretical understandings remain largely unexplored for transformers. This work establishes a theoretical foundation by analyzing the performance of transformers for regression tasks involving noisy input data on a manifold. Specifically, the input data are in a tubular neighborhood of a manifold, while the ground truth function depends on the projection of the noisy data onto the manifold. We prove approximation and generalization errors which crucially depend on the intrinsic dimension of the manifold. Our results demonstrate that transformers can leverage low-complexity structures in learning task even when the input data are perturbed by high-dimensional noise. Our novel proof technique constructs representations of basic arithmetic operations by transformers, which may hold independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03205v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaiming Shen, Alex Havrilla, Rongjie Lai, Alexander Cloninger, Wenjing Liao</dc:creator>
    </item>
    <item>
      <title>Learning Survival Distributions with the Asymmetric Laplace Distribution</title>
      <link>https://arxiv.org/abs/2505.03712</link>
      <description>arXiv:2505.03712v1 Announce Type: cross 
Abstract: Probabilistic survival analysis models seek to estimate the distribution of the future occurrence (time) of an event given a set of covariates. In recent years, these models have preferred nonparametric specifications that avoid directly estimating survival distributions via discretization. Specifically, they estimate the probability of an individual event at fixed times or the time of an event at fixed probabilities (quantiles), using supervised learning. Borrowing ideas from the quantile regression literature, we propose a parametric survival analysis method based on the Asymmetric Laplace Distribution (ALD). This distribution allows for closed-form calculation of popular event summaries such as mean, median, mode, variation, and quantiles. The model is optimized by maximum likelihood to learn, at the individual level, the parameters (location, scale, and asymmetry) of the ALD distribution. Extensive results on synthetic and real-world data demonstrate that the proposed method outperforms parametric and nonparametric approaches in terms of accuracy, discrimination and calibration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03712v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deming Sheng, Ricardo Henao</dc:creator>
    </item>
    <item>
      <title>Method-of-Moments Inference for GLMs and Doubly Robust Functionals under Proportional Asymptotics</title>
      <link>https://arxiv.org/abs/2408.06103</link>
      <description>arXiv:2408.06103v3 Announce Type: replace 
Abstract: In this paper, we consider the estimation of regression coefficients and signal-to-noise (SNR) ratio in high-dimensional Generalized Linear Models (GLMs), and explore their implications in inferring popular estimands such as average treatment effects in high-dimensional observational studies. Under the ``proportional asymptotic'' regime and Gaussian covariates with known (population) covariance $\Sigma$, we derive Consistent and Asymptotically Normal (CAN) estimators of our targets of inference through a Method-of-Moments type of estimators that bypasses estimation of high dimensional nuisance functions and hyperparameter tuning altogether. Additionally, under non-Gaussian covariates, we demonstrate universality of our results under certain additional assumptions on the regression coefficients and $\Sigma$. We also demonstrate that knowing $\Sigma$ is not essential to our proposed methodology when the sample covariance matrix estimator is invertible. Finally, we complement our theoretical results with numerical experiments and comparisons with existing literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06103v3</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xingyu Chen, Lin Liu, Rajarshi Mukherjee</dc:creator>
    </item>
    <item>
      <title>Multiple imputation and full law identifiability</title>
      <link>https://arxiv.org/abs/2410.18688</link>
      <description>arXiv:2410.18688v3 Announce Type: replace 
Abstract: The key problems in missing data models involve the identifiability of two distributions: the target law and the full law. The target law refers to the joint distribution of the data variables, while the full law refers to the joint distribution of both the data variables and the response indicators. It has not been clearly stated how the identifiability of the target law and the full law relate to multiple imputation. We show that imputations can be drawn from the correct conditional distributions if and only if the full law is identifiable. This result means that standard multiple imputation methods may not be directly applicable in cases where the target law is identifiable but the full law is not. We demonstrate that alternative imputation approaches sometimes enable the estimation of the target law in such cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18688v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juha Karvanen, Santtu Tikka</dc:creator>
    </item>
    <item>
      <title>Truncated LinUCB for Stochastic Linear Bandits</title>
      <link>https://arxiv.org/abs/2202.11735</link>
      <description>arXiv:2202.11735v4 Announce Type: replace-cross 
Abstract: This paper considers contextual bandits with a finite number of arms, where the contexts are independent and identically distributed $d$-dimensional random vectors, and the expected rewards are linear in both the arm parameters and contexts. The LinUCB algorithm, which is near minimax optimal for related linear bandits, is shown to have a cumulative regret that is suboptimal in both the dimension $d$ and time horizon $T$, due to its over-exploration. A truncated version of LinUCB is proposed and termed "Tr-LinUCB", which follows LinUCB up to a truncation time $S$ and performs pure exploitation afterwards. The Tr-LinUCB algorithm is shown to achieve $O(d\log(T))$ regret if $S = Cd\log(T)$ for a sufficiently large constant $C$, and a matching lower bound is established, which shows the rate optimality of Tr-LinUCB in both $d$ and $T$ under a low dimensional regime. Further, if $S = d\log^{\kappa}(T)$ for some $\kappa&gt;1$, the loss compared to the optimal is a multiplicative $\log\log(T)$ factor, which does not depend on $d$. This insensitivity to overshooting in choosing the truncation time of Tr-LinUCB is of practical importance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.11735v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanglei Song, Meng zhou</dc:creator>
    </item>
    <item>
      <title>On the asymptotics of extremal lp-blocks cluster inference</title>
      <link>https://arxiv.org/abs/2212.13521</link>
      <description>arXiv:2212.13521v4 Announce Type: replace-cross 
Abstract: Extremes occur in stationary regularly varying time series as short periods with several large observations, known as extremal blocks. We study cluster statistics summarizing the behavior of functions acting on these extremal blocks. Examples of cluster statistics are the extremal index, cluster size probabilities, and other cluster indices. The purpose of our work is twofold. First, we state the asymptotic normality of block estimators for cluster inference based on consecutive observations with large lp-norms, for p &lt; 0. The case p=$\alpha$, where $\alpha$ &gt; 0 is the tail index of the time series, has specific nice properties thus we analyze the asymptotic of blocks estimators when approximating $\alpha$ using the Hill estimator. Second, we verify the conditions we require on classical models such as linear models and solutions of stochastic recurrence equations. Regarding linear models, we prove that the asymptotic variance of classical index cluster-based estimators is null as first conjectured in Hsing T. [26]. We illustrate our findings on simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.13521v4</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gloria Buritic\'a (MIA Paris-Saclay), Olivier Wintenberger (LPSM)</dc:creator>
    </item>
    <item>
      <title>A Debiased Estimator for the Mediation Functional in Ultra-High-Dimensional Setting in the Presence of Interaction Effects</title>
      <link>https://arxiv.org/abs/2412.08827</link>
      <description>arXiv:2412.08827v2 Announce Type: replace-cross 
Abstract: Mediation analysis is a crucial tool for uncovering the mechanisms through which a treatment affects the outcome, providing deeper causal insights and guiding effective interventions. Despite advances in analyzing the mediation effect with fixed/low-dimensional mediators and covariates, our understanding of estimation and inference of mediation functional in the presence of (ultra)-high-dimensional mediators and covariates is still limited. In this paper, we present an estimator for mediation functional in a high-dimensional setting that accommodates the interaction between covariates and treatment in generating mediators, as well as interactions between both covariates and treatment and mediators and treatment in generating the response. We demonstrate that our estimator is $\sqrt{n}$-consistent and asymptotically normal, thus enabling reliable inference on direct and indirect treatment effects with asymptotically valid confidence intervals. A key technical contribution of our work is to develop a multi-step debiasing technique, which may also be valuable in other statistical settings with similar structural complexities where accurate estimation depends on debiasing. We evaluate our proposed methodology through extensive simulation studies and apply it to the TCGA lung cancer dataset to estimate the effect of smoking, mediated by DNA methylation, on the survival time of lung cancer patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08827v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shi Bo, AmirEmad Ghassami, Debarghya Mukherjee</dc:creator>
    </item>
    <item>
      <title>Mining and Intervention of Social Networks Information Cocoon Based on Multi-Layer Network Community Detection</title>
      <link>https://arxiv.org/abs/2504.21357</link>
      <description>arXiv:2504.21357v3 Announce Type: replace-cross 
Abstract: With the rapid development of information technology and the widespread utilization of recommendation algorithms, users are able to access information more conveniently, while the content they receive tends to be homogeneous. Homogeneous viewpoints and preferences tend to cluster users into sub-networks, leading to group polarization and increasing the likelihood of forming information cocoons. This paper aims to handle information cocoon phenomena in debates on social media. In order to investigate potential user connections, we construct a double-layer network that incorporates two dimensions: relational ties and feature-based similarity between users. Based on the structure of the multi-layer network, we promote two graph auto-encoder (GAE) based community detection algorithms, which can be applied to the partition and determination of information cocoons. This paper tests these two algorithms on Cora, Citeseer, and synthetic datasets, comparing them with existing multi-layer network unsupervised community detection algorithms. Numerical experiments illustrate that the algorithms proposed in this paper significantly improve prediction accuracy indicator NMI (normalized mutual information) and network topology indicator Q. Additionally, an influence-based intervention measure on which algorithms can operate is proposed. Through the Markov states transition model, we simulate the intervention effects, which illustrate that our community detection algorithms play a vital role in partitioning and determining information cocoons. Simultaneously, our intervention strategy alleviates the polarization of viewpoints and the formation of information cocoons with minimal intervention effort.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21357v3</guid>
      <category>cs.SI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Suwen Yang, Lei Shi</dc:creator>
    </item>
    <item>
      <title>Resolving Memorization in Empirical Diffusion Model for Manifold Data in High-Dimensional Spaces</title>
      <link>https://arxiv.org/abs/2505.02508</link>
      <description>arXiv:2505.02508v2 Announce Type: replace-cross 
Abstract: Diffusion models is a popular computational tool to generate new data samples. It utilizes a forward diffusion process that add noise to the data distribution and then use a reverse process to remove noises to produce samples from the data distribution. However, when the empirical data distribution consists of $n$ data point, using the empirical diffusion model will necessarily produce one of the existing data points. This is often referred to as the memorization effect, which is usually resolved by sophisticated machine learning procedures in the current literature. This work shows that the memorization problem can be resolved by a simple inertia update step at the end of the empirical diffusion model simulation. Our inertial diffusion model requires only the empirical diffusion model score function and it does not require any further training. We show that choosing the inertia diffusion model sample distribution is an $O\left(n^{-\frac{2}{d+4}}\right)$ Wasserstein-1 approximation of a data distribution lying on a $C^2$ manifold of dimension $d$. Since this estimate is significant smaller the Wasserstein1 distance between population and empirical distributions, it rigorously shows the inertial diffusion model produces new data samples. Remarkably, this upper bound is completely free of the ambient space dimension, since there is no training involved. Our analysis utilizes the fact that the inertial diffusion model samples are approximately distributed as the Gaussian kernel density estimator on the manifold. This reveals an interesting connection between diffusion model and manifold learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02508v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Lyu, Yuchun Qian, Tan Minh Nguyen, Xin T. Tong</dc:creator>
    </item>
  </channel>
</rss>
