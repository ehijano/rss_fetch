<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Jun 2024 02:51:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Multimodal Radiomics Model for Predicting Gold Nanoparticles Accumulation in Mouse Tumors</title>
      <link>https://arxiv.org/abs/2406.10146</link>
      <description>arXiv:2406.10146v1 Announce Type: new 
Abstract: Background: Nanoparticles can accumulate in solid tumors, serving as diagnostic or therapeutic agents for cancer. Clinical translation is challenging due to low accumulation in tumors and heterogeneity between tumor types and individuals. Tools to identify this heterogeneity and predict nanoparticle accumulation are needed. Advanced imaging techniques combined with radiomics and AI may offer a solution.
  Methods: 183 mice were used to create seven subcutaneous tumor models, with three sizes (15nm, 40nm, 70nm) of gold nanoparticles injected via the tail vein. Accumulation was measured using ICP-OES. Data were divided into training and test sets (7:3). Tumors were categorized into high and low uptake groups based on the median value of the training set. Before injection, multimodal imaging data (CT, B-mode ultrasound, SWE, CEUS) were acquired, and radiomics features extracted. LASSO and RFE algorithms built a radiomics signature. This, along with tumor type and mean values from CT and SWE, constructed the best model using SVM. For each tumor in the test set, the radiomics signature predicted gold nanoparticle uptake. Model performance was evaluated by AUC.
  Results: Significant variability in gold nanoparticle accumulation was observed among tumors (P &lt; 0.001). The median accumulation in the training set was 3.37% ID/g. Nanoparticle size was not a main determinant of uptake (P &gt; 0.05). The composite model based on radiomics signature outperformed the basic model in both training (AUC 0.93 vs. 0.68) and testing (0.78 vs. 0.61) datasets.
  Conclusion: The composite model identifies tumor heterogeneity and predicts high uptake of gold nanoparticles, improving patient stratification and supporting nanomedicine's clinical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10146v1</guid>
      <category>q-bio.QM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiajia Tang, Jie Zhang, Jiulou Zhang, Yuxia Tang, Hao Ni, Shouju Wang</dc:creator>
    </item>
    <item>
      <title>Advancing High Resolution Vision-Language Models in Biomedicine</title>
      <link>https://arxiv.org/abs/2406.09454</link>
      <description>arXiv:2406.09454v1 Announce Type: cross 
Abstract: Multi-modal learning has significantly advanced generative AI, especially in vision-language modeling. Innovations like GPT-4V and open-source projects such as LLaVA have enabled robust conversational agents capable of zero-shot task completions. However, applying these technologies in the biomedical field presents unique challenges. Recent initiatives like LLaVA-Med have started to adapt instruction-tuning for biomedical contexts using large datasets such as PMC-15M. Our research offers three key contributions: (i) we present a new instruct dataset enriched with medical image-text pairs from Claude3-Opus and LLaMA3 70B, (ii) we propose a novel image encoding strategy using hierarchical representations to improve fine-grained biomedical visual comprehension, and (iii) we develop the Llama3-Med model, which achieves state-of-the-art zero-shot performance on biomedical visual question answering benchmarks, with an average performance improvement of over 10% compared to previous methods. These advancements provide more accurate and reliable tools for medical professionals, bridging gaps in current multi-modal conversational assistants and promoting further innovations in medical AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09454v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zekai Chen, Arda Pekis, Kevin Brown</dc:creator>
    </item>
    <item>
      <title>Modified Risk Formulation for Improving the Prediction of Knee Osteoarthritis Progression</title>
      <link>https://arxiv.org/abs/2406.10119</link>
      <description>arXiv:2406.10119v1 Announce Type: cross 
Abstract: Current methods for predicting osteoarthritis (OA) outcomes do not incorporate disease specific prior knowledge to improve the outcome prediction models. We developed a novel approach that effectively uses consecutive imaging studies to improve OA outcome predictions by incorporating an OA severity constraint. This constraint ensures that the risk of OA for a knee should either increase or remain the same over time. DL models were trained to predict TKR within multiple time periods (1 year, 2 years, and 4 years) using knee radiographs and MRI scans. Models with and without the risk constraint were evaluated using the area under the receiver operator curve (AUROC) and the area under the precision recall curve (AUPRC) analysis. The novel RiskFORM2 method, leveraging a dual model risk constraint architecture, demonstrated superior performance, yielding an AUROC of 0.87 and AUPRC of 0.47 for 1 year TKR prediction on the OAI radiograph test set, a marked improvement over the 0.79 AUROC and 0.34 AUPRC of the baseline approach. The performance advantage extended to longer followup periods, with RiskFORM2 maintaining a high AUROC of 0.86 and AUPRC of 0.75 in predicting TKR within 4 years. Additionally, when generalizing to the external MOST radiograph test set, RiskFORM2 generalized better with an AUROC of 0.77 and AUPRC of 0.25 for 1 year predictions, which was higher than the 0.71 AUROC and 0.19 AUPRC of the baseline approach. In the MRI test sets, similar patterns emerged, with RiskFORM2 outperforming the baseline approach consistently. However, RiskFORM1 exhibited the highest AUROC of 0.86 and AUPRC of 0.72 for 4 year predictions on the OAI set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10119v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haresh Rengaraj Rajamohan, Richard Kijowski, Kyunghyun Cho, Cem M. Deniz</dc:creator>
    </item>
    <item>
      <title>Topological biomarkers for real-time detection of epileptic seizures</title>
      <link>https://arxiv.org/abs/2211.02523</link>
      <description>arXiv:2211.02523v2 Announce Type: replace 
Abstract: Real time seizure detection is a fundamental problem in computational neuroscience towards diagnosis and treatment's improvement of epileptic disease. We propose a real-time computational method for tracking and detection of epileptic seizures from raw neurophysiological recordings. Our mechanism is based on the topological analysis of the sliding-window embedding of the time series derived from simultaneously recorded channels. We extract topological biomarkers from the signals via the computation of the persistent homology of time-evolving topological spaces. Remarkably, the proposed biomarkers robustly captures the change in the brain dynamics during the ictal state. We apply our methods in different types of signals including scalp and intracranial electroencephalograms and magnetoencephalograms, in patients during interictal and ictal states, showing high accuracy in a range of clinical situations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.02523v2</guid>
      <category>q-bio.QM</category>
      <category>math.AT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ximena Fern\'andez, Diego Mateos</dc:creator>
    </item>
    <item>
      <title>Limits on the Evolutionary Rates of Biological Traits</title>
      <link>https://arxiv.org/abs/2202.07533</link>
      <description>arXiv:2202.07533v4 Announce Type: replace-cross 
Abstract: This paper focuses on the maximum speed at which biological evolution can occur. I derive inequalities that limit the rate of evolutionary processes driven by natural selection, mutations, or genetic drift. These \emph{rate limits} link the variability in a population to evolutionary rates. In particular, high variances in the fitness of a population and of a quantitative trait allow for fast changes in the trait's average. In contrast, low variability makes a trait less susceptible to random changes due to genetic drift. The results in this article generalize Fisher's fundamental theorem of natural selection to dynamics that allow for mutations and genetic drift, via trade-off relations that constrain the evolutionary rates of arbitrary traits. The rate limits can be used to probe questions in various evolutionary biology and ecology settings. They apply, for instance, to trait dynamics within or across species or to the evolution of bacteria strains. They apply to any quantitative trait, e.g., from species' weights to the lengths of DNA strands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.07533v4</guid>
      <category>q-bio.PE</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.bio-ph</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1038/s41598-024-61872-z</arxiv:DOI>
      <dc:creator>Luis Pedro Garc\'ia-Pintos</dc:creator>
    </item>
    <item>
      <title>Motion-based video compression for resource-constrained camera traps</title>
      <link>https://arxiv.org/abs/2405.14419</link>
      <description>arXiv:2405.14419v2 Announce Type: replace-cross 
Abstract: Field-captured video allows for detailed studies of spatiotemporal aspects of animal locomotion, decision-making, and environmental interactions. However, despite the affordability of data capture with mass-produced hardware, storage, processing, and transmission overheads pose a significant hurdle to acquiring high-resolution video from field-deployed camera traps. Therefore, efficient compression algorithms are crucial for monitoring with camera traps that have limited access to power, storage, and bandwidth. In this article, we introduce a new motion analysis-based video compression algorithm designed to run on camera trap devices. We implemented and tested this algorithm using a case study of insect-pollinator motion tracking. The algorithm identifies and stores only image regions depicting motion relevant to pollination monitoring, reducing the overall data size by an average of 84% across a diverse set of test datasets while retaining the information necessary for relevant behavioural analysis. The methods outlined in this paper facilitate the broader application of computer vision-enabled, low-powered camera trap devices for remote, in-situ video-based animal motion monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14419v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Malika Nisal Ratnayake, Lex Gallon, Adel N. Toosi, Alan Dorin</dc:creator>
    </item>
  </channel>
</rss>
