<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.MA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.MA</link>
    <description>cs.MA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.MA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Apr 2025 01:54:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Drawing a Map of Elections</title>
      <link>https://arxiv.org/abs/2504.03809</link>
      <description>arXiv:2504.03809v2 Announce Type: new 
Abstract: Our main contribution is the introduction of the map of elections framework. A map of elections consists of three main elements: (1) a dataset of elections (i.e., collections of ordinal votes over given sets of candidates), (2) a way of measuring similarities between these elections, and (3) a representation of the elections in the 2D Euclidean space as points, so that the more similar two elections are, the closer are their points. In our maps, we mostly focus on datasets of synthetic elections, but we also show an example of a map over real-life ones. To measure similarities, we would have preferred to use, e.g., the isomorphic swap distance, but this is infeasible due to its high computational complexity. Hence, we propose polynomial-time computable positionwise distance and use it instead. Regarding the representations in 2D Euclidean space, we mostly use the Kamada-Kawai algorithm, but we also show two alternatives. We develop the necessary theoretical results to form our maps and argue experimentally that they are accurate and credible. Further, we show how coloring the elections in a map according to various criteria helps in analyzing results of a number of experiments. In particular, we show colorings according to the scores of winning candidates or committees, running times of ILP-based winner determination algorithms, and approximation ratios achieved by particular algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03809v2</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.artint.2025.104332</arxiv:DOI>
      <dc:creator>Stanis{\l}aw Szufa, Niclas Boehmer, Robert Bredereck, Piotr Faliszewski, Rolf Niedermeier, Piotr Skowron, Arkadii Slinko, Nimrod Talmon</dc:creator>
    </item>
    <item>
      <title>Enforcement Agents: Enhancing Accountability and Resilience in Multi-Agent AI Frameworks</title>
      <link>https://arxiv.org/abs/2504.04070</link>
      <description>arXiv:2504.04070v1 Announce Type: new 
Abstract: As autonomous agents become more powerful and widely used, it is becoming increasingly important to ensure they behave safely and stay aligned with system goals, especially in multi-agent settings. Current systems often rely on agents self-monitoring or correcting issues after the fact, but they lack mechanisms for real-time oversight. This paper introduces the Enforcement Agent (EA) Framework, which embeds dedicated supervisory agents into the environment to monitor others, detect misbehavior, and intervene through real-time correction. We implement this framework in a custom drone simulation and evaluate it across 90 episodes using 0, 1, and 2 EA configurations. Results show that adding EAs significantly improves system safety: success rates rise from 0.0% with no EA to 7.4% with one EA and 26.7% with two EAs. The system also demonstrates increased operational longevity and higher rates of malicious drone reformation. These findings highlight the potential of lightweight, real-time supervision for enhancing alignment and resilience in multi-agent systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04070v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sagar Tamang, Dibya Jyoti Bora</dc:creator>
    </item>
    <item>
      <title>Distributed Time Synchronization in NOMA-Assisted Ultra-Dense Networks</title>
      <link>https://arxiv.org/abs/2504.04195</link>
      <description>arXiv:2504.04195v1 Announce Type: new 
Abstract: Ultra-dense networks (UDNs) represent a transformative access architecture for upcoming sixth generation (6G) systems, poised to meet the surging demand for high data rates. Achieving precise synchronization across diverse base stations (BSs) is critical in these networks to mitigate inter-cell interference (ICI). However, traditional centralized synchronization approaches face substantial challenges in dense urban, including limited access to Global Positioning System (GPS), dependence on reliable backhaul, and high signaling overhead demands. This study advances a low-complexity distributed synchronization solution. A primary focus is on assessing the algorithm's accuracy incorporating the effects of information exchange delays, which are pronounced in large-networks. Recognizing the pivotal role of neighbor-gathered information in the proposed approach, this research employs uplink Non-Orthogonal Multiple Access (NOMA) to reduce message-gathering delays between transmitters (TXs) and receivers (RXs). The proposed algorithm is evaluated to assess effectiveness under exchange delays, analyzing impact of system parameters like network connectivity, size, sub-bands, etc., on synchronization speed. The findings demonstrate that the NOMA-based information-gathering technique significantly accelerates network synchronization compared to orthogonal access schemes. This advancement is crucial for meeting the low-latency requirements of beyond fifth generation (5G) systems, underscoring the potential of distributed synchronization as a cornerstone for next-generation UDN deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04195v1</guid>
      <category>cs.MA</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Debjani Goswami, Indrakshi Dey, Nicola Marchetti, Suvra Sekhar Das</dc:creator>
    </item>
    <item>
      <title>Autono: A ReAct-Based Highly Robust Autonomous Agent Framework</title>
      <link>https://arxiv.org/abs/2504.04650</link>
      <description>arXiv:2504.04650v2 Announce Type: new 
Abstract: This paper proposes a highly robust autonomous agent framework based on the ReAct paradigm, designed to solve complex tasks through adaptive decision making and multi-agent collaboration. Unlike traditional frameworks that rely on fixed workflows generated by LLM-based planners, this framework dynamically generates next actions during agent execution based on prior trajectories, thereby enhancing its robustness. To address potential termination issues caused by adaptive execution paths, I propose a timely abandonment strategy incorporating a probabilistic penalty mechanism. For multi-agent collaboration, I introduce a memory transfer mechanism that enables shared and dynamically updated memory among agents. The framework's innovative timely abandonment strategy dynamically adjusts the probability of task abandonment via probabilistic penalties, allowing developers to balance conservative and exploratory tendencies in agent execution strategies by tuning hyperparameters. This significantly improves adaptability and task execution efficiency in complex environments. Additionally, agents can be extended through external tool integration, supported by modular design and MCP protocol compatibility, which enables flexible action space expansion. Through explicit division of labor, the multi-agent collaboration mechanism enables agents to focus on specific task components, thereby significantly improving execution efficiency and quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04650v2</guid>
      <category>cs.MA</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihao Wu</dc:creator>
    </item>
    <item>
      <title>Agentic Business Process Management: The Past 30 Years And Practitioners' Future Perspectives</title>
      <link>https://arxiv.org/abs/2504.03693</link>
      <description>arXiv:2504.03693v1 Announce Type: cross 
Abstract: With the advent of generative Artificial Intelligence (genAI), the notion of an agent has seen a resurgence in popularity. This has also led to speculation about the extent to which business process management, as a discipline and research field, may impact and be impacted by the deployment of genAI-based agents. To better ground such speculations into the state-of-the-art, we draw from the past 30 years of research on agents and business process management to establish the concept of Agentic Business Process Management (agentic BPM) that is only loosely coupled to the genAI hype. We conduct a series of interviews with BPM practitioners to explore their understanding, expectations, and concerns related to agent autonomy, adaptability, human collaboration, and governance in processes. The findings reflect both challenges with respect to data inconsistencies, manual interventions, identification of process bottlenecks, actionability of process improvements, as well as the opportunities of enhanced efficiency, predictive process insights and proactive decision-making support. While the technology offers potential benefits, practitioners also anticipate risks such as biases, over-reliance, lack of transparency, and job displacement within organizations. These concerns underscore the need for a robust methodological framework for managing agents in organizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03693v1</guid>
      <category>cs.SE</category>
      <category>cs.MA</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hoang Vu, Nataliia Klievtsova, Henrik Leopold, Stefanie Rinderle-Ma, Timotheus Kampik</dc:creator>
    </item>
    <item>
      <title>Reinforcing Clinical Decision Support through Multi-Agent Systems and Ethical AI Governance</title>
      <link>https://arxiv.org/abs/2504.03699</link>
      <description>arXiv:2504.03699v1 Announce Type: cross 
Abstract: In the age of data-driven medicine, it is paramount to include explainable and ethically managed artificial intelligence in explaining clinical decision support systems to achieve trustworthy and effective patient care. The focus of this paper is on a new architecture of a multi-agent system for clinical decision support that uses modular agents to analyze laboratory results, vital signs, and the clinical context and then integrates these results to drive predictions and validate outcomes. We describe our implementation with the eICU database to run lab-analysis-specific agents, vitals-only interpreters, and contextual reasoners and then run the prediction module and a validation agent. Everything is a transparent implementation of business logic, influenced by the principles of ethical AI governance such as Autonomy, Fairness, and Accountability. It provides visible results that this agent-based framework not only improves on interpretability and accuracy but also on reinforcing trust in AI-assisted decisions in an intensive care setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03699v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying-Jung Chen, Chi-Sheng Chen, Ahmad Albarqawi</dc:creator>
    </item>
    <item>
      <title>VFlow: Discovering Optimal Agentic Workflows for Verilog Generation</title>
      <link>https://arxiv.org/abs/2504.03723</link>
      <description>arXiv:2504.03723v1 Announce Type: cross 
Abstract: Hardware design automation faces challenges in generating high-quality Verilog code efficiently. This paper introduces VFlow, an automated framework that optimizes agentic workflows for Verilog code generation. Unlike existing approaches that rely on pre-defined prompting strategies, VFlow leverages Monte Carlo Tree Search (MCTS) to discover effective sequences of Large Language Models invocations that maximize code quality while minimizing computational costs. VFlow extends the AFLOW methodology with domain-specific operators addressing hardware design requirements, including syntax validation, simulation-based verification, and synthesis optimization. Experimental evaluation on the VerilogEval benchmark demonstrates VFlow's superiority, achieving an 83.6% average pass@1 rate-a 6.1\% improvement over state-of-the-art PromptV and a 36.9\% gain compared to direct LLM invocation. Most significantly, VFlow enhances the capabilities of smaller models, enabling DeepSeek-V3 to achieve 141.2\% of GPT-4o's performance while reducing API costs to just 13\%. These findings indicate that intelligently optimized workflows enable cost-efficient LLMs to outperform larger models on hardware design tasks, potentially democratizing access to advanced digital circuit development tools and accelerating innovation in the semiconductor industry</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03723v1</guid>
      <category>cs.AR</category>
      <category>cs.MA</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yangbo Wei, Zhen Huang, Huang Li, Wei W. Xing, Ting-Jung Lin, Lei He</dc:creator>
    </item>
    <item>
      <title>Steve: LLM Powered ChatBot for Career Progression</title>
      <link>https://arxiv.org/abs/2504.03789</link>
      <description>arXiv:2504.03789v1 Announce Type: cross 
Abstract: The advancements in systems deploying large language models (LLMs), as well as improvements in their ability to act as agents with predefined templates, provide an opportunity to conduct qualitative, individualized assessments, creating a bridge between qualitative and quantitative methods for candidates seeking career progression. In this paper, we develop a platform that allows candidates to run AI-led interviews to assess their current career stage and curate coursework to enable progression to the next level. Our approach incorporates predefined career trajectories, associated skills, and a method to recommend the best resources for gaining the necessary skills for advancement. We employ OpenAI API calls along with expertly compiled chat templates to assess candidate competence. Our platform is highly configurable due to the modularity of the development, is easy to deploy and use, and available as a web interface where the only requirement is candidate resumes in PDF format. We demonstrate a use-case centered on software engineering and intend to extend this platform to be domain-agnostic, requiring only regular updates to chat templates as industries evolve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03789v1</guid>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Naveen Mathews Renji, Balaji R Rao, Carlo Lipizzi</dc:creator>
    </item>
    <item>
      <title>Offline and Distributional Reinforcement Learning for Wireless Communications</title>
      <link>https://arxiv.org/abs/2504.03804</link>
      <description>arXiv:2504.03804v1 Announce Type: cross 
Abstract: The rapid growth of heterogeneous and massive wireless connectivity in 6G networks demands intelligent solutions to ensure scalability, reliability, privacy, ultra-low latency, and effective control. Although artificial intelligence (AI) and machine learning (ML) have demonstrated their potential in this domain, traditional online reinforcement learning (RL) and deep RL methods face limitations in real-time wireless networks. For instance, these methods rely on online interaction with the environment, which might be unfeasible, costly, or unsafe. In addition, they cannot handle the inherent uncertainties in real-time wireless applications. We focus on offline and distributional RL, two advanced RL techniques that can overcome these challenges by training on static datasets and accounting for network uncertainties. We introduce a novel framework that combines offline and distributional RL for wireless communication applications. Through case studies on unmanned aerial vehicle (UAV) trajectory optimization and radio resource management (RRM), we demonstrate that our proposed Conservative Quantile Regression (CQR) algorithm outperforms conventional RL approaches regarding convergence speed and risk management. Finally, we discuss open challenges and potential future directions for applying these techniques in 6G networks, paving the way for safer and more efficient real-time wireless systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03804v1</guid>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>cs.NI</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eslam Eldeeb, Hirley Alves</dc:creator>
    </item>
    <item>
      <title>Algorithmic Prompt Generation for Diverse Human-like Teaming and Communication with Large Language Models</title>
      <link>https://arxiv.org/abs/2504.03991</link>
      <description>arXiv:2504.03991v1 Announce Type: cross 
Abstract: Understanding how humans collaborate and communicate in teams is essential for improving human-agent teaming and AI-assisted decision-making. However, relying solely on data from large-scale user studies is impractical due to logistical, ethical, and practical constraints, necessitating synthetic models of multiple diverse human behaviors. Recently, agents powered by Large Language Models (LLMs) have been shown to emulate human-like behavior in social settings. But, obtaining a large set of diverse behaviors requires manual effort in the form of designing prompts. On the other hand, Quality Diversity (QD) optimization has been shown to be capable of generating diverse Reinforcement Learning (RL) agent behavior. In this work, we combine QD optimization with LLM-powered agents to iteratively search for prompts that generate diverse team behavior in a long-horizon, multi-step collaborative environment. We first show, through a human-subjects experiment (n=54 participants), that humans exhibit diverse coordination and communication behavior in this domain. We then show that our approach can effectively replicate trends from human teaming data and also capture behaviors that are not easily observed without collecting large amounts of data. Our findings highlight the combination of QD and LLM-powered agents as an effective tool for studying teaming and communication strategies in multi-agent collaboration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03991v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Siddharth Srikanth, Varun Bhatt, Boshen Zhang, Werner Hager, Charles Michael Lewis, Katia P. Sycara, Aaquib Tabrez, Stefanos Nikolaidis</dc:creator>
    </item>
    <item>
      <title>Improving Mixed-Criticality Scheduling with Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2504.03994</link>
      <description>arXiv:2504.03994v2 Announce Type: cross 
Abstract: This paper introduces a novel reinforcement learning (RL) approach to scheduling mixed-criticality (MC) systems on processors with varying speeds. Building upon the foundation laid by [1], we extend their work to address the non-preemptive scheduling problem, which is known to be NP-hard. By modeling this scheduling challenge as a Markov Decision Process (MDP), we develop an RL agent capable of generating near-optimal schedules for real-time MC systems. Our RL-based scheduler prioritizes high-critical tasks while maintaining overall system performance.
  Through extensive experiments, we demonstrate the scalability and effectiveness of our approach. The RL scheduler significantly improves task completion rates, achieving around 80% overall and 85% for high-criticality tasks across 100,000 instances of synthetic data and real data under varying system conditions. Moreover, under stable conditions without degradation, the scheduler achieves 94% overall task completion and 93% for high-criticality tasks. These results highlight the potential of RL-based schedulers in real-time and safety-critical applications, offering substantial improvements in handling complex and dynamic scheduling scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03994v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad El-Mahdy, Nourhan Sakr, Rodrigo Carrasco</dc:creator>
    </item>
    <item>
      <title>OrbitZoo: Multi-Agent Reinforcement Learning Environment for Orbital Dynamics</title>
      <link>https://arxiv.org/abs/2504.04160</link>
      <description>arXiv:2504.04160v1 Announce Type: cross 
Abstract: The increasing number of satellites and orbital debris has made space congestion a critical issue, threatening satellite safety and sustainability. Challenges such as collision avoidance, station-keeping, and orbital maneuvering require advanced techniques to handle dynamic uncertainties and multi-agent interactions. Reinforcement learning (RL) has shown promise in this domain, enabling adaptive, autonomous policies for space operations; however, many existing RL frameworks rely on custom-built environments developed from scratch, which often use simplified models and require significant time to implement and validate the orbital dynamics, limiting their ability to fully capture real-world complexities. To address this, we introduce OrbitZoo, a versatile multi-agent RL environment built on a high-fidelity industry standard library, that enables realistic data generation, supports scenarios like collision avoidance and cooperative maneuvers, and ensures robust and accurate orbital dynamics. The environment is validated against a real satellite constellation, Starlink, achieving a Mean Absolute Percentage Error (MAPE) of 0.16% compared to real-world data. This validation ensures reliability for generating high-fidelity simulations and enabling autonomous and independent satellite operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04160v1</guid>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alexandre Oliveira, Katarina Dyreby, Francisco Caldas, Cl\'audia Soares</dc:creator>
    </item>
    <item>
      <title>Conformal Data-driven Control of Stochastic Multi-Agent Systems under Collaborative Signal Temporal Logic Specifications</title>
      <link>https://arxiv.org/abs/2504.04615</link>
      <description>arXiv:2504.04615v1 Announce Type: cross 
Abstract: We study the control of stochastic discrete-time linear multi-agent systems (MAS) subject to additive stochastic noise and collaborative signal temporal logic (STL) specifications to be satisfied with a desired probability. Given available disturbance datasets, we leverage conformal prediction (CP) to address the underlying chance-constrained multi-agent STL synthesis problem in a distribution-free manner. By introducing nonconformity scores as functions of prediction regions (PRs) of error trajectories, we develop an iterative PR-scaling and disturbance-feedback synthesis approach to bound training error trajectory samples. These bounds are then calibrated using a separate dataset, providing probabilistic guarantees via CP. Subsequently, we relax the underlying stochastic optimal control problem by tightening the robustness functions of collaborative tasks based on their Lipschitz constants and the computed error bounds. To address scalability, we exploit the compositional structure of the multi-agent STL formula and propose a model-predictive-control-like algorithm, where agent-level problems are solved in a distributed fashion. Lastly, we showcase the benefits of the proposed method in comparison with [1] via an illustrative example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04615v1</guid>
      <category>eess.SY</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eleftherios E. Vlahakis, Lars Lindemann, Dimos V. Dimarogonas</dc:creator>
    </item>
    <item>
      <title>Large-Scale Mixed-Traffic and Intersection Control using Multi-agent Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2504.04691</link>
      <description>arXiv:2504.04691v1 Announce Type: cross 
Abstract: Traffic congestion remains a significant challenge in modern urban networks. Autonomous driving technologies have emerged as a potential solution. Among traffic control methods, reinforcement learning has shown superior performance over traffic signals in various scenarios. However, prior research has largely focused on small-scale networks or isolated intersections, leaving large-scale mixed traffic control largely unexplored. This study presents the first attempt to use decentralized multi-agent reinforcement learning for large-scale mixed traffic control in which some intersections are managed by traffic signals and others by robot vehicles. Evaluating a real-world network in Colorado Springs, CO, USA with 14 intersections, we measure traffic efficiency via average waiting time of vehicles at intersections and the number of vehicles reaching their destinations within a time window (i.e., throughput). At 80% RV penetration rate, our method reduces waiting time from 6.17 s to 5.09 s and increases throughput from 454 vehicles per 500 seconds to 493 vehicles per 500 seconds, outperforming the baseline of fully signalized intersections. These findings suggest that integrating reinforcement learning-based control large-scale traffic can improve overall efficiency and may inform future urban planning strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04691v1</guid>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Songyang Liu, Muyang Fan, Weizi Li, Jing Du, Shuai Li</dc:creator>
    </item>
    <item>
      <title>Simulating Persuasive Dialogues on Meat Reduction with Generative Agents</title>
      <link>https://arxiv.org/abs/2504.04872</link>
      <description>arXiv:2504.04872v1 Announce Type: cross 
Abstract: Meat reduction benefits human and planetary health, but social norms keep meat central in shared meals. To date, the development of communication strategies that promote meat reduction while minimizing social costs has required the costly involvement of human participants at each stage of the process. We present work in progress on simulating multi-round dialogues on meat reduction between Generative Agents based on large language models (LLMs). We measure our main outcome using established psychological questionnaires based on the Theory of Planned Behavior and additionally investigate Social Costs. We find evidence that our preliminary simulations produce outcomes that are (i) consistent with theoretical expectations; and (ii) valid when compared to data from previous studies with human participants. Generative agent-based models are a promising tool for identifying novel communication strategies on meat reduction-tailored to highly specific participant groups-to then be tested in subsequent studies with human participants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04872v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Georg Ahnert, Elena Wurth, Markus Strohmaier, Jutta Mata</dc:creator>
    </item>
    <item>
      <title>Joint Pedestrian and Vehicle Traffic Optimization in Urban Environments using Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2504.05018</link>
      <description>arXiv:2504.05018v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) holds significant promise for adaptive traffic signal control. While existing RL-based methods demonstrate effectiveness in reducing vehicular congestion, their predominant focus on vehicle-centric optimization leaves pedestrian mobility needs and safety challenges unaddressed. In this paper, we present a deep RL framework for adaptive control of eight traffic signals along a real-world urban corridor, jointly optimizing both pedestrian and vehicular efficiency. Our single-agent policy is trained using real-world pedestrian and vehicle demand data derived from Wi-Fi logs and video analysis. The results demonstrate significant performance improvements over traditional fixed-time signals, reducing average wait times per pedestrian and per vehicle by up to 67% and 52%, respectively, while simultaneously decreasing total accumulated wait times for both groups by up to 67% and 53%. Additionally, our results demonstrate generalization capabilities across varying traffic demands, including conditions entirely unseen during training, validating RL's potential for developing transportation systems that serve all road users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05018v1</guid>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bibek Poudel, Xuan Wang, Weizi Li, Lei Zhu, Kevin Heaslip</dc:creator>
    </item>
    <item>
      <title>Attention-Augmented Inverse Reinforcement Learning with Graph Convolutions for Multi-Agent Task Allocation</title>
      <link>https://arxiv.org/abs/2504.05045</link>
      <description>arXiv:2504.05045v2 Announce Type: cross 
Abstract: Multi-agent task allocation (MATA) plays a vital role in cooperative multi-agent systems, with significant implications for applications such as logistics, search and rescue, and robotic coordination. Although traditional deep reinforcement learning (DRL) methods have been shown to be promising, their effectiveness is hindered by a reliance on manually designed reward functions and inefficiencies in dynamic environments. In this paper, an inverse reinforcement learning (IRL)-based framework is proposed, in which multi-head self-attention (MHSA) and graph attention mechanisms are incorporated to enhance reward function learning and task execution efficiency. Expert demonstrations are utilized to infer optimal reward densities, allowing dependence on handcrafted designs to be reduced and adaptability to be improved. Extensive experiments validate the superiority of the proposed method over widely used multi-agent reinforcement learning (MARL) algorithms in terms of both cumulative rewards and task execution efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05045v2</guid>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huilin Yin, Zhikun Yang, Daniel Watzenig</dc:creator>
    </item>
    <item>
      <title>MA-DV2F: A Multi-Agent Navigation Framework using Dynamic Velocity Vector Field</title>
      <link>https://arxiv.org/abs/2411.06404</link>
      <description>arXiv:2411.06404v5 Announce Type: replace 
Abstract: In this paper we propose MA-DV2F: Multi-Agent Dynamic Velocity Vector Field. It is a framework for simultaneously controlling a group of vehicles in challenging environments. DV2F is generated for each vehicle independently and provides a map of reference orientation and speed that a vehicle must attain at any point on the navigation grid such that it safely reaches its target. The field is dynamically updated depending on the speed and proximity of the ego-vehicle to other agents. This dynamic adaptation of the velocity vector field allows prevention of imminent collisions. Experimental results show that MA-DV2F outperforms concurrent methods in terms of safety, computational efficiency and accuracy in reaching the target when scaling to a large number of vehicles. Project page for this work can be found here: https://yininghase.github.io/MA-DV2F/</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06404v5</guid>
      <category>cs.MA</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yining Ma, Qadeer Khan, Daniel Cremers</dc:creator>
    </item>
    <item>
      <title>On Word-of-Mouth and Private-Prior Sequential Social Learning</title>
      <link>https://arxiv.org/abs/2504.02913</link>
      <description>arXiv:2504.02913v2 Announce Type: replace 
Abstract: Social learning provides a fundamental framework in economics and social sciences for studying interactions among rational agents who observe each other's actions but lack direct access to individual beliefs. This paper investigates a specific social learning paradigm known as Word-of-Mouth (WoM), where a series of agents seeks to estimate the state of a dynamical system. The first agent receives noisy measurements of the state, while each subsequent agent relies solely on a degraded version of her predecessor's estimate. A defining feature of WoM is that the final agent's belief is publicly broadcast and adopted by all agents, in place of their own. We analyze this setting both theoretically and through numerical simulations, showing that some agents benefit from using the public belief broadcast by the last agent, while others suffer from performance deterioration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02913v2</guid>
      <category>cs.MA</category>
      <category>cs.SI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Da Col, Cristian R. Rojas, Vikram Krishnamurthy</dc:creator>
    </item>
    <item>
      <title>A Game of Pawns</title>
      <link>https://arxiv.org/abs/2305.04096</link>
      <description>arXiv:2305.04096v5 Announce Type: replace-cross 
Abstract: We introduce and study pawn games, a class of two-player zero-sum turn-based graph games. A turn-based graph game proceeds by placing a token on an initial vertex, and whoever controls the vertex on which the token is located, chooses its next location. This leads to a path in the graph, which determines the winner. Traditionally, the control of vertices is predetermined and fixed. The novelty of pawn games is that control of vertices changes dynamically throughout the game as follows. Each vertex of a pawn game is owned by a pawn. In each turn, the pawns are partitioned between the two players, and the player who controls the pawn that owns the vertex on which the token is located, chooses the next location of the token. Control of pawns changes dynamically throughout the game according to a fixed mechanism. Specifically, we define several grabbing-based mechanisms in which control of at most one pawn transfers at the end of each turn. We study the complexity of solving pawn games, where we focus on reachability objectives and parameterize the problem by the mechanism that is being used and by restrictions on pawn ownership of vertices. On the positive side, even though pawn games are exponentially-succinct turn-based games, we identify several natural classes that can be solved in PTIME. On the negative side, we identify several EXPTIME-complete classes, where our hardness proofs are based on a new class of games called Lock &amp; Key games, which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.04096v5</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guy Avni, Pranav Ghorpade, Shibashis Guha</dc:creator>
    </item>
    <item>
      <title>Toward LLM-Agent-Based Modeling of Transportation Systems: A Conceptual Framework</title>
      <link>https://arxiv.org/abs/2412.06681</link>
      <description>arXiv:2412.06681v2 Announce Type: replace-cross 
Abstract: In transportation system demand modeling and simulation, agent-based models and microsimulations are current state-of-the-art approaches. However, existing agent-based models still have some limitations on behavioral realism and resource demand that limit their applicability. In this study, leveraging the emerging technology of large language models (LLMs) and LLM-based agents, we propose a general LLM-agent-based modeling framework for transportation systems. We argue that LLM agents not only possess the essential capabilities to function as agents but also offer promising solutions to overcome some limitations of existing agent-based models. Our conceptual framework design closely replicates the decision-making and interaction processes and traits of human travelers within transportation networks, and we demonstrate that the proposed systems can meet critical behavioral criteria for decision-making and learning behaviors using related studies and a demonstrative example of LLM agents' learning and adjustment in the bottleneck setting. Although further refinement of the LLM-agent-based modeling framework is necessary, we believe that this approach has the potential to improve transportation system modeling and simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06681v2</guid>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianming Liu, Jirong Yang, Yafeng Yin</dc:creator>
    </item>
    <item>
      <title>Multi-agent Auto-Bidding with Latent Graph Diffusion Models</title>
      <link>https://arxiv.org/abs/2503.05805</link>
      <description>arXiv:2503.05805v2 Announce Type: replace-cross 
Abstract: This paper proposes a diffusion-based auto-bidding framework that leverages graph representations to model large-scale auction environments. In such settings, agents must dynamically optimize bidding strategies under constraints defined by key performance indicator (KPI) metrics, all while operating in competitive environments characterized by uncertain, sparse, and stochastic variables. To address these challenges, we introduce a novel approach combining learnable graph-based embeddings with a planning-based latent diffusion model (LDM). By capturing patterns and nuances underlying the interdependence of impression opportunities and the multi-agent dynamics of the auction environment, the graph representation enable expressive computations regarding auto-bidding outcomes. With reward alignment techniques, the LDM's posterior is fine-tuned to generate auto-bidding trajectories that maximize KPI metrics while satisfying constraint thresholds. Empirical evaluations on both real-world and synthetic auction environments demonstrate significant improvements in auto-bidding performance across multiple common KPI metrics, as well as accuracy in forecasting auction outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05805v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dom Huh, Prasant Mohapatra</dc:creator>
    </item>
    <item>
      <title>Data Spatial Programming</title>
      <link>https://arxiv.org/abs/2503.15812</link>
      <description>arXiv:2503.15812v4 Announce Type: replace-cross 
Abstract: We introduce a novel programming model, Data Spatial Programming, which extends the semantics of Object-Oriented Programming (OOP) by introducing new class-like constructs called archetypes. These archetypes encapsulate the topological relationships between data entities and the execution flow in a structured manner, enabling more expressive and semantically rich computations over interconnected data structures or finite states. By formalizing the relationships between data elements in this topological space, our approach allows for more intuitive modeling of complex systems where a topology of connections is formed for the underlying computational model. This paradigm addresses limitations in traditional OOP when representing a wide range of problems in computer science such as agent-based systems, social networks, processing on relational data, neural networks, distributed systems, finite state machines, and other spatially-oriented computational problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15812v4</guid>
      <category>cs.PL</category>
      <category>cs.MA</category>
      <category>cs.SE</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jason Mars</dc:creator>
    </item>
    <item>
      <title>Responsible Development of Offensive AI</title>
      <link>https://arxiv.org/abs/2504.02701</link>
      <description>arXiv:2504.02701v2 Announce Type: replace-cross 
Abstract: As AI advances, broader consensus is needed to determine research priorities. This endeavor discusses offensive AI and provides guidance by leveraging Sustainable Development Goals (SDGs) and interpretability techniques. The objective is to more effectively establish priorities that balance societal benefits against risks. The two forms of offensive AI evaluated in this study are vulnerability detection agents, which solve Capture- The-Flag challenges, and AI-powered malware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02701v2</guid>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryan Marinelli</dc:creator>
    </item>
  </channel>
</rss>
