<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 15 Jan 2025 02:35:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Statistical Modeling of Networked Evolutionary Public Goods Games</title>
      <link>https://arxiv.org/abs/2501.07007</link>
      <description>arXiv:2501.07007v1 Announce Type: new 
Abstract: Repeated small dynamic networks are integral to studies in evolutionary game theory, where networked public goods games offer novel insights into human behaviors. Building on these findings, it is necessary to develop a statistical model that effectively captures dependencies across multiple small dynamic networks. While Separable Temporal Exponential-family Random Graph Models (STERGMs) have demonstrated success in modeling a large single dynamic network, their application to multiple small dynamic networks with less than 10 actors, remains unexplored. In this study, we extend the STERGM framework to accommodate multiple small dynamic networks, offering an approach to analyzing such systems. Taking advantage of the small network sizes, our proposed approach improves accuracy in statistical inference through direct computation, unlike conventional approaches that rely on Markov Chain Monte Carlo methods. We demonstrate the validity of this framework through the analysis of a networked public goods experiment into individual decision-making about cooperation and defection. The resulting statistical inference uncovers novel insights into the dynamics of social dilemmas, showcasing the effectiveness and robustness of this modeling and approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07007v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hiroyasu Ando, Akihiro Nishi, Mark S. Handcock</dc:creator>
    </item>
    <item>
      <title>Quality Control of Lifetime Drift in Discrete Electrical Parameters in Semiconductor Devices via Transition Modeling</title>
      <link>https://arxiv.org/abs/2501.07115</link>
      <description>arXiv:2501.07115v1 Announce Type: new 
Abstract: Semiconductors are widely used in various applications and critical infrastructures. These devices have specified lifetimes and quality targets that manufacturers must achieve. Lifetime estimation is conducted through accelerated stress tests. Electrical parameters are measured at multiple times during a stress test procedure. The change in these Electrical parameters is called lifetime drift. Data from these tests can be used to develop a statistical model predicting the lifetime behavior of the electrical parameters in real devices. These models can provide early warnings in production processes, identify critical parameter drift, and detect outliers. While models for continuous electrical parameters exists, there may be bias when estimating the lifetime of discrete parameters. To address this, we propose a semi-parametric model for degradation trajectories based on longitudinal stress test data. This model optimizes guard bands, or quality guaranteeing tighter limits, for discrete electrical parameters at production testing. It is scalable, data-driven, and explainable, offering improvements over existing methods for continuous underlying data, such as faster calculations, arbitrary non-parametric conditional distribution modeling, and a natural extension of optimization algorithms to the discrete case using Markov transition matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07115v1</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.microrel.2024.115555.</arxiv:DOI>
      <arxiv:journal_reference>Microelectronics Reliability, Volume 164, 2025, 115555, ISSN 0026-2714</arxiv:journal_reference>
      <dc:creator>Lukas Sommeregger, J\"urgen Pilz</dc:creator>
    </item>
    <item>
      <title>Subtype-Aware Registration of Longitudinal Electronic Health Records</title>
      <link>https://arxiv.org/abs/2501.07336</link>
      <description>arXiv:2501.07336v1 Announce Type: new 
Abstract: Electronic Health Records (EHRs) contain extensive patient information that can inform downstream clinical decisions, such as mortality prediction, disease phenotyping, and disease onset prediction. A key challenge in EHR data analysis is the temporal gap between when a condition is first recorded and its actual onset time. Such timeline misalignment can lead to artificially distinct biomarker trends among patients with similar disease progression, undermining the reliability of downstream analysis and complicating tasks like disease subtyping. To address this challenge, we provide a subtype-aware timeline registration method that leverages data projection and discrete optimization to simultaneously correct timeline misalignment and improve disease subtyping. Through simulation and real-world data analyses, we demonstrate that the proposed method effectively aligns distorted observed records with the true disease progression patterns, enhancing subtyping clarity and improving performance in downstream clinical analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07336v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xin Gai, Shiyi Jiang, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Predicting System Dynamics of Universal Growth Patterns in Complex Systems</title>
      <link>https://arxiv.org/abs/2501.07349</link>
      <description>arXiv:2501.07349v1 Announce Type: new 
Abstract: Predicting dynamic behaviors is one of the goals of science in general as well as essential to many specific applications of human knowledge to real world systems. Here we introduce an analytic approach using the sigmoid growth curve to model the dynamics of individual entities within complex systems. Despite the challenges posed by nonlinearity and unpredictability in system behaviors, we demonstrate the applicability of the sigmoid curve to capture the acceleration and deceleration of growth, predicting an entitys ultimate state well in advance of reaching it. We show that our analysis can be applied to diverse systems where entities exhibit nonlinear growth using case studies of (1) customer purchasing and (2) U.S. legislation adoption. This showcases the ability to forecast months to years ahead of time, providing valuable insights for business leaders and policymakers. Moreover, our characterization of individual component dynamics offers a framework to reveal the aggregate behavior of the entire system. We introduce a classification of entities based upon similar lifepaths. This study contributes to the understanding of complex system behaviors, offering a practical tool for prediction and system behavior insight that can inform strategic decision making in multiple domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07349v1</guid>
      <category>stat.AP</category>
      <category>math.DS</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Leila Hedayatifar, Alfredo J. Morales, Dominic E. Saadi, Rachel A. Rigg, Olha Buchel, Amir Akhavan, Egemen Sert, Aabir Abubaker Kar, Mehrzad Sasanpour, Irving R. Epstein, Yaneer Bar-Yam</dc:creator>
    </item>
    <item>
      <title>Predicting House Rental Prices in Ghana Using Machine Learning</title>
      <link>https://arxiv.org/abs/2501.06241</link>
      <description>arXiv:2501.06241v1 Announce Type: cross 
Abstract: This study investigates the efficacy of machine learning models for predicting house rental prices in Ghana, addressing the need for accurate and accessible housing market information. Utilising a comprehensive dataset of rental listings, we trained and evaluated various models, including CatBoost, XGBoost, and Random Forest. CatBoost emerged as the best-performing model, achieving an $R^2$ of 0.876, demonstrating its ability to effectively capture complex relationships within the housing market. Feature importance analysis revealed that location-based features, number of bedrooms, bathrooms, and furnishing status are key drivers of rental prices. Our findings provide valuable insights for stakeholders, including real estate professionals, investors, and policymakers, while also highlighting opportunities for future research, such as incorporating temporal data and exploring regional variations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06241v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.20944/preprints202412.1927.v1</arxiv:DOI>
      <dc:creator>Philip Adzanoukpe</dc:creator>
    </item>
    <item>
      <title>CeViT: Copula-Enhanced Vision Transformer in multi-task learning and bi-group image covariates with an application to myopia screening</title>
      <link>https://arxiv.org/abs/2501.06540</link>
      <description>arXiv:2501.06540v1 Announce Type: cross 
Abstract: We aim to assist image-based myopia screening by resolving two longstanding problems, "how to integrate the information of ocular images of a pair of eyes" and "how to incorporate the inherent dependence among high-myopia status and axial length for both eyes." The classification-regression task is modeled as a novel 4-dimensional muti-response regression, where discrete responses are allowed, that relates to two dependent 3rd-order tensors (3D ultrawide-field fundus images). We present a Vision Transformer-based bi-channel architecture, named CeViT, where the common features of a pair of eyes are extracted via a shared Transformer encoder, and the interocular asymmetries are modeled through separated multilayer perceptron heads. Statistically, we model the conditional dependence among mixture of discrete-continuous responses given the image covariates by a so-called copula loss. We establish a new theoretical framework regarding fine-tuning on CeViT based on latent representations, allowing the black-box fine-tuning procedure interpretable and guaranteeing higher relative efficiency of fine-tuning weight estimation in the asymptotic setting. We apply CeViT to an annotated ultrawide-field fundus image dataset collected by Shanghai Eye \&amp; ENT Hospital, demonstrating that CeViT enhances the baseline model in both accuracy of classifying high-myopia and prediction of AL on both eyes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06540v1</guid>
      <category>cs.CV</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chong Zhong, Yang Li, Jinfeng Xu, Xiang Fu, Yunhao Liu, Qiuyi Huang, Danjuan Yang, Meiyan Li, Aiyi Liu, Alan H. Welsh, Xingtao Zhou, Bo Fu, Catherine C. Liu</dc:creator>
    </item>
    <item>
      <title>OpenAlex2Pajek -- an R Package for converting OpenAlex bibliographic data into Pajek networks</title>
      <link>https://arxiv.org/abs/2501.06656</link>
      <description>arXiv:2501.06656v1 Announce Type: cross 
Abstract: For analysis of bibliographic data, we can obtain from bibliographic databases the corresponding collection of bibliographic networks. Recently OpenAlex, a new open-access bibliographic database, became available. We present OpenAlex2Pajek, an R package for converting OpenAlex data into a collection of Pajek's networks. For an illustration, we created a temporal weighted network describing the co-authorship between world countries for years from 1990 to 2023. We present some analyses of this network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06656v1</guid>
      <category>cs.DL</category>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>In PK Jain, et. al. (Eds). Innovations in Webometrics, Informetrics, and Scientometrics: Proc. COLLNET 2024, 2-14 Dec, Strasbourg, France (pp. 66-77). Delhi: Bookwell. (ISBN 978-93-86578-65-5)</arxiv:journal_reference>
      <dc:creator>Vladimir Batagelj</dc:creator>
    </item>
    <item>
      <title>Variable Selection Methods for Multivariate, Functional, and Complex Biomedical Data in the AI Age</title>
      <link>https://arxiv.org/abs/2501.06868</link>
      <description>arXiv:2501.06868v1 Announce Type: cross 
Abstract: Many problems within personalized medicine and digital health rely on the analysis of continuous-time functional biomarkers and other complex data structures emerging from high-resolution patient monitoring. In this context, this work proposes new optimization-based variable selection methods for multivariate, functional, and even more general outcomes in metrics spaces based on best-subset selection. Our framework applies to several types of regression models, including linear, quantile, or non parametric additive models, and to a broad range of random responses, such as univariate, multivariate Euclidean data, functional, and even random graphs. Our analysis demonstrates that our proposed methodology outperforms state-of-the-art methods in accuracy and, especially, in speed-achieving several orders of magnitude improvement over competitors across various type of statistical responses as the case of mathematical functions. While our framework is general and is not designed for a specific regression and scientific problem, the article is self-contained and focuses on biomedical applications. In the clinical areas, serves as a valuable resource for professionals in biostatistics, statistics, and artificial intelligence interested in variable selection problem in this new technological AI-era.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06868v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcos Matabuena</dc:creator>
    </item>
    <item>
      <title>Uncertainty Guarantees on Automated Precision Weeding using Conformal Prediction</title>
      <link>https://arxiv.org/abs/2501.07185</link>
      <description>arXiv:2501.07185v1 Announce Type: cross 
Abstract: Precision agriculture in general, and precision weeding in particular, have greatly benefited from the major advancements in deep learning and computer vision. A large variety of commercial robotic solutions are already available and deployed. However, the adoption by farmers of such solutions is still low for many reasons, an important one being the lack of trust in these systems. This is in great part due to the opaqueness and complexity of deep neural networks and the manufacturers' inability to provide valid guarantees on their performance. Conformal prediction, a well-established methodology in the machine learning community, is an efficient and reliable strategy for providing trustworthy guarantees on the predictions of any black-box model under very minimal constraints. Bridging the gap between the safe machine learning and precision agriculture communities, this article showcases conformal prediction in action on the task of precision weeding through deep learning-based image classification. After a detailed presentation of the conformal prediction methodology and the development of a precision spraying pipeline based on a ''conformalized'' neural network and well-defined spraying decision rules, the article evaluates this pipeline on two real-world scenarios: one under in-distribution conditions, the other reflecting a near out-of-distribution setting. The results show that we are able to provide formal, i.e. certifiable, guarantees on spraying at least 90% of the weeds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07185v1</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Melki (IMS), Lionel Bombrun (IMS), Boubacar Diallo (IMS), J\'er\^ome Dias (IMS), Jean-Pierre da Costa (IMS)</dc:creator>
    </item>
    <item>
      <title>A data-driven approach to discover and quantify systemic lupus erythematosus etiological heterogeneity from electronic health records</title>
      <link>https://arxiv.org/abs/2501.07206</link>
      <description>arXiv:2501.07206v1 Announce Type: cross 
Abstract: Systemic lupus erythematosus (SLE) is a complex heterogeneous disease with many manifestational facets. We propose a data-driven approach to discover probabilistic independent sources from multimodal imperfect EHR data. These sources represent exogenous variables in the data generation process causal graph that estimate latent root causes of the presence of SLE in the health record. We objectively evaluated the sources against the original variables from which they were discovered by training supervised models to discriminate SLE from negative health records using a reduced set of labelled instances. We found 19 predictive sources with high clinical validity and whose EHR signatures define independent factors of SLE heterogeneity. Using the sources as input patient data representation enables models to provide with rich explanations that better capture the clinical reasons why a particular record is (not) an SLE case. Providers may be willing to trade patient-level interpretability for discrimination especially in challenging cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07206v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marco Barbero Mota, John M. Still, Jorge L. Gamboa, Eric V. Strobl, Charles M. Stein, Vivian K. Kawai, Thomas A. Lasko</dc:creator>
    </item>
    <item>
      <title>Bayesian variable selection using an informed reversible jump in imaging genetics: an application to schizophrenia</title>
      <link>https://arxiv.org/abs/2307.01134</link>
      <description>arXiv:2307.01134v2 Announce Type: replace 
Abstract: From a practical perspective, proposals are one of the main bottleneck for any Markov Chain Monte Carlo (MCMC) algorithm. This paper suggests a novel data driven or informed proposal for reversible jump MCMC for Bayesian variable selection in the context of predictive risk assessment for schizophrenia based on imaging genetic data. Given functional Magnetic Resonance Image and Single Nucleotide Polymorphisms information of healthy and people diagnosed with schizophrenia, we use a Bayesian probit model to select discriminating variables for inferential purposes, while to estimate the predictive risk, the most promising models are combined using a Bayesian model averaging scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.01134v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Djidenou Montcho, Daiane Zuanetti, Thierry Chekouo, Luis Milan</dc:creator>
    </item>
    <item>
      <title>Predicting Stock Price of Construction Companies using Weighted Ensemble Learning</title>
      <link>https://arxiv.org/abs/2311.06397</link>
      <description>arXiv:2311.06397v4 Announce Type: replace 
Abstract: Modeling the behavior of stock price data has always been one of the challengeous applications of Artificial Intelligence (AI) and Machine Learning (ML) due to its high complexity and dependence on various conditions. Recent studies show that this will be difficult to do with just one learning model. The problem can be more complex for companies of construction section, due to the dependency of their behavior on more conditions. This study aims to provide a hybrid model for improving the accuracy of prediction for stock price index of companies in construction section. The contribution of this paper can be considered as follows: First, a combination of several prediction models is used to predict stock price, so that learning models can cover each other's error. In this research, an ensemble model based on Artificial Neural Network (ANN), Gaussian Process Regression (GPR) and Classification and Regression Tree (CART) is presented for predicting stock price index. Second, the optimization technique is used to determine the effect of each learning model on the prediction result. For this purpose, first all three mentioned algorithms process the data simultaneously and perform the prediction operation. Then, using the Cuckoo Search (CS) algorithm, the output weight of each algorithm is determined as a coefficient. Finally, using the ensemble technique, these results are combined and the final output is generated through weighted averaging on optimal coefficients. The results showed that using CS optimization in the proposed ensemble system is highly effective in reducing prediction error. Comparing the evaluation results of the proposed system with similar algorithms, indicates that our model is more accurate and can be useful for predicting stock price index in real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.06397v4</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyuan Song</dc:creator>
    </item>
    <item>
      <title>How to verify that a given process is a L\'evy-Driven Ornstein-Uhlenbeck Process</title>
      <link>https://arxiv.org/abs/2501.03434</link>
      <description>arXiv:2501.03434v2 Announce Type: replace 
Abstract: Assuming that a L\'evy-Driven Ornstein-Uhlenbeck (or CAR(1)) processes is observed at discrete times $0$, $h$, $2h$,$\cdots$ $[T/h]h$. We introduce a step-by-step methodological approach on how a person would verify the model assumptions. The methodology involves estimating the model parameters and approximating the driving process. We demonstrate how to use the increments of the approximated driving process, along with the estimated parameters, to test the assumptions that the CAR(1) process is L\'evy-driven. We then show how to test the hypothesis that the CAR(1) process belongs to a specified class of L\'evy processes. The performance of the tests is illustrated through multiple simulations. Finally, we demonstrate how to apply the methodology step-by-step to a variety of economic and financial data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03434v2</guid>
      <category>stat.AP</category>
      <category>q-fin.ST</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ibrahim Abdelrazeq (Rhodes College), Hardy Smith (Rhodes College), Dinmukhammed Zhanbyrshy (Rhodes College)</dc:creator>
    </item>
    <item>
      <title>Change-point Detection and Segmentation of Discrete Data using Bayesian Context Trees</title>
      <link>https://arxiv.org/abs/2203.04341</link>
      <description>arXiv:2203.04341v3 Announce Type: replace-cross 
Abstract: A new Bayesian modelling framework is introduced for piece-wise homogeneous variable-memory Markov chains, along with a collection of effective algorithmic tools for change-point detection and segmentation of discrete time series. Building on the recently introduced Bayesian Context Trees (BCT) framework, the distributions of different segments in a discrete time series are described as variable-memory Markov chains. Inference for the presence and location of change-points is then performed via Markov chain Monte Carlo sampling. The key observation that facilitates effective sampling is that, using one of the BCT algorithms, the prior predictive likelihood of the data can be computed exactly, integrating out all the models and parameters in each segment. This makes it possible to sample directly from the posterior distribution of the number and location of the change-points, leading to accurate estimates and providing a natural quantitative measure of uncertainty in the results. Estimates of the actual model in each segment can also be obtained, at essentially no additional computational cost. Results on both simulated and real-world data indicate that the proposed methodology performs better than or as well as state-of-the-art techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.04341v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Valentinian Lungu, Ioannis Papageorgiou, Ioannis Kontoyiannis</dc:creator>
    </item>
    <item>
      <title>Revisiting Group Differences in High-Dimensional Choices: Method and Application to Congressional Speech</title>
      <link>https://arxiv.org/abs/2206.10877</link>
      <description>arXiv:2206.10877v2 Announce Type: replace-cross 
Abstract: Gentzkow, Shapiro and Taddy, Econometrica Vol 87, No 4, 2019 (henceforth GST) use a supervised text-based regression model to assess changes in partisanship in U.S. congressional speech over time. Their estimates imply that partisanship is far greater in recent years than in the past, and that it increased sharply in the early 1990s. The paper at hand provides a replication in the wide sense of GST by complementing their analysis in three ways. First, we propose an alternative unsupervised language model, which combines ideas of topic models and ideal point models, to analyze the change in partisanship over time. We apply this model to the Senate speech data used in GST ranging from 1981-2017. Using our model we replicate their results on the specific evolution of partisanship. Second, our model provides additional insights such as the data-driven estimation of evolvement of topical contents over time. Third, we identify key phrases of partisanship on topic level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.10877v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Hofmarcher, Jan V\'avra, Sourav Adhikari, Bettina Gr\"un</dc:creator>
    </item>
    <item>
      <title>Multiway Alignment of Political Attitudes</title>
      <link>https://arxiv.org/abs/2408.00139</link>
      <description>arXiv:2408.00139v2 Announce Type: replace-cross 
Abstract: The related concepts of partisan belief systems, issue alignment, and partisan sorting are central to our understanding of politics. These phenomena have been studied using measures of alignment between pairs of topics, or how much individuals' attitudes toward a topic reveal about their attitudes toward another topic. We introduce a higher-order measure that extends the assessment of alignment beyond pairs of topics by quantifying the amount of information individuals' opinions on one topic reveal about a set of topics simultaneously. Applying this approach to legislative voting behavior shows that parliamentary systems typically exhibit similar multiway alignment characteristics, but can change in response to shifting intergroup dynamics. In American National Election Studies surveys, our approach reveals a growing significance of party identification together with a consistent rise in multiway alignment over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00139v2</guid>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Letizia Iannucci, Ali Faqeeh, Ali Salloum, Ted Hsuan Yun Chen, Mikko Kivel\"a</dc:creator>
    </item>
    <item>
      <title>CDsampling: An R Package for Constrained D-Optimal Sampling in Paid Research Studies</title>
      <link>https://arxiv.org/abs/2410.20606</link>
      <description>arXiv:2410.20606v2 Announce Type: replace-cross 
Abstract: In the context of paid research studies and clinical trials, budget considerations often require patient sampling from available populations which comes with inherent constraints. We introduce the R package CDsampling, which is the first to our knowledge to integrate optimal design theories within the framework of constrained sampling. This package offers the possibility to find both D-optimal approximate and exact allocations for samplings with or without constraints. Additionally, it provides functions to find constrained uniform sampling as a robust sampling strategy when the model information is limited. To demonstrate its efficacy, we provide simulated examples and a real-data example with datasets embedded in the package and compare them with classical sampling methods. Furthermore, it revisits the theoretical results of the Fisher information matrix for generalized linear models (including regular linear regression model) and multinomial logistic models, offering functions for its computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20606v2</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifei Huang, Liping Tong, Jie Yang</dc:creator>
    </item>
    <item>
      <title>Feature Group Tabular Transformer: A Novel Approach to Traffic Crash Modeling and Causality Analysis</title>
      <link>https://arxiv.org/abs/2412.06825</link>
      <description>arXiv:2412.06825v2 Announce Type: replace-cross 
Abstract: Reliable and interpretable traffic crash modeling is essential for understanding causality and improving road safety. This study introduces a novel approach to predicting collision types by utilizing a comprehensive dataset fused from multiple sources, including weather data, crash reports, high-resolution traffic information, pavement geometry, and facility characteristics. Central to our approach is the development of a Feature Group Tabular Transformer (FGTT) model, which organizes disparate data into meaningful feature groups, represented as tokens. These group-based tokens serve as rich semantic components, enabling effective identification of collision patterns and interpretation of causal mechanisms. The FGTT model is benchmarked against widely used tree ensemble models, including Random Forest, XGBoost, and CatBoost, demonstrating superior predictive performance. Furthermore, model interpretation reveals key influential factors, providing fresh insights into the underlying causality of distinct crash types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06825v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.3934/aci.2025003</arxiv:DOI>
      <arxiv:journal_reference>Applied Computing and Intelligence, 2025, Volume 5, Issue 1: 29-56</arxiv:journal_reference>
      <dc:creator>Oscar Lares, Hao Zhen, Jidong J. Yang</dc:creator>
    </item>
  </channel>
</rss>
