<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 24 Jun 2024 04:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 24 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>QuIP: A P4 Quantum Internet Protocol Prototyping Framework</title>
      <link>https://arxiv.org/abs/2406.14597</link>
      <description>arXiv:2406.14597v1 Announce Type: new 
Abstract: Quantum entanglement is so fundamentally different from a network packet that several quantum network stacks have been proposed; one of which has even been experimentally demonstrated. Several simulators have also been developed to make up for limited hardware availability, and which facilitate the design and evaluation of quantum network protocols. However, the lack of shared tooling and community-agreed node architectures has resulted in protocol implementations that are tightly coupled to their simulators. Besides limiting their reusability between different simulators, it also makes building upon prior results and simulations difficult. To address this problem, we have developed QuIP: a P4-based Quantum Internet Protocol prototyping framework for quantum network protocol design. QuIP is a framework for designing and implementing quantum network protocols in a platform-agnostic fashion. It achieves this by providing the means to flexibly, but rigorously, define device architectures against which quantum network protocols can be implemented in the network programming language P4$_{16}$. QuIP also comes with the necessary tooling to enable their execution in existing quantum network simulators. We demonstrate its use by showcasing V1Quantum, a completely new device architecture, implementing a link- and network-layer protocol, and simulating it in the existing simulator NetSquid.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14597v1</guid>
      <category>cs.NI</category>
      <category>quant-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/JSAC.2024.3380096</arxiv:DOI>
      <arxiv:journal_reference>IEEE Journal on Selected Areas in Communications, vol. 42, no. 7, pp. 1936-1949, July 2024</arxiv:journal_reference>
      <dc:creator>Wojciech Kozlowski, Fernando A. Kuipers, Rob Smets, Belma Turkovic</dc:creator>
    </item>
    <item>
      <title>Compression of the Channel State Information with Deep Learning</title>
      <link>https://arxiv.org/abs/2406.14668</link>
      <description>arXiv:2406.14668v1 Announce Type: new 
Abstract: This paper proposes the use of deep autoencoders to compress the channel information in a multiple input and multiple output (MIMO) system. Although autoencoders perform lossy compression, they still have adequate usefulness when applied to MIMO system channel state information (CSI) compression. To demonstrate their impact on the CSI, we measure the performance of the system under two different channel models for different compression ratios. We show through simulation that the run-time complexity of this deep autoencoder is irrelative to the compression ratio and that useful compression ratios depend on the channel model and the signal to noise ratio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14668v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Faris B. Mismar, Aliye \"Ozge Kaya</dc:creator>
    </item>
    <item>
      <title>Energy-Aware Random Access Networks: Connection-Based versus Packet-Based</title>
      <link>https://arxiv.org/abs/2406.14965</link>
      <description>arXiv:2406.14965v1 Announce Type: new 
Abstract: Characterizing and comparing the optimal energy efficiency in energy-aware machine-to-machine (M2M) random access networks remains a challenge due to the distributed nature of the access behavior of nodes. To address this issue, this letter focuses on the energy efficiency limits of two typical random access schemes, i.e., connection-based Aloha and packet-based Aloha, based on which we conducted a performance comparison. Specifically, by integrating limited energy constraints and network throughput, the lifetime throughput can be derived, and further optimized with a guarantee of targeted lifetime via selecting the transmission probability. Then we present a comparative study on the optimal lifetime throughput of packet-based Aloha and connection-based Aloha to characterize criteria for beneficial connection establishment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14965v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anshan Yuan, Fangming Zhao, Xinghua Sun</dc:creator>
    </item>
    <item>
      <title>Modeling and Analysis of Application Interference on Dragonfly+</title>
      <link>https://arxiv.org/abs/2406.15097</link>
      <description>arXiv:2406.15097v1 Announce Type: new 
Abstract: Dragonfly class of networks are considered as promising interconnects for next-generation supercomputers. While Dragonfly+ networks offer more path diversity than the original Dragonfly design, they are still prone to performance variability due to their hierarchical architecture and resource sharing design. Event-driven network simulators are indispensable tools for navigating complex system design. In this study, we quantitatively evaluate a variety of application communication interactions on a 3,456-node Dragonfly+ system by using the CODES toolkit. This study looks at the impact of communication interference from a user's perspective. Specifically, for a given application submitted by a user, we examine how this application will behave with the existing workload running in the system under different job placement policies. Our simulation study considers hundreds of experiment configurations including four target applications with representative communication patterns under a variety of network traffic conditions. Our study shows that intra-job interference can cause severe performance degradation for communication-intensive applications. Inter-job interference can generally be reduced for applications with one-to-one or one-to-many communication patterns through job isolation. Application with one-to-all communication pattern is resilient to network interference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15097v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yao Kang, Xin Wang, Neil McGlohon, Misbah Mubarak, Sudheer Chunduri, Zhiling Lan</dc:creator>
    </item>
    <item>
      <title>Hybrid Intelligent Routing with Optimized Learning (HIROL) for Adaptive Routing Topology management in FANETs</title>
      <link>https://arxiv.org/abs/2406.15105</link>
      <description>arXiv:2406.15105v1 Announce Type: new 
Abstract: Enhancing the routing efficacy of Flying AdHoc Networks (FANETs), a network of numerous Unmanned Aerial Vehicles (UAVs), in which various challenges may arise as a result of the varied mobility, speed, direction, and rapid topology changes. Given the special features of UAVs, in particular their fast mobility, frequent topology changes, and 3D space movements, it is difficult to transport them through a FANET. The suggested study presents a complete hybrid model: HIROL (Hybrid Intelligent Routing with Optimized Learning) that integrates the ABC (Artificial Bee Colony) algorithm, DSR (Dynamic Source Routing) by incorporating Optimized Link State Routing (OLSR) and ANNs (Artificial Neural Networks) to optimize the routing process. The HIROL optimizes link management by ABC optimization algorithm and reliably analyses link status using characteristics from OLSR and DSR; at the same time, an ANN-based technique successfully classifies connection state. In order to provide optimal route design and maintenance, HIROL dynamically migrates between OLSR and DSR approaches according to the network topology conditions. After running thorough tests in Network Simulator 2 (NS-2), when compared to more conventional DSR and OLSR models, the hybrid model HIROL performs far better in simulations and tests. An increase in throughput (3.5 Mbps vs. 3.2-3.4 Mbps), a decrease in communication overhead (15% vs. 18-20%), and an improvement in Packet Delivery Ratio (97.5% vs. 94-95.5%). These results demonstrate that the suggested HIROL model improves FANET routing performance in different types of networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15105v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ch. Naveen Kumar Reddy, M. Anusha</dc:creator>
    </item>
    <item>
      <title>Age of Information Versions: a Semantic View of Markov Source Monitoring</title>
      <link>https://arxiv.org/abs/2406.14594</link>
      <description>arXiv:2406.14594v1 Announce Type: cross 
Abstract: We consider the problem of real-time remote monitoring of a two-state Markov process, where a sensor observes the state of the source and makes a decision on whether to transmit the status updates over an unreliable channel or not. We introduce a modified randomized stationary sampling and transmission policy where the decision to perform sampling occurs probabilistically depending on the current state of the source and whether the system was in a sync state during the previous time slot or not. We then propose two new performance metrics, coined the Version Innovation Age (VIA) and the Age of Incorrect Version (AoIV) and analyze their performance under the modified randomized stationary and other state-of-the-art sampling and transmission policies. Specifically, we derive closed-form expressions for the distribution and the average of VIA, AoIV, and Age of Incorrect Information (AoII) under these policies. Furthermore, we formulate and solve three constrained optimization problems. The first optimization problem aims to minimize the average VIA subject to constraints on the time-averaged sampling cost and time-averaged reconstruction error. In the second and third problems, the objective is to minimize the average AoIV and AoII, respectively, while considering a constraint on the time-averaged sampling cost. Finally, we compare the performance of various sampling and transmission policies and identify the conditions under which each policy outperforms the others in optimizing the proposed metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14594v1</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.IT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mehrdad Salimnejad, Marios Kountouris, Anthony Ephremides, Nikolaos Pappas</dc:creator>
    </item>
    <item>
      <title>Towards Timely Video Analytics Services at the Network Edge</title>
      <link>https://arxiv.org/abs/2406.14820</link>
      <description>arXiv:2406.14820v1 Announce Type: cross 
Abstract: Real-time video analytics services aim to provide users with accurate recognition results timely. However, existing studies usually fall into the dilemma between reducing delay and improving accuracy. The edge computing scenario imposes strict transmission and computation resource constraints, making balancing these conflicting metrics under dynamic network conditions difficult. In this regard, we introduce the age of processed information (AoPI) concept, which quantifies the time elapsed since the generation of the latest accurately recognized frame. AoPI depicts the integrated impact of recognition accuracy, transmission, and computation efficiency. We derive closed-form expressions for AoPI under preemptive and non-preemptive computation scheduling policies w.r.t. the transmission/computation rate and recognition accuracy of video frames. We then investigate the joint problem of edge server selection, video configuration adaptation, and bandwidth/computation resource allocation to minimize the long-term average AoPI over all cameras. We propose an online method, i.e., Lyapunov-based block coordinate descent (LBCD), to solve the problem, which decouples the original problem into two subproblems to optimize the video configuration/resource allocation and edge server selection strategy separately. We prove that LBCD achieves asymptotically optimal performance. According to the testbed experiments and simulation results, LBCD reduces the average AoPI by up to 10.94X compared to state-of-the-art baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14820v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TMC.2024.3376769</arxiv:DOI>
      <dc:creator>Xishuo Li, Shan Zhang, Yuejiao Huang, Xiao Ma, Zhiyuan Wang, Hongbin Luo</dc:creator>
    </item>
    <item>
      <title>Probabilistic and Differentiable Wireless Simulation with Geometric Transformers</title>
      <link>https://arxiv.org/abs/2406.14995</link>
      <description>arXiv:2406.14995v1 Announce Type: cross 
Abstract: Modelling the propagation of electromagnetic signals is critical for designing modern communication systems. While there are precise simulators based on ray tracing, they do not lend themselves to solving inverse problems or the integration in an automated design loop. We propose to address these challenges through differentiable neural surrogates that exploit the geometric aspects of the problem. We first introduce the Wireless Geometric Algebra Transformer (Wi-GATr), a generic backbone architecture for simulating wireless propagation in a 3D environment. It uses versatile representations based on geometric algebra and is equivariant with respect to E(3), the symmetry group of the underlying physics. Second, we study two algorithmic approaches to signal prediction and inverse problems based on differentiable predictive modelling and diffusion models. We show how these let us predict received power, localize receivers, and reconstruct the 3D environment from the received signal. Finally, we introduce two large, geometry-focused datasets of wireless signal propagation in indoor scenes. In experiments, we show that our geometry-forward approach achieves higher-fidelity predictions with less data than various baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14995v1</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Hehn, Markus Peschl, Tribhuvanesh Orekondy, Arash Behboodi, Johann Brehmer</dc:creator>
    </item>
    <item>
      <title>On the Computing and Communication Tradeoff in Reasoning-Based Multi-User Semantic Communications</title>
      <link>https://arxiv.org/abs/2406.15199</link>
      <description>arXiv:2406.15199v1 Announce Type: cross 
Abstract: Semantic communication (SC) is recognized as a promising approach for enabling reliable communication with minimal data transfer while maintaining seamless connectivity for a group of wireless users. Unlocking the advantages of SC for multi-user cases requires revisiting how communication and computing resources are allocated. This reassessment should consider the reasoning abilities of end-users, enabling receiving nodes to fill in missing information or anticipate future events more effectively. Yet, state-of-the-art SC systems primarily focus on resource allocation through compression based on semantic relevance, while overlooking the underlying data generation mechanisms and the tradeoff between communications and computing. Thus, they cannot help prevent a disruption in connectivity. In contrast, in this paper, a novel framework for computing and communication resource allocation is proposed that seeks to demonstrate how SC systems with reasoning capabilities at the end nodes can improve reliability in an end-to-end multi-user wireless system with intermittent communication links. Towards this end, a novel reasoning-aware SC system is proposed for enabling users to utilize their local computing resources to reason the representations when the communication links are unavailable. To optimize communication and computing resource allocation in this system, a noncooperative game is formulated among multiple users whose objective is to maximize the effective semantic information (computed as a product of reliability and semantic information) while controlling the number of semantically relevant links that are disrupted. Simulation results show that the proposed reasoning-aware SC system results in at least a $16.6\%$ enhancement in throughput and a significant improvement in reliability compared to classical communications systems that do not incorporate reasoning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15199v1</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nitisha Singh, Christo Kurisummoottil Thomas, Walid Saad, Emilio Calvanese Strinati</dc:creator>
    </item>
    <item>
      <title>Advancing Ultra-Reliable 6G: Transformer and Semantic Localization Empowered Robust Beamforming in Millimeter-Wave Communications</title>
      <link>https://arxiv.org/abs/2406.02000</link>
      <description>arXiv:2406.02000v2 Announce Type: replace 
Abstract: Advancements in 6G wireless technology have elevated the importance of beamforming, especially for attaining ultra-high data rates via millimeter-wave (mmWave) frequency deployment. Although promising, mmWave bands require substantial beam training to achieve precise beamforming. While initial deep learning models that use RGB camera images demonstrated promise in reducing beam training overhead, their performance suffers due to sensitivity to lighting and environmental variations. Due to this sensitivity, Quality of Service (QoS) fluctuates, eventually affecting the stability and dependability of networks in dynamic environments. This emphasizes a critical need for more robust solutions. This paper proposes a robust beamforming technique to ensure consistent QoS under varying environmental conditions. An optimization problem has been formulated to maximize users' data rates. To solve the formulated NP-hard optimization problem, we decompose it into two subproblems: the semantic localization problem and the optimal beam selection problem. To solve the semantic localization problem, we propose a novel method that leverages the k-means clustering and YOLOv8 model. To solve the beam selection problem, we propose a novel lightweight hybrid architecture that utilizes various data sources and a weighted entropy-based mechanism to predict the optimal beams. Rapid and accurate beam predictions are needed to maintain QoS. A novel metric, Accuracy-Complexity Efficiency (ACE), has been proposed to quantify this. Six testing scenarios have been developed to evaluate the robustness of the proposed model. Finally, the simulation result demonstrates that the proposed model outperforms several state-of-the-art baselines regarding beam prediction accuracy, received power, and ACE in the developed test scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02000v2</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Avi Deb Raha, Kitae Kim, Apurba Adhikary, Mrityunjoy Gain, Choong Seon Hong</dc:creator>
    </item>
    <item>
      <title>A Survey on Intelligent Internet of Things: Applications, Security, Privacy, and Future Directions</title>
      <link>https://arxiv.org/abs/2406.03820</link>
      <description>arXiv:2406.03820v2 Announce Type: replace 
Abstract: The rapid advances in the Internet of Things (IoT) have promoted a revolution in communication technology and offered various customer services. Artificial intelligence (AI) techniques have been exploited to facilitate IoT operations and maximize their potential in modern application scenarios. In particular, the convergence of IoT and AI has led to a new networking paradigm called Intelligent IoT (IIoT), which has the potential to significantly transform businesses and industrial domains. This paper presents a comprehensive survey of IIoT by investigating its significant applications in mobile networks, as well as its associated security and privacy issues. Specifically, we explore and discuss the roles of IIoT in a wide range of key application domains, from smart healthcare and smart cities to smart transportation and smart industries. Through such extensive discussions, we investigate important security issues in IIoT networks, where network attacks, confidentiality, integrity, and intrusion are analyzed, along with a discussion of potential countermeasures. Privacy issues in IIoT networks were also surveyed and discussed, including data, location, and model privacy leakage. Finally, we outline several key challenges and highlight potential research directions in this important area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03820v2</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ons Aouedi, Thai-Hoc Vu, Alessio Sacco, Dinh C. Nguyen, Kandaraj Piamrat, Guido Marchetto, Quoc-Viet Pham</dc:creator>
    </item>
    <item>
      <title>Count-Min sketches for Telemetry: analysis of performance in P4 implementations</title>
      <link>https://arxiv.org/abs/2406.12586</link>
      <description>arXiv:2406.12586v2 Announce Type: replace 
Abstract: Monitoring streams of packets at 100~Gb/s and beyond requires using compact and efficient hashing-techniques like HyperLogLog (HLL) or Count-Min Sketch (CMS). In this work, we evaluate the uses and applications of Count-Min Sketch for Metro Networks employing P4-based packet-optical nodes. We provide dimensioning rules for CMS at 100~Gb/s and 400~Gb/s and evaluate its performance in a real implementation testbed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12586v2</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>The 28th International Conference on Optical Network Design and Modelling (ONDM 2024)</arxiv:journal_reference>
      <dc:creator>Jos\'e A. Hern\'andez, Davide Scano, Filippo Cugini, Gonzalo Mart\'inez, Natalia Koneva, Alvaro S\'anchez-Maci\'an, \'Oscar Gonz\'alez de Dios</dc:creator>
    </item>
    <item>
      <title>On optimizing Inband Telemetry systems for accurate latency-based service deployments</title>
      <link>https://arxiv.org/abs/2406.12594</link>
      <description>arXiv:2406.12594v2 Announce Type: replace 
Abstract: The power of Machine Learning and Artificial Intelligence algorithms based on collected datasets, along with the programmability and flexibility provided by Software Defined Networking can provide the building blocks for constructing the so-called Zero-Touch Network and Service Management systems. However, the fuel towards this goal relies on the availability of sufficient and good-quality data collected from measurements and telemetry. This article provides a telemetry methodology to collect accurate latency measurements, as a first step toward building intelligent control planes that make correct decisions based on precise information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12594v2</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>The 28th International Conference on Optical Network Design and Modelling (ONDM 2024)</arxiv:journal_reference>
      <dc:creator>Nataliia Koneva, Alfonso S\'anchez-Maci\'an, Jos\'e Alberto Hern\'andez, \'Oscar Gonz\'alez de Dios</dc:creator>
    </item>
    <item>
      <title>Reinforcement-Learning based routing for packet-optical networks with hybrid telemetry</title>
      <link>https://arxiv.org/abs/2406.12602</link>
      <description>arXiv:2406.12602v2 Announce Type: replace 
Abstract: This article provides a methodology and open-source implementation of Reinforcement Learning algorithms for finding optimal routes in a packet-optical network scenario. The algorithm uses measurements provided by the physical layer (pre-FEC bit error rate and propagation delay) and the link layer (link load) to configure a set of latency-based rewards and penalties based on such measurements. Then, the algorithm executes Q-learning based on this set of rewards for finding the optimal routing strategies. It is further shown that the algorithm dynamically adapts to changing network conditions by re-calculating optimal policies upon either link load changes or link degradation as measured by pre-FEC BER.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12602v2</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>The 28th International Conference on Optical Network Design and Modelling (ONDM 2024)</arxiv:journal_reference>
      <dc:creator>A. L. Garc\'ia Navarro, Nataliia Koneva, Alfonso S\'anchez-Maci\'an, Jos\'e Alberto Hern\'andez, \'Oscar Gonz\'alez de Dios, J. M. Rivas-Moscoso</dc:creator>
    </item>
    <item>
      <title>Large Language Model-Driven Curriculum Design for Mobile Networks</title>
      <link>https://arxiv.org/abs/2405.18039</link>
      <description>arXiv:2405.18039v2 Announce Type: replace-cross 
Abstract: This study introduces an innovative framework that employs large language models (LLMs) to automate the design and generation of curricula for reinforcement learning (RL). As mobile networks evolve towards the 6G era, managing their increasing complexity and dynamic nature poses significant challenges. Conventional RL approaches often suffer from slow convergence and poor generalization due to conflicting objectives and the large state and action spaces associated with mobile networks. To address these shortcomings, we introduce curriculum learning, a method that systematically exposes the RL agent to progressively challenging tasks, improving convergence and generalization. However, curriculum design typically requires extensive domain knowledge and manual human effort. Our framework mitigates this by utilizing the generative capabilities of LLMs to automate the curriculum design process, significantly reducing human effort while improving the RL agent's convergence and performance. We deploy our approach within a simulated mobile network environment and demonstrate improved RL convergence rates, generalization to unseen scenarios, and overall performance enhancements. As a case study, we consider autonomous coordination and user association in mobile networks. Our obtained results highlight the potential of combining LLM-based curriculum generation with RL for managing next-generation wireless networks, marking a significant step towards fully autonomous network operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18039v2</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Omar Erak, Omar Alhussein, Shimaa Naser, Nouf Alabbasi, De Mi, Sami Muhaidat</dc:creator>
    </item>
  </channel>
</rss>
