<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 May 2025 04:00:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Monotone Bounded-Depth Complexity of Homomorphism Polynomials</title>
      <link>https://arxiv.org/abs/2505.22894</link>
      <description>arXiv:2505.22894v1 Announce Type: new 
Abstract: For every fixed graph $H$, it is known that homomorphism counts from $H$ and colorful $H$-subgraph counts can be determined in $O(n^{t+1})$ time on $n$-vertex input graphs $G$, where $t$ is the treewidth of $H$. On the other hand, a running time of $n^{o(t / \log t)}$ would refute the exponential-time hypothesis. Komarath, Pandey and Rahul (Algorithmica, 2023) studied algebraic variants of these counting problems, i.e., homomorphism and subgraph $\textit{polynomials}$ for fixed graphs $H$. These polynomials are weighted sums over the objects counted above, where each object is weighted by the product of variables corresponding to edges contained in the object. As shown by Komarath et al., the $\textit{monotone}$ circuit complexity of the homomorphism polynomial for $H$ is $\Theta(n^{\mathrm{tw}(H)+1})$.
  In this paper, we characterize the power of monotone $\textit{bounded-depth}$ circuits for homomorphism and colorful subgraph polynomials. This leads us to discover a natural hierarchy of graph parameters $\mathrm{tw}_\Delta(H)$, for fixed $\Delta \in \mathbb N$, which capture the width of tree-decompositions for $H$ when the underlying tree is required to have depth at most $\Delta$. We prove that monotone circuits of product-depth $\Delta$ computing the homomorphism polynomial for $H$ require size $\Theta(n^{\mathrm{tw}_\Delta(H^{\dagger})+1})$, where $H^{\dagger}$ is the graph obtained from $H$ by removing all degree-$1$ vertices. This allows us to derive an optimal depth hierarchy theorem for monotone bounded-depth circuits through graph-theoretic arguments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22894v1</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>C. S. Bhargav, Shiteng Chen, Radu Curticapean, Prateek Dwivedi</dc:creator>
    </item>
    <item>
      <title>Fast Compressed-Domain N-Point Discrete Fourier Transform</title>
      <link>https://arxiv.org/abs/2505.23718</link>
      <description>arXiv:2505.23718v1 Announce Type: new 
Abstract: This paper presents a novel algorithm for computing the N-point Discrete Fourier Transform (DFT) based solely on recursive Rectangular Index Compression (RIC) [1][2] and structured frequency shifts. The RIC DFT algorithm compresses a signal from $N=CL$ to $C\in[2,N/2]$ points at the expense of $N-1$ complex additions and no complex multiplication. It is shown that a $C$-point DFT on the compressed signal corresponds exactly to $C$ DFT coefficients of the original $N$-point DFT, namely, $X_{kL}$, $k=0,1,\ldots,C-1$ with no need for twiddle factors. We rely on this strategy to decompose the DFT by recursively compressing the input signal and applying global frequency shifts (to get odd-indexed DFT coefficients). We show that this new structure can relax the power-of-two assumption of the radix-2 FFT by enabling signal input lengths such as $N=c\cdot 2^k$ (for $k\geq 0$ and a non-power-of-two $c&gt;0$). Thus, our algorithm potentially outperforms radix-2 FFTs for the cases where significant zero-padding is needed. The proposed approach achieves a computational complexity of $O(N \log N)$ and offers a new structural perspective on DFT computation, with potential impacts on several DFT issues like numerical stability, hardware implementation, sparse transforms, convolutions, and others DFT-based procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23718v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>eess.SP</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saulo Queiroz</dc:creator>
    </item>
    <item>
      <title>Information-Computation Gaps in Quantum Learning via Low-Degree Likelihood</title>
      <link>https://arxiv.org/abs/2505.22743</link>
      <description>arXiv:2505.22743v1 Announce Type: cross 
Abstract: In a variety of physically relevant settings for learning from quantum data, designing protocols that can computationally efficiently extract information remains largely an art, and there are important cases where we believe this to be impossible, that is, where there is an information-computation gap. While there is a large array of tools in the classical literature for giving evidence for average-case hardness of statistical inference problems, the corresponding tools in the quantum literature are far more limited. One such framework in the classical literature, the low-degree method, makes predictions about hardness of inference problems based on the failure of estimators given by low-degree polynomials. In this work, we extend this framework to the quantum setting.
  We establish a general connection between state designs and low-degree hardness. We use this to obtain the first information-computation gaps for learning Gibbs states of random, sparse, non-local Hamiltonians. We also use it to prove hardness for learning random shallow quantum circuit states in a challenging model where states can be measured in adaptively chosen bases. To our knowledge, the ability to model adaptivity within the low-degree framework was open even in classical settings. In addition, we also obtain a low-degree hardness result for quantum error mitigation against strategies with single-qubit measurements.
  We define a new quantum generalization of the planted biclique problem and identify the threshold at which this problem becomes computationally hard for protocols that perform local measurements. Interestingly, the complexity landscape for this problem shifts when going from local measurements to more entangled single-copy measurements.
  We show average-case hardness for the "standard" variant of Learning Stabilizers with Noise and for agnostically learning product states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22743v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sitan Chen, Weiyuan Gong, Jonas Haferkamp, Yihui Quek</dc:creator>
    </item>
    <item>
      <title>Finding $d$-Cuts in Graphs of Bounded Diameter, Graphs of Bounded Radius and $H$-Free Graphs</title>
      <link>https://arxiv.org/abs/2404.11389</link>
      <description>arXiv:2404.11389v3 Announce Type: replace-cross 
Abstract: The $d$-Cut problem is to decide if a graph has an edge cut such that each vertex has at most $d$ neighbours at the opposite side of the cut. If $d=1$, we obtain the intensively studied Matching Cut problem. The $d$-Cut problem has been studied as well, but a systematic study for special graph classes was lacking. We initiate such a study and consider classes of bounded diameter, bounded radius and $H$-free graphs. We prove that for all $d\geq 2$, $d$-Cut is polynomial-time solvable for graphs of diameter $2$, $(P_3+P_4)$-free graphs and $P_5$-free graphs. These results extend known results for $d=1$. However, we also prove several NP-hardness results for $d$-Cut that contrast known polynomial-time results for $d=1$. Our results lead to full dichotomies for bounded diameter and bounded radius and to almost-complete dichotomies for $H$-free graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11389v3</guid>
      <category>math.CO</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felicia Lucke, Ali Momeni, Dani\"el Paulusma, Siani Smith</dc:creator>
    </item>
    <item>
      <title>Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives</title>
      <link>https://arxiv.org/abs/2502.04358</link>
      <description>arXiv:2502.04358v2 Announce Type: replace-cross 
Abstract: Decomposing hard problems into subproblems often makes them easier and more efficient to solve. With large language models (LLMs) crossing critical reliability thresholds for a growing slate of capabilities, there is an increasing effort to decompose systems into sets of LLM-based agents, each of whom can be delegated sub-tasks. However, this decomposition (even when automated) is often intuitive, e.g., based on how a human might assign roles to members of a human team. How close are these role decompositions to optimal? This position paper argues that asymptotic analysis with LLM primitives is needed to reason about the efficiency of such decomposed systems, and that insights from such analysis will unlock opportunities for scaling them. By treating the LLM forward pass as the atomic unit of computational cost, one can separate out the (often opaque) inner workings of a particular LLM from the inherent efficiency of how a set of LLMs are orchestrated to solve hard problems. In other words, if we want to scale the deployment of LLMs to the limit, instead of anthropomorphizing LLMs, asymptotic analysis with LLM primitives should be used to reason about and develop more powerful decompositions of large problems into LLM agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04358v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elliot Meyerson, Xin Qiu</dc:creator>
    </item>
    <item>
      <title>More-efficient Quantum Multivariate Mean Value Estimator from Generalized Grover Operator</title>
      <link>https://arxiv.org/abs/2504.06940</link>
      <description>arXiv:2504.06940v3 Announce Type: replace-cross 
Abstract: In this work, we present an efficient algorithm for multivariate mean value estimation. Our algorithm outperforms previous work by polylog factors and nearly saturates the known lower bound. More formally, given a random vector $\vec{X}$ of dimension $d$, we find an algorithm that uses $O\left(n \log \frac{d}{\delta}\right)$ samples to find a mean estimate that $\vec{\tilde{\mu}}$ that differs from the true mean $\vec{\mu}$ by $\frac{\sqrt{\text{tr } \Sigma}}{n}$ in $\ell^\infty$ norm and hence $\frac{\sqrt{d \text{ tr } \Sigma}}{n}$ in $\ell^2$ norm, where $\Sigma$ is the covariance matrix of the components of the random vector. We also presented another algorithm that uses smaller memory but costs an extra $d^\frac{1}{4}$ in complexity. Consider the Grover operator, the unitary operator used in Grover's algorithm. It contains an oracle that uses a $\pm 1$ phase for each candidate for the search space. Previous work has demonstrated that when we substitute the oracle in Grover operator with generic phases, it ended up being a good mean value estimator in some mathematical notion. We used this idea to build our algorithm. Our result remains not exactly optimal due to a $\log \frac{d}{\delta}$ term in our complexity, as opposed to something nicer such as $\log \frac{1}{\delta}$; This comes from the phase estimation primitive in our algorithm. So far, this primitive is the only major known method to tackle the problem, and moving beyond this idea seems hard. Our results demonstrates that the methodology with generalized Grover operator can be used develop the optimal algorithm without polylog overhead for different tasks relating to mean value estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06940v3</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Letian Tang</dc:creator>
    </item>
  </channel>
</rss>
