<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 13 Aug 2025 01:23:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>SSD Offloading for LLM Mixture-of-Experts Weights Considered Harmful in Energy Efficiency</title>
      <link>https://arxiv.org/abs/2508.06978</link>
      <description>arXiv:2508.06978v1 Announce Type: new 
Abstract: Large Language Models (LLMs) applying Mixture-of-Experts (MoE) scale to trillions of parameters but require vast memory, motivating a line of research to offload expert weights from fast-but-small DRAM (HBM) to denser Flash SSDs. While SSDs provide cost-effective capacity, their read energy per bit is substantially higher than that of DRAM. This paper quantitatively analyzes the energy implications of offloading MoE expert weights to SSDs during the critical decode stage of LLM inference. Our analysis, comparing SSD, CPU memory (DDR), and HBM storage scenarios for models like DeepSeek-R1, reveals that offloading MoE weights to current SSDs drastically increases per-token-generation energy consumption (e.g., by up to ~12x compared to the HBM baseline), dominating the total inference energy budget. Although techniques like prefetching effectively hide access latency, they cannot mitigate this fundamental energy penalty. We further explore future technological scaling, finding that the inherent sparsity of MoE models could potentially make SSDs energy-viable if Flash read energy improves significantly, roughly by an order of magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06978v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/LCA.2025.3592563</arxiv:DOI>
      <dc:creator>Kwanhee Kyung, Sungmin Yun, Jung Ho Ahn</dc:creator>
    </item>
    <item>
      <title>Physical Design Exploration of a Wire-Friendly Domain-Specific Processor for Angstrom-Era Nodes</title>
      <link>https://arxiv.org/abs/2508.07110</link>
      <description>arXiv:2508.07110v1 Announce Type: new 
Abstract: This paper presents the physical design exploration of a domain-specific processor (DSIP) architecture targeted at machine learning (ML), addressing the challenges of interconnect efficiency in advanced Angstrom-era technologies. The design emphasizes reduced wire length and high core density by utilizing specialized memory structures and SIMD (Single Instruction, Multiple Data) units. Five configurations are synthesized and evaluated using the IMEC A10 nanosheet node PDK. Key physical design metrics are compared across configurations and against VWR2A, a state-of-the-art (SoA) DSIP baseline. Results show that our architecture achieves over 2x lower normalized wire length and more than 3x higher density than the SoA, with low variability in the metrics across all configurations, making it a promising solution for next-generation DSIP designs. These improvements are achieved with minimal manual layout intervention, demonstrating the architecture's intrinsic physical efficiency and potential for low-cost wire-friendly implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07110v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Ruotolo, Lara Orlandic, Pengbo Yu, Moritz Brunion, Daniele Jahier Pagliari, Dwaipayan Biswas, Giovanni Ansaloni, David Atienza, Julien Ryckaert, Francky Catthoor, Yukai Chen</dc:creator>
    </item>
    <item>
      <title>LP-Spec: Leveraging LPDDR PIM for Efficient LLM Mobile Speculative Inference with Architecture-Dataflow Co-Optimization</title>
      <link>https://arxiv.org/abs/2508.07227</link>
      <description>arXiv:2508.07227v1 Announce Type: new 
Abstract: LLM inference on mobile devices faces extraneous challenges due to limited memory bandwidth and computational resources. To address these issues, speculative inference and processing-in-memory (PIM) techniques have been explored at the algorithmic and hardware levels. However, speculative inference results in more compute-intensive GEMM operations, creating new design trade-offs for existing GEMV-accelerated PIM architectures. Furthermore, there exists a significant amount of redundant draft tokens in tree-based speculative inference, necessitating efficient token management schemes to minimize energy consumption. In this work, we present LP-Spec, an architecture-dataflow co-design leveraging hybrid LPDDR5 performance-enhanced PIM architecture with draft token pruning and dynamic workload scheduling to accelerate LLM speculative inference. A near-data memory controller is proposed to enable data reallocation between DRAM and PIM banks. Furthermore, a data allocation unit based on the hardware-aware draft token pruner is developed to minimize energy consumption and fully exploit parallel execution opportunities. Compared to end-to-end LLM inference on other mobile solutions such as mobile NPUs or GEMV-accelerated PIMs, our LP-Spec achieves 13.21x, 7.56x, and 99.87x improvements in performance, energy efficiency, and energy-delay-product (EDP). Compared with prior AttAcc PIM and RTX 3090 GPU, LP-Spec can obtain 12.83x and 415.31x EDP reduction benefits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07227v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyuan He, Zhantong Zhu, Yandong He, Tianyu Jia</dc:creator>
    </item>
    <item>
      <title>Tasa: Thermal-aware 3D-Stacked Architecture Design with Bandwidth Sharing for LLM Inference</title>
      <link>https://arxiv.org/abs/2508.07252</link>
      <description>arXiv:2508.07252v1 Announce Type: new 
Abstract: The autoregressive decoding in LLMs is the major inference bottleneck due to the memory-intensive operations and limited hardware bandwidth. 3D-stacked architecture is a promising solution with significantly improved memory bandwidth, which vertically stacked multi DRAM dies on top of logic die. However, our experiments also show the 3D-stacked architecture faces severer thermal issues compared to 2D architecture, in terms of thermal temperature, gradient and scalability. To better exploit the potential of 3D-stacked architecture, we present Tasa, a heterogeneous architecture with cross-stack thermal optimizations to balance the temperature distribution and maximize the performance under the thermal constraints. High-performance core is designed for compute-intensive operations, while high-efficiency core is used for memory-intensive operators, e.g. attention layers. Furthermore, we propose a bandwidth sharing scheduling to improve the bandwidth utilization in such heterogeneous architecture. Extensive thermal experiments show that our Tasa architecture demonstrates greater scalability compared with the homogeneous 3D-stacked architecture, i.e. up to 5.55 $\tccentigrade$, 9.37 $\tccentigrade$, and 7.91 $\tccentigrade$ peak temperature reduction for 48, 60, and 72 core configurations. Our experimental for Llama-65B and GPT-3 66B inferences also demonstrate 2.85x and 2.21x speedup are obtained over the GPU baselines and state-of-the-art heterogeneous PIM-based LLM accelerator</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07252v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyuan He, Peiran Yan, Yandong He, Youwei Zhuo, Tianyu Jia</dc:creator>
    </item>
    <item>
      <title>The Monte Carlo Method and New Device and Architectural Techniques for Accelerating It</title>
      <link>https://arxiv.org/abs/2508.07457</link>
      <description>arXiv:2508.07457v1 Announce Type: new 
Abstract: Computing systems interacting with real-world processes must safely and reliably process uncertain data. The Monte Carlo method is a popular approach for computing with such uncertain values. This article introduces a framework for describing the Monte Carlo method and highlights two advances in the domain of physics-based non-uniform random variate generators (PPRVGs) to overcome common limitations of traditional Monte Carlo sampling. This article also highlights recent advances in architectural techniques that eliminate the need to use the Monte Carlo method by leveraging distributional microarchitectural state to natively compute on probability distributions. Unlike Monte Carlo methods, uncertainty-tracking processor architectures can be said to be convergence-oblivious.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07457v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Janith Petangoda, Chatura Samarakoon, James Meech, Divya Thekke Kanapram, Hamid Toshani, Nathaniel Tye, Vasileios Tsoutsouras, Phillip Stanley-Marbell</dc:creator>
    </item>
    <item>
      <title>A Matrix Decomposition Method for Odd-Type Gaussian Normal Basis Multiplication</title>
      <link>https://arxiv.org/abs/2508.07541</link>
      <description>arXiv:2508.07541v1 Announce Type: new 
Abstract: Normal basis is used in many applications because of the efficiency of the implementation. However, most space complexity reduction techniques for binary field multiplier are applicable for only optimal normal basis or Gaussian normal basis of even type. There are 187 binary fields GF(2^k) for k from 2 to 1,000 that use odd-type Gaussian normal basis. This paper presents a method to reduce the space complexity of odd-type Gaussian normal basis multipliers over binary field GF(2^k). The idea is adapted from the matrix decomposition method for optimal normal basis. The result shows that our space complexity reduction method can reduce the number of XOR gates used in the implementation comparing to previous works with a small trade-off in critical path delay.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07541v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ccoms.2018.8463251</arxiv:DOI>
      <arxiv:journal_reference>3rd International Conference on Computer and Communication Systems. ICCCS 2018. IEEE</arxiv:journal_reference>
      <dc:creator>Kittiphon Phalakarn, Athasit Surarerks</dc:creator>
    </item>
    <item>
      <title>ARISE: Automating RISC-V Instruction Set Extension</title>
      <link>https://arxiv.org/abs/2508.07725</link>
      <description>arXiv:2508.07725v1 Announce Type: new 
Abstract: RISC-V is an extendable Instruction Set Architecture, growing in popularity for embedded systems. However, optimizing it to specific requirements, imposes a great deal of manual effort. To bridge the gap between software and ISA, the tool ARISE is presented. It automates the generation of RISC-V instructions based on assembly patterns, which are selected by an extendable set of metrics. These metrics implement the optimization goals of code size and instruction count reduction, both statically and dynamically. The instruction set extensions are generated using the ISA description language CoreDSL. Allowing seamless embedding in advanced tools such as the retargeting compiler Seal5 or the instruction set simulator ETISS. ARISE improves the static code size by 1.48% and the dynamic code size by 3.84%, as well as the number of instructions to be executed by 7.39% on average for Embench-Iot.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07725v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andreas Hager-Clukas, Philipp van Kempen, Stefan Wallentowitz</dc:creator>
    </item>
    <item>
      <title>TLV-HGNN: Thinking Like a Vertex for Memory-efficient HGNN Inference</title>
      <link>https://arxiv.org/abs/2508.07796</link>
      <description>arXiv:2508.07796v1 Announce Type: new 
Abstract: Heterogeneous graph neural networks (HGNNs) excel at processing heterogeneous graph data and are widely applied in critical domains. In HGNN inference, the neighbor aggregation stage is the primary performance determinant, yet it suffers from two major sources of memory inefficiency. First, the commonly adopted per-semantic execution paradigm stores intermediate aggregation results for each semantic prior to semantic fusion, causing substantial memory expansion. Second, the aggregation process incurs extensive redundant memory accesses, including repeated loading of target vertex features across semantics and repeated accesses to shared neighbors due to cross-semantic neighborhood overlap. These inefficiencies severely limit scalability and reduce HGNN inference performance.
  In this work, we first propose a semantics-complete execution paradigm from a vertex perspective that eliminates per-semantic intermediate storage and redundant target vertex accesses. Building on this paradigm, we design TVL-HGNN, a reconfigurable hardware accelerator optimized for efficient aggregation. In addition, we introduce a vertex grouping technique based on cross-semantic neighborhood overlap, with hardware implementation, to reduce redundant accesses to shared neighbors. Experimental results demonstrate that TVL-HGNN achieves average speedups of 7.85x and 1.41x over the NVIDIA A100 GPU and the state-of-the-art HGNN accelerator HiHGNN, respectively, while reducing energy consumption by 98.79% and 32.61%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07796v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dengke Han, Duo Wang, Mingyu Yan, Xiaochun Ye, Dongrui Fan</dc:creator>
    </item>
    <item>
      <title>PiKV: KV Cache Management System for Mixture of Experts</title>
      <link>https://arxiv.org/abs/2508.06526</link>
      <description>arXiv:2508.06526v1 Announce Type: cross 
Abstract: As large language models continue to scale up in both size and context length, the memory and communication cost of key-value (KV) cache storage has become a major bottleneck in multi-GPU and multi-node inference. While MoE-based architectures sparsify computation across experts, the corresponding KV caches remain dense and globally synchronized, resulting in significant overhead.
  We introduce \textbf{PiKV}, a parallel and distributed KV cache serving framework tailored for MoE architecture. PiKV leverages \textit{expert-sharded KV storage} to partition caches across GPUs, \textit{PiKV routing} to reduce token-to-KV access, and a \textit{PiKV Scheduling} to adaptively retain query-relevant entries. To further reduce memory usage, PiKV integrates \textit{PiKV Compression} modules the caching pipeline for acceleration.
  PiKV is recently publicly available as an open-source software library: \href{https://github.com/NoakLiu/PiKV}{https://github.com/NoakLiu/PiKV}. Experiments details is recorded at: \href{https://github.com/NoakLiu/PiKV/blob/main/downstream_tasks/README.md}{https://github.com/NoakLiu/PiKV/Experimental\_Results}. We also have PiKV integrated with Nvidia kvpress for acceleration, details see \href{https://github.com/NoakLiu/PiKVpress}{https://github.com/NoakLiu/PiKVpress}. PiKV is still a living project, aiming to become a comprehesive KV Cache management system for MoE Architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06526v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dong Liu, Yanxuan Yu, Ben Lengerich, Ying Nian Wu, Xuhong Wang</dc:creator>
    </item>
    <item>
      <title>ELF: Efficient Logic Synthesis by Pruning Redundancy in Refactoring</title>
      <link>https://arxiv.org/abs/2508.08073</link>
      <description>arXiv:2508.08073v1 Announce Type: cross 
Abstract: In electronic design automation, logic optimization operators play a crucial role in minimizing the gate count of logic circuits. However, their computation demands are high. Operators such as refactor conventionally form iterative cuts for each node, striving for a more compact representation - a task which often fails 98% on average. Prior research has sought to mitigate computational cost through parallelization. In contrast, our approach leverages a classifier to prune unsuccessful cuts preemptively, thus eliminating unnecessary resynthesis operations. Experiments on the refactor operator using the EPFL benchmark suite and 10 large industrial designs demonstrate that this technique can speedup logic optimization by 3.9x on average compared with the state-of-the-art ABC implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08073v1</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dimitris Tsaras, Xing Li, Lei Chen, Zhiyao Xie, Mingxuan Yuan</dc:creator>
    </item>
    <item>
      <title>SSM-RDU: A Reconfigurable Dataflow Unit for Long-Sequence State-Space Models</title>
      <link>https://arxiv.org/abs/2503.22937</link>
      <description>arXiv:2503.22937v2 Announce Type: replace 
Abstract: Long-sequence state-space models (SSMs) such as Hyena and Mamba replace the quadratic complexity of self-attention with more efficient FFT and scan operations. However, modern accelerators like GPUs are poorly suited to these non-GEMM workloads due to rigid execution models and specialization for dense matrix operations. This paper proposes architectural extensions to a baseline Reconfigurable Dataflow Unit (RDU) that efficiently support FFT-based and scan-based SSMs. By introducing lightweight interconnect enhancements within compute tiles, the extended RDU enables spatial mapping of FFT and scan dataflows with less than 1% area and power overhead. The resulting architecture achieves a 5.95X speedup over the GPU and a 1.95X speedup over the baseline RDU for Hyena, and a 2.12X and 1.75X speedup over the GPU and baseline RDU, respectively, for Mamba.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22937v2</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sho Ko, Kunle Olukotun</dc:creator>
    </item>
    <item>
      <title>SystolicAttention: Fusing FlashAttention within a Single Systolic Array</title>
      <link>https://arxiv.org/abs/2507.11331</link>
      <description>arXiv:2507.11331v3 Announce Type: replace 
Abstract: Transformer models rely heavily on scaled dot-product attention (SDPA), typically implemented using the FlashAttention algorithm. However, current systolic-array-based accelerators face significant challenges when executing FlashAttention. Systolic arrays achieve high utilization primarily for consecutive and large matrix multiplications, whereas FlashAttention requires frequent interleaving of matrix multiplications and softmax operations.
  The frequent data swaps between matrix multiplications on the systolic array and softmax operations on external units result in low array utilization. Moreover, when these computations run concurrently, the softmax stage contends with matrix multiplication for register file and SRAM ports, further degrading performance.
  To overcome these limitations, we propose FSA, an enhanced systolic array architecture that enables the FlashAttention algorithm to run entirely within a single systolic array, eliminating the need for external vector units. At the core of FSA is SystolicAttention, a novel scheduling algorithm that maps FlashAttention operations onto systolic arrays with fine-grained, element-wise overlap. This approach significantly improves array utilization while preserving the original floating-point operation order to maintain numerical stability.
  We implement FSA in synthesizable RTL and evaluate its performance against state-of-the-art commercial accelerators. Our results show that FSA achieves 1.77 and 4.83 times higher attention FLOPs/s utilization compared to AWS Neuron v2 and Google TPUv5e, respectively, with only 12% area overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11331v3</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawei Lin, Guokai Chen, Yuanlong Li, Thomas Bourgeat</dc:creator>
    </item>
    <item>
      <title>ChipletPart: Cost-Aware Partitioning for 2.5D Systems</title>
      <link>https://arxiv.org/abs/2507.19819</link>
      <description>arXiv:2507.19819v2 Announce Type: replace 
Abstract: Industry adoption of chiplets has been increasing as a cost-effective option for making larger high-performance systems. Consequently, partitioning large systems into chiplets is increasingly important. In this work, we introduce ChipletPart - a cost-driven 2.5D system partitioner that addresses the unique constraints of chiplet systems, including complex objective functions, limited reach of inter-chiplet I/O transceivers, and the assignment of heterogeneous manufacturing technologies to different chiplets. ChipletPart integrates a sophisticated chiplet cost model with its underlying genetic algorithm-based technology assignment and partitioning methodology, along with a simulated annealing-based chiplet floorplanner. Our results show that: (i) ChipletPart reduces chiplet cost by up to 58% (20% geometric mean) compared to state-of-the-art min-cut partitioners, which often yield floorplan-infeasible solutions; (ii) ChipletPart generates partitions with up to 47% (6% geometric mean) lower cost as compared to the prior work Floorplet; and (iii) for the testcases we study, heterogeneous integration reduces cost by up to 43% (15% geometric mean) compared to homogeneous implementations. Additionally, we explore Bayesian optimization (BO) for finding low cost and floorplan-feasible chiplet solutions with technology assignments. On some testcases, our BO framework achieves better system cost (up to 5.3% improvement) with higher runtime overhead (up to 4x) compared to our GA framework. We also present case studies that show how changes in packaging and inter-chiplet signaling technologies can affect partitioning solutions. Finally, we make ChipletPart, the underlying chiplet cost model, and a chiplet testcase generator available as open-source tools for the community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19819v2</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Graening, Puneet Gupta, Andrew B. Kahng, Bodhisatta Pramanik, Zhiang Wang</dc:creator>
    </item>
    <item>
      <title>DGEMM without FP64 Arithmetic -- Using FP64 Emulation and FP8 Tensor Cores with Ozaki Scheme</title>
      <link>https://arxiv.org/abs/2508.00441</link>
      <description>arXiv:2508.00441v2 Announce Type: replace-cross 
Abstract: As the demand for AI computations rapidly increases, more hardware is being developed to efficiently perform the low-precision matrix multiplications required by such workloads. However, these operations are generally not directly applicable to scientific computations due to accuracy requirements. The Ozaki scheme -- an accurate matrix multiplication method proposed by Ozaki et al. in 2012 -- enables FP64 matrix multiplication (DGEMM) using low-precision matrix multiplication units, such as FP16 Tensor Cores. This approach has since been extended to utilize integer arithmetic, offering lower computational cost compared to floating-point-based implementations. In fact, it has achieved higher performance than hardware FP64 operations on GPUs equipped with fast INT8 Tensor Cores designed for AI workloads. However, recent hardware trends have shifted toward improving the performance of low-precision floating-point operations, such as FP8, rather than integer operations. Motivated by this shift, this study revisits the use of low-precision floating-point operations in the Ozaki scheme. Specifically, we explore the use of FP8 Tensor Cores to perform DGEMM. In addition, for processors that support very slow or no FP64 operations, we also consider FP64 emulation based on integer arithmetic. Furthermore, we explore the use of blocking in the inner-product direction to accelerate FP16-based implementations. We demonstrate the effectiveness of these methods by evaluating the performance on an NVIDIA Blackwell architecture GPU.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00441v2</guid>
      <category>cs.PF</category>
      <category>cs.AR</category>
      <category>cs.MS</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daichi Mukunoki</dc:creator>
    </item>
  </channel>
</rss>
