<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.RO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.RO</link>
    <description>cs.RO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.RO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Jul 2024 01:37:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 24 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Memory Management for Real-Time Appearance-Based Loop Closure Detection</title>
      <link>https://arxiv.org/abs/2407.15890</link>
      <description>arXiv:2407.15890v1 Announce Type: new 
Abstract: Loop closure detection is the process involved when trying to find a match between the current and a previously visited locations in SLAM. Over time, the amount of time required to process new observations increases with the size of the internal map, which may influence real-time processing. In this paper, we present a novel real-time loop closure detection approach for large-scale and long-term SLAM. Our approach is based on a memory management method that keeps computation time for each new observation under a fixed limit. Results demonstrate the approach's adaptability and scalability using four standard data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15890v1</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/IROS.2011.6094602</arxiv:DOI>
      <arxiv:journal_reference>IEEE/RSJ International Conference on Intelligent Robots and Systems, San Francisco, CA, USA, 2011, pp. 1271-1276</arxiv:journal_reference>
      <dc:creator>Mathieu Labb\'e, Fran\c{c}ois Michaud</dc:creator>
    </item>
    <item>
      <title>On Flange-based 3D Hand-Eye Calibration for Soft Robotic Tactile Welding</title>
      <link>https://arxiv.org/abs/2407.16041</link>
      <description>arXiv:2407.16041v1 Announce Type: new 
Abstract: This paper investigates the direct application of standardized designs on the robot for conducting robot hand-eye calibration by employing 3D scanners with collaborative robots. The well-established geometric features of the robot flange are exploited by directly capturing its point cloud data. In particular, an iterative method is proposed to facilitate point cloud processing toward a refined calibration outcome. Several extensive experiments are conducted over a range of collaborative robots, including Universal Robots UR5 &amp; UR10 e-series, Franka Emika, and AUBO i5 using an industrial-grade 3D scanner Photoneo Phoxi S &amp; M and a commercial-grade 3D scanner Microsoft Azure Kinect DK. Experimental results show that translational and rotational errors converge efficiently to less than 0.28 mm and 0.25 degrees, respectively, achieving a hand-eye calibration accuracy as high as the camera's resolution, probing the hardware limit. A welding seam tracking system is presented, combining the flange-based calibration method with soft tactile sensing. The experiment results show that the system enables the robot to adjust its motion in real-time, ensuring consistent weld quality and paving the way for more efficient and adaptable manufacturing processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16041v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xudong Han, Ning Guo, Yu Jie, He Wang, Fang Wan, Chaoyang Song</dc:creator>
    </item>
    <item>
      <title>Development of Tendon-Driven Compliant Snake Robot with Global Bending and Twisting Actuation</title>
      <link>https://arxiv.org/abs/2407.16054</link>
      <description>arXiv:2407.16054v1 Announce Type: new 
Abstract: Snake robots have been studied for decades with the aim of achieving biological snakes' fluent locomotion. Yet, as of today, their locomotion remains far from that of the biological snakes. Our recent study suggested that snake locomotion utilizing partial ground contacts can be achieved with robots by using body compliance and lengthwise-globally applied body tensions. In this paper, we present the first hardware implementation of this locomotion principle. Our snake robot comprises serial tendon-driven continuum sections and is bent and twisted globally using tendons. We demonstrate how the tendons are actuated to achieve the ground contacts for forward and backward locomotion and sidewinding. The robot's capability to generate snake locomotion in various directions and its steerability were validated in a series of indoor experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16054v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seongil Kwon, Serdar Incekara, Gangil Kwon, Junhyoung Ha</dc:creator>
    </item>
    <item>
      <title>PECAN: Personalizing Robot Behaviors through a Learned Canonical Space</title>
      <link>https://arxiv.org/abs/2407.16081</link>
      <description>arXiv:2407.16081v1 Announce Type: new 
Abstract: Robots should personalize how they perform tasks to match the needs of individual human users. Today's robot achieve this personalization by asking for the human's feedback in the task space. For example, an autonomous car might show the human two different ways to decelerate at stoplights, and ask the human which of these motions they prefer. This current approach to personalization is indirect: based on the behaviors the human selects (e.g., decelerating slowly), the robot tries to infer their underlying preference (e.g., defensive driving). By contrast, our paper develops a learning and interface-based approach that enables humans to directly indicate their desired style. We do this by learning an abstract, low-dimensional, and continuous canonical space from human demonstration data. Each point in the canonical space corresponds to a different style (e.g., defensive or aggressive driving), and users can directly personalize the robot's behavior by simply clicking on a point. Given the human's selection, the robot then decodes this canonical style across each task in the dataset -- e.g., if the human selects a defensive style, the autonomous car personalizes its behavior to drive defensively when decelerating, passing other cars, or merging onto highways. We refer to our resulting approach as PECAN: Personalizing Robot Behaviors through a Learned Canonical Space. Our simulations and user studies suggest that humans prefer using PECAN to directly personalize robot behavior (particularly when those users become familiar with PECAN), and that users find the learned canonical space to be intuitive and consistent. See videos here: https://youtu.be/wRJpyr23PKI</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16081v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heramb Nemlekar, Robert Ramirez Sanchez, Dylan P. Losey</dc:creator>
    </item>
    <item>
      <title>Gel-OPTOFORT Sensor: Multi-axis Force/Torque Measurement and Geometry Observation Using GelSight and Optoelectronic Sensor Technology</title>
      <link>https://arxiv.org/abs/2407.16082</link>
      <description>arXiv:2407.16082v1 Announce Type: new 
Abstract: Although conventional GelSight-based tactile and force/torque sensors excel in detecting objects' geometry and texture information while simultaneously sensing multi-axis forces, their performance is limited by the camera's lower frame rates and the inherent properties of the elastomer. These limitations restrict their ability to measure higher force ranges at high sampling frequencies. Besides, due to the coupling of the Gelsight sensor unit and multi-axis force/torque unit structurally, the force/torque measurement ranges of the Gelsight-based force/torque sensors are not adjustable. To address these weaknesses, this paper proposes the GEL-OPTOFORT sensor that combines a GelSight sensor and an optoelectronic sensor-based force/torque sensor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16082v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yohan Noh, Harshal Upare, Dalia Osman, Wanlin Li</dc:creator>
    </item>
    <item>
      <title>Miniature Fibre-Optic based Shape Sensing for Robotic Applications using Curved Reflectors</title>
      <link>https://arxiv.org/abs/2407.16085</link>
      <description>arXiv:2407.16085v1 Announce Type: new 
Abstract: continuum-robots, prosthetic devices and wearable body-shape sensors. A miniaturised one-degree-of-freedom joint-angle sensor is devised, using a single light emitting/receiving optical fibre with a coupler connected to a Keyence (FS-N11MN, Osaka, Japan) sensor that supplies and detects light through the optical fibres. A curvature-varying reflective surface integrated in the joint demonstrates non-contact light intensity-based sensing. Various reflector geometries and surface colours are designed to compare sensor output for achieving a large angle range and improved sensitivity for the proposed miniaturised robotic shape-sensing applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16085v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dalia Osman, Vignesh Vignesh, Yohan Noh</dc:creator>
    </item>
    <item>
      <title>Robotically adjustable kinematics in a wrist-driven orthosis eases grasping across tasks</title>
      <link>https://arxiv.org/abs/2407.16095</link>
      <description>arXiv:2407.16095v1 Announce Type: new 
Abstract: Without finger function, people with C5-7 spinal cord injury (SCI) regularly utilize wrist extension to passively close the fingers and thumb together for grasping. Wearable assistive grasping devices often focus on this familiar wrist-driven technique to provide additional support and amplify grasp force. Despite recent research advances in modernizing these tools, people with SCI often abandon such wearable assistive devices in the long term. We suspect that the wrist constraints imposed by such devices generate undesirable reach and grasp kinematics. Here we show that using continuous robotic motor assistance to give users more adaptability in their wrist posture prior to wrist-driven grasping reduces task difficulty and perceived exertion. Our results demonstrate that more free wrist mobility allows users to select comfortable and natural postures depending on task needs, which improves the versatility of the assistive grasping device for easier use across different hand poses in the arm's workspace. This behavior holds the potential to improve ease of use and desirability of future device designs through new modes of combining both body-power and robotic automation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16095v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erin Y. Chang, Andrew I. W. McPherson, Hannah S. Stuart</dc:creator>
    </item>
    <item>
      <title>Optimal camera-robot pose estimation in linear time from points and lines</title>
      <link>https://arxiv.org/abs/2407.16151</link>
      <description>arXiv:2407.16151v1 Announce Type: new 
Abstract: Camera pose estimation is a fundamental problem in robotics. This paper focuses on two issues of interest: First, point and line features have complementary advantages, and it is of great value to design a uniform algorithm that can fuse them effectively; Second, with the development of modern front-end techniques, a large number of features can exist in a single image, which presents a potential for highly accurate robot pose estimation. With these observations, we propose AOPnP(L), an optimal linear-time camera-robot pose estimation algorithm from points and lines. Specifically, we represent a line with two distinct points on it and unify the noise model for point and line measurements where noises are added to 2D points in the image. By utilizing Plucker coordinates for line parameterization, we formulate a maximum likelihood (ML) problem for combined point and line measurements. To optimally solve the ML problem, AOPnP(L) adopts a two-step estimation scheme. In the first step, a consistent estimate that can converge to the true pose is devised by virtue of bias elimination. In the second step, a single Gauss-Newton iteration is executed to refine the initial estimate. AOPnP(L) features theoretical optimality in the sense that its mean squared error converges to the Cramer-Rao lower bound. Moreover, it owns a linear time complexity. These properties make it well-suited for precision-demanding and real-time robot pose estimation. Extensive experiments are conducted to validate our theoretical developments and demonstrate the superiority of AOPnP(L) in both static localization and dynamic odometry systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16151v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guangyang Zeng, Biqiang Mu, Qingcheng Zeng, Yuchen Song, Chulin Dai, Guodong Shi, Junfeng Wu</dc:creator>
    </item>
    <item>
      <title>Plant robots</title>
      <link>https://arxiv.org/abs/2407.16162</link>
      <description>arXiv:2407.16162v1 Announce Type: new 
Abstract: Plants display physical displacements during their growth due to photosynthesis, which converts light into chemical energy. This can be interpreted as plants acting as actuators with a built-in power source. This paper presents a method to create plant robots that move and perform tasks by harnessing the actuation output of plants: displacement and force generated from the growing process. As the target plant, radish sprouts are employed, and their displacement and force are characterized, followed by the calculation of power and energy densities. Based on the characterization, two different plant robots are designed and fabricated: a rotational robot and a gripper. The former demonstrates ground locomotion, achieving a travel distance of 14.6 mm with an average speed of 0.8 mm/h. The latter demonstrates the picking and placing of an object with a 0.1-g mass by the light-controlled open-close motion of plant fingers. A good agreement between the experimental and model values is observed in the specific data of the mobile robot, suggesting that obtaining the actuation characteristics of plants can enable the design and prediction of behavior in plant robots. These results pave the way for the realization of novel types of environmentally friendly and sustainable robots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16162v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kazuya Murakami, Misao Sato, Momoki Kubota, Jun Shintake</dc:creator>
    </item>
    <item>
      <title>Consideration of Vehicle Characteristics on the Motion Planner Algorithm</title>
      <link>https://arxiv.org/abs/2407.16167</link>
      <description>arXiv:2407.16167v1 Announce Type: new 
Abstract: Autonomous vehicle control is generally divided in two main areas; trajectory planning and tracking. Currently, the trajectory planning is mostly done by particle or kinematic model-based optimization controllers. The output of these planners, since they do not consider CG height and its effects, is not unique for different vehicle types, especially for high CG vehicles. As a result, the tracking controller may have to work hard to avoid vehicle handling and comfort constraints while trying to realize these sub-optimal trajectories. This paper tries to address this problem by considering a planner with simplified double track model with estimation of lateral and roll based load transfer using steady state equations and a simplified tire model to reduce solver workload. The developed planner is compared with the widely used particle and kinematic model planners in collision avoidance scenarios in both high and low acceleration conditions and with different vehicle heights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16167v1</guid>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Syed Adil Ahmed, Taehyun Shim</dc:creator>
    </item>
    <item>
      <title>Automatic Environment Shaping is the Next Frontier in RL</title>
      <link>https://arxiv.org/abs/2407.16186</link>
      <description>arXiv:2407.16186v1 Announce Type: new 
Abstract: Many roboticists dream of presenting a robot with a task in the evening and returning the next morning to find the robot capable of solving the task. What is preventing us from achieving this? Sim-to-real reinforcement learning (RL) has achieved impressive performance on challenging robotics tasks, but requires substantial human effort to set up the task in a way that is amenable to RL. It's our position that algorithmic improvements in policy optimization and other ideas should be guided towards resolving the primary bottleneck of shaping the training environment, i.e., designing observations, actions, rewards and simulation dynamics. Most practitioners don't tune the RL algorithm, but other environment parameters to obtain a desirable controller. We posit that scaling RL to diverse robotic tasks will only be achieved if the community focuses on automating environment shaping procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16186v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Younghyo Park, Gabriel B. Margolis, Pulkit Agrawal</dc:creator>
    </item>
    <item>
      <title>Probabilistic Parameter Estimators and Calibration Metrics for Pose Estimation from Image Features</title>
      <link>https://arxiv.org/abs/2407.16223</link>
      <description>arXiv:2407.16223v1 Announce Type: new 
Abstract: This paper addresses the challenge of probabilistic parameter estimation given measurement uncertainty in real-time. We provide a general formulation and apply this to pose estimation for an autonomous visual landing system. We present three probabilistic parameter estimators: a least-squares sampling approach, a linear approximation method, and a probabilistic programming estimator. To evaluate these estimators, we introduce novel closed-form expressions for measuring calibration and sharpness specifically for multivariate normal distributions. Our experimental study compares the three estimators under various noise conditions. We demonstrate that the linear approximation estimator can produce sharp and well-calibrated pose predictions significantly faster than the other methods but may yield overconfident predictions in certain scenarios. Additionally, we demonstrate that these estimators can be integrated with a Kalman filter for continuous pose estimation during a runway approach where we observe a 50\% improvement in sharpness while maintaining marginal calibration. This work contributes to the integration of data-driven computer vision models into complex safety-critical aircraft systems and provides a foundation for developing rigorous certification guidelines for such systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16223v1</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Romeo Valentin, Sydney M. Katz, Joonghyun Lee, Don Walker, Matthew Sorgenfrei, Mykel J. Kochenderfer</dc:creator>
    </item>
    <item>
      <title>Negotiating Control: Neurosymbolic Variable Autonomy</title>
      <link>https://arxiv.org/abs/2407.16254</link>
      <description>arXiv:2407.16254v1 Announce Type: new 
Abstract: Variable autonomy equips a system, such as a robot, with mixed initiatives such that it can adjust its independence level based on the task's complexity and the surrounding environment. Variable autonomy solves two main problems in robotic planning: the first is the problem of humans being unable to keep focus in monitoring and intervening during robotic tasks without appropriate human factor indicators, and the second is achieving mission success in unforeseen and uncertain environments in the face of static reward structures. An open problem in variable autonomy is developing robust methods to dynamically balance autonomy and human intervention in real-time, ensuring optimal performance and safety in unpredictable and evolving environments. We posit that addressing unpredictable and evolving environments through an addition of rule-based symbolic logic has the potential to make autonomy adjustments more contextually reliable and adding feedback to reinforcement learning through data from mixed-initiative control further increases efficacy and safety of autonomous behaviour.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16254v1</guid>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Georgios Bakirtzis, Manolis Chiou, Andreas Theodorou</dc:creator>
    </item>
    <item>
      <title>Continuous-Time Robust Control for Cancer Treatment Robots</title>
      <link>https://arxiv.org/abs/2407.16276</link>
      <description>arXiv:2407.16276v1 Announce Type: new 
Abstract: The control system in surgical robots must ensure patient safety and real time control. As such, all the uncertainties which could appear should be considered into an extended model of the plant. After such an uncertain plant is formed, an adequate controller which ensures a minimum set of performances for each situation should be computed. As such, the continuous-time robust control paradigm is suitable for such scenarios. However, the problem is generally solved only for linear and time invariant plants. The main focus of the current paper is to include m-link serial surgical robots into Robust Control Framework by considering all nonlinearities as uncertainties. Moreover, the paper studies an incipient problem of numerical implementation of such control structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16276v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-59257-7_19</arxiv:DOI>
      <arxiv:journal_reference>Advances in Service and Industrial Robotics, 157, Springer Nature Switzerland, pp.175-183, 2024, Mechanisms and Machine Science</arxiv:journal_reference>
      <dc:creator>Vlad Mihaly (LS2N, LS2N - \'equipe RoMas, CESTER), Iosif Birlescu (LS2N, LS2N - \'equipe RoMas, CESTER), Mircea \c{S}u\c{s}c\u{a} (LS2N, LS2N - \'equipe RoMas, CESTER), Damien Chablat (LS2N, LS2N - \'equipe RoMas, CESTER), Petru Dobra</dc:creator>
    </item>
    <item>
      <title>Re-expression of manual expertise through semi-automatic control of a teleoperated system</title>
      <link>https://arxiv.org/abs/2407.16278</link>
      <description>arXiv:2407.16278v1 Announce Type: new 
Abstract: While the search for new solvents in the chemical industry is of uttermost importance with respect to environmental considerations, this domain remains strongly tied to highly manual and visual inspection tasks by human experts. As the manipulated chemicals may imply a critical danger (CMR substances), mechanical protection barrier are used (fume hoods, gloveboxes). This, in turn, can induce postural discomfort in the long term. Carrying out this task using a remotely controlled robot to reproduce the desired vial motions would alleviate these postural constraints. Nevertheless, the adoption of such a system will depend on its ability to transcribe the users' expertise. Particular attention must be paid to the intuitiveness of the system : transparency of the actions performed, relevance of the perceptual feedback, etc. and, in particular, the fidelity of the movements performed in relation to the user's commands. However, the extent of the rotational movements to be generated and the task interactivity complicates the problem both from the point of view of the motor capacities of industrial robots and for the transparency/responsiveness of the control.To tackle the problen of guaranteeing a secure and reactive expression of the manual characteristics of this task, we propose to separate the control of movement into two parts: control of the path (set of spatial poses) and of the trajectories associated with this path (speed, direction of travel along the path). The user can then partially control the robot's movements, by choosing the type of generic, secure path and modulating the trajectory performed on this path in real time. Although this drastically limits the possibilities for interaction, we assume that this teleoperated system can enable this type of observation task to be carried out as effectively as for direct manipulation. This hypothesis was tested through an experiment in which a reading task, less dangerous but with similar characteristics to the application task, had to be performed using different variants of trajectory modulation. This experiment consisted in reading words printed on four white capsules (dimensions 6 x 12 mm) placed into cylindrical vials ( dimensions 16 mm x 70 mm). Four randomly selected vials were tested by each variant. Firstly, users had to perform the task via direct handling, then under conditions secured by a protection barrier. Users were then invited to perform the task using different trajectory modulation variants (modulation and passive viewing of a pre-recorded video, modulation of the trajectory of a Franka-Emika Panda robot performing the task in real time in front of a monocular Logitech Brio 4K camera). After each trial of a variant, users evaluate different aspects of this variant (manual and visual performance, ease of use, acceptability of the interface) through a questionnaire. During the trials, various objective criteria are also measured (number and nature of interaction with the interface, time and degree of success in the task). This experiment was carried out with 37 subjects (age : 27$\pm$5, 20 females). The data recorded showed that the proportion of successes, as well as the subjects' perceptions of visual performance, comfort of use and acceptability of the interface, were similar and high for all the variants. This suggests that this task is indeed achievable via the proposed interface. However, data also showed that average task completion times when using the trajectory modulation variants were significantly higher than handling by hand variants, which implies that the proposed remote semi-automatic control procedure fails to achieve satisfactory performance regarding execution time. An interface allowing more reactive manipulation of the vial's movements seems necessary, and will be tested in a future experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16278v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.54941/ahfe1005012</arxiv:DOI>
      <dc:creator>Erwann Landais (AUCTUS), Nasser Rezzoug (RoBioSS), Vincent Padois (AUCTUS)</dc:creator>
    </item>
    <item>
      <title>On the Use of Immersive Digital Technologies for Designing and Operating UAVs</title>
      <link>https://arxiv.org/abs/2407.16288</link>
      <description>arXiv:2407.16288v1 Announce Type: new 
Abstract: Unmanned Aerial Vehicles (UAVs) provide agile and safe solutions to communication relay networks, offering improved throughput. However, their modeling and control present challenges, and real-world deployment is hindered by the gap between simulation and reality. Moreover, enhancing situational awareness is critical. Several works in the literature proposed integrating UAV operation with immersive digital technologies, such as Digital Twin (DT) and Extended Reality (XR), to address these challenges. This paper provides a comprehensive overview of current research and developments involving immersive digital technologies for UAVs, including the latest advancements and emerging trends. We also explore the integration of DT and XR with Artificial Intelligence (AI) algorithms to create more intelligent, adaptive, and responsive UAV systems. Finally, we provide discussions, identify gaps in current research, and suggest future directions for studying the application of immersive technologies in UAVs, fostering further innovation and development in this field. We envision the fusion of DTs with XR will transform how UAVs operate, offering tools that enhance visualization, improve decision-making, and enable effective collaboration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16288v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yousef Emami, Kai Li, Luis Almeida, Wei Ni</dc:creator>
    </item>
    <item>
      <title>Optimizing Robotic Manipulation with Decision-RWKV: A Recurrent Sequence Modeling Approach for Lifelong Learning</title>
      <link>https://arxiv.org/abs/2407.16306</link>
      <description>arXiv:2407.16306v1 Announce Type: new 
Abstract: Models based on the Transformer architecture have seen widespread application across fields such as natural language processing, computer vision, and robotics, with large language models like ChatGPT revolutionizing machine understanding of human language and demonstrating impressive memory and reproduction capabilities. Traditional machine learning algorithms struggle with catastrophic forgetting, which is detrimental to the diverse and generalized abilities required for robotic deployment. This paper investigates the Receptance Weighted Key Value (RWKV) framework, known for its advanced capabilities in efficient and effective sequence modeling, and its integration with the decision transformer and experience replay architectures. It focuses on potential performance enhancements in sequence decision-making and lifelong robotic learning tasks. We introduce the Decision-RWKV (DRWKV) model and conduct extensive experiments using the D4RL database within the OpenAI Gym environment and on the D'Claw platform to assess the DRWKV model's performance in single-task tests and lifelong learning scenarios, showcasing its ability to handle multiple subtasks efficiently. The code for all algorithms, training, and image rendering in this study is open-sourced at https://github.com/ancorasir/DecisionRWKV.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16306v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yujian Dong, Tianyu Wu, Chaoyang Song</dc:creator>
    </item>
    <item>
      <title>Deep Learning Assisted Inertial Dead Reckoning and Fusion</title>
      <link>https://arxiv.org/abs/2407.16387</link>
      <description>arXiv:2407.16387v1 Announce Type: new 
Abstract: The interest in mobile platforms across a variety of applications has increased significantly in recent years. One of the reasons is the ability to achieve accurate navigation by using low-cost sensors. To this end, inertial sensors are fused with global navigation satellite systems (GNSS) signals. GNSS outages during platform operation can result in pure inertial navigation, causing the navigation solution to drift. In such situations, periodic trajectories with dedicated algorithms were suggested to mitigate the drift. With periodic dynamics, inertial deep learning approaches can capture the motion more accurately and provide accurate dead-reckoning for drones and mobile robots. In this paper, we propose approaches to extend deep learning-assisted inertial sensing and fusion capabilities during periodic motion. We begin by demonstrating that fusion between GNSS and inertial sensors in periodic trajectories achieves better accuracy compared to straight-line trajectories. Next, we propose an empowered network architecture to accurately regress the change in distance of the platform. Utilizing this network, we drive a hybrid approach for a neural-inertial fusion filter. Finally, we utilize this approach for situations when GNSS is available and show its benefits. A dataset of 337 minutes of data collected from inertial sensors mounted on a mobile robot and a quadrotor is used to evaluate our approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16387v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dror Hurwitz, Nadav Cohen, Itzik Klein</dc:creator>
    </item>
    <item>
      <title>Cleaning Robots in Public Spaces: A Survey and Proposal for Benchmarking Based on Stakeholders Interviews</title>
      <link>https://arxiv.org/abs/2407.16393</link>
      <description>arXiv:2407.16393v1 Announce Type: new 
Abstract: Autonomous cleaning robots for public spaces have potential for addressing current societal challenges, such as labor shortages and cleanliness in public spaces. Other application domains like autonomous driving, bin picking, or search and rescue have shown that benchmarking platforms and approaches in competitive settings can advance their respective research fields, resulting in more applicable systems under real-world conditions. For this paper, we analyzed seven semi-structured, qualitative stakeholder interviews about outdoor cleaning, identified current needs as well as limitations, and considered those results for the development of a benchmarking scenario based on the previous observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16393v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Raphael Memmesheimer, Martina Overbeck, Bjoern Kral, Lea Steffen, Sven Behnke, Martin Gersch, Arne Roennau</dc:creator>
    </item>
    <item>
      <title>Cross Anything: General Quadruped Robot Navigation through Complex Terrains</title>
      <link>https://arxiv.org/abs/2407.16412</link>
      <description>arXiv:2407.16412v1 Announce Type: new 
Abstract: The application of vision-language models (VLMs) has achieved impressive success in various robotics tasks, but there are few explorations for foundation models used in quadruped robot navigation. We introduce Cross Anything System (CAS), an innovative system composed of a high-level reasoning module and a low-level control policy, enabling the robot to navigate across complex 3D terrains and reach the goal position. For high-level reasoning and motion planning, we propose a novel algorithmic system taking advantage of a VLM, with a design of task decomposition and a closed-loop sub-task execution mechanism. For low-level locomotion control, we utilize the Probability Annealing Selection (PAS) method to train a control policy by reinforcement learning. Numerous experiments show that our whole system can accurately and robustly navigate across complex 3D terrains, and its strong generalization ability ensures the applications in diverse indoor and outdoor scenarios and terrains. Project page: https://cross-anything.github.io/</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16412v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaoting Zhu, Derun Li, Yong Liu, Ningyi Xu, Hang Zhao</dc:creator>
    </item>
    <item>
      <title>Real-Time Interactions Between Human Controllers and Remote Devices in Metaverse</title>
      <link>https://arxiv.org/abs/2407.16591</link>
      <description>arXiv:2407.16591v1 Announce Type: new 
Abstract: Supporting real-time interactions between human controllers and remote devices remains a challenging goal in the Metaverse due to the stringent requirements on computing workload, communication throughput, and round-trip latency. In this paper, we establish a novel framework for real-time interactions through the virtual models in the Metaverse. Specifically, we jointly predict the motion of the human controller for 1) proactive rendering in the Metaverse and 2) generating control commands to the real-world remote device in advance. The virtual model is decoupled into two components for rendering and control, respectively. To dynamically adjust the prediction horizons for rendering and control, we develop a two-step human-in-the-loop continuous reinforcement learning approach and use an expert policy to improve the training efficiency. An experimental prototype is built to verify our algorithm with different communication latencies. Compared with the baseline policy without prediction, our proposed method can reduce 1) the Motion-To-Photon (MTP) latency between human motion and rendering feedback and 2) the root mean squared error (RMSE) between human motion and real-world remote devices significantly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16591v1</guid>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kan Chen, Zhen Meng, Xiangmin Xu, Changyang She, Philip G. Zhao</dc:creator>
    </item>
    <item>
      <title>Learning to Play Foosball: System and Baselines</title>
      <link>https://arxiv.org/abs/2407.16606</link>
      <description>arXiv:2407.16606v1 Announce Type: new 
Abstract: This work stages Foosball as a versatile platform for advancing scientific research, particularly in the realm of robot learning. We present an automated Foosball table along with its corresponding simulated counterpart, showcasing a diverse range of challenges through example tasks within the Foosball environment. Initial findings are shared using a simple baseline approach. Foosball constitutes a versatile learning environment with the potential to yield cutting-edge research in various fields of artificial intelligence and machine learning, notably robust learning, while also extending its applicability to industrial robotics and automation setups. To transform our physical Foosball table into a research-friendly system, we augmented it with a 2 degrees of freedom kinematic chain to control the goalkeeper rod as an initial setup with the intention to be extended to the full game as soon as possible. Our experiments reveal that a realistic simulation is essential for mastering complex robotic tasks, yet translating these accomplishments to the real system remains challenging, often accompanied by a performance decline. This emphasizes the critical importance of research in this direction. In this concern, we spotlight the automated Foosball table as an invaluable tool, possessing numerous desirable attributes, to serve as a demanding learning environment for advancing robotics and automation research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16606v1</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Janosch Moos, Cedric Derstroff, Niklas Schr\"oder, Debora Clever</dc:creator>
    </item>
    <item>
      <title>No-brainer: Morphological Computation driven Adaptive Behavior in Soft Robots</title>
      <link>https://arxiv.org/abs/2407.16613</link>
      <description>arXiv:2407.16613v1 Announce Type: new 
Abstract: It is prevalent in contemporary AI and robotics to separately postulate a brain modeled by neural networks and employ it to learn intelligent and adaptive behavior. While this method has worked very well for many types of tasks, it isn't the only type of intelligence that exists in nature. In this work, we study the ways in which intelligent behavior can be created without a separate and explicit brain for robot control, but rather solely as a result of the computation occurring within the physical body of a robot. Specifically, we show that adaptive and complex behavior can be created in voxel-based virtual soft robots by using simple reactive materials that actively change the shape of the robot, and thus its behavior, under different environmental cues. We demonstrate a proof of concept for the idea of closed-loop morphological computation, and show that in our implementation, it enables behavior mimicking logic gates, enabling us to demonstrate how such behaviors may be combined to build up more complex collective behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16613v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alican Mertan, Nick Cheney</dc:creator>
    </item>
    <item>
      <title>Motion Accuracy and Computational Effort in QP-based Robot Control</title>
      <link>https://arxiv.org/abs/2407.16617</link>
      <description>arXiv:2407.16617v1 Announce Type: new 
Abstract: Quadratic Programs (QPs) have become a mature technology for the control of robots of all kinds, including humanoid robots. One aspect has been largely overlooked, however, which is the accuracy with which these QPs should be solved. Typical QP solvers aim at providing solutions accurate up to floating point precision ($\approx10^{-8}$). Considering physical quantities expressed in SI or similar units (meters, radians, etc.), such precision seems completely unrelated to both task requirements and hardware capacity. Typically, humanoid robots never achieve, nor are capable of achieving sub-millimeter precision in manipulation tasks. With this observation in mind, our objectives in this paper are two-fold: first examine how the QP solution accuracy impacts the resulting robot motion accuracy, then evaluate how a reduced solution accuracy requirement can be leveraged to reduce the corresponding computational effort. Numerical experiments with a dynamic simulation of a HRP-4 robot indicate that computational effort can be divided by more than 20 while maintaining the desired motion accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16617v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S\'elim Chefchaouni, Adrien Escande, Pierre-Brice Wieber</dc:creator>
    </item>
    <item>
      <title>Velocity Driven Vision: Asynchronous Sensor Fusion Birds Eye View Models for Autonomous Vehicles</title>
      <link>https://arxiv.org/abs/2407.16636</link>
      <description>arXiv:2407.16636v2 Announce Type: new 
Abstract: Fusing different sensor modalities can be a difficult task, particularly if they are asynchronous. Asynchronisation may arise due to long processing times or improper synchronisation during calibration, and there must exist a way to still utilise this previous information for the purpose of safe driving, and object detection in ego vehicle/ multi-agent trajectory prediction. Difficulties arise in the fact that the sensor modalities have captured information at different times and also at different positions in space. Therefore, they are not spatially nor temporally aligned. This paper will investigate the challenge of radar and LiDAR sensors being asynchronous relative to the camera sensors, for various time latencies. The spatial alignment will be resolved before lifting into BEV space via the transformation of the radar/LiDAR point clouds into the new ego frame coordinate system. Only after this can we concatenate the radar/LiDAR point cloud and lifted camera features. Temporal alignment will be remedied for radar data only, we will implement a novel method of inferring the future radar point positions using the velocity information. Our approach to resolving the issue of sensor asynchrony yields promising results. We demonstrate velocity information can drastically improve IoU for asynchronous datasets, as for a time latency of 360 milliseconds (ms), IoU improves from 49.54 to 53.63. Additionally, for a time latency of 550ms, the camera+radar (C+R) model outperforms the camera+LiDAR (C+L) model by 0.18 IoU. This is an advancement in utilising the often-neglected radar sensor modality, which is less favoured than LiDAR for autonomous driving purposes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16636v2</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the Irish Machine Vision and Image Processing Conference 2024</arxiv:journal_reference>
      <dc:creator>Seamie Hayes, Sushil Sharma, Ciar\'an Eising</dc:creator>
    </item>
    <item>
      <title>From Imitation to Refinement -- Residual RL for Precise Visual Assembly</title>
      <link>https://arxiv.org/abs/2407.16677</link>
      <description>arXiv:2407.16677v1 Announce Type: new 
Abstract: Behavior cloning (BC) currently stands as a dominant paradigm for learning real-world visual manipulation. However, in tasks that require locally corrective behaviors like multi-part assembly, learning robust policies purely from human demonstrations remains challenging. Reinforcement learning (RL) can mitigate these limitations by allowing policies to acquire locally corrective behaviors through task reward supervision and exploration. This paper explores the use of RL fine-tuning to improve upon BC-trained policies in precise manipulation tasks. We analyze and overcome technical challenges associated with using RL to directly train policy networks that incorporate modern architectural components like diffusion models and action chunking. We propose training residual policies on top of frozen BC-trained diffusion models using standard policy gradient methods and sparse rewards, an approach we call ResiP (Residual for Precise manipulation). Our experimental results demonstrate that this residual learning framework can significantly improve success rates beyond the base BC-trained models in high-precision assembly tasks by learning corrective actions. We also show that by combining ResiP with teacher-student distillation and visual domain randomization, our method can enable learning real-world policies for robotic assembly directly from RGB images. Find videos and code at \url{https://residual-assembly.github.io}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16677v1</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars Ankile, Anthony Simeonov, Idan Shenfeld, Marcel Torne, Pulkit Agrawal</dc:creator>
    </item>
    <item>
      <title>A Simulation Benchmark for Autonomous Racing with Large-Scale Human Data</title>
      <link>https://arxiv.org/abs/2407.16680</link>
      <description>arXiv:2407.16680v2 Announce Type: new 
Abstract: Despite the availability of international prize-money competitions, scaled vehicles, and simulation environments, research on autonomous racing and the control of sports cars operating close to the limit of handling has been limited by the high costs of vehicle acquisition and management, as well as the limited physics accuracy of open-source simulators. In this paper, we propose a racing simulation platform based on the simulator Assetto Corsa to test, validate, and benchmark autonomous driving algorithms, including reinforcement learning (RL) and classical Model Predictive Control (MPC), in realistic and challenging scenarios. Our contributions include the development of this simulation platform, several state-of-the-art algorithms tailored to the racing environment, and a comprehensive dataset collected from human drivers. Additionally, we evaluate algorithms in the offline RL setting. All the necessary code (including environment and benchmarks), working examples, datasets, and videos are publicly released and can be found at: https://assetto-corsa-gym.github.io</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16680v2</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrian Remonda, Nicklas Hansen, Ayoub Raji, Nicola Musiu, Marko Bertogna, Eduardo Veas, Xiaolong Wang</dc:creator>
    </item>
    <item>
      <title>Diffusion Models as Optimizers for Efficient Planning in Offline RL</title>
      <link>https://arxiv.org/abs/2407.16142</link>
      <description>arXiv:2407.16142v1 Announce Type: cross 
Abstract: Diffusion models have shown strong competitiveness in offline reinforcement learning tasks by formulating decision-making as sequential generation. However, the practicality of these methods is limited due to the lengthy inference processes they require. In this paper, we address this problem by decomposing the sampling process of diffusion models into two decoupled subprocesses: 1) generating a feasible trajectory, which is a time-consuming process, and 2) optimizing the trajectory. With this decomposition approach, we are able to partially separate efficiency and quality factors, enabling us to simultaneously gain efficiency advantages and ensure quality assurance. We propose the Trajectory Diffuser, which utilizes a faster autoregressive model to handle the generation of feasible trajectories while retaining the trajectory optimization process of diffusion models. This allows us to achieve more efficient planning without sacrificing capability. To evaluate the effectiveness and efficiency of the Trajectory Diffuser, we conduct experiments on the D4RL benchmarks. The results demonstrate that our method achieves $\it 3$-$\it 10 \times$ faster inference speed compared to previous sequence modeling methods, while also outperforming them in terms of overall performance. https://github.com/RenMing-Huang/TrajectoryDiffuser
  Keywords: Reinforcement Learning and Efficient Planning and Diffusion Model</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16142v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Renming Huang, Yunqiang Pei, Guoqing Wang, Yangming Zhang, Yang Yang, Peng Wang, Hengtao Shen</dc:creator>
    </item>
    <item>
      <title>LiCROcc: Teach Radar for Accurate Semantic Occupancy Prediction using LiDAR and Camera</title>
      <link>https://arxiv.org/abs/2407.16197</link>
      <description>arXiv:2407.16197v1 Announce Type: cross 
Abstract: Semantic Scene Completion (SSC) is pivotal in autonomous driving perception, frequently confronted with the complexities of weather and illumination changes. The long-term strategy involves fusing multi-modal information to bolster the system's robustness. Radar, increasingly utilized for 3D target detection, is gradually replacing LiDAR in autonomous driving applications, offering a robust sensing alternative. In this paper, we focus on the potential of 3D radar in semantic scene completion, pioneering cross-modal refinement techniques for improved robustness against weather and illumination changes, and enhancing SSC performance.Regarding model architecture, we propose a three-stage tight fusion approach on BEV to realize a fusion framework for point clouds and images. Based on this foundation, we designed three cross-modal distillation modules-CMRD, BRD, and PDD. Our approach enhances the performance in both radar-only (R-LiCROcc) and radar-camera (RC-LiCROcc) settings by distilling to them the rich semantic and structural information of the fused features of LiDAR and camera. Finally, our LC-Fusion (teacher model), R-LiCROcc and RC-LiCROcc achieve the best performance on the nuScenes-Occupancy dataset, with mIOU exceeding the baseline by 22.9%, 44.1%, and 15.5%, respectively. The project page is available at https://hr-zju.github.io/LiCROcc/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16197v1</guid>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yukai Ma, Jianbiao Mei, Xuemeng Yang, Licheng Wen, Weihua Xu, Jiangning Zhang, Botian Shi, Yong Liu, Xingxing Zuo</dc:creator>
    </item>
    <item>
      <title>TAPTRv2: Attention-based Position Update Improves Tracking Any Point</title>
      <link>https://arxiv.org/abs/2407.16291</link>
      <description>arXiv:2407.16291v1 Announce Type: cross 
Abstract: In this paper, we present TAPTRv2, a Transformer-based approach built upon TAPTR for solving the Tracking Any Point (TAP) task. TAPTR borrows designs from DEtection TRansformer (DETR) and formulates each tracking point as a point query, making it possible to leverage well-studied operations in DETR-like algorithms. TAPTRv2 improves TAPTR by addressing a critical issue regarding its reliance on cost-volume,which contaminates the point query\'s content feature and negatively impacts both visibility prediction and cost-volume computation. In TAPTRv2, we propose a novel attention-based position update (APU) operation and use key-aware deformable attention to realize. For each query, this operation uses key-aware attention weights to combine their corresponding deformable sampling positions to predict a new query position. This design is based on the observation that local attention is essentially the same as cost-volume, both of which are computed by dot-production between a query and its surrounding features. By introducing this new operation, TAPTRv2 not only removes the extra burden of cost-volume computation, but also leads to a substantial performance improvement. TAPTRv2 surpasses TAPTR and achieves state-of-the-art performance on many challenging datasets, demonstrating the superiority</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16291v1</guid>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hongyang Li, Hao Zhang, Shilong Liu, Zhaoyang Zeng, Feng Li, Tianhe Ren, Bohan Li, Lei Zhang</dc:creator>
    </item>
    <item>
      <title>Two Results on LPT: A Near-Linear Time Algorithm and Parcel Delivery using Drones</title>
      <link>https://arxiv.org/abs/2407.16323</link>
      <description>arXiv:2407.16323v1 Announce Type: cross 
Abstract: The focus of this paper is to increase our understanding of the Longest Processing Time First (LPT) heuristic. LPT is a classical heuristic for the fundamental problem of uniform machine scheduling. For different machine speeds, LPT was first considered by Gonzalez et al (SIAM J. Computing, 1977). Since then, extensive work has been done to improve the approximation factor of the LPT heuristic. However, all known implementations of the LPT heuristic take $O(mn)$ time, where $m$ is the number of machines and $n$ is the number of jobs. In this work, we come up with the first near-linear time implementation for LPT. Specifically, the running time is $O((n+m)(\log^2{m}+\log{n}))$. Somewhat surprisingly, the result is obtained by mapping the problem to dynamic maintenance of lower envelope of lines, which has been well studied in the computational geometry community.
  Our second contribution is to analyze the performance of LPT for the Drones Warehouse Problem (DWP), which is a natural generalization of the uniform machine scheduling problem motivated by drone-based parcel delivery from a warehouse. In this problem, a warehouse has multiple drones and wants to deliver parcels to several customers. Each drone picks a parcel from the warehouse, delivers it, and returns to the warehouse (where it can also get charged). The speeds and battery lives of the drones could be different, and due to the limited battery life, each drone has a bounded range in which it can deliver parcels. The goal is to assign parcels to the drones so that the time taken to deliver all the parcels is minimized. We prove that the natural approach of solving this problem via the LPT heuristic has an approximation factor of $\phi$, where $\phi \approx 1.62$ is the golden ratio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16323v1</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>L. Sunil Chandran, Rishikesh Gajjala, Shravan Mehra, Saladi Rahul</dc:creator>
    </item>
    <item>
      <title>Virtue Ethics For Ethically Tunable Robotic Assistants</title>
      <link>https://arxiv.org/abs/2407.16361</link>
      <description>arXiv:2407.16361v1 Announce Type: cross 
Abstract: The common consensus is that robots designed to work alongside or serve humans must adhere to the ethical standards of their operational environment. To achieve this, several methods based on established ethical theories have been suggested. Nonetheless, numerous empirical studies show that the ethical requirements of the real world are very diverse and can change rapidly from region to region. This eliminates the idea of a universal robot that can fit into any ethical context. However, creating customised robots for each deployment, using existing techniques is challenging. This paper presents a way to overcome this challenge by introducing a virtue ethics inspired computational method that enables character-based tuning of robots to accommodate the specific ethical needs of an environment. Using a simulated elder-care environment, we illustrate how tuning can be used to change the behaviour of a robot that interacts with an elderly resident in an ambient-assisted environment. Further, we assess the robot's responses by consulting ethicists to identify potential shortcomings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16361v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajitha Ramanayake, Vivek Nallur</dc:creator>
    </item>
    <item>
      <title>Learning General Continuous Constraint from Demonstrations via Positive-Unlabeled Learning</title>
      <link>https://arxiv.org/abs/2407.16485</link>
      <description>arXiv:2407.16485v1 Announce Type: cross 
Abstract: Planning for a wide range of real-world tasks necessitates to know and write all constraints. However, instances exist where these constraints are either unknown or challenging to specify accurately. A possible solution is to infer the unknown constraints from expert demonstration. The majority of prior works limit themselves to learning simple linear constraints, or require strong knowledge of the true constraint parameterization or environmental model. To mitigate these problems, this paper presents a positive-unlabeled (PU) learning approach to infer a continuous, arbitrary and possibly nonlinear, constraint from demonstration. From a PU learning view, We treat all data in demonstrations as positive (feasible) data, and learn a (sub)-optimal policy to generate high-reward-winning but potentially infeasible trajectories, which serve as unlabeled data containing both feasible and infeasible states. Under an assumption on data distribution, a feasible-infeasible classifier (i.e., constraint model) is learned from the two datasets through a postprocessing PU learning technique. The entire method employs an iterative framework alternating between updating the policy, which generates and selects higher-reward policies, and updating the constraint model. Additionally, a memory buffer is introduced to record and reuse samples from previous iterations to prevent forgetting. The effectiveness of the proposed method is validated in two Mujoco environments, successfully inferring continuous nonlinear constraints and outperforming a baseline method in terms of constraint accuracy and policy safety.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16485v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Baiyu Peng, Aude Billard</dc:creator>
    </item>
    <item>
      <title>HAPFI: History-Aware Planning based on Fused Information</title>
      <link>https://arxiv.org/abs/2407.16533</link>
      <description>arXiv:2407.16533v1 Announce Type: cross 
Abstract: Embodied Instruction Following (EIF) is a task of planning a long sequence of sub-goals given high-level natural language instructions, such as "Rinse a slice of lettuce and place on the white table next to the fork". To successfully execute these long-term horizon tasks, we argue that an agent must consider its past, i.e., historical data, when making decisions in each step. Nevertheless, recent approaches in EIF often neglects the knowledge from historical data and also do not effectively utilize information across the modalities. To this end, we propose History-Aware Planning based on Fused Information (HAPFI), effectively leveraging the historical data from diverse modalities that agents collect while interacting with the environment. Specifically, HAPFI integrates multiple modalities, including historical RGB observations, bounding boxes, sub-goals, and high-level instructions, by effectively fusing modalities via our Mutually Attentive Fusion method. Through experiments with diverse comparisons, we show that an agent utilizing historical multi-modal information surpasses all the compared methods that neglect the historical data in terms of action planning capability, enabling the generation of well-informed action plans for the next step. Moreover, we provided qualitative evidence highlighting the significance of leveraging historical multi-modal data, particularly in scenarios where the agent encounters intermediate failures, showcasing its robust re-planning capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16533v1</guid>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sujin Jeon, Suyeon Shin, Byoung-Tak Zhang</dc:creator>
    </item>
    <item>
      <title>Constrained Stein Variational Trajectory Optimization</title>
      <link>https://arxiv.org/abs/2308.12110</link>
      <description>arXiv:2308.12110v3 Announce Type: replace 
Abstract: We present Constrained Stein Variational Trajectory Optimization (CSVTO), an algorithm for performing trajectory optimization with constraints on a set of trajectories in parallel. We frame constrained trajectory optimization as a novel form of constrained functional minimization over trajectory distributions, which avoids treating the constraints as a penalty in the objective and allows us to generate diverse sets of constraint-satisfying trajectories. Our method uses Stein Variational Gradient Descent (SVGD) to find a set of particles that approximates a distribution over low-cost trajectories while obeying constraints. CSVTO is applicable to problems with differentiable equality and inequality constraints and includes a novel particle re-sampling step to escape local minima. By explicitly generating diverse sets of trajectories, CSVTO is better able to avoid poor local minima and is more robust to initialization. We demonstrate that CSVTO outperforms baselines in challenging highly-constrained tasks, such as a 7DoF wrench manipulation task, where CSVTO outperforms all baselines both in success and constraint satisfaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.12110v3</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>IEEE Transactions on Robotics, 2024</arxiv:journal_reference>
      <dc:creator>Thomas Power, Dmitry Berenson</dc:creator>
    </item>
    <item>
      <title>Functional Eigen-Grasping Using Approach Heatmaps</title>
      <link>https://arxiv.org/abs/2401.11681</link>
      <description>arXiv:2401.11681v2 Announce Type: replace 
Abstract: This work presents a framework for a robot with a multi-fingered hand to freely utilize daily tools, including functional parts like buttons and triggers. An approach heatmap is generated by selecting a functional finger, indicating optimal palm positions on the object's surface that enable the functional finger to contact the tool's functional part. Once the palm position is identified through the heatmap, achieving the functional grasp becomes a straightforward process where the fingers stably grasp the object with low-dimensional inputs using the eigengrasp. As our approach does not need human demonstrations, it can easily adapt to various sizes and designs, extending its applicability to different objects. In our approach, we use directional manipulability to obtain the approach heatmap. In addition, we add two kinds of energy functions, i.e., palm energy and functional energy functions, to realize the eigengrasp. Using this method, each robotic gripper can autonomously identify its optimal workspace for functional grasping, extending its applicability to non-anthropomorphic robotic hands. We show that several daily tools like spray, drill, and remotes can be efficiently used by not only an anthropomorphic Shadow hand but also a non-anthropomorphic Barrett hand.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11681v2</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Malek Aburub, Kazuki Higashi, Weiwei Wan, Kensuke Harada</dc:creator>
    </item>
    <item>
      <title>AutoGPT+P: Affordance-based Task Planning with Large Language Models</title>
      <link>https://arxiv.org/abs/2402.10778</link>
      <description>arXiv:2402.10778v2 Announce Type: replace 
Abstract: Recent advances in task planning leverage Large Language Models (LLMs) to improve generalizability by combining such models with classical planning algorithms to address their inherent limitations in reasoning capabilities. However, these approaches face the challenge of dynamically capturing the initial state of the task planning problem. To alleviate this issue, we propose AutoGPT+P, a system that combines an affordance-based scene representation with a planning system. Affordances encompass the action possibilities of an agent on the environment and objects present in it. Thus, deriving the planning domain from an affordance-based scene representation allows symbolic planning with arbitrary objects. AutoGPT+P leverages this representation to derive and execute a plan for a task specified by the user in natural language. In addition to solving planning tasks under a closed-world assumption, AutoGPT+P can also handle planning with incomplete information, e. g., tasks with missing objects by exploring the scene, suggesting alternatives, or providing a partial plan. The affordance-based scene representation combines object detection with an automatically generated object-affordance-mapping using ChatGPT. The core planning tool extends existing work by automatically correcting semantic and syntactic errors. Our approach achieves a success rate of 98%, surpassing the current 81% success rate of the current state-of-the-art LLM-based planning method SayCan on the SayCan instruction set. Furthermore, we evaluated our approach on our newly created dataset with 150 scenarios covering a wide range of complex tasks with missing objects, achieving a success rate of 79% on our dataset. The dataset and the code are publicly available at https://git.h2t.iar.kit.edu/birr/autogpt-p-standalone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10778v2</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timo Birr, Christoph Pohl, Abdelrahman Younes, Tamim Asfour</dc:creator>
    </item>
    <item>
      <title>Learning-based Methods for Adaptive Informative Path Planning</title>
      <link>https://arxiv.org/abs/2404.06940</link>
      <description>arXiv:2404.06940v3 Announce Type: replace 
Abstract: Adaptive informative path planning (AIPP) is important to many robotics applications, enabling mobile robots to efficiently collect useful data about initially unknown environments. In addition, learning-based methods are increasingly used in robotics to enhance adaptability, versatility, and robustness across diverse and complex tasks. Our survey explores research on applying robotic learning to AIPP, bridging the gap between these two research fields. We begin by providing a unified mathematical framework for general AIPP problems. Next, we establish two complementary taxonomies of current work from the perspectives of (i) learning algorithms and (ii) robotic applications. We explore synergies, recent trends, and highlight the benefits of learning-based methods in AIPP frameworks. Finally, we discuss key challenges and promising future directions to enable more generally applicable and robust robotic data-gathering systems through learning. We provide a comprehensive catalogue of papers reviewed in our survey, including publicly available repositories, to facilitate future studies in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06940v3</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marija Popovic, Joshua Ott, Julius R\"uckin, Mykel J. Kochenderfer</dc:creator>
    </item>
    <item>
      <title>Enabling Stateful Behaviors for Diffusion-based Policy Learning</title>
      <link>https://arxiv.org/abs/2404.12539</link>
      <description>arXiv:2404.12539v3 Announce Type: replace 
Abstract: While imitation learning provides a simple and effective framework for policy learning, acquiring consistent actions during robot execution remains a challenging task. Existing approaches primarily focus on either modifying the action representation at data curation stage or altering the model itself, both of which do not fully address the scalability of consistent action generation. To overcome this limitation, we introduce the Diff-Control policy, which utilizes a diffusion-based model to learn the action representation from a state-space modeling viewpoint. We demonstrate that we can reduce diffusion-based policies' uncertainty by making it stateful through a Bayesian formulation facilitated by ControlNet, leading to improved robustness and success rates. Our experimental results demonstrate the significance of incorporating action statefulness in policy learning, where Diff-Control shows improved performance across various tasks. Specifically, Diff-Control achieves an average success rate of 72% and 84% on stateful and dynamic tasks, respectively. Project page: https://github.com/ir-lab/Diff-Control</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12539v3</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiao Liu, Fabian Weigend, Yifan Zhou, Heni Ben Amor</dc:creator>
    </item>
    <item>
      <title>WING: Wheel-Inertial Neural Odometry with Ground Manifold Constraints</title>
      <link>https://arxiv.org/abs/2407.10101</link>
      <description>arXiv:2407.10101v2 Announce Type: replace 
Abstract: In this paper, we propose an interoceptive-only odometry system for ground robots with neural network processing and soft constraints based on the assumption of a globally continuous ground manifold. Exteroceptive sensors such as cameras, GPS and LiDAR may encounter difficulties in scenarios with poor illumination, indoor environments, dusty areas and straight tunnels. Therefore, improving the pose estimation accuracy only using interoceptive sensors is important to enhance the reliability of navigation system even in degrading scenarios mentioned above. However, interoceptive sensors like IMU and wheel encoders suffer from large drift due to noisy measurements. To overcome these challenges, the proposed system trains deep neural networks to correct the measurements from IMU and wheel encoders, while considering their uncertainty. Moreover, because ground robots can only travel on the ground, we model the ground surface as a globally continuous manifold using a dual cubic B-spline manifold to further improve the estimation accuracy by this soft constraint. A novel space-based sliding-window filtering framework is proposed to fully exploit the $C^2$ continuity of ground manifold soft constraints and fuse all the information from raw measurements and neural networks in a yaw-independent attitude convention. Extensive experiments demonstrate that our proposed approach can outperform state-of-the-art learning-based interoceptive-only odometry methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10101v2</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenxing Jiang, Kunyi Zhang, Sheng Yang, Shaojie Shen, Chao Xu, Fei Gao</dc:creator>
    </item>
    <item>
      <title>QueST: Self-Supervised Skill Abstractions for Learning Continuous Control</title>
      <link>https://arxiv.org/abs/2407.15840</link>
      <description>arXiv:2407.15840v2 Announce Type: replace 
Abstract: Generalization capabilities, or rather a lack thereof, is one of the most important unsolved problems in the field of robot learning, and while several large scale efforts have set out to tackle this problem, unsolved it remains. In this paper, we hypothesize that learning temporal action abstractions using latent variable models (LVMs), which learn to map data to a compressed latent space and back, is a promising direction towards low-level skills that can readily be used for new tasks. Although several works have attempted to show this, they have generally been limited by architectures that do not faithfully capture shareable representations. To address this we present Quantized Skill Transformer (QueST), which learns a larger and more flexible latent encoding that is more capable of modeling the breadth of low-level skills necessary for a variety of tasks. To make use of this extra flexibility, QueST imparts causal inductive bias from the action sequence data into the latent space, leading to more semantically useful and transferable representations. We compare to state-of-the-art imitation learning and LVM baselines and see that QueST's architecture leads to strong performance on several multitask and few-shot learning benchmarks. Further results and videos are available at https://quest-model.github.io/</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15840v2</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Atharva Mete, Haotian Xue, Albert Wilcox, Yongxin Chen, Animesh Garg</dc:creator>
    </item>
    <item>
      <title>A Coarse-to-Fine Place Recognition Approach using Attention-guided Descriptors and Overlap Estimation</title>
      <link>https://arxiv.org/abs/2303.06881</link>
      <description>arXiv:2303.06881v3 Announce Type: replace-cross 
Abstract: Place recognition is a challenging but crucial task in robotics. Current description-based methods may be limited by representation capabilities, while pairwise similarity-based methods require exhaustive searches, which is time-consuming. In this paper, we present a novel coarse-to-fine approach to address these problems, which combines BEV (Bird's Eye View) feature extraction, coarse-grained matching and fine-grained verification. In the coarse stage, our approach utilizes an attention-guided network to generate attention-guided descriptors. We then employ a fast affinity-based candidate selection process to identify the Top-K most similar candidates. In the fine stage, we estimate pairwise overlap among the narrowed-down place candidates to determine the final match. Experimental results on the KITTI and KITTI-360 datasets demonstrate that our approach outperforms state-of-the-art methods. The code will be released publicly soon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.06881v3</guid>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chencan Fu, Lin Li, Jianbiao Mei, Yukai Ma, Linpeng Peng, Xiangrui Zhao, Yong Liu</dc:creator>
    </item>
    <item>
      <title>EndoGS: Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting</title>
      <link>https://arxiv.org/abs/2401.11535</link>
      <description>arXiv:2401.11535v3 Announce Type: replace-cross 
Abstract: Surgical 3D reconstruction is a critical area of research in robotic surgery, with recent works adopting variants of dynamic radiance fields to achieve success in 3D reconstruction of deformable tissues from single-viewpoint videos. However, these methods often suffer from time-consuming optimization or inferior quality, limiting their adoption in downstream tasks. Inspired by 3D Gaussian Splatting, a recent trending 3D representation, we present EndoGS, applying Gaussian Splatting for deformable endoscopic tissue reconstruction. Specifically, our approach incorporates deformation fields to handle dynamic scenes, depth-guided supervision with spatial-temporal weight masks to optimize 3D targets with tool occlusion from a single viewpoint, and surface-aligned regularization terms to capture the much better geometry. As a result, EndoGS reconstructs and renders high-quality deformable endoscopic tissues from a single-viewpoint video, estimated depth maps, and labeled tool masks. Experiments on DaVinci robotic surgery videos demonstrate that EndoGS achieves superior rendering quality. Code is available at https://github.com/HKU-MedAI/EndoGS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11535v3</guid>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lingting Zhu, Zhao Wang, Jiahao Cui, Zhenchao Jin, Guying Lin, Lequan Yu</dc:creator>
    </item>
    <item>
      <title>Development of Compositionality and Generalization through Interactive Learning of Language and Action of Robots</title>
      <link>https://arxiv.org/abs/2403.19995</link>
      <description>arXiv:2403.19995v2 Announce Type: replace-cross 
Abstract: Humans excel at applying learned behavior to unlearned situations. A crucial component of this generalization behavior is our ability to compose/decompose a whole into reusable parts, an attribute known as compositionality. One of the fundamental questions in robotics concerns this characteristic. "How can linguistic compositionality be developed concomitantly with sensorimotor skills through associative learning, particularly when individuals only learn partial linguistic compositions and their corresponding sensorimotor patterns?" To address this question, we propose a brain-inspired neural network model that integrates vision, proprioception, and language into a framework of predictive coding and active inference, based on the free-energy principle. The effectiveness and capabilities of this model were assessed through various simulation experiments conducted with a robot arm. Our results show that generalization in learning to unlearned verb-noun compositions, is significantly enhanced when training variations of task composition are increased. We attribute this to self-organized compositional structures in linguistic latent state space being influenced significantly by sensorimotor learning. Ablation studies show that visual attention and working memory are essential to accurately generate visuo-motor sequences to achieve linguistically represented goals. These insights advance our understanding of mechanisms underlying development of compositionality through interactions of linguistic and sensorimotor experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19995v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prasanna Vijayaraghavan, Jeffrey Frederic Queisser, Sergio Verduzco Flores, Jun Tani</dc:creator>
    </item>
    <item>
      <title>Fast LiDAR Upsampling using Conditional Diffusion Models</title>
      <link>https://arxiv.org/abs/2405.04889</link>
      <description>arXiv:2405.04889v2 Announce Type: replace-cross 
Abstract: The search for refining 3D LiDAR data has attracted growing interest motivated by recent techniques such as supervised learning or generative model-based methods. Existing approaches have shown the possibilities for using diffusion models to generate refined LiDAR data with high fidelity, although the performance and speed of such methods have been limited. These limitations make it difficult to execute in real-time, causing the approaches to struggle in real-world tasks such as autonomous navigation and human-robot interaction. In this work, we introduce a novel approach based on conditional diffusion models for fast and high-quality sparse-to-dense upsampling of 3D scene point clouds through an image representation. Our method employs denoising diffusion probabilistic models trained with conditional inpainting masks, which have been shown to give high performance on image completion tasks. We introduce a series of experiments, including multiple datasets, sampling steps, and conditional masks. This paper illustrates that our method outperforms the baselines in sampling speed and quality on upsampling tasks using the KITTI-360 dataset. Furthermore, we illustrate the generalization ability of our approach by simultaneously training on real-world and synthetic datasets, introducing variance in quality and environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04889v2</guid>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sander Elias Magnussen Helgesen, Kazuto Nakashima, Jim T{\o}rresen, Ryo Kurazume</dc:creator>
    </item>
  </channel>
</rss>
