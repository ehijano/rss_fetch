<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Sep 2024 04:00:52 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Bounding the probability of causality under ordinal outcomes</title>
      <link>https://arxiv.org/abs/2409.09297</link>
      <description>arXiv:2409.09297v1 Announce Type: new 
Abstract: The probability of causation (PC) is often used in liability assessments. In a legal context, for example, where a patient suffered the side effect after taking a medication and sued the pharmaceutical company as a result, the value of the PC can help assess the likelihood that the side effect was caused by the medication, in other words, how likely it is that the patient will win the case. Beyond the issue of legal disputes, the PC plays an equally large role when one wants to go about explaining causal relationships between events that have already occurred in other areas. This article begins by reviewing the definitions and bounds of the probability of causality for binary outcomes, then generalizes them to ordinal outcomes. It demonstrates that incorporating additional mediator variable information in a complete mediation analysis provides a more refined bound compared to the simpler scenario where only exposure and outcome variables are considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09297v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanmei Sun, Chengfeng Shi, Qiang Zhao</dc:creator>
    </item>
    <item>
      <title>Asymptotics for irregularly observed long memory processes</title>
      <link>https://arxiv.org/abs/2409.09498</link>
      <description>arXiv:2409.09498v1 Announce Type: new 
Abstract: We study the effect of observing a stationary process at irregular time points via a renewal process. We establish a sharp difference in the asymptotic behaviour of the self-normalized sample mean of the observed process depending on the renewal process. In particular, we show that if the renewal process has a moderate heavy tail distribution then the limit is a so-called Normal Variance Mixture (NVM) and we characterize the randomized variance part of the limiting NVM as an integral function of a L\'evy stable motion. Otherwise, the normalized sample mean will be asymptotically normal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09498v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohamedou Ould-Haye, Anne Philippe</dc:creator>
    </item>
    <item>
      <title>The Asymptotics of Wide Remedians</title>
      <link>https://arxiv.org/abs/2409.09528</link>
      <description>arXiv:2409.09528v1 Announce Type: new 
Abstract: The remedian uses a $k\times b$ matrix to approximate the median of $n\leq b^{k}$ streaming input values by recursively replacing buffers of $b$ values with their medians, thereby ignoring its $200(\lceil b/2\rceil / b)^{k}%$ most extreme inputs. Rousseeuw &amp; Bassett (1990) and Chao &amp; Lin (1993); Chen &amp; Chen (2005) study the remedian's distribution as $k\rightarrow\infty$ and as $k,b\rightarrow\infty$. The remedian's breakdown point vanishes as $k\rightarrow\infty$, but approaches $(1/2)^{k}$ as $b\rightarrow\infty$. We study the remedian's robust-regime distribution as $b\rightarrow\infty$, deriving a normal distribution for standardized (mean, median, remedian, remedian rank) as $b\rightarrow\infty$, thereby illuminating the remedian's accuracy in approximating the sample median. We derive the asymptotic efficiency of the remedian relative to the mean and the median. Finally, we discuss the estimation of more than one quantile at once, proposing an asymptotic distribution for the random vector that results when we apply remedian estimation in parallel to the components of i.i.d. random vectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09528v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philip T. Labo</dc:creator>
    </item>
    <item>
      <title>RandALO: Out-of-sample risk estimation in no time flat</title>
      <link>https://arxiv.org/abs/2409.09781</link>
      <description>arXiv:2409.09781v1 Announce Type: new 
Abstract: Estimating out-of-sample risk for models trained on large high-dimensional datasets is an expensive but essential part of the machine learning process, enabling practitioners to optimally tune hyperparameters. Cross-validation (CV) serves as the de facto standard for risk estimation but poorly trades off high bias ($K$-fold CV) for computational cost (leave-one-out CV). We propose a randomized approximate leave-one-out (RandALO) risk estimator that is not only a consistent estimator of risk in high dimensions but also less computationally expensive than $K$-fold CV. We support our claims with extensive simulations on synthetic and real data and provide a user-friendly Python package implementing RandALO available on PyPI as randalo and at https://github.com/cvxgrp/randalo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09781v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Parth T. Nobel, Daniel LeJeune, Emmanuel J. Cand\`es</dc:creator>
    </item>
    <item>
      <title>Accuracy of the Ensemble Kalman Filter in the Near-Linear Setting</title>
      <link>https://arxiv.org/abs/2409.09800</link>
      <description>arXiv:2409.09800v1 Announce Type: new 
Abstract: The filtering distribution captures the statistics of the state of a dynamical system from partial and noisy observations. Classical particle filters provably approximate this distribution in quite general settings; however they behave poorly for high dimensional problems, suffering weight collapse. This issue is circumvented by the ensemble Kalman filter which is an equal-weight interacting particle system. However, this finite particle system is only proven to approximate the true filter in the linear Gaussian case. In practice, however, it is applied in much broader settings; as a result, establishing its approximation properties more generally is important. There has been recent progress in the theoretical analysis of the algorithm, establishing stability and error estimates in non-Gaussian settings, but the assumptions on the dynamics and observation models rule out the unbounded vector fields that arise in practice and the analysis applies only to the mean field limit of the ensemble Kalman filter. The present work establishes error bounds between the filtering distribution and the finite particle ensemble Kalman filter when the model exhibits linear growth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09800v1</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edoardo Calvello, Pierre Monmarch\'e, Andrew M. Stuart, Urbain Vaes</dc:creator>
    </item>
    <item>
      <title>Towards a Unified Theory for Semiparametric Data Fusion with Individual-Level Data</title>
      <link>https://arxiv.org/abs/2409.09973</link>
      <description>arXiv:2409.09973v1 Announce Type: new 
Abstract: We address the goal of conducting inference about a smooth finite-dimensional parameter by utilizing individual-level data from various independent sources. Recent advancements have led to the development of a comprehensive theory capable of handling scenarios where different data sources align with, possibly distinct subsets of, conditional distributions of a single factorization of the joint target distribution. While this theory proves effective in many significant contexts, it falls short in certain common data fusion problems, such as two-sample instrumental variable analysis, settings that integrate data from epidemiological studies with diverse designs (e.g., prospective cohorts and retrospective case-control studies), and studies with variables prone to measurement error that are supplemented by validation studies. In this paper, we extend the aforementioned comprehensive theory to allow for the fusion of individual-level data from sources aligned with conditional distributions that do not correspond to a single factorization of the target distribution. Assuming conditional and marginal distribution alignments, we provide universal results that characterize the class of all influence functions of regular asymptotically linear estimators and the efficient influence function of any pathwise differentiable parameter, irrespective of the number of data sources, the specific parameter of interest, or the statistical model for the target distribution. This theory paves the way for machine-learning debiased, semiparametric efficient estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09973v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ellen Graham (University of Washington), Marco Carone (University of Washington), Andrea Rotnitzky (University of Washington)</dc:creator>
    </item>
    <item>
      <title>Privately Learning Smooth Distributions on the Hypercube by Projections</title>
      <link>https://arxiv.org/abs/2409.10083</link>
      <description>arXiv:2409.10083v1 Announce Type: new 
Abstract: Fueled by the ever-increasing need for statistics that guarantee the privacy of their training sets, this article studies the centrally-private estimation of Sobolev-smooth densities of probability over the hypercube in dimension d. The contributions of this article are two-fold : Firstly, it generalizes the one dimensional results of (Lalanne et al., 2023) to non-integer levels of smoothness and to a high-dimensional setting, which is important for two reasons : it is more suited for modern learning tasks, and it allows understanding the relations between privacy, dimensionality and smoothness, which is a central question with differential privacy. Secondly, this article presents a private strategy of estimation that is data-driven (usually referred to as adaptive in Statistics) in order to privately choose an estimator that achieves a good bias-variance trade-off among a finite family of private projection estimators without prior knowledge of the ground-truth smoothness $\beta$. This is achieved by adapting the Lepskii method for private selection, by adding a new penalization term that makes the estimation privacy-aware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10083v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>ICML 2024 - 41st International Conference on Machine Learning, Jul 2024, Vienna, Austria. 39 p</arxiv:journal_reference>
      <dc:creator>Cl\'ement Lalanne (TSE-R), S\'ebastien Gadat (TSE-R, IUF)</dc:creator>
    </item>
    <item>
      <title>Extending the Gini Index to Higher Dimensions via Whitening Processes</title>
      <link>https://arxiv.org/abs/2409.10119</link>
      <description>arXiv:2409.10119v1 Announce Type: new 
Abstract: Measuring the degree of inequality expressed by a multivariate statistical distribution is a challenging problem, which appears in many fields of science and engineering. In this paper, we propose to extend the well known univariate Gini coefficient to multivariate distributions, by maintaining most of its properties. Our extension is based on the application of whitening processes that possess the property of scale stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10119v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gennaro Auricchio, Paolo Giudici, Giuseppe Toscani</dc:creator>
    </item>
    <item>
      <title>Consistent complete independence test in high dimensions based on Chatterjee correlation coefficient</title>
      <link>https://arxiv.org/abs/2409.10315</link>
      <description>arXiv:2409.10315v1 Announce Type: new 
Abstract: In this article, we consider the complete independence test of high-dimensional data. Based on Chatterjee coefficient, we pioneer the development of quadratic test and extreme value test which possess good testing performance for oscillatory data, and establish the corresponding large sample properties under both null hypotheses and alternative hypotheses. In order to overcome the shortcomings of quadratic statistic and extreme value statistic, we propose a testing method termed as power enhancement test by adding a screening statistic to the quadratic statistic. The proposed method do not reduce the testing power under dense alternative hypotheses, but can enhance the power significantly under sparse alternative hypotheses. Three synthetic data examples and two real data examples are further used to illustrate the performance of our proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10315v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liqi Xia, Ruiyuan Cao, Jiang Du, Jun Dai</dc:creator>
    </item>
    <item>
      <title>Mean Residual Life Ageing Intensity Function</title>
      <link>https://arxiv.org/abs/2409.10456</link>
      <description>arXiv:2409.10456v1 Announce Type: new 
Abstract: The ageing intensity function is a powerful analytical tool that provides valuable insights into the ageing process across diverse domains such as reliability engineering, actuarial science, and healthcare. Its applications continue to expand as researchers delve deeper into understanding the complex dynamics of ageing and its implications for society. One common approach to defining the ageing intensity function is through the hazard rate or failure rate function, extensively explored in scholarly literature. Equally significant to the hazard rate function is the mean residual life function, which plays a crucial role in analyzing the ageing patterns exhibited by units or components. This article introduces the mean residual life ageing intensity (MRLAI) function to delve into component ageing behaviours across various distributions. Additionally, we scrutinize the closure properties of the MRLAI function across different reliability operations. Furthermore, a new order termed the mean residual life ageing intensity order is defined to analyze the ageing behaviour of a system, and the closure property of this order under various reliability operations is discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10456v1</guid>
      <category>math.ST</category>
      <category>physics.soc-ph</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ashutosh Singh, Ishapathik Das, Asok Kumar Nanda, Sumen Sen</dc:creator>
    </item>
    <item>
      <title>Noncommutative Donoho-Elad-Gribonval-Nielsen-Fuchs Sparsity Theorem</title>
      <link>https://arxiv.org/abs/2409.09060</link>
      <description>arXiv:2409.09060v1 Announce Type: cross 
Abstract: Breakthrough Sparsity Theorem, derived independently by Donoho and Elad \textit{[Proc. Natl. Acad. Sci. USA, 2003]}, Gribonval and Nielsen \textit{[IEEE Trans. Inform. Theory, 2003]} and Fuchs \textit{[IEEE Trans. Inform. Theory, 2004]} says that unique sparse solution to NP-Hard $\ell_0$-minimization problem can be obtained using unique solution of P-Type $\ell_1$-minimization problem. In this paper, we derive noncommutative version of their result using frames for Hilbert C*-modules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09060v1</guid>
      <category>math.FA</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OA</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>K. Mahesh Krishna</dc:creator>
    </item>
    <item>
      <title>A Random-effects Approach to Regression Involving Many Categorical Predictors and Their Interactions</title>
      <link>https://arxiv.org/abs/2409.09355</link>
      <description>arXiv:2409.09355v1 Announce Type: cross 
Abstract: Linear model prediction with a large number of potential predictors is both statistically and computationally challenging. The traditional approaches are largely based on shrinkage selection/estimation methods, which are applicable even when the number of potential predictors is (much) larger than the sample size. A situation of the latter scenario occurs when the candidate predictors involve many binary indicators corresponding to categories of some categorical predictors as well as their interactions. We propose an alternative approach to the shrinkage prediction methods in such a case based on mixed model prediction, which effectively treats combinations of the categorical effects as random effects. We establish theoretical validity of the proposed method, and demonstrate empirically its advantage over the shrinkage methods. We also develop measures of uncertainty for the proposed method and evaluate their performance empirically. A real-data example is considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09355v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanmei Sun, Jiangshan Zhang, Jiming Jiang</dc:creator>
    </item>
    <item>
      <title>A Statistical Viewpoint on Differential Privacy: Hypothesis Testing, Representation and Blackwell's Theorem</title>
      <link>https://arxiv.org/abs/2409.09558</link>
      <description>arXiv:2409.09558v1 Announce Type: cross 
Abstract: Differential privacy is widely considered the formal privacy for privacy-preserving data analysis due to its robust and rigorous guarantees, with increasingly broad adoption in public services, academia, and industry. Despite originating in the cryptographic context, in this review paper we argue that, fundamentally, differential privacy can be considered a \textit{pure} statistical concept. By leveraging a theorem due to David Blackwell, our focus is to demonstrate that the definition of differential privacy can be formally motivated from a hypothesis testing perspective, thereby showing that hypothesis testing is not merely convenient but also the right language for reasoning about differential privacy. This insight leads to the definition of $f$-differential privacy, which extends other differential privacy definitions through a representation theorem. We review techniques that render $f$-differential privacy a unified framework for analyzing privacy bounds in data analysis and machine learning. Applications of this differential privacy definition to private deep learning, private convex optimization, shuffled mechanisms, and U.S.~Census data are discussed to highlight the benefits of analyzing privacy bounds under this framework compared to existing alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09558v1</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weijie J. Su</dc:creator>
    </item>
    <item>
      <title>Learning large softmax mixtures with warm start EM</title>
      <link>https://arxiv.org/abs/2409.09903</link>
      <description>arXiv:2409.09903v1 Announce Type: cross 
Abstract: Mixed multinomial logits are discrete mixtures introduced several decades ago to model the probability of choosing an attribute from $p$ possible candidates, in heterogeneous populations. The model has recently attracted attention in the AI literature, under the name softmax mixtures, where it is routinely used in the final layer of a neural network to map a large number $p$ of vectors in $\mathbb{R}^L$ to a probability vector. Despite its wide applicability and empirical success, statistically optimal estimators of the mixture parameters, obtained via algorithms whose running time scales polynomially in $L$, are not known. This paper provides a solution to this problem for contemporary applications, such as large language models, in which the mixture has a large number $p$ of support points, and the size $N$ of the sample observed from the mixture is also large. Our proposed estimator combines two classical estimators, obtained respectively via a method of moments (MoM) and the expectation-minimization (EM) algorithm. Although both estimator types have been studied, from a theoretical perspective, for Gaussian mixtures, no similar results exist for softmax mixtures for either procedure. We develop a new MoM parameter estimator based on latent moment estimation that is tailored to our model, and provide the first theoretical analysis for a MoM-based procedure in softmax mixtures. Although consistent, MoM for softmax mixtures can exhibit poor numerical performance, as observed other mixture models. Nevertheless, as MoM is provably in a neighborhood of the target, it can be used as warm start for any iterative algorithm. We study in detail the EM algorithm, and provide its first theoretical analysis for softmax mixtures. Our final proposal for parameter estimation is the EM algorithm with a MoM warm start.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09903v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Bing, Florentina Bunea, Jonathan Niles-Weed, Marten Wegkamp</dc:creator>
    </item>
    <item>
      <title>Why you should also use OLS estimation of tail exponents</title>
      <link>https://arxiv.org/abs/2409.10448</link>
      <description>arXiv:2409.10448v1 Announce Type: cross 
Abstract: Even though practitioners often estimate Pareto exponents running OLS rank-size regressions, the usual recommendation is to use the Hill MLE with a small-sample correction instead, due to its unbiasedness and efficiency. In this paper, we advocate that you should also apply OLS in empirical applications. On the one hand, we demonstrate that, with a small-sample correction, the OLS estimator is also unbiased. On the other hand, we show that the MLE assigns significantly greater weight to smaller observations. This suggests that the OLS estimator may outperform the MLE in cases where the distribution is (i) strictly Pareto but only in the upper tail or (ii) regularly varying rather than strictly Pareto. We substantiate our theoretical findings with Monte Carlo simulations and real-world applications, demonstrating the practical relevance of the OLS method in estimating tail exponents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10448v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thiago Trafane Oliveira Santos (Central Bank of Brazil, Bras\'ilia, Brazil. Department of %Economics, University of Brasilia, Brazil), Daniel Oliveira Cajueiro (Department of Economics, University of Brasilia, Brazil. National Institute of Science and Technology for Complex Systems)</dc:creator>
    </item>
    <item>
      <title>Permutation groups, partition lattices and block structures</title>
      <link>https://arxiv.org/abs/2409.10461</link>
      <description>arXiv:2409.10461v1 Announce Type: cross 
Abstract: Let $G$ be a transitive permutation group on $\Omega$. The $G$-invariant partitions form a sublattice of the lattice of all partitions of $\Omega$, having the further property that all its elements are uniform (that is, have all parts of the same size). If, in addition, all the equivalence relations defining the partitions commute, then the relations form an \emph{orthogonal block structure}, a concept from statistics; in this case the lattice is modular. If it is distributive, then we have a \emph{poset block structure}, whose automorphism group is a \emph{generalised wreath product}. We examine permutation groups with these properties, which we call the \emph{OB property} and \emph{PB property} respectively, and in particular investigate when direct and wreath products of groups with these properties also have these properties.
  A famous theorem on permutation groups asserts that a transitive imprimitive group $G$ is embeddable in the wreath product of two factors obtained from the group (the group induced on a block by its setwise stabiliser, and the group induced on the set of blocks by~$G$). We extend this theorem to groups with the PB property, embeddng them into generalised wreath products. We show that the map from posets to generalised wreath products preserves intersections and inclusions.
  We have included background and historical material on these concepts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10461v1</guid>
      <category>math.GR</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marina Anagnostopoulou-Merkouri, R. A. Bailey, Peter J. Cameron</dc:creator>
    </item>
    <item>
      <title>A Shrinkage Likelihood Ratio Test for High-Dimensional Subgroup Analysis with a Logistic-Normal Mixture Model</title>
      <link>https://arxiv.org/abs/2307.10272</link>
      <description>arXiv:2307.10272v2 Announce Type: replace 
Abstract: In subgroup analysis, testing the existence of a subgroup with a differential treatment effect serves as protection against spurious subgroup discovery. Despite its importance, this hypothesis testing possesses a complicated nature: parameter characterizing subgroup classification is not identified under the null hypothesis of no subgroup. Due to this irregularity, the existing methods have the following two limitations. First, the asymptotic null distribution of test statistics often takes an intractable form, which necessitates computationally demanding resampling methods to calculate the critical value. Second, the dimension of personal attributes characterizing subgroup membership is not allowed to be of high dimension. To solve these two problems simultaneously, this study develops a shrinkage likelihood ratio test for the existence of a subgroup using a logistic-normal mixture model. The proposed test statistics are built on a modified likelihood function that shrinks possibly high-dimensional unidentified parameters toward zero under the null hypothesis while retaining power under the alternative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10272v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shota Takeishi</dc:creator>
    </item>
    <item>
      <title>Analysis of the rSVDdpd Algorithm: A Robust Singular Value Decomposition Method using Density Power Divergence</title>
      <link>https://arxiv.org/abs/2307.10591</link>
      <description>arXiv:2307.10591v2 Announce Type: replace 
Abstract: The traditional method of computing singular value decomposition (SVD) of a data matrix is based on a least squares principle, thus, is very sensitive to the presence of outliers. Hence the resulting inferences across different applications using the classical SVD are extremely degraded in the presence of data contamination (e.g., video surveillance background modelling tasks, etc.). A robust singular value decomposition method using the minimum density power divergence estimator (rSVDdpd) has been found to provide a satisfactory solution to this problem and works well in applications. For example, it provides a neat solution to the background modelling problem of video surveillance data in the presence of camera tampering. In this paper, we investigate the theoretical properties of the rSVDdpd estimator such as convergence, equivariance and consistency under reasonable assumptions. Since the dimension of the parameters, i.e., the number of singular values and the dimension of singular vectors can grow linearly with the size of the data, the usual M-estimation theory has to be suitably modified with concentration bounds to establish the asymptotic properties. We believe that we have been able to accomplish this satisfactorily in the present work. We also demonstrate the efficiency of rSVDdpd through extensive simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10591v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11222-024-10493-7</arxiv:DOI>
      <arxiv:journal_reference>Stat Comput 34, 178 (2024)</arxiv:journal_reference>
      <dc:creator>Subhrajyoty Roy, Abhik Ghosh, Ayanendranath Basu</dc:creator>
    </item>
    <item>
      <title>Tensor Time Series Imputation through Tensor Factor Modelling</title>
      <link>https://arxiv.org/abs/2403.13153</link>
      <description>arXiv:2403.13153v2 Announce Type: replace 
Abstract: We propose tensor time series imputation when the missing pattern in the tensor data can be general, as long as any two data positions along a tensor fibre are both observed for enough time points. The method is based on a tensor time series factor model with Tucker decomposition of the common component. One distinguished feature of the tensor time series factor model used is that there can be weak factors in the factor loadings matrix for each mode. This reflects reality better when real data can have weak factors which drive only groups of observed variables, for instance, a sector factor in financial market driving only stocks in a particular sector. Using the data with missing entries, asymptotic normality is derived for rows of estimated factor loadings, while consistent covariance matrix estimation enables us to carry out inferences. As a first in the literature, we also propose a ratio-based estimator for the rank of the core tensor under general missing patterns. Rates of convergence are spelt out for the imputations from the estimated tensor factor models. Simulation results show that our imputation procedure works well, with asymptotic normality and corresponding inferences also demonstrated. Re-imputation performances are also gauged when we demonstrate that using slightly larger rank then estimated gives superior re-imputation performances. A Fama-French portfolio example with matrix returns and an OECD data example with matrix of Economic indicators are presented and analyzed, showing the efficacy of our imputation approach compared to direct vector imputation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13153v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zetai Cen, Clifford Lam</dc:creator>
    </item>
    <item>
      <title>Asymptotics of predictive distributions driven by sample means and variances</title>
      <link>https://arxiv.org/abs/2403.16828</link>
      <description>arXiv:2403.16828v3 Announce Type: replace 
Abstract: Let $\alpha_n(\cdot)=P\bigl(X_{n+1}\in\cdot\mid X_1,\ldots,X_n\bigr)$ be the predictive distributions of a sequence $(X_1,X_2,\ldots)$ of $p$-dimensional random vectors. Suppose $$\alpha_n= \mathcal{N} _p (M_n,Q_n)$$ where $M_n=\frac{1}{n}\sum_{i=1}^nX_i$ and $Q_n=\frac{1}{n}\sum_{i=1}^n(X_i-M_n)(X_i-M_n)^t$. Then, there is a random probability measure $\alpha$ on the Borel subsets of $\mathbb{R}^p$ such that $\lVert\alpha_n-\alpha\rVert\overset{a.s.}\longrightarrow 0$ where $\lVert\cdot\rVert$ is total variation distance. An explicit expression for $\alpha$ is provided and the convergence rate of $\lVert\alpha_n-\alpha\rVert$ is shown to be arbitrarily close to $n^{-1/2}$. Moreover, it is still true that $\lVert\alpha_n-\alpha\rVert\overset{a.s.}\longrightarrow 0$ even if $\alpha_n=\mathcal{L}(M_n,Q_n)$ where $\mathcal{L}$ belongs to a class of distributions much larger than the normal. The predictives $\alpha_n$ are useful in various frameworks, including Bayesian predictive inference and predictive resampling. Finally, the asymptotic behavior of copula-based predictive distributions (introduced in [13]) is investigated and a numerical experiment is performed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16828v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samuele Garelli, Fabrizio Leisen, Luca Pratelli, Pietro Rigo</dc:creator>
    </item>
    <item>
      <title>Higher-Order Graphon Theory: Fluctuations, Degeneracies, and Inference</title>
      <link>https://arxiv.org/abs/2404.13822</link>
      <description>arXiv:2404.13822v2 Announce Type: replace 
Abstract: Exchangeable random graphs, which include some of the most widely studied network models, have emerged as the mainstay of statistical network analysis in recent years. Graphons, which are the central objects in graph limit theory, provide a natural way to sample exchangeable random graphs. It is well known that network moments (motif/subgraph counts) identify a graphon (up to an isomorphism), hence, understanding the sampling distribution of subgraph counts in random graphs sampled from a graphon is pivotal for nonparametric network inference. In this paper, we derive the joint asymptotic distribution of any finite collection of network moments in random graphs sampled from a graphon, that includes both the non-degenerate case (where the distribution is Gaussian) as well as the degenerate case (where the distribution has both Gaussian or non-Gaussian components). This provides the higher-order fluctuation theory for subgraph counts in the graphon model. We also develop a novel multiplier bootstrap for graphons that consistently approximates the limiting distribution of the network moments (both in the Gaussian and non-Gaussian regimes). Using this and a procedure for testing degeneracy, we construct joint confidence sets for any finite collection of motif densities. This provides a general framework for statistical inference based on network moments in the graphon model. To illustrate the broad scope of our results we also consider the problem of detecting global structure (that is, testing whether the graphon is a constant function) based on small subgraphs. We propose a consistent test for this problem, invoking celebrated results on quasi-random graphs, and derive its limiting distribution both under the null and the alternative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13822v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anirban Chatterjee, Soham Dan, Bhaswar B. Bhattacharya</dc:creator>
    </item>
    <item>
      <title>On Admissibility in Bipartite Incidence Graph Sampling</title>
      <link>https://arxiv.org/abs/2409.07970</link>
      <description>arXiv:2409.07970v3 Announce Type: replace 
Abstract: In bipartite incidence graph sampling, the target study units may be formed as connected population elements, which are distinct to the units of sampling and there may exist generally more than one way by which a given study unit can be observed via sampling units. This generalizes finite-population element or multistage sampling, where each element can only be sampled directly or via a single primary sampling unit. We study the admissibility of estimators in bipartite incidence graph sampling and identify other admissible estimators than the classic Horvitz-Thompson estimator. Our admissibility results encompass those for finite-population sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07970v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pedro Garc\'ia-Segador, Li-Chun Zhang</dc:creator>
    </item>
    <item>
      <title>Theory of Low Frequency Contamination from Nonstationarity and Misspecification: Consequences for HAR Inference</title>
      <link>https://arxiv.org/abs/2103.01604</link>
      <description>arXiv:2103.01604v3 Announce Type: replace-cross 
Abstract: We establish theoretical results about the low frequency contamination (i.e., long memory effects) induced by general nonstationarity for estimates such as the sample autocovariance and the periodogram, and deduce consequences for heteroskedasticity and autocorrelation robust (HAR) inference. We present explicit expressions for the asymptotic bias of these estimates. We distinguish cases where this contamination only occurs as a small-sample problem and cases where the contamination continues to hold asymptotically. We show theoretically that nonparametric smoothing over time is robust to low frequency contamination. Our results provide new insights on the debate between consistent versus inconsistent long-run variance (LRV) estimation. Existing LRV estimators tend to be in inflated when the data are nonstationary. This results in HAR tests that can be undersized and exhibit dramatic power losses. Our theory indicates that long bandwidths or fixed-b HAR tests suffer more from low frequency contamination relative to HAR tests based on HAC estimators, whereas recently introduced double kernel HAC estimators do not super from this problem. Finally, we present second-order Edgeworth expansions under nonstationarity about the distribution of HAC and DK-HAC estimators and about the corresponding t-test in the linear regression model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2103.01604v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alessandro Casini, Taosong Deng, Pierre Perron</dc:creator>
    </item>
    <item>
      <title>A central limit theorem for a sequence of conditionally centered random fields</title>
      <link>https://arxiv.org/abs/2301.08942</link>
      <description>arXiv:2301.08942v4 Announce Type: replace-cross 
Abstract: A central limit theorem is established for a sum of random variables belonging to a sequence of random fields. The fields are assumed to have zero mean conditional on the past history and to satisfy certain conditional $\alpha$-mixing conditions in space or time. Exploiting conditional centering and the space-time structure, the limiting normal distribution is obtained for increasing spatial domain, increasing length of the sequence, or both of these. The theorem is very well suited for establishing asymptotic normality in the context of unbiased estimating function inference for a wide range of space-time processes. This is pertinent given the abundance of space-time data. Two examples demonstrate the applicability of the theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.08942v4</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdollah Jalilian, Arnaud Poinas, Ganggang Xu, Rasmus Waagepetersen</dc:creator>
    </item>
    <item>
      <title>Markov properties of Gaussian random fields on compact metric graphs</title>
      <link>https://arxiv.org/abs/2304.03190</link>
      <description>arXiv:2304.03190v3 Announce Type: replace-cross 
Abstract: There has recently been much interest in Gaussian fields on linear networks and, more generally, on compact metric graphs. One proposed strategy for defining such fields on a metric graph $\Gamma$ is through a covariance function that is isotropic in a metric on the graph. Another is through a fractional-order differential equation $L^{\alpha/2} (\tau u) = \mathcal{W}$ on $\Gamma$, where $L = \kappa^2 - \nabla(a\nabla)$ for (sufficiently nice) functions $\kappa, a$, and $\mathcal{W}$ is Gaussian white noise. We study Markov properties of these two types of fields. First, we show that no Gaussian random fields exist on general metric graphs that are both isotropic and Markov. Then, we show that the second type of fields, the generalized Whittle--Mat\'ern fields, are Markov if $\alpha\in\mathbb{N}$, and conversely, if $a$ and $\kappa$ are constant and $u$ is Markov, then $\alpha\in\mathbb{N}$. Further, if $\alpha\in\mathbb{N}$, a generalized Whittle--Mat\'ern field $u$ is Markov of order $\alpha$, which means that the field $u$ in one region $S\subset\Gamma$ is conditionally independent of $u$ in $\Gamma\setminus S$ given the values of $u$ and its $\alpha-1$ derivatives on $\partial S$. Finally, we provide two results as consequences of the theory developed: first we prove that the Markov property implies an explicit characterization of $u$ on a fixed edge $e$, revealing that the conditional distribution of $u$ on $e$ given the values at the two vertices connected to $e$ is independent of the geometry of $\Gamma$; second, we show that the solution to $L^{1/2}(\tau u) = \mathcal{W}$ on $\Gamma$ can obtained by conditioning independent generalized Whittle--Mat\'ern processes on the edges, with $\alpha=1$ and Neumann boundary conditions, on being continuous at the vertices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.03190v3</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Bolin, Alexandre B. Simas, Jonas Wallin</dc:creator>
    </item>
    <item>
      <title>A Survey on Statistical Theory of Deep Learning: Approximation, Training Dynamics, and Generative Models</title>
      <link>https://arxiv.org/abs/2401.07187</link>
      <description>arXiv:2401.07187v3 Announce Type: replace-cross 
Abstract: In this article, we review the literature on statistical theories of neural networks from three perspectives: approximation, training dynamics and generative models. In the first part, results on excess risks for neural networks are reviewed in the nonparametric framework of regression (and classification in Appendix~{\color{blue}B}). These results rely on explicit constructions of neural networks, leading to fast convergence rates of excess risks. Nonetheless, their underlying analysis only applies to the global minimizer in the highly non-convex landscape of deep neural networks. This motivates us to review the training dynamics of neural networks in the second part. Specifically, we review papers that attempt to answer ``how the neural network trained via gradient-based methods finds the solution that can generalize well on unseen data.'' In particular, two well-known paradigms are reviewed: the Neural Tangent Kernel (NTK) paradigm, and Mean-Field (MF) paradigm. Last but not least, we review the most recent theoretical advancements in generative models including Generative Adversarial Networks (GANs), diffusion models, and in-context learning (ICL) in the Large Language Models (LLMs) from two perpsectives reviewed previously, i.e., approximation and training dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07187v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Namjoon Suh, Guang Cheng</dc:creator>
    </item>
    <item>
      <title>Degree-heterogeneous Latent Class Analysis for High-dimensional Discrete Data</title>
      <link>https://arxiv.org/abs/2402.18745</link>
      <description>arXiv:2402.18745v3 Announce Type: replace-cross 
Abstract: The latent class model is a widely used mixture model for multivariate discrete data. Besides the existence of qualitatively heterogeneous latent classes, real data often exhibit additional quantitative heterogeneity nested within each latent class. The modern latent class analysis also faces extra challenges, including the high-dimensionality, sparsity, and heteroskedastic noise inherent in discrete data. Motivated by these phenomena, we introduce the Degree-heterogeneous Latent Class Model and propose an easy-to-implement HeteroClustering algorithm for it. HeteroClustering uses heteroskedastic PCA with l2 normalization to remove degree effects and perform clustering in the top singular subspace of the data matrix. We establish the result of exact clustering under minimal signal-to-noise conditions. We further investigate the estimation and inference of the high-dimensional continuous item parameters in the model, which are crucial to interpreting and finding useful markers for latent classes. We provide comprehensive procedures for global testing and multiple testing of these parameters with valid error controls. The superior performance of our methods is demonstrated through extensive simulations and applications to three diverse real-world datasets from political voting records, genetic variations, and single-cell sequencing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18745v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongyuan Lyu, Ling Chen, Yuqi Gu</dc:creator>
    </item>
    <item>
      <title>Asymptotic Expansions of the Limit Laws of Gaussian and Laguerre (Wishart) Ensembles at the Soft Edge</title>
      <link>https://arxiv.org/abs/2403.07628</link>
      <description>arXiv:2403.07628v3 Announce Type: replace-cross 
Abstract: The large-matrix limit laws of the rescaled largest eigenvalue of the orthogonal, unitary and symplectic $n$-dimensional Gaussian ensembles -- and of the corresponding Laguerre ensembles (Wishart distributions) for various regimes of the parameter $\alpha$ (degrees of freedom $p$) -- are known to be the Tracy-Widom distributions $F_\beta$ ($\beta=1,2,4$). We will establish (paying particular attention to large, or small, ratios $p/n$) that, with careful choices of the rescaling constants and the expansion parameter $h$, the limit laws embed into asymptotic expansions in powers of $h$, where $h \asymp n^{-2/3}$ resp. $h \asymp (n\,\wedge\,p)^{-2/3}$. We find explicit analytic expressions of the first few expansions terms as linear combinations, with rational polynomial coefficients, of higher order derivatives of the limit law $F_\beta$. With a proper parametrization, the expansions in the Gaussian cases can be understood, for given $n$, as the limit $p\to\infty$ of the Laguerre cases. Whereas the results for $\beta=2$ are presented with proof, the discussion of the cases $\beta=1,4$ is based on some hypotheses, focussing on the algebraic aspects of actually computing the polynomial coefficients. For the purposes of illustration and validation, the various results are checked against simulation data with large sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07628v3</guid>
      <category>math.PR</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Folkmar Bornemann</dc:creator>
    </item>
    <item>
      <title>Sharp bounds on aggregate expert error</title>
      <link>https://arxiv.org/abs/2407.16642</link>
      <description>arXiv:2407.16642v2 Announce Type: replace-cross 
Abstract: We revisit the classic problem of aggregating binary advice from conditionally independent experts, also known as the Naive Bayes setting. Our quantity of interest is the error probability of the optimal decision rule. In the case of symmetric errors (sensitivity = specificity), reasonably tight bounds on the optimal error probability are known. In the general asymmetric case, we are not aware of any nontrivial estimates on this quantity. Our contribution consists of sharp upper and lower bounds on the optimal error probability in the general case, which recover and sharpen the best known results in the symmetric special case. Since this turns out to be equivalent to estimating the total variation distance between two product distributions, our results also have bearing on this important and challenging problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16642v2</guid>
      <category>math.PR</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aryeh Kontorovich</dc:creator>
    </item>
  </channel>
</rss>
