<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Sep 2024 01:39:52 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Demystified: double robustness with nuisance parameters estimated at rate n-to-the-1/4</title>
      <link>https://arxiv.org/abs/2409.02320</link>
      <description>arXiv:2409.02320v1 Announce Type: new 
Abstract: Have you also been wondering what is this thing with double robustness and nuisance parameters estimated at rate n^(1/4)? It turns out that to understand this phenomenon one just needs the Middle Value Theorem (or a Taylor expansion) and some smoothness conditions. This note explains why under some fairly simple conditions, as long as the nuisance parameter theta in R^k is estimated at rate n^(1/4) or faster, 1. the resulting variance of the estimator of the parameter of interest psi in R^d does not depend on how the nuisance parameter theta is estimated, and 2. the sandwich estimator of the variance of psi-hat ignoring estimation of theta is consistent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02320v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Judith J. Lok</dc:creator>
    </item>
    <item>
      <title>Bulk Spectra of Truncated Sample Covariance Matrices</title>
      <link>https://arxiv.org/abs/2409.02911</link>
      <description>arXiv:2409.02911v1 Announce Type: new 
Abstract: Determinantal Point Processes (DPPs), which originate from quantum and statistical physics, are known for modelling diversity. Recent research [Ghosh and Rigollet (2020)] has demonstrated that certain matrix-valued $U$-statistics (that are truncated versions of the usual sample covariance matrix) can effectively estimate parameters in the context of Gaussian DPPs and enhance dimension reduction techniques, outperforming standard methods like PCA in clustering applications. This paper explores the spectral properties of these matrix-valued $U$-statistics in the null setting of an isotropic design. These matrices may be represented as $X L X^\top$, where $X$ is a data matrix and $L$ is the Laplacian matrix of a random geometric graph associated to $X$. The main mathematically interesting twist here is that the matrix $L$ is dependent on $X$. We give complete descriptions of the bulk spectra of these matrix-valued $U$-statistics in terms of the Stieltjes transforms of their empirical spectral measures. The results and the techniques are in fact able to address a broader class of kernelised random matrices, connecting their limiting spectra to generalised Mar\v{c}enko-Pastur laws and free probability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02911v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subhroshekhar Ghosh, Soumendu Sundar Mukherjee, Himasish Talukdar</dc:creator>
    </item>
    <item>
      <title>Personalized and uncertainty-aware coronary hemodynamics simulations: From Bayesian estimation to improved multi-fidelity uncertainty quantification</title>
      <link>https://arxiv.org/abs/2409.02247</link>
      <description>arXiv:2409.02247v1 Announce Type: cross 
Abstract: Simulations of coronary hemodynamics have improved non-invasive clinical risk stratification and treatment outcomes for coronary artery disease, compared to relying on anatomical imaging alone. However, simulations typically use empirical approaches to distribute total coronary flow amongst the arteries in the coronary tree. This ignores patient variability, the presence of disease, and other clinical factors. Further, uncertainty in the clinical data often remains unaccounted for in the modeling pipeline. We present an end-to-end uncertainty-aware pipeline to (1) personalize coronary flow simulations by incorporating branch-specific coronary flows as well as cardiac function; and (2) predict clinical and biomechanical quantities of interest with improved precision, while accounting for uncertainty in the clinical data. We assimilate patient-specific measurements of myocardial blood flow from CT myocardial perfusion imaging to estimate branch-specific coronary flows. We use adaptive Markov Chain Monte Carlo sampling to estimate the joint posterior distributions of model parameters with simulated noise in the clinical data. Additionally, we determine the posterior predictive distribution for relevant quantities of interest using a new approach combining multi-fidelity Monte Carlo estimation with non-linear, data-driven dimensionality reduction. Our framework recapitulates clinically measured cardiac function as well as branch-specific coronary flows under measurement uncertainty. We substantially shrink the confidence intervals for estimated quantities of interest compared to single-fidelity and state-of-the-art multi-fidelity Monte Carlo methods. This is especially true for quantities that showed limited correlation between the low- and high-fidelity model predictions. Moreover, the proposed estimators are significantly cheaper to compute for a specified confidence level or variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02247v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.CE</category>
      <category>math.ST</category>
      <category>physics.comp-ph</category>
      <category>physics.med-ph</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karthik Menon, Andrea Zanoni, Owais Khan, Gianluca Geraci, Koen Nieman, Daniele E. Schiavazzi, Alison L. Marsden</dc:creator>
    </item>
    <item>
      <title>Simulation-calibration testing for inference in Lasso regressions</title>
      <link>https://arxiv.org/abs/2409.02269</link>
      <description>arXiv:2409.02269v1 Announce Type: cross 
Abstract: We propose a test of the significance of a variable appearing on the Lasso path and use it in a procedure for selecting one of the models of the Lasso path, controlling the Family-Wise Error Rate. Our null hypothesis depends on a set A of already selected variables and states that it contains all the active variables. We focus on the regularization parameter value from which a first variable outside A is selected. As the test statistic, we use this quantity's conditional p-value, which we define conditional on the non-penalized estimated coefficients of the model restricted to A. We estimate this by simulating outcome vectors and then calibrating them on the observed outcome's estimated coefficients. We adapt the calibration heuristically to the case of generalized linear models in which it turns into an iterative stochastic procedure. We prove that the test controls the risk of selecting a false positive in linear models, both under the null hypothesis and, under a correlation condition, when A does not contain all active variables. We assess the performance of our procedure through extensive simulation studies. We also illustrate it in the detection of exposures associated with drug-induced liver injuries in the French pharmacovigilance database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02269v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthieu Pluntz, Cyril Dalmasso, Pascale Tubert-Bitter, Ismail Ahmed</dc:creator>
    </item>
    <item>
      <title>Fitting an Equation to Data Impartially</title>
      <link>https://arxiv.org/abs/2409.02573</link>
      <description>arXiv:2409.02573v1 Announce Type: cross 
Abstract: We consider the problem of fitting a relationship (e.g. a potential scientific law) to data involving multiple variables. Ordinary (least squares) regression is not suitable for this because the estimated relationship will differ according to which variable is chosen as being dependent, and the dependent variable is unrealistically assumed to be the only variable which has any measurement error (noise). We present a very general method for estimating a linear functional relationship between multiple noisy variables, which are treated impartially, i.e. no distinction between dependent and independent variables. The data are not assumed to follow any distribution, but all variables are treated as being equally reliable. Our approach extends the geometric mean functional relationship to multiple dimensions. This is especially useful with variables measured in different units, as it is naturally scale-invariant, whereas orthogonal regression is not. This is because our approach is not based on minimizing distances, but on the symmetric concept of correlation. The estimated coefficients are easily obtained from the covariances or correlations, and correspond to geometric means of associated least squares coefficients. The ease of calculation will hopefully allow widespread application of impartial fitting to estimate relationships in a neutral way.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02573v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>physics.data-an</category>
      <category>q-fin.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.3390/math11183957</arxiv:DOI>
      <arxiv:journal_reference>Mathematics, 11(18), 3957 (2023)</arxiv:journal_reference>
      <dc:creator>Chris Tofallis</dc:creator>
    </item>
    <item>
      <title>Convex Regression in Multidimensions: Suboptimality of Least Squares Estimators</title>
      <link>https://arxiv.org/abs/2006.02044</link>
      <description>arXiv:2006.02044v2 Announce Type: replace 
Abstract: Under the usual nonparametric regression model with Gaussian errors, Least Squares Estimators (LSEs) over natural subclasses of convex functions are shown to be suboptimal for estimating a $d$-dimensional convex function in squared error loss when the dimension $d$ is 5 or larger. The specific function classes considered include: (i) bounded convex functions supported on a polytope (in random design), (ii) Lipschitz convex functions supported on any convex domain (in random design), (iii) convex functions supported on a polytope (in fixed design). For each of these classes, the risk of the LSE is proved to be of the order $n^{-2/d}$ (up to logarithmic factors) while the minimax risk is $n^{-4/(d+4)}$, when $d \ge 5$. In addition, the first rate of convergence results (worst case and adaptive) for the unrestricted convex LSE are established in fixed-design for polytopal domains for all $d \geq 1$. Some new metric entropy results for convex functions are also proved which are of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2006.02044v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gil Kur, Fuchang Gao, Adityanand Guntuboyina, Bodhisattva Sen</dc:creator>
    </item>
    <item>
      <title>Nonparametric Bayesian estimation in a multidimensional diffusion model with high frequency data</title>
      <link>https://arxiv.org/abs/2211.12267</link>
      <description>arXiv:2211.12267v3 Announce Type: replace 
Abstract: We consider nonparametric Bayesian inference in a multidimensional diffusion model with reflecting boundary conditions based on discrete high-frequency observations. We prove a general posterior contraction rate theorem in $L^2$-loss, which is applied to Gaussian priors. The resulting posteriors, as well as their posterior means, are shown to converge to the ground truth at the minimax optimal rate over H\"older smoothness classes in any dimension. Of independent interest and as part of our proofs, we show that certain frequentist penalized least squares estimators are also minimax optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.12267v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marc Hoffmann, Kolyan Ray</dc:creator>
    </item>
    <item>
      <title>Bayesian Analysis of Generalized Hierarchical Indian Buffet Processes for Within and Across Group Sharing of Latent Features</title>
      <link>https://arxiv.org/abs/2304.05244</link>
      <description>arXiv:2304.05244v2 Announce Type: replace 
Abstract: Bayesian nonparametric hierarchical priors are highly effective in providing flexible models for latent data structures exhibiting sharing of information within and across groups. In this work, we focus on latent feature allocation models, where the data structures correspond to multi-sets or unbounded sparse matrices, which we refer to as generalized hierarchical Indian Buffet processes (HIBP). These are based on hierarchical versions of generalized spike and slab Indian Buffet processes (IBP), where the fundamental development in this regard is the Bernoulli-based HIBP, devised by Thibaux-Jordan (2007), as a hierarchical extension of the IBP devised by Griffiths-Ghahramani (2005). With a focus on Bayesian inference, we provide novel explicit descriptions of the joint, marginal, and posterior distributions of the HIBP, significantly advancing our understanding of these processes. Our results allow for exact sampling for the otherwise complex joint marginal distributions. We provide a general characterization of their posterior distributions as well as highlight bottlenecks for practical implementation. Our main focus then shifts to specific tractable results for the remarkable case of Poisson HIBP, which correspond to generalizations of mixed Poisson random count models arising in genetics, imaging, topic modeling, random occupancy, and species sampling models. We show they also have important relations to Bayesian nonparametric latent class models appearing in the literature. Furthermore, we show that all general HIBP may be coupled to Poisson HIBP, allowing for further analysis of such processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.05244v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lancelot Fitzgerald James, Juho Lee, Abhinav Pandey</dc:creator>
    </item>
    <item>
      <title>Large-scale Multiple Testing of Cross-covariance Functions with Applications to Functional Network Models</title>
      <link>https://arxiv.org/abs/2407.19399</link>
      <description>arXiv:2407.19399v2 Announce Type: replace 
Abstract: The estimation of functional networks through functional covariance and graphical models have recently attracted increasing attention in settings with high dimensional functional data, where the number of functional variables p is comparable to, and maybe larger than, the number of subjects. However, the existing methods all depend on regularization techniques, which make it unclear how the involved tuning parameters are related to the number of false edges. In this paper, we first reframe the functional covariance model estimation as a tuning-free problem of simultaneously testing p(p-1)/2 hypotheses for cross-covariance functions, and introduce a novel multiple testing procedure. We then explore the multiple testing procedure under a general error-contamination framework and establish that our procedure can control false discoveries asymptotically. Additionally, we demonstrate that our proposed methods for two concrete examples: the functional covariance model for discretely observed functional data and, importantly, the more challenging functional graphical model, can be seamlessly integrated into the general error-contamination framework, and, with verifiable conditions, achieve theoretical guarantees on effective false discovery control. Finally, we showcase the superiority of our proposals through extensive simulations and brain connectivity analysis of two neuroimaging datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19399v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qin Fang, Qing Jiang, Xinghao Qiao</dc:creator>
    </item>
    <item>
      <title>Rank-transformed subsampling: inference for multiple data splitting and exchangeable p-values</title>
      <link>https://arxiv.org/abs/2301.02739</link>
      <description>arXiv:2301.02739v3 Announce Type: replace-cross 
Abstract: Many testing problems are readily amenable to randomised tests such as those employing data splitting. However despite their usefulness in principle, randomised tests have obvious drawbacks. Firstly, two analyses of the same dataset may lead to different results. Secondly, the test typically loses power because it does not fully utilise the entire sample. As a remedy to these drawbacks, we study how to combine the test statistics or p-values resulting from multiple random realisations such as through random data splits. We develop rank-transformed subsampling as a general method for delivering large sample inference about the combined statistic or p-value under mild assumptions. We apply our methodology to a wide range of problems, including testing unimodality in high-dimensional data, testing goodness-of-fit of parametric quantile regression models, testing no direct effect in a sequentially randomised trial and calibrating cross-fit double machine learning confidence intervals. In contrast to existing p-value aggregation schemes that can be highly conservative, our method enjoys type-I error control that asymptotically approaches the nominal level. Moreover, compared to using the ordinary subsampling, we show that our rank transform can remove the first-order bias in approximating the null under alternatives and greatly improve power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.02739v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/jrsssb/qkae091</arxiv:DOI>
      <dc:creator>F. Richard Guo, Rajen D. Shah</dc:creator>
    </item>
    <item>
      <title>Structural Measures of Resilience for Supply Chains</title>
      <link>https://arxiv.org/abs/2303.12660</link>
      <description>arXiv:2303.12660v3 Announce Type: replace-cross 
Abstract: We investigate the structural factors that drive cascading failures in production networks, focusing on quantifying these risks with a topological resilience metric corresponding to the largest exogenous systemic shock that the production network can withstand, such that almost all of the network survives with high probability. We model failures using a node percolation process where systemic shocks cause suppliers to fail, leading to further breakdowns. We classify networks into two categories -- resilient and fragile -- based on their ability to handle shocks as the network grows large, and give bounds on their resilience. We show that the main factors affecting resilience are the number of raw products (primary sector), the number of final goods (final sector), and the source and supply dependencies. Further, we give methods to lower bound resilience based on bounding the cascade size with a linear program that can be efficiently calculated. We establish connections between our model, the independent cascade model, the Risk Exposure Index, and the Eisenberg-Noe contagion model. We give an almost linear-time deterministic algorithm to approximate the cascade size, which matches known lower bounds up to logarithmic factors. Finally, we design intervention algorithms and show that under reasonable assumptions, targeting nodes based on Katz centrality in the edge-reversed network is optimal. Finally, we account for network heterogeneities and validate our findings with real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.12660v3</guid>
      <category>cs.SI</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marios Papachristou, M. Amin Rahimian</dc:creator>
    </item>
    <item>
      <title>Bayesian Cram\'er-Rao Bound Estimation with Score-Based Models</title>
      <link>https://arxiv.org/abs/2309.16076</link>
      <description>arXiv:2309.16076v2 Announce Type: replace-cross 
Abstract: The Bayesian Cram\'er-Rao bound (CRB) provides a lower bound on the mean square error of any Bayesian estimator under mild regularity conditions. It can be used to benchmark the performance of statistical estimators, and provides a principled metric for system design and optimization. However, the Bayesian CRB depends on the underlying prior distribution, which is often unknown for many problems of interest. This work introduces a new data-driven estimator for the Bayesian CRB using score matching, i.e., a statistical estimation technique that models the gradient of a probability distribution from a given set of training data. The performance of the proposed estimator is analyzed in both the classical parametric modeling regime and the neural network modeling regime. In both settings, we develop novel non-asymptotic bounds on the score matching error and our Bayesian CRB estimator based on the results from empirical process theory, including classical bounds and recently introduced techniques for characterizing neural networks. We illustrate the performance of the proposed estimator with two application examples: a signal denoising problem and a dynamic phase offset estimation problem in communication systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.16076v2</guid>
      <category>stat.ML</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TIT.2024.3447552</arxiv:DOI>
      <dc:creator>Evan Scope Crafts, Xianyang Zhang, Bo Zhao</dc:creator>
    </item>
    <item>
      <title>Variance Inequalities for Transformed Fr\'echet Means in Hadamard Spaces</title>
      <link>https://arxiv.org/abs/2310.13668</link>
      <description>arXiv:2310.13668v2 Announce Type: replace-cross 
Abstract: The Fr\'echet mean (or barycenter) generalizes the expectation of a random variable to metric spaces by minimizing the expected squared distance to the random variable. Similarly, the median can be generalized by its property of minimizing the expected absolute distance. We consider the class of transformed Fr\'echet means with nondecreasing, convex transformations that have a concave derivative. This class includes the Fr\'echet median, the Fr\'echet mean, the Huber loss-induced Fr\'echet mean, and other statistics related to robust statistics in metric spaces. We study variance inequalities for these transformed Fr\'echet means. These inequalities describe how the expected transformed distance grows when moving away from a minimizer, i.e., from a transformed Fr\'echet mean. Variance inequalities are useful in the theory of estimation and numerical approximation of transformed Fr\'echet means. Our focus is on variance inequalities in Hadamard spaces - metric spaces with globally nonpositive curvature. Notably, some results are new also for Euclidean spaces. Additionally, we are able to characterize uniqueness of transformed Fr\'echet means, in particular of the Fr\'echet median.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13668v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christof Sch\"otz</dc:creator>
    </item>
  </channel>
</rss>
