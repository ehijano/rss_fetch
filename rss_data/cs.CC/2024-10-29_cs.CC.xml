<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Oct 2024 02:04:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Communication Complexity of Approximating Matrix Rank</title>
      <link>https://arxiv.org/abs/2410.20094</link>
      <description>arXiv:2410.20094v1 Announce Type: new 
Abstract: We fully determine the communication complexity of approximating matrix rank, over any finite field $\mathbb{F}$. We study the most general version of this problem, where $0\leq r&lt;R\leq n$ are given integers, Alice and Bob's inputs are matrices $A,B\in\mathbb{F}^{n\times n}$, respectively, and they need to distinguish between the cases $\mathrm{rk}(A+B)=r$ and $\mathrm{rk}(A+B)=R$. We show that this problem has randomized communication complexity $\Omega(1+r^{2}\log|\mathbb{F}|)$. This is optimal in a strong sense because $O(1+r^{2}\log|\mathbb{F}|)$ communication is sufficient to determine, for arbitrary $A,B$, whether $\mathrm{rk}(A+B)\leq r$. Prior to our work, lower bounds were known only for consecutive integers $r$ and $R$, with no implication for the approximation of matrix rank. Our lower bound holds even for quantum protocols and even for error probability $\frac{1}{2}-\frac{1}{4}|\mathbb{F}|^{-r/3}$, which too is virtually optimal because the problem has a two-bit classical protocol with error $\frac{1}{2}-\Theta(|\mathbb{F}|^{-r})$.
  As an application, we obtain an $\Omega(\frac{1}{k}\cdot n^{2}\log|\mathbb{F}|)$ space lower bound for any streaming algorithm with $k$ passes that approximates the rank of an input matrix $M\in\mathbb{F}^{n\times n}$ within a factor of $\sqrt{2}-\delta$, for any $\delta&gt;0$. Our result is an exponential improvement in $k$ over previous work.
  We also settle the randomized and quantum communication complexity of several other linear-algebraic problems, for all settings of parameters. This includes the determinant problem (given matrices $A$ and $B$, distinguish between the cases $\mathrm{det}(A+B)=a$ and $\mathrm{det}(A+B)=b$, for fixed field elements $a\ne b)$ and the subspace sum and subspace intersection problem (given subspaces $S$ and $T$ of known dimensions $m$ and $\ell$, respectively, approximate the dimensions of $S+T$ and $S\cap T$).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20094v1</guid>
      <category>cs.CC</category>
      <category>quant-ph</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander A. Sherstov, Andrey A. Storozhenko</dc:creator>
    </item>
    <item>
      <title>Solving Polynomial Equations Over Finite Fields</title>
      <link>https://arxiv.org/abs/2410.20162</link>
      <description>arXiv:2410.20162v1 Announce Type: new 
Abstract: We present a randomized algorithm for solving low-degree polynomial equation systems over finite fields faster than exhaustive search. In order to do so, we follow a line of work by Lokshtanov, Paturi, Tamaki, Williams, and Yu (SODA 2017), Bj\"orklund, Kaski, and Williams (ICALP 2019), and Dinur (SODA 2021). In particular, we generalize Dinur's algorithm for $\mathbb{F}_2$ to all finite fields, in particular the "symbolic interpolation" of Bj\"orklund, Kaski, and Williams, and we use an efficient trimmed multipoint evaluation and interpolation procedure for multivariate polynomials over finite fields by Van der Hoeven and Schost (AAECC 2013). The running time of our algorithm matches that of Dinur's algorithm for $\mathbb{F}_2$ and is significantly faster than the one of Lokshtanov et al. for $q&gt;2$.
  We complement our results with tight conditional lower bounds that, surprisingly, we were not able to find in the literature. In particular, under the strong exponential time hypothesis, we prove that it is impossible to solve $n$-variate low-degree polynomial equation systems over $\mathbb{F}_q$ in time $O((q-\varepsilon)^{n})$. As a bonus, we show that under the counting version of the strong exponential time hypothesis, it is impossible to compute the number of roots of a single $n$-variate low-degree polynomial over $\mathbb{F}_q$ in time ${O((q-\varepsilon)^{n})}$; this generalizes a result of Williams (SOSA 2018) from $\mathbb{F}_2$ to all finite fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20162v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Holger Dell, Anselm Haak, Melvin Kallmayer, Leo Wennmann</dc:creator>
    </item>
    <item>
      <title>Improving the Leading Constant of Matrix Multiplication</title>
      <link>https://arxiv.org/abs/2410.20538</link>
      <description>arXiv:2410.20538v1 Announce Type: new 
Abstract: Algebraic matrix multiplication algorithms are designed by bounding the rank of matrix multiplication tensors, and then using a recursive method. However, designing algorithms in this way quickly leads to large constant factors: if one proves that the tensor for multiplying $n \times n$ matrices has rank $\leq t$, then the resulting recurrence shows that $M \times M$ matrices can be multiplied using $O(n^2 \cdot M^{\log_n t})$ operations, where the leading constant scales proportionally to $n^2$. Even modest increases in $n$ can blow up the leading constant too much to be worth the slight decrease in the exponent of $M$. Meanwhile, the asymptotically best algorithms use very large $n$, such that $n^2$ is larger than the number of atoms in the visible universe!
  In this paper, we give new ways to use tensor rank bounds to design matrix multiplication algorithms, which lead to smaller leading constants than the standard recursive method. Our main result shows that, if the tensor for multiplying $n \times n$ matrices has rank $\leq t$, then $M \times M$ matrices can be multiplied using only $n^{O(1/(\log n)^{0.33})} \cdot M^{\log_n t}$ operations. In other words, we improve the leading constant in general from $O(n^2)$ to $n^{O(1/(\log n)^{0.33})} &lt; n^{o(1)}$. We then apply this and further improve the leading constant in a number of situations of interest. We show that, in the popularly-conjectured case where $\omega=2$, a new, different recursive approach can lead to an improvement. We also show that the leading constant of the current asymptotically fastest matrix multiplication algorithm, and any algorithm designed using the group-theoretic method, can be further improved by taking advantage of additional structure of the underlying tensor identities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20538v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Josh Alman, Hantao Yu</dc:creator>
    </item>
    <item>
      <title>A universal bound on the space complexity of Directed Acyclic Graph computations</title>
      <link>https://arxiv.org/abs/2410.20547</link>
      <description>arXiv:2410.20547v1 Announce Type: new 
Abstract: It is shown that $S(G) = O\left(m/\log_2 m + d\right)$ pebbles are sufficient to pebble any DAG $G=(V,E)$, with $m$ edges and maximum in-degree $d$. It was previously known that $S(G) = O\left(d n/\log n\right)$. The result builds on two novel ideas. The first is the notion of $B-budget\ decomposition$ of a DAG $G$, an efficiently computable partition of $G$ into at most $2^{\lfloor \frac{m}{B} \rfloor}$ sub-DAGs, whose cumulative space requirement is at most $B$. The second is the challenging vertices technique, which constructs a pebbling schedule for $G$ from a pebbling schedule for a simplified DAG $G'$, obtained by removing from $G$ a selected set of vertices $W$ and their incident edges. This technique also yields improved pebbling upper bounds for DAGs with bounded genus and for DAGs with bounded topological depth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20547v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gianfranco Bilardi, Lorenzo De Stefani</dc:creator>
    </item>
    <item>
      <title>Improved separation between quantum and classical computers for sampling and functional tasks</title>
      <link>https://arxiv.org/abs/2410.20935</link>
      <description>arXiv:2410.20935v1 Announce Type: cross 
Abstract: This paper furthers existing evidence that quantum computers are capable of computations beyond classical computers. Specifically, we strengthen the collapse of the polynomial hierarchy to the second level if: (i) Quantum computers with postselection are as powerful as classical computers with postselection ($\mathsf{PostBQP=PostBPP}$), (ii) any one of several quantum sampling experiments ($\mathsf{BosonSampling}$, $\mathsf{IQP}$, $\mathsf{DQC1}$) can be approximately performed by a classical computer (contingent on existing assumptions). This last result implies that if any of these experiment's hardness conjectures hold, then quantum computers can implement functions classical computers cannot ($\mathsf{FBQP\neq FBPP}$) unless the polynomial hierarchy collapses to its 2nd level. These results are an improvement over previous work which either achieved a collapse to the third level or were concerned with exact sampling, a physically impractical case.
  The workhorse of these results is a new technical complexity-theoretic result which we believe could have value beyond quantum computation. In particular, we prove that if there exists an equivalence between problems solvable with an exact counting oracle and problems solvable with an approximate counting oracle, then the polynomial hierarchy collapses to its second level, indeed to $\mathsf{ZPP^{NP}}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20935v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Simon C. Marshall, Scott Aaronson, Vedran Dunjko</dc:creator>
    </item>
    <item>
      <title>Matrix-by-matrix multiplication algorithm with $O(N^2log_2N)$ computational complexity for variable precision arithmetic</title>
      <link>https://arxiv.org/abs/2410.21050</link>
      <description>arXiv:2410.21050v1 Announce Type: cross 
Abstract: We show that assuming the availability of the processor with variable precision arithmetic, we can compute matrix-by-matrix multiplications in $O(N^2log_2N)$ computational complexity. We replace the standard matrix-by-matrix multiplications algorithm $\begin{bmatrix}A_{11}&amp;A_{12}\\A_{21}&amp;A_{22}\end{bmatrix}\begin{bmatrix}B_{11}&amp;B_{12}\\B_{21}&amp;B_{22}\end{bmatrix}=\begin{bmatrix}A_{11}B_{11}+A_{12}B_{21}&amp;A_{11}B_{12}+A_{12}B_{22}\\A_{21}B_{11}+A_{22}B_{21}&amp;A_{21}B_{12}+A_{22}B_{22}\end{bmatrix}$ by $\begin{bmatrix}A_{11}&amp;A_{12}\\A_{21}&amp;A_{22}\end{bmatrix}\begin{bmatrix}B_{11}&amp;B_{12}\\B_{21}&amp;B_{22}\end{bmatrix}=\Bigl\lfloor\begin{bmatrix} (A_{11}+\epsilon A_{12})(B_{11}+1/{\epsilon}B_{21})&amp;(A_{11}+\epsilon A_{12})(B_{12}+1/{\epsilon}B_{22})\\(A_{21}+\epsilon A_{22})(B_{11}+1/{\epsilon}B_{21})&amp;(A_{21}+\epsilon A_{22})(B_{12}+1/{\epsilon}B_{22})\end{bmatrix}\Bigr\rfloor \mod \frac{1}{\epsilon}$. The resulting computational complexity for $N\times N$ matrices can be estimated from recursive equation $T(N)=4(N/2)^2$ (multiplication of a matrix by number)+$4(N/2)^2$ (additions of matrices)+$2N^2$ (floor and modulo)+$4T(N/2)$ (recursive calls) as $O(N^2log_2N)$. The novelty of the method lies in the observation, somehow ignored by other matrix-by-matrix multiplication algorithms, that we can multiply matrix entries by non-integer numbers to improve computational complexity. In other words, while having a processor that can compute multiplications, additions, modulo and floor operations with variable precision arithmetic in $O(1)$, we can obtain a matrix-by-matrix multiplication algorithm with $O(N^2log_2N)$ computational complexity. We also present a MATLAB code using VPA variable precision arithmetic emulator that can multiply matrices of size $N\times N$ using $(4log_2N+1)N^2$ variable precision arithmetic operations. This emulator uses $O(N)$ digits to run our algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21050v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.MS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maciej Paszy\'nski</dc:creator>
    </item>
    <item>
      <title>Sample-Optimal Quantum Estimators for Pure-State Trace Distance and Fidelity via Samplizer</title>
      <link>https://arxiv.org/abs/2410.21201</link>
      <description>arXiv:2410.21201v1 Announce Type: cross 
Abstract: Trace distance and infidelity (induced by square root fidelity), as basic measures of the closeness of quantum states, are commonly used in quantum state discrimination, certification, and tomography. However, the sample complexity for their estimation still remains open. In this paper, we solve this problem for pure states. We present a quantum algorithm that estimates the trace distance and square root fidelity between pure states to within additive error $\varepsilon$, given sample access to their identical copies. Our algorithm achieves the optimal sample complexity $\Theta(1/\varepsilon^2)$, improving the long-standing folklore $O(1/\varepsilon^4)$. Our algorithm is composed of a samplized phase estimation of the product of two Householder reflections. Notably, an improved (multi-)samplizer for pure states is used as an algorithmic tool in our construction, through which any quantum query algorithm using $Q$ queries to the reflection operator about a pure state $|\psi\rangle$ can be converted to a $\delta$-close (in the diamond norm) quantum sample algorithm using $\Theta(Q^2/\delta)$ samples of $|\psi\rangle$. This samplizer for pure states is shown to be optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21201v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qisheng Wang, Zhicheng Zhang</dc:creator>
    </item>
    <item>
      <title>Quantum computing and persistence in topological data analysis</title>
      <link>https://arxiv.org/abs/2410.21258</link>
      <description>arXiv:2410.21258v1 Announce Type: cross 
Abstract: Topological data analysis (TDA) aims to extract noise-robust features from a data set by examining the number and persistence of holes in its topology. We show that a computational problem closely related to a core task in TDA -- determining whether a given hole persists across different length scales -- is $\mathsf{BQP}_1$-hard and contained in $\mathsf{BQP}$. This result implies an exponential quantum speedup for this problem under standard complexity-theoretic assumptions. Our approach relies on encoding the persistence of a hole in a variant of the guided sparse Hamiltonian problem, where the guiding state is constructed from a harmonic representative of the hole.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21258v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Casper Gyurik, Alexander Schmidhuber, Robbie King, Vedran Dunjko, Ryu Hayakawa</dc:creator>
    </item>
    <item>
      <title>Exponential Lower Bounds for Smooth 3-LCCs and Sharp Bounds for Designs</title>
      <link>https://arxiv.org/abs/2404.06513</link>
      <description>arXiv:2404.06513v2 Announce Type: replace 
Abstract: We give improved lower bounds for binary $3$-query locally correctable codes (3-LCCs) $C \colon \{0,1\}^k \rightarrow \{0,1\}^n$. Specifically, we prove:
  (1) If $C$ is a linear design 3-LCC, then $n \geq 2^{(1 - o(1))\sqrt{k} }$. A design 3-LCC has the additional property that the correcting sets for every codeword bit form a perfect matching and every pair of codeword bits is queried an equal number of times across all matchings. Our bound is tight up to a factor $\sqrt{8}$ in the exponent of $2$, as the best construction of binary $3$-LCCs (obtained by taking Reed-Muller codes on $\mathbb{F}_4$ and applying a natural projection map) is a design $3$-LCC with $n \leq 2^{\sqrt{8 k}}$. Up to a $\sqrt{8}$ factor, this resolves the Hamada conjecture on the maximum $\mathbb{F}_2$-codimension of a $4$-design.
  (2) If $C$ is a smooth, non-linear, adaptive $3$-LCC with perfect completeness, then, $n \geq 2^{\Omega(k^{1/5})}$.
  (3) If $C$ is a smooth, non-linear, adaptive $3$-LCC with completeness $1 - \varepsilon$, then $n \geq \tilde{\Omega}(k^{\frac{1}{2\varepsilon}})$. In particular, when $\varepsilon$ is a small constant, this implies a lower bound for general non-linear LCCs that beats the prior best $n \geq \tilde{\Omega}(k^3)$ lower bound of [AGKM23] by a polynomial factor.
  Our design LCC lower bound is obtained via a fine-grained analysis of the Kikuchi matrix method applied to a variant of the matrix used in [KM23]. Our lower bounds for non-linear codes are obtained by designing a from-scratch reduction from nonlinear $3$-LCCs to a system of "chain XOR equations": polynomial equations with similar structure to the long chain derivations that arise in the lower bounds for linear $3$-LCCs [KM23].</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06513v2</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pravesh K. Kothari, Peter Manohar</dc:creator>
    </item>
    <item>
      <title>Inapproximability of Sparsest Vector in a Real Subspace</title>
      <link>https://arxiv.org/abs/2410.02636</link>
      <description>arXiv:2410.02636v2 Announce Type: replace 
Abstract: We establish strong inapproximability for finding the sparsest nonzero vector in a real subspace. We show that it is NP-Hard (under randomized reductions) to approximate the sparsest vector in a subspace within any constant factor (or almost polynomial factors in quasipolynomial time). We recover as a corollary state of the art inapproximability for the shortest vector problem (SVP), a foundational problem in lattice based cryptography. Our proof is surprisingly simple, bypassing even the PCP theorem.
  We are inspired by the homogenization framework from the inapproximability theory of minimum distance problems (MDC) in integer lattices and error correcting codes. We use a combination of (a) \emph{product testing via tensor codes} and (b) \emph{encoding an assignment as a coset of a random code in higher dimensional space} in order to embed non-homogeneous quadratic equations into the sparsest vector problem. (a) is inspired by Austrin and Khot's simplified proof of hardness of MDC over finite fields, and (b) is inspired by Micciancio's semi-derandomization of hardness of SVP. Our reduction involves the challenge of performing (a) over the reals. We prove that tensoring of the kernel of a +1/-1 random matrix furnishes an adequate product test (while still allowing (b)). The proof exposes a connection to Littlewood-Offord theory and relies on a powerful anticoncentration result of Rudelson and Vershynin.
  Our main motivation in this work is the development of inapproximability theory for problems over the reals. Analytic variants of sparsest vector have connections to small set expansion, quantum separability and polynomial maximization over convex sets, all of which cause similar barriers to inapproximability. The approach we develop could lead to progress on the hardness of some of these problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02636v2</guid>
      <category>cs.CC</category>
      <category>cs.CR</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vijay Bhattiprolu, Euiwoong Lee</dc:creator>
    </item>
    <item>
      <title>On the exact quantum query complexity of $\text{MOD}_m^n$ and $\text{EXACT}_{k,l}^n$</title>
      <link>https://arxiv.org/abs/2303.10935</link>
      <description>arXiv:2303.10935v5 Announce Type: replace-cross 
Abstract: The query model has generated considerable interest in both classical and quantum computing communities. Typically, quantum advantages are demonstrated by showcasing a quantum algorithm with a better query complexity compared to its classical counterpart. Exact quantum query algorithms play a pivotal role in developing quantum algorithms. For example, the Deutsch-Jozsa algorithm demonstrated exponential quantum advantages over classical deterministic algorithms. As an important complexity measure, exact quantum query complexity describes the minimum number of queries required to solve a specific problem exactly using a quantum algorithm.
  In this paper, we consider the exact quantum query complexity of the following two $n$-bit symmetric functions $\text{MOD}_m^n:\{0,1\}^n \rightarrow \{0,...,m-1\}$ and $\text{EXACT}_{k,l}^n:\{0,1\}^n \rightarrow \{0,1\}$, which are defined as $\text{MOD}_m^n(x) = |x| \bmod m$ and $ \text{EXACT}_{k,l}^n(x) = 1$ iff $|x| \in \{k,l\}$, where $|x|$ is the number of $1$'s in $x$. Our results are as follows: i) We present an optimal quantum algorithm for computing $\text{MOD}_m^n$, achieving a query complexity of $\lceil n(1-\frac{1}{m}) \rceil$ for $1 &lt; m \le n$. This settles a conjecture proposed by Cornelissen, Mande, Ozols and de Wolf (2021). Based on this algorithm, we show the exact quantum query complexity of a broad class of symmetric functions that map $\{0,1\}^n$ to a finite set $X$ is less than $n$. ii) When $l-k \ge 2$, we give an optimal exact quantum query algorithm to compute $\text{EXACT}_{k,l}^n$ for the case $k=0$ or $k=1,l=n-1$. This resolves the conjecture proposed by Ambainis, Iraids and Nagaj (2017) partially.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.10935v5</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Penghui Yao, Zekun Ye</dc:creator>
    </item>
    <item>
      <title>Seeding with Differentially Private Network Information</title>
      <link>https://arxiv.org/abs/2305.16590</link>
      <description>arXiv:2305.16590v3 Announce Type: replace-cross 
Abstract: In public health interventions such as distributing pre-exposure prophylaxis (PrEP) for HIV prevention, decision makers rely on seeding algorithms to identify key individuals who can amplify the impact of their interventions. In such cases, constructing a complete sexual activity network is often infeasible due to privacy concerns. Instead, contact tracing can provide influence samples, i.e., sequences of sexual contacts without requiring complete network information. This brings two challenges: protecting individual privacy in the contact data and adapting seeding algorithms to work effectively with incomplete network information. To solve these two problems, we study privacy guarantees for influence maximization algorithms when the social network is unknown and the inputs are samples of prior influence cascades that are collected at random and need privacy protection. Building on recent results that address seeding with costly network information, our privacy-preserving algorithms introduce randomization in the collected data or the algorithm output, and can bound the privacy loss of each node (or group of nodes) in deciding to include their data in the algorithm input. We provide theoretical guarantees of seeding performance with a limited sample size subject to differential privacy budgets in both central and local privacy regimes. Simulations on synthetic random graphs and empirically grounded sexual contacts of men who have sex with men reveal the diminishing value of network information with decreasing privacy budget in both regimes and graceful decrease in performance with decreasing privacy budget in the central regime. Achieving good performance with local privacy guarantees requires relatively higher privacy budgets that confirm our theoretical expectations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.16590v3</guid>
      <category>cs.SI</category>
      <category>cs.CC</category>
      <category>cs.MA</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. Amin Rahimian, Fang-Yi Yu, Yuxin Liu, Carlos Hurtado</dc:creator>
    </item>
    <item>
      <title>Quantum Fast Implementation of Functional Bootstrapping and Private Information Retrieval</title>
      <link>https://arxiv.org/abs/2409.20182</link>
      <description>arXiv:2409.20182v3 Announce Type: replace-cross 
Abstract: Classical privacy-preserving computation techniques safeguard sensitive data in cloud computing, but often suffer from low computational efficiency. In this paper, we show that employing a single quantum server can significantly enhance both the efficiency and security of privacy-preserving computation.
  We propose an efficient quantum algorithm for functional bootstrapping of large-precision plaintexts, reducing the time complexity from exponential to polynomial in plaintext-size compared to classical algorithms. To support general functional bootstrapping, we design a fast quantum private information retrieval (PIR) protocol with logarithmic query time. The security relies on the learning with errors (LWE) problem with polynomial modulus, providing stronger security than classical ``exponentially fast'' PIR protocol based on ring-LWE with super-polynomial modulus.
  Technically, we extend a key classical homomorphic operation, known as blind rotation, to the quantum setting through encrypted conditional rotation. Underlying our extension are insights for the quantum extension of polynomial-based cryptographic tools that may gain dramatic speedups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20182v3</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.CR</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guangsheng Ma, Hongbo Li</dc:creator>
    </item>
    <item>
      <title>Recognizing Sumsets is NP-Complete</title>
      <link>https://arxiv.org/abs/2410.18661</link>
      <description>arXiv:2410.18661v2 Announce Type: replace-cross 
Abstract: Sumsets are central objects in additive combinatorics. In 2007, Granville asked whether one can efficiently recognize whether a given set $S$ is a sumset, i.e. whether there is a set $A$ such that $A+A=S$. Granville suggested an algorithm that takes exponential time in the size of the given set, but can we do polynomial or even linear time? This basic computational question is indirectly asking a fundamental structural question: do the special characteristics of sumsets allow them to be efficiently recognizable? In this paper, we answer this question negatively by proving that the problem is NP-complete. Specifically, our results hold for integer sets and over any finite field. Assuming the Exponential Time Hypothesis, our lower bound becomes $2^{\Omega(n^{1/4})}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18661v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amir Abboud, Nick Fischer, Ron Safier, Nathan Wallheimer</dc:creator>
    </item>
  </channel>
</rss>
