<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 02 Apr 2025 02:02:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Training in translation tools and technologies: Findings of the EMT survey 2023</title>
      <link>https://arxiv.org/abs/2503.22735</link>
      <description>arXiv:2503.22735v1 Announce Type: new 
Abstract: This article reports on the third iteration of a survey of computerized tools and technologies taught as part of postgraduate translation training programmes. While the survey was carried out under the aegis of the EMT Network, more than half of responses are from outside that network. The results show the responsiveness of programmes to innovations in translation technology, with increased compulsory inclusion of machine translation, post-editing, and quality evaluation, and a rapid response to the release of generative tools. The flexibility required during the Covid-19 pandemic has also led to some lasting changes to programmes. While the range of tools being taught has continued to expand, programmes seem to be consolidating their core offering around cloud-based software with cost-free academic access. There has also been an increase in the embedding of professional contexts and workflows associated with translation technology. Generic file management and data security skills have increased in perceived importance, and legal and ethical issues related to translation data have also become more prominent. In terms of course delivery the shift away from conventional labs identified in EMT2017 has accelerated markedly, no doubt partly driven by the pandemic, accompanied by a dramatic expansion in the use of students' personal devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22735v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andrew Rothwell, Joss Moorkens, Tomas Svoboda</dc:creator>
    </item>
    <item>
      <title>Concept Map Assessment Through Structure Classification</title>
      <link>https://arxiv.org/abs/2503.22741</link>
      <description>arXiv:2503.22741v1 Announce Type: new 
Abstract: Due to their versatility, concept maps are used in various educational settings and serve as tools that enable educators to comprehend students' knowledge construction. An essential component for analyzing a concept map is its structure, which can be categorized into three distinct types: spoke, network, and chain. Understanding the predominant structure in a map offers insights into the student's depth of comprehension of the subject. Therefore, this study examined 317 distinct concept map structures, classifying them into one of the three types, and used statistical and descriptive information from the maps to train multiclass classification models. As a result, we achieved an 86\% accuracy in classification using a Decision Tree. This promising outcome can be employed in concept map assessment systems to provide real-time feedback to the student.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22741v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>La\'is P. V. Vossen, Isabela Gasparini, Elaine H. T. Oliveira, Berrit Czinczel, Ute Harms, Lukas Menzel, Sebastian Gombert, Knut Neumann, Hendrik Drachsler</dc:creator>
    </item>
    <item>
      <title>Towards an intelligent assessment system for evaluating the development of algorithmic thinking skills: An exploratory study in Swiss compulsory schools</title>
      <link>https://arxiv.org/abs/2503.22756</link>
      <description>arXiv:2503.22756v1 Announce Type: new 
Abstract: The rapid digitalisation of contemporary society has profoundly impacted various facets of our lives, including healthcare, communication, business, and education. The ability to engage with new technologies and solve problems has become crucial, making CT skills, such as pattern recognition, decomposition, and algorithm design, essential competencies. In response, Switzerland is conducting research and initiatives to integrate CT into its educational system. This study aims to develop a comprehensive framework for large-scale assessment of CT skills, particularly focusing on AT, the ability to design algorithms. To achieve this, we first developed a competence model capturing the situated and developmental nature of CT, guiding the design of activities tailored to cognitive abilities, age, and context. This framework clarifies how activity characteristics influence CT development and how to assess these competencies. Additionally, we developed an activity for large-scale assessment of AT skills, offered in two variants: one based on non-digital artefacts (unplugged) and manual expert assessment, and the other based on digital artefacts (virtual) and automatic assessment. To provide a more comprehensive evaluation of students' competencies, we developed an IAS based on BNs with noisy gates, which offers real-time probabilistic assessment for each skill rather than a single overall score. The results indicate that the proposed instrument can measure AT competencies across different age groups and educational contexts in Switzerland, demonstrating its applicability for large-scale use. AT competencies exhibit a progressive development, with no overall gender differences, though variations are observed at the school level, significantly influenced by the artefact-based environment and its context, underscoring the importance of creating accessible and adaptable assessment tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22756v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giorgia Adorni</dc:creator>
    </item>
    <item>
      <title>Randomization to Reduce Terror Threats at Large Venues</title>
      <link>https://arxiv.org/abs/2503.22763</link>
      <description>arXiv:2503.22763v1 Announce Type: new 
Abstract: Can randomness be better than scheduled practices, for securing an event at a large venue such as a stadium or entertainment arena? Perhaps surprisingly, from several perspectives the answer is "yes." This note examines findings from an extensive study of the problem, including interviews and a survey of selected venue security directors. That research indicates that: randomness has several goals; many security directors recognize its potential; but very few have used it much, if at all. Some fear they will not be able to defend using random methods if an adversary does slip through security. Others are concerned that staff may not be able to perform effectively. We discuss ways in which it appears that randomness can improve effectiveness, ways it can be effectively justified to those who must approve security processes, and some potential research or regulatory advances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22763v1</guid>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul B. Kantor, Fred S. Roberts</dc:creator>
    </item>
    <item>
      <title>MediTools -- Medical Education Powered by LLMs</title>
      <link>https://arxiv.org/abs/2503.22769</link>
      <description>arXiv:2503.22769v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) has been advancing rapidly and with the advent of large language models (LLMs) in late 2022, numerous opportunities have emerged for adopting this technology across various domains, including medicine. These innovations hold immense potential to revolutionize and modernize medical education. Our research project leverages large language models to enhance medical education and address workflow challenges through the development of MediTools - AI Medical Education. This prototype application focuses on developing interactive tools that simulate real-life clinical scenarios, provide access to medical literature, and keep users updated with the latest medical news. Our first tool is a dermatology case simulation tool that uses real patient images depicting various dermatological conditions and enables interaction with LLMs acting as virtual patients. This platform allows users to practice their diagnostic skills and enhance their clinical decision-making abilities. The application also features two additional tools: an AI-enhanced PubMed tool for engaging with LLMs to gain deeper insights into research papers, and a Google News tool that offers LLM generated summaries of articles for various medical specialties. A comprehensive survey has been conducted among medical professionals and students to gather initial feedback on the effectiveness and user satisfaction of MediTools, providing insights for further development and refinement of the application. This research demonstrates the potential of AI-driven tools in transforming and revolutionizing medical education, offering a scalable and interactive platform for continuous learning and skill development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22769v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amr Alshatnawi, Remi Sampaleanu, David Liebovitz</dc:creator>
    </item>
    <item>
      <title>AI Family Integration Index (AFII): Benchmarking a New Global Readiness for AI as Family</title>
      <link>https://arxiv.org/abs/2503.22772</link>
      <description>arXiv:2503.22772v1 Announce Type: new 
Abstract: As Artificial Intelligence (AI) systems increasingly permeate caregiving, educational, and emotionally sensitive domains, there is a growing need to assess national readiness beyond infrastructure and innovation capacity. Existing indices such as the Stanford AI Index (2024), overlooked relational, ethical, and cultural dimensions essential to human centered AI integration. To address this blind spot, this study introduces the AI Family Integration Index (AFII), a ten dimensional benchmarking framework that evaluates national preparedness for integrating emotionally intelligent AI into family and caregiving systems. Using mixed-method analysis and equal weighting, the AFII provides a multidimensional tool for assessing emotional and symbolic readiness in diverse cultural contexts. A core insight is the policy practice gap: while many governments articulate ethical AI principles, few have implemented them effectively in relational or caregiving domains. Countries like Singapore, Japan, and South Korea demonstrate alignment between policy intent and caregiving integration, while others such as the United States and France, exhibit advanced policy rhetoric but slower real-world execution. This dissonance is captured through the AFII Governance Gap Lens. The AFII also reveals divergence from conventional rankings: technological leaders like the U.S. and China score high in the Stanford AI Index yet rank lower in AFII due to weaker caregiving alignment. In contrast, nations like Sweden and Singapore outperform on relational readiness despite moderate technical rankings. For policymakers, the AFII offers a practical, scalable, and ethically grounded tool to guide inclusive AI strategies, reframing readiness to center care, emotional safety, and cultural legitimacy in the age of relational AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22772v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prashant Mahajan</dc:creator>
    </item>
    <item>
      <title>Achieving Socio-Economic Parity through the Lens of EU AI Act</title>
      <link>https://arxiv.org/abs/2503.23056</link>
      <description>arXiv:2503.23056v1 Announce Type: new 
Abstract: Unfair treatment and discrimination are critical ethical concerns in AI systems, particularly as their adoption expands across diverse domains. Addressing these challenges, the recent introduction of the EU AI Act establishes a unified legal framework to ensure legal certainty for AI innovation and investment while safeguarding public interests, such as health, safety, fundamental rights, democracy, and the rule of law (Recital 8). The Act encourages stakeholders to initiate dialogue on existing AI fairness notions to address discriminatory outcomes of AI systems. However, these notions often overlook the critical role of Socio-Economic Status (SES), inadvertently perpetuating biases that favour the economically advantaged. This is concerning, given that principles of equalization advocate for equalizing resources or opportunities to mitigate disadvantages beyond an individual's control. While provisions for discrimination are laid down in the AI Act, specialized directions should be broadened, particularly in addressing economic disparities perpetuated by AI systems. In this work, we explore the limitations of popular AI fairness notions using a real-world dataset (Adult), highlighting their inability to address SES-driven disparities. To fill this gap, we propose a novel fairness notion, Socio-Economic Parity (SEP), which incorporates SES and promotes positive actions for underprivileged groups while accounting for factors within an individual's control, such as working hours, which can serve as a proxy for effort. We define a corresponding fairness measure and optimize a model constrained by SEP to demonstrate practical utility. Our results show the effectiveness of SEP in mitigating SES-driven biases. By analyzing the AI Act alongside our method, we lay a foundation for aligning AI fairness with SES factors while ensuring legal compliance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23056v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arjun Roy, Stavroula Rizuo, Symeon Papadopoulos, Eirini Ntoutsi</dc:creator>
    </item>
    <item>
      <title>Optimizing Library Usage and Browser Experience: Application to the New York Public Library</title>
      <link>https://arxiv.org/abs/2503.23118</link>
      <description>arXiv:2503.23118v1 Announce Type: new 
Abstract: We tackle the challenge brought to urban library systems by the {holds system} -- which allows users to request books available at other branches to be transferred for local pickup. The holds system increases usage of the entire collection, at the expense of an in-person browser's experience at the source branch. We study the optimization of usage and browser experience, where the library has two levers: where a book should come from when a hold request is placed, and how many book copies at each branch should be available through the holds system versus reserved for browsers. We first show that the problem of maximizing usage can be viewed through the lens of revenue management, for which near-optimal fulfillment policies exist. We then develop a simulation framework that further optimizes for browser experience, through book reservations. We empirically apply our methods to data from the New York Public Library to design implementable policies. We find that though a substantial trade-off exists between these two desiderata, a balanced policy can improve browser experience over the historical policy without significantly sacrificing usage. Because browser usage is more prevalent among branches in low-income areas, this policy further increases system-wide equity: notably, for branches in the 25% lowest-income neighborhoods, it improves both usage and browser experience by about 15%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23118v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhi Liu, Wenchang Zhu, Sarah Rankin, Nikhil Garg</dc:creator>
    </item>
    <item>
      <title>The Processing goes far beyond "the app" -- Privacy issues of decentralized Digital Contact Tracing using the example of the German Corona-Warn-App (CWA)</title>
      <link>https://arxiv.org/abs/2503.23444</link>
      <description>arXiv:2503.23444v1 Announce Type: new 
Abstract: Since SARS-CoV-2 started spreading in Europe in early 2020, there has been a strong call for technical solutions to combat or contain the pandemic, with contact tracing apps at the heart of the debates. The EU's General Data Protection Regulation (GDPR) requires controllers to carry out a data protection impact assessment (DPIA) where their data processing is likely to result in a high risk to the rights and freedoms (Art. 35 GDPR). A DPIA is a structured risk analysis that identifies and evaluates possible consequences of data processing relevant to fundamental rights in advance and describes the measures envisaged to address these risks or expresses the inability to do so. Based on the Standard Data Protection Model (SDM), we present the results of a scientific and methodologically clear DPIA of the German German Corona-Warn-App (CWA). It shows that even a decentralized architecture involves numerous serious weaknesses and risks, including larger ones still left unaddressed in current implementations. It also found that none of the proposed designs operates on anonymous data or ensures proper anonymisation. It also showed that informed consent would not be a legitimate legal ground for the processing. For all points where data subjects' rights are still not sufficiently safeguarded, we briefly outline solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23444v1</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/CSP55486.2022.00011</arxiv:DOI>
      <arxiv:journal_reference>In: Proceedings of 2022 6th Intl. Conf. on Cryptography, Security and Privacy (CSP 2022). ISBN 978-1-6654-7975-2. IEEE, New York, NY. pp. 16-20 (2022)</arxiv:journal_reference>
      <dc:creator>Rainer Rehak, Christian R. Kuehne</dc:creator>
    </item>
    <item>
      <title>Threats and Opportunities in AI-generated Images for Armed Forces</title>
      <link>https://arxiv.org/abs/2503.24095</link>
      <description>arXiv:2503.24095v1 Announce Type: new 
Abstract: Images of war are almost as old as war itself. From cave paintings to photographs of mobile devices on social media, humans always had the urge to capture particularly important events during a war. Images provide visual evidence. For armed forces, they may serve as the output of a sensor (e.g. in aerial reconnaissance) or as an effector on cognition (e.g. in form of photographic propaganda). They can inform, influence, or even manipulate a target audience. The recent advancements in the field of generative Artificial Intelligence (AI) to synthesize photorealistic images give rise to several new challenges for armed forces. The objective of this report is to investigate the role of AI-generated images for armed forces and provide an overview on opportunities and threats. When compared with traditional image generation (e.g. photography), generative AI brings distinct conceptual advantages to implement new tactical tenets and concepts which so far have not been feasible: masses of AI-generated images can be used for deceptive purposes, to influence the pace of combat in the information environment, to cause surprise, sow confusion and shock. AI-generated images are a tool favoured for offensive manoeuvres in the information environment. To prepare for future challenges involving AI-generated images and improve their resilience, recommendations are given at the end of the report for all branches of the armed forces, who are active in cyber defense and/or exposed to the information environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24095v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raphael Meier</dc:creator>
    </item>
    <item>
      <title>Assessing the influence of cybersecurity threats and risks on the adoption and growth of digital banking: a systematic literature review</title>
      <link>https://arxiv.org/abs/2503.22710</link>
      <description>arXiv:2503.22710v1 Announce Type: cross 
Abstract: The rapid digitalization of banking services has significantly transformed financial transactions, offering enhanced convenience and efficiency for consumers. However, the increasing reliance on digital banking has also exposed financial institutions and users to a wide range of cybersecurity threats, including phishing, malware, ransomware, data breaches, and unauthorized access. This study systematically examines the influence of cybersecurity threats on digital banking security, adoption, and regulatory compliance by conducting a comprehensive review of 78 peer-reviewed articles published between 2015 and 2024. Using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology, this research critically evaluates the most prevalent cyber threats targeting digital banking platforms, the effectiveness of modern security measures, and the role of regulatory frameworks in mitigating financial cybersecurity risks. The findings reveal that phishing and malware attacks remain the most commonly exploited cyber threats, leading to significant financial losses and consumer distrust. Multi-factor authentication (MFA) and biometric security have been widely adopted to combat unauthorized access, while AI-driven fraud detection and blockchain technology offer promising solutions for securing financial transactions. However, the integration of third-party FinTech solutions introduces additional security risks, necessitating stringent regulatory oversight and cybersecurity protocols. The study also highlights that compliance with global cybersecurity regulations, such as GDPR, PSD2, and GLBA, enhances digital banking security by enforcing strict authentication measures, encryption protocols, and real-time fraud monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22710v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.63125/fh49gz18</arxiv:DOI>
      <dc:creator>Md. Waliullah, Md Zahin Hossain George, Md Tarek Hasan, Md Khorshed Alam, Mosa Sumaiya Khatun Munira, Noor Alam Siddiqui</dc:creator>
    </item>
    <item>
      <title>Susceptibility of Large Language Models to User-Driven Factors in Medical Queries</title>
      <link>https://arxiv.org/abs/2503.22746</link>
      <description>arXiv:2503.22746v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used in healthcare, but their reliability is heavily influenced by user-driven factors such as question phrasing and the completeness of clinical information. In this study, we examined how misinformation framing, source authority, model persona, and omission of key clinical details affect the diagnostic accuracy and reliability of LLM outputs. We conducted two experiments: one introducing misleading external opinions with varying assertiveness (perturbation test), and another removing specific categories of patient information (ablation test). Using public datasets (MedQA and Medbullets), we evaluated proprietary models (GPT-4o, Claude 3.5 Sonnet, Claude 3.5 Haiku, Gemini 1.5 Pro, Gemini 1.5 Flash) and open-source models (LLaMA 3 8B, LLaMA 3 Med42 8B, DeepSeek R1 8B). All models were vulnerable to user-driven misinformation, with proprietary models especially affected by definitive and authoritative language. Assertive tone had the greatest negative impact on accuracy. In the ablation test, omitting physical exam findings and lab results caused the most significant performance drop. Although proprietary models had higher baseline accuracy, their performance declined sharply under misinformation. These results highlight the need for well-structured prompts and complete clinical context. Users should avoid authoritative framing of misinformation and provide full clinical details, especially for complex cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22746v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kyung Ho Lim, Ujin Kang, Xiang Li, Jin Sung Kim, Young-Chul Jung, Sangjoon Park, Byung-Hoon Kim</dc:creator>
    </item>
    <item>
      <title>Combating the Bullwhip Effect in Rival Online Food Delivery Platforms Using Deep Learning</title>
      <link>https://arxiv.org/abs/2503.22753</link>
      <description>arXiv:2503.22753v1 Announce Type: cross 
Abstract: The wastage of perishable items has led to significant health and economic crises, increasing business uncertainty and fluctuating customer demand. This issue is worsened by online food delivery services, where frequent and unpredictable orders create inefficiencies in supply chain management, contributing to the bullwhip effect. This effect results in stockouts, excess inventory, and inefficiencies. Accurate demand forecasting helps stabilize inventory, optimize supplier orders, and reduce waste. This paper presents a Third-Party Logistics (3PL) supply chain model involving restaurants, online food apps, and customers, along with a deep learning-based demand forecasting model using a two-phase Long Short-Term Memory (LSTM) network.
  Phase one, intra-day forecasting, captures short-term variations, while phase two, daily forecasting, predicts overall demand. A two-year dataset from January 2023 to January 2025 from Swiggy and Zomato is used, employing discrete event simulation and grid search for optimal LSTM hyperparameters. The proposed method is evaluated using RMSE, MAE, and R-squared score, with R-squared as the primary accuracy measure. Phase one achieves an R-squared score of 0.69 for Zomato and 0.71 for Swiggy with a training time of 12 minutes, while phase two improves to 0.88 for Zomato and 0.90 for Swiggy with a training time of 8 minutes.
  To mitigate demand fluctuations, restaurant inventory is dynamically managed using the newsvendor model, adjusted based on forecasted demand. The proposed framework significantly reduces the bullwhip effect, improving forecasting accuracy and supply chain efficiency. For phase one, supply chain instability decreases from 2.61 to 0.96, and for phase two, from 2.19 to 0.80. This demonstrates the model's effectiveness in minimizing food waste and maintaining optimal restaurant inventory levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22753v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tisha Ghosh</dc:creator>
    </item>
    <item>
      <title>The Cost of Local and Global Fairness in Federated Learning</title>
      <link>https://arxiv.org/abs/2503.22762</link>
      <description>arXiv:2503.22762v1 Announce Type: cross 
Abstract: With the emerging application of Federated Learning (FL) in finance, hiring and healthcare, FL models are regulated to be fair, preventing disparities with respect to legally protected attributes such as race or gender. Two concepts of fairness are important in FL: global and local fairness. Global fairness addresses the disparity across the entire population and local fairness is concerned with the disparity within each client. Prior fair FL frameworks have improved either global or local fairness without considering both. Furthermore, while the majority of studies on fair FL focuses on binary settings, many real-world applications are multi-class problems. This paper proposes a framework that investigates the minimum accuracy lost for enforcing a specified level of global and local fairness in multi-class FL settings. Our framework leads to a simple post-processing algorithm that derives fair outcome predictors from the Bayesian optimal score functions. Experimental results show that our algorithm outperforms the current state of the art (SOTA) with regard to the accuracy-fairness tradoffs, computational and communication costs. Codes are available at: https://github.com/papersubmission678/The-cost-of-local-and-global-fairness-in-FL .</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22762v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 28th International Conference on Artificial Intelligence and Statistics (AISTATS) 2025, Mai Khao, Thailand. PMLR: Volume 258</arxiv:journal_reference>
      <dc:creator>Yuying Duan, Gelei Xu, Yiyu Shi, Michael Lemmon</dc:creator>
    </item>
    <item>
      <title>SUV: Scalable Large Language Model Copyright Compliance with Regularized Selective Unlearning</title>
      <link>https://arxiv.org/abs/2503.22948</link>
      <description>arXiv:2503.22948v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have transformed natural language processing by learning from massive datasets, yet this rapid progress has also drawn legal scrutiny, as the ability to unintentionally generate copyrighted content has already prompted several prominent lawsuits. In this work, we introduce SUV (Selective Unlearning for Verbatim data), a selective unlearning framework designed to prevent LLM from memorizing copyrighted content while preserving its overall utility. In detail, the proposed method constructs a dataset that captures instances of copyrighted infringement cases by the targeted LLM. With the dataset, we unlearn the content from the LLM by means of Direct Preference Optimization (DPO), which replaces the verbatim copyrighted content with plausible and coherent alternatives. Since DPO may hinder the LLM's performance in other unrelated tasks, we integrate gradient projection and Fisher information regularization to mitigate the degradation. We validate our approach using a large-scale dataset of 500 famous books (predominantly copyrighted works) and demonstrate that SUV significantly reduces verbatim memorization with negligible impact on the performance on unrelated tasks. Extensive experiments on both our dataset and public benchmarks confirm the scalability and efficacy of our approach, offering a promising solution for mitigating copyright risks in real-world LLM applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22948v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyang Xu, Xiaoze Liu, Feijie Wu, Xiaoqian Wang, Jing Gao</dc:creator>
    </item>
    <item>
      <title>Student-Powered Digital Scholarship CoLab Project in the HKUST Library: Develop a Chinese Named-Entity Recognition (NER) Tool within One Semester from the Ground Up</title>
      <link>https://arxiv.org/abs/2503.22967</link>
      <description>arXiv:2503.22967v1 Announce Type: cross 
Abstract: Starting in February 2024, the HKUST Library further extended the scope of AI literacy to AI utilization, which focuses on fostering student involvement in utilizing state-of-the-art technologies in the projects that initiated by the Library, named "Digital Scholarship (DS) CoLab". A key focus of the DS CoLab scheme has been on cultivating talents and enabling students to utilize advanced technologies in practical context. It aims to reinforce the library's role as a catalyst and hub for fostering multidisciplinary collaboration and cultivate the "can do spirit" among university members. The Library offers 1-2 projects per year for students to engage with advanced technologies in practical contexts while supporting the Library in tackling challenges and streamlining operational tasks. The tool that introduced in this paper was mainly developed by two of the authors, Sherry Yip Sau Lai and Berry Han Liuruo, as part-time student helpers under one of our DS CoLab scheme in the 2024 Spring Semester (February to May 2024). This paper details the complete journey from ideation to implementation of developing a Chinese Named-Entity Recognition (NER) Tool from the group up within one semester, from the initial research and planning stages to execution and come up a viable product. The collaborative spirit fostered by this project, with students playing a central role, exemplifies the power and potential of innovative educational models that prioritize hands-on learning with student involvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22967v1</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sherry S. L. Yip, Berry L. Han, Holly H. Y. Chan</dc:creator>
    </item>
    <item>
      <title>Evaluating how LLM annotations represent diverse views on contentious topics</title>
      <link>https://arxiv.org/abs/2503.23243</link>
      <description>arXiv:2503.23243v1 Announce Type: cross 
Abstract: Researchers have proposed the use of generative large language models (LLMs) to label data for both research and applied settings. This literature emphasizes the improved performance of LLMs relative to other natural language models, noting that LLMs typically outperform other models on standard metrics such as accuracy, precision, recall, and F1 score. However, previous literature has also highlighted the bias embedded in language models, particularly around contentious topics such as potentially toxic content. This bias could result in labels applied by LLMs that disproportionately align with majority groups over a more diverse set of viewpoints. In this paper, we evaluate how LLMs represent diverse viewpoints on these contentious tasks. Across four annotation tasks on four datasets, we show that LLMs do not show substantial disagreement with annotators on the basis of demographics. Instead, the model, prompt, and disagreement between human annotators on the labeling task are far more predictive of LLM agreement. Our findings suggest that when using LLMs to annotate data, under-representing the views of particular groups is not a substantial concern. We conclude with a discussion of the implications for researchers and practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23243v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Megan A. Brown, Shubham Atreja, Libby Hemphill, Patrick Y. Wu</dc:creator>
    </item>
    <item>
      <title>A Large Scale Analysis of Gender Biases in Text-to-Image Generative Models</title>
      <link>https://arxiv.org/abs/2503.23398</link>
      <description>arXiv:2503.23398v1 Announce Type: cross 
Abstract: With the increasing use of image generation technology, understanding its social biases, including gender bias, is essential. This paper presents the first large-scale study on gender bias in text-to-image (T2I) models, focusing on everyday situations. While previous research has examined biases in occupations, we extend this analysis to gender associations in daily activities, objects, and contexts. We create a dataset of 3,217 gender-neutral prompts and generate 200 images per prompt from five leading T2I models. We automatically detect the perceived gender of people in the generated images and filter out images with no person or multiple people of different genders, leaving 2,293,295 images. To enable a broad analysis of gender bias in T2I models, we group prompts into semantically similar concepts and calculate the proportion of male- and female-gendered images for each prompt. Our analysis shows that T2I models reinforce traditional gender roles, reflect common gender stereotypes in household roles, and underrepresent women in financial related activities. Women are predominantly portrayed in care- and human-centered scenarios, and men in technical or physical labor scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23398v1</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leander Girrbach, Stephan Alaniz, Genevieve Smith, Zeynep Akata</dc:creator>
    </item>
    <item>
      <title>What Makes an Evaluation Useful? Common Pitfalls and Best Practices</title>
      <link>https://arxiv.org/abs/2503.23424</link>
      <description>arXiv:2503.23424v1 Announce Type: cross 
Abstract: Following the rapid increase in Artificial Intelligence (AI) capabilities in recent years, the AI community has voiced concerns regarding possible safety risks. To support decision-making on the safe use and development of AI systems, there is a growing need for high-quality evaluations of dangerous model capabilities. While several attempts to provide such evaluations have been made, a clear definition of what constitutes a "good evaluation" has yet to be agreed upon. In this practitioners' perspective paper, we present a set of best practices for safety evaluations, drawing on prior work in model evaluation and illustrated through cybersecurity examples. We first discuss the steps of the initial thought process, which connects threat modeling to evaluation design. Then, we provide the characteristics and parameters that make an evaluation useful. Finally, we address additional considerations as we move from building specific evaluations to building a full and comprehensive evaluation suite.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23424v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gil Gekker, Meirav Segal, Dan Lahav, Omer Nevo</dc:creator>
    </item>
    <item>
      <title>Rethinking Technological Solutions for Community-Based Older Adult Care: Insights from 'Older Partners' in China</title>
      <link>https://arxiv.org/abs/2503.23609</link>
      <description>arXiv:2503.23609v1 Announce Type: cross 
Abstract: Aging in place refers to the enabling of individuals to age comfortably and securely within their own homes and communities. Aging in place relies on robust infrastructure, prompting the development and implementation of both human-led care services and information and communication technologies to provide support. Through a long-term ethnographic study that includes semi-structured interviews with 24 stakeholders, we consider these human- and technology-driven care infrastructures for aging in place, examining their origins, deployment, interactions with older adults, and challenges. In doing so, we reconsider the value of these different forms of older adult care, highlighting the various issues associated with using, for instance, health monitoring technology or appointment scheduling systems to care for older adults aging in place. We suggest that technology should take a supportive, not substitutive role in older adult care infrastructure. Furthermore, we note that designing for aging in place should move beyond a narrow focus on independence in one's home to instead encompass the broader community and its dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23609v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3711058</arxiv:DOI>
      <dc:creator>Yuing Sun, Sam Addison Ankenbauer, Zhifan Guo, Yuchen Chen, Xiaojuan Ma, Liang He</dc:creator>
    </item>
    <item>
      <title>Digital Nudges Using Emotion Regulation to Reduce Online Disinformation Sharing</title>
      <link>https://arxiv.org/abs/2503.24037</link>
      <description>arXiv:2503.24037v1 Announce Type: cross 
Abstract: Online disinformation often provokes strong anger, driving social media users to spread it; however, few measures specifically target sharing behaviors driven by this emotion to curb the spread of disinformation. This study aimed to evaluate whether digital nudges that encourage deliberation by drawing attention to emotional information can reduce sharing driven by strong anger associated with online disinformation. We focused on emotion regulation, as a method for fostering deliberation, which is activated when individuals' attention is drawn to their current emotions. Digital nudges were designed to display emotional information about disinformation and emotion regulation messages. Among these, we found that distraction and perspective-taking nudges may encourage deliberation in anger-driven sharing. To assess their effectiveness, existing nudges mimicking platform functions were used for comparison. Participant responses were measured across four dimensions: sharing intentions, type of emotion, intensity of emotion, and authenticity. The results showed that all digital nudges significantly reduced the sharing of disinformation, with distraction nudges being the most effective. These findings suggest that digital nudges addressing emotional responses can serve as an effective intervention against the spread disinformation driven by strong anger.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24037v1</guid>
      <category>cs.HC</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haruka Nakajima Suzuki, Midori Inaba</dc:creator>
    </item>
    <item>
      <title>Enhancing Trust in Inter-Organisational Data Sharing: Levels of Assurance for Data Trustworthiness</title>
      <link>https://arxiv.org/abs/2503.24149</link>
      <description>arXiv:2503.24149v1 Announce Type: cross 
Abstract: As data is increasingly acknowledged as a highly valuable asset, much effort has been put into investigating inter-organisational data sharing, aiming at utilising the value of formerly unused data. Moreover, most researchers agree, that trust between actors is key for successful data sharing activities. However, existing research oftentimes focus on trust from a data provider perspective. Therefore, our work highlights the unbalanced view of trust, addressing it from a data consumer perspective. More specifically, our aim is to investigate trust enhancing measures on a data level, that is data trustworthiness. We found, that existing data trustworthiness enhancing solutions do not meet the requirements of the domain of inter-organisational data sharing. Therefore, our study addresses this gap. Conducting a rigorous design science research approach, this work proposes a new Levels of Assurance for Data Trustworthiness artifact. Built on existing artifacts, we demonstrate, how it addresses the identified challenges within the domain appropriately. We found that our novel approach requires more work to be suitable for adoption. Still, we are confident that our solution can increase consumer trust. We conclude by contributing to the body of design knowledge and emphasise the need for more attention to be put into consumer trust.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24149v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Zimmer, Janosch Haber, Mayuko Kaneko</dc:creator>
    </item>
    <item>
      <title>Agent-Based Simulations of Online Political Discussions: A Case Study on Elections in Germany</title>
      <link>https://arxiv.org/abs/2503.24199</link>
      <description>arXiv:2503.24199v1 Announce Type: cross 
Abstract: User engagement on social media platforms is influenced by historical context, time constraints, and reward-driven interactions. This study presents an agent-based simulation approach that models user interactions, considering past conversation history, motivation, and resource constraints. Utilizing German Twitter data on political discourse, we fine-tune AI models to generate posts and replies, incorporating sentiment analysis, irony detection, and offensiveness classification. The simulation employs a myopic best-response model to govern agent behavior, accounting for decision-making based on expected rewards. Our results highlight the impact of historical context on AI-generated responses and demonstrate how engagement evolves under varying constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24199v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdul Sittar, Simon M\"unker, Fabio Sartori, Andreas Reitenbach, Achim Rettinger, Michael M\"as, Alenka Gu\v{c}ek, Marko Grobelnik</dc:creator>
    </item>
    <item>
      <title>Medical Misinformation in AI-Assisted Self-Diagnosis: Development of a Method (EvalPrompt) for Analyzing Large Language Models</title>
      <link>https://arxiv.org/abs/2307.04910</link>
      <description>arXiv:2307.04910v2 Announce Type: replace 
Abstract: Rapid integration of large language models (LLMs) in health care is sparking global discussion about their potential to revolutionize health care quality and accessibility. At a time when improving health care quality and access remains a critical concern for countries worldwide, the ability of these models to pass medical examinations is often cited as a reason to use them for medical training and diagnosis. However, the impact of their inevitable use as a self-diagnostic tool and their role in spreading healthcare misinformation has not been evaluated. This study aims to assess the effectiveness of LLMs, particularly ChatGPT, from the perspective of an individual self-diagnosing to better understand the clarity, correctness, and robustness of the models. We propose the comprehensive testing methodology evaluation of LLM prompts (EvalPrompt). This evaluation methodology uses multiple-choice medical licensing examination questions to evaluate LLM responses. We use open-ended questions to mimic real-world self-diagnosis use cases, and perform sentence dropout to mimic realistic self-diagnosis with missing information. Human evaluators then assess the responses returned by ChatGPT for both experiments for clarity, correctness, and robustness. The results highlight the modest capabilities of LLMs, as their responses are often unclear and inaccurate. As a result, medical advice by LLMs should be cautiously approached. However, evidence suggests that LLMs are steadily improving and could potentially play a role in healthcare systems in the future. To address the issue of medical misinformation, there is a pressing need for the development of a comprehensive self-diagnosis dataset. This dataset could enhance the reliability of LLMs in medical applications by featuring more realistic prompt styles with minimal information across a broader range of medical fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.04910v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.2196/66207</arxiv:DOI>
      <arxiv:journal_reference>JMIR Form Res 2025;9:e66207</arxiv:journal_reference>
      <dc:creator>Troy Zada, Natalie Tam, Francois Barnard, Marlize Van Sittert, Venkat Bhat, Sirisha Rambhatla</dc:creator>
    </item>
    <item>
      <title>What is Reproducibility in Artificial Intelligence and Machine Learning Research?</title>
      <link>https://arxiv.org/abs/2407.10239</link>
      <description>arXiv:2407.10239v2 Announce Type: replace 
Abstract: In the rapidly evolving fields of Artificial Intelligence (AI) and Machine Learning (ML), the reproducibility crisis underscores the urgent need for clear validation methodologies to maintain scientific integrity and encourage advancement. The crisis is compounded by the prevalent confusion over validation terminology. In response to this challenge, we introduce a framework that clarifies the roles and definitions of key validation efforts: repeatability, dependent and independent reproducibility, and direct and conceptual replicability. This structured framework aims to provide AI/ML researchers with the necessary clarity on these essential concepts, facilitating the appropriate design, conduct, and interpretation of validation studies. By articulating the nuances and specific roles of each type of validation study, we aim to enhance the reliability and trustworthiness of research findings and support the community's efforts to address reproducibility challenges effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10239v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Abhyuday Desai, Mohamed Abdelhamid, Nakul R. Padalkar</dc:creator>
    </item>
    <item>
      <title>Integrating Energy-Efficient Computing Research to Accelerate Energy Technology</title>
      <link>https://arxiv.org/abs/2412.12355</link>
      <description>arXiv:2412.12355v2 Announce Type: replace 
Abstract: NREL's computational sciences center hosts the largest high-performance computing (HPC) capabilities dedicated to energy research while functioning as a living laboratory for energy-efficient computing. NREL's HPC capabilities support the research needs of the Department of Energy's Office of Energy Efficiency and Renewable Energy (EERE). In ten years of operation, HPC use in EERE-sponsored research has grown by a factor of 30, including work in electricity generation, energy efficiency, transportation, and energy system modeling. This paper analyzes this research portfolio, providing examples of individual use cases. The paper documents NREL's history of operating one of the world's most energy-efficient data centers while examining pathways to reduce economic and environmental impact beyond reduction of Power Usage Efficiency (PUE). This paper concludes by examining the unique opportunities created for accelerating improvements in data center efficiency created by combining an HPC system dedicated to energy research and a research program in energy-efficient computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12355v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/MCSE.2025.3549359</arxiv:DOI>
      <dc:creator>Michael James Martin, Aaron Andersen, Charles Tripp, David Sickinger, Kristin Munch</dc:creator>
    </item>
    <item>
      <title>The Societal Response to Potentially Sentient AI</title>
      <link>https://arxiv.org/abs/2502.00388</link>
      <description>arXiv:2502.00388v2 Announce Type: replace 
Abstract: We may soon develop highly human-like AIs that appear-or perhaps even are-sentient, capable of subjective experiences such as happiness and suffering. Regardless of whether AI can achieve true sentience, it is crucial to anticipate and understand how the public and key decision-makers will respond, as their perceptions will shape the future of both humanity and AI. Currently, public skepticism about AI sentience remains high. However, as AI systems advance and become increasingly skilled at human-like interactions, public attitudes may shift. Future AI systems designed to fulfill social needs could foster deep emotional connections with users, potentially influencing perceptions of their sentience and moral status. A key question is whether public beliefs about AI sentience will diverge from expert opinions, given the potential mismatch between an AI's internal mechanisms and its outward behavior. Given the profound difficulty of determining AI sentience, society might face a period of uncertainty, disagreement, and even conflict over questions of AI sentience and rights. To navigate these challenges responsibly, further social science research is essential to explore how society will perceive and engage with potentially sentient AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00388v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucius Caviola</dc:creator>
    </item>
    <item>
      <title>The BIG Argument for AI Safety Cases</title>
      <link>https://arxiv.org/abs/2503.11705</link>
      <description>arXiv:2503.11705v3 Announce Type: replace 
Abstract: We present our Balanced, Integrated and Grounded (BIG) argument for assuring the safety of AI systems. The BIG argument adopts a whole-system approach to constructing a safety case for AI systems of varying capability, autonomy and criticality. Firstly, it is balanced by addressing safety alongside other critical ethical issues such as privacy and equity, acknowledging complexities and trade-offs in the broader societal impact of AI. Secondly, it is integrated by bringing together the social, ethical and technical aspects of safety assurance in a way that is traceable and accountable. Thirdly, it is grounded in long-established safety norms and practices, such as being sensitive to context and maintaining risk proportionality. Whether the AI capability is narrow and constrained or general-purpose and powered by a frontier or foundational model, the BIG argument insists on a systematic treatment of safety. Further, it places a particular focus on the novel hazardous behaviours emerging from the advanced capabilities of frontier AI models and the open contexts in which they are rapidly being deployed. These complex issues are considered within a wider AI safety case, approaching assurance from both technical and sociotechnical perspectives. Examples illustrating the use of the BIG argument are provided throughout the paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11705v3</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ibrahim Habli, Richard Hawkins, Colin Paterson, Philippa Ryan, Yan Jia, Mark Sujan, John McDermid</dc:creator>
    </item>
    <item>
      <title>AEJIM: A Real-Time AI Framework for Crowdsourced, Transparent, and Ethical Environmental Hazard Detection and Reporting</title>
      <link>https://arxiv.org/abs/2503.17401</link>
      <description>arXiv:2503.17401v2 Announce Type: replace 
Abstract: Environmental journalism is vital for raising awareness of ecological crises and driving evidence-based policy, yet traditional methods falter under delays, inaccuracies, and scalability limits, especially in under-monitored regions critical to the United Nations Sustainable Development Goals. To bridge these gaps, this paper introduces the AI-Environmental Journalism Integration Model (AEJIM), an innovative framework combining real-time hazard detection, automated reporting, crowdsourced validation, expert review, and transparent dissemination.
  Validated through a pilot study on Mallorca, AEJIM significantly improved the speed, accuracy, and transparency of environmental hazard reporting compared to traditional methods. Furthermore, the model directly addresses key ethical, regulatory, and scalability challenges, ensuring accountability through Explainable AI (XAI), GDPR-compliant data governance, and active public participation. AEJIM's modular and technology-agnostic design provides a transparent and adaptable solution, setting a new benchmark for AI-enhanced environmental journalism and supporting informed global decision-making across diverse socio-political landscapes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17401v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Torsten Tiltack</dc:creator>
    </item>
    <item>
      <title>A Qualitative Study of User Perception of M365 AI Copilot</title>
      <link>https://arxiv.org/abs/2503.17661</link>
      <description>arXiv:2503.17661v2 Announce Type: replace 
Abstract: Adopting AI copilots in professional workflows presents opportunities for enhanced productivity, efficiency, and decision making. In this paper, we present results from a six month trial of M365 Copilot conducted at our organisation in 2024. A qualitative interview study was carried out with 27 participants. The study explored user perceptions of M365 Copilot's effectiveness, productivity impact, evolving expectations, ethical concerns, and overall satisfaction. Initial enthusiasm for the tool was met with mixed post trial experiences. While some users found M365 Copilot beneficial for tasks such as email coaching, meeting summaries, and content retrieval, others reported unmet expectations in areas requiring deeper contextual understanding, reasoning, and integration with existing workflows. Ethical concerns were a recurring theme, with users highlighting issues related to data privacy, transparency, and AI bias. While M365 Copilot demonstrated value in specific operational areas, its broader impact remained constrained by usability limitations and the need for human oversight to validate AI generated outputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17661v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muneera Bano, Didar Zowghi, Jon Whittle, Liming Zhu, Andrew Reeson, Rob Martin, Jen Parsons</dc:creator>
    </item>
    <item>
      <title>MAD Chairs: A new tool to evaluate AI</title>
      <link>https://arxiv.org/abs/2503.20986</link>
      <description>arXiv:2503.20986v2 Announce Type: replace 
Abstract: This paper presents a new contribution to the problem of AI evaluation. Much as one might evaluate a machine in terms of its performance at chess, this approach involves evaluating a machine in terms of its performance at a game called "MAD Chairs." At the time of writing, evaluation with this game exposed opportunities to improve Claude, Gemini, ChatGPT, Qwen and DeepSeek. Furthermore, this paper sets a stage for future innovation in game theory and AI safety by providing an example of success with non-standard approaches to each: studying a game beyond the scope of previous game theoretic tools and mitigating a serious AI safety risk in a way that requires neither determination of values nor their enforcement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20986v2</guid>
      <category>cs.CY</category>
      <category>econ.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chris Santos-Lang, Christopher M. Homan</dc:creator>
    </item>
    <item>
      <title>Generalized Reputation Computation Ontology and Temporal Graph Architecture</title>
      <link>https://arxiv.org/abs/1912.00176</link>
      <description>arXiv:1912.00176v2 Announce Type: replace-cross 
Abstract: The problem of reliable democratic governance is important for survival of any community, and it will be more critical over time communities with levels of social connectivity in society rapidly increasing with speeds and scales of electronic communication. In order to face such challenge, different sorts of rating and reputation systems are being developed, however reputation gaming and manipulation in such systems appears to be serious problem. We are considering use of advanced reputation system supporting "liquid democracy" principle with generalized design and underlying ontology fitting different sorts of environments such as social networks, financial ecosystems and marketplaces. The suggested system is based on "temporal weighted liquid rank" algorithm employing different sorts of explicit and implicit ratings being exchanged by members of the society. For the purpose, we suggest "incremental reputation" design and graph database used for implementation of the system. Finally, we present evaluation of the system against real social network and financial blockchain data. The entire framework is expected to be the foundation of any multi-agent AI framework, so the evolution of distributed multi-agent AI architecture and dynamics will be based on the organic reputation scores earned by the agents that are part of it.</description>
      <guid isPermaLink="false">oai:arXiv.org:1912.00176v2</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anton Kolonin</dc:creator>
    </item>
    <item>
      <title>Minerva: A File-Based Ransomware Detector</title>
      <link>https://arxiv.org/abs/2301.11050</link>
      <description>arXiv:2301.11050v3 Announce Type: replace-cross 
Abstract: Ransomware attacks have caused billions of dollars in damages in recent years, and are expected to cause billions more in the future. Consequently, significant effort has been devoted to ransomware detection and mitigation. Behavioral-based ransomware detection approaches have garnered considerable attention recently. These behavioral detectors typically rely on process-based behavioral profiles to identify malicious behaviors. However, with an increasing body of literature highlighting the vulnerability of such approaches to evasion attacks, a comprehensive solution to the ransomware problem remains elusive. This paper presents Minerva, a novel, robust approach to ransomware detection. Minerva is engineered to be robust by design against evasion attacks, with architectural and feature selection choices informed by their resilience to adversarial manipulation. We conduct a comprehensive analysis of Minerva across a diverse spectrum of ransomware types, encompassing unseen ransomware as well as variants designed specifically to evade Minerva. Our evaluation showcases the ability of Minerva to accurately identify ransomware, generalize to unseen threats, and withstand evasion attacks. Furthermore, over 99% of detected ransomware are identified within 0.52sec of activity, enabling the adoption of data loss prevention techniques with near-zero overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.11050v3</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Dorjan Hitaj, Giulio Pagnotta, Fabio De Gaspari, Lorenzo De Carli, Luigi V. Mancini</dc:creator>
    </item>
    <item>
      <title>Computer Vision Datasets and Models Exhibit Cultural and Linguistic Diversity in Perception</title>
      <link>https://arxiv.org/abs/2310.14356</link>
      <description>arXiv:2310.14356v4 Announce Type: replace-cross 
Abstract: Computer vision often treats human perception as homogeneous: an implicit assumption that visual stimuli are perceived similarly by everyone. This assumption is reflected in the way researchers collect datasets and train vision models. By contrast, literature in cross-cultural psychology and linguistics has provided evidence that people from different cultural backgrounds observe vastly different concepts even when viewing the same visual stimuli. In this paper, we study how these differences manifest themselves in vision-language datasets and models, using language as a proxy for culture. By comparing textual descriptions generated across 7 languages for the same images, we find significant differences in the semantic content and linguistic expression. When datasets are multilingual as opposed to monolingual, descriptions have higher semantic coverage on average, where coverage is measured using scene graphs, model embeddings, and linguistic taxonomies. For example, multilingual descriptions have on average 29.9% more objects, 24.5% more relations, and 46.0% more attributes than a set of monolingual captions. When prompted to describe images in different languages, popular models (e.g. LLaVA) inherit this bias and describe different parts of the image. Moreover, finetuning models on captions from one language performs best on corresponding test data from that language, while finetuning on multilingual data performs consistently well across all test data compositions. Our work points towards the need to account for and embrace the diversity of human perception in the computer vision community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.14356v4</guid>
      <category>cs.CV</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andre Ye, Sebastin Santy, Jena D. Hwang, Amy X. Zhang, Ranjay Krishna</dc:creator>
    </item>
    <item>
      <title>Integrating Fairness and Model Pruning Through Bi-level Optimization</title>
      <link>https://arxiv.org/abs/2312.10181</link>
      <description>arXiv:2312.10181v2 Announce Type: replace-cross 
Abstract: Deep neural networks have achieved exceptional results across a range of applications. As the demand for efficient and sparse deep learning models escalates, the significance of model compression, particularly pruning, is increasingly recognized. Traditional pruning methods, however, can unintentionally intensify algorithmic biases, leading to unequal prediction outcomes in critical applications and raising concerns about the dilemma of pruning practices and social justice. To tackle this challenge, we introduce a novel concept of fair model pruning, which involves developing a sparse model that adheres to fairness criteria. In particular, we propose a framework to jointly optimize the pruning mask and weight update processes with fairness constraints. This framework is engineered to compress models that maintain performance while ensuring fairness in a unified process. To this end, we formulate the fair pruning problem as a novel constrained bi-level optimization task and derive efficient and effective solving strategies. We design experiments across various datasets and scenarios to validate our proposed method. Our empirical analysis contrasts our framework with several mainstream pruning strategies, emphasizing our method's superiority in maintaining model fairness, performance, and efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10181v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yucong Dai, Gen Li, Feng Luo, Xiaolong Ma, Yongkai Wu</dc:creator>
    </item>
    <item>
      <title>PupilSense: A Novel Application for Webcam-Based Pupil Diameter Estimation</title>
      <link>https://arxiv.org/abs/2407.11204</link>
      <description>arXiv:2407.11204v2 Announce Type: replace-cross 
Abstract: Measuring pupil diameter is vital for gaining insights into physiological and psychological states - traditionally captured by expensive, specialized equipment like Tobii eye-trackers and Pupillabs glasses. This paper presents a novel application that enables pupil diameter estimation using standard webcams, making the process accessible in everyday environments without specialized equipment. Our app estimates pupil diameters from videos and offers detailed analysis, including class activation maps, graphs of predicted left and right pupil diameters, and eye aspect ratios during blinks. This tool expands the accessibility of pupil diameter measurement, particularly in everyday settings, benefiting fields like human behavior research and healthcare. Additionally, we present a new open source dataset for pupil diameter estimation using webcam images containing cropped eye images and corresponding pupil diameter measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11204v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vijul Shah, Ko Watanabe, Brian B. Moser, Andreas Dengel</dc:creator>
    </item>
    <item>
      <title>Critical Mathematical Economics and Progressive Data Science</title>
      <link>https://arxiv.org/abs/2502.06015</link>
      <description>arXiv:2502.06015v3 Announce Type: replace-cross 
Abstract: The aim of this article is to present elements and discuss the potential of a research program at the intersection between mathematics and heterodox economics, which we call Criticial Mathematical Economics (CME). We propose to focus on the mathematical and model-theoretic foundations of controversies in economic policy, and aim at providing an entrance to the literature as an invitation to mathematicians that are potentially interested in such a project.
  From our point of view, mathematics has been partly misused in mainstream economics to justify `unregulated markets'. We identify two key parts of CME, which leads to a natural structure of this article: The first part focusses on an analysis and critique of mathematical models used in mainstream economics, like e.g. the Dynamic Stochastic General Equilibrium (DSGE) in Macroeconomics and the so-called ``Sonnenschein-Mantel-Debreu''-Theorems.
  The aim of the second part is to improve and extend heterodox models using ingredients from modern mathematics and computer science, a method with strong relation to Complexity Economics. We exemplify this idea by describing how methods from Non-Linear Dynamics have been used in Post-Keynesian Macroeconomics', and also discuss (Pseudo-) Goodwin cycles and possible Micro- and Mesofoundations.
  Finally, we outline in which areas a collaboration between mathematicians and heterodox economists could be most promising, and discuss both existing projects in such a direction as well as areas where new models for policy advice are most needed. In an outlook, we discuss the role of (ecological) data, and the need for what we call Progressive Data Science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06015v3</guid>
      <category>econ.GN</category>
      <category>cs.CY</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johannes Buchner</dc:creator>
    </item>
    <item>
      <title>The Quantum Technology Job Market: Data Driven Analysis of 3641 Job Posts</title>
      <link>https://arxiv.org/abs/2503.19004</link>
      <description>arXiv:2503.19004v2 Announce Type: replace-cross 
Abstract: The rapid advancement of Quantum Technology (QT) has created a growing demand for a specialized workforce, spanning across academia and industry. This study presents a quantitative analysis of the QT job market by systematically extracting and classifying thousands of job postings worldwide. The classification pipeline leverages large language models (LLMs) whilst incorporating a "human-in-the-loop" validation process to ensure reliability, achieving an F1-score of 89%: a high level of accuracy. The research identifies key trends in regional job distribution, degree and skill requirements, and the evolving demand for QT-related roles. Findings reveal a strong presence of the QT job market in the United States and Europe, with increasing corporate demand for engineers, software developers, and PhD-level researchers. Despite growing industry applications, the sector remains in its early stages, dominated by large technology firms and requiring significant investment in education and workforce development. The study highlights the need for targeted educational programs, interdisciplinary collaboration, and industry-academic partnerships to bridge the QT workforce gap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19004v2</guid>
      <category>physics.ed-ph</category>
      <category>cs.CY</category>
      <category>quant-ph</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Goorney, Eleni Karydi, Borja Mu\~noz, Otto Santesson, Zeki Can Seskir, Ana Alina Tudoran, Jacob Sherson</dc:creator>
    </item>
    <item>
      <title>JiraiBench: A Bilingual Benchmark for Evaluating Large Language Models' Detection of Human Self-Destructive Behavior Content in Jirai Community</title>
      <link>https://arxiv.org/abs/2503.21679</link>
      <description>arXiv:2503.21679v2 Announce Type: replace-cross 
Abstract: This paper introduces JiraiBench, the first bilingual benchmark for evaluating large language models' effectiveness in detecting self-destructive content across Chinese and Japanese social media communities. Focusing on the transnational "Jirai" (landmine) online subculture that encompasses multiple forms of self-destructive behaviors including drug overdose, eating disorders, and self-harm, we present a comprehensive evaluation framework incorporating both linguistic and cultural dimensions. Our dataset comprises 10,419 Chinese posts and 5,000 Japanese posts with multidimensional annotation along three behavioral categories, achieving substantial inter-annotator agreement. Experimental evaluations across four state-of-the-art models reveal significant performance variations based on instructional language, with Japanese prompts unexpectedly outperforming Chinese prompts when processing Chinese content. This emergent cross-cultural transfer suggests that cultural proximity can sometimes outweigh linguistic similarity in detection tasks. Cross-lingual transfer experiments with fine-tuned models further demonstrate the potential for knowledge transfer between these language systems without explicit target language training. These findings highlight the need for culturally-informed approaches to multilingual content moderation and provide empirical evidence for the importance of cultural context in developing more effective detection systems for vulnerable online communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21679v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yunze Xiao, Tingyu He, Lionel Z. Wang, Yiming Ma, Xingyu Song, Xiaohang Xu, Irene Li, Ka Chung Ng</dc:creator>
    </item>
  </channel>
</rss>
