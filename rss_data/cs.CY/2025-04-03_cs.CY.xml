<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 04 Apr 2025 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Exploring the Societal and Economic Impacts of Artificial Intelligence: A Scenario Generation Methodology</title>
      <link>https://arxiv.org/abs/2504.01992</link>
      <description>arXiv:2504.01992v1 Announce Type: new 
Abstract: This paper explores artificial intelligence's potential societal and economic impacts (AI) through generating scenarios that assess how AI may influence various sectors. We categorize and analyze key factors affecting AI's integration and adoption by applying an Impact-Uncertainty Matrix. A proposed methodology involves querying academic databases, identifying emerging trends and topics, and categorizing these into an impact uncertainty framework. The paper identifies critical areas where AI may bring significant change and outlines potential future scenarios based on these insights. This research aims to inform policymakers, industry leaders, and researchers on the strategic planning required to address the challenges and opportunities AI presents</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01992v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>econ.TH</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos J. Costa, Joao Tiago Aparicio</dc:creator>
    </item>
    <item>
      <title>AI Regulation and Capitalist Growth: Balancing Innovation, Ethics, and Global Governance</title>
      <link>https://arxiv.org/abs/2504.02000</link>
      <description>arXiv:2504.02000v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) is increasingly central to economic growth, promising new efficiencies and markets. This economic significance has sparked debate over AI regulation: do rules and oversight bolster long term growth by building trust and safeguarding the public, or do they constrain innovation and free enterprise? This paper examines the balance between AI regulation and capitalist ideals, focusing on how different approaches to AI data privacy can impact innovation in AI-driven applications. The central question is whether AI regulation enhances or inhibits growth in a capitalist economy. Our analysis synthesizes historical precedents, the current U.S. regulatory landscape, economic projections, legal challenges, and case studies of recent AI policies. We discuss that carefully calibrated AI data privacy regulations-balancing innovation incentives with the public interest can foster sustainable growth by building trust and ensuring responsible data use, while excessive regulation may risk stifling innovation and entrenching incumbents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02000v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vikram Kulothungan, Priya Ranjani Mohan, Deepti Gupta</dc:creator>
    </item>
    <item>
      <title>Urban Computing in the Era of Large Language Models</title>
      <link>https://arxiv.org/abs/2504.02009</link>
      <description>arXiv:2504.02009v1 Announce Type: new 
Abstract: Urban computing has emerged as a multidisciplinary field that harnesses data-driven technologies to address challenges and improve urban living. Traditional approaches, while beneficial, often face challenges with generalization, scalability, and contextual understanding. The advent of Large Language Models (LLMs) offers transformative potential in this domain. This survey explores the intersection of LLMs and urban computing, emphasizing the impact of LLMs in processing and analyzing urban data, enhancing decision-making, and fostering citizen engagement. We provide a concise overview of the evolution and core technologies of LLMs. Additionally, we survey their applications across key urban domains, such as transportation, public safety, and environmental monitoring, summarizing essential tasks and prior works in various urban contexts, while highlighting LLMs' functional roles and implementation patterns. Building on this, we propose potential LLM-based solutions to address unresolved challenges. To facilitate in-depth research, we compile a list of available datasets and tools applicable to diverse urban scenarios. Finally, we discuss the limitations of current approaches and outline future directions for advancing LLMs in urban computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02009v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhonghang Li, Lianghao Xia, Xubin Ren, Jiabin Tang, Tianyi Chen, Yong Xu, Chao Huang</dc:creator>
    </item>
    <item>
      <title>Reinsuring AI: Energy, Agriculture, Finance &amp; Medicine as Precedents for Scalable Governance of Frontier Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2504.02127</link>
      <description>arXiv:2504.02127v1 Announce Type: new 
Abstract: The governance of frontier artificial intelligence (AI) systems--particularly those capable of catastrophic misuse or systemic failure--requires institutional structures that are robust, adaptive, and innovation-preserving. This paper proposes a novel framework for governing such high-stakes models through a three-tiered insurance architecture: (1) mandatory private liability insurance for frontier model developers; (2) an industry-administered risk pool to absorb recurring, non-catastrophic losses; and (3) federally backed reinsurance for tail-risk events. Drawing from historical precedents in nuclear energy (Price-Anderson), terrorism risk (TRIA), agricultural crop insurance, flood reinsurance, and medical malpractice, the proposal shows how the federal government can stabilize private AI insurance markets without resorting to brittle regulation or predictive licensing regimes. The structure aligns incentives between AI developers and downstream stakeholders, transforms safety practices into insurable standards, and enables modular oversight through adaptive eligibility criteria. By focusing on risk-transfer mechanisms rather than prescriptive rules, this framework seeks to render AI safety a structural feature of the innovation ecosystem itself--integrated into capital markets, not external to them. The paper concludes with a legal and administrative feasibility analysis, proposing avenues for statutory authorization and agency placement within existing federal structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02127v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nicholas Stetler</dc:creator>
    </item>
    <item>
      <title>The Author Is Sovereign: A Manifesto for Ethical Copyright in the Age of AI</title>
      <link>https://arxiv.org/abs/2504.02239</link>
      <description>arXiv:2504.02239v1 Announce Type: new 
Abstract: In the age of AI, authorship is being quietly eroded by algorithmic content scraping, legal gray zones like "fair use," and platforms that profit from creative labor without consent or compensation. This short manifesto proposes a radical alternative: a system in which the author is sovereign of their intellectual domain. It presents seven ethical principles that challenge prevailing assumptions about open access, copyright ownership, and the public domain - arguing that voluntary, negotiated consent must replace coercive norms. The text exposes how weakened authorship fuels structural exploitation. In place of reactive solutions, it calls for a new ethic of authorship rooted in consent, dignity, and contractual fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02239v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ricardo Fitas</dc:creator>
    </item>
    <item>
      <title>Am I Being Treated Fairly? A Conceptual Framework for Individuals to Ascertain Fairness</title>
      <link>https://arxiv.org/abs/2504.02461</link>
      <description>arXiv:2504.02461v1 Announce Type: new 
Abstract: Current fairness metrics and mitigation techniques provide tools for practitioners to asses how non-discriminatory Automatic Decision Making (ADM) systems are. What if I, as an individual facing a decision taken by an ADM system, would like to know: Am I being treated fairly? We explore how to create the affordance for users to be able to ask this question of ADM. In this paper, we argue for the reification of fairness not only as a property of ADM, but also as an epistemic right of an individual to acquire information about the decisions that affect them and use that information to contest and seek effective redress against those decisions, in case they are proven to be discriminatory. We examine key concepts from existing research not only in algorithmic fairness but also in explainable artificial intelligence, accountability, and contestability. Integrating notions from these domains, we propose a conceptual framework to ascertain fairness by combining different tools that empower the end-users of ADM systems. Our framework shifts the focus from technical solutions aimed at practitioners to mechanisms that enable individuals to understand, challenge, and verify the fairness of decisions, and also serves as a blueprint for organizations and policymakers, bridging the gap between technical requirements and practical, user-centered accountability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02461v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Juliett Su\'arez Ferreira, Marija Slavkovik, Jorge Casillas</dc:creator>
    </item>
    <item>
      <title>Ethics of Blockchain Technologies</title>
      <link>https://arxiv.org/abs/2504.02504</link>
      <description>arXiv:2504.02504v1 Announce Type: new 
Abstract: This chapter explores three key questions in blockchain ethics. First, it situates blockchain ethics within the broader field of technology ethics, outlining its goals and guiding principles. Second, it examines the unique ethical challenges of blockchain applications, including permissionless systems, incentive mechanisms, and privacy concerns. Key obstacles, such as conceptual modeling and information asymmetries, are identified as critical issues. Finally, the chapter argues that blockchain ethics should be approached as an engineering discipline, emphasizing the analysis and design of trade-offs in complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02504v1</guid>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georgy Ishmaev</dc:creator>
    </item>
    <item>
      <title>A Framework for Developing University Policies on Generative AI Governance: A Cross-national Comparative Study</title>
      <link>https://arxiv.org/abs/2504.02636</link>
      <description>arXiv:2504.02636v1 Announce Type: new 
Abstract: As generative artificial intelligence (GAI) becomes more integrated into higher education and research, universities adopt varied approaches to GAI policy development. To explore these variations, this study conducts a comparative analysis of leading universities in the United States, Japan, and China, examining their institution-wide policies on GAI application and governance. Based on these findings, the study proposes a University Policy Development Framework for GAI (UPDF-GAI) to provide both theoretical insights and practical guidance for universities in developing and refining their GAI policies. A qualitative content analysis of 124 policy documents from 110 universities was conducted, employing thematic coding to synthesize 20 key themes and 9 sub-themes. These themes and sub-themes formed the basis for developing the framework. The analysis reveals varying priorities and focus of GAI policy of universities in different countries. U.S. universities emphasize faculty autonomy, practical application, and policy adaptability, shaped by cutting-edge research and peer collaboration. Japanese universities take a government-regulated approach, prioritizing ethics and risk management, but provide limited support for AI implementation and flexibility. Chinese universities follow a centralized, government-led model, focusing on technology application over early policy development, while actively exploring GAI integration in education and research. The UPDF-GAI framework offers a systematic, adaptable framework for assessing and optimizing GAI policies across different educational contexts. By identifying key policy characteristics, enhancing policy effectiveness, and balancing technology, ethics, and education, enabling universities to develop sustainable, contextually relevant policies that strengthen their digital competitiveness and institutional readiness for AI-driven education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02636v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ming Li, Qin Xie, Ariunaa Enkhtur, Shuoyang Meng, Lilan Chen, Beverley Anne Yamamoto, Fei Cheng, Masayuki Murakami</dc:creator>
    </item>
    <item>
      <title>Optimizing Resource Allocation to Mitigate the Risk of Disruptive Events in Homeland Security and Emergency Management</title>
      <link>https://arxiv.org/abs/2504.02652</link>
      <description>arXiv:2504.02652v1 Announce Type: new 
Abstract: Homeland security in the United States faces a daunting task due to the multiple threats and hazards that can occur. Natural disasters, human-caused incidents such as terrorist attacks, and technological failures can result in significant damage, fatalities, injuries, and economic losses. The increasing frequency and severity of disruptive events in the United States highlight the urgent need for effectively allocating resources in homeland security and emergency preparedness. This article presents an optimization-based decision support model to help homeland security policymakers identify and select projects that best mitigate the risk of threats and hazards while satisfying a budget constraint. The model incorporates multiple hazards, probabilistic risk assessments, and multidimensional consequences and integrates historical data and publicly available sources to evaluate and select the most effective risk mitigation projects and optimize resource allocation across various disaster scenarios. We apply this model to the state of Iowa, considering 16 hazards, six types of consequences, and 52 mitigation projects. Our results demonstrate how different budget levels influence project selection, emphasizing cost-effective solutions that maximize risk reduction. Sensitivity analysis examines the robustness of project selection under varying effectiveness assumptions and consequence estimations. The findings offer critical insights for policymakers in homeland security and emergency management and provide a basis for more efficient resource allocation and improved disaster resilience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02652v1</guid>
      <category>cs.CY</category>
      <category>math.OC</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parastoo Akbari, Cameron A. MacKenzie</dc:creator>
    </item>
    <item>
      <title>Disinformation about autism in Latin America and the Caribbean: Mapping 150 false causes and 150 false cures of ASD in conspiracy theory communities on Telegram</title>
      <link>https://arxiv.org/abs/2504.01991</link>
      <description>arXiv:2504.01991v1 Announce Type: cross 
Abstract: How do conspiracy theory communities in Latin America and the Caribbean structure, articulate, and sustain the dissemination of disinformation about autism? To answer this question, this research investigates the structuring, articulation, and promotion of autism-related disinformation in conspiracy theory communities in Latin America and the Caribbean. By analyzing publications from 1,659 Telegram communities over ten years (2015 - 2025) and examining more than 58 million pieces of shared content from approximately 5.3 million users, this study explores how false narratives about autism are promoted, including unfounded claims about its causes and promises of miraculous cures. The adopted methodology combines network analysis, time series analysis, thematic clustering, and content analysis, enabling the identification of dissemination patterns, key influencers, and interconnections with other conspiracy theories. Among the key findings, Brazilian communities stand out as the leading producers and distributors of these narratives in the region, accounting for 46% of the analyzed content. Additionally, there has been an exponential 15,000% (x151) increase in the volume of autism-related disinformation since the COVID-19 pandemic in Latin America and the Caribbean, highlighting the correlation between health crises and the rise of conspiracy beliefs. The research also reveals that false cures, such as chlorine dioxide (CDS), ozone therapy, and extreme diets, are widely promoted within these communities and commercially exploited, often preying on desperate families in exchange for money. By addressing the research question, this study aims to contribute to the understanding of the disinformation ecosystem and proposes critical reflections on how to confront these harmful narratives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01991v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ergon Cugler de Moraes Silva, Arthur Ataide Ferreira Garcia, Guilherme de Almeida, Julie Ricard</dc:creator>
    </item>
    <item>
      <title>Exploring the Privacy and Security Challenges Faced by Migrant Domestic Workers in Chinese Smart Homes</title>
      <link>https://arxiv.org/abs/2504.02149</link>
      <description>arXiv:2504.02149v1 Announce Type: cross 
Abstract: The growing use of smart home devices poses considerable privacy and security challenges, especially for individuals like migrant domestic workers (MDWs) who may be surveilled by their employers. This paper explores the privacy and security challenges experienced by MDWs in multi-user smart homes through in-depth semi-structured interviews with 26 MDWs and 5 staff members of agencies that recruit and/or train domestic workers in China. Our findings reveal that the relationships between MDWs, their employers, and agencies are characterized by significant power imbalances, influenced by Chinese cultural and social factors (such as Confucianism and collectivism), as well as legal ones. Furthermore, the widespread and normalized use of surveillance technologies in China, particularly in public spaces, exacerbates these power imbalances, reinforcing a sense of constant monitoring and control. Drawing on our findings, we provide recommendations to domestic worker agencies and policymakers to address the privacy and security challenges facing MDWs in Chinese smart homes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02149v1</guid>
      <category>cs.HC</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shijing He, Xiao Zhan, Yaxiong Lei, Yueyan Liu, Ruba Abu-Salma, Jose Such</dc:creator>
    </item>
    <item>
      <title>Niche Dynamics in Complex Online Community Ecosystems</title>
      <link>https://arxiv.org/abs/2504.02153</link>
      <description>arXiv:2504.02153v1 Announce Type: cross 
Abstract: Online communities are important organizational forms where members socialize and share information. Curiously, different online communities often overlap considerably in topic and membership. Recent research has investigated competition and mutualism among overlapping online communities through the lens of organizational ecology; however, it has not accounted for how the nonlinear dynamics of online attention may lead to episodic competition and mutualism. Neither has it explored the origins of competition and mutualism in the processes by which online communities select or adapt to their niches. This paper presents a large-scale study of 8,806 Reddit communities belonging to 1,919 clusters of high user overlap over a 5-year period. The method uses nonlinear time series methods to infer bursty, often short-lived ecological dynamics. Results reveal that mutualism episodes are longer lived and slightly more frequent than competition episodes. Next, it tests whether online communities find their niches by specializing to avoid competition using panel regression models. It finds that competitive ecological interactions lead to decreasing topic and user overlaps; however, changes that decrease such niche overlaps do not lead to mutualism. The discussion proposes that future designs may enable online community ecosystem management by informing online community leaders to organize "spin-off" communities or via feeds and recommendations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02153v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nathan TeBlunthuis</dc:creator>
    </item>
    <item>
      <title>LLM Social Simulations Are a Promising Research Method</title>
      <link>https://arxiv.org/abs/2504.02234</link>
      <description>arXiv:2504.02234v1 Announce Type: cross 
Abstract: Accurate and verifiable large language model (LLM) simulations of human research subjects promise an accessible data source for understanding human behavior and training new AI systems. However, results to date have been limited, and few social scientists have adopted these methods. In this position paper, we argue that the promise of LLM social simulations can be achieved by addressing five tractable challenges. We ground our argument in a literature survey of empirical comparisons between LLMs and human research subjects, commentaries on the topic, and related work. We identify promising directions with prompting, fine-tuning, and complementary methods. We believe that LLM social simulations can already be used for exploratory research, such as pilot experiments for psychology, economics, sociology, and marketing. More widespread use may soon be possible with rapidly advancing LLM capabilities, and researchers should prioritize developing conceptual models and evaluations that can be iteratively deployed and refined at pace with ongoing AI advances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02234v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacy Reese Anthis, Ryan Liu, Sean M. Richardson, Austin C. Kozlowski, Bernard Koch, James Evans, Erik Brynjolfsson, Michael Bernstein</dc:creator>
    </item>
    <item>
      <title>DaKultur: Evaluating the Cultural Awareness of Language Models for Danish with Native Speakers</title>
      <link>https://arxiv.org/abs/2504.02403</link>
      <description>arXiv:2504.02403v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have seen widespread societal adoption. However, while they are able to interact with users in languages beyond English, they have been shown to lack cultural awareness, providing anglocentric or inappropriate responses for underrepresented language communities. To investigate this gap and disentangle linguistic versus cultural proficiency, we conduct the first cultural evaluation study for the mid-resource language of Danish, in which native speakers prompt different models to solve tasks requiring cultural awareness. Our analysis of the resulting 1,038 interactions from 63 demographically diverse participants highlights open challenges to cultural adaptation: Particularly, how currently employed automatically translated data are insufficient to train or measure cultural adaptation, and how training on native-speaker data can more than double response acceptance rates. We release our study data as DaKultur - the first native Danish cultural awareness dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02403v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max M\"uller-Eberstein, Mike Zhang, Elisa Bassignana, Peter Brunsgaard Trolle, Rob van der Goot</dc:creator>
    </item>
    <item>
      <title>A Framework for Situating Innovations, Opportunities, and Challenges in Advancing Vertical Systems with Large AI Models</title>
      <link>https://arxiv.org/abs/2504.02793</link>
      <description>arXiv:2504.02793v1 Announce Type: cross 
Abstract: Large artificial intelligence (AI) models have garnered significant attention for their remarkable, often "superhuman", performance on standardized benchmarks. However, when these models are deployed in high-stakes verticals such as healthcare, education, and law, they often reveal notable limitations. For instance, they exhibit brittleness to minor variations in input data, present contextually uninformed decisions in critical settings, and undermine user trust by confidently producing or reproducing inaccuracies. These challenges in applying large models necessitate cross-disciplinary innovations to align the models' capabilities with the needs of real-world applications. We introduce a framework that addresses this gap through a layer-wise abstraction of innovations aimed at meeting users' requirements with large models. Through multiple case studies, we illustrate how researchers and practitioners across various fields can operationalize this framework. Beyond modularizing the pipeline of transforming large models into useful "vertical systems", we also highlight the dynamism that exists within different layers of the framework. Finally, we discuss how our framework can guide researchers and practitioners to (i) optimally situate their innovations (e.g., when vertical-specific insights can empower broadly impactful vertical-agnostic innovations), (ii) uncover overlooked opportunities (e.g., spotting recurring problems across verticals to develop practically useful foundation models instead of chasing benchmarks), and (iii) facilitate cross-disciplinary communication of critical challenges (e.g., enabling a shared vocabulary for AI developers, domain experts, and human-computer interaction scholars).</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02793v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gaurav Verma, Jiawei Zhou, Mohit Chandra, Srijan Kumar, Munmun De Choudhury</dc:creator>
    </item>
    <item>
      <title>Can AI Solve the Peer Review Crisis? A Large Scale Cross Model Experiment of LLMs' Performance and Biases in Evaluating over 1000 Economics Papers</title>
      <link>https://arxiv.org/abs/2502.00070</link>
      <description>arXiv:2502.00070v2 Announce Type: replace 
Abstract: This study examines the potential of large language models (LLMs) to augment the academic peer review process by reliably evaluating the quality of economics research without introducing systematic bias. We conduct one of the first large-scale experimental assessments of four LLMs (GPT-4o, Claude 3.5, Gemma 3, and LLaMA 3.3) across two complementary experiments. In the first, we use nonparametric binscatter and linear regression techniques to analyze over 29,000 evaluations of 1,220 anonymized papers drawn from 110 economics journals excluded from the training data of current LLMs, along with a set of AI-generated submissions. The results show that LLMs consistently distinguish between higher- and lower-quality research based solely on textual content, producing quality gradients that closely align with established journal prestige measures. Claude and Gemma perform exceptionally well in capturing these gradients, while GPT excels in detecting AI-generated content. The second experiment comprises 8,910 evaluations designed to assess whether LLMs replicate human like biases in single blind reviews. By systematically varying author gender, institutional affiliation, and academic prominence across 330 papers, we find that GPT, Gemma, and LLaMA assign significantly higher ratings to submissions from top male authors and elite institutions relative to the same papers presented anonymously. These results emphasize the importance of excluding author-identifying information when deploying LLMs in editorial screening. Overall, our findings provide compelling evidence and practical guidance for integrating LLMs into peer review to enhance efficiency, improve accuracy, and promote equity in the publication process of economics research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00070v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pat Pataranutaporn, Nattavudh Powdthavee, Chayapatr Achiwaranguprok, Pattie Maes</dc:creator>
    </item>
    <item>
      <title>Large Language Models in Healthcare</title>
      <link>https://arxiv.org/abs/2503.04748</link>
      <description>arXiv:2503.04748v2 Announce Type: replace 
Abstract: Large language models (LLMs) hold promise for transforming healthcare, from streamlining administrative and clinical workflows to enriching patient engagement and advancing clinical decision-making. However, their successful integration requires rigorous development, adaptation, and evaluation strategies tailored to clinical needs. In this Review, we highlight recent advancements, explore emerging opportunities for LLM-driven innovation, and propose a framework for their responsible implementation in healthcare settings. We examine strategies for adapting LLMs to domain-specific healthcare tasks, such as fine-tuning, prompt engineering, and multimodal integration with electronic health records. We also summarize various evaluation metrics tailored to healthcare, addressing clinical accuracy, fairness, robustness, and patient outcomes. Furthermore, we discuss the challenges associated with deploying LLMs in healthcare--including data privacy, bias mitigation, regulatory compliance, and computational sustainability--and underscore the need for interdisciplinary collaboration. Finally, these challenges present promising future research directions for advancing LLM implementation in clinical settings and healthcare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04748v2</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mohammed Al-Garadi, Tushar Mungle, Abdulaziz Ahmed, Abeed Sarker, Zhuqi Miao, Michael E. Matheny</dc:creator>
    </item>
    <item>
      <title>LEACE: Perfect linear concept erasure in closed form</title>
      <link>https://arxiv.org/abs/2306.03819</link>
      <description>arXiv:2306.03819v4 Announce Type: replace-cross 
Abstract: Concept erasure aims to remove specified features from an embedding. It can improve fairness (e.g. preventing a classifier from using gender or race) and interpretability (e.g. removing a concept to observe changes in model behavior). We introduce LEAst-squares Concept Erasure (LEACE), a closed-form method which provably prevents all linear classifiers from detecting a concept while changing the embedding as little as possible, as measured by a broad class of norms. We apply LEACE to large language models with a novel procedure called "concept scrubbing," which erases target concept information from every layer in the network. We demonstrate our method on two tasks: measuring the reliance of language models on part-of-speech information, and reducing gender bias in BERT embeddings. Code is available at https://github.com/EleutherAI/concept-erasure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.03819v4</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward Raff, Stella Biderman</dc:creator>
    </item>
    <item>
      <title>Networking Systems for Video Anomaly Detection: A Tutorial and Survey</title>
      <link>https://arxiv.org/abs/2405.10347</link>
      <description>arXiv:2405.10347v4 Announce Type: replace-cross 
Abstract: The increasing utilization of surveillance cameras in smart cities, coupled with the surge of online video applications, has heightened concerns regarding public security and privacy protection, which propelled automated Video Anomaly Detection (VAD) into a fundamental research task within the Artificial Intelligence (AI) community. With the advancements in deep learning and edge computing, VAD has made significant progress and advances synergized with emerging applications in smart cities and video internet, which has moved beyond the conventional research scope of algorithm engineering to deployable Networking Systems for VAD (NSVAD), a practical hotspot for intersection exploration in the AI, IoVT, and computing fields. In this article, we delineate the foundational assumptions, learning frameworks, and applicable scenarios of various deep learning-driven VAD routes, offering an exhaustive tutorial for novices in NSVAD. In addition, this article elucidates core concepts by reviewing recent advances and typical solutions and aggregating available research resources accessible at https://github.com/fdjingliu/NSVAD. Lastly, this article projects future development trends and discusses how the integration of AI and computing technologies can address existing research challenges and promote open opportunities, serving as an insightful guide for prospective researchers and engineers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10347v4</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jing Liu, Yang Liu, Jieyu Lin, Jielin Li, Liang Cao, Peng Sun, Bo Hu, Liang Song, Azzedine Boukerche, Victor C. M. Leung</dc:creator>
    </item>
    <item>
      <title>The Potential of Citizen Platforms for Requirements Engineering of Large Socio-Technical Software Systems</title>
      <link>https://arxiv.org/abs/2410.03195</link>
      <description>arXiv:2410.03195v2 Announce Type: replace-cross 
Abstract: Participatory citizen platforms are innovative solutions to digitally better engage citizens in policy-making and deliberative democracy in general. Although these platforms have been used also in an engineering context, thus far, there is no existing work for connecting the platforms to requirements engineering. The present paper fills this notable gap. In addition to discussing the platforms in conjunction with requirements engineering, the paper elaborates potential advantages and disadvantages, thus paving the way for a future pilot study in a software engineering context. With these engineering tenets, the paper also contributes to the research of large socio-technical software systems in a public sector context, including their implementation and governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03195v2</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-88531-0_21</arxiv:DOI>
      <dc:creator>Jukka Ruohonen, Kalle Hjerppe</dc:creator>
    </item>
    <item>
      <title>Measuring Large Language Models Capacity to Annotate Journalistic Sourcing</title>
      <link>https://arxiv.org/abs/2501.00164</link>
      <description>arXiv:2501.00164v2 Announce Type: replace-cross 
Abstract: Since the launch of ChatGPT in late 2022, the capacities of Large Language Models and their evaluation have been in constant discussion and evaluation both in academic research and in the industry. Scenarios and benchmarks have been developed in several areas such as law, medicine and math (Bommasani et al., 2023) and there is continuous evaluation of model variants. One area that has not received sufficient scenario development attention is journalism, and in particular journalistic sourcing and ethics. Journalism is a crucial truth-determination function in democracy (Vincent, 2023), and sourcing is a crucial pillar to all original journalistic output. Evaluating the capacities of LLMs to annotate stories for the different signals of sourcing and how reporters justify them is a crucial scenario that warrants a benchmark approach. It offers potential to build automated systems to contrast more transparent and ethically rigorous forms of journalism with everyday fare. In this paper we lay out a scenario to evaluate LLM performance on identifying and annotating sourcing in news stories on a five-category schema inspired from journalism studies (Gans, 2004). We offer the use case, our dataset and metrics and as the first step towards systematic benchmarking. Our accuracy findings indicate LLM-based approaches have more catching to do in identifying all the sourced statements in a story, and equally, in matching the type of sources. An even harder task is spotting source justifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00164v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subramaniam Vincent, Phoebe Wang, Zhan Shi, Sahas Koka, Yi Fang</dc:creator>
    </item>
    <item>
      <title>Limits of trust in medical AI</title>
      <link>https://arxiv.org/abs/2503.16692</link>
      <description>arXiv:2503.16692v2 Announce Type: replace-cross 
Abstract: Artificial intelligence (AI) is expected to revolutionize the practice of medicine. Recent advancements in the field of deep learning have demonstrated success in a variety of clinical tasks: detecting diabetic retinopathy from images, predicting hospital readmissions, aiding in the discovery of new drugs, etc. AI's progress in medicine, however, has led to concerns regarding the potential effects of this technology upon relationships of trust in clinical practice. In this paper, I will argue that there is merit to these concerns, since AI systems can be relied upon, and are capable of reliability, but cannot be trusted, and are not capable of trustworthiness. Insofar as patients are required to rely upon AI systems for their medical decision-making, there is potential for this to produce a deficit of trust in relationships in clinical practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16692v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1136/medethics-2019-105935</arxiv:DOI>
      <arxiv:journal_reference>Journal of Medical Ethics 46(7): 478-481 (2020)</arxiv:journal_reference>
      <dc:creator>Joshua Hatherley</dc:creator>
    </item>
    <item>
      <title>Multi-Modal Framing Analysis of News</title>
      <link>https://arxiv.org/abs/2503.20960</link>
      <description>arXiv:2503.20960v2 Announce Type: replace-cross 
Abstract: Automated frame analysis of political communication is a popular task in computational social science that is used to study how authors select aspects of a topic to frame its reception. So far, such studies have been narrow, in that they use a fixed set of pre-defined frames and focus only on the text, ignoring the visual contexts in which those texts appear. Especially for framing in the news, this leaves out valuable information about editorial choices, which include not just the written article but also accompanying photographs. To overcome such limitations, we present a method for conducting multi-modal, multi-label framing analysis at scale using large (vision-)language models. Grounding our work in framing theory, we extract latent meaning embedded in images used to convey a certain point and contrast that to the text by comparing the respective frames used. We also identify highly partisan framing of topics with issue-specific frame analysis found in prior qualitative work. We demonstrate a method for doing scalable integrative framing analysis of both text and image in news, providing a more complete picture for understanding media bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20960v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arnav Arora, Srishti Yadav, Maria Antoniak, Serge Belongie, Isabelle Augenstein</dc:creator>
    </item>
  </channel>
</rss>
