<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Jun 2024 04:00:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Conditional uncorrelation equals independence</title>
      <link>https://arxiv.org/abs/2406.01849</link>
      <description>arXiv:2406.01849v1 Announce Type: new 
Abstract: It is well known that the independent random variables $X$ and $Y$ are uncorrelated in the sense $E[XY]=E[X]\cdot E[Y]$ and that the implication may be reversed in very specific cases only. This paper proves that under general assumptions the conditional uncorrelation of random variables, where the conditioning takes place over the suitable class of test sets, is equivalent to the independence. It is also shown that the mutual independence of $X_1,\dots,X_n$ is equivalent to the fact that any conditional correlation matrix equals to the identity matrix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01849v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dawid Tar{\l}owski</dc:creator>
    </item>
    <item>
      <title>An Axiomatisation of Error Intolerant Estimation</title>
      <link>https://arxiv.org/abs/2406.02031</link>
      <description>arXiv:2406.02031v1 Announce Type: new 
Abstract: Point estimation is a fundamental statistical task. Given the wide selection of available point estimators, it is unclear, however, what, if any, would be universally-agreed theoretical reasons to generally prefer one such estimator over another. In this paper, we define a class of estimation scenarios which includes commonly-encountered problem situations such as both ``high stakes'' estimation and scientific inference, and introduce a new class of estimators, Error Intolerance Candidates (EIC) estimators, which we prove is optimal for it.
  EIC estimators are parameterised by an externally-given loss function. We prove, however, that even without such a loss function if one accepts a small number of incontrovertible-seeming assumptions regarding what constitutes a reasonable loss function, the optimal EIC estimator can be characterised uniquely.
  The optimal estimator derived in this second case is a previously-studied combination of maximum a posteriori (MAP) estimation and Wallace-Freeman (WF) estimation which has long been advocated among Minimum Message Length (MML) researchers, where it is derived as an approximation to the information-theoretic Strict MML estimator. Our results provide a novel justification for it that is purely Bayesian and requires neither approximations nor coding, placing both MAP and WF as special cases in the larger class of EIC estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02031v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Brand</dc:creator>
    </item>
    <item>
      <title>Singular Subspace Perturbation Bounds via Rectangular Random Matrix Diffusions</title>
      <link>https://arxiv.org/abs/2406.02502</link>
      <description>arXiv:2406.02502v1 Announce Type: new 
Abstract: Given a matrix $A \in \mathbb{R}^{m\times d}$ with singular values $\sigma_1\geq \cdots \geq \sigma_d$, and a random matrix $G \in \mathbb{R}^{m\times d}$ with iid $N(0,T)$ entries for some $T&gt;0$, we derive new bounds on the Frobenius distance between subspaces spanned by the top-$k$ (right) singular vectors of $A$ and $A+G$. This problem arises in numerous applications in statistics where a data matrix may be corrupted by Gaussian noise, and in the analysis of the Gaussian mechanism in differential privacy, where Gaussian noise is added to data to preserve private information. We show that, for matrices $A$ where the gaps in the top-$k$ singular values are roughly $\Omega(\sigma_k-\sigma_{k+1})$ the expected Frobenius distance between the subspaces is $\tilde{O}(\frac{\sqrt{d}}{\sigma_k-\sigma_{k+1}} \times \sqrt{T})$, improving on previous bounds by a factor of $\frac{\sqrt{m}}{\sqrt{d}} \sqrt{k}$. To obtain our bounds we view the perturbation to the singular vectors as a diffusion process -- the Dyson-Bessel process -- and use tools from stochastic calculus to track the evolution of the subspace spanned by the top-$k$ singular vectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02502v1</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peiyao Lai, Oren Mangoubi</dc:creator>
    </item>
    <item>
      <title>Orthogonal Causal Calibration</title>
      <link>https://arxiv.org/abs/2406.01933</link>
      <description>arXiv:2406.01933v1 Announce Type: cross 
Abstract: Estimates of causal parameters such as conditional average treatment effects and conditional quantile treatment effects play an important role in real-world decision making. Given this importance, one should ensure these estimators are calibrated. While there is a rich literature on calibrating estimators of non-causal parameters, very few methods have been derived for calibrating estimators of causal parameters, or more generally estimators of quantities involving nuisance parameters.
  In this work, we provide a general framework for calibrating predictors involving nuisance estimation. We consider a notion of calibration defined with respect to an arbitrary, nuisance-dependent loss $\ell$, under which we say an estimator $\theta$ is calibrated if its predictions cannot be changed on any level set to decrease loss. We prove generic upper bounds on the calibration error of any causal parameter estimate $\theta$ with respect to any loss $\ell$ using a concept called Neyman Orthogonality. Our bounds involve two decoupled terms - one measuring the error in estimating the unknown nuisance parameters, and the other representing the calibration error in a hypothetical world where the learned nuisance estimates were true. We use our bound to analyze the convergence of two sample splitting algorithms for causal calibration. One algorithm, which applies to universally orthogonalizable loss functions, transforms the data into generalized pseudo-outcomes and applies an off-the-shelf calibration procedure. The other algorithm, which applies to conditionally orthogonalizable loss functions, extends the classical uniform mass binning algorithm to include nuisance estimation. Our results are exceedingly general, showing that essentially any existing calibration algorithm can be used in causal settings, with additional loss only arising from errors in nuisance estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01933v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Justin Whitehouse, Christopher Jung, Vasilis Syrgkanis, Bryan Wilder, Zhiwei Steven Wu</dc:creator>
    </item>
    <item>
      <title>A sequential test procedure for the choice of the number of regimes in multivariate nonlinear models</title>
      <link>https://arxiv.org/abs/2406.02152</link>
      <description>arXiv:2406.02152v1 Announce Type: cross 
Abstract: This paper proposes a sequential test procedure for determining the number of regimes in nonlinear multivariate autoregressive models. The procedure relies on linearity and no additional nonlinearity tests for both multivariate smooth transition and threshold autoregressive models. We conduct a simulation study to evaluate the finite-sample properties of the proposed test in small samples. Our findings indicate that the test exhibits satisfactory size properties, with the rescaled version of the Lagrange Multiplier test statistics demonstrating the best performance in most simulation settings. The sequential procedure is also applied to two empirical cases, the US monthly interest rates and Icelandic river flows. In both cases, the detected number of regimes aligns well with the existing literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02152v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Bucci</dc:creator>
    </item>
    <item>
      <title>Contextual Dynamic Pricing: Algorithms, Optimality, and Local Differential Privacy Constraints</title>
      <link>https://arxiv.org/abs/2406.02424</link>
      <description>arXiv:2406.02424v1 Announce Type: cross 
Abstract: We study the contextual dynamic pricing problem where a firm sells products to $T$ sequentially arriving consumers that behave according to an unknown demand model. The firm aims to maximize its revenue, i.e. minimize its regret over a clairvoyant that knows the model in advance. The demand model is a generalized linear model (GLM), allowing for a stochastic feature vector in $\mathbb R^d$ that encodes product and consumer information. We first show that the optimal regret upper bound is of order $\sqrt{dT}$, up to a logarithmic factor, improving upon existing upper bounds in the literature by a $\sqrt{d}$ factor. This sharper rate is materialised by two algorithms: a confidence bound-type (supCB) algorithm and an explore-then-commit (ETC) algorithm. A key insight of our theoretical result is an intrinsic connection between dynamic pricing and the contextual multi-armed bandit problem with many arms based on a careful discretization. We further study contextual dynamic pricing under the local differential privacy (LDP) constraints. In particular, we propose a stochastic gradient descent based ETC algorithm that achieves an optimal regret upper bound of order $d\sqrt{T}/\epsilon$, up to a logarithmic factor, where $\epsilon&gt;0$ is the privacy parameter. The regret upper bounds with and without LDP constraints are accompanied by newly constructed minimax lower bounds, which further characterize the cost of privacy. Extensive numerical experiments and a real data application on online lending are conducted to illustrate the efficiency and practical value of the proposed algorithms in dynamic pricing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02424v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zifeng Zhao, Feiyu Jiang, Yi Yu</dc:creator>
    </item>
    <item>
      <title>Conditional Aalen--Johansen estimation</title>
      <link>https://arxiv.org/abs/2303.02119</link>
      <description>arXiv:2303.02119v2 Announce Type: replace 
Abstract: The conditional Aalen--Johansen estimator, a general-purpose non-parametric estimator of conditional state occupation probabilities, is introduced. The estimator is applicable for any finite-state jump process and supports conditioning on external as well as internal covariate information. The conditioning feature permits for a much more detailed analysis of the distributional characteristics of the process. The estimator reduces to the conditional Kaplan--Meier estimator in the special case of a survival model and also englobes other, more recent, landmark estimators when covariates are discrete. Strong uniform consistency and asymptotic normality are established under lax moment conditions on the multivariate counting process, allowing in particular for an unbounded number of transitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.02119v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Bladt, Christian Furrer</dc:creator>
    </item>
    <item>
      <title>Random measure priors in Bayesian recovery from sketches</title>
      <link>https://arxiv.org/abs/2303.15029</link>
      <description>arXiv:2303.15029v3 Announce Type: replace 
Abstract: This paper introduces a Bayesian nonparametric approach to frequency recovery from lossy-compressed discrete data, leveraging all information contained in a sketch obtained through random hashing. By modeling the data points as random samples from an unknown discrete distribution endowed with a Poisson-Kingman prior, we derive the posterior distribution of a symbol's empirical frequency given the sketch. This leads to principled frequency estimates through mean functionals, e.g., the posterior mean, median and mode. We highlight applications of this general result to Dirichlet process and Pitman-Yor process priors. Notably, we prove that the former prior uniquely satisfies a sufficiency property that simplifies the posterior distribution, while the latter enables a convenient large-sample asymptotic approximation. Additionally, we extend our approach to the problem of cardinality recovery, estimating the number of distinct symbols in the sketched dataset. Our approach to frequency recovery also adapts to a more general ``traits'' setting, where each data point has integer levels of association with multiple symbols, typically referred to as ``traits''. By employing a generalized Indian buffet process, we compute the posterior distribution of a trait's frequency using both the Poisson and Bernoulli distributions for the trait association levels, respectively yielding exact and approximate posterior frequency distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.15029v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Beraha, Stefano Favaro, Matteo Sesia</dc:creator>
    </item>
    <item>
      <title>Arcade Processes for Informed Martingale Interpolation</title>
      <link>https://arxiv.org/abs/2301.05936</link>
      <description>arXiv:2301.05936v2 Announce Type: replace-cross 
Abstract: Arcade processes are a class of continuous stochastic processes that interpolate in a strong sense, i.e., omega by omega, between zeros at fixed pre-specified times. Their additive randomization allows one to match any finite sequence of target random variables, indexed by the given fixed dates, on the whole probability space. The randomized arcade processes (RAPs) can thus be interpreted as a generalization of anticipative stochastic bridges. The filtrations generated by these processes are utilized to construct a class of martingales which interpolate between the given target random variables. These so-called filtered arcade martingales (FAMs) are almost-sure solutions to the martingale interpolation problem and reveal an underlying stochastic filtering structure. In the special case of conditionally-Markov randomized arcade processes, the dynamics of FAMs are informed by Bayesian updating. The same ideas are applied to filtered arcade reverse-martingales, which are constructed in a similar fashion, using reverse-filtrations of RAPs, instead. As a potential application of this theory, optimal transportation is explored: FAMs may be used to introduce noise in martingale optimal transport, in a similar fashion to how Schr\"odinger's problem introduces noise in optimal transport. This information-based approach to transport is concerned with selecting an optimal martingale coupling for the target random variables under the influence of the noise that is generated by an arcade process, and suggests application in finance or climate science, for instance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.05936v2</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Georges Kassis, Andrea Macrina</dc:creator>
    </item>
    <item>
      <title>Piecewise Polynomial Regression of Tame Functions via Integer Programming</title>
      <link>https://arxiv.org/abs/2311.13544</link>
      <description>arXiv:2311.13544v3 Announce Type: replace-cross 
Abstract: Tame functions are a class of nonsmooth, nonconvex functions, which feature in a wide range of applications: functions encountered in the training of deep neural networks with all common activations, value functions of mixed-integer programs, or wave functions of small molecules. We consider approximating tame functions with piecewise polynomial functions. We bound the quality of approximation of a tame function by a piecewise polynomial function with a given number of segments on any full-dimensional cube. We also present the first mixed-integer programming formulation of piecewise polynomial regression. Together, these can be used to estimate tame functions. We demonstrate promising computational results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13544v3</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gilles Bareilles, Johannes Aspman, Jiri Nemecek, Jakub Marecek</dc:creator>
    </item>
    <item>
      <title>Sliding down the stairs: how correlated latent variables accelerate learning with neural networks</title>
      <link>https://arxiv.org/abs/2404.08602</link>
      <description>arXiv:2404.08602v2 Announce Type: replace-cross 
Abstract: Neural networks extract features from data using stochastic gradient descent (SGD). In particular, higher-order input cumulants (HOCs) are crucial for their performance. However, extracting information from the $p$th cumulant of $d$-dimensional inputs is computationally hard: the number of samples required to recover a single direction from an order-$p$ tensor (tensor PCA) using online SGD grows as $d^{p-1}$, which is prohibitive for high-dimensional inputs. This result raises the question of how neural networks extract relevant directions from the HOCs of their inputs efficiently. Here, we show that correlations between latent variables along the directions encoded in different input cumulants speed up learning from higher-order correlations. We show this effect analytically by deriving nearly sharp thresholds for the number of samples required by a single neuron to weakly-recover these directions using online SGD from a random start in high dimensions. Our analytical results are confirmed in simulations of two-layer neural networks and unveil a new mechanism for hierarchical learning in neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08602v2</guid>
      <category>stat.ML</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Bardone, Sebastian Goldt</dc:creator>
    </item>
    <item>
      <title>Generalised Bayes Linear Inference</title>
      <link>https://arxiv.org/abs/2405.14145</link>
      <description>arXiv:2405.14145v2 Announce Type: replace-cross 
Abstract: Motivated by big data and the vast parameter spaces in modern machine learning models, optimisation approaches to Bayesian inference have seen a surge in popularity in recent years. In this paper, we address the connection between the popular new methods termed generalised Bayesian inference and Bayes linear methods. We propose a further generalisation to Bayesian inference that unifies these and other recent approaches by considering the Bayesian inference problem as one of finding the closest point in a particular solution space to a data generating process, where these notions differ depending on user-specified geometries and foundational belief systems. Motivated by this framework, we propose a generalisation to Bayes linear approaches that enables fast and principled inferences that obey the coherence requirements implied by domain restrictions on random quantities. We demonstrate the efficacy of generalised Bayes linear inference on a number of examples, including monotonic regression and inference for spatial counts. This paper is accompanied by an R package available at github.com/astfalckl/bayeslinear.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14145v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lachlan Astfalck, Cassandra Bird, Daniel Williamson</dc:creator>
    </item>
    <item>
      <title>Semi-Supervised Learning guided by the Generalized Bayes Rule under Soft Revision</title>
      <link>https://arxiv.org/abs/2405.15294</link>
      <description>arXiv:2405.15294v2 Announce Type: replace-cross 
Abstract: We provide a theoretical and computational investigation of the Gamma-Maximin method with soft revision, which was recently proposed as a robust criterion for pseudo-label selection (PLS) in semi-supervised learning. Opposed to traditional methods for PLS we use credal sets of priors ("generalized Bayes") to represent the epistemic modeling uncertainty. These latter are then updated by the Gamma-Maximin method with soft revision. We eventually select pseudo-labeled data that are most likely in light of the least favorable distribution from the so updated credal set. We formalize the task of finding optimal pseudo-labeled data w.r.t. the Gamma-Maximin method with soft revision as an optimization problem. A concrete implementation for the class of logistic models then allows us to compare the predictive power of the method with competing approaches. It is observed that the Gamma-Maximin method with soft revision can achieve very promising results, especially when the proportion of labeled data is low.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15294v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stefan Dietrich, Julian Rodemann, Christoph Jansen</dc:creator>
    </item>
    <item>
      <title>Comparing experiments in discounted problems</title>
      <link>https://arxiv.org/abs/2405.16458</link>
      <description>arXiv:2405.16458v2 Announce Type: replace-cross 
Abstract: This paper compares statistical experiments in discounted problems, ranging from the simplest ones where the state is fixed and the flow of information exogenous to more complex ones, where the decision-maker controls the flow of information or the state changes over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16458v2</guid>
      <category>econ.TH</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ludovic Renou, Xavier Venel</dc:creator>
    </item>
    <item>
      <title>Unveiling the Cycloid Trajectory of EM Iterations in Mixed Linear Regression</title>
      <link>https://arxiv.org/abs/2405.18237</link>
      <description>arXiv:2405.18237v2 Announce Type: replace-cross 
Abstract: We study the trajectory of iterations and the convergence rates of the Expectation-Maximization (EM) algorithm for two-component Mixed Linear Regression (2MLR). The fundamental goal of MLR is to learn the regression models from unlabeled observations. The EM algorithm finds extensive applications in solving the mixture of linear regressions. Recent results have established the super-linear convergence of EM for 2MLR in the noiseless and high SNR settings under some assumptions and its global convergence rate with random initialization has been affirmed. However, the exponent of convergence has not been theoretically estimated and the geometric properties of the trajectory of EM iterations are not well-understood. In this paper, first, using Bessel functions we provide explicit closed-form expressions for the EM updates under all SNR regimes. Then, in the noiseless setting, we completely characterize the behavior of EM iterations by deriving a recurrence relation at the population level and notably show that all the iterations lie on a certain cycloid. Based on this new trajectory-based analysis, we exhibit the theoretical estimate for the exponent of super-linear convergence and further improve the statistical error bound at the finite-sample level. Our analysis provides a new framework for studying the behavior of EM for Mixed Linear Regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18237v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhankun Luo, Abolfazl Hashemi</dc:creator>
    </item>
  </channel>
</rss>
