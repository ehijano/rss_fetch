<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 16 Jul 2025 04:01:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Tax-Efficient Model Predictive Control Policy for Retirement Funding</title>
      <link>https://arxiv.org/abs/2507.10603</link>
      <description>arXiv:2507.10603v1 Announce Type: new 
Abstract: The retirement funding problem addresses the question of how to manage a retiree's savings to provide her with a constant post-tax inflation adjusted consumption throughout her lifetime. This consists of choosing withdrawals and transfers from and between several accounts with different tax treatments, taking into account basic rules such as required minimum distributions and limits on Roth conversions, additional income, liabilities, taxes, and the bequest when the retiree dies. We develop a retirement funding policy in two steps. In the first step, we consider a simplified planning problem in which various future quantities, such as the retiree's remaining lifetime, future investment returns, and future inflation, are known. Using a simplified model of taxes, we pose this planning problem as a convex optimization problem, where we maximize the bequest subject to providing a constant inflation adjusted consumption target. Since this problem is convex, it can be solved quickly and reliably. We leverage this planning method to form a retirement funding policy that determines the actions to take each year, based on information known at that time. Each year the retiree forms a new plan for the future years, using the current account values and life expectancy, and optionally, updated information such as changes in tax rates or rules. The retiree then carries out the actions from the first year of the current plan. This update-plan-act cycle is repeated each year, a general policy called model predictive control (MPC). The MPC retirement policy reacts to the effects of uncertain investment returns and inflation, changes in the retiree's expected lifetime or external income and liabilities, and changes in tax rules and rates. We demonstrate the effectiveness of the MPC retirement policy using Monte Carlo simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10603v1</guid>
      <category>math.OC</category>
      <category>cs.CE</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kasper Johansson, Stephen Boyd</dc:creator>
    </item>
    <item>
      <title>A Mean Field Game for Capacity Expansion Modeling</title>
      <link>https://arxiv.org/abs/2507.10604</link>
      <description>arXiv:2507.10604v1 Announce Type: new 
Abstract: This paper studies the optimal investment behavior of renewable electricity producers in a competitive market, where both prices and installation costs are influenced by aggregate industry activity. We model the resulting crowding effects using a mean field game framework, capturing the strategic interactions among a continuum of heterogeneous producers. The equilibrium dynamics are characterized via a coupled system of Hamilton-Jacobi-Bellman and Fokker-Planck equations, which describe the value function of a representative producer and the evolution of the distribution of installed capacities over time. We analyze both deterministic and stochastic versions of the model, providing analytical insights in tractable cases and developing numerical methods to approximate the general solution. Simulation results illustrate how aggregate investment responds to changing market conditions, cost structures, and exogenous productivity shocks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10604v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emma Hubert, Dimitrios Lolas, Ronnie Sircar</dc:creator>
    </item>
    <item>
      <title>Reconciling Risk-Aversion Paradoxes in the Distribution-Free Newsvendor Problem: Scarf's Rule Meets Dual Utility</title>
      <link>https://arxiv.org/abs/2507.10735</link>
      <description>arXiv:2507.10735v1 Announce Type: new 
Abstract: How should a risk-averse newsvendor order optimally under distributional ambiguity? Attempts to extend Scarf's celebrated distribution-free ordering rule using risk measures have led to conflicting prescriptions: CVaR-based models invariably recommend ordering less as risk aversion increases, while mean-standard deviation models -- paradoxically -- suggest ordering more, particularly when ordering costs are high. We resolve this behavioral paradox through a coherent generalization of Scarf's distribution-free framework, modeling risk aversion via distortion functionals from dual utility theory. Despite the generality of this class, we derive closed-form optimal ordering rules for any coherent risk preference. These rules uncover a consistent behavioral principle: a more risk-averse newsvendor may rationally order more when overstocking is inexpensive (i.e., when the cost-to-price ratio is low), but will always order less when ordering is costly. Our framework offers a more nuanced, managerially intuitive, and behaviorally coherent understanding of risk-averse inventory decisions. It exposes the limitations of non-coherent models, delivers interpretable and easy-to-compute ordering rules grounded in coherent preferences, and unifies prior work under a single, tractable approach. We further extend the results to multi-product settings with arbitrary demand dependencies, showing that optimal order quantities remain separable and can be obtained by solving single-product problems independently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10735v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan Yu-Meng Li, Tiantian Mao, Reza Valimoradi</dc:creator>
    </item>
    <item>
      <title>Non-smooth stochastic gradient descent using smoothing functions</title>
      <link>https://arxiv.org/abs/2507.10901</link>
      <description>arXiv:2507.10901v1 Announce Type: new 
Abstract: In this paper, we address stochastic optimization problems involving a composition of a non-smooth outer function and a smooth inner function, a formulation frequently encountered in machine learning and operations research. To deal with the non-differentiability of the outer function, we approximate the original non-smooth function using smoothing functions, which are continuously differentiable and approach the original function as a smoothing parameter goes to zero (at the price of increasingly higher Lipschitz constants). The proposed smoothing stochastic gradient method iteratively drives the smoothing parameter to zero at a designated rate. We establish convergence guarantees under strongly convex, convex, and nonconvex settings, proving convergence rates that match known results for non-smooth stochastic compositional optimization. In particular, for convex objectives, smoothing stochastic gradient achieves a 1/T^(1/4) rate in terms of the number of stochastic gradient evaluations. We further show how general compositional and finite-sum compositional problems (widely used frameworks in large-scale machine learning and risk-averse optimization) fit the assumptions needed for the rates (unbiased gradient estimates, bounded second moments, and accurate smoothing errors). We present preliminary numerical results indicating that smoothing stochastic gradient descent can be competitive for certain classes of problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10901v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tommaso Giovannelli, Jingfu Tan, Luis Nunes Vicente</dc:creator>
    </item>
    <item>
      <title>On the optimality conditions for a fractional diffusive equation with a nonlocal term</title>
      <link>https://arxiv.org/abs/2507.11058</link>
      <description>arXiv:2507.11058v1 Announce Type: new 
Abstract: We study a bilinear OCP for an evolution equation governed by the fractional Laplacian of order $0 &lt; s &lt; 1$, incorporating a nonlocal time component modeled by an integral kernel. After establishing well-posedness of the problem, we analyze the properties of the control-to-state operator. We prove the existence of at least one optimal control and derive both first-order and second-order optimality conditions, which ensure local uniqueness. Under further assumptions, we also demonstrate that global uniqueness of the optimal control can be achieved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11058v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jasarat Gasimov, Nazim Mahmudov</dc:creator>
    </item>
    <item>
      <title>Performance Enhancement of the Recursive Least Squares Algorithms with Rank Two Updates</title>
      <link>https://arxiv.org/abs/2507.11095</link>
      <description>arXiv:2507.11095v1 Announce Type: new 
Abstract: New recursive least squares algorithms with rank two updates (RLSR2) that include both exponential and instantaneous forgetting (implemented via a proper choice of the forgetting factor and the window size) are introduced and systematically associated in this report with well-known RLS algorithms with rank one updates. Moreover, new properties (which can be used for further performance improvement) of the recursive algorithms associated with the convergence of the inverse of information matrix and parameter vector are established in this report. The performance of new algorithms is examined in the problem of estimation of the grid events in the presence of significant harmonic emissions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11095v1</guid>
      <category>math.OC</category>
      <category>cs.IT</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.HO</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/00051144.2025.2517431</arxiv:DOI>
      <arxiv:journal_reference>Automatika, vol.66, issue 4 , 2025, pp. 619-624</arxiv:journal_reference>
      <dc:creator>Alexander Stotsky</dc:creator>
    </item>
    <item>
      <title>A Mathematical Optimization Approach to Multisphere Support Vector Data Description</title>
      <link>https://arxiv.org/abs/2507.11106</link>
      <description>arXiv:2507.11106v1 Announce Type: new 
Abstract: We present a novel mathematical optimization framework for outlier detection in multimodal datasets, extending Support Vector Data Description approaches. We provide a primal formulation, in the shape of a Mixed Integer Second Order Cone model, that constructs Euclidean hyperspheres to identify anomalous observations. Building on this, we develop a dual model that enables the application of the kernel trick, thus allowing for the detection of outliers within complex, non-linear data structures. An extensive computational study demonstrates the effectiveness of our exact method, showing clear advantages over existing heuristic techniques in terms of accuracy and robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11106v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>V\'ictor Blanco, Inmaculada Espejo, Ra\'ul P\'aez, Antonio M. Rodr\'iguez-Ch\'ia</dc:creator>
    </item>
    <item>
      <title>Second-Order Characterizations of Tilt Stability in Composite Optimization</title>
      <link>https://arxiv.org/abs/2507.11253</link>
      <description>arXiv:2507.11253v1 Announce Type: new 
Abstract: Tilt stability is a fundamental concept of variational analysis and optimization that plays a pivotal role in both theoretical issues and numerical computations. This paper investigates tilt stability of local minimizers for a general class of composite optimization problems in finite dimensions, where extended-real-valued objectives are compositions of parabolically regular and smooth functions. Under the weakest metric subregularity constraint qualification and other verifiable conditions, we establish unified neighborhood and pointbased characterizations of tilt stability via second-order generalized differentiation. The obtained results provide a rigorous theoretical foundation for further developments on variational stability and numerical algorithms of optimization and related topics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11253v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Boris S. Mordukhovich, Peipei Tang, Chengjing Wang</dc:creator>
    </item>
    <item>
      <title>Distributionally Robust Optimization is a Multi-Objective Problem</title>
      <link>https://arxiv.org/abs/2507.11350</link>
      <description>arXiv:2507.11350v1 Announce Type: new 
Abstract: Distributionally Robust Optimization (DRO) is a worst-case approach to decision making when there is model uncertainty. Though formulated as a single-objective problem, we show that it is intrinsically multi-objective in that DRO solutions map out a near-Pareto-optimal frontier between expected cost and a measure of robustness called worst-case sensitivity (WCS). We take this as the starting point and explore robust decision making through a multi-objective lens. We show that WCS is a measure of spread and derive WCS for a collection of uncertainty sets commonly used in DRO. These sensitivity measures identify the errors against which the nominal expected cost is most vulnerable and the uncertainty set for the worst-case problem that most effectively mitigates it. The associated mean-sensitivity frontier is used to select its size. The multi-objective perspective provides a quantitative measure of robustness and a sensitivity-based approach to addressing important conceptual gaps in DRO -- how to choose the family and size of uncertainty sets for a given cost distribution, and how this affects the solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11350v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jun-ya Gotoh, Michael Jong Kim, Andrew E. B. Lim</dc:creator>
    </item>
    <item>
      <title>Deep Equilibrium models for Poisson Imaging Inverse problems via Mirror Descent</title>
      <link>https://arxiv.org/abs/2507.11461</link>
      <description>arXiv:2507.11461v1 Announce Type: new 
Abstract: Deep Equilibrium Models (DEQs) are implicit neural networks with fixed points, which have recently gained attention for learning image regularization functionals, particularly in settings involving Gaussian fidelities, where assumptions on the forward operator ensure contractiveness of standard (proximal) Gradient Descent operators. In this work, we extend the application of DEQs to Poisson inverse problems, where the data fidelity term is more appropriately modeled by the Kullback-Leibler divergence. To this end, we introduce a novel DEQ formulation based on Mirror Descent defined in terms of a tailored non-Euclidean geometry that naturally adapts with the structure of the data term. This enables the learning of neural regularizers within a principled training framework. We derive sufficient conditions to guarantee the convergence of the learned reconstruction scheme and propose computational strategies that enable both efficient training and fully parameter-free inference. Numerical experiments show that our method outperforms traditional model-based approaches and it is comparable to the performance of Bregman Plug-and-Play methods, while mitigating their typical drawbacks - namely, sensitivity to initialization and careful tuning of hyperparameters. The code is publicly available at https://github.com/christiandaniele/DEQ-MD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11461v1</guid>
      <category>math.OC</category>
      <category>cs.CV</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Daniele, Silvia Villa, Samuel Vaiter, Luca Calatroni</dc:creator>
    </item>
    <item>
      <title>Solving Integrated Periodic Railway Timetabling with Satisfiability Modulo Theories: A Scalable Approach to Routing and Vehicle Circulation</title>
      <link>https://arxiv.org/abs/2507.11489</link>
      <description>arXiv:2507.11489v1 Announce Type: new 
Abstract: This paper introduces a novel approach for jointly solving the periodic Train Timetabling Problem (TTP), train routing, and Vehicle Circulation Problem (VCP) through a unified optimization model. While these planning stages are traditionally addressed sequentially, their interdependencies often lead to suboptimal vehicle usage. We propose the VCR-PESP, an integrated formulation that minimizes fleet size while ensuring feasible and infrastructure-compliant periodic timetables. We present the first Satisfiability Modulo Theories (SMT)-based method for the VCR-PESP to solve the resulting large-scale instances. Unlike the Boolean Satisfiability Problem (SAT), which requires time discretisation, SMT supports continuous time via difference constraints, eliminating the trade-off between temporal precision and encoding size. Our approach avoids rounding artifacts and scales effectively, outperforming both SAT and Mixed Integer Program (MIP) models across non-trivial instances. Using real-world data from the Swiss narrow-gauge operator RhB, we conduct extensive experiments to assess the impact of time discretisation, vehicle circulation strategies, route flexibility, and planning integration. We show that discrete models inflate vehicle requirements and that fully integrated solutions substantially reduce fleet needs compared to sequential approaches. Our framework consistently delivers high-resolution solutions with tractable runtimes, even in large and complex networks. By combining modeling accuracy with scalable solver technology, this work establishes SMT as a powerful tool for integrated railway planning. It demonstrates how relaxing discretisation and solving across planning layers enables more efficient and implementable timetables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11489v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Florian Fuchs, Bernardo Martin-Iradi, Francesco Corman</dc:creator>
    </item>
    <item>
      <title>Recursive Bound-Constrained AdaGrad with Applications to Multilevel and Domain Decomposition Minimization</title>
      <link>https://arxiv.org/abs/2507.11513</link>
      <description>arXiv:2507.11513v1 Announce Type: new 
Abstract: Two OFFO (Objective-Function Free Optimization) noise tolerant algorithms are presented that handle bound constraints, inexact gradients and use second-order information when available.The first is a multi-level method exploiting a hierarchical description of the problem and the second is a domain-decomposition method covering the standard addditive Schwarz decompositions. Both are generalizations of the first-order AdaGrad algorithm for unconstrained optimization. Because these algorithms share a common theoretical framework, a single convergence/complexity theory is provided which covers them both. Its main result is that, with high probability, both methods need at most $O(\epsilon^{-2})$ iterations and noisy gradient evaluations to compute an $\epsilon$-approximate first-order critical point of the bound-constrained problem. Extensive numerical experiments are discussed on applications ranging from PDE-based problems to deep neural network training, illustrating their remarkable computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11513v1</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Serge Gratton, Alena Kopani\v{c}\'akov\'a, Philippe Toint</dc:creator>
    </item>
    <item>
      <title>Distributionally Robust Optimization with Adversarial Data Contamination</title>
      <link>https://arxiv.org/abs/2507.10718</link>
      <description>arXiv:2507.10718v1 Announce Type: cross 
Abstract: Distributionally Robust Optimization (DRO) provides a framework for decision-making under distributional uncertainty, yet its effectiveness can be compromised by outliers in the training data. This paper introduces a principled approach to simultaneously address both challenges. We focus on optimizing Wasserstein-1 DRO objectives for generalized linear models with convex Lipschitz loss functions, where an $\epsilon$-fraction of the training data is adversarially corrupted. Our primary contribution lies in a novel modeling framework that integrates robustness against training data contamination with robustness against distributional shifts, alongside an efficient algorithm inspired by robust statistics to solve the resulting optimization problem. We prove that our method achieves an estimation error of $O(\sqrt{\epsilon})$ for the true DRO objective value using only the contaminated data under the bounded covariance assumption. This work establishes the first rigorous guarantees, supported by efficient computation, for learning under the dual challenges of data contamination and distributional shifts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10718v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuyao Li, Ilias Diakonikolas, Jelena Diakonikolas</dc:creator>
    </item>
    <item>
      <title>Multi-Armed Sampling Problem and the End of Exploration</title>
      <link>https://arxiv.org/abs/2507.10797</link>
      <description>arXiv:2507.10797v1 Announce Type: cross 
Abstract: This paper introduces the framework of multi-armed sampling, as the sampling counterpart to the optimization problem of multi-arm bandits. Our primary motivation is to rigorously examine the exploration-exploitation trade-off in the context of sampling. We systematically define plausible notions of regret for this framework and establish corresponding lower bounds. We then propose a simple algorithm that achieves these optimal regret bounds. Our theoretical results demonstrate that in contrast to optimization, sampling does not require exploration. To further connect our findings with those of multi-armed bandits, we define a continuous family of problems and associated regret measures that smoothly interpolates and unifies multi-armed sampling and multi-armed bandit problems using a temperature parameter. We believe the multi-armed sampling framework, and our findings in this setting can have a foundational role in the study of sampling including recent neural samplers, akin to the role of multi-armed bandits in reinforcement learning. In particular, our work sheds light on the need for exploration and the convergence properties of algorithm for entropy-regularized reinforcement learning, fine-tuning of pretrained models and reinforcement learning with human feedback (RLHF).</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10797v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Pedramfar, Siamak Ravanbakhsh</dc:creator>
    </item>
    <item>
      <title>Functional Neural Wavefunction Optimization</title>
      <link>https://arxiv.org/abs/2507.10835</link>
      <description>arXiv:2507.10835v1 Announce Type: cross 
Abstract: We propose a framework for the design and analysis of optimization algorithms in variational quantum Monte Carlo, drawing on geometric insights into the corresponding function space. The framework translates infinite-dimensional optimization dynamics into tractable parameter-space algorithms through a Galerkin projection onto the tangent space of the variational ansatz. This perspective unifies existing methods such as stochastic reconfiguration and Rayleigh-Gauss-Newton, provides connections to classic function-space algorithms, and motivates the derivation of novel algorithms with geometrically principled hyperparameter choices. We validate our framework with numerical experiments demonstrating its practical relevance through the accurate estimation of ground-state energies for several prototypical models in condensed matter physics modeled with neural network wavefunctions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10835v1</guid>
      <category>cond-mat.str-el</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>physics.comp-ph</category>
      <category>quant-ph</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Armegioiu, Juan Carrasquilla, Siddhartha Mishra, Johannes M\"uller, Jannes Nys, Marius Zeinhofer, Hang Zhang</dc:creator>
    </item>
    <item>
      <title>Waterfilling at the Edge: Optimal Percentile Resource Allocation via Risk-Averse Reduction</title>
      <link>https://arxiv.org/abs/2507.10838</link>
      <description>arXiv:2507.10838v1 Announce Type: cross 
Abstract: We address deterministic resource allocation in point-to-point multi-terminal AWGN channels without inter-terminal interference, with particular focus on optimizing quantile transmission rates for cell-edge terminal service. Classical utility-based approaches -- such as minimum rate, sumrate, and proportional fairness -- are either overconservative, or inappropriate, or do not provide a rigorous and/or interpretable foundation for fair rate optimization at the edge. To overcome these challenges, we employ Conditional Value-at-Risk (CVaR), a popular coherent risk measure, and establish its equivalence with the sum-least-$\alpha$th-quantile (SL$\alpha$Q) utility. This connection enables an exact convex reformulation of the SL$\alpha$Q maximization problem, facilitating analytical tractability and precise and interpretable control over cell-edge terminal performance. Utilizing Lagrangian duality, we provide (for the first time) parameterized closed-form solutions for the optimal resource policy -- which is of waterfilling-type -- as well as the associated (auxiliary) Value-at-Risk variable. We further develop a novel inexact dual subgradient descent algorithm of minimal complexity to determine globally optimal resource policies, and we rigorously establish its convergence. The resulting edge waterfilling algorithm iteratively and efficiently allocates resources while explicitly ensuring transmission rate fairness across (cell-edge) terminals. Several (even large-scale) numerical experiments validate the effectiveness of the proposed method for enabling robust quantile rate optimization at the edge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10838v1</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gokberk Yaylali, Ahmad Ali Khan, Dionysios S. Kalogerias</dc:creator>
    </item>
    <item>
      <title>A Noise-Aware Scalable Subspace Classical Optimizer for the Quantum Approximate Optimization Algorithm</title>
      <link>https://arxiv.org/abs/2507.10992</link>
      <description>arXiv:2507.10992v1 Announce Type: cross 
Abstract: We introduce ANASTAARS, a noise-aware scalable classical optimizer for variational quantum algorithms such as the quantum approximate optimization algorithm (QAOA). ANASTAARS leverages adaptive random subspace strategies to efficiently optimize the ansatz parameters of a QAOA circuit, in an effort to address challenges posed by a potentially large number of QAOA layers. ANASTAARS iteratively constructs random interpolation models within low-dimensional affine subspaces defined via Johnson--Lindenstrauss transforms. This adaptive strategy allows the selective reuse of previously acquired measurements, significantly reducing computational costs associated with shot acquisition. Furthermore, to robustly handle noisy measurements, ANASTAARS incorporates noise-aware optimization techniques by estimating noise magnitude and adjusts trust-region steps accordingly. Numerical experiments demonstrate the practical scalability of the proposed method for near-term quantum computing applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10992v1</guid>
      <category>quant-ph</category>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kwassi Joseph Dzahini, Jeffrey M. Larson, Matt Menickelly, Stefan M. Wild</dc:creator>
    </item>
    <item>
      <title>Adaptive Reduced Basis Trust Region Methods for Parabolic Inverse Problems</title>
      <link>https://arxiv.org/abs/2507.11130</link>
      <description>arXiv:2507.11130v1 Announce Type: cross 
Abstract: We consider nonlinear inverse problems arising in the context of parameter identification for parabolic partial differential equations (PDEs). For stable reconstructions, regularization methods such as the iteratively regularized Gauss-Newton method (IRGNM) are commonly used, but their application is computationally demanding due to the high-dimensional nature of PDE discretizations. To address this bottleneck, we propose a reduced-order modeling approach that accelerates both the state and adjoint evaluations required for derivative-based optimization. Our method builds on the recent contribution [Kartmann et al. Adaptive reduced basis trust region methods for parameter identification problems. Comput. Sci. Eng. 1, 3 (2024)] for elliptic forward operators and constructs the reduced forward operator adaptively in an online fashion, combining both parameter and state space reduction. To ensure reliability, we embed the IRGNM iteration within an adaptive, error-aware trust-region framework that certifies the accuracy of the reduced-order approximations. We demonstrate the effectiveness of the proposed approach through numerical results for both time-dependent and time-independent parameter identification problems in dynamic reaction-diffusion systems. The implementation is made available for reproducibility and further use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11130v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Kartmann, Benedikt Klein, Mario Ohlberger, Thomas Schuster, Stefan Volkwein</dc:creator>
    </item>
    <item>
      <title>Gradient Descent on Logistic Regression: Do Large Step-Sizes Work with Data on the Sphere?</title>
      <link>https://arxiv.org/abs/2507.11228</link>
      <description>arXiv:2507.11228v1 Announce Type: cross 
Abstract: Gradient descent (GD) on logistic regression has many fascinating properties. When the dataset is linearly separable, it is known that the iterates converge in direction to the maximum-margin separator regardless of how large the step size is. In the non-separable case, however, it has been shown that GD can exhibit a cycling behaviour even when the step sizes is still below the stability threshold $2/\lambda$, where $\lambda$ is the largest eigenvalue of the Hessian at the solution. This short paper explores whether restricting the data to have equal magnitude is a sufficient condition for global convergence, under any step size below the stability threshold. We prove that this is true in a one dimensional space, but in higher dimensions cycling behaviour can still occur. We hope to inspire further studies on quantifying how common these cycles are in realistic datasets, as well as finding sufficient conditions to guarantee global convergence with large step sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11228v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Si Yi Meng, Baptiste Goujaud, Antonio Orvieto, Christopher De Sa</dc:creator>
    </item>
    <item>
      <title>LyAm: Robust Non-Convex Optimization for Stable Learning in Noisy Environments</title>
      <link>https://arxiv.org/abs/2507.11262</link>
      <description>arXiv:2507.11262v1 Announce Type: cross 
Abstract: Training deep neural networks, particularly in computer vision tasks, often suffers from noisy gradients and unstable convergence, which hinder performance and generalization. In this paper, we propose LyAm, a novel optimizer that integrates Adam's adaptive moment estimation with Lyapunov-based stability mechanisms. LyAm dynamically adjusts the learning rate using Lyapunov stability theory to enhance convergence robustness and mitigate training noise. We provide a rigorous theoretical framework proving the convergence guarantees of LyAm in complex, non-convex settings. Extensive experiments on like as CIFAR-10 and CIFAR-100 show that LyAm consistently outperforms state-of-the-art optimizers in terms of accuracy, convergence speed, and stability, establishing it as a strong candidate for robust deep learning optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11262v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elmira Mirzabeigi, Sepehr Rezaee, Kourosh Parand</dc:creator>
    </item>
    <item>
      <title>Fast Last-Iterate Convergence of SGD in the Smooth Interpolation Regime</title>
      <link>https://arxiv.org/abs/2507.11274</link>
      <description>arXiv:2507.11274v1 Announce Type: cross 
Abstract: We study population convergence guarantees of stochastic gradient descent (SGD) for smooth convex objectives in the interpolation regime, where the noise at optimum is zero or near zero. The behavior of the last iterate of SGD in this setting -- particularly with large (constant) stepsizes -- has received growing attention in recent years due to implications for the training of over-parameterized models, as well as to analyzing forgetting in continual learning and to understanding the convergence of the randomized Kaczmarz method for solving linear systems. We establish that after $T$ steps of SGD on $\beta$-smooth convex loss functions with stepsize $\eta \leq 1/\beta$, the last iterate exhibits expected excess risk $\widetilde{O}(1/(\eta T^{1-\beta\eta/2}) + \eta T^{\beta\eta/2} \sigma_\star^2)$, where $\sigma_\star^2$ denotes the variance of the stochastic gradients at the optimum. In particular, for a well-tuned stepsize we obtain a near optimal $\widetilde{O}(1/T + \sigma_\star/\sqrt{T})$ rate for the last iterate, extending the results of Varre et al. (2021) beyond least squares regression; and when $\sigma_\star=0$ we obtain a rate of $O(1/\sqrt{T})$ with $\eta=1/\beta$, improving upon the best-known $O(T^{-1/4})$ rate recently established by Evron et al. (2025) in the special case of realizable linear regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11274v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amit Attia, Matan Schliserman, Uri Sherman, Tomer Koren</dc:creator>
    </item>
    <item>
      <title>Markov approximation for controlled Hawkes Jump-Diffusions with general kernels</title>
      <link>https://arxiv.org/abs/2507.11294</link>
      <description>arXiv:2507.11294v1 Announce Type: cross 
Abstract: We present a Markov approximation for jump-diffusions whose jump part consists in a Hawkes process with intensity driven by a general (possibly non-monotone) kernel. Under minimal integrability conditions, the kernel can be approximated by a linear combination of exponential functions. This implies that Hawkes jump-diffusions can be approximated with Markov jump-diffusions. We illustrate the usefulness of this approximation by applying it to a class of stochastic control problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11294v1</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahmoud Khabou, Mehdi Talbi</dc:creator>
    </item>
    <item>
      <title>Joint Power Allocation and Reflecting-Element Activation for Energy Efficiency Maximization in IRS-Aided Communications Under CSI Uncertainty</title>
      <link>https://arxiv.org/abs/2507.11413</link>
      <description>arXiv:2507.11413v1 Announce Type: cross 
Abstract: We study the joint power allocation and reflecting element (RE) activation to maximize the energy efficiency (EE) in communication systems assisted by an intelligent reflecting surface (IRS), taking into account imperfections in channel state information (CSI). The robust optimization problem is mixed integer, i.e., the optimization variables are continuous (transmit power) and discrete (binary states of REs). In order to solve this challenging problem we develop two algorithms. The first one is an alternating optimization (AO) method that attains a suboptimal solution with low complexity, based on the Lambert W function and a dynamic programming (DP) algorithm. The second one is a branch-and-bound (B&amp;B) method that uses AO as its subroutine and is formally guaranteed to achieve a globally optimal solution. Both algorithms do not require any external optimization solver for their implementation. Furthermore, numerical results show that the proposed algorithms outperform the baseline schemes, AO achieves near-optimal performance in most cases, and B&amp;B has low computational complexity on average.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11413v1</guid>
      <category>eess.SP</category>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christos N. Efrem, Ioannis Krikidis</dc:creator>
    </item>
    <item>
      <title>Gain and phase type multipliers for feedback robustness</title>
      <link>https://arxiv.org/abs/2203.11837</link>
      <description>arXiv:2203.11837v2 Announce Type: replace 
Abstract: It is known that the stability of a feedback interconnection of two linear time-invariant systems implies that the graphs of the open-loop systems are quadratically separated. This separation is defined by an object known as the multiplier. The theory of integral quadratic constraints shows that the converse also holds under certain conditions. This paper establishes that if the feedback is robustly stable against certain structured uncertainty, then there always exists a multiplier that takes a corresponding form. In particular, if the feedback is robustly stable to certain gain-type uncertainty, then there exists a corresponding multiplier that is of phase-type, i.e., its diagonal blocks are zeros. These results build on the notion of phases of matrices and systems, which was recently introduced in the field of control. Similarly, if the feedback is robustly stable to certain phase-type uncertainty, then there exists a gain-type multiplier, i.e., its off-diagonal blocks are zeros. The results are meaningfully instructive in the search for a valid multiplier for establishing robust closed-loop stability, and cover the well-known small-gain and the recent small-phase theorems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.11837v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Axel Ringh, Xin Mao, Wei Chen, Li Qiu, Sei Zhen Khong</dc:creator>
    </item>
    <item>
      <title>Hautus--Yamamoto criteria for approximate and exact controllability of linear difference delay equations</title>
      <link>https://arxiv.org/abs/2210.13590</link>
      <description>arXiv:2210.13590v3 Announce Type: replace 
Abstract: The paper deals with the controllability of finite-dimensional linear difference delay equations, i.e., dynamics for which the state at a given time $t$ is obtained as a linear combination of the control evaluated at time $t$ and of the state evaluated at finitely many previous instants of time $t-\Lambda_1,\dots,t-\Lambda_N$. Based on the realization theory developed by Y.Yamamoto for general infinite-dimensional dynamical systems, we obtain necessary and sufficient conditions, expressed in the frequency domain, for the approximate controllability in finite time in $L^q$ spaces, $q \in [1, +\infty)$. We also provide a necessary condition for $L^1$ exact controllability, which can be seen as the closure of the $L^1$ approximate controllability criterion. Furthermore, we provide an explicit upper bound on the minimal times of approximate and exact controllability, given by $d\max\{\Lambda_1,\dots,\Lambda_N\}$, where $d$ is the dimension of the state space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.13590v3</guid>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.3934/dcds.2023049</arxiv:DOI>
      <arxiv:journal_reference>Discrete Contin. Dyn. Syst., 49(3):3306--3337, 2023</arxiv:journal_reference>
      <dc:creator>Yacine Chitour, S\'ebastien Fueyo, Guilherme Mazanti, Mario Sigalotti</dc:creator>
    </item>
    <item>
      <title>Projecting onto a Capped Rotated Second-Order Cone</title>
      <link>https://arxiv.org/abs/2307.07290</link>
      <description>arXiv:2307.07290v3 Announce Type: replace 
Abstract: We derive a closed-form expression for the projection onto a capped rotated second-order cone -- a convex set that arises in perspective relaxations of nonlinear programs with binary indicator variables. The closed-form solution involves three distinct cases, one of which reduces to the classical projection onto a second-order cone. The remaining two cases yield nontrivial projections, for which we provide necessary and sufficient conditions under which the solution lies on the intersection of the cone and a facet of a box.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.07290v3</guid>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noam Goldberg, Ishy Zagdoun</dc:creator>
    </item>
    <item>
      <title>A Practical and Optimal First-Order Method for Large-Scale Convex Quadratic Programming</title>
      <link>https://arxiv.org/abs/2311.07710</link>
      <description>arXiv:2311.07710v4 Announce Type: replace 
Abstract: Convex quadratic programming (QP) is an important class of optimization problem with wide applications in practice. The classic QP solvers are based on either simplex or barrier method, both of which suffer from the scalability issue because their computational bottleneck is solving linear equations. In this paper, we design and analyze a first-order method for QP, called restarted accelerated primal-dual hybrid gradient (rAPDHG), whose computational bottleneck is matrix-vector multiplication. We show that rAPDHG has a linear convergence rate to an optimal solution when solving QP, and the obtained linear rate is optimal among a wide class of primal-dual methods. Furthermore, we connect the linear rate with a sharpness constant of the KKT system of QP, which is a standard quantity to measure the hardness of a continuous optimization problem. Numerical experiments demonstrate that both restarts and acceleration can significantly improve the performance of the algorithm. Lastly, we present PDQP.jl, an open-source solver based on rAPDHG that can be run on both GPU and CPU. With a numerical comparison with SCS and OSQP on standard QP benchmark sets and large-scale synthetic QP instances, we demonstrate the effectiveness of rAPDHG for solving QP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.07710v4</guid>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haihao Lu, Jinwen Yang</dc:creator>
    </item>
    <item>
      <title>Non-asymptotic Global Convergence Rates of BFGS with Exact Line Search</title>
      <link>https://arxiv.org/abs/2404.01267</link>
      <description>arXiv:2404.01267v2 Announce Type: replace 
Abstract: In this paper, we explore the non-asymptotic global convergence rates of the Broyden-Fletcher-Goldfarb-Shanno (BFGS) method implemented with exact line search. Notably, due to Dixon's equivalence result, our findings are also applicable to other quasi-Newton methods in the convex Broyden class employing exact line search, such as the Davidon-Fletcher-Powell (DFP) method. Specifically, we focus on problems where the objective function is strongly convex with Lipschitz continuous gradient and Hessian. Our results hold for any initial point and any symmetric positive definite initial Hessian approximation matrix. The analysis unveils a detailed three-phase convergence process, characterized by distinct linear and superlinear rates, contingent on the iteration progress. Additionally, our theoretical findings demonstrate the trade-offs between linear and superlinear convergence rates for BFGS when we modify the initial Hessian approximation matrix, a phenomenon further corroborated by our numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01267v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiujiang Jin, Ruichen Jiang, Aryan Mokhtari</dc:creator>
    </item>
    <item>
      <title>Control of high-dimensional collective dynamics by deep neural feedback laws and kinetic modelling</title>
      <link>https://arxiv.org/abs/2404.02825</link>
      <description>arXiv:2404.02825v2 Announce Type: replace 
Abstract: Modeling and control of agent-based models is twice cursed by the dimensionality of the problem, as both the number of agents and their state space dimension can be large. Even though the computational barrier posed by a large ensemble of agents can be overcome through a mean field formulation of the control problem, the feasibility of its solution is generally guaranteed only for agents operating in low-dimensional spaces. To circumvent the difficulty posed by the high dimensionality of the state space a kinetic model is proposed, requiring the sampling of high-dimensional, two-agent sub-problems, to evolve the agents' density using a Boltzmann type equation. Such density evolution requires a high-frequency sampling of two-agent optimal control problems, which is efficiently approximated by means of deep neural networks and supervised learning, enabling the fast simulation of high-dimensional, large-scale ensembles of controlled particles. Numerical experiments demonstrate the effectiveness of the proposed approach in the control of consensus and attraction-repulsion dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02825v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giacomo Albi, Sara Bicego, Dante Kalise</dc:creator>
    </item>
    <item>
      <title>Stochastic Control with Signatures</title>
      <link>https://arxiv.org/abs/2406.01585</link>
      <description>arXiv:2406.01585v2 Announce Type: replace 
Abstract: This paper proposes to parameterize open loop controls in stochastic optimal control problems via suitable classes of functionals depending on the driver's path signature, a concept adopted from rough path integration theory. We rigorously prove that these controls are dense in the class of progressively measurable controls and use rough path methods to establish suitable conditions for stability of the controlled dynamics and target functional. These results pave the way for Monte Carlo methods to stochastic optimal control for generic target functionals and dynamics. We discuss the rather versatile numerical algorithms for computing approximately optimal controls and verify their accurateness in benchmark problems from Mathematical Finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01585v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>P. Bank, C. Bayer, P. P. Hager, S. Riedel, T. Nauen</dc:creator>
    </item>
    <item>
      <title>Subsidizing a New Technology: An Impulse Stackelberg Game Approach</title>
      <link>https://arxiv.org/abs/2407.05393</link>
      <description>arXiv:2407.05393v2 Announce Type: replace 
Abstract: Governments are motivated to subsidize profit-driven firms that manufacture zero-emission vehicles to ensure they become price-competitive. This paper introduces a dynamic Stackelberg game to determine the government's optimal subsidy strategy for zero-emission vehicles, taking into account the pricing decisions of a profit-maximizing firm. While firms have the flexibility to change prices continuously, subsidies are adjusted at specific time intervals. This is captured in our game formulation by using impulse controls for discrete-time interventions. We provide a verification theorem to characterize the Feedback Stackelberg equilibrium and illustrate our results with numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05393v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Utsav Sadana, Georges Zaccour</dc:creator>
    </item>
    <item>
      <title>Optimal control under unknown intensity with Bayesian learning</title>
      <link>https://arxiv.org/abs/2411.04917</link>
      <description>arXiv:2411.04917v2 Announce Type: replace 
Abstract: We consider an optimal control problem inspired by neuroscience, where the dynamics is driven by a Poisson process with a controlled stochastic intensity and an uncertain parameter. Given a prior distribution for the unknown parameter, we describe its evolution according to Bayes' rule. We reformulate the optimization problem using Girsanov's theorem and establish a dynamic programming principle. Finally, we characterize the value function as the unique viscosity solution to a finite-dimensional Hamilton-Jacobi-Bellman equation, which can be solved numerically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04917v2</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Baradel, Quentin Cormier</dc:creator>
    </item>
    <item>
      <title>Inertial primal-dual dynamics with Hessian-driven damping and Tikhonov regularization for convex-concave bilinear saddle point problems</title>
      <link>https://arxiv.org/abs/2412.05931</link>
      <description>arXiv:2412.05931v2 Announce Type: replace 
Abstract: This paper deals with a second-order primal-dual dynamical system with Hessian-driven damping and Tikhonov regularization terms in connection with a convex-concave bilinear saddle point problem. We first obtain a fast convergence rate of the primal-dual gap along the trajectory generated by the dynamical system, and provide some integral estimates. Then, based on the setting of the parameters involved, we demonstrate that both the convergence rate of the primal-dual gap and the strong convergence of the trajectory can be achieved simultaneously. Furthermore, we evaluate the performance of the proposed system using two numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05931v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiangkai Sun, Liang He, Xianjun Long</dc:creator>
    </item>
    <item>
      <title>Fully Adaptive Zeroth-Order Method for Minimizing Functions with Compressible Gradients</title>
      <link>https://arxiv.org/abs/2501.11616</link>
      <description>arXiv:2501.11616v3 Announce Type: replace 
Abstract: We propose an adaptive zeroth-order method for minimizing differentiable functions with $L$-Lipschitz continuous gradients. The method is designed to take advantage of the eventual compressibility of the gradient of the objective function, but it does not require knowledge of the approximate sparsity level $s$ or the Lipschitz constant $L$ of the gradient. We show that the new method performs no more than $O\left(n^{2}\epsilon^{-2}\right)$ function evaluations to find an $\epsilon$-approximate stationary point of an objective function with $n$ variables. Assuming additionally that the gradients of the objective function are compressible, we obtain an improved complexity bound of $O\left(s\log\left(n\right)\epsilon^{-2}\right)$ function evaluations, which holds with high probability. Preliminary numerical results illustrate the efficiency of the proposed method and demonstrate that it can significantly outperform its non-adaptive counterpart.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11616v3</guid>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geovani Nunes Grapiglia, Daniel McKenzie</dc:creator>
    </item>
    <item>
      <title>A Zeroth-Order Extra-Gradient Method for Black-Box Constrained Optimization</title>
      <link>https://arxiv.org/abs/2506.20546</link>
      <description>arXiv:2506.20546v2 Announce Type: replace 
Abstract: Non-analytical objectives and constraints often arise in control systems, particularly in problems with complex dynamics, which are challenging yet lack efficient solution methods. In this work, we consider general constrained optimization problems involving black-box objectives and constraints. To solve it, we reformulate it as a min-max problem and propose a zeroth-order extra gradient (ZOEG) algorithm that combines the extra gradient method with a feedback-based stochastic zeroth-order gradient estimator. Then, we apply another coordinate gradient estimator to design the zeroth-order coordinate extra gradient algorithm (ZOCEG) to further improve efficiency. The theoretical analysis shows that ZOEG can achieve the best-known oracle complexity of $\mathcal{O}(d\epsilon^{-2})$ to get an $\epsilon$-optimal solution ($d$ is the dimension of decision space), and ZOCEG can improve it to $\mathcal{O}(d\epsilon^{-1})$. Furthermore, we develop a variant of ZOCEG, which applies block coordinate updates to enhance the efficiency of single-step gradient estimation. Finally, numerical experiments on a load tracking problem validate our theoretical results and the effectiveness of the proposed algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20546v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuke Zhou, Ruiyang Jin, Siyang Gao, Jianxiao Wang, Jie Song</dc:creator>
    </item>
    <item>
      <title>An experimental approach: Converting verbal expressions to numerical scales</title>
      <link>https://arxiv.org/abs/2507.04539</link>
      <description>arXiv:2507.04539v2 Announce Type: replace 
Abstract: One of the key issues in decision problems is the selection and use of the appropriate response scale. In this paper verbal expressions are converted into numerical scales for a subjective problem instance. The main motivation for our research was that verbal values in decision tasks are often mechanically converted into numbers, which thus typically do not fully represent the respondent's true evaluation. In our experiment, we conducted a color selection test with 462 subjects by testing six colors (red, green, blue, magenta, turquoise, yellow) defined from the Color Namer database on color-calibrated tablets in ISO standardized sensory test booths of a sensory laboratory. The colors were evaluated both in a pairwise comparison matrix (indirect ranking with four-item verbal category scale) and on a direct scoring basis. We determined scales that provide the closest results on average and individually to the direct scoring, based on the eigenvector and the logarithmic least squares methods. All results show that the difference between verbal expressions is much smaller than the one used by most of the common numerical scales. The respondents' inconsistency was also analyzed, even with a repeated question regarding their preference between a given pair of colors. It is shown that most decision makers answer fairly similarly for the second time, but there can be significant (even ordinal) differences. The respondents whose answers are further from the original tend to be more inconsistent in general.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04539v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zsombor Sz\'adoczki, S\'andor Boz\'oki, L\'aszl\'o Sipos, Zs\'ofia Galambosi</dc:creator>
    </item>
    <item>
      <title>Parallel Batch Scheduling With Incompatible Job Families Via Constraint Programming</title>
      <link>https://arxiv.org/abs/2410.11981</link>
      <description>arXiv:2410.11981v4 Announce Type: replace-cross 
Abstract: This paper addresses the incompatible case of parallel batch scheduling, where compatible jobs belong to the same family, and jobs from different families cannot be processed together in the same batch. The state-of-the-art constraint programming (CP) model for this problem relies on specific functions and global constraints only available in a well established commercial CP solver. This paper expands the literature around this problem by proposing four new CP models that can be implemented in commercial and open-source solvers: a new model that relies on automaton constraints, and three alternative models that integrate assignment and scheduling decisions with different strategies and global constraints. Extensive computational experiments on standard test cases under multiple objectives and multiple solvers demonstrate the implementation flexibility and competitive performance of the proposed models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11981v4</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jorge A. Huertas, Pascal Van Hentenryck</dc:creator>
    </item>
    <item>
      <title>A finite difference method with symmetry properties for the high-dimensional Bratu equation</title>
      <link>https://arxiv.org/abs/2410.12553</link>
      <description>arXiv:2410.12553v3 Announce Type: replace-cross 
Abstract: Solving the three-dimensional (3D) Bratu equation is highly challenging due to the presence of multiple and sharp solutions. Research on this equation began in the late 1990s, but there are no satisfactory results to date. To address this issue, we introduce a symmetric finite difference method (SFDM) which embeds the symmetry properties of the solutions into a finite difference method (FDM). This SFDM is primarily used to obtain more accurate solutions and bifurcation diagrams for the 3D Bratu equation. Additionally, we propose modifying the Bratu equation by incorporating a new constraint that facilitates the construction of bifurcation diagrams and simplifies handling the turning points. The proposed method, combined with the use of sparse matrix representation, successfully solves the 3D Bratu equation on grids of up to $301^3$ points. The results demonstrate that SFDM outperforms all previously employed methods for the 3D Bratu equation. Furthermore, we provide bifurcation diagrams for the 1D, 2D, 4D, and 5D cases, and accurately identify the first turning points in all dimensions. All simulations indicate that the bifurcation diagrams of the Bratu equation on the cube domains closely resemble the well-established behavior on the ball domains described by Joseph and Lundgren [1]. Furthermore, when SFDM is applied to linear stability analysis, it yields the same largest real eigenvalue as the standard FDM despite having fewer equations and variables in the nonlinear system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12553v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Luthfi Shahab, Hadi Susanto, Haralampos Hatzikirou</dc:creator>
    </item>
    <item>
      <title>Parallel subspace correction methods for semicoercive and nearly semicoercive convex optimization with applications to nonlinear PDEs</title>
      <link>https://arxiv.org/abs/2412.17318</link>
      <description>arXiv:2412.17318v3 Announce Type: replace-cross 
Abstract: We present new convergence analyses for parallel subspace correction methods for unconstrained semicoercive and nearly semicoercive convex optimization problems, generalizing the theory of singular and nearly singular linear problems to a class of nonlinear problems. Our results demonstrate that the elegant theoretical framework developed for singular and nearly singular linear problems can be extended to unconstrained semicoercive and nearly semicoercive convex optimization problems. For semicoercive problems, we show that the convergence rate can be estimated in terms of a seminorm stable decomposition over the subspaces and the kernel of the problem, aligning with the theory for singular linear problems. For nearly semicoercive problems, we establish a parameter-independent convergence rate, assuming the kernel of the semicoercive part can be decomposed into a sum of local kernels, which aligns with the theory for nearly singular problems. To demonstrate the applicability of our results, we provide convergence analyses of two-level additive Schwarz methods for solving certain nonlinear partial differential equations with Neumann boundary conditions, within the proposed abstract framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17318v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Young-Ju Lee, Jongho Park</dc:creator>
    </item>
    <item>
      <title>Constrained Online Convex Optimization with Polyak Feasibility Steps</title>
      <link>https://arxiv.org/abs/2502.13112</link>
      <description>arXiv:2502.13112v2 Announce Type: replace-cross 
Abstract: In this work, we study online convex optimization with a fixed constraint function $g : \mathbb{R}^d \rightarrow \mathbb{R}$. Prior work on this problem has shown $O(\sqrt{T})$ regret and cumulative constraint satisfaction $\sum_{t=1}^{T} g(x_t) \leq 0$, while only accessing the constraint value and subgradient at the played actions $g(x_t), \partial g(x_t)$. Using the same constraint information, we show a stronger guarantee of anytime constraint satisfaction $g(x_t) \leq 0 \ \forall t \in [T]$, and matching $O(\sqrt{T})$ regret guarantees. These contributions are thanks to our approach of using Polyak feasibility steps to ensure constraint satisfaction, without sacrificing regret. Specifically, after each step of online gradient descent, our algorithm applies a subgradient descent step on the constraint function where the step-size is chosen according to the celebrated Polyak step-size. We further validate this approach with numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13112v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Spencer Hutchinson, Mahnoosh Alizadeh</dc:creator>
    </item>
    <item>
      <title>Weak Optimal Transport: When is the Dual Potential Convex?</title>
      <link>https://arxiv.org/abs/2507.07200</link>
      <description>arXiv:2507.07200v2 Announce Type: replace-cross 
Abstract: Weak optimal transport generalizes the classical theory of optimal transportation to nonlinear cost functions and covers a range of problems that lie beyond the traditional theory - including entropic transport, martingale transport, and applications in mechanism design. As in the classical case, the weak transport problem can also be written as a dual maximization problem over a pair of conjugate potentials.
  We identify sharp monotonicity conditions on the cost under which the dual problem can be restricted to convex potentials. This framework unifies several known results from the literature, including barycentric transport, martingale Benamou-Brenier, the multiple-good monopolist problem, Strassen's theorem, stochastic order projections and the classical Brenier theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07200v2</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Filip Pramenkovi\'c</dc:creator>
    </item>
  </channel>
</rss>
