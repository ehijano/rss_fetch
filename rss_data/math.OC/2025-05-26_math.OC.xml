<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 27 May 2025 03:12:51 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Global Optimization Through Heterogeneous Oscillator Ising Machines</title>
      <link>https://arxiv.org/abs/2505.17027</link>
      <description>arXiv:2505.17027v1 Announce Type: new 
Abstract: Oscillator Ising machines (OIMs) are networks of coupled oscillators that seek the minimum energy state of an Ising model. Since many NP-hard problems are equivalent to the minimization of an Ising Hamiltonian, OIMs have emerged as a promising computing paradigm for solving complex optimization problems that are intractable on existing digital computers. However, their performance is sensitive to the choice of tunable parameters, and convergence guarantees are mostly lacking. Here, we show that lower energy states are more likely to be stable, and that convergence to the global minimizer is often improved by introducing random heterogeneities in the regularization parameters. Our analysis relates the stability properties of Ising configurations to the spectral properties of a signed graph Laplacian. By examining the spectra of random ensembles of these graphs, we show that the probability of an equilibrium being asymptotically stable depends inversely on the value of the Ising Hamiltonian, biasing the system toward low-energy states. Our numerical results confirm our findings and demonstrate that heterogeneously designed OIMs efficiently converge to globally optimal solutions with high probability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17027v1</guid>
      <category>math.OC</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahmed Allibhoy, Arthur N. Montanari, Fabio Pasqualetti, Adilson E. Motter</dc:creator>
    </item>
    <item>
      <title>Successive Convexification for Passively-Safe Spacecraft Rendezvous on Near Rectilinear Halo Orbit</title>
      <link>https://arxiv.org/abs/2505.17251</link>
      <description>arXiv:2505.17251v1 Announce Type: new 
Abstract: We present an optimization-based approach for fuel-efficient spacecraft rendezvous to the Gateway, a space station that will be deployed on a near rectilinear halo orbit (NRHO) around the Moon. The approach: i) ensures passive safety and satisfies path constraints at all times, ii) meets the specifications for critical decision points along the trajectory, iii) accounts for uncertainties that are common in real-world operation, such as due to orbital insertion, actuation, and navigation measurement, via chance constraints and utilizes a stabilizing feedback controller to bound the effect of uncertainties. We leverage sequential convex programming (SCP) and isoperimetric reformulation of path constraints, including passive safety, to eliminate the risk of inter-sample constraint violations that is common in existing methods. We demonstrate the proposed approach on a realistic simulation of a rendezvous to the Gateway.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17251v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Purnanand Elango, Abraham P. Vinod, Kenji Kitamura, Beh\c{c}et A\c{c}{\i}kme\c{s}e, Stefano Di Cairano, Avishai Weiss</dc:creator>
    </item>
    <item>
      <title>Parallelizing the Circumcentered-Reflection Method</title>
      <link>https://arxiv.org/abs/2505.17258</link>
      <description>arXiv:2505.17258v1 Announce Type: new 
Abstract: This paper introduces the Parallelized Circumcentered Reflection Method (P-CRM), a circumcentric approach that parallelizes the Circumcentered Reflection Method (CRM) for solving Convex Feasibility Problems in affine settings. Beyond feasibility, P-CRM solves the best approximation problem for any finite collection of affine subspaces; that is, it not only finds a feasible point but directly computes the projection of an initial point onto the intersection. Within a fully self-contained scheme, we also introduce the Framework for the Simultaneous Projection Method (F-SPM) which includes Cimmino's method as a special case. Theoretical results show that both P-CRM and F-SPM achieve linear convergence. Moreover, P-CRM converges at a rate that is at least as fast as, and potentially superior to, the best convergence rate of F-SPM. As a byproduct, this also yields a new and simplified convergence proof for Cimmino's method. Numerical experiments show that P-CRM is competitive compared to CRM and indicate that it offers a scalable and flexible alternative, particularly suited for large-scale problems and modern computing environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17258v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pablo Barros, Roger Behling, Vincent Guigues, Luiz-Rafael Santos</dc:creator>
    </item>
    <item>
      <title>Subspace Newton's Method for $\ell_0$-Regularized Optimization Problems with Box Constraint</title>
      <link>https://arxiv.org/abs/2505.17382</link>
      <description>arXiv:2505.17382v1 Announce Type: new 
Abstract: This paper investigates the box-constrained $\ell_0$-regularized sparse optimization problem. We introduce the concept of a $\tau$-stationary point and establish its connection to the local and global minima of the box-constrained $\ell_0$-regularized sparse optimization problem. We utilize the $\tau$-stationary points to define the support set, which we divide into active and inactive components. Subsequently, the Newton's method is employed to update the non-active variables, while the proximal gradient method is utilized to update the active variables. If the Newton's method fails, we use the proximal gradient step to update all variables. Under some mild conditions, we prove the global convergence and the local quadratic convergence rate. Finally, experimental results demonstrate the efficiency of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17382v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuge Ye, Qingna Li</dc:creator>
    </item>
    <item>
      <title>Multi-cut stochastic approximation methods for solving stochastic convex composite optimization</title>
      <link>https://arxiv.org/abs/2505.17463</link>
      <description>arXiv:2505.17463v1 Announce Type: new 
Abstract: The development of a multi-cut stochastic approximation (SA) method for solving stochastic convex composite optimization (SCCO) problems has remained an open challenge. The difficulty arises from the fact that the stochastic multi-cut model, constructed as the pointwise maximum of individual stochastic linearizations, provides a biased estimate of the objective function, with the error being uncontrollable. This paper introduces multi-cut SA methods for solving SCCO problems, achieving near-optimal convergence rates. The cutting-plane models used in these methods are the pointwise maxima of appropriately chosen one-cut models. To the best of our knowledge, these are the first multi-cut SA methods specifically designed for SCCO problems. Finally, computational experiments demonstrate that these methods generally outperform both the robust stochastic approximation method and the stochastic dual averaging method across all instances tested.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17463v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaming Liang, Renato D. C. Monteiro, Honghao Zhang</dc:creator>
    </item>
    <item>
      <title>On the equivalence between static and dynamic optimal transport governed by linear control systems</title>
      <link>https://arxiv.org/abs/2505.17570</link>
      <description>arXiv:2505.17570v1 Announce Type: new 
Abstract: In this paper we revisit a class of optimal transport problems associated to non-autonomous linear control systems. Building on properties of the cost functions on $\mathbb{R}^{d}\times\mathbb{R}^{d}$ derived from suitable variational problems, we show the equivalence between the static and dynamic versions of the corresponding transport problems. Our analysis is constructive in nature and relies on functional analytic properties of the end-point map and the fine properties of the optimal control functions. These lead to some new quantitative estimates which play a crucial role in our investigation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17570v1</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amit Einav, Yue Jiang, Alp\'ar R. M\'esz\'aros</dc:creator>
    </item>
    <item>
      <title>Worst-case complexity analysis of derivative-free methods for multi-objective optimization</title>
      <link>https://arxiv.org/abs/2505.17594</link>
      <description>arXiv:2505.17594v1 Announce Type: new 
Abstract: In this work, we are concerned with the worst case complexity analysis of "a posteriori" methods for unconstrained multi-objective optimization problems where objective function values can only be obtained by querying a black box. We present two main algorithms, namely DFMOnew and DFMOlight which are based on a linesearch expansion technique. In particular, \DFMOnew, requires a complete exploration of the points in the current set of non-dominated solutions, whereas DFMOlight only requires the exploration around a single point in the set of non-dominated solutions. For these algorithms, we derive worst case iteration and evaluation complexity results. In particular, the complexity results for DFMOlight aligns with those recently proved in the literature for a directional multisearch method. Furthermore, exploiting an expansion technique of the step, we are also able to give further complexity results concerning the number of iterations with a measure of stationarity above a prefixed tolerance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17594v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giampaolo Liuzzi, Stefano Lucidi</dc:creator>
    </item>
    <item>
      <title>Optimal control of variable-exponent subdiffusion</title>
      <link>https://arxiv.org/abs/2505.17678</link>
      <description>arXiv:2505.17678v1 Announce Type: new 
Abstract: This work investigates the optimal control of the variable-exponent subdiffusion, which extends the work [Gunzburger and Wang, {\it SIAM J. Control Optim.} 2019] to the variable-exponent case to account for the multiscale and crossover diffusion behavior. To resolve the difficulties caused by the leading variable-exponent operator, we adopt the convolution method to reformulate the model into an equivalent but more tractable form, and then prove the well-posedness and weighted regularity of the optimal control. As the convolution kernels in reformulated models are indefinite-sign, non-positive-definite, and non-monotonic, we adopt the discrete convolution kernel approach in numerical analysis to show the $O(\tau(1+|\ln\tau|)+h^2)$ accuracy of the schemes for state and adjoint equations. Numerical experiments are performed to substantiate the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17678v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiqun Li, Mengmeng Liu, Wenlin Qiu, Hong Wang, Xiangcheng Zheng</dc:creator>
    </item>
    <item>
      <title>A model-free approach to control barrier functions using funnel control</title>
      <link>https://arxiv.org/abs/2505.17887</link>
      <description>arXiv:2505.17887v1 Announce Type: new 
Abstract: Control barrier functions (CBFs) are a popular approach to design feedback laws that achieve safety guarantees for nonlinear systems. The CBF-based controller design relies on the availability of a model to select feasible inputs from the set of CBF-based controls. In this paper, we develop a model-free approach to design CBF-based control laws, eliminating the need for knowledge of system dynamics or parameters. Specifically, we address safety requirements characterized by a time-varying distance to a reference trajectory in the output space and construct a CBF that depends only on the measured output. Utilizing this particular CBF, we determine a subset of CBF-based controls without relying on a model of the dynamics by using techniques from funnel control. The latter is a model-free high-gain adaptive control methodology, which achieves tracking guarantees via reactive feedback. In this paper, we discover and establish a connection between the modular controller synthesis via zeroing CBFs and model-free reactive feedback. The theoretical results are illustrated by a numerical simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17887v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lukas Lanza, Johannes K\"ohler, Dario Dennst\"adt, Thomas Berger, Karl Worthmann</dc:creator>
    </item>
    <item>
      <title>LMask: Learn to Solve Constrained Routing Problems with Lazy Masking</title>
      <link>https://arxiv.org/abs/2505.17938</link>
      <description>arXiv:2505.17938v1 Announce Type: new 
Abstract: Routing problems are canonical combinatorial optimization tasks with wide-ranging applications in logistics, transportation, and supply chain management. However, solving these problems becomes significantly more challenging when complex constraints are involved. In this paper, we propose LMask, a novel learning framework that utilizes dynamic masking to generate high-quality feasible solutions for constrained routing problems. LMask introduces the LazyMask decoding method, which lazily refines feasibility masks with the backtracking mechanism. In addition, it employs the refinement intensity embedding to encode the search trace into the model, mitigating representation ambiguities induced by backtracking. To further reduce sampling cost, LMask sets a backtracking budget during decoding, while constraint violations are penalized in the loss function during training to counteract infeasibility caused by this budget. We provide theoretical guarantees for the validity and probabilistic optimality of our approach. Extensive experiments on the traveling salesman problem with time windows (TSPTW) and TSP with draft limits (TSPDL) demonstrate that LMask achieves state-of-the-art feasibility rates and solution quality, outperforming existing neural methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17938v1</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyou Li, Haijun Zou, Jiayuan Wu, Zaiwen Wen</dc:creator>
    </item>
    <item>
      <title>New Tight Bounds for SGD without Variance Assumption: A Computer-Aided Lyapunov Analysis</title>
      <link>https://arxiv.org/abs/2505.17965</link>
      <description>arXiv:2505.17965v1 Announce Type: new 
Abstract: The analysis of Stochastic Gradient Descent (SGD) often relies on making some assumption on the variance of the stochastic gradients, which is usually not satisfied or difficult to verify in practice. This paper contributes to a recent line of works which attempt to provide guarantees without making any variance assumption, leveraging only the (strong) convexity and smoothness of the loss functions. In this context, we prove new theoretical bounds derived from the monotonicity of a simple Lyapunov energy, improving the current state-of-the-art and extending their validity to larger step-sizes. Our theoretical analysis is backed by a Performance Estimation Problem analysis, which allows us to claim that, empirically, the bias term in our bounds is tight within our framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17965v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Cortild, Lucas Ketels, Juan Peypouquet, Guillaume Garrigos</dc:creator>
    </item>
    <item>
      <title>Deep Operator Neural Network Model Predictive Control</title>
      <link>https://arxiv.org/abs/2505.18008</link>
      <description>arXiv:2505.18008v1 Announce Type: new 
Abstract: In this paper, we consider the design of model predictive control (MPC) algorithms based on deep operator neural networks (DeepONets). These neural networks are capable of accurately approximating real and complex valued solutions of continuous time nonlinear systems without relying on recurrent architectures. The DeepONet architecture is made up of two feedforward neural networks: the branch network, which encodes the input function space, and the trunk network, which represents dependencies on temporal variables or initial conditions. Utilizing the original DeepONet architecture as a predictor within MPC for Multi Input Multi Output (MIMO) systems requires multiple branch networks, to generate multi output predictions, one for each input. Moreover, to predict multiple time steps into the future, the network has to be evaluated multiple times. Motivated by this, we introduce a multi step DeepONet (MS-DeepONet) architecture that computes in one shot multi step predictions of system outputs from multi step input sequences, which is better suited for MPC. We prove that the MS DeepONet is a universal approximator in terms of multi step sequence prediction. Additionally, we develop automated hyper parameter selection strategies and implement MPC frameworks using both the standard DeepONet and the proposed MS DeepONet architectures in PyTorch. The implementation is publicly available on GitHub. Simulation results demonstrate that MS-DeepONet consistently outperforms the standard DeepONet in learning and predictive control tasks across several nonlinear benchmark systems: the van der Pol oscillator, the quadruple tank process, and a cart pendulum unstable system, where it successfully learns and executes multiple swing up and stabilization policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18008v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Oliver de Jong, Khemraj Shukla, Mircea Lazar</dc:creator>
    </item>
    <item>
      <title>Empathic network learning for multi-expert emergency decision-making under incomplete and inconsistent information</title>
      <link>https://arxiv.org/abs/2505.18009</link>
      <description>arXiv:2505.18009v1 Announce Type: new 
Abstract: Challenges, such as a lack of information for emergency decision-making, time pressure, and limited knowledge of experts acting as decision-makers (DMs), can result in the generation of poor or inconsistent indirect information regarding DMs' preferences. Simultaneously, the empathic relationship represents a tangible social connection within the context of actual emergency decision-making, with the structure of the empathic network being a significant factor influencing the outcomes of the decision-making process. To deduce the empathic network underpinning the decision behaviors of DMs from incomplete and inconsistent preference information, we introduce an empathic network learning methodology rooted in the concept of robust ordinal regression via preference disaggregation. Firstly, we complete incomplete fuzzy judgment matrices including holistic preference information given in terms of decision examples on some reference alternatives, independently by each DM, and we calculate the intrinsic utilities of DMs. Secondly, we establish constraints for empathic network learning models based on empathic preference information and information about relations between some reference nodes. Then, the necessary and possible empathic relationships between any two DMs are calculated. Lastly, tailored to the specific requirements of different emergency scenarios, we design six target networks and construct models to derive the most representative empathic network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18009v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.inffus.2024.102844.</arxiv:DOI>
      <arxiv:journal_reference>Inf. Fusion 117 (2025) 102844</arxiv:journal_reference>
      <dc:creator>Simin Shen, Zaiwu Gong, Bin Zhou, Roman Slowinski</dc:creator>
    </item>
    <item>
      <title>Efficient Conditional Gradient Methods for Solving Stochastic Convex Bilevel Optimization Problems</title>
      <link>https://arxiv.org/abs/2505.18037</link>
      <description>arXiv:2505.18037v1 Announce Type: new 
Abstract: We propose efficient methods for solving stochastic convex bilevel optimization problems, where the goal is to minimize an outer stochastic objective function subject to the solution set of an inner stochastic optimization problem. Existing methods often rely on costly projection or linear optimization oracles over complex sets, which limits scalability. To overcome this, we propose an iteratively regularized conditional gradient framework that leverages efficient linear optimization oracles exclusively over the base feasible set. Our proposed methods employ a vanishing regularization sequence that progressively emphasizes the inner problem while biasing towards desirable minimal outer objective solutions. Under standard convexity assumptions, we establish non-asymptotic convergence rates of $O(t^{-({1}/{2}-p)})$ for the outer objective and $O(t^{-p})$ for the inner objective, where $p \in (0,1/2)$ controls the regularization decay, in the one-sample stochastic setting, and $O(t^{-(1-p)})$ and $O(t^{-p})$ in the finite-sum setting using a mini-batch scheme, where $p \in (0,1)$. Experimental results on over-parametrized regression and $\ell_1$-constrained logistics regression tasks demonstrate the practical advantages of our approach over existing methods, confirming our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18037v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Khanh-Hung Giang-Tran, Soroosh Shafiee, Nam Ho-Nguyen</dc:creator>
    </item>
    <item>
      <title>Blindfolded Spider-man Optimization: A Single-Point Metaheuristics Suitable for Continuous and Discrete Spaces</title>
      <link>https://arxiv.org/abs/2505.17069</link>
      <description>arXiv:2505.17069v1 Announce Type: cross 
Abstract: In this study, we introduce a new single point metaheuristic optimization approach suitable for both continuous and discrete domains. The proposed algorithm, entitled Blindfolded Spiderman Optimization, follows a piecewise linear search trajectory where each line segment considers a move to an improved solution point. The trajectory resembles spiderman jumping from one building to the highest neighbor building in a blindfolded manner. Blindfolded Spiderman Optimization builds on top of the Buggy Pinball Optimization algorithm. Blindfolded Spiderman Optimization is tested on 16 mathematical optimization functions and one discrete problem of Unbounded Knapsack. We perform a thorough evaluation of Blindfolded Spiderman Optimization against established and state-of-the-art metaheuristic optimization methods, including Whale Optimization, Grey Wolf Optimization, Particle Swarm Optimization, Simulated Annealing, Threshold Accepting, and Buggy Pinball Optimization considering various optimization domains and dimensions. We show that Blindfolded Spiderman Optimization achieves great performance on both continuous and discrete spaces, and superior performance compared to all single-point metaheuristic approaches considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17069v1</guid>
      <category>cs.NE</category>
      <category>math.OC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Satyam Mittal</dc:creator>
    </item>
    <item>
      <title>Deconfounded Warm-Start Thompson Sampling with Applications to Precision Medicine</title>
      <link>https://arxiv.org/abs/2505.17283</link>
      <description>arXiv:2505.17283v1 Announce Type: cross 
Abstract: Randomized clinical trials often require large patient cohorts before drawing definitive conclusions, yet abundant observational data from parallel studies remains underutilized due to confounding and hidden biases. To bridge this gap, we propose Deconfounded Warm-Start Thompson Sampling (DWTS), a practical approach that leverages a Doubly Debiased LASSO (DDL) procedure to identify a sparse set of reliable measured covariates and combines them with key hidden covariates to form a reduced context. By initializing Thompson Sampling (LinTS) priors with DDL-estimated means and variances on these measured features -- while keeping uninformative priors on hidden features -- DWTS effectively harnesses confounded observational data to kick-start adaptive clinical trials. Evaluated on both a purely synthetic environment and a virtual environment created using real cardiovascular risk dataset, DWTS consistently achieves lower cumulative regret than standard LinTS, showing how offline causal insights from observational data can improve trial efficiency and support more personalized treatment decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17283v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prateek Jaiswal, Esmaeil Keyvanshokooh, Junyu Cao</dc:creator>
    </item>
    <item>
      <title>Implicit Regularization of Infinitesimally-perturbed Gradient Descent Toward Low-dimensional Solutions</title>
      <link>https://arxiv.org/abs/2505.17304</link>
      <description>arXiv:2505.17304v1 Announce Type: cross 
Abstract: Implicit regularization refers to the phenomenon where local search algorithms converge to low-dimensional solutions, even when such structures are neither explicitly specified nor encoded in the optimization problem. While widely observed, this phenomenon remains theoretically underexplored, particularly in modern over-parameterized problems. In this paper, we study the conditions that enable implicit regularization by investigating when gradient-based methods converge to second-order stationary points (SOSPs) within an implicit low-dimensional region of a smooth, possibly nonconvex function. We show that successful implicit regularization hinges on two key conditions: $(i)$ the ability to efficiently escape strict saddle points, while $(ii)$ maintaining proximity to the implicit region. Existing analyses enabling the convergence of gradient descent (GD) to SOSPs often rely on injecting large perturbations to escape strict saddle points. However, this comes at the cost of deviating from the implicit region. The central premise of this paper is that it is possible to achieve the best of both worlds: efficiently escaping strict saddle points using infinitesimal perturbations, while controlling deviation from the implicit region via a small deviation rate. We show that infinitesimally perturbed gradient descent (IPGD), which can be interpreted as GD with inherent ``round-off errors'', can provably satisfy both conditions. We apply our framework to the problem of over-parameterized matrix sensing, where we establish formal guarantees for the implicit regularization behavior of IPGD. We further demonstrate through extensive experiments that these insights extend to a broader class of learning problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17304v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianhao Ma, Geyu Liang, Salar Fattahi</dc:creator>
    </item>
    <item>
      <title>SEvoBench : A C++ Framework For Evolutionary Single-Objective Optimization Benchmarking</title>
      <link>https://arxiv.org/abs/2505.17430</link>
      <description>arXiv:2505.17430v1 Announce Type: cross 
Abstract: We present SEvoBench, a modern C++ framework for evolutionary computation (EC), specifically designed to systematically benchmark evolutionary single-objective optimization algorithms. The framework features modular implementations of Particle Swarm Optimization (PSO) and Differential Evolution (DE) algorithms, organized around three core components: (1) algorithm construction with reusable modules, (2) efficient benchmark problem suites, and (3) parallel experimental analysis. Experimental evaluations demonstrate the framework's superior performance in benchmark testing and algorithm comparison. Case studies further validate its capabilities in algorithm hybridization and parameter analysis. Compared to existing frameworks, SEvoBench demonstrates three key advantages: (i) highly efficient and reusable modular implementations of PSO and DE algorithms, (ii) accelerated benchmarking through parallel execution, and (iii) enhanced computational efficiency via SIMD (Single Instruction Multiple Data) vectorization for large-scale problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17430v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.MS</category>
      <category>math.OC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3712255.3734350</arxiv:DOI>
      <dc:creator>Yongkang Yang, Jian Zhao, Tengfei Yang</dc:creator>
    </item>
    <item>
      <title>Efficient compression of neural networks and datasets</title>
      <link>https://arxiv.org/abs/2505.17469</link>
      <description>arXiv:2505.17469v1 Announce Type: cross 
Abstract: We compare, improve, and contribute methods that substantially decrease the number of parameters of neural networks while maintaining high test accuracy. When applying our methods to minimize description length, we obtain very effective data compression algorithms. In particular, we develop a probabilistic reformulation of $\ell_0$ regularized optimization for nonlinear models that does not require Monte-Carlo sampling and thus improves upon previous methods. We also improve upon methods involving smooth approximations to the $\ell_0$ norm, and investigate layerwise methods. We compare the methods on different architectures and datasets, including convolutional networks trained on image datasets and transformers trained on parts of Wikipedia. We also created a synthetic teacher-student setup to investigate compression in a controlled continuous setting. Finally, we conceptually relate compression algorithms to Solomonoff's theory of inductive inference and empirically verify the prediction that regularized models can exhibit more sample-efficient convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17469v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lukas Silvester Barth, Paulo von Petersenn</dc:creator>
    </item>
    <item>
      <title>Surface-Encoded Partial Coherence Transformation: Modeling Source Coherence Effects in Wave Optics</title>
      <link>https://arxiv.org/abs/2505.17754</link>
      <description>arXiv:2505.17754v1 Announce Type: cross 
Abstract: We present a new mathematical framework for incorporating partial coherence effects into wave optics simulations through a comprehensive surface-to-detector approach. Unlike traditional ensemble averaging methods, our dual-component framework models partial coherence through: (1) a surface-encoded transformation implemented via a linear integral operator with a spatially-dependent kernel that modifies coherence properties at the reflection interface, followed by (2) a propagation component that evolves these coherence properties to the detection plane. This approach differs fundamentally from conventional models by explicitly separating surface interactions from propagation effects, while maintaining a unified mathematical structure. We derive the mathematical foundation based on the coherence function formalism, establish the connection to the Van Cittert-Zernike theorem, and prove the equivalence of our framework to conventional partial coherence theory. The method reduces the dimensional complexity of coherence calculations and offers potential computational advantages, particularly for systems involving multiple surfaces and propagation steps. Applications include optical testing and astronomical instrumentation. We provide rigorous mathematical proofs, demonstrate the convergence properties, and analyze the relative importance of surface and propagation effects across different optical scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17754v1</guid>
      <category>physics.optics</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.OC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Netzer Moriya</dc:creator>
    </item>
    <item>
      <title>Sufficient Conditions for Detectability of Approximately Discretized Nonlinear Systems</title>
      <link>https://arxiv.org/abs/2505.17857</link>
      <description>arXiv:2505.17857v1 Announce Type: cross 
Abstract: In many sampled-data applications, observers are designed based on approximately discretized models of continuous-time systems, where usually only the discretized system is analyzed in terms of its detectability. In this paper, we show that if the continuous-time system satisfies certain linear matrix inequality (LMI) conditions, and the sampling period of the discretization scheme is sufficiently small, then the whole family of discretized systems (parameterized by the sampling period) satisfies analogous discrete-time LMI conditions that imply detectability. Our results are applicable to general discretization schemes, as long as they produce approximate models whose linearizations are in some sense consistent with the linearizations of the continuous-time ones. We explicitly show that the Euler and second-order Runge-Kutta methods satisfy this condition. A batch-reactor system example is provided to highlight the usefulness of our results from a practical perspective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17857v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seth Siriya, Julian D. Schiller, Victor G. Lopez, Matthias A. M\"uller</dc:creator>
    </item>
    <item>
      <title>SpectraLDS: Provable Distillation for Linear Dynamical Systems</title>
      <link>https://arxiv.org/abs/2505.17868</link>
      <description>arXiv:2505.17868v1 Announce Type: cross 
Abstract: We present the first provable method for identifying symmetric linear dynamical systems (LDS) with accuracy guarantees that are independent of the systems' state dimension or effective memory. Our approach builds upon recent work that represents symmetric LDSs as convolutions learnable via fixed spectral transformations. We show how to invert this representation, thereby recovering an LDS model from its spectral transform and yielding an end-to-end convex optimization procedure. This distillation preserves predictive accuracy while enabling constant-time and constant-space inference per token, independent of sequence length. We evaluate our method, SpectraLDS, as a component in sequence prediction architectures and demonstrate that accuracy is preserved while inference efficiency is improved on tasks such as language modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17868v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Devan Shah, Shlomo Fortgang, Sofiia Druchyna, Elad Hazan</dc:creator>
    </item>
    <item>
      <title>Beyond Discreteness: Finite-Sample Analysis of Straight-Through Estimator for Quantization</title>
      <link>https://arxiv.org/abs/2505.18113</link>
      <description>arXiv:2505.18113v1 Announce Type: cross 
Abstract: Training quantized neural networks requires addressing the non-differentiable and discrete nature of the underlying optimization problem. To tackle this challenge, the straight-through estimator (STE) has become the most widely adopted heuristic, allowing backpropagation through discrete operations by introducing surrogate gradients. However, its theoretical properties remain largely unexplored, with few existing works simplifying the analysis by assuming an infinite amount of training data. In contrast, this work presents the first finite-sample analysis of STE in the context of neural network quantization. Our theoretical results highlight the critical role of sample size in the success of STE, a key insight absent from existing studies. Specifically, by analyzing the quantization-aware training of a two-layer neural network with binary weights and activations, we derive the sample complexity bound in terms of the data dimensionality that guarantees the convergence of STE-based optimization to the global minimum. Moreover, in the presence of label noises, we uncover an intriguing recurrence property of STE-gradient method, where the iterate repeatedly escape from and return to the optimal binary weights. Our analysis leverages tools from compressed sensing and dynamical systems theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18113v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Halyun Jeong, Jack Xin, Penghang Yin</dc:creator>
    </item>
    <item>
      <title>Risk-averse optimal control of random elliptic variational inequalities</title>
      <link>https://arxiv.org/abs/2210.03425</link>
      <description>arXiv:2210.03425v2 Announce Type: replace 
Abstract: We consider a risk-averse optimal control problem governed by an elliptic variational inequality (VI) subject to random inputs. By deriving KKT-type optimality conditions for a penalised and smoothed problem and studying convergence of the stationary points with respect to the penalisation parameter, we obtain two forms of stationarity conditions. The lack of regularity with respect to the uncertain parameters and complexities induced by the presence of the risk measure give rise to new challenges unique to the stochastic setting. We also propose a path-following stochastic approximation algorithm using variance reduction techniques and demonstrate the algorithm on a modified benchmark problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.03425v2</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <category>math.PR</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amal Alphonse, Caroline Geiersbach, Michael Hinterm\"uller, Thomas M. Surowiec</dc:creator>
    </item>
    <item>
      <title>The Barzilai-Borwein Method for Distributed Optimization over Unbalanced Directed Networks</title>
      <link>https://arxiv.org/abs/2305.11469</link>
      <description>arXiv:2305.11469v4 Announce Type: replace 
Abstract: This paper studies optimization problems over multi-agent systems, in which all agents cooperatively minimize a global objective function expressed as a sum of local cost functions. Each agent in the systems uses only local computation and communication in the overall process without leaking their private information. Based on the Barzilai-Borwein (BB) method and multi-consensus inner loops, a distributed algorithm with the availability of larger stepsizes and accelerated convergence, namely ADBB, is proposed. Moreover, owing to employing only row-stochastic weight matrices, ADBB can resolve the optimization problems over unbalanced directed networks without requiring the knowledge of neighbors' out-degree for each agent. Via establishing contraction relationships between the consensus error, the optimality gap, and the gradient tracking error, ADBB is theoretically proved to converge linearly to the globally optimal solution. A real-world data set is used in simulations to validate the correctness of the theoretical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.11469v4</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.engappai.2020.104151</arxiv:DOI>
      <arxiv:journal_reference>Engineering Applications of Artificial Intelligence 99 (2021) 104151</arxiv:journal_reference>
      <dc:creator>Jinhui Hu, Xin Chen, Lifeng Zheng, Ling Zhang, Huaqing Li</dc:creator>
    </item>
    <item>
      <title>A semidefinite programming hierarchy for covering problems in discrete geometry</title>
      <link>https://arxiv.org/abs/2312.11267</link>
      <description>arXiv:2312.11267v2 Announce Type: replace 
Abstract: In this paper we present a new semidefinite programming hierarchy for covering problems in compact metric spaces. Over the last years, these kind of hierarchies were developed primarily for geometric packing and for energy minimization problems; they frequently provide the best known bounds. Starting from a semidefinite programming hierarchy for the dominating set problem in graph theory, we derive the new hierarchy for covering and show some of its basic properties: The hierarchy converges in finitely many steps, but the first level collapses to the volume bound when the compact metric space is homogeneous.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.11267v2</guid>
      <category>math.OC</category>
      <category>math.MG</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cordian Riener, Jan Rolfes, Frank Vallentin</dc:creator>
    </item>
    <item>
      <title>A Unifying System Theory Framework for Distributed Optimization and Games</title>
      <link>https://arxiv.org/abs/2401.12623</link>
      <description>arXiv:2401.12623v4 Announce Type: replace 
Abstract: This paper introduces a systematic methodological framework to design and analyze distributed algorithms for optimization and games over networks. Starting from a centralized method, we identify an aggregation function involving all the decision variables (e.g., a global cost gradient or constraint) and introduce a distributed consensus-oriented scheme to asymptotically approximate the unavailable information at each agent. Then, we delineate the proper methodology for intertwining the identified building blocks, i.e., the optimization-oriented method and the consensus-oriented one. The key intuition is to interpret the obtained interconnection as a singularly perturbed system. We rely on this interpretation to provide sufficient conditions for the building blocks to be successfully connected into a distributed scheme exhibiting the convergence guarantees of the centralized algorithm. Finally, we show the potential of our approach by developing a new distributed scheme for constraint-coupled problems with a linear convergence rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12623v4</guid>
      <category>math.OC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guido Carnevale, Nicola Mimmo, Giuseppe Notarstefano</dc:creator>
    </item>
    <item>
      <title>Small-Time Local Controllability of the multi-input bilinear Schr\"odinger equation thanks to a quadratic term</title>
      <link>https://arxiv.org/abs/2407.07446</link>
      <description>arXiv:2407.07446v5 Announce Type: replace 
Abstract: The goal of this article is to contribute to a better understanding of the relations between the exact controllability of nonlinear PDEs and the control theory for ODEs based on Lie brackets, through a study of the Schr\"odinger PDE with bilinear control. We focus on the small-time local controllability (STLC) around an equilibrium, when the linearized system is not controllable. We study the second-order term in the Taylor expansion of the state, with respect to the control. For scalar-input ODEs, quadratic terms never recover controllability: they induce signed drifts in the dynamics. Thus proving STLC requires to go at least to the third order. Similar results were proved for the bilinear Schr\"odinger PDE with scalar-input controls. In this article, we study the case of multi-input systems. We clarify among the quadratic Lie brackets, those that allow to recover STLC: they are bilinear with respect to two different controls. For ODEs, our result is a consequence of Sussman's sufficient condition $S(\theta)$ (when focused on quadratic terms), but we propose a new proof, designed to prepare an easier transfer to PDEs. This proof relies on a representation formula of the state inspired by the Magnus formula. By adapting it, we prove a new STLC result for the bilinear Schr\"odinger PDE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07446v5</guid>
      <category>math.OC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Th\'eo Gherdaoui (IRMAR, ENS Rennes)</dc:creator>
    </item>
    <item>
      <title>A posteriori error estimates for a bang-bang optimal control problem</title>
      <link>https://arxiv.org/abs/2409.03064</link>
      <description>arXiv:2409.03064v2 Announce Type: replace 
Abstract: We propose and analyze a posteriori error estimates for a control-constrained optimal control problem with bang-bang solutions. We consider a solution strategy based on the variational approach, where the control variable is not discretized; no Tikhonov regularization is made. We design, for the proposed scheme, a residual-type a posteriori error estimator that can be decomposed as the sum of two individual contributions related to the discretization of the state and adjoint equations. We explore reliability and efficiency properties of the aforementioned error estimator. We illustrate the theory with numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03064v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francisco Fuica</dc:creator>
    </item>
    <item>
      <title>Power of Generalized Smoothness in Stochastic Convex Optimization: First- and Zero-Order Algorithms</title>
      <link>https://arxiv.org/abs/2501.18198</link>
      <description>arXiv:2501.18198v2 Announce Type: replace 
Abstract: This paper is devoted to the study of stochastic optimization problems under the generalized smoothness assumption. By considering the unbiased gradient oracle in Stochastic Gradient Descent, we provide strategies to achieve in bounds the summands describing linear rate. In particular, in the case $L_0 = 0$, we obtain in the convex setup the iteration complexity: $N = \mathcal{O}\left(L_1R \log\frac{1}{\varepsilon} + \frac{L_1 c R^2}{\varepsilon}\right)$ for Clipped Stochastic Gradient Descent and $N = \mathcal{O}\left(L_1R \log\frac{1}{\varepsilon}\right)$ for Normalized Stochastic Gradient Descent. Furthermore, we generalize the convergence results to the case with a biased gradient oracle, and show that the power of $(L_0,L_1)$-smoothness extends to zero-order algorithms. Finally, we demonstrate the possibility of linear convergence in the convex setup through numerical experimentation, which has aroused some interest in the machine learning community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18198v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aleksandr Lobanov, Alexander Gasnikov</dc:creator>
    </item>
    <item>
      <title>Numerical Approximation of Delay Differential Equations via Operator Splitting in Fractional Domains</title>
      <link>https://arxiv.org/abs/2502.05483</link>
      <description>arXiv:2502.05483v4 Announce Type: replace 
Abstract: This paper develops a rigorous framework for the numerical approximation of both autonomous and non-autonomous delay differential equations (DDEs), with a focus on the implicit Euler method and sequential operator splitting.
  To overcome the difficulty that the delay operator does not generate an analytic semigroup in the standard space \( L^1[\tau, 0] \), we embed the problem into the interpolation space \( \left(L^1[\tau, 0], W^{1,1}_0[\tau, 0]\right)_{\theta, 1} \) for \( 0 &lt; \theta &lt; 1 \), where the differential operator becomes sectorial. This allows the full operator \( L = A + B \) to generate an analytic semigroup \( T_L(t) \), enabling the use of semigroup theory to derive sharp error estimates.
  We prove that the implicit Euler method achieves a global error of order \( \mathcal{O}(h) \), while the Lie--Trotter splitting method yields an error of order \( \mathcal{O}(h^{2\theta - 1}) \) in the interpolation norm. These theoretical rates are confirmed by numerical experiments, including comparisons with exact solutions obtained via semi-analytical Fourier-based methods in the non-autonomous setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05483v4</guid>
      <category>math.OC</category>
      <category>math.FA</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hideki Kawahara</dc:creator>
    </item>
    <item>
      <title>A low-complexity funnel control approach for non-linear systems of higher-order</title>
      <link>https://arxiv.org/abs/2503.11370</link>
      <description>arXiv:2503.11370v2 Announce Type: replace 
Abstract: We address the problem of output reference tracking for unknown non-linear multi-input, multi-output systems described by functional differential equations. This class of systems includes those with a strict relative degree, and bounded-input bounded-output (BIBO) stable internal dynamics. The objective is to ensure that the tracking error evolves within a prescribed performance funnel. To achieve this, we propose a novel model-free adaptive controller with lower complexity than existing funnel control methods, avoiding the use of non-linearities in intermediate error signals. We establish the feasibility and effectiveness of the proposed design through theoretical analysis and demonstrate its performance with simulations, comparing it to previous approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11370v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Dario Dennst\"adt</dc:creator>
    </item>
    <item>
      <title>Robust Nash equilibrium seeking based on semi-Markov switching topologies</title>
      <link>https://arxiv.org/abs/2504.19229</link>
      <description>arXiv:2504.19229v2 Announce Type: replace 
Abstract: This paper investigates a distributed robust Nash Equilibrium (NE) seeking problem in fluctuating environments. Specifically, the players, subject to the second-order dynamics, are considered to be influenced by external disturbances and uncertain dynamics while communicating via semi-Markov switching topologies. In such constantly changing network circumstances, the existence of disturbances and uncertain dynamics may directly affect the performance of most existing NE seeking algorithms. Moreover, the semi-Markov switching topologies may cause communication uncertainty, which are considered in NE seeking for the first time. To accommodate the above concerns, the following targets require to be reached simultaneously: (1) Disturbances and uncertain dynamics rejection in finite time; (2) Distributed estimation on unknown information required for players' cost functions; (3) A reasonable estimation consensus protocol under semi-Markov switching; (4) NE seeking for the second-order players. By combining supertwisting-based Integral Sliding-Mode Control (ISMC) with average consensus tracking, a novel robust NE seeking algorithm is constructed, incorporating an effective leader-follower consensus protocol. Furthermore, to lessen dispensable information transmission, a sampled-data-based event-triggered mechanism is introduced. Incorporating the advantages of both semi-Markov switching and event-triggered mechanism, another NE seeking algorithm is proposed. Through designing an appropriate Lyapunov-Krasovskii functional, it is shown that the leader-follower consensus can be achieved in the mean-square sense under event-triggered mechanism. Finally, a connectivity control game is formulated to illustrate the validity of the designed algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19229v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianing Chen, Sitian Qin, Chuangyin Dang</dc:creator>
    </item>
    <item>
      <title>Normalized Cut with Reinforcement Learning in Constrained Action Space</title>
      <link>https://arxiv.org/abs/2505.13986</link>
      <description>arXiv:2505.13986v2 Announce Type: replace 
Abstract: Reinforcement Learning (RL) has emerged as an important paradigm to solve combinatorial optimization problems primarily due to its ability to learn heuristics that can generalize across problem instances. However, integrating external knowledge that will steer combinatorial optimization problem solutions towards domain appropriate outcomes remains an extremely challenging task. In this paper, we propose the first RL solution that uses constrained action spaces to guide the normalized cut problem towards pre-defined template instances. Using transportation networks as an example domain, we create a Wedge and Ring Transformer that results in graph partitions that are shaped in form of Wedges and Rings and which are likely to be closer to natural optimal partitions. However, our approach is general as it is based on principles that can be generalized to other domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13986v2</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qize Jiang, Linsey Pang, Alice Gatti, Mahima Aggarwal, Giovanna Vantini, Xiaosong Ma, Weiwei Sun, Sanjay Chawla</dc:creator>
    </item>
    <item>
      <title>On the equivalence between functionally affine LPV state-space representations and LFT models</title>
      <link>https://arxiv.org/abs/2505.14993</link>
      <description>arXiv:2505.14993v2 Announce Type: replace 
Abstract: We propose a transformation algorithm for a class of Linear Parameter-Varying (LPV) systems with functional affine dependence on parameters, where the system matrices depend affinely on nonlinear functions of the scheduling varable, into Linear Fractional Transformation (LFT) systems. The transformation preserves input-output behavior and minimality, and the uncertainity block of the resulting LFT system is linear in the scheduling variables of the LPV system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14993v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mih\'aly Petreczky, Ziad Alkhoury, Guillaume Merc\`ere</dc:creator>
    </item>
    <item>
      <title>Achieving Linear Speedup with ProxSkip in Distributed Stochastic Optimization</title>
      <link>https://arxiv.org/abs/2310.07983</link>
      <description>arXiv:2310.07983v3 Announce Type: replace-cross 
Abstract: The ProxSkip algorithm for distributed optimization is gaining increasing attention due to its proven benefits in accelerating communication complexity while maintaining robustness against data heterogeneity. However, existing analyses of ProxSkip are limited to the strongly convex setting and do not achieve linear speedup, where convergence performance increases linearly with respect to the number of nodes. So far, questions remain open about how ProxSkip behaves in the non-convex setting and whether linear speedup is achievable. In this paper, we revisit distributed ProxSkip and address both questions. We demonstrate that the leading communication complexity of ProxSkip is $\mathcal{O}(\frac{p\sigma^2}{n\epsilon^2})$ for non-convex and convex settings, and $\mathcal{O}(\frac{p\sigma^2}{n\epsilon})$ for the strongly convex setting, where $n$ represents the number of nodes, $p$ denotes the probability of communication, $\sigma^2$ signifies the level of stochastic noise, and $\epsilon$ denotes the desired accuracy level. This result illustrates that ProxSkip achieves linear speedup and can asymptotically reduce communication overhead proportional to the probability of communication. Additionally, for the strongly convex setting, we further prove that ProxSkip can achieve linear speedup with network-independent stepsizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07983v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luyao Guo, Sulaiman A. Alghunaim, Kun Yuan, Laurent Condat, Jinde Cao</dc:creator>
    </item>
    <item>
      <title>On non-approximability of zero loss global ${\mathcal L}^2$ minimizers by gradient descent in Deep Learning</title>
      <link>https://arxiv.org/abs/2311.07065</link>
      <description>arXiv:2311.07065v3 Announce Type: replace-cross 
Abstract: We analyze geometric aspects of the gradient descent algorithm in Deep Learning (DL), and give a detailed discussion of the circumstance that in underparametrized DL networks, zero loss minimization can generically not be attained. As a consequence, we conclude that the distribution of training inputs must necessarily be non-generic in order to produce zero loss minimizers, both for the method constructed in [Chen-Munoz Ewald 2023, 2024], or for gradient descent [Chen 2025] (which assume clustering of training data).</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.07065v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Chen, Patricia Mu\~noz Ewald</dc:creator>
    </item>
    <item>
      <title>Subdifferentials and penalty approximations of the obstacle problem</title>
      <link>https://arxiv.org/abs/2412.13029</link>
      <description>arXiv:2412.13029v2 Announce Type: replace-cross 
Abstract: We consider a framework for approximating the obstacle problem through a penalty approach by nonlinear PDEs. By using tools from capacity theory, we show that derivatives of the solution maps of the penalised problems converge in the weak operator topology to an element of the strong-weak Bouligand subdifferential. We are able to treat smooth penalty terms as well as nonsmooth ones involving for example the positive part function $\max(0,\cdot)$. Our abstract framework applies to several specific choices of penalty functions which are omnipresent in the literature. We conclude with consequences to the theory of optimal control of the obstacle problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13029v2</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amal Alphonse, Gerd Wachsmuth</dc:creator>
    </item>
    <item>
      <title>From Score Matching to Diffusion: A Fine-Grained Error Analysis in the Gaussian Setting</title>
      <link>https://arxiv.org/abs/2503.11615</link>
      <description>arXiv:2503.11615v2 Announce Type: replace-cross 
Abstract: Sampling from an unknown distribution, accessible only through discrete samples, is a fundamental problem at the core of generative AI. The current state-of-the-art methods follow a two-step process: first, estimating the score function (the gradient of a smoothed log-distribution) and then applying a diffusion-based sampling algorithm -- such as Langevin or Diffusion models. The resulting distribution's correctness can be impacted by four major factors: the generalization and optimization errors in score matching, and the discretization and minimal noise amplitude in the diffusion. In this paper, we make the sampling error explicit when using a diffusion sampler in the Gaussian setting. We provide a sharp analysis of the Wasserstein sampling error that arises from these four error sources. This allows us to rigorously track how the anisotropy of the data distribution (encoded by its power spectrum) interacts with key parameters of the end-to-end sampling method, including the number of initial samples, the stepsizes in both score matching and diffusion, and the noise amplitude. Notably, we show that the Wasserstein sampling error can be expressed as a kernel-type norm of the data power spectrum, where the specific kernel depends on the method parameters. This result provides a foundation for further analysis of the tradeoffs involved in optimizing sampling accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11615v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Hurault, Matthieu Terris, Thomas Moreau, Gabriel Peyr\'e</dc:creator>
    </item>
    <item>
      <title>The Ces\`aro Value Iteration</title>
      <link>https://arxiv.org/abs/2504.04889</link>
      <description>arXiv:2504.04889v2 Announce Type: replace-cross 
Abstract: In this paper, we address the problem of undiscouted infinite-horizon optimal control for deterministic systems where the classic value iteration does not converge. For such systems, we propose to use the Ces\`aro mean to define the infinite-horizon optimal control problem and the corresponding infinite-horizon value function. Moreover, for this value function, we introduce the Ces\`aro value iteration and prove its convergence for the special case of systems with periodic optimal operating behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04889v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonas Mair, Lukas Schwenkel, Matthias A. M\"uller, Frank Allg\"ower</dc:creator>
    </item>
    <item>
      <title>Asymptotic Optimality of Projected Inventory Level Policies for Lost Sales Inventory Systems with Large Leadtime and Penalty Cost</title>
      <link>https://arxiv.org/abs/2504.10132</link>
      <description>arXiv:2504.10132v2 Announce Type: replace-cross 
Abstract: We study the canonical periodic review lost sales inventory system with positive leadtime and independent and identically distributed (i.i.d.) demand under the average cost criterion. We demonstrate that the relative value function under the constant order policy satisfies the Wiener-Hopf equation. We employ ladder processes associated with a random walk featuring i.i.d. increments, to obtain an explicit solution for the relative value function. This solution can be expressed as a quadratic form and a term that grows sublinearly. Then we perform an approximate policy iteration step on the constant order policy and bound the approximation errors as a function of the cost of losing a sale. This leads to our main result that projected inventory level policies are asymptotically optimal as the leadtime grows when the cost of losing a sale is sufficiently large and demand has a finite second moment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10132v2</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Poulad Moradi, Joachim Arts, Melvin Drent</dc:creator>
    </item>
  </channel>
</rss>
