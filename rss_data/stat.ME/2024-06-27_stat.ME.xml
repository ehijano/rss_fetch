<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Jun 2024 04:00:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 27 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Practical identifiability and parameter estimation of compartmental epidemiological models</title>
      <link>https://arxiv.org/abs/2406.17827</link>
      <description>arXiv:2406.17827v1 Announce Type: new 
Abstract: Practical parameter identifiability in ODE-based epidemiological models is a known issue, yet one that merits further study. It is essentially ubiquitous due to noise and errors in real data. In this study, to avoid uncertainty stemming from data of unknown quality, simulated data with added noise are used to investigate practical identifiability in two distinct epidemiological models. Particular emphasis is placed on the role of initial conditions, which are assumed unknown, except those that are directly measured. Instead of just focusing on one method of estimation, we use and compare results from various broadly used methods, including maximum likelihood and Markov Chain Monte Carlo (MCMC) estimation.
  Among other findings, our analysis revealed that the MCMC estimator is overall more robust than the point estimators considered. Its estimates and predictions are improved when the initial conditions of certain compartments are fixed so that the model becomes globally identifiable. For the point estimators, whether fixing or fitting the that are not directly measured improves parameter estimates is model-dependent. Specifically, in the standard SEIR model, fixing the initial condition for the susceptible population S(0) improved parameter estimates, while this was not true when fixing the initial condition of the asymptomatic population in a more involved model. Our study corroborates the change in quality of parameter estimates upon usage of pre-peak or post-peak time-series under consideration. Finally, our examples suggest that in the presence of significantly noisy data, the value of structural identifiability is moot.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17827v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Q. Y. Chen, Z. Rapti, Y. Drossinos, J. Cuevas-Maraver, G. A. Kevrekidis, P. G. Kevrekidis</dc:creator>
    </item>
    <item>
      <title>Robust integration of external control data in randomized trials</title>
      <link>https://arxiv.org/abs/2406.17971</link>
      <description>arXiv:2406.17971v1 Announce Type: new 
Abstract: One approach for increasing the efficiency of randomized trials is the use of "external controls" -- individuals who received the control treatment in the trial during routine practice or in prior experimental studies. Existing external control methods, however, can have substantial bias if the populations underlying the trial and the external control data are not exchangeable. Here, we characterize a randomization-aware class of treatment effect estimators in the population underlying the trial that remain consistent and asymptotically normal when using external control data, even when exchangeability does not hold. We consider two members of this class of estimators: the well-known augmented inverse probability weighting trial-only estimator, which is the efficient estimator when only trial data are used; and a more efficient member of the class when exchangeability holds and external control data are available, which we refer to as the optimized randomization-aware estimator. To achieve robust integration of external control data in trial analyses, we then propose a combined estimator based on the efficient trial-only estimator and the optimized randomization-aware estimator. We show that the combined estimator is consistent and no less efficient than the most efficient of the two component estimators, whether the exchangeability assumption holds or not. We examine the estimators' performance in simulations and we illustrate their use with data from two trials of paliperidone extended-release for schizophrenia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17971v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rickard Karlsson, Guanbo Wang, Jesse H. Krijthe, Issa J. Dahabreh</dc:creator>
    </item>
    <item>
      <title>Shrinkage Estimators for Beta Regression Models</title>
      <link>https://arxiv.org/abs/2406.18047</link>
      <description>arXiv:2406.18047v1 Announce Type: new 
Abstract: The beta regression model is a useful framework to model response variables that are rates or proportions, that is to say, response variables which are continuous and restricted to the interval (0,1). As with any other regression model, parameter estimates may be affected by collinearity or even perfect collinearity among the explanatory variables. To handle these situations shrinkage estimators are proposed. In particular we develop ridge regression and LASSO estimators from a penalized likelihood perspective with a logit link function. The properties of the resulting estimators are evaluated through a simulation study and a real data application</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18047v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luis Firinguetti, Manuel Gonz\'alez-Navarrete, Romer Machaca-Aguilar</dc:creator>
    </item>
    <item>
      <title>Flexible Conformal Highest Predictive Conditional Density Sets</title>
      <link>https://arxiv.org/abs/2406.18052</link>
      <description>arXiv:2406.18052v1 Announce Type: new 
Abstract: We introduce our method, conformal highest conditional density sets (CHCDS), that forms conformal prediction sets using existing estimated conditional highest density predictive regions. We prove the validity of the method and that conformal adjustment is negligible under some regularity conditions. In particular, if we correctly specify the underlying conditional density estimator, the conformal adjustment will be negligible. When the underlying model is incorrect, the conformal adjustment provides guaranteed nominal unconditional coverage. We compare the proposed method via simulation and a real data analysis to other existing methods. Our numerical results show that the flexibility of being able to use any existing conditional density estimation method is a large advantage for CHCDS compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18052v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Sampson, Kung-Sik Chan</dc:creator>
    </item>
    <item>
      <title>Errors-In-Variables Model Fitting for Partially Unpaired Data Utilizing Mixture Models</title>
      <link>https://arxiv.org/abs/2406.18154</link>
      <description>arXiv:2406.18154v1 Announce Type: new 
Abstract: The goal of this paper is to introduce a general argumentation framework for regression in the errors-in-variables regime, allowing for full flexibility about the dimensionality of the data, error probability density types, the (linear or nonlinear) model type and the avoidance of explicit definition of loss functions. Further, we introduce in this framework model fitting for partially unpaired data, i.e. for given data groups the pairing information of input and output is lost (semi-supervised). This is achieved by constructing mixture model densities, which directly model this loss of pairing information allowing for inference. In a numerical simulation study linear and nonlinear model fits are illustrated as well as a real data study is presented based on life expectancy data from the world bank utilizing a multiple linear regression model. These results allow the conclusion that high quality model fitting is possible with partially unpaired data, which opens the possibility for new applications with unfortunate or deliberate loss of pairing information in the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18154v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wolfgang Hoegele, Sarah Brockhaus</dc:creator>
    </item>
    <item>
      <title>Functional knockoffs selection with applications to functional data analysis in high dimensions</title>
      <link>https://arxiv.org/abs/2406.18189</link>
      <description>arXiv:2406.18189v1 Announce Type: new 
Abstract: The knockoffs is a recently proposed powerful framework that effectively controls the false discovery rate (FDR) for variable selection. However, none of the existing knockoff solutions are directly suited to handle multivariate or high-dimensional functional data, which has become increasingly prevalent in various scientific applications. In this paper, we propose a novel functional model-X knockoffs selection framework tailored to sparse high-dimensional functional models, and show that our proposal can achieve the effective FDR control for any sample size. Furthermore, we illustrate the proposed functional model-X knockoffs selection procedure along with the associated theoretical guarantees for both FDR control and asymptotic power using examples of commonly adopted functional linear additive regression models and the functional graphical model. In the construction of functional knockoffs, we integrate essential components including the correlation operator matrix, the Karhunen-Lo\`eve expansion, and semidefinite programming, and develop executable algorithms. We demonstrate the superiority of our proposed methods over the competitors through both extensive simulations and the analysis of two brain imaging datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18189v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinghao Qiao, Mingya Long, Qizhai Li</dc:creator>
    </item>
    <item>
      <title>Asymptotic Uncertainty in the Estimation of Frequency Domain Causal Effects for Linear Processes</title>
      <link>https://arxiv.org/abs/2406.18191</link>
      <description>arXiv:2406.18191v1 Announce Type: new 
Abstract: Structural vector autoregressive (SVAR) processes are commonly used time series models to identify and quantify causal interactions between dynamically interacting processes from observational data. The causal relationships between these processes can be effectively represented by a finite directed process graph - a graph that connects two processes whenever there is a direct delayed or simultaneous effect between them. Recent research has introduced a framework for quantifying frequency domain causal effects along paths on the process graph. This framework allows to identify how the spectral density of one process is contributing to the spectral density of another. In the current work, we characterise the asymptotic distribution of causal effect and spectral contribution estimators in terms of algebraic relations dictated by the process graph. Based on the asymptotic distribution we construct approximate confidence intervals and Wald type hypothesis tests for the estimated effects and spectral contributions. Under the assumption of causal sufficiency, we consider the class of differentiable estimators for frequency domain causal quantities, and within this class we identify the asymptotically optimal estimator. We illustrate the frequency domain Wald tests and uncertainty approximation on synthetic data, and apply them to analyse the impact of the 10 to 11 year solar cycle on the North Atlantic Oscillation (NAO). Our results confirm a significant effect of the solar cycle on the NAO at the 10 to 11 year time scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18191v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas-Domenic Reiter, Jonas Wahl, Gabriele C. Hegerl, Jakob Runge</dc:creator>
    </item>
    <item>
      <title>The $\ell$-test: leveraging sparsity in the Gaussian linear model for improved inference</title>
      <link>https://arxiv.org/abs/2406.18390</link>
      <description>arXiv:2406.18390v1 Announce Type: new 
Abstract: We develop novel LASSO-based methods for coefficient testing and confidence interval construction in the Gaussian linear model with $n\ge d$. Our methods' finite-sample guarantees are identical to those of their ubiquitous ordinary-least-squares-$t$-test-based analogues, yet have substantially higher power when the true coefficient vector is sparse. In particular, our coefficient test, which we call the $\ell$-test, performs like the one-sided $t$-test (despite not being given any information about the sign) under sparsity, and the corresponding confidence intervals are more than 10% shorter than the standard $t$-test based intervals. The nature of the $\ell$-test directly provides a novel exact adjustment conditional on LASSO selection for post-selection inference, allowing for the construction of post-selection p-values and confidence intervals. None of our methods require resampling or Monte Carlo estimation. We perform a variety of simulations and a real data analysis on an HIV drug resistance data set to demonstrate the benefits of the $\ell$-test. We end with a discussion of how the $\ell$-test may asymptotically apply to a much more general class of parametric models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18390v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Souhardya Sengupta, Lucas Janson</dc:creator>
    </item>
    <item>
      <title>An Understanding of Principal Differential Analysis</title>
      <link>https://arxiv.org/abs/2406.18484</link>
      <description>arXiv:2406.18484v1 Announce Type: new 
Abstract: In functional data analysis, replicate observations of a smooth functional process and its derivatives offer a unique opportunity to flexibly estimate continuous-time ordinary differential equation models. Ramsay (1996) first proposed to estimate a linear ordinary differential equation from functional data in a technique called Principal Differential Analysis, by formulating a functional regression in which the highest-order derivative of a function is modelled as a time-varying linear combination of its lower-order derivatives. Principal Differential Analysis was introduced as a technique for data reduction and representation, using solutions of the estimated differential equation as a basis to represent the functional data. In this work, we re-formulate PDA as a generative statistical model in which functional observations arise as solutions of a deterministic ODE that is forced by a smooth random error process. This viewpoint defines a flexible class of functional models based on differential equations and leads to an improved understanding and characterisation of the sources of variability in Principal Differential Analysis. It does, however, result in parameter estimates that can be heavily biased under the standard estimation approach of PDA. Therefore, we introduce an iterative bias-reduction algorithm that can be applied to improve parameter estimates. We also examine the utility of our approach when the form of the deterministic part of the differential equation is unknown and possibly non-linear, where Principal Differential Analysis is treated as an approximate model based on time-varying linearisation. We demonstrate our approach on simulated data from linear and non-linear differential equations and on real data from human movement biomechanics. Supplementary R code for this manuscript is available at \url{https://github.com/edwardgunning/UnderstandingOfPDAManuscript}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18484v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edward Gunning, Giles Hooker</dc:creator>
    </item>
    <item>
      <title>Concordance in basal cell carcinoma diagnosis. Building a proper ground truth to train Artificial Intelligence tools</title>
      <link>https://arxiv.org/abs/2406.18240</link>
      <description>arXiv:2406.18240v1 Announce Type: cross 
Abstract: Background: The existence of different basal cell carcinoma (BCC) clinical criteria cannot be objectively validated. An adequate ground-truth is needed to train an artificial intelligence (AI) tool that explains the BCC diagnosis by providing its dermoscopic features. Objectives: To determine the consensus among dermatologists on dermoscopic criteria of 204 BCC. To analyze the performance of an AI tool when the ground-truth is inferred. Methods: A single center, diagnostic and prospective study was conducted to analyze the agreement in dermoscopic criteria by four dermatologists and then derive a reference standard. 1434 dermoscopic images have been used, that were taken by a primary health physician, sent via teledermatology, and diagnosed by a dermatologist. They were randomly selected from the teledermatology platform (2019-2021). 204 of them were tested with an AI tool; the remainder trained it. The performance of the AI tool trained using the ground-truth of one dermatologist versus the ground-truth statistically inferred from the consensus of four dermatologists was analyzed using McNemar's test and Hamming distance. Results: Dermatologists achieve perfect agreement in the diagnosis of BCC (Fleiss-Kappa=0.9079), and a high correlation with the biopsy (PPV=0.9670). However, there is low agreement in detecting some dermoscopic criteria. Statistical differences were found in the performance of the AI tool trained using the ground-truth of one dermatologist versus the ground-truth statistically inferred from the consensus of four dermatologists. Conclusions: Care should be taken when training an AI tool to determine the BCC patterns present in a lesion. Ground-truth should be established from multiple dermatologists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18240v1</guid>
      <category>q-bio.QM</category>
      <category>cs.CV</category>
      <category>cs.IR</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francisca Silva-Claver\'ia, Carmen Serrano, Iv\'an Matas, Amalia Serrano, Tom\'as Toledo-Pastrana, David Moreno-Ram\'irez, Bego\~na Acha</dc:creator>
    </item>
    <item>
      <title>Bayesian $L_{\frac{1}{2}}$ regression</title>
      <link>https://arxiv.org/abs/2108.03464</link>
      <description>arXiv:2108.03464v2 Announce Type: replace 
Abstract: It is well known that Bridge regression enjoys superior theoretical properties when compared to traditional LASSO. However, the current latent variable representation of its Bayesian counterpart, based on the exponential power prior, is computationally expensive in higher dimensions. In this paper, we show that the exponential power prior has a closed form scale mixture of normal decomposition for $\alpha=(\frac{1}{2})^\gamma, \gamma \in \{1, 2,\ldots\}$. We call these types of priors $L_{\frac{1}{2}}$ prior for short. We develop an efficient partially collapsed Gibbs sampling scheme for computation using the $L_{\frac{1}{2}}$ prior and study theoretical properties when $p&gt;n$. In addition, we introduce a non-separable Bridge penalty function inspired by the fully Bayesian formulation and a novel, efficient coordinate descent algorithm. We prove the algorithm's convergence and show that the local minimizer from our optimisation algorithm has an oracle property. Finally, simulation studies were carried out to illustrate the performance of the new algorithms. Supplementary materials for this article are available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.03464v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiongwen Ke, Yanan Fan</dc:creator>
    </item>
    <item>
      <title>Improving estimation for asymptotically independent bivariate extremes via global estimators for the angular dependence function</title>
      <link>https://arxiv.org/abs/2303.13237</link>
      <description>arXiv:2303.13237v4 Announce Type: replace 
Abstract: Modelling the extremal dependence of bivariate variables is important in a wide variety of practical applications, including environmental planning, catastrophe modelling and hydrology. The majority of these approaches are based on the framework of bivariate regular variation, and a wide range of literature is available for estimating the dependence structure in this setting. However, such procedures are only applicable to variables exhibiting asymptotic dependence, even though asymptotic independence is often observed in practice. In this paper, we consider the so-called `angular dependence function'; this quantity summarises the extremal dependence structure for asymptotically independent variables. Until recently, only pointwise estimators of the angular dependence function have been available. We introduce a range of global estimators and compare them to another recently introduced technique for global estimation through a systematic simulation study, and a case study on river flow data from the north of England, UK.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.13237v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>C. J. R. Murphy-Barltrop, J. L. Wadsworth, E. F. Eastoe</dc:creator>
    </item>
    <item>
      <title>A Meta-Learning Method for Estimation of Causal Excursion Effects to Assess Time-Varying Moderation</title>
      <link>https://arxiv.org/abs/2306.16297</link>
      <description>arXiv:2306.16297v2 Announce Type: replace 
Abstract: Twin revolutions in wearable technologies and health interventions delivered by smartphones have greatly increased the accessibility of mobile health (mHealth) interventions. Micro-randomized trials (MRTs) are designed to assess the effectiveness of the mHealth intervention and introduce a novel class of causal estimands called "causal excursion effects." These estimands enable the evaluation of how intervention effects change over time and are influenced by individual characteristics or context. However, existing analysis methods for causal excursion effects require prespecified features of the observed high-dimensional history to build a working model for a critical nuisance parameter. Machine learning appears ideal for automatic feature construction, but their naive application can lead to bias under model misspecification. To address this issue, this paper revisits the estimation of causal excursion effects from a meta-learner perspective, where the analyst remains agnostic to the supervised learning algorithms used to estimate nuisance parameters. We present the bidirectional asymptotic properties of the proposed estimators and compare them both theoretically and through extensive simulations. The results show relative efficiency gains and support the suggestion of a doubly robust alternative to existing methods. Finally, the proposed methods' practical utilities are demonstrated by analyzing data from a multi-institution cohort of first-year medical residents in the United States (NeCamp et al., 2020).</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16297v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jieru Shi, Walter Dempsey</dc:creator>
    </item>
    <item>
      <title>Simultaneous Modeling of Disease Screening and Severity Prediction: A Multi-task and Sparse Regularization Approach</title>
      <link>https://arxiv.org/abs/2309.04685</link>
      <description>arXiv:2309.04685v2 Announce Type: replace 
Abstract: The exploration of biomarkers, which are clinically useful biomolecules, and the development of prediction models using them are important problems in biomedical research. Biomarkers are widely used for disease screening, and some are related not only to the presence or absence of a disease but also to its severity. These biomarkers can be useful for prioritization of treatment and clinical decision-making. Considering a model helpful for both disease screening and severity prediction, this paper focuses on regression modeling for an ordinal response equipped with a hierarchical structure.
  If the response variable is a combination of the presence of disease and severity such as \{{\it healthy, mild, intermediate, severe}\}, for example, the simplest method would be to apply the conventional ordinal regression model. However, the conventional model has flexibility issues and may not be suitable for the problems addressed in this paper, where the levels of the response variable might be heterogeneous. Therefore, this paper proposes a model assuming screening and severity prediction as different tasks, and an estimation method based on structural sparse regularization that leverages any common structure between the tasks when such commonality exists. In numerical experiments, the proposed method demonstrated stable performance across many scenarios compared to existing ordinal regression methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.04685v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kazuharu Harada, Shuichi Kawano, Masataka Taguri</dc:creator>
    </item>
    <item>
      <title>Hierarchical Causal Models</title>
      <link>https://arxiv.org/abs/2401.05330</link>
      <description>arXiv:2401.05330v2 Announce Type: replace 
Abstract: Scientists often want to learn about cause and effect from hierarchical data, collected from subunits nested inside units. Consider students in schools, cells in patients, or cities in states. In such settings, unit-level variables (e.g. each school's budget) may affect subunit-level variables (e.g. the test scores of each student in each school) and vice versa. To address causal questions with hierarchical data, we propose hierarchical causal models, which extend structural causal models and causal graphical models by adding inner plates. We develop a general graphical identification technique for hierarchical causal models that extends do-calculus. We find many situations in which hierarchical data can enable causal identification even when it would be impossible with non-hierarchical data, that is, if we had only unit-level summaries of subunit-level variables (e.g. the school's average test score, rather than each student's score). We develop estimation techniques for hierarchical causal models, using methods including hierarchical Bayesian models. We illustrate our results in simulation and via a reanalysis of the classic "eight schools" study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.05330v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Eli N. Weinstein, David M. Blei</dc:creator>
    </item>
    <item>
      <title>Robustly estimating heterogeneity in factorial data using Rashomon Partitions</title>
      <link>https://arxiv.org/abs/2404.02141</link>
      <description>arXiv:2404.02141v2 Announce Type: replace 
Abstract: Many statistical analyses, in both observational data and randomized control trials, ask: how does the outcome of interest vary with combinations of observable covariates? How do various drug combinations affect health outcomes, or how does technology adoption depend on incentives and demographics? Our goal is to partition this factorial space into "pools" of covariate combinations where the outcome differs across the pools (but not within a pool). Existing approaches (i) search for a single "optimal" partition under assumptions about the association between covariates or (ii) sample from the entire set of possible partitions. Both these approaches ignore the reality that, especially with correlation structure in covariates, many ways to partition the covariate space may be statistically indistinguishable, despite very different implications for policy or science. We develop an alternative perspective, called Rashomon Partition Sets (RPSs). Each item in the RPS partitions the space of covariates using a tree-like geometry. RPSs incorporate all partitions that have posterior values near the maximum a posteriori partition, even if they offer substantively different explanations, and do so using a prior that makes no assumptions about associations between covariates. This prior is the $\ell_0$ prior, which we show is minimax optimal. Given the RPS we calculate the posterior of any measurable function of the feature effects vector on outcomes, conditional on being in the RPS. We also characterize approximation error relative to the entire posterior and provide bounds on the size of the RPS. Simulations demonstrate this framework allows for robust conclusions relative to conventional regularization techniques. We apply our method to three empirical settings: price effects on charitable giving, chromosomal structure (telomere length), and the introduction of microfinance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02141v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aparajithan Venkateswaran, Anirudh Sankar, Arun G. Chandrasekhar, Tyler H. McCormick</dc:creator>
    </item>
    <item>
      <title>Neural Optimization with Adaptive Heuristics for Intelligent Marketing System</title>
      <link>https://arxiv.org/abs/2405.10490</link>
      <description>arXiv:2405.10490v3 Announce Type: replace 
Abstract: Computational marketing has become increasingly important in today's digital world, facing challenges such as massive heterogeneous data, multi-channel customer journeys, and limited marketing budgets. In this paper, we propose a general framework for marketing AI systems, the Neural Optimization with Adaptive Heuristics (NOAH) framework. NOAH is the first general framework for marketing optimization that considers both to-business (2B) and to-consumer (2C) products, as well as both owned and paid channels. We describe key modules of the NOAH framework, including prediction, optimization, and adaptive heuristics, providing examples for bidding and content optimization. We then detail the successful application of NOAH to LinkedIn's email marketing system, showcasing significant wins over the legacy ranking system. Additionally, we share details and insights that are broadly useful, particularly on: (i) addressing delayed feedback with lifetime value, (ii) performing large-scale linear programming with randomization, (iii) improving retrieval with audience expansion, (iv) reducing signal dilution in targeting tests, and (v) handling zero-inflated heavy-tail metrics in statistical testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10490v3</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3637528.3671591</arxiv:DOI>
      <dc:creator>Changshuai Wei, Benjamin Zelditch, Joyce Chen, Andre Assuncao Silva T Ribeiro, Jingyi Kenneth Tay, Borja Ocejo Elizondo, Keerthi Selvaraj, Aman Gupta, Licurgo Benemann De Almeida</dc:creator>
    </item>
    <item>
      <title>STEEL: Singularity-aware Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2301.13152</link>
      <description>arXiv:2301.13152v5 Announce Type: replace-cross 
Abstract: Batch reinforcement learning (RL) aims at leveraging pre-collected data to find an optimal policy that maximizes the expected total rewards in a dynamic environment. The existing methods require absolutely continuous assumption (e.g., there do not exist non-overlapping regions) on the distribution induced by target policies with respect to the data distribution over either the state or action or both. We propose a new batch RL algorithm that allows for singularity for both state and action spaces (e.g., existence of non-overlapping regions between offline data distribution and the distribution induced by the target policies) in the setting of an infinite-horizon Markov decision process with continuous states and actions. We call our algorithm STEEL: SingulariTy-awarE rEinforcement Learning. Our algorithm is motivated by a new error analysis on off-policy evaluation, where we use maximum mean discrepancy, together with distributionally robust optimization, to characterize the error of off-policy evaluation caused by the possible singularity and to enable model extrapolation. By leveraging the idea of pessimism and under some technical conditions, we derive a first finite-sample regret guarantee for our proposed algorithm under singularity. Compared with existing algorithms,by requiring only minimal data-coverage assumption, STEEL improves the applicability and robustness of batch RL. In addition, a two-step adaptive STEEL, which is nearly tuning-free, is proposed. Extensive simulation studies and one (semi)-real experiment on personalized pricing demonstrate the superior performance of our methods in dealing with possible singularity in batch RL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.13152v5</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaohong Chen, Zhengling Qi, Runzhe Wan</dc:creator>
    </item>
    <item>
      <title>Sharp variance estimator and causal bootstrap in stratified randomized experiments</title>
      <link>https://arxiv.org/abs/2401.16667</link>
      <description>arXiv:2401.16667v2 Announce Type: replace-cross 
Abstract: The design-based finite-population asymptotic theory provides a normal approximation for the sampling distribution of the average treatment effect estimator in stratified randomized experiments. The asymptotic variance could be estimated by a Neyman-type conservative variance estimator. However, the variance estimator can be overly conservative, and the asymptotic theory may fail in small samples. To solve these issues, we propose a sharp variance estimator for the weighted difference-in-means in stratified randomized experiments. Furthermore, we propose two causal bootstrap procedures to more accurately approximate the sampling distribution of the weighted difference-in-means estimator. The first causal bootstrap procedure is based on rank-preserving imputation and we prove its second-order refinement over normal approximation. The second causal bootstrap procedure is based on constant-treatment-effect imputation and is applicable in paired experiments. We prove its validity even when the assumption of constant treatment effect is violated for the true potential outcomes. Our analysis is randomization-based or design-based by conditioning on the potential outcomes, with treatment assignment being the sole source of randomness. Numerical studies and two real data applications demonstrate advantages of our proposed methods in finite samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16667v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyang Yu, Ke Zhu, Hanzhong Liu</dc:creator>
    </item>
  </channel>
</rss>
