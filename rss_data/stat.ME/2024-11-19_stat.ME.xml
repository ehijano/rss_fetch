<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 20 Nov 2024 02:48:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Efficient inference for differential equation models without numerical solvers</title>
      <link>https://arxiv.org/abs/2411.10494</link>
      <description>arXiv:2411.10494v2 Announce Type: new 
Abstract: Parameter inference is essential when interpreting observational data using mathematical models. Standard inference methods for differential equation models typically rely on obtaining repeated numerical solutions of the differential equation(s). Recent results have explored how numerical truncation error can have major, detrimental, and sometimes hidden impacts on likelihood-based inference by introducing false local maxima into the log-likelihood function. We present a straightforward approach for inference that eliminates the need for solving the underlying differential equations, thereby completely avoiding the impact of truncation error. Open-access Jupyter notebooks, available on GitHub, allow others to implement this method for a broad class of widely-used models to interpret biological data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10494v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Johnston, Oliver J. Maclaren, Ruth E. Baker, Matthew J. Simpson</dc:creator>
    </item>
    <item>
      <title>Inference for overparametrized hierarchical Archimedean copulas</title>
      <link>https://arxiv.org/abs/2411.10615</link>
      <description>arXiv:2411.10615v1 Announce Type: new 
Abstract: Hierarchical Archimedean copulas (HACs) are multivariate uniform distributions constructed by nesting Archimedean copulas into one another, and provide a flexible approach to modeling non-exchangeable data. However, this flexibility in the model structure may lead to over-fitting when the model estimation procedure is not performed properly. In this paper, we examine the problem of structure estimation and more generally on the selection of a parsimonious model from the hypothesis testing perspective. Formal tests for structural hypotheses concerning HACs have been lacking so far, most likely due to the restrictions on their associated parameter space which hinders the use of standard inference methodology. Building on previously developed asymptotic methods for these non-standard parameter spaces, we provide an asymptotic stochastic representation for the maximum likelihood estimators of (potentially) overparametrized HACs, which we then use to formulate a likelihood ratio test for certain common structural hypotheses. Additionally, we also derive analytical expressions for the first- and second-order partial derivatives of two-level HACs based on Clayton and Gumbel generators, as well as general numerical approximation schemes for the Fisher information matrix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10615v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Perreault, Yanbo Tang, Ruyi Pan, Nancy Reid</dc:creator>
    </item>
    <item>
      <title>Doubly Robust Estimation of Causal Excursion Effects in Micro-Randomized Trials with Missing Longitudinal Outcomes</title>
      <link>https://arxiv.org/abs/2411.10620</link>
      <description>arXiv:2411.10620v1 Announce Type: new 
Abstract: Micro-randomized trials (MRTs) are increasingly utilized for optimizing mobile health interventions, with the causal excursion effect (CEE) as a central quantity for evaluating interventions under policies that deviate from the experimental policy. However, MRT often contains missing data due to reasons such as missed self-reports or participants not wearing sensors, which can bias CEE estimation. In this paper, we propose a two-stage, doubly robust estimator for CEE in MRTs when longitudinal outcomes are missing at random, accommodating continuous, binary, and count outcomes. Our two-stage approach allows for both parametric and nonparametric modeling options for two nuisance parameters: the missingness model and the outcome regression. We demonstrate that our estimator is doubly robust, achieving consistency and asymptotic normality if either the missingness or the outcome regression model is correctly specified. Simulation studies further validate the estimator's desirable finite-sample performance. We apply the method to HeartSteps, an MRT for developing mobile health interventions that promote physical activity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10620v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaxin Yu, Tianchen Qian</dc:creator>
    </item>
    <item>
      <title>Sensitivity Analysis for Observational Studies with Flexible Matched Designs</title>
      <link>https://arxiv.org/abs/2411.10623</link>
      <description>arXiv:2411.10623v1 Announce Type: new 
Abstract: Observational studies provide invaluable opportunities to draw causal inference, but they may suffer from biases due to pretreatment difference between treated and control units. Matching is a popular approach to reduce observed covariate imbalance. To tackle unmeasured confounding, a sensitivity analysis is often conducted to investigate how robust a causal conclusion is to the strength of unmeasured confounding. For matched observational studies, Rosenbaum proposed a sensitivity analysis framework that uses the randomization of treatment assignments as the "reasoned basis" and imposes no model assumptions on the potential outcomes as well as their dependence on the observed and unobserved confounding factors. However, this otherwise appealing framework requires exact matching to guarantee its validity, which is hard to achieve in practice. In this paper we provide an alternative inferential framework that shares the same procedure as Rosenbaum's approach but relies on a different justification. Our framework allows flexible matching algorithms and utilizes alternative source of randomness, in particular random permutations of potential outcomes instead of treatment assignments, to guarantee statistical validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10623v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinran Li</dc:creator>
    </item>
    <item>
      <title>False Discovery Control in Multiple Testing: A Brief Overview of Theories and Methodologies</title>
      <link>https://arxiv.org/abs/2411.10647</link>
      <description>arXiv:2411.10647v1 Announce Type: new 
Abstract: As the volume and complexity of data continue to expand across various scientific disciplines, the need for robust methods to account for the multiplicity of comparisons has grown widespread. A popular measure of type 1 error rate in multiple testing literature is the false discovery rate (FDR). The FDR provides a powerful and practical approach to large-scale multiple testing and has been successfully used in a wide range of applications. The concept of FDR has gained wide acceptance in the statistical community and various methods has been proposed to control the FDR. In this work, we review the latest developments in FDR control methodologies. We also develop a conceptual framework to better describe this vast literature; understand its intuition and key ideas; and provide guidance for the researcher interested in both the application and development of the methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10647v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jianliang He, Bowen Gang, Luella Fu</dc:creator>
    </item>
    <item>
      <title>Subsampling-based Tests in Mediation Analysis</title>
      <link>https://arxiv.org/abs/2411.10648</link>
      <description>arXiv:2411.10648v2 Announce Type: new 
Abstract: Testing for mediation effect poses a challenge since the null hypothesis (i.e., the absence of mediation effects) is composite, making most existing mediation tests quite conservative and often underpowered. In this work, we propose a subsampling-based procedure to construct a test statistic whose asymptotic null distribution is pivotal and remains the same regardless of the three null cases encountered in mediation analysis. The method, when combined with the popular Sobel test, leads to an accurate size control under the null. We further introduce a Cauchy combination test to construct p-values from different subsample splits, which reduces variability in the testing results and increases detection power. Through numerical studies, our approach has demonstrated a more accurate size and higher detection power than the competing classical and contemporary methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10648v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Asmita Roy, Huijuan Zhou, Ni Zhao, Xianyang Zhang</dc:creator>
    </item>
    <item>
      <title>Improving Causal Estimation by Mixing Samples to Address Weak Overlap in Observational Studies</title>
      <link>https://arxiv.org/abs/2411.10801</link>
      <description>arXiv:2411.10801v1 Announce Type: new 
Abstract: Sufficient overlap of propensity scores is one of the most critical assumptions in observational studies. In this article, we will cover the severity in statistical inference under such assumption failure with weighting, one of the most dominating causal inference methodologies. Then we propose a simple, yet novel remedy: "mixing" the treated and control groups in the observed dataset. We state that our strategy has three key advantages: (1) Improvement in estimators' accuracy especially in weak overlap, (2) Identical targeting population of treatment effect, (3) High flexibility. We introduce a property of mixed sample that offers a safer inference by implementing onto both traditional and modern weighting methods. We illustrate this with several extensive simulation studies and guide the readers with a real-data analysis for practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10801v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaehyuk Jang, Suehyun Kim, Kwonsang Lee</dc:creator>
    </item>
    <item>
      <title>Scalable Gaussian Process Regression Via Median Posterior Inference for Estimating Multi-Pollutant Mixture Health Effects</title>
      <link>https://arxiv.org/abs/2411.10858</link>
      <description>arXiv:2411.10858v1 Announce Type: new 
Abstract: Humans are exposed to complex mixtures of environmental pollutants rather than single chemicals, necessitating methods to quantify the health effects of such mixtures. Research on environmental mixtures provides insights into realistic exposure scenarios, informing regulatory policies that better protect public health. However, statistical challenges, including complex correlations among pollutants and nonlinear multivariate exposure-response relationships, complicate such analyses. A popular Bayesian semi-parametric Gaussian process regression framework (Coull et al., 2015) addresses these challenges by modeling exposure-response functions with Gaussian processes and performing feature selection to manage high-dimensional exposures while accounting for confounders. Originally designed for small to moderate-sized cohort studies, this framework does not scale well to massive datasets. To address this, we propose a divide-and-conquer strategy, partitioning data, computing posterior distributions in parallel, and combining results using the generalized median. While we focus on Gaussian process models for environmental mixtures, the proposed distributed computing strategy is broadly applicable to other Bayesian models with computationally prohibitive full-sample Markov Chain Monte Carlo fitting. We provide theoretical guarantees for the convergence of the proposed posterior distributions to those derived from the full sample. We apply this method to estimate associations between a mixture of ambient air pollutants and ~650,000 birthweights recorded in Massachusetts during 2001-2012. Our results reveal negative associations between birthweight and traffic pollution markers, including elemental and organic carbon and PM2.5, and positive associations with ozone and vegetation greenness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10858v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaron Sonabend, Jiangshan Zhang, Joel Schwartz, Brent A. Coull, Junwei Lu</dc:creator>
    </item>
    <item>
      <title>The Conflict Graph Design: Estimating Causal Effects under Arbitrary Neighborhood Interference</title>
      <link>https://arxiv.org/abs/2411.10908</link>
      <description>arXiv:2411.10908v1 Announce Type: new 
Abstract: A fundamental problem in network experiments is selecting an appropriate experimental design in order to precisely estimate a given causal effect of interest. In fact, optimal rates of estimation remain unknown for essentially all causal effects in network experiments. In this work, we propose a general approach for constructing experiment designs under network interference with the goal of precisely estimating a pre-specified causal effect. A central aspect of our approach is the notion of a conflict graph, which captures the fundamental unobservability associated with the casual effect and the underlying network. We refer to our experimental design as the Conflict Graph Design. In order to estimate effects, we propose a modified Horvitz--Thompson estimator. We show that its variance under the Conflict Graph Design is bounded as $O(\lambda(H) / n )$, where $\lambda(H)$ is the largest eigenvalue of the adjacency matrix of the conflict graph. These rates depend on both the underlying network and the particular causal effect under investigation. Not only does this yield the best known rates of estimation for several well-studied causal effects (e.g. the global and direct effects) but it also provides new methods for effects which have received less attention from the perspective of experiment design (e.g. spill-over effects). Our results corroborate two implicitly understood points in the literature: (1) that in order to increase precision, experiment designs should be tailored to specific causal effects of interest and (2) that "more local" effects are easier to estimate than "more global" effects. In addition to point estimation, we construct conservative variance estimators which facilitate the construction of asymptotically valid confidence intervals for the casual effect of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10908v1</guid>
      <category>stat.ME</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vardis Kandiros, Charilaos Pipis, Constantinos Daskalakis, Christopher Harshaw</dc:creator>
    </item>
    <item>
      <title>A novel density-based approach for estimating unknown means, distribution visualisations and meta-analyses of quantiles</title>
      <link>https://arxiv.org/abs/2411.10971</link>
      <description>arXiv:2411.10971v1 Announce Type: new 
Abstract: In meta-analysis with continuous outcomes, the use of effect sizes based on the means is the most common. It is often found, however, that only the quantile summary measures are reported in some studies, and in certain scenarios, a meta-analysis of the quantiles themselves are of interest. We propose a novel density-based approach to support the implementation of a comprehensive meta-analysis, when only the quantile summary measures are reported. The proposed approach uses flexible quantile-based distributions and percentile matching to estimate the unknown parameters without making any prior assumptions about the underlying distributions. Using simulated and real data, we show that the proposed novel density-based approach works as well as or better than the widely-used methods in estimating the means using quantile summaries without assuming a distribution apriori, and provides a novel tool for distribution visualisations. In addition to this, we introduce quantile-based meta-analysis methods for situations where a comparison of quantiles between groups themselves are of interest and found to be more suitable. Using both real and simulated data, we also demonstrate the applicability of these quantile-based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10971v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alysha M De Livera, Luke Prendergast, Udara Kumaranathunga</dc:creator>
    </item>
    <item>
      <title>A joint modeling approach to treatment effects estimation with unmeasured confounders</title>
      <link>https://arxiv.org/abs/2411.10980</link>
      <description>arXiv:2411.10980v1 Announce Type: new 
Abstract: Estimating treatment effects using observation data often relies on the assumption of no unmeasured confounders. However, unmeasured confounding variables may exist in many real-world problems. It can lead to a biased estimation without incorporating the unmeasured confounding effect. To address this problem, this paper proposes a new mixed-effects joint modeling approach to identifying and estimating the OR function and the PS function in the presence of unmeasured confounders in longitudinal data settings. As a result, we can obtain the estimators of the average treatment effect and heterogeneous treatment effects. In our proposed setting, we allow interaction effects of the treatment and unmeasured confounders on the outcome. Moreover, we propose a new Laplacian-variant EM algorithm to estimate the parameters in the joint models. We apply the method to a real-world application from the CitieS-Health Barcelona Panel Study, in which we study the effect of short-term air pollution exposure on mental health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10980v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Namhwa Lee, Shujie Ma</dc:creator>
    </item>
    <item>
      <title>A model-free test of the time-reversibility of climate change processes</title>
      <link>https://arxiv.org/abs/2411.11248</link>
      <description>arXiv:2411.11248v1 Announce Type: new 
Abstract: Time-reversibility is a crucial feature of many time series models, while time-irreversibility is the rule rather than the exception in real-life data. Testing the null hypothesis of time-reversibilty, therefore, should be an important step preliminary to the identification and estimation of most traditional time-series models. Existing procedures, however, mostly consist of testing necessary but not sufficient conditions, leading to under-rejection, or sufficient but non-necessary ones, which leads to over-rejection. Moreover, they generally are model-besed. In contrast, the copula spectrum studied by Goto et al. ($\textit{Ann. Statist.}$ 2022, $\textbf{50}$: 3563--3591) allows for a model-free necessary and sufficient time-reversibility condition. A test based on this copula-spectrum-based characterization has been proposed by authors. This paper illustrates the performance of this test, with an illustration in the analysis of climatic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11248v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuichi Goto, Marc Hallin</dc:creator>
    </item>
    <item>
      <title>Unbiased Approximations for Stationary Distributions of McKean-Vlasov SDEs</title>
      <link>https://arxiv.org/abs/2411.11270</link>
      <description>arXiv:2411.11270v1 Announce Type: new 
Abstract: We consider the development of unbiased estimators, to approximate the stationary distribution of Mckean-Vlasov stochastic differential equations (MVSDEs). These are an important class of processes, which frequently appear in applications such as mathematical finance, biology and opinion dynamics. Typically the stationary distribution is unknown and indeed one cannot simulate such processes exactly. As a result one commonly requires a time-discretization scheme which results in a discretization bias and a bias from not being able to simulate the associated stationary distribution. To overcome this bias, we present a new unbiased estimator taking motivation from the literature on unbiased Monte Carlo. We prove the unbiasedness of our estimator, under assumptions. In order to prove this we require developing ergodicity results of various discrete time processes, through an appropriate discretization scheme, towards the invariant measure. Numerous numerical experiments are provided, on a range of MVSDEs, to demonstrate the effectiveness of our unbiased estimator. Such examples include the Currie-Weiss model, a 3D neuroscience model and a parameter estimation problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11270v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Elsiddig Awadelkarim, Neil K. Chada, Ajay Jasra</dc:creator>
    </item>
    <item>
      <title>Subgroup analysis in multi level hierarchical cluster randomized trials</title>
      <link>https://arxiv.org/abs/2411.11301</link>
      <description>arXiv:2411.11301v1 Announce Type: new 
Abstract: Cluster or group randomized trials (CRTs) are increasingly used for both behavioral and system-level interventions, where entire clusters are randomly assigned to a study condition or intervention. Apart from the assigned cluster-level analysis, investigating whether an intervention has a differential effect for specific subgroups remains an important issue, though it is often considered an afterthought in pivotal clinical trials. Determining such subgroup effects in a CRT is a challenging task due to its inherent nested cluster structure. Motivated by a real-life HIV prevention CRT, we consider a three-level cross-sectional CRT, where randomization is carried out at the highest level and subgroups may exist at different levels of the hierarchy. We employ a linear mixed-effects model to estimate the subgroup-specific effects through their maximum likelihood estimators (MLEs). Consequently, we develop a consistent test for the significance of the differential intervention effect between two subgroups at different levels of the hierarchy, which is the key methodological contribution of this work. We also derive explicit formulae for sample size determination to detect a differential intervention effect between two subgroups, aiming to achieve a given statistical power in the case of a planned confirmatory subgroup analysis. The application of our methodology is illustrated through extensive simulation studies using synthetic data, as well as with real-world data from an HIV prevention CRT in The Bahamas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11301v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shubhadeep Chakraborty, Bo Wang, Ram Tiwari, Samiran Ghosh</dc:creator>
    </item>
    <item>
      <title>Efficient smoothness selection for nonparametric Markov-switching models via quasi restricted maximum likelihood</title>
      <link>https://arxiv.org/abs/2411.11498</link>
      <description>arXiv:2411.11498v1 Announce Type: new 
Abstract: Markov-switching models are powerful tools that allow capturing complex patterns from time series data driven by latent states. Recent work has highlighted the benefits of estimating components of these models nonparametrically, enhancing their flexibility and reducing biases, which in turn can improve state decoding, forecasting, and overall inference. Formulating such models using penalized splines is straightforward, but practically feasible methods for a data-driven smoothness selection in these models are still lacking. Traditional techniques, such as cross-validation and information criteria-based selection suffer from major drawbacks, most importantly their reliance on computationally expensive grid search methods, hampering practical usability for Markov-switching models. Michelot (2022) suggested treating spline coefficients as random effects with a multivariate normal distribution and using the R package TMB (Kristensen et al., 2016) for marginal likelihood maximization. While this method avoids grid search and typically results in adequate smoothness selection, it entails a nested optimization problem, thus being computationally demanding. We propose to exploit the simple structure of penalized splines treated as random effects, thereby greatly reducing the computational burden while potentially improving fixed effects parameter estimation accuracy. Our proposed method offers a reliable and efficient mechanism for smoothness selection, rendering the estimation of Markov-switching models involving penalized splines feasible for complex data structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11498v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan-Ole Koslik</dc:creator>
    </item>
    <item>
      <title>Metric Oja Depth, New Statistical Tool for Estimating the Most Central Objects</title>
      <link>https://arxiv.org/abs/2411.11580</link>
      <description>arXiv:2411.11580v1 Announce Type: new 
Abstract: The Oja depth (simplicial volume depth) is one of the classical statistical techniques for measuring the central tendency of data in multivariate space. Despite the widespread emergence of object data like images, texts, matrices or graphs, a well-developed and suitable version of Oja depth for object data is lacking. To address this shortcoming, in this study we propose a novel measure of statistical depth, the metric Oja depth applicable to any object data. Then, we develop two competing strategies for optimizing metric depth functions, i.e., finding the deepest objects with respect to them. Finally, we compare the performance of the metric Oja depth with three other depth functions (half-space, lens, and spatial) in diverse data scenarios.
  Keywords: Object Data, Metric Oja depth, Statistical depth, Optimization, Genetic algorithm, Metric statistics</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11580v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vida Zamanifarizhandi, Joni Virta</dc:creator>
    </item>
    <item>
      <title>Nonparametric Bayesian approach for dynamic borrowing of historical control data</title>
      <link>https://arxiv.org/abs/2411.11675</link>
      <description>arXiv:2411.11675v1 Announce Type: new 
Abstract: When incorporating historical control data into the analysis of current randomized controlled trial data, it is critical to account for differences between the datasets. When the cause of the difference is an unmeasured factor and adjustment for observed covariates only is insufficient, it is desirable to use a dynamic borrowing method that reduces the impact of heterogeneous historical controls. We propose a nonparametric Bayesian approach for borrowing historical controls that are homogeneous with the current control. Additionally, to emphasize the resolution of conflicts between the historical controls and current control, we introduce a method based on the dependent Dirichlet process mixture. The proposed methods can be implemented using the same procedure, regardless of whether the outcome data comprise aggregated study-level data or individual participant data. We also develop a novel index of similarity between the historical and current control data, based on the posterior distribution of the parameter of interest. We conduct a simulation study and analyze clinical trial examples to evaluate the performance of the proposed methods compared to existing methods. The proposed method based on the dependent Dirichlet process mixture can more accurately borrow from homogeneous historical controls while reducing the impact of heterogeneous historical controls compared to the typical Dirichlet process mixture. The proposed methods outperform existing methods in scenarios with heterogeneous historical controls, in which the meta-analytic approach is ineffective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11675v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomohiro Ohigashi, Kazushi Maruo, Takashi Sozu, Masahiko Gosho</dc:creator>
    </item>
    <item>
      <title>Davis-Kahan Theorem in the two-to-infinity norm and its application to perfect clustering</title>
      <link>https://arxiv.org/abs/2411.11728</link>
      <description>arXiv:2411.11728v1 Announce Type: new 
Abstract: Many statistical applications, such as the Principal Component Analysis, matrix completion, tensor regression and many others, rely on accurate estimation of leading eigenvectors of a matrix. The Davis-Khan theorem is known to be instrumental for bounding above the distances between matrices $U$ and $\widehat{U}$ of population eigenvectors and their sample versions. While those distances can be measured in various metrics, the recent developments showed advantages of evaluation of the deviation in the 2-to-infinity norm. The purpose of this paper is to provide upper bounds for the distances between $U$ and $\widehat{U}$ in the two-to-infinity norm for a variety of possible scenarios and competitive approaches. Although this problem has been studied by several authors, the difference between this paper and its predecessors is that the upper bounds are obtained with no or mild probabilistic assumptions on the error distributions. Those bounds are subsequently refined, when some generic probabilistic assumptions on the errors hold. In addition, the paper provides alternative methods for evaluation of $\widehat{U}$ and, therefore, enables one to compare the resulting accuracies. As an example of an application of the results in the paper, we derive sufficient conditions for perfect clustering in a generic setting, and then employ them in various scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11728v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marianna Pensky</dc:creator>
    </item>
    <item>
      <title>Randomization-based Z-estimation for evaluating average and individual treatment effects</title>
      <link>https://arxiv.org/abs/2411.11737</link>
      <description>arXiv:2411.11737v1 Announce Type: new 
Abstract: Randomized experiments have been the gold standard for drawing causal inference. The conventional model-based approach has been one of the most popular ways for analyzing treatment effects from randomized experiments, which is often carried through inference for certain model parameters. In this paper, we provide a systematic investigation of model-based analyses for treatment effects under the randomization-based inference framework. This framework does not impose any distributional assumptions on the outcomes, covariates and their dependence, and utilizes only randomization as the "reasoned basis". We first derive the asymptotic theory for Z-estimation in completely randomized experiments, and propose sandwich-type conservative covariance estimation. We then apply the developed theory to analyze both average and individual treatment effects in randomized experiments. For the average treatment effect, we consider three estimation strategies: model-based, model-imputed, and model-assisted, where the first two can be sensitive to model misspecification or require specific ways for parameter estimation. The model-assisted approach is robust to arbitrary model misspecification and always provides consistent average treatment effect estimation. We propose optimal ways to conduct model-assisted estimation using generally nonlinear least squares for parameter estimation. For the individual treatment effects, we propose to directly model the relationship between individual effects and covariates, and discuss the model's identifiability, inference and interpretation allowing model misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11737v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyi Qu, Jiangchuan Du, Xinran Li</dc:creator>
    </item>
    <item>
      <title>Monetary Incentives, Landowner Preferences: Estimating Cross-Elasticities in Farmland Conversion to Renewable Energy</title>
      <link>https://arxiv.org/abs/2411.10600</link>
      <description>arXiv:2411.10600v1 Announce Type: cross 
Abstract: This study examines the impact of monetary factors on the conversion of farmland to renewable energy generation, specifically solar and wind, in the context of expanding U.S. energy production. We propose a new econometric method that accounts for the diverse circumstances of landowners, including their unordered alternative land use options, non-monetary benefits from farming, and the influence of local regulations. We demonstrate that identifying the cross elasticity of landowners' farming income in relation to the conversion of farmland to renewable energy requires an understanding of their preferences. By utilizing county legislation that we assume to be shaped by land-use preferences, we estimate the cross-elasticities of farming income. Our findings indicate that monetary incentives may only influence landowners' decisions in areas with potential for future residential development, underscoring the importance of considering both preferences and regulatory contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10600v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chad Fiechter, Binayak Kunwar, Guy Tchuente</dc:creator>
    </item>
    <item>
      <title>Wasserstein Spatial Depth</title>
      <link>https://arxiv.org/abs/2411.10646</link>
      <description>arXiv:2411.10646v1 Announce Type: cross 
Abstract: Modeling observations as random distributions embedded within Wasserstein spaces is becoming increasingly popular across scientific fields, as it captures the variability and geometric structure of the data more effectively. However, the distinct geometry and unique properties of Wasserstein space pose challenges to the application of conventional statistical tools, which are primarily designed for Euclidean spaces. Consequently, adapting and developing new methodologies for analysis within Wasserstein spaces has become essential. The space of distributions on $\mathbb{R}^d$ with $d&gt;1$ is not linear, and ''mimic'' the geometry of a Riemannian manifold. In this paper, we extend the concept of statistical depth to distribution-valued data, introducing the notion of {\it Wasserstein spatial depth}. This new measure provides a way to rank and order distributions, enabling the development of order-based clustering techniques and inferential tools. We show that Wasserstein spatial depth (WSD) preserves critical properties of conventional statistical depths, notably, ranging within $[0,1]$, transformation invariance, vanishing at infinity, reaching a maximum at the geometric median, and continuity. Additionally, the population WSD has a straightforward plug-in estimator based on sampled empirical distributions. We establish the estimator's consistency and asymptotic normality. Extensive simulation and real-data application showcase the practical efficacy of WSD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10646v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Bachoc, Alberto Gonz\'alez-Sanz, Jean-Michel Loubes, Yisha Yao</dc:creator>
    </item>
    <item>
      <title>Program Evaluation with Remotely Sensed Outcomes</title>
      <link>https://arxiv.org/abs/2411.10959</link>
      <description>arXiv:2411.10959v1 Announce Type: cross 
Abstract: While traditional program evaluations typically rely on surveys to measure outcomes, certain economic outcomes such as living standards or environmental quality may be infeasible or costly to collect. As a result, recent empirical work estimates treatment effects using remotely sensed variables (RSVs), such mobile phone activity or satellite images, instead of ground-truth outcome measurements. Common practice predicts the economic outcome from the RSV, using an auxiliary sample of labeled RSVs, and then uses such predictions as the outcome in the experiment. We prove that this approach leads to biased estimates of treatment effects when the RSV is a post-outcome variable. We nonparametrically identify the treatment effect, using an assumption that reflects the logic of recent empirical research: the conditional distribution of the RSV remains stable across both samples, given the outcome and treatment. Our results do not require researchers to know or consistently estimate the relationship between the RSV, outcome, and treatment, which is typically mis-specified with unstructured data. We form a representation of the RSV for downstream causal inference by predicting the outcome and predicting the treatment, with better predictions leading to more precise causal estimates. We re-evaluate the efficacy of a large-scale public program in India, showing that the program's measured effects on local consumption and poverty can be replicated using satellite</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10959v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ashesh Rambachan, Rahul Singh, Davide Viviano</dc:creator>
    </item>
    <item>
      <title>Towards a framework on tabular synthetic data generation: a minimalist approach: theory, use cases, and limitations</title>
      <link>https://arxiv.org/abs/2411.10982</link>
      <description>arXiv:2411.10982v1 Announce Type: cross 
Abstract: We propose and study a minimalist approach towards synthetic tabular data generation. The model consists of a minimalistic unsupervised SparsePCA encoder (with contingent clustering step or log transformation to handle nonlinearity) and XGboost decoder which is SOTA for structured data regression and classification tasks. We study and contrast the methodologies with (variational) autoencoders in several toy low dimensional scenarios to derive necessary intuitions. The framework is applied to high dimensional simulated credit scoring data which parallels real-life financial applications. We applied the method to robustness testing to demonstrate practical use cases. The case study result suggests that the method provides an alternative to raw and quantile perturbation for model robustness testing. We show that the method is simplistic, guarantees interpretability all the way through, does not require extra tuning and provide unique benefits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10982v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Agus Sudjianto, Yueyang Shen, Arun Prakash R, Anwesha Bhattacharyya, Maorong Rao, Yaqun Wang, Joel Vaughan, Nengfeng Zhou</dc:creator>
    </item>
    <item>
      <title>Variational Bayesian Bow tie Neural Networks with Shrinkage</title>
      <link>https://arxiv.org/abs/2411.11132</link>
      <description>arXiv:2411.11132v2 Announce Type: cross 
Abstract: Despite the dominant role of deep models in machine learning, limitations persist, including overconfident predictions, susceptibility to adversarial attacks, and underestimation of variability in predictions. The Bayesian paradigm provides a natural framework to overcome such issues and has become the gold standard for uncertainty estimation with deep models, also providing improved accuracy and a framework for tuning critical hyperparameters. However, exact Bayesian inference is challenging, typically involving variational algorithms that impose strong independence and distributional assumptions. Moreover, existing methods are sensitive to the architectural choice of the network. We address these issues by constructing a relaxed version of the standard feed-forward rectified neural network, and employing Polya-Gamma data augmentation tricks to render a conditionally linear and Gaussian model. Additionally, we use sparsity-promoting priors on the weights of the neural network for data-driven architectural design. To approximate the posterior, we derive a variational inference algorithm that avoids distributional assumptions and independence across layers and is a faster alternative to the usual Markov Chain Monte Carlo schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11132v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alisa Sheinkman, Sara Wade</dc:creator>
    </item>
    <item>
      <title>Debiasing Watermarks for Large Language Models via Maximal Coupling</title>
      <link>https://arxiv.org/abs/2411.11203</link>
      <description>arXiv:2411.11203v1 Announce Type: cross 
Abstract: Watermarking language models is essential for distinguishing between human and machine-generated text and thus maintaining the integrity and trustworthiness of digital communication. We present a novel green/red list watermarking approach that partitions the token set into ``green'' and ``red'' lists, subtly increasing the generation probability for green tokens. To correct token distribution bias, our method employs maximal coupling, using a uniform coin flip to decide whether to apply bias correction, with the result embedded as a pseudorandom watermark signal. Theoretical analysis confirms this approach's unbiased nature and robust detection capabilities. Experimental results show that it outperforms prior techniques by preserving text quality while maintaining high detectability, and it demonstrates resilience to targeted modifications aimed at improving text quality. This research provides a promising watermarking solution for language models, balancing effective detection with minimal impact on text quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11203v1</guid>
      <category>stat.ML</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yangxinyu Xie, Xiang Li, Tanwi Mallick, Weijie J. Su, Ruixun Zhang</dc:creator>
    </item>
    <item>
      <title>Mean Estimation in Banach Spaces Under Infinite Variance and Martingale Dependence</title>
      <link>https://arxiv.org/abs/2411.11271</link>
      <description>arXiv:2411.11271v1 Announce Type: cross 
Abstract: We consider estimating the shared mean of a sequence of heavy-tailed random variables taking values in a Banach space. In particular, we revisit and extend a simple truncation-based mean estimator by Catoni and Giulini. While existing truncation-based approaches require a bound on the raw (non-central) second moment of observations, our results hold under a bound on either the central or non-central $p$th moment for some $p &gt; 1$. In particular, our results hold for distributions with infinite variance. The main contributions of the paper follow from exploiting connections between truncation-based mean estimation and the concentration of martingales in 2-smooth Banach spaces. We prove two types of time-uniform bounds on the distance between the estimator and unknown mean: line-crossing inequalities, which can be optimized for a fixed sample size $n$, and non-asymptotic law of the iterated logarithm type inequalities, which match the tightness of line-crossing inequalities at all points in time up to a doubly logarithmic factor in $n$. Our results do not depend on the dimension of the Banach space, hold under martingale dependence, and all constants in the inequalities are known and small.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11271v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Justin Whitehouse, Ben Chugg, Diego Martinez-Taboada, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Debiased Regression for Root-N-Consistent Conditional Mean Estimation</title>
      <link>https://arxiv.org/abs/2411.11748</link>
      <description>arXiv:2411.11748v2 Announce Type: cross 
Abstract: This study introduces a debiasing method for regression estimators, including high-dimensional and nonparametric regression estimators. For example, nonparametric regression methods allow for the estimation of regression functions in a data-driven manner with minimal assumptions; however, these methods typically fail to achieve $\sqrt{n}$-consistency in their convergence rates, and many, including those in machine learning, lack guarantees that their estimators asymptotically follow a normal distribution. To address these challenges, we propose a debiasing technique for nonparametric estimators by adding a bias-correction term to the original estimators, extending the conventional one-step estimator used in semiparametric analysis. Specifically, for each data point, we estimate the conditional expected residual of the original nonparametric estimator, which can, for instance, be computed using kernel (Nadaraya-Watson) regression, and incorporate it as a bias-reduction term. Our theoretical analysis demonstrates that the proposed estimator achieves $\sqrt{n}$-consistency and asymptotic normality under a mild convergence rate condition for both the original nonparametric estimator and the conditional expected residual estimator. Notably, this approach remains model-free as long as the original estimator and the conditional expected residual estimator satisfy the convergence rate condition. The proposed method offers several advantages, including improved estimation accuracy and simplified construction of confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11748v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>Theoretical Foundations of Conformal Prediction</title>
      <link>https://arxiv.org/abs/2411.11824</link>
      <description>arXiv:2411.11824v1 Announce Type: cross 
Abstract: This book is about conformal prediction and related inferential techniques that build on permutation tests and exchangeability. These techniques are useful in a diverse array of tasks, including hypothesis testing and providing uncertainty quantification guarantees for machine learning systems. Much of the current interest in conformal prediction is due to its ability to integrate into complex machine learning workflows, solving the problem of forming prediction sets without any assumptions on the form of the data generating distribution. Since contemporary machine learning algorithms have generally proven difficult to analyze directly, conformal prediction's main appeal is its ability to provide formal, finite-sample guarantees when paired with such methods.
  The goal of this book is to teach the reader about the fundamental technical arguments that arise when researching conformal prediction and related questions in distribution-free inference. Many of these proof strategies, especially the more recent ones, are scattered among research papers, making it difficult for researchers to understand where to look, which results are important, and how exactly the proofs work. We hope to bridge this gap by curating what we believe to be some of the most important results in the literature and presenting their proofs in a unified language, with illustrations, and with an eye towards pedagogy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11824v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anastasios N. Angelopoulos, Rina Foygel Barber, Stephen Bates</dc:creator>
    </item>
    <item>
      <title>Bayesian Quantile Regression with Subset Selection: A Decision Analysis Perspective</title>
      <link>https://arxiv.org/abs/2311.02043</link>
      <description>arXiv:2311.02043v4 Announce Type: replace 
Abstract: Quantile regression is a powerful tool for inferring how covariates affect specific percentiles of the response distribution. Existing methods either estimate conditional quantiles separately for each quantile of interest or estimate the entire conditional distribution using semi- or non-parametric models. The former often produce inadequate models for real data and do not share information across quantiles, while the latter are characterized by complex and constrained models that can be difficult to interpret and computationally inefficient. Neither approach is well-suited for quantile-specific subset selection. Instead, we pose the fundamental problems of linear quantile estimation, uncertainty quantification, and subset selection from a Bayesian decision analysis perspective. For any Bayesian regression model -- including, but not limited to existing Bayesian quantile regression models -- we derive optimal point estimates, interpretable uncertainty quantification, and scalable subset selection techniques for all model-based conditional quantiles. Our approach introduces a quantile-focused squared error loss that enables efficient, closed-form computing and maintains a close relationship with Wasserstein-based density estimation. In an extensive simulation study, our methods demonstrate substantial gains in quantile estimation accuracy, inference, and variable selection over frequentist and Bayesian competitors. We use these tools to identify and quantify the heterogeneous impacts of multiple social stressors and environmental exposures on educational outcomes across the full spectrum of low-, medium-, and high-achieving students in North Carolina.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02043v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joseph Feldman, Daniel Kowal</dc:creator>
    </item>
    <item>
      <title>Type I Error Rates are Not Usually Inflated</title>
      <link>https://arxiv.org/abs/2312.06265</link>
      <description>arXiv:2312.06265v4 Announce Type: replace 
Abstract: The inflation of Type I error rates is thought to be one of the causes of the replication crisis. Questionable research practices such as p-hacking are thought to inflate Type I error rates above their nominal level, leading to unexpectedly high levels of false positives in the literature and, consequently, unexpectedly low replication rates. In this article, I offer an alternative view. I argue that questionable and other research practices do not usually inflate relevant Type I error rates. I begin by introducing the concept of Type I error rates and distinguishing between statistical errors and theoretical errors. I then illustrate my argument with respect to model misspecification, multiple testing, selective inference, forking paths, exploratory analyses, p-hacking, optional stopping, double dipping, and HARKing. In each case, I demonstrate that relevant Type I error rates are not usually inflated above their nominal level, and in the rare cases that they are, the inflation is easily identified and resolved. I conclude that the replication crisis may be explained, at least in part, by researchers' misinterpretation of statistical errors and their underestimation of theoretical errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.06265v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.36850/4d35-44bd</arxiv:DOI>
      <dc:creator>Mark Rubin</dc:creator>
    </item>
    <item>
      <title>Robust Learning for Optimal Dynamic Treatment Regimes with Observational Data</title>
      <link>https://arxiv.org/abs/2404.00221</link>
      <description>arXiv:2404.00221v3 Announce Type: replace 
Abstract: Public policies and medical interventions often involve dynamics in their treatment assignments, where individuals receive a series of interventions over multiple stages. We study the statistical learning of optimal dynamic treatment regimes (DTRs) that guide the optimal treatment assignment for each individual at each stage based on the individual's evolving history. We propose a doubly robust, classification-based approach to learning the optimal DTR using observational data under the assumption of sequential ignorability. This approach learns the optimal DTR through backward induction. At each step, it constructs an augmented inverse probability weighting (AIPW) estimator of the policy value function and maximizes it to learn the optimal policy for the corresponding stage. We show that the resulting DTR can achieve an optimal convergence rate of $n^{-1/2}$ for welfare regret under mild convergence conditions on estimators of the nuisance components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00221v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shosei Sakaguchi</dc:creator>
    </item>
    <item>
      <title>Calibrated sensitivity models</title>
      <link>https://arxiv.org/abs/2405.08738</link>
      <description>arXiv:2405.08738v3 Announce Type: replace 
Abstract: In causal inference, sensitivity models assess how unmeasured confounders could alter causal analyses, but the sensitivity parameter -- which quantifies the degree of unmeasured confounding -- is often difficult to interpret. For this reason, researchers sometimes compare the sensitivity parameter to an estimate of measured confounding. This is known as calibration, or benchmarking. Although it can aid interpretation, calibration is typically conducted post hoc, and uncertainty in the estimate for unmeasured confounding is rarely accounted for. To address these limitations, we propose calibrated sensitivity models, which directly bound the degree of unmeasured confounding by a multiple of measured confounding. The calibrated sensitivity parameter is interpretable as a ratio of unmeasured to measured confounding, and uncertainty due to estimating measured confounding can be incorporated. Incorporating this uncertainty shows causal analyses can be less or more robust to unmeasured confounding than suggested by standard approaches. We develop efficient estimators and inferential methods for bounds on the average treatment effect with three calibrated sensitivity models, establishing parametric efficiency and asymptotic normality under doubly robust style nonparametric conditions. We illustrate our methods with an analysis of the effect of mothers' smoking on infant birthweight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08738v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alec McClean, Zach Branson, Edward H. Kennedy</dc:creator>
    </item>
    <item>
      <title>Sample Size and Bias Approximations For Continuous Exposures Measured with Error</title>
      <link>https://arxiv.org/abs/2406.02369</link>
      <description>arXiv:2406.02369v5 Announce Type: replace 
Abstract: Understanding of sample size and the accuracy and precision of the estimator is very limited when continuous exposure is heteroskedastic, measured with error that may be autocorrelated, or when multiple exposure time points are of interest. Therefore, this article develops approximation equations for sample size, estimates of the estimators, and standard errors, including polynomials for non-linear effect estimation in the absence or presence of autocorrelated measurement error for distributed lags of heteroskedastic exposures. The theory and methods developed here can be used to efficiently design research in various settings when exposure variables are continuous and measured with error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02369v5</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Honghyok Kim</dc:creator>
    </item>
    <item>
      <title>Gaussian Copula Models for Nonignorable Missing Data Using Auxiliary Marginal Quantiles</title>
      <link>https://arxiv.org/abs/2406.03463</link>
      <description>arXiv:2406.03463v2 Announce Type: replace 
Abstract: We present an approach for modeling and imputation of nonignorable missing data. Our approach uses Bayesian data integration to combine (1) a Gaussian copula model for all study variables and missingness indicators, which allows arbitrary marginal distributions, nonignorable missingess, and other dependencies, and (2) auxiliary information in the form of marginal quantiles for some study variables. We prove that, remarkably, one only needs a small set of accurately-specified quantiles to estimate the copula correlation consistently. The remaining marginal distribution functions are inferred nonparametrically and jointly with the copula parameters using an efficient MCMC algorithm. We also characterize the (additive) nonignorable missingness mechanism implied by the copula model. Simulations confirm the effectiveness of this approach for multivariate imputation with nonignorable missing data. We apply the model to analyze associations between lead exposure and end-of-grade test scores for 170,000 North Carolina students. Lead exposure has nonignorable missingness: children with higher exposure are more likely to be measured. We elicit marginal quantiles for lead exposure using statistics provided by the Centers for Disease Control and Prevention. Multiple imputation inferences under our model support stronger, more adverse associations between lead exposure and educational outcomes relative to complete case and missing-at-random analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03463v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph Feldman, Jerome P. Reiter, Daniel R. Kowal</dc:creator>
    </item>
    <item>
      <title>Errors-In-Variables Model Fitting for Partially Unpaired Data Utilizing Mixture Models</title>
      <link>https://arxiv.org/abs/2406.18154</link>
      <description>arXiv:2406.18154v4 Announce Type: replace 
Abstract: We introduce a general framework for regression in the errors-in-variables regime, allowing for full flexibility about the dimensionality of the data, observational error probability density types, the (nonlinear) model type and the avoidance of ad-hoc definitions of loss functions. In this framework, we introduce model fitting for partially unpaired data, i.e. for given data groups the pairing information of input and output is lost (semi-supervised). This is achieved by constructing mixture model densities, which directly model the loss of pairing information allowing inference. In a numerical simulation study linear and nonlinear model fits are illustrated as well as a real data study is presented based on life expectancy data from the world bank utilizing a multiple linear regression model. These results show that high quality model fitting is possible with partially unpaired data, which opens the possibility for new applications with unfortunate or deliberate loss of pairing information in data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18154v4</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wolfgang Hoegele, Sarah Brockhaus</dc:creator>
    </item>
    <item>
      <title>Valid standard errors for Bayesian quantile regression with clustered and independent data</title>
      <link>https://arxiv.org/abs/2407.09772</link>
      <description>arXiv:2407.09772v3 Announce Type: replace 
Abstract: In Bayesian quantile regression, the most commonly used likelihood is the asymmetric Laplace (AL) likelihood. The reason for this choice is not that it is a plausible data-generating model but that the corresponding maximum likelihood estimator is identical to the classical estimator by Koenker and Bassett (1978), and in that sense, the AL likelihood can be thought of as a working likelihood. AL-based quantile regression has been shown to produce good finite-sample Bayesian point estimates and to be consistent. However, if the AL distribution does not correspond to the data-generating distribution, credible intervals based on posterior standard deviations can have poor coverage. Yang, Wang, and He (2016) proposed an adjustment to the posterior covariance matrix that produces asymptotically valid intervals. However, we show that this adjustment is sensitive to the choice of scale parameter for the AL likelihood and can lead to poor coverage when the sample size is small to moderate. We therefore propose using Infinitesimal Jackknife (IJ) standard errors (Giordano &amp; Broderick, 2023). These standard errors do not require resampling but can be obtained from a single MCMC run. We also propose a version of IJ standard errors for clustered data. Simulations and applications to real data show that the IJ standard errors have good frequentist properties, both for independent and clustered data. We provide an R-package, IJSE, that computes IJ standard errors for clustered or independent data after estimation with the brms wrapper in R for Stan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09772v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Feng Ji, JoonHo Lee, Sophia Rabe-Hesketh</dc:creator>
    </item>
    <item>
      <title>Win Ratio with Multiple Thresholds for Composite Endpoints</title>
      <link>https://arxiv.org/abs/2407.18341</link>
      <description>arXiv:2407.18341v2 Announce Type: replace 
Abstract: Composite endpoints consisting of both terminal and non-terminal events, such as death and hospitalization, are frequently used as primary endpoints in cardiovascular clinical trials. The Win Ratio method (WR) proposed by Pocock et al. (2012)1 employs a hierarchical structure to combine fatal and non-fatal events by giving death information an absolute priority, which adversely affects power if the treatment effect is mainly on the non-fatal outcomes. We hereby propose the Win Ratio with Multiple Thresholds (WR-MT) that releases the strict hierarchical structure of the standard WR by adding stages with non-zero thresholds. A weighted adaptive approach is developed to determine the thresholds in WR-MT. This method preserves the good statistical properties of the standard WR and has a greater capacity to detect treatment effects on non-fatal events. We show that WR-MT has an overall more favorable performance than WR in our simulation that addresses the influence of follow-up time, the association between events, and the treatment effect levels, as well as a case study based on the Digitalis Investigation Group clinical trial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18341v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunhan Mou, Tassos Kyriakides, Scott Hummel, Fan Li, Yuan Huang</dc:creator>
    </item>
    <item>
      <title>Multiscale Multi-Type Spatial Bayesian Analysis for High-Dimensional Data with Application to Wildfires and Migration</title>
      <link>https://arxiv.org/abs/2410.02905</link>
      <description>arXiv:2410.02905v2 Announce Type: replace 
Abstract: Wildfires have significantly increased in the United States (U.S.), making certain areas harder to live in. This motivates us to jointly analyze active fires and population changes in the U.S. from July 2020 to June 2021. The available data are recorded on different scales (or spatial resolutions) and by different types of distributions (referred to as multi-type data). Moreover, wildfires are known to have feedback mechanism that creates signal-to-noise dependence. We analyze point-referenced remote sensing fire data from National Aeronautics and Space Administration (NASA) and county-level population change data provided by U.S. Census Bureau's Population Estimates Program (PEP). We develop a multiscale multi-type spatial Bayesian model that assumes the average number of fires is zero-inflated normal, the incidence of fire as Bernoulli, and the percentage population change as normally distributed. This high-dimensional dataset makes Markov chain Monte Carlo (MCMC) implementation infeasible. We bypass MCMC by extending a recently introduced computationally efficient Bayesian framework to directly sample from the exact posterior distribution, which includes a term to model signal-to-noise dependence. Such signal-to-noise dependence is known to be present in wildfire data, but is commonly not accounted for. A simulation study is used to highlight the computational performance of our method. In our analysis, we obtained predictions of wildfire probabilities, identified several useful covariates, and found that regions with many fires were associated with population change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02905v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijie Zhou, Jonathan R. Bradley</dc:creator>
    </item>
    <item>
      <title>Model-Agnostic Covariate-Assisted Inference on Partially Identified Causal Effects</title>
      <link>https://arxiv.org/abs/2310.08115</link>
      <description>arXiv:2310.08115v2 Announce Type: replace-cross 
Abstract: Many causal estimands are only partially identifiable since they depend on the unobservable joint distribution between potential outcomes. Stratification on pretreatment covariates can yield sharper bounds; however, unless the covariates are discrete with relatively small support, this approach typically requires binning covariates or estimating the conditional distributions of the potential outcomes given the covariates. Binning can result in substantial efficiency loss and become challenging to implement, even with a moderate number of covariates. Estimating conditional distributions, on the other hand, may yield invalid inference if the distributions are inaccurately estimated, such as when a misspecified model is used or when the covariates are high-dimensional. In this paper, we propose a unified and model-agnostic inferential approach for a wide class of partially identified estimands. Our method, based on duality theory for optimal transport problems, has four key properties. First, in randomized experiments, our approach can wrap around any estimates of the conditional distributions and provide uniformly valid inference, even if the initial estimates are arbitrarily inaccurate. A simple extension of our method to observational studies is doubly robust in the usual sense. Second, if nuisance parameters are estimated at semiparametric rates, our estimator is asymptotically unbiased for the sharp partial identification bound. Third, we can apply the multiplier bootstrap to select covariates and models without sacrificing validity, even if the true model is not selected. Finally, our method is computationally efficient. Overall, in three empirical applications, our method consistently reduces the width of estimated identified sets and confidence intervals without making additional structural assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.08115v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenlong Ji, Lihua Lei, Asher Spector</dc:creator>
    </item>
    <item>
      <title>Interpretable Machine Learning for Survival Analysis</title>
      <link>https://arxiv.org/abs/2403.10250</link>
      <description>arXiv:2403.10250v2 Announce Type: replace-cross 
Abstract: With the spread and rapid advancement of black box machine learning models, the field of interpretable machine learning (IML) or explainable artificial intelligence (XAI) has become increasingly important over the last decade. This is particularly relevant for survival analysis, where the adoption of IML techniques promotes transparency, accountability and fairness in sensitive areas, such as clinical decision making processes, the development of targeted therapies, interventions or in other medical or healthcare related contexts. More specifically, explainability can uncover a survival model's potential biases and limitations and provide more mathematically sound ways to understand how and which features are influential for prediction or constitute risk factors. However, the lack of readily available IML methods may have deterred medical practitioners and policy makers in public health from leveraging the full potential of machine learning for predicting time-to-event data. We present a comprehensive review of the limited existing amount of work on IML methods for survival analysis within the context of the general IML taxonomy. In addition, we formally detail how commonly used IML methods, such as such as individual conditional expectation (ICE), partial dependence plots (PDP), accumulated local effects (ALE), different feature importance measures or Friedman's H-interaction statistics can be adapted to survival outcomes. An application of several IML methods to real data on data on under-5 year mortality of Ghanaian children from the Demographic and Health Surveys (DHS) Program serves as a tutorial or guide for researchers, on how to utilize the techniques in practice to facilitate understanding of model decisions or predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10250v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sophie Hanna Langbein, Mateusz Krzyzi\'nski, Miko{\l}aj Spytek, Hubert Baniecki, Przemys{\l}aw Biecek, Marvin N. Wright</dc:creator>
    </item>
    <item>
      <title>Feature-wise and Sample-wise Adaptive Transfer Learning for High-dimensional Linear Regression</title>
      <link>https://arxiv.org/abs/2403.13565</link>
      <description>arXiv:2403.13565v2 Announce Type: replace-cross 
Abstract: We consider the transfer learning problem in the high dimensional linear regression setting, where the feature dimension is larger than the sample size. To learn transferable information, which may vary across features or the source samples, we propose an adaptive transfer learning method that can detect and aggregate the feature-wise (F-AdaTrans) or sample-wise (S-AdaTrans) transferable structures. We achieve this by employing a fused-penalty, coupled with weights that can adapt according to the transferable structure. To choose the weight, we propose a theoretically informed, data-driven procedure, enabling F-AdaTrans to selectively fuse the transferable signals with the target while filtering out non-transferable signals, and S-AdaTrans to obtain the optimal combination of information transferred from each source sample. We show that, with appropriately chosen weights, F-AdaTrans achieves a convergence rate close to that of an oracle estimator with a known transferable structure, and S-AdaTrans recovers existing near-minimax optimal rates as a special case. The effectiveness of the proposed method is validated using both simulation and real data, demonstrating favorable performance compared to the existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13565v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zelin He, Ying Sun, Jingyuan Liu, Runze Li</dc:creator>
    </item>
    <item>
      <title>Spatio-Temporal Jump Model for Urban Thermal Comfort Monitoring</title>
      <link>https://arxiv.org/abs/2411.09726</link>
      <description>arXiv:2411.09726v2 Announce Type: replace-cross 
Abstract: Thermal comfort is essential for well-being in urban spaces, especially as cities face increasing heat from urbanization and climate change. Existing thermal comfort models usually overlook temporal dynamics alongside spatial dependencies. We address this problem by introducing a spatio-temporal jump model that clusters data with persistence across both spatial and temporal dimensions. This framework enhances interpretability, minimizes abrupt state changes, and easily handles missing data. We validate our approach through extensive simulations, demonstrating its accuracy in recovering the true underlying partition. When applied to hourly environmental data gathered from a set of weather stations located across the city of Singapore, our proposal identifies meaningful thermal comfort regimes, demonstrating its effectiveness in dynamic urban settings and suitability for real-world monitoring. The comparison of these regimes with feedback on thermal preference indicates the potential of an unsupervised approach to avoid extensive surveys.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09726v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Federico P. Cortese, Antonio Pievatolo</dc:creator>
    </item>
  </channel>
</rss>
