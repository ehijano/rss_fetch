<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Jan 2025 05:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Poxel: Voxel Reconstruction for 3D Printing</title>
      <link>https://arxiv.org/abs/2501.10474</link>
      <description>arXiv:2501.10474v1 Announce Type: new 
Abstract: Recent advancements in 3D reconstruction, especially through neural rendering approaches like Neural Radiance Fields (NeRF) and Plenoxel, have led to high-quality 3D visualizations. However, these methods are optimized for digital environments and employ view-dependent color models (RGB) and 2D splatting techniques, which do not translate well to physical 3D printing. This paper introduces "Poxel", which stands for Printable-Voxel, a voxel-based 3D reconstruction framework optimized for photopolymer jetting 3D printing, which allows for high-resolution, full-color 3D models using a CMYKWCl color model. Our framework directly outputs printable voxel grids by removing view-dependency and converting the digital RGB color space to a physical CMYKWCl color space suitable for multi-material jetting. The proposed system achieves better fidelity and quality in printed models, aligning with the requirements of physical 3D objects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10474v1</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ruixiang Cao, Satoshi Yagi, Satoshi Yamamori, Jun Morimoto</dc:creator>
    </item>
    <item>
      <title>ENTIRE: Learning-based Volume Rendering Time Prediction</title>
      <link>https://arxiv.org/abs/2501.12119</link>
      <description>arXiv:2501.12119v1 Announce Type: new 
Abstract: We present ENTIRE, a novel approach for volume rendering time prediction. Time-dependent volume data from simulations or experiments typically comprise complex deforming structures across hundreds or thousands of time steps, which in addition to the camera configuration has a significant impact on rendering performance. We first extract a feature vector from a volume that captures its structure that is relevant for rendering time performance. Then we combine this feature vector with further relevant parameters (e.g. camera setup), and with this perform the final prediction. Our experiments conducted on various datasets demonstrate that our model is capable of efficiently achieving high prediction accuracy with fast response rates. We showcase ENTIRE's capability of enabling dynamic parameter adaptation for stable frame rates and load balancing in two case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12119v1</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zikai Yin, Hamid Gadirov, Jiri Kosinka, Steffen Frey</dc:creator>
    </item>
    <item>
      <title>PhyDeformer: High-Quality Non-Rigid Garment Registration with Physics-Awareness</title>
      <link>https://arxiv.org/abs/2501.10455</link>
      <description>arXiv:2501.10455v1 Announce Type: cross 
Abstract: We present PhyDeformer, a new deformation method for high-quality garment mesh registration. It operates in two phases: In the first phase, a garment grading is performed to achieve a coarse 3D alignment between the mesh template and the target mesh, accounting for proportional scaling and fit (e.g. length, size). Then, the graded mesh is refined to align with the fine-grained details of the 3D target through an optimization coupled with the Jacobian-based deformation framework. Both quantitative and qualitative evaluations on synthetic and real garments highlight the effectiveness of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10455v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Boyang Yu, Frederic Cordier, Hyewon Seo</dc:creator>
    </item>
    <item>
      <title>BloomScene: Lightweight Structured 3D Gaussian Splatting for Crossmodal Scene Generation</title>
      <link>https://arxiv.org/abs/2501.10462</link>
      <description>arXiv:2501.10462v1 Announce Type: cross 
Abstract: With the widespread use of virtual reality applications, 3D scene generation has become a new challenging research frontier. 3D scenes have highly complex structures and need to ensure that the output is dense, coherent, and contains all necessary structures. Many current 3D scene generation methods rely on pre-trained text-to-image diffusion models and monocular depth estimators. However, the generated scenes occupy large amounts of storage space and often lack effective regularisation methods, leading to geometric distortions. To this end, we propose BloomScene, a lightweight structured 3D Gaussian splatting for crossmodal scene generation, which creates diverse and high-quality 3D scenes from text or image inputs. Specifically, a crossmodal progressive scene generation framework is proposed to generate coherent scenes utilizing incremental point cloud reconstruction and 3D Gaussian splatting. Additionally, we propose a hierarchical depth prior-based regularization mechanism that utilizes multi-level constraints on depth accuracy and smoothness to enhance the realism and continuity of the generated scenes. Ultimately, we propose a structured context-guided compression mechanism that exploits structured hash grids to model the context of unorganized anchor attributes, which significantly eliminates structural redundancy and reduces storage overhead. Comprehensive experiments across multiple scenes demonstrate the significant potential and advantages of our framework compared with several baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10462v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaolu Hou, Mingcheng Li, Dingkang Yang, Jiawei Chen, Ziyun Qian, Xiao Zhao, Yue Jiang, Jinjie Wei, Qingyao Xu, Lihua Zhang</dc:creator>
    </item>
    <item>
      <title>DARB-Splatting: Generalizing Splatting with Decaying Anisotropic Radial Basis Functions</title>
      <link>https://arxiv.org/abs/2501.12369</link>
      <description>arXiv:2501.12369v1 Announce Type: cross 
Abstract: Splatting-based 3D reconstruction methods have gained popularity with the advent of 3D Gaussian Splatting, efficiently synthesizing high-quality novel views. These methods commonly resort to using exponential family functions, such as the Gaussian function, as reconstruction kernels due to their anisotropic nature, ease of projection, and differentiability in rasterization. However, the field remains restricted to variations within the exponential family, leaving generalized reconstruction kernels largely underexplored, partly due to the lack of easy integrability in 3D to 2D projections. In this light, we show that a class of decaying anisotropic radial basis functions (DARBFs), which are non-negative functions of the Mahalanobis distance, supports splatting by approximating the Gaussian function's closed-form integration advantage. With this fresh perspective, we demonstrate up to 34% faster convergence during training and a 15% reduction in memory consumption across various DARB reconstruction kernels, while maintaining comparable PSNR, SSIM, and LPIPS results. We will make the code available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12369v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vishagar Arunan (University of Moratuwa), Saeedha Nazar (University of Moratuwa), Hashiru Pramuditha (University of Moratuwa), Vinasirajan Viruthshaan (University of Moratuwa), Sameera Ramasinghe (University of Adelaide), Simon Lucey (University of Adelaide), Ranga Rodrigo (University of Moratuwa)</dc:creator>
    </item>
    <item>
      <title>Toolpath Generation for High Density Spatial Fiber Printing Guided by Principal Stresses</title>
      <link>https://arxiv.org/abs/2410.16851</link>
      <description>arXiv:2410.16851v2 Announce Type: replace 
Abstract: While multi-axis 3D printing can align continuous fibers along principal stresses in continuous fiber-reinforced thermoplastic (CFRTP) composites to enhance mechanical strength, existing methods have difficulty generating toolpaths with high fiber coverage. This is mainly due to the orientation consistency constraints imposed by vector-field-based methods and the turbulent stress fields around stress concentration regions. This paper addresses these challenges by introducing a 2-RoSy representation for computing the direction field, which is then converted into a periodic scalar field to generate partial iso-curves for fiber toolpaths with nearly equal hatching distance. To improve fiber coverage in stress-concentrated regions, such as around holes, we extend the quaternion-based method for curved slicing by incorporating winding compatibility considerations. Our proposed method can achieve toolpaths coverage between 87.5% and 90.6% by continuous fibers with 1.1mm width. Models fabricated using our toolpaths show up to 84.6% improvement in failure load and 54.4% increase in stiffness when compared to the results obtained from multi-axis 3D printing with sparser fibers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16851v2</guid>
      <category>cs.GR</category>
      <category>cs.CG</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Composites Part B: Engineering, 2025</arxiv:journal_reference>
      <dc:creator>Tianyu Zhang, Tao Liu, Neelotpal Dutta, Yongxue Chen, Renbo Su, Zhizhou Zhang, Weiming Wang, Charlie C. L. Wang</dc:creator>
    </item>
    <item>
      <title>Flexible Mesh Segmentation via Reeb Graph Representation of Geometrical and Topological Features</title>
      <link>https://arxiv.org/abs/2412.05335</link>
      <description>arXiv:2412.05335v2 Announce Type: replace 
Abstract: This paper presents a new mesh segmentation method that integrates geometrical and topological features through a flexible Reeb graph representation. The algorithm consists of three phases: construction of the Reeb graph using the improved topological skeleton approach, topological simplification of the graph by cancelling critical points while preserving essential features, and generation of contiguous segments via an adaptive region-growth process that takes geometric and topological criteria into account. Operating with a computational complexity of O(n log(n)) for a mesh of n vertices, the method demonstrates both efficiency and scalability. An evaluation through case studies, including part-based decomposition with Shape Diameter Function and terrain analysis with Shape Index, validates the effectiveness of the method in completely different applications. The results establish this approach as a robust framework for advanced geometric analysis of meshes, connecting the geometric and topological features of shapes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05335v2</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Beguet, Sandrine Lanquetin, Romain Raffin</dc:creator>
    </item>
  </channel>
</rss>
