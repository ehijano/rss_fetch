<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 16 May 2025 04:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Pure Component Property Estimation Framework Using Explainable Machine Learning Methods</title>
      <link>https://arxiv.org/abs/2505.09783</link>
      <description>arXiv:2505.09783v1 Announce Type: new 
Abstract: Accurate prediction of pure component physiochemical properties is crucial for process integration, multiscale modeling, and optimization. In this work, an enhanced framework for pure component property prediction by using explainable machine learning methods is proposed. In this framework, the molecular representation method based on the connectivity matrix effectively considers atomic bonding relationships to automatically generate features. The supervised machine learning model random forest is applied for feature ranking and pooling. The adjusted R2 is introduced to penalize the inclusion of additional features, providing an assessment of the true contribution of features. The prediction results for normal boiling point (Tb), liquid molar volume, critical temperature (Tc) and critical pressure (Pc) obtained using Artificial Neural Network and Gaussian Process Regression models confirm the accuracy of the molecular representation method. Comparison with GC based models shows that the root-mean-square error on the test set can be reduced by up to 83.8%. To enhance the interpretability of the model, a feature analysis method based on Shapley values is employed to determine the contribution of each feature to the property predictions. The results indicate that using the feature pooling method reduces the number of features from 13316 to 100 without compromising model accuracy. The feature analysis results for Tb, Tc, and Pc confirms that different molecular properties are influenced by different structural features, aligning with mechanistic interpretations. In conclusion, the proposed framework is demonstrated to be feasible and provides a solid foundation for mixture component reconstruction and process integration modelling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09783v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianfeng Jiao, Xi Gao, Jie Li</dc:creator>
    </item>
    <item>
      <title>Robust Bayesian hierarchical models for basket trials enabling joint evaluation of toxicity and efficacy</title>
      <link>https://arxiv.org/abs/2505.10317</link>
      <description>arXiv:2505.10317v1 Announce Type: new 
Abstract: Basket trials have gained increasing attention for their efficiency, as multiple patient subgroups are evaluated simultaneously. Conducted basket trials focus primarily on establishing the early efficacy of a treatment, yet continued monitoring of toxicity is essential. In this paper, we propose two Bayesian hierarchical models that enable bivariate analyses of toxicity and efficacy, while accounting for heterogeneity present in the treatment effects across patient subgroups. Specifically, one assumes the subgroup-specific toxicity and efficacy treatment effects, as a parameter vector, can be exchangeable or non-exchangeable; the other allows either the toxicity or efficacy parameters specific to the subgroups, to be exchangeable or non-exchangeable. The bivariate exchangeability and non-exchangeability distributions introduce a correlation parameter between treatment effects, while we stipulate a zero correlation when only toxicity or efficacy parameters are exchangeable. Simulation results show that our models perform robustly under different scenarios compared to the standard Bayesian hierarchical model and the stand-alone analyses, especially in producing higher power when the subgroup-specific effects are exchangeable in toxicity or efficacy only. When considerable correlation between the toxicity and efficacy effects exists, our methodology gives small error rates and greater power than alternatives that analyse toxicity and efficacy by parts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10317v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhi Cao, Pavel Mozgunov, Haiyan Zheng</dc:creator>
    </item>
    <item>
      <title>Uncovering Drivers of EU Carbon Futures with Bayesian Networks</title>
      <link>https://arxiv.org/abs/2505.10384</link>
      <description>arXiv:2505.10384v1 Announce Type: new 
Abstract: The European Union Emissions Trading System (EU ETS) is a key policy tool for reducing greenhouse gas emissions and advancing toward a net-zero economy. Under this scheme, tradeable carbon credits, European Union Allowances (EUAs), are issued to large emitters, who can buy and sell them on regulated markets. We investigate the influence of financial, economic, and energy-related factors on EUA futures prices using discrete and dynamic Bayesian networks to model both contemporaneous and time-lagged dependencies. The analysis is based on daily data spanning the third and fourth ETS trading phases (2013-2025), incorporating a wide range of indicators including energy commodities, equity indices, exchange rates, and bond markets. Results reveal that EUA pricing is most influenced by energy commodities, especially coal and oil futures, and by the performance of the European energy sector. Broader market sentiment, captured through stock indices and volatility measures, affects EUA prices indirectly via changes in energy demand. The dynamic model confirms a modest next-day predictive influence from oil markets, while most other effects remain contemporaneous. These insights offer regulators, institutional investors, and firms subject to ETS compliance a clearer understanding of the interconnected forces shaping the carbon market, supporting more effective hedging, investment strategies, and policy design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10384v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Maciejowski, Manuele Leonelli</dc:creator>
    </item>
    <item>
      <title>Mathematical Modeling, Analysis and Simulation Utilizing Machine Learning Tools for Assessing the Impact of Climate Lobbying</title>
      <link>https://arxiv.org/abs/2505.09617</link>
      <description>arXiv:2505.09617v1 Announce Type: cross 
Abstract: Climate policy and legislation has a significant influence on both domestic and global responses to the pressing environmental challenges of our time. The effectiveness of such climate legislation is closely tied to the complex dynamics among elected officials, a dynamic significantly shaped by the relentless efforts of lobbying. This project aims to develop a novel compartmental model to forecast the trajectory of climate legislation within the United States. By understanding the dynamics surrounding floor votes, the ramifications of lobbying, and the flow of campaign donations within the chambers of the U.S. Congress, we aim to validate our model through a comprehensive case study of the American Clean Energy and Security Act (ACESA). Our model adeptly captures the nonlinear dynamics among diverse legislative factions, including centrists, ardent supporters, and vocal opponents of the bill, culminating in a rich dynamics of final voting outcomes. We conduct a stability analysis of the model, estimating parameters from public lobbying records and a robust body of existing literature. The numerical verification against the pivotal 2009 ACESA vote, alongside contemporary research, underscores the models promising potential as a tool to understand the dynamics of climate lobbying. We also analyse the pathways of the model that aims to guide future legislative endeavors in the pursuit of effective climate action.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09617v1</guid>
      <category>q-fin.CP</category>
      <category>stat.AP</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Jacoby, Samiran Ghosh, Malay Banerjee, Aditi Ghosh, Padmanabhan Seshaiyer</dc:creator>
    </item>
    <item>
      <title>Contextual Phenotyping of Pediatric Sepsis Cohort Using Large Language Models</title>
      <link>https://arxiv.org/abs/2505.09805</link>
      <description>arXiv:2505.09805v1 Announce Type: cross 
Abstract: Clustering patient subgroups is essential for personalized care and efficient resource use. Traditional clustering methods struggle with high-dimensional, heterogeneous healthcare data and lack contextual understanding. This study evaluates Large Language Model (LLM) based clustering against classical methods using a pediatric sepsis dataset from a low-income country (LIC), containing 2,686 records with 28 numerical and 119 categorical variables. Patient records were serialized into text with and without a clustering objective. Embeddings were generated using quantized LLAMA 3.1 8B, DeepSeek-R1-Distill-Llama-8B with low-rank adaptation(LoRA), and Stella-En-400M-V5 models. K-means clustering was applied to these embeddings. Classical comparisons included K-Medoids clustering on UMAP and FAMD-reduced mixed data. Silhouette scores and statistical tests evaluated cluster quality and distinctiveness. Stella-En-400M-V5 achieved the highest Silhouette Score (0.86). LLAMA 3.1 8B with the clustering objective performed better with higher number of clusters, identifying subgroups with distinct nutritional, clinical, and socioeconomic profiles. LLM-based methods outperformed classical techniques by capturing richer context and prioritizing key features. These results highlight potential of LLMs for contextual phenotyping and informed decision-making in resource-limited settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09805v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aditya Nagori, Ayush Gautam, Matthew O. Wiens, Vuong Nguyen, Nathan Kenya Mugisha, Jerome Kabakyenga, Niranjan Kissoon, John Mark Ansermino, Rishikesan Kamaleswaran</dc:creator>
    </item>
    <item>
      <title>Automated Alert Classification and Triage (AACT): An Intelligent System for the Prioritisation of Cybersecurity Alerts</title>
      <link>https://arxiv.org/abs/2505.09843</link>
      <description>arXiv:2505.09843v1 Announce Type: cross 
Abstract: Enterprise networks are growing ever larger with a rapidly expanding attack surface, increasing the volume of security alerts generated from security controls. Security Operations Centre (SOC) analysts triage these alerts to identify malicious activity, but they struggle with alert fatigue due to the overwhelming number of benign alerts. Organisations are turning to managed SOC providers, where the problem is amplified by context switching and limited visibility into business processes.
  A novel system, named AACT, is introduced that automates SOC workflows by learning from analysts' triage actions on cybersecurity alerts. It accurately predicts triage decisions in real time, allowing benign alerts to be closed automatically and critical ones prioritised. This reduces the SOC queue allowing analysts to focus on the most severe, relevant or ambiguous threats. The system has been trained and evaluated on both real SOC data and an open dataset, obtaining high performance in identifying malicious alerts from benign alerts.
  Additionally, the system has demonstrated high accuracy in a real SOC environment, reducing alerts shown to analysts by 61% over six months, with a low false negative rate of 1.36% over millions of alerts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09843v1</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Melissa Turcotte, Fran\c{c}ois Labr\`eche, Serge-Olivier Paquette</dc:creator>
    </item>
    <item>
      <title>Robust and Computationally Efficient Trimmed L-Moments Estimation for Parametric Distributions</title>
      <link>https://arxiv.org/abs/2505.09860</link>
      <description>arXiv:2505.09860v1 Announce Type: cross 
Abstract: This paper proposes a robust and computationally efficient estimation framework for fitting parametric distributions based on trimmed L-moments. Trimmed L-moments extend classical L-moment theory by downweighting or excluding extreme order statistics, resulting in estimators that are less sensitive to outliers and heavy tails. We construct estimators for both location-scale and shape parameters using asymmetric trimming schemes tailored to different moments, and establish their asymptotic properties for inferential justification using the general structural theory of L-statistics, deriving simplified single-integration expressions to ensure numerical stability. State-of-the-art algorithms are developed to resolve the sign ambiguity in estimating the scale parameter for location-scale models and the tail index for the Frechet model. The proposed estimators offer improved efficiency over traditional robust alternatives for selected asymmetric trimming configurations, while retaining closed-form expressions for a wide range of common distributions, facilitating fast and stable computation. Simulation studies demonstrate strong finite-sample performance. An application to financial claim severity modeling highlights the practical relevance and flexibility of the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09860v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chudamani Poudyal, Qian Zhao, Hari Sitaula</dc:creator>
    </item>
    <item>
      <title>Advanced Crash Causation Analysis for Freeway Safety: A Large Language Model Approach to Identifying Key Contributing Factors</title>
      <link>https://arxiv.org/abs/2505.09949</link>
      <description>arXiv:2505.09949v1 Announce Type: cross 
Abstract: Understanding the factors contributing to traffic crashes and developing strategies to mitigate their severity is essential. Traditional statistical methods and machine learning models often struggle to capture the complex interactions between various factors and the unique characteristics of each crash. This research leverages large language model (LLM) to analyze freeway crash data and provide crash causation analysis accordingly. By compiling 226 traffic safety studies related to freeway crashes, a training dataset encompassing environmental, driver, traffic, and geometric design factors was created. The Llama3 8B model was fine-tuned using QLoRA to enhance its understanding of freeway crashes and their contributing factors, as covered in these studies. The fine-tuned Llama3 8B model was then used to identify crash causation without pre-labeled data through zero-shot classification, providing comprehensive explanations to ensure that the identified causes were reasonable and aligned with existing research. Results demonstrate that LLMs effectively identify primary crash causes such as alcohol-impaired driving, speeding, aggressive driving, and driver inattention. Incorporating event data, such as road maintenance, offers more profound insights. The model's practical applicability and potential to improve traffic safety measures were validated by a high level of agreement among researchers in the field of traffic safety, as reflected in questionnaire results with 88.89%. This research highlights the complex nature of traffic crashes and how LLMs can be used for comprehensive analysis of crash causation and other contributing factors. Moreover, it provides valuable insights and potential countermeasures to aid planners and policymakers in developing more effective and efficient traffic safety practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09949v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahmed S. Abdelrahman, Mohamed Abdel-Aty, Samgyu Yang, Abdulrahman Faden</dc:creator>
    </item>
    <item>
      <title>A mover-stayer model with time-dependent stayer fraction</title>
      <link>https://arxiv.org/abs/2505.10065</link>
      <description>arXiv:2505.10065v1 Announce Type: cross 
Abstract: Mover-stayer models are used in social sciences and economics to model heterogeneous population dynamics in which some individuals never experience the event of interest ("stayers"), while others transition between states over time ("movers"). Conventionally, the mover-stayer status is determined at baseline and time-dependent covariates are only incorporated in the movers' transition probabilities. In this paper, we present a novel dynamic version of the mover-stayer model, allowing potential movers to become stayers over time based on time-varying circumstances. Using a multinomial logistic framework, our model incorporates both time-fixed and exogenous time-varying covariates to estimate transition probabilities among the states of potential movers, movers, and stayers. Both the initial state and transitions to the stayer state are treated as latent. The introduction of this new model is motivated by the study of student mobility. Specifically focusing on panel data on the inter-university mobility of Italian students, factors such as the students' change of course and university size are considered as time-varying covariates in modelling their probability of moving or becoming stayers; sex and age at enrolment as time-fixed covariates. We propose a maximum likelihood estimation approach and investigate its finite-sample performance through simulations, comparing it to established models in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10065v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eni Musta, Martina Vittorietti</dc:creator>
    </item>
    <item>
      <title>Informed Forecasting: Leveraging Auxiliary Knowledge to Boost LLM Performance on Time Series Forecasting</title>
      <link>https://arxiv.org/abs/2505.10213</link>
      <description>arXiv:2505.10213v1 Announce Type: cross 
Abstract: With the widespread adoption of Large Language Models (LLMs), there is a growing need to establish best practices for leveraging their capabilities beyond traditional natural language tasks. In this paper, a novel cross-domain knowledge transfer framework is proposed to enhance the performance of LLMs in time series forecasting -- a task of increasing relevance in fields such as energy systems, finance, and healthcare. The approach systematically infuses LLMs with structured temporal information to improve their forecasting accuracy. This study evaluates the proposed method on a real-world time series dataset and compares it to a naive baseline where the LLM receives no auxiliary information. Results show that knowledge-informed forecasting significantly outperforms the uninformed baseline in terms of predictive accuracy and generalization. These findings highlight the potential of knowledge transfer strategies to bridge the gap between LLMs and domain-specific forecasting tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10213v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mohammadmahdi Ghasemloo, Alireza Moradi</dc:creator>
    </item>
    <item>
      <title>Comparative Analysis of Richardson-Lucy Deconvolution and Data Unfolding with Mean Integrated Square Error Optimization</title>
      <link>https://arxiv.org/abs/2505.10283</link>
      <description>arXiv:2505.10283v1 Announce Type: cross 
Abstract: Two maximum likelihood-based algorithms for unfolding or deconvolution are considered: the Richardson-Lucy method and the Data Unfolding method with Mean Integrated Square Error (MISE) optimization [10]. Unfolding is viewed as a procedure for estimating an unknown probability density function. Both external and internal quality assessment methods can be applied for this purpose. In some cases, external criteria exist to evaluate deconvolution quality. A typical example is the deconvolution of a blurred image, where the sharpness of the restored image serves as an indicator of quality. However, defining such external criteria can be challenging, particularly when a measurement has not been performed previously. In such instances, internal criteria are necessary to assess the quality of the result independently of external information. The article discusses two internal criteria: MISE for the unfolded distribution and the condition number of the correlation matrix of the unfolded distribution. These internal quality criteria are applied to a comparative analysis of the two methods using identical numerical data. The results of the analysis demonstrate the superiority of the Data Unfolding method with MISE optimization over the Richardson-Lucy method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10283v1</guid>
      <category>physics.data-an</category>
      <category>astro-ph.IM</category>
      <category>hep-ex</category>
      <category>nucl-ex</category>
      <category>stat.AP</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikolay D. Gagunashvili</dc:creator>
    </item>
    <item>
      <title>Whitened Score Diffusion: A Structured Prior for Imaging Inverse Problems</title>
      <link>https://arxiv.org/abs/2505.10311</link>
      <description>arXiv:2505.10311v1 Announce Type: cross 
Abstract: Conventional score-based diffusion models (DMs) may struggle with anisotropic Gaussian diffusion processes due to the required inversion of covariance matrices in the denoising score matching training objective \cite{vincent_connection_2011}. We propose Whitened Score (WS) diffusion models, a novel SDE-based framework that learns the Whitened Score function instead of the standard score. This approach circumvents covariance inversion, extending score-based DMs by enabling stable training of DMs on arbitrary Gaussian forward noising processes. WS DMs establish equivalence with FM for arbitrary Gaussian noise, allow for tailored spectral inductive biases, and provide strong Bayesian priors for imaging inverse problems with structured noise. We experiment with a variety of computational imaging tasks using the CIFAR and CelebA ($64\times64$) datasets and demonstrate that WS diffusion priors trained on anisotropic Gaussian noising processes consistently outperform conventional diffusion priors based on isotropic Gaussian noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10311v1</guid>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeffrey Alido, Tongyu Li, Yu Sun, Lei Tian</dc:creator>
    </item>
    <item>
      <title>FactsR: A Safer Method for Producing High Quality Healthcare Documentation</title>
      <link>https://arxiv.org/abs/2505.10360</link>
      <description>arXiv:2505.10360v1 Announce Type: cross 
Abstract: There are now a multitude of AI-scribing solutions for healthcare promising the utilization of large language models for ambient documentation. However, these AI scribes still rely on one-shot, or few-shot prompts for generating notes after the consultation has ended, employing little to no reasoning. This risks long notes with an increase in hallucinations, misrepresentation of the intent of the clinician, and reliance on the proofreading of the clinician to catch errors. A dangerous combination for patient safety if vigilance is compromised by workload and fatigue. In this paper, we introduce a method for extracting salient clinical information in real-time alongside the healthcare consultation, denoted Facts, and use that information recursively to generate the final note. The FactsR method results in more accurate and concise notes by placing the clinician-in-the-loop of note generation, while opening up new use cases within real-time decision support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10360v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victor Petr\'en Bach Hansen, Lasse Krogsb{\o}ll, Jonas Lyngs{\o}, Mathias Baltzersen, Andreas Motzfeldt, Kevin Pelgrims, Lars Maal{\o}e</dc:creator>
    </item>
    <item>
      <title>Towards more realistic climate model outputs: A multivariate bias correction based on zero-inflated vine copulas</title>
      <link>https://arxiv.org/abs/2410.15931</link>
      <description>arXiv:2410.15931v2 Announce Type: replace 
Abstract: Climate model large ensembles are an essential research tool for analysing and quantifying natural climate variability and providing robust information for rare extreme events. The models simulated representations of reality are susceptible to bias due to incomplete understanding of physical processes. This paper aims to correct the bias of five climate variables from the CRCM5 Large Ensemble over Central Europe at a 3-hourly temporal resolution. At this high temporal resolution, two variables, precipitation and radiation, exhibit a high share of zero inflation. We propose a novel bias-correction method, VBC (Vine copula bias correction), that models and transfers multivariate dependence structures for zero-inflated margins in the data from its error-prone model domain to a reference domain. VBC estimates the model and reference distribution using vine copulas and corrects the model distribution via (inverse) Rosenblatt transformation. To deal with the variables' zero-inflated nature, we develop a new vine density decomposition that accommodates such variables and employs an adequately randomized version of the Rosenblatt transform. This novel approach allows for more accurate modelling of multivariate zero-inflated climate data. Compared with state-of-the-art correction methods, VBC is generally the best-performing correction and the most accurate method for correcting zero-inflated events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15931v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henri Funk, Ralf Ludwig, Helmut Kuechenhoff, Thomas Nagler</dc:creator>
    </item>
    <item>
      <title>One citation, one vote! A new approach for analysing check-all-that-apply (CATA) data, using L1 norm methods</title>
      <link>https://arxiv.org/abs/2502.15945</link>
      <description>arXiv:2502.15945v2 Announce Type: replace 
Abstract: A unified framework is provided for analysing check-all-that-apply (CATA) product data following the "one citation, one vote" principle. CATA data arise from studies where A assessors evaluate P products by describing samples by checking all of the T terms that apply. Giving every citation the same weight, regardless of the assessor, product, or term, leads to analyses based on the L1 norm where the median absolute deviation is the measure of dispersion. Five permutation tests are proposed to answer the following questions. Do any products differ? For which terms do products differ? Within each of the terms, which products differ? Which product pairs differ? On which terms does each product pair differ? Additionally, we show how products and terms can be clustered following the "one citation, one vote" principle and how principal component analysis using the L1-norm (L1-PCA) can be applied to visualize CATA results in few dimensions. Together, the permutation tests, clustering methods, and L1-PCA provide a unified approach that provides robust results measured in citation percentages. The proposed methods are illustrated using a data set in which 100 consumers evaluated 11 products using 34 CATA terms. R code is provided to perform the analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15945v2</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carolina Chaya, John Castura, Michael Greenacre</dc:creator>
    </item>
    <item>
      <title>Doubly protected estimation for survival outcomes utilizing external controls for randomized clinical trials</title>
      <link>https://arxiv.org/abs/2410.18409</link>
      <description>arXiv:2410.18409v2 Announce Type: replace-cross 
Abstract: Censored survival data are common in clinical trials, but small control groups can pose challenges, particularly in rare diseases or where balanced randomization is impractical. Recent approaches leverage external controls from historical studies or real-world data to strengthen treatment evaluation for survival outcomes. However, using external controls directly may introduce biases due to data heterogeneity. We propose a doubly protected estimator for the treatment-specific restricted mean survival time difference that is more efficient than trial-only estimators and mitigates biases from external data. Our method adjusts for covariate shifts via doubly robust estimation and addresses outcome drift using the DR-Learner for selective borrowing. The approach can incorporate machine learning to approximate survival curves and detect outcome drifts without strict parametric assumptions, borrowing only comparable external controls. Extensive simulation studies and a real-data application evaluating the efficacy of Galcanezumab in mitigating migraine headaches have been conducted to illustrate the effectiveness of our proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18409v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenyin Gao, Shu Yang, Mingyang Shan, Wenyu Wendy Ye, Ilya Lipkovich, Douglas Faries</dc:creator>
    </item>
    <item>
      <title>Valid Bootstraps for Network Embeddings with Applications to Network Visualisation</title>
      <link>https://arxiv.org/abs/2410.20895</link>
      <description>arXiv:2410.20895v4 Announce Type: replace-cross 
Abstract: Quantifying uncertainty in networks is an important step in modelling relationships and interactions between entities. We consider the challenge of bootstrapping an inhomogeneous random graph when only a single observation of the network is made and the underlying data generating function is unknown. We address this problem by considering embeddings of the observed and bootstrapped network that are statistically indistinguishable. We utilise an exchangeable network test that can empirically validate bootstrap samples generated by any method. Existing methods fail this test, so we propose a principled, distribution-free network bootstrap using k-nearest neighbour smoothing, that can pass this exchangeable network test in many synthetic and real-data scenarios. We demonstrate the utility of this work in combination with the popular data visualisation method t-SNE, where uncertainty estimates from bootstrapping are used to explain whether visible structures represent real statistically sound structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20895v4</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emerald Dilworth, Ed Davis, Daniel J. Lawson</dc:creator>
    </item>
  </channel>
</rss>
