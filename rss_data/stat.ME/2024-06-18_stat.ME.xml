<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Jun 2024 04:00:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 19 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Simultaneous comparisons of the variances of k treatments with that of a control: a Levene-Dunnett type procedure</title>
      <link>https://arxiv.org/abs/2406.11892</link>
      <description>arXiv:2406.11892v1 Announce Type: new 
Abstract: There are some global tests for heterogeneity of variance in k-sample one-way layouts, but few consider pairwise comparisons between treatment levels. For experimental designs with a control, comparisons of the variances between the treatment levels and the control are of interest - in analogy to the location parameter with the Dunnett (1955) procedure. Such a many-to-one approach for variances is proposed using the Levene transformation, a kind of residuals. Its properties are characterized with simulation studies and corresponding data examples are evaluated with R code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11892v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ludwig A. Hothorn</dc:creator>
    </item>
    <item>
      <title>Model-Based Inference and Experimental Design for Interference Using Partial Network Data</title>
      <link>https://arxiv.org/abs/2406.11940</link>
      <description>arXiv:2406.11940v1 Announce Type: new 
Abstract: The stable unit treatment value assumption states that the outcome of an individual is not affected by the treatment statuses of others, however in many real world applications, treatments can have an effect on many others beyond the immediately treated. Interference can generically be thought of as mediated through some network structure. In many empirically relevant situations however, complete network data (required to adjust for these spillover effects) are too costly or logistically infeasible to collect. Partially or indirectly observed network data (e.g., subsamples, aggregated relational data (ARD), egocentric sampling, or respondent-driven sampling) reduce the logistical and financial burden of collecting network data, but the statistical properties of treatment effect adjustments from these design strategies are only beginning to be explored. In this paper, we present a framework for the estimation and inference of treatment effect adjustments using partial network data through the lens of structural causal models. We also illustrate procedures to assign treatments using only partial network data, with the goal of either minimizing estimator variance or optimally seeding. We derive single network asymptotic results applicable to a variety of choices for an underlying graph model. We validate our approach using simulated experiments on observed graphs with applications to information diffusion in India and Malawi.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11940v1</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <category>stat.OT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Steven Wilkins Reeves, Shane Lubold, Arun G. Chandrasekhar, Tyler H. McCormick</dc:creator>
    </item>
    <item>
      <title>Clustering functional data with measurement errors: a simulation-based approach</title>
      <link>https://arxiv.org/abs/2406.11942</link>
      <description>arXiv:2406.11942v1 Announce Type: new 
Abstract: Clustering analysis of functional data, which comprises observations that evolve continuously over time or space, has gained increasing attention across various scientific disciplines. Practical applications often involve functional data that are contaminated with measurement errors arising from imprecise instruments, sampling errors, or other sources. These errors can significantly distort the inherent data structure, resulting in erroneous clustering outcomes. In this paper, we propose a simulation-based approach designed to mitigate the impact of measurement errors. Our proposed method estimates the distribution of functional measurement errors through repeated measurements. Subsequently, the clustering algorithm is applied to simulated data generated from the conditional distribution of the unobserved true functional data given the observed contaminated functional data, accounting for the adjustments made to rectify measurement errors. We illustrate through simulations show that the proposed method has improved numerical performance than the naive methods that neglect such errors. Our proposed method was applied to a childhood obesity study, giving more reliable clustering results</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11942v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tingyu Zhu, Lan Xue, Carmen Tekwe, Keith Diaz, Mark Benden, Roger Zoh</dc:creator>
    </item>
    <item>
      <title>Mixed-resolution hybrid modeling in an element-based framework</title>
      <link>https://arxiv.org/abs/2406.12028</link>
      <description>arXiv:2406.12028v1 Announce Type: new 
Abstract: Computational modeling of a complex system is limited by the parts of the system with the least information. While detailed models and high-resolution data may be available for parts of a system, abstract relationships are often necessary to connect the parts and model the full system. For example, modeling food security necessitates the interaction of climate and socioeconomic factors, with models of system components existing at different levels of information in terms of granularity and resolution. Connecting these models is an ongoing challenge. In this work, we demonstrate methodology to quantize and integrate information from data and detailed component models alongside abstract relationships in a hybrid element-based modeling and simulation framework. In a case study of modeling food security, we apply quantization methods to generate (1) time-series model input from climate data and (2) a discrete representation of a component model (a statistical emulator of crop yield), which we then incorporate as an update rule in the hybrid element-based model, bridging differences in model granularity and resolution. Simulation of the hybrid element-based model recapitulated the trends of the original emulator, supporting the use of this methodology to integrate data and information from component models to simulate complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12028v1</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kara Bocan, Natasa Miskov-Zivanov</dc:creator>
    </item>
    <item>
      <title>Model Selection for Causal Modeling in Missing Exposure Problems</title>
      <link>https://arxiv.org/abs/2406.12171</link>
      <description>arXiv:2406.12171v1 Announce Type: new 
Abstract: In causal inference, properly selecting the propensity score (PS) model is a popular topic and has been widely investigated in observational studies. In addition, there is a large literature concerning the missing data problem. However, there are very few studies investigating the model selection issue for causal inference when the exposure is missing at random (MAR). In this paper, we discuss how to select both imputation and PS models, which can result in the smallest RMSE of the estimated causal effect. Then, we provide a new criterion, called the ``rank score" for evaluating the overall performance of both models. The simulation studies show that the full imputation plus the outcome-related PS models lead to the smallest RMSE and the rank score can also pick the best models. An application study is conducted to study the causal effect of CVD on the mortality of COVID-19 patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12171v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuliang Shi, Yeying Zhu, Joel A. Dubin</dc:creator>
    </item>
    <item>
      <title>Lasso regularization for mixture experiments with noise variables</title>
      <link>https://arxiv.org/abs/2406.12237</link>
      <description>arXiv:2406.12237v1 Announce Type: new 
Abstract: We apply classical and Bayesian lasso regularizations to a family of models with the presence of mixture and process variables. We analyse the performance of these estimates with respect to ordinary least squares estimators by a simulation study and a real data application. Our results demonstrate the superior performance of Bayesian lasso, particularly via coordinate ascent variational inference, in terms of variable selection accuracy and response optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12237v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manuel Gonz\'alez-Navarrete, Fabi\'an Manr\'iquez-M\'endez, Manuel Pereira-Barahona</dc:creator>
    </item>
    <item>
      <title>Bayesian Consistency for Long Memory Processes: A Semiparametric Perspective</title>
      <link>https://arxiv.org/abs/2406.12780</link>
      <description>arXiv:2406.12780v1 Announce Type: new 
Abstract: In this work, we will investigate a Bayesian approach to estimating the parameters of long memory models. Long memory, characterized by the phenomenon of hyperbolic autocorrelation decay in time series, has garnered significant attention. This is because, in many situations, the assumption of short memory, such as the Markovianity assumption, can be deemed too restrictive. Applications for long memory models can be readily found in fields such as astronomy, finance, and environmental sciences. However, current parametric and semiparametric approaches to modeling long memory present challenges, particularly in the estimation process.
  In this study, we will introduce various methods applied to this problem from a Bayesian perspective, along with a novel semiparametric approach for deriving the posterior distribution of the long memory parameter. Additionally, we will establish the asymptotic properties of the model. An advantage of this approach is that it allows to implement state-of-the-art efficient algorithms for nonparametric Bayesian models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12780v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Clara Grazian</dc:creator>
    </item>
    <item>
      <title>Intrinsic Modeling of Shape-Constrained Functional Data, With Applications to Growth Curves and Activity Profiles</title>
      <link>https://arxiv.org/abs/2406.12817</link>
      <description>arXiv:2406.12817v1 Announce Type: new 
Abstract: Shape-constrained functional data encompass a wide array of application fields especially in the life sciences, such as activity profiling, growth curves, healthcare and mortality. Most existing methods for general functional data analysis often ignore that such data are subject to inherent shape constraints, while some specialized techniques rely on strict distributional assumptions. We propose an approach for modeling such data that harnesses the intrinsic geometry of functional trajectories by decomposing them into size and shape components. We focus on the two most prevalent shape constraints, positivity and monotonicity, and develop individual-level estimators for the size and shape components. Furthermore, we demonstrate the applicability of our approach by conducting subsequent analyses involving Fr\'{e}chet mean and Fr\'{e}chet regression and establish rates of convergence for the empirical estimators. Illustrative examples include simulations and data applications for activity profiles for Mediterranean fruit flies during their entire lifespan and for data from the Z\"{u}rich longitudinal growth study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12817v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Poorbita Kundu, Hans-Georg M\"uller</dc:creator>
    </item>
    <item>
      <title>Identifying Genetic Variants for Obesity Incorporating Prior Insights: Quantile Regression with Insight Fusion for Ultra-high Dimensional Data</title>
      <link>https://arxiv.org/abs/2406.12212</link>
      <description>arXiv:2406.12212v1 Announce Type: cross 
Abstract: Obesity is widely recognized as a critical and pervasive health concern. We strive to identify important genetic risk factors from hundreds of thousands of single nucleotide polymorphisms (SNPs) for obesity. We propose and apply a novel Quantile Regression with Insight Fusion (QRIF) approach that can integrate insights from established studies or domain knowledge to simultaneously select variables and modeling for ultra-high dimensional genetic data, focusing on high conditional quantiles of body mass index (BMI) that are of most interest. We discover interesting new SNPs and shed new light on a comprehensive view of the underlying genetic risk factors for different levels of BMI. This may potentially pave the way for more precise and targeted treatment strategies. The QRIF approach intends to balance the trade-off between the prior insights and the observed data while being robust to potential false information. We further establish the desirable asymptotic properties under the challenging non-differentiable check loss functions via Huber loss approximation and nonconvex SCAD penalty via local linear approximation. Finally, we develop an efficient algorithm for the QRIF approach. Our simulation studies further demonstrate its effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12212v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiantong Wang, Heng Lian, Yan Yu, Heping Zhang</dc:creator>
    </item>
    <item>
      <title>Exploring Intra and Inter-language Consistency in Embeddings with ICA</title>
      <link>https://arxiv.org/abs/2406.12474</link>
      <description>arXiv:2406.12474v1 Announce Type: cross 
Abstract: Word embeddings represent words as multidimensional real vectors, facilitating data analysis and processing, but are often challenging to interpret. Independent Component Analysis (ICA) creates clearer semantic axes by identifying independent key features. Previous research has shown ICA's potential to reveal universal semantic axes across languages. However, it lacked verification of the consistency of independent components within and across languages. We investigated the consistency of semantic axes in two ways: both within a single language and across multiple languages. We first probed into intra-language consistency, focusing on the reproducibility of axes by performing ICA multiple times and clustering the outcomes. Then, we statistically examined inter-language consistency by verifying those axes' correspondences using statistical tests. We newly applied statistical methods to establish a robust framework that ensures the reliability and universality of semantic axes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12474v1</guid>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rongzhi Li, Takeru Matsuda, Hitomi Yanaka</dc:creator>
    </item>
    <item>
      <title>Statistical significance revisited</title>
      <link>https://arxiv.org/abs/2104.00262</link>
      <description>arXiv:2104.00262v3 Announce Type: replace 
Abstract: Statistical significance measures the reliability of a result obtained from a random experiment. We investigate the number of repetitions needed for a statistical result to have a certain significance. In the first step, we consider binomially distributed variables in the example of medication testing with fixed placebo efficacy, asking how many experiments are needed in order to achieve a significance of 95 %. In the next step, we take the probability distribution of the placebo efficacy into account, which to the best of our knowledge has not been done so far. Depending on the specifics, we show that in order to obtain identical significance, it may be necessary to perform twice as many experiments than in a setting where the placebo distribution is neglected. We proceed by considering more general probability distributions and close with comments on some erroneous assumptions on probability distributions which lead, for instance, to a trivial explanation of the fat tail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2104.00262v3</guid>
      <category>stat.ME</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.20944/preprints202103.0398.v1</arxiv:DOI>
      <dc:creator>Maike Torm\"ahlen, Galiya Klinkova, Michael Grabinski</dc:creator>
    </item>
    <item>
      <title>Statistical Principles for Platform Trials</title>
      <link>https://arxiv.org/abs/2302.12728</link>
      <description>arXiv:2302.12728v4 Announce Type: replace 
Abstract: While within a clinical study there may be multiple doses and endpoints, across different studies each study will result in either an approval or a lack of approval of the drug compound studied. The term False Approval Rate (FAR) is the term this paper utilizes to represent the proportion of drug compounds that lack efficacy incorrectly approved by regulators. (In the U.S., compounds that have efficacy and are approved are not involved in the FAR consideration, according to our reading of the relevant U.S. Congressional statute).
  While Tukey's (1953) Error Rate Familywise (ERFw) is meant to be applied within a clinical study, Tukey's (1953) Error Rate per Family (ERpF), defined along-side ERFw, is meant to be applied across studies. We show that controlling Error Rate Familywise (ERFw) within a clinical study at 5% in turn controls Error Rate per Family (ERpF) across studies at 5-per-100, regardless of whether the studies are correlated or not. Further, we show that ongoing regulatory practice, the additive multiplicity adjustment method of controlling ERpF, is controlling False Approval Rate (FAR) exactly (not conservatively) at 5-per-100 (even for Platform trials).
  In contrast, if a regulatory agency chooses to control the False Discovery Rate (FDR) across studies at 5% instead, then this change in policy from ERpF control to FDR control will result in incorrectly approving drug compounds that lack efficacy at a rate higher than 5-per-100, because in essence it gives the industry additional rewards for successfully developing compounds that have efficacy and are approved. Seems to us the discussion of such a change in policy would be at a level higher than merely statistical, needing harmonizsation/harmonization (In the U.S., policy is set by the Congress).</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.12728v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinping Cui, Emily Ouyang, Yi Liu, Jingjing Schneider, Hong Tian, Bushi Wang, Jason C. Hsu</dc:creator>
    </item>
    <item>
      <title>Neural Bayes estimators for censored inference with peaks-over-threshold models</title>
      <link>https://arxiv.org/abs/2306.15642</link>
      <description>arXiv:2306.15642v4 Announce Type: replace 
Abstract: Making inference with spatial extremal dependence models can be computationally burdensome since they involve intractable and/or censored likelihoods. Building on recent advances in likelihood-free inference with neural Bayes estimators, that is, neural networks that approximate Bayes estimators, we develop highly efficient estimators for censored peaks-over-threshold models that {use data augmentation techniques} to encode censoring information in the neural network {input}. Our new method provides a paradigm shift that challenges traditional censored likelihood-based inference methods for spatial extremal dependence models. Our simulation studies highlight significant gains in both computational and statistical efficiency, relative to competing likelihood-based approaches, when applying our novel estimators to make inference with popular extremal dependence models, such as max-stable, $r$-Pareto, and random scale mixture process models. We also illustrate that it is possible to train a single neural Bayes estimator for a general censoring level, precluding the need to retrain the network when the censoring level is changed. We illustrate the efficacy of our estimators by making fast inference on hundreds-of-thousands of high-dimensional spatial extremal dependence models to assess extreme particulate matter 2.5 microns or less in diameter (${\rm PM}_{2.5}$) concentration over the whole of Saudi Arabia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15642v4</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jordan Richards, Matthew Sainsbury-Dale, Andrew Zammit-Mangion, Rapha\"el Huser</dc:creator>
    </item>
    <item>
      <title>Polynomial Chaos Surrogate Construction for Random Fields with Parametric Uncertainty</title>
      <link>https://arxiv.org/abs/2311.00553</link>
      <description>arXiv:2311.00553v2 Announce Type: replace 
Abstract: Engineering and applied science rely on computational experiments to rigorously study physical systems. The mathematical models used to probe these systems are highly complex, and sampling-intensive studies often require prohibitively many simulations for acceptable accuracy. Surrogate models provide a means of circumventing the high computational expense of sampling such complex models. In particular, polynomial chaos expansions (PCEs) have been successfully used for uncertainty quantification studies of deterministic models where the dominant source of uncertainty is parametric. We discuss an extension to conventional PCE surrogate modeling to enable surrogate construction for stochastic computational models that have intrinsic noise in addition to parametric uncertainty. We develop a PCE surrogate on a joint space of intrinsic and parametric uncertainty, enabled by Rosenblatt transformations, and then extend the construction to random field data via the Karhunen-Loeve expansion. We then take advantage of closed-form solutions for computing PCE Sobol indices to perform a global sensitivity analysis of the model which quantifies the intrinsic noise contribution to the overall model output variance. Additionally, the resulting joint PCE is generative in the sense that it allows generating random realizations at any input parameter setting that are statistically approximately equivalent to realizations from the underlying stochastic model. The method is demonstrated on a chemical catalysis example model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00553v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joy N. Mueller, Khachik Sargsyan, Craig J. Daniels, Habib N. Najm</dc:creator>
    </item>
    <item>
      <title>Perturbation-based Effect Measures for Compositional Data</title>
      <link>https://arxiv.org/abs/2311.18501</link>
      <description>arXiv:2311.18501v2 Announce Type: replace 
Abstract: Existing effect measures for compositional features are inadequate for many modern applications for two reasons. First, modern datasets with compositional covariates, for example in microbiome research, display traits such as high-dimensionality and sparsity that can be poorly modelled with traditional parametric approaches. Second, assessing -- in an unbiased way -- how summary statistics of a composition (e.g., racial diversity) affect a response variable is not straightforward. In this work, we propose a framework based on hypothetical data perturbations that addresses both issues. Unlike many existing effect measures for compositional features, we do not define our effects based on a parametric model or a transformation of the data. Instead, we use perturbations to define interpretable statistical functionals on the compositions themselves, which we call average perturbation effects. These effects naturally account for confounding that biases frequently used marginal dependence analyses. We show how average perturbation effects can be estimated efficiently by deriving a perturbation-dependent reparametrization and applying semiparametric estimation techniques. We analyze the proposed estimators empirically on simulated and semi-synthetic data and demonstrate advantages over existing techniques on data from New York schools and microbiome data. For all proposed estimators, we provide confidence intervals with uniform asymptotic coverage guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18501v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anton Rask Lundborg, Niklas Pfister</dc:creator>
    </item>
    <item>
      <title>Valid Cross-Covariance Models via Multivariate Mixtures with an Application to the Confluent Hypergeometric Class</title>
      <link>https://arxiv.org/abs/2312.05682</link>
      <description>arXiv:2312.05682v2 Announce Type: replace 
Abstract: Modeling of multivariate random fields through Gaussian processes calls for the construction of valid cross-covariance functions describing the dependence between any two component processes at different spatial locations. The required validity conditions often present challenges that lead to complicated restrictions on the parameter space. The purpose of this work is to present techniques using multivariate mixtures for establishing validity that are simultaneously simplified and comprehensive. This is accomplished using results on conditionally negative semidefinite matrices and the Schur product theorem. For illustration, we use the recently-introduced Confluent Hypergeometric (CH) class of covariance functions. In addition, we establish the spectral density of the Confluent Hypergeometric covariance and use this to construct valid multivariate models as well as propose new cross-covariances. Our approach leads to valid multivariate cross-covariance models that inherit the desired marginal properties of the Confluent Hypergeometric model and outperform the multivariate Mat\'ern model in out-of-sample prediction under slowly-decaying correlation of the underlying multivariate random field. We also establish properties of the new models, including results on equivalence of Gaussian measures. We demonstrate the new model's use for multivariate oceanography dataset consisting of temperature, salinity and oxygen, as measured by autonomous floats in the Southern Ocean.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.05682v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Drew Yarger, Anindya Bhadra</dc:creator>
    </item>
    <item>
      <title>High-dimensional Outlier Detection via Stability</title>
      <link>https://arxiv.org/abs/2401.14359</link>
      <description>arXiv:2401.14359v3 Announce Type: replace 
Abstract: The Minimum Covariance Determinant (MCD) method is a widely adopted tool for robust estimation and outlier detection. In this paper, we introduce a new framework for model selection in MCD with spectral embedding based on the notion of stability. Our best subset algorithm leverages principal component analysis for dimension reduction, statistical depths for effective initialization, and concentration steps for subset refinement. Subsequently, we construct a bootstrap procedure to estimate the instability of the best subset algorithm. The parameter combination exhibiting minimal instability proves ideal for the purposes of high-dimensional outlier detection, while the instability path offers insights into the inlier/outlier structure. We rigorously benchmark the proposed framework against existing MCD variants and illustrate its practical utility on two spectra data sets and a cancer genomics data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14359v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiang Heng, Hui Shen, Kenneth Lange</dc:creator>
    </item>
    <item>
      <title>Estimating the linear relation between variables that are never jointly observed</title>
      <link>https://arxiv.org/abs/2403.00140</link>
      <description>arXiv:2403.00140v3 Announce Type: replace 
Abstract: In modern experimental science there is a commonly encountered problem of estimating the coefficients of a linear regression in the context where the variables of interest can never be observed simultaneously. Assuming that the global experiment can be decomposed into sub-experiments with distinct first moments, we propose two estimators of the linear regression that take this additional information into account. We consider an estimator based on moments, and an estimator based on optimal transport theory. These estimators are proven to be consistent as well as asymptotically Gaussian under weak hypotheses. The asymptotic variance has no explicit expression, except in some particular cases, for which reason a stratified bootstrap approach is developed to build confidence intervals for the estimated parameters, whose consistency is also shown. A simulation study, assessing and comparing the finite sample performances of these estimators, demonstrated the advantages of the bootstrap approach in multiple realistic scenarios. An application to in vivo experiments, conducted in the context of studying radio-induced adverse effects on mice, revealed important relationships between the biomarkers of interest that could not be identified with the considered naive approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00140v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Polina Arsenteva, Mohamed Amine Benadjaoud, Herv\'e Cardot</dc:creator>
    </item>
    <item>
      <title>Bridging Binarization: Causal Inference with Dichotomized Continuous Treatments</title>
      <link>https://arxiv.org/abs/2405.07109</link>
      <description>arXiv:2405.07109v2 Announce Type: replace 
Abstract: The average treatment effect (ATE) is a common parameter estimated in causal inference literature, but it is only defined for binary treatments. Thus, despite concerns raised by some researchers, many studies seeking to estimate the causal effect of a continuous treatment create a new binary treatment variable by dichotomizing the continuous values into two categories. In this paper, we affirm binarization as a statistically valid method for answering causal questions about continuous treatments by showing the equivalence between the binarized ATE and the difference in the average outcomes of two specific modified treatment policies. These policies impose cut-offs corresponding to the binarized treatment variable and assume preservation of relative self-selection. Relative self-selection is the ratio of the probability density of an individual having an exposure equal to one value of the continuous treatment variable versus another. The policies assume that, for any two values of the treatment variable with non-zero probability density after the cut-off, this ratio will remain unchanged. Through this equivalence, we clarify the assumptions underlying binarization and discuss how to properly interpret the resulting estimator. Additionally, we introduce a new target parameter that can be computed after binarization that considers the status-quo world. We argue that this parameter addresses more relevant causal questions than the traditional binarized ATE parameter. Finally, we present a simulation study to illustrate the implications of these assumptions when analyzing data and to demonstrate how to correctly implement estimators of the parameters discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07109v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaitlyn J. Lee, Alan Hubbard, Alejandro Schuler</dc:creator>
    </item>
    <item>
      <title>Identifying Sample Size and Accuracy and Precision of the Estimators in Case-Crossover Designs with Distributed Lags of Heteroskedastic Time-Varying Continuous Exposures Measured with Simple or Complex Error</title>
      <link>https://arxiv.org/abs/2406.02369</link>
      <description>arXiv:2406.02369v3 Announce Type: replace 
Abstract: Understanding of sample size, statistical power, and the accuracy and precision of the estimator in epidemiological research can facilitate power and bias analyses. However, such understanding can become complicated for several reasons. First, exposures varying spatiotemporally may be heteroskedastic. Second, distributed lags of exposures may be used to identify critical exposure time-windows. Third, exposure measurement error may exist, impacting the accuracy and/or precision of the estimator that consequently affects sample size and statistical power. Fourth, research may rely on different study designs, so understanding may differ. For example, case-crossover designs as matched case-control designs, are used to estimate health effects of short-term exposures. To address these gaps, I developed approximation equations for sample size, estimates of the estimators and standard errors, including polynomials for non-linear effect estimation. With air pollution exposure estimates, I examined approximations using statistical simulations. Overall, sample size, the accuracy and precision of the estimators can be approximated based on external information about validation, without validation data in hand. For distributed lags, approximations may perform well if residual confounding due to covariate measurement errors is not severe. This condition may be difficult to identify without validation data, so validation research is recommended in identifying critical exposure time-windows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02369v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Honghyok Kim</dc:creator>
    </item>
    <item>
      <title>Numerically robust square root implementations of statistical linear regression filters and smoothers</title>
      <link>https://arxiv.org/abs/2406.05188</link>
      <description>arXiv:2406.05188v2 Announce Type: replace 
Abstract: In this article, square-root formulations of the statistical linear regression filter and smoother are developed. Crucially, the method uses QR decompositions rather than Cholesky downdates. This makes the method inherently more numerically robust than the downdate based methods, which may fail in the face of rounding errors. This increased robustness is demonstrated in an ill-conditioned problem, where it is compared against a reference implementation in both double and single precision arithmetic. The new implementation is found to be more robust, when implemented in lower precision arithmetic as compared to the alternative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05188v2</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Filip Tronarp</dc:creator>
    </item>
    <item>
      <title>Orthogonal and Linear Regressions and Pencils of Confocal Quadrics</title>
      <link>https://arxiv.org/abs/2209.01679</link>
      <description>arXiv:2209.01679v3 Announce Type: replace-cross 
Abstract: This paper enhances and develops bridges between statistics, mechanics, and geometry. For a given system of points in $\mathbb R^k$ representing a sample of full rank, we construct an explicit pencil of confocal quadrics with the following properties: (i) All the hyperplanes for which the hyperplanar moments of inertia for the given system of points are equal, are tangent to the same quadrics from the pencil of quadrics. As an application, we develop regularization procedures for the orthogonal least square method, analogues of lasso and ridge methods from linear regression. (ii) For any given point $P$ among all the hyperplanes that contain it, the best fit is the tangent hyperplane to the quadric from the confocal pencil corresponding to the maximal Jacobi coordinate of the point $P$; the worst fit among the hyperplanes containing $P$ is the tangent hyperplane to the ellipsoid from the confocal pencil that contains $P$. The confocal pencil of quadrics provides a universal tool to solve the restricted principal component analysis restricted at any given point. Both results (i) and (ii) can be seen as generalizations of the classical result of Pearson on orthogonal regression. They have natural and important applications in the statistics of the errors-in-variables models (EIV). For the classical linear regressions we provide a geometric characterization of hyperplanes of least squares in a given direction among all hyperplanes which contain a given point. The obtained results have applications in restricted regressions, both ordinary and orthogonal ones. For the latter, a new formula for test statistic is derived. The developed methods and results are illustrated in natural statistics examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.01679v3</guid>
      <category>math.AG</category>
      <category>math.DS</category>
      <category>math.ST</category>
      <category>nlin.SI</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladimir Dragovi\'c, Borislav Gaji\'c</dc:creator>
    </item>
    <item>
      <title>SUrvival Control Chart EStimation Software in R: the success package</title>
      <link>https://arxiv.org/abs/2302.07658</link>
      <description>arXiv:2302.07658v2 Announce Type: replace-cross 
Abstract: Monitoring the quality of statistical processes has been of great importance, mostly in industrial applications. Control charts are widely used for this purpose, but often lack the possibility to monitor survival outcomes. Recently, inspecting survival outcomes has become of interest, especially in medical settings where outcomes often depend on risk factors of patients. For this reason many new survival control charts have been devised and existing ones have been extended to incorporate survival outcomes. The R package success allows users to construct risk-adjusted control charts for survival data. Functions to determine control chart parameters are included, which can be used even without expert knowledge on the subject of control charts. The package allows to create static as well as interactive charts, which are built using ggplot2 (Wickham 2016) and plotly (Sievert 2020).</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.07658v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Gomon, Marta Fiocco, Hein Putter, Mirko Signorelli</dc:creator>
    </item>
    <item>
      <title>Fast post-process Bayesian inference with Variational Sparse Bayesian Quadrature</title>
      <link>https://arxiv.org/abs/2303.05263</link>
      <description>arXiv:2303.05263v2 Announce Type: replace-cross 
Abstract: In applied Bayesian inference scenarios, users may have access to a large number of pre-existing model evaluations, for example from maximum-a-posteriori (MAP) optimization runs. However, traditional approximate inference techniques make little to no use of this available information. We propose the framework of post-process Bayesian inference as a means to obtain a quick posterior approximation from existing target density evaluations, with no further model calls. Within this framework, we introduce Variational Sparse Bayesian Quadrature (VSBQ), a method for post-process approximate inference for models with black-box and potentially noisy likelihoods. VSBQ reuses existing target density evaluations to build a sparse Gaussian process (GP) surrogate model of the log posterior density function. Subsequently, we leverage sparse-GP Bayesian quadrature combined with variational inference to achieve fast approximate posterior inference over the surrogate. We validate our method on challenging synthetic scenarios and real-world applications from computational neuroscience. The experiments show that VSBQ builds high-quality posterior approximations by post-processing existing optimization traces, with no further model evaluations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.05263v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengkun Li, Gr\'egoire Clart\'e, Martin J{\o}rgensen, Luigi Acerbi</dc:creator>
    </item>
    <item>
      <title>An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek</title>
      <link>https://arxiv.org/abs/2311.00541</link>
      <description>arXiv:2311.00541v4 Announce Type: replace-cross 
Abstract: Word meanings change over time, and word senses evolve, emerge or die out in the process. For ancient languages, where the corpora are often small and sparse, modelling such changes accurately proves challenging, and quantifying uncertainty in sense-change estimates consequently becomes important. GASC (Genre-Aware Semantic Change) and DiSC (Diachronic Sense Change) are existing generative models that have been used to analyse sense change for target words from an ancient Greek text corpus, using unsupervised learning without the help of any pre-training. These models represent the senses of a given target word such as ``kosmos'' (meaning decoration, order or world) as distributions over context words, and sense prevalence as a distribution over senses. The models are fitted using Markov Chain Monte Carlo (MCMC) methods to measure temporal changes in these representations. This paper introduces EDiSC, an Embedded DiSC model, which combines word embeddings with DiSC to provide superior model performance. It is shown empirically that EDiSC offers improved predictive accuracy, ground-truth recovery and uncertainty quantification, as well as better sampling efficiency and scalability properties with MCMC methods. The challenges of fitting these models are also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00541v4</guid>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Schyan Zafar, Geoff K. Nicholls</dc:creator>
    </item>
    <item>
      <title>When Graph Neural Network Meets Causality: Opportunities, Methodologies and An Outlook</title>
      <link>https://arxiv.org/abs/2312.12477</link>
      <description>arXiv:2312.12477v3 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) have emerged as powerful representation learning tools for capturing complex dependencies within diverse graph-structured data. Despite their success in a wide range of graph mining tasks, GNNs have raised serious concerns regarding their trustworthiness, including susceptibility to distribution shift, biases towards certain populations, and lack of explainability. Recently, integrating causal learning techniques into GNNs has sparked numerous ground-breaking studies since many GNN trustworthiness issues can be alleviated by capturing the underlying data causality rather than superficial correlations. In this survey, we comprehensively review recent research efforts on Causality-Inspired GNNs (CIGNNs). Specifically, we first employ causal tools to analyze the primary trustworthiness risks of existing GNNs, underscoring the necessity for GNNs to comprehend the causal mechanisms within graph data. Moreover, we introduce a taxonomy of CIGNNs based on the type of causal learning capability they are equipped with, i.e., causal reasoning and causal representation learning. Besides, we systematically introduce typical methods within each category and discuss how they mitigate trustworthiness risks. Finally, we summarize useful resources and discuss several future directions, hoping to shed light on new research opportunities in this emerging field. The representative papers, along with open-source data and codes, are available in https://github.com/usail-hkust/Causality-Inspired-GNNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12477v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenzhao Jiang, Hao Liu, Hui Xiong</dc:creator>
    </item>
    <item>
      <title>Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models</title>
      <link>https://arxiv.org/abs/2402.15301</link>
      <description>arXiv:2402.15301v2 Announce Type: replace-cross 
Abstract: Causal graph recovery is traditionally done using statistical estimation-based methods or based on individual's knowledge about variables of interests. They often suffer from data collection biases and limitations of individuals' knowledge. The advance of large language models (LLMs) provides opportunities to address these problems. We propose a novel method that leverages LLMs to deduce causal relationships in general causal graph recovery tasks. This method leverages knowledge compressed in LLMs and knowledge LLMs extracted from scientific publication database as well as experiment data about factors of interest to achieve this goal. Our method gives a prompting strategy to extract associational relationships among those factors and a mechanism to perform causality verification for these associations. Comparing to other LLM-based methods that directly instruct LLMs to do the highly complex causal reasoning, our method shows clear advantage on causal graph quality on benchmark datasets. More importantly, as causality among some factors may change as new research results emerge, our method show sensitivity to new evidence in the literature and can provide useful information for updating causal graphs accordingly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15301v2</guid>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuzhe Zhang, Yipeng Zhang, Yidong Gan, Lina Yao, Chen Wang</dc:creator>
    </item>
    <item>
      <title>Teleporter Theory: A General and Simple Approach for Modeling Cross-World Counterfactual Causality</title>
      <link>https://arxiv.org/abs/2406.11501</link>
      <description>arXiv:2406.11501v2 Announce Type: replace-cross 
Abstract: Leveraging the development of structural causal model (SCM), researchers can establish graphical models for exploring the causal mechanisms behind machine learning techniques. As the complexity of machine learning applications rises, single-world interventionism causal analysis encounters theoretical adaptation limitations. Accordingly, cross-world counterfactual approach extends our understanding of causality beyond observed data, enabling hypothetical reasoning about alternative scenarios. However, the joint involvement of cross-world variables, encompassing counterfactual variables and real-world variables, challenges the construction of the graphical model. Twin network is a subtle attempt, establishing a symbiotic relationship, to bridge the gap between graphical modeling and the introduction of counterfactuals albeit with room for improvement in generalization. In this regard, we demonstrate the theoretical breakdowns of twin networks in certain cross-world counterfactual scenarios. To this end, we propose a novel teleporter theory to establish a general and simple graphical representation of counterfactuals, which provides criteria for determining teleporter variables to connect multiple worlds. In theoretical application, we determine that introducing the proposed teleporter theory can directly obtain the conditional independence between counterfactual variables and real-world variables from the cross-world SCM without requiring complex algebraic derivations. Accordingly, we can further identify counterfactual causal effects through cross-world symbolic derivation. We demonstrate the generality of the teleporter theory to the practical application. Adhering to the proposed theory, we build a plug-and-play module, and the effectiveness of which are substantiated by experiments on benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11501v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiangmeng Li, Bin Qin, Qirui Ji, Yi Li, Wenwen Qiang, Jianwen Cao, Fanjiang Xu</dc:creator>
    </item>
  </channel>
</rss>
