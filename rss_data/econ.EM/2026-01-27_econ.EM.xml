<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 28 Jan 2026 02:58:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Curious Case of Aid and Conflict: Causal Evidence from Panel Econometrics and Composite Indices</title>
      <link>https://arxiv.org/abs/2601.16992</link>
      <description>arXiv:2601.16992v1 Announce Type: new 
Abstract: This paper examines the relationship between Official Development Assistance (ODA) and conflict in the ten largest aid-receiving African countries between 2009 and 2023. Using Ordinary Least Squares, Principal Component Analysis, and Ridge (L2) regression, the study assesses whether conflict, proxied by political stability, governance indicators, and macroeconomic conditions, systematically influences aid inflows. Results reveal a nuanced relationship. Pooled regressions indicate that aid is positively associated with poverty, inflation, and fragility, while voice and accountability are negatively related to ODA. Fixed-effects estimates instead show positive associations between aid, political stability, and GDP per capita over time, alongside negative correlations with perceived corruption. Ridge regression confirms the robustness of various governance variables under multicollinearity. Overall, donors appear responsive to both humanitarian need and institutional quality, producing an aid-conflict-institutions trilemma: aid is most concentrated where conflict risk and institutional weakness are greatest, yet these same conditions which constrain aid effectiveness. The paper contributes by integrating theory with panel-econometric tools to to explore international development aid allocation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.16992v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Muhammad Usman Anwar Goraya</dc:creator>
    </item>
    <item>
      <title>Decomposition of Brazil's 5-year DI Futures in Basis Points</title>
      <link>https://arxiv.org/abs/2601.16995</link>
      <description>arXiv:2601.16995v1 Announce Type: new 
Abstract: This paper proposes an empirical, replicable, and interpretable framework to decompose, in basis points (bps), daily changes in Brazil's 5-year DI futures rate (DI5Y). The approach combines three building blocks: (i) macroeconomic and fiscal expectations from the Central Bank of Brazil Focus survey, converted into daily changes; (ii) a supervised macro factor built with Partial Least Squares (PLS) that summarizes changes in expectations together with a high-frequency macro "surprise" indicator; and (iii) a decomposition of sovereign risk using Brazil CDS into global and domestic components, obtained by regressing CDS on external financial conditions (DXY, CRB, VIX, and the US 10-year yield). The final step maps these drivers into daily bps contributions through a linear regression of the daily change in DI5Y on the three factors, producing a cumulative decomposition that adds up with an intercept and a residual. In the final sample (2015-01-13 to 2025-12-12; 2,741 observations), the model explains about 22.45% of the daily variance in DI5Y changes. The explained share is dominated by domestic risk, with a smaller but statistically significant contribution from the macro factor. The residual remains large, highlighting the limits of linearity and omitted drivers such as monetary policy event windows, term premia, liquidity, and positioning. Overall, the framework delivers a transparent accounting of how much of the daily (and cumulative) movement in DI5Y is associated with macro/central bank forces, domestic Brazil risk, and external risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.16995v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel de Macedo Santos</dc:creator>
    </item>
    <item>
      <title>Recovering Counterfactual Distributions via Wasserstein GANs</title>
      <link>https://arxiv.org/abs/2601.17296</link>
      <description>arXiv:2601.17296v1 Announce Type: new 
Abstract: Standard Distributional Synthetic Controls (DSC) estimate counterfactual distributions by minimizing the Euclidean $L_2$ distance between quantile functions. We demonstrate that this geometric reliance renders estimators fragile: they lack informative gradients under support mismatch and produce structural artifacts when outcomes are multimodal. This paper proposes a robust estimator grounded in Optimal Transport (OT). We construct the synthetic control by minimizing the Wasserstein-1 distance between probability measures, implemented via a Wasserstein Generative Adversarial Network (WGAN). We establish the formal point identification of synthetic weights under an affine independence condition on the donor pool. Monte Carlo simulations confirm that while standard estimators exhibit catastrophic variance explosions under heavy-tailed contamination and support mismatch, our WGAN-based approach remains consistent and stable. Furthermore, we show that our measure-based method correctly recovers complex bimodal mixtures where traditional quantile averaging fails structurally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17296v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinran Liu</dc:creator>
    </item>
    <item>
      <title>Statistical Decisions and Partial Identification: With Application to Boundary Discontinuity Design</title>
      <link>https://arxiv.org/abs/2601.17648</link>
      <description>arXiv:2601.17648v1 Announce Type: new 
Abstract: We are delighted to respond to the excellent surveys by Cattaneo et al. (2026) and Hirano (2026). Our discussion will attempt two things: first, we show how statistical decision theory can be applied to situations with partial identification; second, we connect the surveys' themes by applying these insights to an imagined policy experiment in one of Cattaneo et al.'s (2025) applications.
  To do so, we lay out a stylized scenario of statistical decision making under partial identification and, drawing on our own and others' earlier work, provide a complete solution for that scenario. We then apply these results to a hypothetical reduction (modelled on actual policies) in eligibility for educational subsidies. We will see that something of interest can be said, but also that bringing the theory to the application involves some leaps of faith and leaves some questions open. This leads to the final section, where we discuss what we see as the main open challenges in statistical decision theory under partial identification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17648v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chen Qiu, J\"org Stoye</dc:creator>
    </item>
    <item>
      <title>The Proximal Surrogate Index: Long-Term Treatment Effects under Unobserved Confounding</title>
      <link>https://arxiv.org/abs/2601.17712</link>
      <description>arXiv:2601.17712v1 Announce Type: new 
Abstract: We study the identification and estimation of long-term treatment effects under unobserved confounding by combining an experimental sample, where the long-term outcome is missing, with an observational sample, where the treatment assignment is unobserved. While standard surrogate index methods fail when unobserved confounders exist, we establish novel identification results by leveraging proxy variables for the unobserved confounders. We further develop multiply robust estimation and inference procedures based on these results. Applying our method to the Job Corps program, we demonstrate its ability to recover experimental benchmarks even when unobserved confounders bias standard surrogate index estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17712v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ting-Chih Hung, Yu-Chang Chen</dc:creator>
    </item>
    <item>
      <title>Best Feasible Conditional Critical Values for a More Powerful Subvector Anderson-Rubin Test</title>
      <link>https://arxiv.org/abs/2601.17843</link>
      <description>arXiv:2601.17843v1 Announce Type: new 
Abstract: For subvector inference in the linear instrumental variables model under homoskedasticity but allowing for weak instruments, Guggenberger, Kleibergen, and Mavroeidis (2019) (GKM) propose a conditional subvector Anderson and Rubin (1949) (AR) test that uses data-dependent critical values that adapt to the strength of the parameters not under test. This test has correct size and strictly higher power than the test that uses standard asymptotic chi-square critical values. The subvector AR test is the minimum eigenvalue of a data dependent matrix. The GKM critical value function conditions on the largest eigenvalue of this matrix. We consider instead the data dependent critical value function conditioning on the second-smallest eigenvalue, as this eigenvalue is the appropriate indicator for weak identification. We find that the data dependent critical value function of GKM also applies to this conditioning and show that this test has correct size and power strictly higher than the GKM test when the number of parameters not under test is larger than one. Our proposed procedure further applies to the subvector AR test statistic that is robust to an approximate kronecker product structure of conditional heteroskedasticity as proposed by Guggenberger, Kleibergen, and Mavroeidis (2024), carrying over its power advantage to this setting as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17843v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesse Hoekstra, Frank Windmeijer</dc:creator>
    </item>
    <item>
      <title>From annual to quarterly data: challenges and strategies in the estimation of Italian General Government Compensation of employees</title>
      <link>https://arxiv.org/abs/2601.16997</link>
      <description>arXiv:2601.16997v1 Announce Type: cross 
Abstract: This paper addresses the methodology for the quarterly estimation of Compensation of Employees paid by the General Government (GG) sector, in accordance with the European System of Accounts (ESA 2010). Due to the limited high-frequency data availability and the need to guarantee the consistency with annual constraints, quarterly estimation relies on indirect temporal disaggregation techniques. These methods use specific infra-annual indicators as proxies for the variables being estimated. The specific case of the quarterly estimation of Compensation of employees presents several additional challenges. Firstly, the information provided by the sources, based on cash or legal-accrual data, is elaborated to define indicators which respect the accrual ESA 2010 principle as the annual estimates, based on more compliant data sources such as final budgets of public entities. Secondly, at a quarterly level the extraordinary events - such as the recording of delayed collective bargaining agreements which result in arrears - have a strong impact on quarterly indicators, whereas their effect is mitigated at annual level. To attribute these flows to the period when the work is performed, multi-source data harmonization techniques are employed. Thirdly, to accurately reflect intra-annual dynamics, information is collected for specific groups of GG entities (e.g., regions and provinces) and aggregated into ESA 2010 GG sub-sectors (Central Government, Local Government, Social Security Funds) leading to three specific estimates. To validate temporal disaggregation models and ensure methodological rigor and data quality, statistical tests are applied throughout the process. The results confirm the effectiveness of this methodology in providing accurate and timely quarterly estimates of Compensation of employees for the GG sector, thereby supporting reliable short-term economic analysis and policy making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.16997v1</guid>
      <category>econ.GN</category>
      <category>econ.EM</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sara Cannavacciuolo, Maria Saiz, Maria Liviana Mattonetti</dc:creator>
    </item>
    <item>
      <title>MarketGANs: Multivariate financial time-series data augmentation using generative adversarial networks</title>
      <link>https://arxiv.org/abs/2601.17773</link>
      <description>arXiv:2601.17773v1 Announce Type: cross 
Abstract: This paper introduces MarketGAN, a factor-based generative framework for high-dimensional asset return generation under severe data scarcity. We embed an explicit asset-pricing factor structure as an economic inductive bias and generate returns as a single joint vector, thereby preserving cross-sectional dependence and tail co-movement alongside inter-temporal dynamics. MarketGAN employs generative adversarial learning with a temporal convolutional network (TCN) backbone, which models stochastic, time-varying factor loadings and volatilities and captures long-range temporal dependence. Using daily returns of large U.S. equities, we find that MarketGAN more closely matches empirical stylized facts of asset returns, including heavy-tailed marginal distributions, volatility clustering, leverage effects, and, most notably, high-dimensional cross-sectional correlation structures and tail co-movement across assets, than conventional factor-model-based bootstrap approaches. In portfolio applications, covariance estimates derived from MarketGAN-generated samples outperform those derived from other methods when factor information is at least weakly informative, demonstrating tangible economic value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17773v1</guid>
      <category>q-fin.ST</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jeonggyu Huh, Seungwon Jeong, Hyun-Gyoon Kim, Hyeng Keun Koo, Byung Hwa Lim</dc:creator>
    </item>
    <item>
      <title>The Hellinger Bounds on the Kullback-Leibler Divergence and the Bernstein Norm</title>
      <link>https://arxiv.org/abs/2601.17860</link>
      <description>arXiv:2601.17860v1 Announce Type: cross 
Abstract: The Kullback-Leibler divergence, the Kullback-Leibler variation, and the Bernstein "norm" are used to quantify discrepancies among probability distributions in likelihood models such as nonparametric maximum likelihood and nonparametric Bayes. They are closely related to the Hellinger distance, which is often easier to work with. Consequently, it is of interest to characterize conditions under which the Hellinger distance serves as an upper bound for these measures. This article characterizes a necessary and sufficient condition for each of the discrepancy measures to be bounded by the Hellinger distance. It accommodates unbounded likelihood ratios and generalizes all previously known results. We then apply it to relax the regularity condition for the sieve maximum likelihood estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17860v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tetsuya Kaji</dc:creator>
    </item>
    <item>
      <title>Threshold Regression in Heterogeneous Panel Data with Interactive Fixed Effects</title>
      <link>https://arxiv.org/abs/2308.04057</link>
      <description>arXiv:2308.04057v3 Announce Type: replace 
Abstract: This paper introduces unit-specific heterogeneity in panel data threshold regression. We develop the asymptotic theory for models with heterogeneous thresholds, heterogeneous slope coefficients, and interactive fixed effects. The estimation methodology employs the Common Correlated Effects approach, which is able to handle heterogeneous parameters while maintaining computational simplicity. We also propose a semi-homogeneous model with heterogeneous slopes but a common threshold, revealing novel mean group estimator convergence rates due to the interaction of heterogeneity with the shrinking threshold assumption. Tests for linearity are provided, as well as a modified information criterion which can select between the fully heterogeneous and semi-homogeneous models. Monte Carlo simulations demonstrate the good performance of the new methods in small samples. The new theory is used to examine the Feldstein-Horioka puzzle, showing that threshold nonlinearity with respect to trade openness occurs only in a small subset of countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.04057v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Barassi (University of Birmingham), Yiannis Karavias (Brunel University of London), Chongxian Zhu (University of Birmingham)</dc:creator>
    </item>
    <item>
      <title>Bounds on Average Effects in Discrete Choice Panel Data Models</title>
      <link>https://arxiv.org/abs/2309.09299</link>
      <description>arXiv:2309.09299v4 Announce Type: replace 
Abstract: In discrete choice panel data, estimation of average effects is crucial for quantifying the effect of covariates, and for policy evaluation and counterfactual analysis. However, in short panels with individual-specific effects, challenges arise due to partial identification and the incidental parameter problem. In particular, estimating the sharp identified set on average effects becomes impractical when covariates have large support sets, such as when they are continuous. This paper proposes a method for estimating outer bounds on the identified set of average effects, which are easy to construct, converge at the parametric rate, and remain computationally feasible even for moderately large samples. Asymptotically valid confidence intervals are also provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.09299v4</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cavit Pakel, Martin Weidner</dc:creator>
    </item>
    <item>
      <title>Wald inference on varying coefficients</title>
      <link>https://arxiv.org/abs/2502.03084</link>
      <description>arXiv:2502.03084v3 Announce Type: replace 
Abstract: We present simple to implement Wald-type statistics that deliver a general nonparametric inference theory for linear restrictions on varying coefficients in a range of regression models allowing for cross-sectional or spatial dependence. We provide a general central limit theorem that covers a broad range of error spatial dependence structures, allows for a degree of misspecification robustness via nonparametric spatial weights and permits inference on both varying regression and spatial dependence parameters. Using our method, we first uncover evidence of constant returns to scale in the Chinese nonmetal mineral industry's production function, and then show that Boston house prices respond nonlinearly to proximity to employment centers. A simulation study confirms that our tests perform very well in finite samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03084v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhimanyu Gupta, Xi Qu, Sorawoot Srisuma, Jiajun Zhang</dc:creator>
    </item>
    <item>
      <title>Efficient Difference-in-Differences Estimation when Outcomes are Missing at Random</title>
      <link>https://arxiv.org/abs/2509.25009</link>
      <description>arXiv:2509.25009v2 Announce Type: replace-cross 
Abstract: The Difference-in-Differences (DiD) method is a fundamental tool for causal inference, yet its application is often complicated by missing data. Although recent work has developed robust DiD estimators for complex settings like staggered treatment adoption, these methods typically assume complete data and fail to address the critical challenge of outcomes that are missing at random (MAR) -- a common problem that invalidates standard estimators. We develop a rigorous framework, rooted in semiparametric theory, for identifying and efficiently estimating the Average Treatment Effect on the Treated (ATT) when either pre- or post-treatment (or both) outcomes are missing at random. We first establish nonparametric identification of the ATT under two minimal sets of sufficient conditions. For each, we derive the semiparametric efficiency bound, which provides a formal benchmark for asymptotic optimality. We then propose novel estimators that are asymptotically efficient, achieving this theoretical bound. A key feature of our estimators is their multiple robustness, which ensures consistency even if some nuisance function models are misspecified. We validate the properties of our estimators and showcase their broad applicability through an extensive simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25009v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Testa, Edward H. Kennedy, Matthew Reimherr</dc:creator>
    </item>
    <item>
      <title>LLM Personas as a Substitute for Field Experiments in Method Benchmarking</title>
      <link>https://arxiv.org/abs/2512.21080</link>
      <description>arXiv:2512.21080v2 Announce Type: replace-cross 
Abstract: Field experiments (A/B tests) are often the most credible benchmark for methods (algorithms) in societal systems, but their cost and latency bottleneck rapid methodological progress. LLM-based persona simulation offers a cheap synthetic alternative, yet it is unclear whether replacing humans with personas preserves the benchmark interface that adaptive methods optimize against. We prove an if-and-only-if characterization: when (i) methods observe only the aggregate outcome (aggregate-only observation) and (ii) evaluation depends only on the submitted artifact and not on the method's identity or provenance (method-blind evaluation), swapping humans for personas is just panel change from the method's point of view, indistinguishable from changing the evaluation population (e.g., New York to Jakarta). Furthermore, we move from validity to usefulness: we define an information-theoretic discriminability of the induced aggregate channel and show that making persona benchmarking as decision-relevant as a field experiment is fundamentally a sample-size question, yielding explicit bounds on the number of independent persona evaluations required to reliably distinguish meaningfully different methods at a chosen resolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21080v2</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enoch Hyunwook Kang</dc:creator>
    </item>
    <item>
      <title>On the Anchoring Effect of Monetary Policy on the Labor Share of Income and the Rationality of Its Setting Mechanism</title>
      <link>https://arxiv.org/abs/2601.13675</link>
      <description>arXiv:2601.13675v2 Announce Type: replace-cross 
Abstract: Modern macroeconomic monetary theory suggests that the labor share of income has effectively become a core macroe-conomic parameter anchored by top policymakers through Open Market Operations (OMO). However, the setting of this parameter remains a subject of intense economic debate. This paper provides a detailed summary of these controversies, analyzes the scope of influence exerted by market agents other than the top policymakers on the labor share, and explores the rationality of its setting mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13675v2</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Li Tuobang</dc:creator>
    </item>
  </channel>
</rss>
