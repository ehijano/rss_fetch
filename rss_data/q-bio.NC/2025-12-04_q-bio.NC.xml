<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Dec 2025 02:33:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Optimal Griffiths Phase in Heterogeneous Human Brain Networks: Brain Criticality Embracing Stability and Flexibility across Individuals</title>
      <link>https://arxiv.org/abs/2512.03409</link>
      <description>arXiv:2512.03409v1 Announce Type: new 
Abstract: A prominent hypothesis in neuroscience proposes that brains achieve optimal performance by operating near a critical point. However, this framework, which often assumes a universal critical point, fails to account for the extensive individual variability observed in neural dynamics and cognitive functions. These variabilities are not noise but rather an inherent manifestation of a fundamental systems-biology principle: the necessary trade-off between robustness and flexibility in human populations. Here, we propose that the Griffiths phase (GP), an extended critical regime synergically induced by two kinds of heterogeneities in brain network region and connectivity, offers a unified framework for brain criticality that better reconciles robustness and flexibility and accounts for individual variability. Using Human Connectome Project data and whole-brain modeling, we demonstrated that the synergic interplay between structural network modularity and regional heterogeneity in local excitability yields biologically viable GP featured with widely extended global excitability ranges, with an embedded optimal point that balances global/local information transmission. Crucially, an individua's position within the GP gives rise to unique global network dynamics, which in turn confer a distinctive cognitive profile via flexible configuration of functional connectivity for segregation, integration, and balance between them. These results establish GP as an evolved adaptive mechanism resolving the robustness-flexibility trade-off, fulfilling diverse cognitive demands through individualized criticality landscapes, providing a new framework of brain criticality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03409v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Kejian Wu, Dante R. Chialvo, Changsong Zhou, Lianchun Yu</dc:creator>
    </item>
    <item>
      <title>Emergent Spatiotemporal Dynamics in Large-Scale Brain Networks with Next Generation Neural Mass Models</title>
      <link>https://arxiv.org/abs/2512.03907</link>
      <description>arXiv:2512.03907v1 Announce Type: new 
Abstract: Understanding the dynamics of large-scale brain models remains a central challenge due to the inherent complexity of these systems. In this work, we explore the emergence of complex spatiotemporal patterns in a large scale-brain model composed of 90 interconnected brain regions coupled through empirically derived anatomical connectivity. An important aspect of our formulation is that the local dynamics of each brain region are described by a next-generation neural mass model, which explicitly captures the macroscopic gamma activity of coupled excitatory and inhibitory neural populations (PING mechanism). We first identify the system's homogeneous states-both resting and oscillatory-and analyze their stability under uniform perturbations. Then, we determine the stability against non-uniform perturbations by obtaining dispersion relations for the perturbation growth rate. This analysis enables us to link unstable directions of the homogeneous solutions to the emergence of rich spatiotemporal patterns, that we characterize by means of Lyapunov exponents and frequency spectrum analysis. Our results show that, compared to previous studies with classical neural mass models, next-generation neural mass models provide a broader dynamical repertoire, both within homogeneous states and in the heterogeneous regime. Additionally, we identify a key role for anatomical connectivity in cross-frequency coupling, allowing for the emergence of gamma oscillations with amplitude modulated by slower rhythms. These findings suggest that such models are not only more biophysically grounded but also particularly well-suited to capture the full complexity of large-scale brain dynamics. Overall, our study advances the analytical understanding of emerging spatiotemporal patterns in whole-brain models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03907v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rosa Maria Delicado, Gemma Huguet, Pau Clusella</dc:creator>
    </item>
    <item>
      <title>Prior preferences in active inference agents: soft, hard, and goal shaping</title>
      <link>https://arxiv.org/abs/2512.03293</link>
      <description>arXiv:2512.03293v1 Announce Type: cross 
Abstract: Active inference proposes expected free energy as an objective for planning and decision-making to adequately balance exploitative and explorative drives in learning agents. The exploitative drive, or what an agent wants to achieve, is formalised as the Kullback-Leibler divergence between a variational probability distribution, updated at each inference step, and a preference probability distribution that indicates what states or observations are more likely for the agent, hence determining the agent's goal in a certain environment. In the literature, the questions of how the preference distribution should be specified and of how a certain specification impacts inference and learning in an active inference agent have been given hardly any attention. In this work, we consider four possible ways of defining the preference distribution, either providing the agents with hard or soft goals and either involving or not goal shaping (i.e., intermediate goals). We compare the performances of four agents, each given one of the possible preference distributions, in a grid world navigation task. Our results show that goal shaping enables the best performance overall (i.e., it promotes exploitation) while sacrificing learning about the environment's transition dynamics (i.e., it hampers exploration).</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03293v1</guid>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Filippo Torresan, Ryota Kanai, Manuel Baltieri</dc:creator>
    </item>
    <item>
      <title>Sleep Modulation: The Challenge of Transitioning from Open Loop to Closed Loop</title>
      <link>https://arxiv.org/abs/2512.03784</link>
      <description>arXiv:2512.03784v1 Announce Type: cross 
Abstract: Sleep disorders have emerged as a critical global health issue, highlighting the urgent need for effective and widely accessible intervention technologies. Non-invasive brain stimulation has garnered attention as it enables direct or indirect modulation of neural activity, thereby promoting sleep enhancement in a safe and unobtrusive manner. This class of approaches is collectively referred to as sleep modulation. To date, the majority of sleep modulation research relies on open-loop paradigms with empirically determined parameters, while achieving individual adaptation and modulation accuracy remains a distant objective. The paradigm-specific constraints inherent to open-loop designs represent a major obstacle to clinical translation and large-scale deployment in home environments. In this paper, we delineate fundamental paradigms of sleep modulation, critically examine the intrinsic limitations of open-loop approaches, and formally conceptualize sleep closed-loop modulation. We further provide a comprehensive synthesis of prior studies involving five commonly employed modulation techniques, evaluating their potential integration within a closed-loop framework. Finally, we identify three primary challenges in constructing an effective sleep closed-loop modulation system: sensor solution selection, monitoring model design, and modulation strategy design, while also proposing potential solutions. Collectively, this work aims to advance the paradigm shift of sleep modulation from open-loop toward closed-loop systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03784v1</guid>
      <category>cs.HC</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guisong Liu, Jiansong Zhang, Yinpei Luo, Guoliang Wei, Shuqing Sun, Shiyang Deng, Pengfei Wei, Nanxi Chen</dc:creator>
    </item>
    <item>
      <title>Kubo-Martin-Schwinger states of Path-structured Flow in Directed Brain Synaptic Networks</title>
      <link>https://arxiv.org/abs/2410.18222</link>
      <description>arXiv:2410.18222v2 Announce Type: replace 
Abstract: The brain's synaptic network, characterized by parallel connections and feedback loops, drives interaction pathways between neurons through a large system with infinitely many degrees of freedom. This system is best modeled by the graph C*-algebra of the underlying directed graph, the Toeplitz-Cuntz-Krieger (TCK) algebra, which captures the diversity of path-structured flow connectivity. Equipped with the gauge action, the TCK algebra defines an {\em algebraic quantum system}, and here we demonstrate that its thermodynamic properties provide a natural framework for describing the dynamic mappings of potential flow pathways within the network. Specifically, the KMS states of this system represent the stationary distributions of a non-Markovian stochastic process with memory decay, capturing how influence propagates along exponentially weighted paths, and yield global statistical measures of neuronal interactions. Applied to the {\em C. elegans} synaptic network, our framework reveals that neurolocomotor neurons emerge as the primary hubs of incoming path-structured flow at inverse temperatures where the entropy of KMS states peaks. This finding aligns with experimental evidence of the foundational role of locomotion in {\em C. elegans} behavior, suggesting that functional centrality may arise from the topological embedding of neurons rather than solely from local physiological properties. Our results highlight the potential of algebraic quantum methods and graph algebras to uncover patterns of functional organization in complex systems and neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18222v2</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>quant-ph</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>El-ka\"ioum M. Moutuou, Habib Benali</dc:creator>
    </item>
    <item>
      <title>Implicit Generative Modeling by Kernel Similarity Matching</title>
      <link>https://arxiv.org/abs/2503.00655</link>
      <description>arXiv:2503.00655v2 Announce Type: replace 
Abstract: Understanding how the brain encodes stimuli has been a fundamental problem in computational neuroscience. Insights into this problem have led to the design and development of artificial neural networks that learn representations by incorporating brain-like learning abilities. Recently, learning representations by capturing similarity between input samples has been studied to tackle this problem. This approach, however, has thus far been used to only learn downstream features from an input and has not been studied in the context of a generative paradigm, where one can map the representations back to the input space, incorporating not only bottom-up interactions (stimuli to latent) but also learning features in a top-down manner (latent to stimuli). We investigate a kernel similarity matching framework for generative modeling. Starting with a modified sparse coding objective for learning representations proposed in prior work, we demonstrate that representation learning in this context is equivalent to maximizing similarity between the input kernel and a latent kernel. We show that an implicit generative model arises from learning the kernel structure in the latent space and show how the framework can be adapted to learn manifold structures, potentially providing insights as to how task representations can be encoded in the brain. To solve the objective, we propose a novel Alternate Direction Method of Multipliers (ADMM) based algorithm and discuss the interpretation of the optimization process. Finally, we discuss how this representation learning problem can lead towards a biologically plausible architecture to learn the model parameters that ties together representation learning using similarity matching (a bottom-up approach) with predictive coding (a top-down approach).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00655v2</guid>
      <category>q-bio.NC</category>
      <category>eess.SP</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shubham Choudhary, Paul Masset, Demba Ba</dc:creator>
    </item>
    <item>
      <title>Identifying multi-compartment Hodgkin-Huxley models with high-density extracellular voltage recordings</title>
      <link>https://arxiv.org/abs/2506.20233</link>
      <description>arXiv:2506.20233v2 Announce Type: replace 
Abstract: Multi-compartment Hodgkin-Huxley models are biophysical models of how electrical signals propagate throughout a neuron, and they form the basis of our knowledge of neural computation at the cellular level. However, these models have many free parameters that must be estimated for each cell, and existing fitting methods rely on intracellular voltage measurements that are highly challenging to obtain in vivo. Recent advances in neural recording technology with high-density probes and arrays enable dense sampling of extracellular voltage from many sites surrounding a neuron, allowing indirect measurement of many compartments of a cell simultaneously. Here, we propose a method for inferring the underlying membrane voltage, biophysical parameters, and the neuron's position relative to the probe, using extracellular measurements alone. We use an Extended Kalman Filter to infer membrane voltage and channel states using efficient, differentiable simulators. Then, we learn the model parameters by maximizing the marginal likelihood using gradient-based methods. We demonstrate the performance of this approach using simulated data and real neuron morphologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20233v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ian Christopher Tanoh, Michael Deistler, Jakob H. Macke, Scott W. Linderman</dc:creator>
    </item>
    <item>
      <title>The generalized Hierarchical Gaussian Filter</title>
      <link>https://arxiv.org/abs/2305.10937</link>
      <description>arXiv:2305.10937v3 Announce Type: replace-cross 
Abstract: Hierarchical Bayesian models of perception and learning feature prominently in contemporary cognitive neuroscience where, for example, they inform computational concepts of mental disorders. This includes predictive coding and hierarchical Gaussian filtering (HGF), which differ in the nature of hierarchical representations. In this work, we present a new class of artificial neural networks that unifies computational principles of PC and HGFs. We extend the space of generative models underlying HGF to include a form of nonlinear hierarchical coupling between state values akin to predictive coding and artificial neural networks in general. We derive the update equations corresponding to this generalization of HGF and conceptualize them as connecting a network of (belief) nodes where parent nodes either predict the state of child nodes or their rate of change. This enables us to (1) create modular architectures with generic computational steps in each node of the network, and (2) disclose the hierarchical message passing implied by generalized HGF models and to compare this to comparable schemes under predictive coding. The practical advances of this work are twofold: on the one hand, our extension allows for a modular construction of ANNs of arbitrarily complex hierarchical structure under the general principles of HGF. On the other hand, by providing a highly flexible implementation of hierarchical Bayesian models available as open source software, it enables new types of empirical data analysis in computational psychiatry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.10937v3</guid>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lilian Aline Weber, Peter Thestrup Waade, Nicolas Legrand, Anna Hedvig M{\o}ller, Klaas Enno Stephan, Christoph Mathys</dc:creator>
    </item>
  </channel>
</rss>
