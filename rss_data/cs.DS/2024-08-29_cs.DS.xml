<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 Aug 2024 04:00:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>When to Give Up on a Parallel Implementation</title>
      <link>https://arxiv.org/abs/2408.16092</link>
      <description>arXiv:2408.16092v1 Announce Type: new 
Abstract: In the Serial Parallel Decision Problem (SPDP), introduced by Kuszmaul and Westover [SPAA'24], an algorithm receives a series of tasks online, and must choose for each between a serial implementation and a parallelizable (but less efficient) implementation. Kuszmaul and Westover describe three decision models: (1) \defn{Instantly-committing} schedulers must decide on arrival, irrevocably, which implementation of the task to run. (2) \defn{Eventually-committing} schedulers can delay their decision beyond a task's arrival time, but cannot revoke their decision once made. (3) \defn{Never-committing} schedulers are always free to abandon their progress on the task and start over using a different implementation. Kuszmaul and Westover gave a simple instantly-committing scheduler whose total completion time is $3$-competitive with the offline optimal schedule. They conjectured that the three decision models should admit different competitive ratios, but left upper bounds below $3$ in any model as an open problem.
  In this paper, we show that the powers of instantly, eventually, and never committing schedulers are distinct, at least in the ``massively parallel regime''. The massively parallel regime of the SPDP is the special case where the number of available processors is asymptotically larger than the number of tasks to process, meaning that the \emph{work} associated with running a task in serial is negligible compared to its \emph{runtime}. In this regime, we show (1) The optimal competitive ratio for instantly-committing schedulers is $2$, (2) The optimal competitive ratio for eventually-committing schedulers lies in $[1.618, 1.678]$, (3) The optimal competitive ratio for never-committing schedulers lies in $[1.366, 1.500]$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16092v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathan S. Sheffield, Alek Westover</dc:creator>
    </item>
    <item>
      <title>Improving Lagarias-Odlyzko Algorithm For Average-Case Subset Sum: Modular Arithmetic Approach</title>
      <link>https://arxiv.org/abs/2408.16108</link>
      <description>arXiv:2408.16108v1 Announce Type: new 
Abstract: Lagarias and Odlyzko (J.~ACM~1985) proposed a polynomial time algorithm for solving ``\emph{almost all}'' instances of the Subset Sum problem with $n$ integers of size $\Omega(\Gamma_{\text{LO}})$, where $\log_2(\Gamma_{\text{LO}}) &gt; n^2 \log_2(\gamma)$ and $\gamma$ is a parameter of the lattice basis reduction ($\gamma &gt; \sqrt{4/3}$ for LLL). The algorithm of Lagarias and Odlyzko is a cornerstone result in cryptography. However, the theoretical guarantee on the density of feasible instances has remained unimproved for almost 40 years.
  In this paper, we propose an algorithm to solve ``almost all'' instances of Subset Sum with integers of size $\Omega(\sqrt{\Gamma_{\text{LO}}})$ after a single call to the lattice reduction. Additionally, our argument allows us to solve the Subset Sum problem for multiple targets while the previous approach could only answer one target per call to lattice basis reduction. We introduce a modular arithmetic approach to the Subset Sum problem. The idea is to use the lattice reduction to solve a linear system modulo a suitably large prime. We show that density guarantees can be improved, by analysing the lengths of the LLL reduced basis vectors, of both the primal and the dual lattices simultaneously.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16108v1</guid>
      <category>cs.DS</category>
      <category>cs.CR</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Antoine Joux, Karol W\k{e}grzycki</dc:creator>
    </item>
    <item>
      <title>Online Probabilistic Metric Embedding: A General Framework for Bypassing Inherent Bounds</title>
      <link>https://arxiv.org/abs/2408.16298</link>
      <description>arXiv:2408.16298v1 Announce Type: new 
Abstract: Probabilistic metric embedding into trees is a powerful technique for designing online algorithms. The standard approach is to embed the entire underlying metric into a tree metric and then solve the problem on the latter. The overhead in the competitive ratio depends on the expected distortion of the embedding, which is logarithmic in $n$, the size of the underlying metric. For many online applications, such as online network design problems, it is natural to ask if it is possible to construct such embeddings in an online fashion such that the distortion would be a polylogarithmic function of $k$, the number of terminals.
  Our first main contribution is answering this question negatively, exhibiting a \emph{lower bound} of $\tilde{\Omega}(\log k \log \Phi)$, where $\Phi$ is the aspect ratio of the set of terminals, showing that a simple modification of the probabilistic embedding into trees of Bartal (FOCS 1996), which has expected distortion of $O(\log k \log \Phi)$, is \emph{nearly-tight}. Unfortunately, this may result in a very bad dependence in terms of $k$, namely, a power of $k$.
  Our second main contribution is a general framework for bypassing this limitation. We show that for a large class of online problems this online probabilistic embedding can still be used to devise an algorithm with $O(\min\{\log k\log (k\lambda),\log^3 k\})$ overhead in the competitive ratio, where $k$ is the current number of terminals, and $\lambda$ is a measure of subadditivity of the cost function, which is at most $r$, the current number of requests. In particular, this implies the first algorithms with competitive ratio $\operatorname{polylog}(k)$ for online subadditive network design (buy-at-bulk network design being a special case), and $\operatorname{polylog}(k,r)$ for online group Steiner forest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16298v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yair Bartal, Ora N. Fandina, Seeun William Umboh</dc:creator>
    </item>
    <item>
      <title>Approximation Algorithms for Correlated Knapsack Orienteering</title>
      <link>https://arxiv.org/abs/2408.16566</link>
      <description>arXiv:2408.16566v1 Announce Type: new 
Abstract: We consider the {\em correlated knapsack orienteering} (CSKO) problem: we are given a travel budget $B$, processing-time budget $W$, finite metric space $(V,d)$ with root $\rho\in V$, where each vertex is associated with a job with possibly correlated random size and random reward that become known only when the job completes. Random variables are independent across different vertices. The goal is to compute a $\rho$-rooted path of length at most $B$, in a possibly adaptive fashion, that maximizes the reward collected from jobs that processed by time $W$. To our knowledge, CSKO has not been considered before, though prior work has considered the uncorrelated problem, {\em stochastic knapsack orienteering}, and {\em correlated orienteering}, which features only one budget constraint on the {\em sum} of travel-time and processing-times.
  We show that the {\em adaptivity gap of CSKO is not a constant, and is at least $\Omega\bigl(\max\sqrt{\log{B}},\sqrt{\log\log{W}}\}\bigr)$}. Complementing this, we devise {\em non-adaptive} algorithms that obtain: (a) $O(\log\log W)$-approximation in quasi-polytime; and (b) $O(\log W)$-approximation in polytime. We obtain similar guarantees for CSKO with cancellations, wherein a job can be cancelled before its completion time, foregoing its reward. We also consider the special case of CSKO, wherein job sizes are weighted Bernoulli distributions, and more generally where the distributions are supported on at most two points (2-CSKO). Although weighted Bernoulli distributions suffice to yield an $\Omega(\sqrt{\log\log B})$ adaptivity-gap lower bound for (uncorrelated) {\em stochastic orienteering}, we show that they are easy instances for CSKO. We develop non-adaptive algorithms that achieve $O(1)$-approximation in polytime for weighted Bernoulli distributions, and in $(n+\log B)^{O(\log W)}$-time for the more general case of 2-CSKO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16566v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Aleman Espinosa, Chaitanya Swamy</dc:creator>
    </item>
    <item>
      <title>Fast and Simple $(1+\epsilon)\Delta$-Edge-Coloring of Dense Graphs</title>
      <link>https://arxiv.org/abs/2408.16692</link>
      <description>arXiv:2408.16692v1 Announce Type: new 
Abstract: Let $\epsilon \in (0, 1)$ and $n, \Delta \in \mathbb N$ be such that $\Delta = \Omega\left(\max\left\{\frac{\log n}{\epsilon},\, \left(\frac{1}{\epsilon}\log \frac{1}{\epsilon}\right)^2\right\}\right)$. Given an $n$-vertex $m$-edge simple graph $G$ of maximum degree $\Delta$, we present a randomized $O\left(m\,\log^3 \Delta\,/\,\epsilon^2\right)$-time algorithm that computes a proper $(1+\epsilon)\Delta$-edge-coloring of $G$ with high probability. This improves upon the best known results for a wide range of the parameters $\epsilon$, $n$, and $\Delta$. Our approach combines a flagging strategy from earlier work of the author with a shifting procedure employed by Duan, He, and Zhang for dynamic edge-coloring. The resulting algorithm is simple to implement and may be of practical interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16692v1</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhishek Dhawan</dc:creator>
    </item>
    <item>
      <title>Optimal Trace Distance and Fidelity Estimations for Pure Quantum States</title>
      <link>https://arxiv.org/abs/2408.16655</link>
      <description>arXiv:2408.16655v1 Announce Type: cross 
Abstract: Measuring the distinguishability between quantum states is a basic problem in quantum information theory. In this paper, we develop optimal quantum algorithms that estimate both the trace distance and the (square root) fidelity between pure states to within additive error $\varepsilon$ using $\Theta(1/\varepsilon)$ queries to their state-preparation circuits, quadratically improving the long-standing folklore $O(1/\varepsilon^2)$. At the heart of our construction, is an algorithmic tool for quantum square root amplitude estimation, which generalizes the well-known quantum amplitude estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16655v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TIT.2024.3447915</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Information Theory, 2024</arxiv:journal_reference>
      <dc:creator>Qisheng Wang</dc:creator>
    </item>
    <item>
      <title>Reducing Isotropy and Volume to KLS: Faster Rounding and Volume Algorithms</title>
      <link>https://arxiv.org/abs/2008.02146</link>
      <description>arXiv:2008.02146v3 Announce Type: replace 
Abstract: We show that the volume of a convex body in $\mathbb{R}^{n}$ in the general membership oracle model can be computed to within relative error $\varepsilon$ using $\widetilde{O}(n^{3.5}\psi^{2} + n^3/\varepsilon^{2})$ oracle queries, where $\psi$ is the KLS constant. With the current bound of $\psi=\widetilde{O}(1)$, this gives an $\widetilde{O}(n^{3.5} + n^3/\varepsilon^{2})$ algorithm, improving on the Lov\'{a}sz-Vempala $\widetilde{O}(n^{4}/\varepsilon^{2})$ algorithm from 2003. The main new ingredient is an $\widetilde{O}(n^{3}\psi^{2})$ algorithm for isotropic transformation of a well-rounded convex body; we apply this iteratively to isotropicize a general convex body. Following this, we can apply the $\widetilde{O}(n^{3}/\varepsilon^{2})$ volume algorithm of Cousins and Vempala for well-rounded convex bodies. We also give an efficient implementation of the new algorithm for convex polytopes defined by $m$ inequalities in $\mathbb{R}^{n}$: polytope volume can be estimated in time $\widetilde{O}(mn^{c}/\varepsilon^{2})$ where $c&lt;3.7$ depends on the current matrix multiplication exponent and improves on the previous best bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2008.02146v3</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>math.FA</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>He Jia, Aditi Laddha, Yin Tat Lee, Santosh S. Vempala</dc:creator>
    </item>
    <item>
      <title>Harvesting Brownian Motion: Zero Energy Computational Sampling</title>
      <link>https://arxiv.org/abs/2309.06957</link>
      <description>arXiv:2309.06957v2 Announce Type: replace 
Abstract: The key factor currently limiting the advancement of computational power of electronic computation is no longer the manufacturing density and speed of components, but rather their high energy consumption. While it has been widely argued that reversible computation can escape the fundamental Landauer limit of $k_B T\ln(2)$ Joules per irreversible computational step, there is disagreement around whether indefinitely reusable computation can be achieved without energy dissipation. Here we focus on the relatively simpler context of sampling problems, which take no input, so avoids modeling the energy costs of the observer perturbing the machine to change its input. Given an algorithm $A$ for generating samples from a distribution, we desire a device that can perpetually generate samples from that distribution driven entirely by Brownian motion. We show that such a device can efficiently execute algorithm $A$ in the sense that we must wait only $O(\text{time}(A)^2)$ between samples. We consider two output models: Las Vegas, which samples from the exact probability distribution every $4$ tries in expectation, and Monte Carlo, in which every try succeeds but the distribution is only approximated. We base our model on continuous-time random walks over the state space graph of a general computational machine, with a space-bounded Turing machine as one instantiation. The problem of sampling a computationally complex probability distribution with no energy dissipation informs our understanding of the energy requirements of computation, and may lead to more energy efficient randomized algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.06957v2</guid>
      <category>cs.DS</category>
      <category>cs.ET</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Doty, Niels Kornerup, Austin Luchsinger, Leo Orshansky, David Soloveichik, Damien Woods</dc:creator>
    </item>
    <item>
      <title>Correlation Clustering with Vertex Splitting</title>
      <link>https://arxiv.org/abs/2402.10335</link>
      <description>arXiv:2402.10335v2 Announce Type: replace 
Abstract: We explore Cluster Editing and its generalization Correlation Clustering with a new operation called permissive vertex splitting which addresses finding overlapping clusters in the face of uncertain information. We determine that both problems are NP-hard, yet they exhibit significant differences in parameterized complexity and approximability. For Cluster Editing with Permissive Vertex Splitting, we show a polynomial kernel when parameterized by the solution size and develop a polynomial-time algorithm with approximation factor 7. In the case of Correlation Clustering, we establish para-NP-hardness when parameterized by solution size and demonstrate that computing an $n^{1-\epsilon}$-approximation is NP-hard for any constant $\epsilon &gt; 0$. Additionally, we extend the established link between Correlation Clustering and Multicut to the setting with permissive vertex splitting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10335v2</guid>
      <category>cs.DS</category>
      <category>cs.SI</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Bentert, Alex Crane, P{\aa}l Gr{\o}n{\aa}s Drange, Felix Reidl, Blair D. Sullivan</dc:creator>
    </item>
    <item>
      <title>Accelerating Spectral Clustering on Quantum and Analog Platforms</title>
      <link>https://arxiv.org/abs/2408.08486</link>
      <description>arXiv:2408.08486v4 Announce Type: replace 
Abstract: We introduce a novel hybrid quantum-analog algorithm to perform graph clustering that exploits connections between the evolution of dynamical systems on graphs and the underlying graph spectra. This approach constitutes a new class of algorithms that combine emerging quantum and analog platforms to accelerate computations. Our hybrid algorithm is equivalent to spectral clustering and has a computational complexity of $O(N)$, where $N$ is the number of nodes in the graph, compared to $O(N^3)$ scaling on classical computing platforms. The proposed method employs the dynamic mode decomposition (DMD) framework on the data generated by Schr\"{o}dinger dynamics that evolves on the manifold induced by the graph Laplacian. In particular, we prove and demonstrate that one can extract the eigenvalues and scaled eigenvectors of the normalized graph Laplacian by evolving Schr\"{o}dinger dynamics on quantum computers followed by DMD computations on analog devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08486v4</guid>
      <category>cs.DS</category>
      <category>math.DS</category>
      <category>math.SP</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingzi Xu, Tuhin Sahai</dc:creator>
    </item>
    <item>
      <title>On the Advice Complexity of Online Matching on the Line</title>
      <link>https://arxiv.org/abs/2408.11161</link>
      <description>arXiv:2408.11161v2 Announce Type: replace 
Abstract: We consider the weighted matching problem on the line with advice complexity. We give a 1-competitive online algorithm with advice complexity $n-1,$ and show that there is no 1-competitive online algorithm reading less than $n-1$ bits of advice. Moreover, we present a weakly $c(n/k)$-competitive online algorithm with advice complexity $O(k(\log N + \log n))$ where $n$ is the number of servers, $N$ is the distance of the minimal and maximal servers, and $c(n)$ is the complexity of the best online algorithm without advice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11161v2</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>B\'ela Csaba, Judit Nagy-Gy\"orgy</dc:creator>
    </item>
  </channel>
</rss>
