<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 08 Aug 2025 01:28:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Predicting fall risk in older adults: A machine learning comparison of accelerometric and non-accelerometric factors</title>
      <link>https://arxiv.org/abs/2508.03756</link>
      <description>arXiv:2508.03756v1 Announce Type: new 
Abstract: This study investigates fall risk prediction in older adults using various machine learning models trained on accelerometric, non-accelerometric, and combined data from 146 participants. Models combining both data types achieved superior performance, with Bayesian Ridge Regression showing the highest accuracy (MSE = 0.6746, R2 = 0.9941). Non-accelerometric variables, such as age and comorbidities, proved critical for prediction. Results support the use of integrated data and Bayesian approaches to enhance fall risk assessment and inform prevention strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03756v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1177/20552076251331752</arxiv:DOI>
      <arxiv:journal_reference>DIGITAL HEALTH, 11, 20552076251331752</arxiv:journal_reference>
      <dc:creator>Ana Gonz\'alez-Castro, Jos\'e Alberto Ben\'itez-Andrades, Rub\'en Gonz\'alez-Gonz\'alez, Camino Prada-Garc\'ia, Raquel Leir\'os-Rodr\'iguez</dc:creator>
    </item>
    <item>
      <title>Leveraging Minute-by-Minute Soccer Match Event Data to Adjust Team's Offensive Production for Game Context</title>
      <link>https://arxiv.org/abs/2508.04008</link>
      <description>arXiv:2508.04008v1 Announce Type: new 
Abstract: In soccer, game context can result in skewing offensive statistics in ways that might misrepresent how well a team has played. For instance, in England's 1-2 loss to France in the 2022 FIFA World Cup quarterfinal, England attempted considerably more shots (16 to France's 8) and more corners (5 to 2), potentially suggesting they played better despite the loss. However, these statistics were largely accumulated when France was ahead and more willing to concede offensive initiative to England. To explore how game context influences offensive performance, we analyze minute-by-minute event-sequenced match data from 15 seasons across five major European leagues. Using count-response Generalized Additive Modeling, we consider features such as score and red card differential, home/away status, pre-match win probabilities, and game minute. Moreover, we leverage interaction terms to test several intuitive hypotheses about how these features might cooperate in explaining offensive production. The selected model is then applied to project offensive statistics onto a standardized "common denominator" scenario: a tied home game with even men on both sides. The adjusted numbers - in contrast to regular game totals that disregard game context - offer a more contextualized comparison, reducing the likelihood of misrepresenting the relative quality of play.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04008v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrey Skripnikov, Ahmet Cemek, David Gillman</dc:creator>
    </item>
    <item>
      <title>Matrix Factorization-Based Solar Spectral Irradiance Missing Data Imputation with Uncertainty Quantification</title>
      <link>https://arxiv.org/abs/2508.04074</link>
      <description>arXiv:2508.04074v1 Announce Type: new 
Abstract: The solar spectral irradiance (SSI) depicts the spectral distribution of solar energy flux reaching the top of the Earth's atmosphere. The SSI data constitute a matrix with spectrally (rows) and temporally (columns) resolved solar energy flux measurements. The most recent SSI measurements have been made by NASA's Total and Spectral Solar Irradiance Sensor-1 (TSIS-1) Spectral Irradiance Monitor (SIM) since March 2018. This data have considerable missing data due to both random factors and instrument downtime, a periodic trend related to the Sun's cyclical magnetic activity, and varying degrees of correlation among the spectra, some approaching unity. We propose a novel low-rank matrix factorization method that uses autoregressive regularization and periodic spline detrending to recover the missingness. The method is a two-step procedure, each of which tackles scattered and downtime missingness, respectively. We design efficient alternating algorithms to jointly estimate the model parameters. Moreover, we build a distribution-free uncertainty quantification method using conformal prediction. We validate the prediction interval coverage rates and assess the imputation accuracy against competing models such as Gaussian process regression and linear time series smoothing via numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04074v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuxuan Ke, Xianglei Huang, Odele Coddington, Yang Chen</dc:creator>
    </item>
    <item>
      <title>Cluster-specific ranking and variable importance for Scottish regional deprivation via vine mixtures</title>
      <link>https://arxiv.org/abs/2508.04533</link>
      <description>arXiv:2508.04533v1 Announce Type: new 
Abstract: Socioeconomic deprivation is a key determinant of public health, as highlighted by the Scottish Government's Scottish Index of Multiple Deprivation (SIMD). We propose an approach for clustering Scottish zones based on multiple deprivation indicators using vine mixture models. This framework uses the flexibility of vine copulas to capture tail dependent and asymmetric relationships among the indicators. From the fitted vine mixture model, we obtain posterior probabilities for each zone's membership in clusters. This allows the construction of a cluster-driven deprivation ranking by sorting zones according to their probability of belonging to the most deprived cluster. To assess variable importance in this unsupervised learning setting, we adopt a leave-one-variable-out procedure by refitting the model without each variable and calculating the resulting change in the Bayesian information criterion. Our analysis of 21 continuous indicators across 1964 zones in Glasgow and the surrounding areas in Scotland shows that socioeconomic measures, particularly income and employment rates, are major drivers of deprivation, while certain health- and crime-related indicators appear less influential. These findings are consistent across the approach of variable importance and the analysis of the fitted vine structures of the identified clusters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04533v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\"Ozge \c{S}ahin, Ozan Evkaya, Ariane Hanebeck</dc:creator>
    </item>
    <item>
      <title>Novel Risk Measures for Portfolio Optimization Using Equal-Correlation Portfolio Strategy</title>
      <link>https://arxiv.org/abs/2508.03704</link>
      <description>arXiv:2508.03704v1 Announce Type: cross 
Abstract: Portfolio optimization has long been dominated by covariance-based strategies, such as the Markowitz Mean-Variance framework. However, these approaches often fail to ensure a balanced risk structure across assets, leading to concentration in a few securities. In this paper, we introduce novel risk measures grounded in the equal-correlation portfolio strategy, aiming to construct portfolios where each asset maintains an equal correlation with the overall portfolio return. We formulate a mathematical optimization framework that explicitly controls portfolio-wide correlation while preserving desirable risk-return trade-offs. The proposed models are empirically validated using historical stock market data. Our findings show that portfolios constructed via this approach demonstrate superior risk diversification and more stable returns under diverse market conditions. This methodology offers a compelling alternative to conventional diversification techniques and holds practical relevance for institutional investors, asset managers, and quantitative trading strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03704v1</guid>
      <category>q-fin.PM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Biswarup Chakraborty</dc:creator>
    </item>
    <item>
      <title>A semi-automatic approach to study population dynamics based on population pyramids</title>
      <link>https://arxiv.org/abs/2508.03788</link>
      <description>arXiv:2508.03788v1 Announce Type: cross 
Abstract: The depiction of populations - of humans or animals - as "population pyramids" is a useful tool for the assessment of various characteristics of populations at a glance. Although these visualisations are well-known objects in various communities, formalised and algorithmic approaches to gain information from these data are less present. Here, we present an algorithm-based classification of population data into "pyramids" of different shapes ([normal and inverted] pyramid / plunger / bell, [lower / middle / upper] diamond, column, hourglass) that are linked to specific characteristics of the population. To develop the algorithmic approach, we used data describing global zoo populations of mammals from 1970-2024. This algorithm-based approach delivers plausible classifications, in particular with respect to changes in population size linked to specific series of, and transitions between, different "pyramid" shapes. We believe this approach might become a useful tool for analysing and communicating historical population developments in multiple contexts and is of broad interest. Moreover, it might be useful for animal population management strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03788v1</guid>
      <category>q-bio.PE</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Hahn-Klimroth, Jo\~ao Pedro Meireles, Laurie Bingaman Lackey, Nick van Eeuwijk Mads F. Bertelsen, Paul W. Dierkes, Marcus Clauss</dc:creator>
    </item>
    <item>
      <title>Exact and Conservative Inference for the Average Treatment Effect in Stratified Experiments with Binary Outcomes</title>
      <link>https://arxiv.org/abs/2508.03834</link>
      <description>arXiv:2508.03834v1 Announce Type: cross 
Abstract: We extend methods for finite-sample inference about the average treatment effect (ATE) in randomized experiments with binary outcomes to accommodate stratification (blocking). We present three valid methods that differ in their computational and statistical efficiency. The first method constructs conservative, Bonferroni-adjusted confidence intervals separately for the mean response in the treatment and control groups in each stratum, then takes appropriate weighted differences of their endpoints to find a confidence interval for the ATE. The second method inverts permutation tests for the overall ATE, maximizing the $P$-value over all ways a given ATE can be attained. The third method applies permutation tests for the ATE in separate strata, then combines those tests to form a confidence interval for the overall ATE. We compare the statistical and computational performance of the methods using simulations and a case study. The second approach is most efficient statistically in the simulations, but a naive implementation requires O(\Pi_{k=1}^{K} n_{k}^{4}) permutation tests, the highest computational burden among the three methods. That computational burden can be reduced to O(\sum_{k=1}^K n_k \times\Pi_{k=1}^{K} n_{k}^{2}) if all strata are balanced and to O(\Pi_{k=1}^{K} n_{k}^{3}) otherwise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03834v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jiaxun Li, Jacob Spertus, Philip B. Stark</dc:creator>
    </item>
    <item>
      <title>Operational convection-permitting COSMO/ICON ensemble predictions at observation sites (CIENS)</title>
      <link>https://arxiv.org/abs/2508.03845</link>
      <description>arXiv:2508.03845v1 Announce Type: cross 
Abstract: We present the CIENS dataset, which contains ensemble weather forecasts from the operational convection-permitting numerical weather prediction model of the German Weather Service. It comprises forecasts for 55 meteorological variables mapped to the locations of synoptic stations, as well as additional spatially aggregated forecasts from surrounding grid points, available for a subset of these variables. Forecasts are available at hourly lead times from 0 to 21 hours for two daily model runs initialized at 00 and 12 UTC, covering the period from December 2010 to June 2023. Additionally, the dataset provides station observations for six key variables at 170 locations across Germany: pressure, temperature, hourly precipitation accumulation, wind speed, wind direction, and wind gusts. Since the forecast are mapped to the observed locations, the data is delivered in a convenient format for analysis. The CIENS dataset complements the growing collection of benchmark datasets for weather and climate modeling. A key distinguishing feature is its long temporal extent, which encompasses multiple updates to the underlying numerical weather prediction model and thus supports investigations into how forecasting methods can account for such changes. In addition to detailing the design and contents of the CIENS dataset, we outline potential applications in ensemble post-processing, forecast verification, and related research areas. A use case focused on ensemble post-processing illustrates the benefits of incorporating the rich set of available model predictors into machine learning-based forecasting models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03845v1</guid>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Lerch, Benedikt Schulz, Reinhold Hess, Annette M\"oller, Cristina Primo, Sebastian Trepte, Susanne Theis</dc:creator>
    </item>
    <item>
      <title>The Regression Discontinuity Design in Medical Science</title>
      <link>https://arxiv.org/abs/2508.03878</link>
      <description>arXiv:2508.03878v1 Announce Type: cross 
Abstract: This article provides an introduction to the Regression Discontinuity (RD) design, and its application to empirical research in the medical sciences. While the main focus of this article is on causal interpretation, key concepts of estimation and inference are also briefly mentioned. A running medical empirical example is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03878v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Rocio Titiunik</dc:creator>
    </item>
    <item>
      <title>Rapid parameter estimation with the full symphony of compact binary mergers using meshfree approximation</title>
      <link>https://arxiv.org/abs/2508.04172</link>
      <description>arXiv:2508.04172v1 Announce Type: cross 
Abstract: We present a fast Bayesian inference framework to address the growing computational cost of gravitational-wave parameter estimation. The increased cost is driven by improved broadband detector sensitivity, particularly at low frequencies due to advances in detector commissioning, resulting in longer in-band signals and a higher detection rate. Waveform models now incorporate features like higher-order modes, further increasing the complexity of standard inference methods. Our framework employs meshfree likelihood interpolation with radial basis functions to accelerate Bayesian inference using the IMRPhenomXHM waveform model that incorporates higher modes of the gravitational-wave signal. In the initial start-up stage, interpolation nodes are placed within a constant-match metric ellipsoid in the intrinsic parameter space. During sampling, likelihood is evaluated directly using the precomputed interpolants, bypassing the costly steps of on-the-fly waveform generation and overlap-integral computation. We improve efficiency by sampling in a rotated parameter space aligned with the eigenbasis of the metric ellipsoid, where parameters are uncorrelated by construction. This speeds up sampler convergence. This method yields unbiased parameter recovery when applied to 100 simulated neutron-star-black-hole signals (NSBH) in LIGO-Virgo data, while reducing computational cost by up to an order of magnitude for the longest-duration signal. The meshfree framework equally applies to symmetric compact binary systems dominated by the quadrupole mode, supporting parameter estimation across a broad range of sources. Applied to a simulated NSBH signal in Einstein Telescope data, where the effects of Earth's rotation are neglected for simplicity, our method achieves an O(10^4) speed-up, demonstrating its potential use in the third-generation (3G) era.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04172v1</guid>
      <category>gr-qc</category>
      <category>astro-ph.IM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhishek Sharma, Lalit Pathak, Soumen Roy, Anand S. Sengupta</dc:creator>
    </item>
    <item>
      <title>Generative Flexible Latent Structure Regression (GFLSR) model</title>
      <link>https://arxiv.org/abs/2508.04393</link>
      <description>arXiv:2508.04393v1 Announce Type: cross 
Abstract: Latent structure methods, specifically linear continuous latent structure methods, are a type of fundamental statistical learning strategy. They are widely used for dimension reduction, regression and prediction, in the fields of chemometrics, economics, social science and etc. However, due to the lack of model inference, generative form, and unidentifiable parameters, most of these methods are always used as an algorithm, instead of a model. This paper proposed a Generative Flexible Latent Structure Regression (GFLSR) model structure to address this problem. Moreover, we show that most linear continuous latent variable methods can be represented under the proposed framework. The recursive structure allows potential model inference and residual analysis. Then, the traditional Partial Least Squares (PLS) is focused; we show that the PLS can be specialised in the proposed model structure, named Generative-PLS. With a model structure, we analyse the convergence of the parameters and the latent variables. Under additional distribution assumptions, we show that the proposed model structure can lead to model inference without solving the probabilistic model. Additionally, we proposed a novel bootstrap algorithm that enables uncertainty on parameters and on prediction for new datasets. A simulation study and a Real-world dataset are used to verify the proposed Generative-PLS model structure. Although the traditional PLS is a special case, this proposed GFLSRM structure leads to a potential inference structure for all the linear continuous latent variable methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04393v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Clara Grazian, Qian Jin, Pierre Lafaye De Micheaux</dc:creator>
    </item>
    <item>
      <title>Bias in Meta-Analytic Modeling of Surrogate Endpoints in Cancer Screening Trials</title>
      <link>https://arxiv.org/abs/2508.04633</link>
      <description>arXiv:2508.04633v1 Announce Type: cross 
Abstract: In meta-analytic modeling, the functional relationship between a primary and surrogate endpoint is estimated using summary data from a set of completed clinical trials. Parameters in the meta-analytic model are used to assess the quality of the proposed surrogate. Recently, meta-analytic models have been employed to evaluate whether late-stage cancer incidence can serve as a surrogate for cancer mortality in cancer screening trials. A major challenge in meta-analytic models is that uncertainty of trial-level estimates affects the evaluation of surrogacy, since each trial provides only estimates of the primary and surrogate endpoints rather than their true parameter values. In this work, we show via simulation and theory that trial-level estimate uncertainty may bias the results of meta-analytic models towards positive findings of the quality of the surrogate. We focus on cancer screening trials and the late stage incidence surrogate. We reassess correlations between primary and surrogate endpoints in Ovarian cancer screening trials. Our findings indicate that completed trials provide limited information regarding quality of the late-stage incidence surrogate. These results support restricting meta-analytic regression usage to settings where trial-level estimate uncertainty is incorporated into the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04633v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>James P. Long, Abhishikta Roy, Ehsan Irajizad, Kim-Anh Do, Yu Shen</dc:creator>
    </item>
    <item>
      <title>Stochastic Taylor expansion via Poisson point processes</title>
      <link>https://arxiv.org/abs/2508.04703</link>
      <description>arXiv:2508.04703v1 Announce Type: cross 
Abstract: We generalize Taylor's theorem by introducing a stochastic formulation based on an underlying Poisson point process model. We utilize this approach to propose a novel non-linear regression framework and perform statistical inference of the model parameters. Theoretical properties of the proposed estimator are also proven, including its convergence, uniformly almost surely, to the true function. The theory is presented for the univariate and multivariate cases, and we exemplify the proposed methodology using several examples via simulations and an application to stock market data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04703v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weichao Wu, Athanasios C. Micheas</dc:creator>
    </item>
    <item>
      <title>A Time-Scaled ETAS Model for Earthquake Forecasting</title>
      <link>https://arxiv.org/abs/2505.24412</link>
      <description>arXiv:2505.24412v2 Announce Type: replace 
Abstract: The Himalayan region, including Nepal, is prone to frequent and large earthquakes. Accurate forecasting of these earthquakes is crucial for minimizing loss of life and damage to infrastructure. In this study, we propose various time-scaled Epidemic Type Aftershock Sequence (ETAS) models to forecast earthquakes in Nepal. The ETAS model is a statistical model that describes the temporal and spatial patterns of aftershocks following a main shock. A dataset of earthquake occurrences in Nepal from 2000 to 2020 was collected, and this data was used to fit the models showcased in this article. Our results show that the time-scaled ETAS model is able to accurately forecast earthquake occurrences in Nepal, and could be a useful tool for earthquake early warning systems in the region.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24412v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-981-96-7556-2</arxiv:DOI>
      <arxiv:journal_reference>Data-Centric Approaches to Industrial Decisions: Technology, Digitisation nad Business Decisions, Asset Analytics, Performance and Safety Management (2025) 97-114; ISBN: 978-981-96-7555-5</arxiv:journal_reference>
      <dc:creator>Agniva Das, Muralidharan K</dc:creator>
    </item>
    <item>
      <title>Time Series Transformer-Based Modeling of Pavement Skid and Texture Deterioration</title>
      <link>https://arxiv.org/abs/2507.01842</link>
      <description>arXiv:2507.01842v2 Announce Type: replace 
Abstract: This study investigates the deterioration of skid resistance and surface macrotexture following preventive maintenance using micro-milling techniques. Field data were collected from 31 asphalt pavement sections located across four climatic zones in Texas. The data encompasses a variety of surface types, milling depths, operational speeds, and drum configurations. A standardized data collection protocol was followed, with measurements taken before milling, immediately after treatment, and at 3, 6, 12, and 18 months post-treatment. Skid number and Mean Profile Depth (MPD) were used to evaluate surface friction and texture characteristics. The dataset was reformatted into a time-series structure with 930 observations, including contextual variables such as climatic zone, treatment parameters, and baseline surface condition. A comparative modeling framework was applied to predict the deterioration trends of both skid resistance and macrotexture over time. Eight regression models, including linear, tree-based, and ensemble methods, were evaluated alongside a time series transformer model. Results show that the transformer model achieved the highest prediction accuracy for skid resistance (R2 = 0.981), while Random Forest performing best for macrotexture prediction (R2 = 0.838). The findings indicate that the degradation of surface characteristics after preventive maintenance is nonlinear and influenced by a combination of environmental and operational factors. This study demonstrates the effectiveness of data-driven modeling in supporting transportation agencies with pavement performance forecasting and maintenance planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01842v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lu Gao, Zia Din, Kinam Kim, Ahmed Senouci</dc:creator>
    </item>
    <item>
      <title>Assessing Heterogeneity of Treatment Effects</title>
      <link>https://arxiv.org/abs/2306.15048</link>
      <description>arXiv:2306.15048v4 Announce Type: replace-cross 
Abstract: Heterogeneous treatment effects are of major interest in economics. For example, a poverty reduction measure would be best evaluated by its effects on those who would be poor in the absence of the treatment, or by the share among the poor who would increase their earnings because of the treatment. While these quantities are not identified, we derive nonparametrically sharp bounds using only the marginal distributions of the control and treated outcomes. Applications to microfinance and welfare reform demonstrate their utility even when the average treatment effects are not significant and when economic theory makes opposite predictions between heterogeneous individuals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15048v4</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tetsuya Kaji, Jianfei Cao</dc:creator>
    </item>
    <item>
      <title>Producing treatment hierarchies in network meta-analysis using probabilistic models and treatment-choice criteria</title>
      <link>https://arxiv.org/abs/2406.10612</link>
      <description>arXiv:2406.10612v2 Announce Type: replace-cross 
Abstract: A key output of network meta-analysis (NMA) is the relative ranking of treatments; nevertheless, it has attracted substantial criticism. Existing ranking methods often lack clear interpretability and fail to adequately account for uncertainty, over-emphasizing small differences in treatment effects. We propose a novel framework to estimate treatment hierarchies in NMA using a probabilistic model, focusing on a clinically relevant treatment-choice criterion (TCC). Initially, we formulate a mathematical expression to define a TCC based on smallest worthwhile differences (SWD), converting NMA relative treatment effects into treatment preference format. This data is then synthesized using a probabilistic ranking model, assigning each treatment a latent 'ability' parameter, representing its propensity to yield clinically important and beneficial true treatment effects relative to the rest of the treatments in the network. Parameter estimation relies on the maximum likelihood theory, with standard errors derived asymptotically from Fisher's information matrix. To facilitate the use of our methods, we launched the R package mtrank. We applied our method to two clinical datasets: one comparing 18 antidepressants for major depression and another comparing 6 antihypertensives for the incidence of diabetes. Our approach provided robust, interpretable treatment hierarchies that account for a concrete TCC. We further examined the agreement between the proposed method and existing ranking metrics in 153 published networks, concluding that the degree of agreement depends on the precision of the NMA estimates. Our framework offers a valuable alternative for NMA treatment ranking, mitigating over-interpretation of minor differences. This enables more reliable and clinically meaningful treatment hierarchies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10612v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Theodoros Evrenoglou, Adriani Nikolakopoulou, Guido Schwarzer, Gerta R\"ucker, Anna Chaimani</dc:creator>
    </item>
    <item>
      <title>Survey Data Integration for Distribution Function Estimation</title>
      <link>https://arxiv.org/abs/2409.14284</link>
      <description>arXiv:2409.14284v3 Announce Type: replace-cross 
Abstract: Integration of probabilistic and non-probabilistic samples for the estimation of finite population totals (or means) has recently received considerable attention in the field of survey sampling; yet, to the best of our knowledge, this framework has not been extended to cumulative distribution function (CDF) estimation. To address this gap, we propose a novel CDF estimator that integrates data from probability samples with data from, potentially big, nonprobability samples. Assuming that a set of shared covariates are observed in both, while the response variable is observed only in the latter, the proposed estimator uses a survey-weighted empirical CDF of regression residuals trained on the convenience sample to estimate the CDF of the response variable. Under some assumptions, we derive the asymptotic bias and variance of our CDF estimator and show that it is asymptotically unbiased for the finite population CDF if ignorability holds. Our empirical results imply that the proposed CDF estimator is robust to model misspecification under ignorability, and robust to ignorability under model misspecification; when both assumptions are violated, our residual-based CDF estimator still outperforms its `plug-in' mass imputation and naive siblings, albeit with noted decreases in efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14284v3</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremy Flood, Sayed Mostafa</dc:creator>
    </item>
    <item>
      <title>What Lives? A meta-analysis of diverse opinions on the definition of life</title>
      <link>https://arxiv.org/abs/2505.15849</link>
      <description>arXiv:2505.15849v2 Announce Type: replace-cross 
Abstract: The question of "what is life?" has challenged scientists and philosophers for centuries, producing an array of definitions that reflect both the mystery of its emergence and the diversity of disciplinary perspectives brought to bear on the question. Despite significant progress in our understanding of biological systems, psychology, computation, and information theory, no single definition for life has yet achieved universal acceptance. This challenge becomes increasingly urgent as advances in synthetic biology, artificial intelligence, and astrobiology challenge our traditional conceptions of what it means to be alive. We undertook a methodological approach that leverages large language models (LLMs) to analyze a set of definitions of life provided by a curated set of cross-disciplinary experts. We used a novel pairwise correlation analysis to map the definitions into distinct feature vectors, followed by agglomerative clustering, intra-cluster semantic analysis, and t-SNE projection to reveal underlying conceptual archetypes. This methodology revealed a continuous landscape of the themes relating to the definition of life, suggesting that what has historically been approached as a binary taxonomic problem should be instead conceived as differentiated perspectives within a unified conceptual latent space. We offer a new methodological bridge between reductionist and holistic approaches to fundamental questions in science and philosophy, demonstrating how computational semantic analysis can reveal conceptual patterns across disciplinary boundaries, and opening similar pathways for addressing other contested definitional territories across the sciences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15849v2</guid>
      <category>q-bio.OT</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>q-bio.BM</category>
      <category>q-bio.CB</category>
      <category>q-bio.SC</category>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Reed Bender, Karina Kofman, Blaise Ag\"uera y Arcas, Michael Levin</dc:creator>
    </item>
  </channel>
</rss>
