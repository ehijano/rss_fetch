<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 30 May 2024 01:44:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 29 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Emergent Inequalities in a Primitive Agent-Based Good-Exchange Model</title>
      <link>https://arxiv.org/abs/2405.18116</link>
      <description>arXiv:2405.18116v1 Announce Type: new 
Abstract: Rising inequalities around the globe bring into question our economic systems and the origin of such inequalities. Here we propose a toy agent-based model where each entity is simultaneously producing and consuming indivisible goods. We find that the system exhibits a non-trivial phase transition beyond which a market clearing equilibrium exists but becomes dynamically unreachable. When production capacity exceeds a threshold and adapts too slowly, some agents cannot sell all their goods. This leads to global price deflation and induces strong wealth inequalities, with the spontaneous separation of the population into a rich class and a poor class. We explore ways to alleviate poverty in this model and whether they have real life significance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18116v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nirbhay Patil, Jean-Philippe Bouchaud</dc:creator>
    </item>
    <item>
      <title>Dataset-learning duality and emergent criticality</title>
      <link>https://arxiv.org/abs/2405.17391</link>
      <description>arXiv:2405.17391v1 Announce Type: cross 
Abstract: In artificial neural networks, the activation dynamics of non-trainable variables is strongly coupled to the learning dynamics of trainable variables. During the activation pass, the boundary neurons (e.g., input neurons) are mapped to the bulk neurons (e.g., hidden neurons), and during the learning pass, both bulk and boundary neurons are mapped to changes in trainable variables (e.g., weights and biases). For example, in feed-forward neural networks, forward propagation is the activation pass and backward propagation is the learning pass. We show that a composition of the two maps establishes a duality map between a subspace of non-trainable boundary variables (e.g., dataset) and a tangent subspace of trainable variables (i.e., learning). In general, the dataset-learning duality is a complex non-linear map between high-dimensional spaces, but in a learning equilibrium, the problem can be linearized and reduced to many weakly coupled one-dimensional problems. We use the duality to study the emergence of criticality, or the power-law distributions of fluctuations of the trainable variables. In particular, we show that criticality can emerge in the learning system even from the dataset in a non-critical state, and that the power-law distribution can be modified by changing either the activation function or the loss function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17391v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.NE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ekaterina Kukleva, Vitaly Vanchurin</dc:creator>
    </item>
    <item>
      <title>Bayesian RG Flow in Neural Network Field Theories</title>
      <link>https://arxiv.org/abs/2405.17538</link>
      <description>arXiv:2405.17538v1 Announce Type: cross 
Abstract: The Neural Network Field Theory correspondence (NNFT) is a mapping from neural network (NN) architectures into the space of statistical field theories (SFTs). The Bayesian renormalization group (BRG) is an information-theoretic coarse graining scheme that generalizes the principles of the Exact Renormalization Group (ERG) to arbitrarily parameterized probability distributions, including those of NNs. In BRG, coarse graining is performed in parameter space with respect to an information-theoretic distinguishability scale set by the Fisher information metric. In this paper, we unify NNFT and BRG to form a powerful new framework for exploring the space of NNs and SFTs, which we coin BRG-NNFT. With BRG-NNFT, NN training dynamics can be interpreted as inducing a flow in the space of SFTs from the information-theoretic `IR' $\rightarrow$ `UV'. Conversely, applying an information-shell coarse graining to the trained network's parameters induces a flow in the space of SFTs from the information-theoretic `UV' $\rightarrow$ `IR'. When the information-theoretic cutoff scale coincides with a standard momentum scale, BRG is equivalent to ERG. We demonstrate the BRG-NNFT correspondence on two analytically tractable examples. First, we construct BRG flows for trained, infinite-width NNs, of arbitrary depth, with generic activation functions. As a special case, we then restrict to architectures with a single infinitely-wide layer, scalar outputs, and generalized cos-net activations. In this case, we show that BRG coarse-graining corresponds exactly to the momentum-shell ERG flow of a free scalar SFT. Our analytic results are corroborated by a numerical experiment in which an ensemble of asymptotically wide NNs are trained and subsequently renormalized using an information-shell BRG scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17538v1</guid>
      <category>hep-th</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jessica N. Howard, Marc S. Klinger, Anindita Maiti, Alexander G. Stapleton</dc:creator>
    </item>
    <item>
      <title>Approximately-symmetric neural networks for quantum spin liquids</title>
      <link>https://arxiv.org/abs/2405.17541</link>
      <description>arXiv:2405.17541v1 Announce Type: cross 
Abstract: We propose and analyze a family of approximately-symmetric neural networks for quantum spin liquid problems. These tailored architectures are parameter-efficient, scalable, and significantly out-perform existing symmetry-unaware neural network architectures. Utilizing the mixed-field toric code model, we demonstrate that our approach is competitive with the state-of-the-art tensor network and quantum Monte Carlo methods. Moreover, at the largest system sizes (N=480), our method allows us to explore Hamiltonians with sign problems beyond the reach of both quantum Monte Carlo and finite-size matrix-product states. The network comprises an exactly symmetric block following a non-symmetric block, which we argue learns a transformation of the ground state analogous to quasiadiabatic continuation. Our work paves the way toward investigating quantum spin liquid problems within interpretable neural network architectures</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17541v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.str-el</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dominik S. Kufel, Jack Kemp, Simon M. Linsel, Chris R. Laumann, Norman Y. Yao</dc:creator>
    </item>
    <item>
      <title>Characterizing dynamical criticality of many-body localization transitions from the Fock-space perspective</title>
      <link>https://arxiv.org/abs/2405.18188</link>
      <description>arXiv:2405.18188v1 Announce Type: cross 
Abstract: Characterizing the nature of many-body localization transitions (MBLTs) and their potential critical behaviors has remained a challenging problem. In this work, we study the dynamics of the displacement, quantifying the spread of the radial probability distribution in the Fock space, for systems with MBLTs, and perform a finite-size scaling analysis. We find that the scaling exponents satisfy theoretical bounds, and can identify universality classes. We show that reliable extrapolations to the thermodynamic limit for the MBLT induced by quasiperiodic fields is possible even for computationally accessible system sizes. Our work highlights that the displacement is a valuable tool for studying MBLTs, as relevant to ongoing experimental efforts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18188v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zheng-Hang Sun, Yong-Yi Wang, Jian Cui, Heng Fan, Markus Heyl</dc:creator>
    </item>
    <item>
      <title>Bias in Motion: Theoretical Insights into the Dynamics of Bias in SGD Training</title>
      <link>https://arxiv.org/abs/2405.18296</link>
      <description>arXiv:2405.18296v1 Announce Type: cross 
Abstract: Machine learning systems often acquire biases by leveraging undesired features in the data, impacting accuracy variably across different sub-populations. Current understanding of bias formation mostly focuses on the initial and final stages of learning, leaving a gap in knowledge regarding the transient dynamics. To address this gap, this paper explores the evolution of bias in a teacher-student setup modeling different data sub-populations with a Gaussian-mixture model. We provide an analytical description of the stochastic gradient descent dynamics of a linear classifier in this setting, which we prove to be exact in high dimension. Notably, our analysis reveals how different properties of sub-populations influence bias at different timescales, showing a shifting preference of the classifier during training. Applying our findings to fairness and robustness, we delineate how and when heterogeneous data and spurious features can generate and amplify bias. We empirically validate our results in more complex scenarios by training deeper networks on synthetic and real datasets, including CIFAR10, MNIST, and CelebA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18296v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anchit Jain, Rozhin Nobahari, Aristide Baratin, Stefano Sarao Mannelli</dc:creator>
    </item>
    <item>
      <title>Collective dynamics and long-range order in thermal neuristor networks</title>
      <link>https://arxiv.org/abs/2312.12899</link>
      <description>arXiv:2312.12899v2 Announce Type: replace 
Abstract: In the pursuit of scalable and energy-efficient neuromorphic devices, recent research has unveiled a novel category of spiking oscillators, termed "thermal neuristors". These devices function via thermal interactions among neighboring vanadium dioxide resistive memories, emulating biological neuronal behavior. Here, we show that the collective dynamical behavior of networks of these neurons showcases a rich phase structure, tunable by adjusting the thermal coupling and input voltage. Notably, we identify phases exhibiting long-range order that, however, does not arise from criticality, but rather from the time non-local response of the system. In addition, we show that these thermal neuristor arrays achieve high accuracy in image recognition and time series prediction through reservoir computing, without leveraging long-range order. Our findings highlight a crucial aspect of neuromorphic computing with possible implications on the functioning of the brain: criticality may not be necessary for the efficient performance of neuromorphic systems in certain computational tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12899v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.mtrl-sci</category>
      <category>nlin.AO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan-Hang Zhang, Chesson Sipling, Erbin Qiu, Ivan K. Schuller, Massimiliano Di Ventra</dc:creator>
    </item>
    <item>
      <title>Generating-functional analysis of random Lotka-Volterra systems: A step-by-step guide</title>
      <link>https://arxiv.org/abs/2405.14289</link>
      <description>arXiv:2405.14289v3 Announce Type: replace 
Abstract: This paper provides what is hopefully a self-contained set of notes describing the detailed steps of a generating-functional analysis of systems of generalised Lotka-Volterra equations with random interaction coefficients. Nothing in these notes is original, instead the generating-functional method (also known as the Martin-Siggia-Rose-DeDominic-Janssen formalism) and the resulting dynamic mean field theories have been used for the study of disordered systems and spin glasses for decades. But it is hard to find unifying sources which would allow a beginner to learn step-by-step how these methods can be used. My aim is to provide such a source. Most of the calculations are specific to generalised Lotka-Volterra systems, but much can be transferred to disordered systems in more general.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14289v3</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>q-bio.PE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tobias Galla</dc:creator>
    </item>
    <item>
      <title>A Dynamical Model of Neural Scaling Laws</title>
      <link>https://arxiv.org/abs/2402.01092</link>
      <description>arXiv:2402.01092v3 Announce Type: replace-cross 
Abstract: On a variety of tasks, the performance of neural networks predictably improves with training time, dataset size and model size across many orders of magnitude. This phenomenon is known as a neural scaling law. Of fundamental importance is the compute-optimal scaling law, which reports the performance as a function of units of compute when choosing model sizes optimally. We analyze a random feature model trained with gradient descent as a solvable model of network training and generalization. This reproduces many observations about neural scaling laws. First, our model makes a prediction about why the scaling of performance with training time and with model size have different power law exponents. Consequently, the theory predicts an asymmetric compute-optimal scaling rule where the number of training steps are increased faster than model parameters, consistent with recent empirical observations. Second, it has been observed that early in training, networks converge to their infinite-width dynamics at a rate $1/\textit{width}$ but at late time exhibit a rate $\textit{width}^{-c}$, where $c$ depends on the structure of the architecture and task. We show that our model exhibits this behavior. Lastly, our theory shows how the gap between training and test loss can gradually build up over time due to repeated reuse of data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01092v3</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Blake Bordelon, Alexander Atanasov, Cengiz Pehlevan</dc:creator>
    </item>
    <item>
      <title>Towards Understanding Inductive Bias in Transformers: A View From Infinity</title>
      <link>https://arxiv.org/abs/2402.05173</link>
      <description>arXiv:2402.05173v2 Announce Type: replace-cross 
Abstract: We study inductive bias in Transformers in the infinitely over-parameterized Gaussian process limit and argue transformers tend to be biased towards more permutation symmetric functions in sequence space. We show that the representation theory of the symmetric group can be used to give quantitative analytical predictions when the dataset is symmetric to permutations between tokens. We present a simplified transformer block and solve the model at the limit, including accurate predictions for the learning curves and network outputs. We show that in common setups, one can derive tight bounds in the form of a scaling law for the learnability as a function of the context length. Finally, we argue WikiText dataset, does indeed possess a degree of permutation symmetry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.05173v2</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024</arxiv:journal_reference>
      <dc:creator>Itay Lavie, Guy Gur-Ari, Zohar Ringel</dc:creator>
    </item>
    <item>
      <title>Operator Learning Renormalization Group</title>
      <link>https://arxiv.org/abs/2403.03199</link>
      <description>arXiv:2403.03199v2 Announce Type: replace-cross 
Abstract: In this paper, we present a general framework for quantum many-body simulations called the operator learning renormalization group (OLRG). Inspired by machine learning perspectives, OLRG is a generalization of Wilson's numerical renormalization group and White's density matrix renormalization group, which recursively builds a simulatable system to approximate a target system of the same number of sites via operator maps. OLRG uses a loss function to minimize the error of a target property directly by learning the operator map in lieu of a state ansatz. This loss function is designed by a scaling consistency condition that also provides a provable bound for real-time evolution. We implement two versions of the operator maps for classical and quantum simulations. The former, which we call the Operator Matrix Map, can be implemented via neural networks on classical computers. The latter, which we call the Hamiltonian Expression Map, generates device pulse sequences to leverage the capabilities of quantum computing hardware. We illustrate the performance of both maps for calculating time-dependent quantities in the quantum Ising model Hamiltonian.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03199v2</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xiu-Zhe Luo, Di Luo, Roger G. Melko</dc:creator>
    </item>
    <item>
      <title>Non-equilibrium orbital edge magnetization</title>
      <link>https://arxiv.org/abs/2405.11979</link>
      <description>arXiv:2405.11979v2 Announce Type: replace-cross 
Abstract: Uncompensated non-equilibrium orbital magnetization may arise at sample edges in the presence of charge current. The value of the effect scales as the product of the current density and the electron mean free path without any additional smallness. This non-relativistic phenomenon originates in a lack of inversion symmetry of the electron wave functions in a vicinity of sample interfaces. In a conducting layer, where $z$ direction is chosen perpendicular to the surface, and the current flows in $x$ direction, the non-equilibrium orbital magnetization points in $y$ direction. In a top-bottom symmetric layer, the orbital magnetization has an opposite sign near the top and bottom interfaces thus mimicking the symmetry of the spin-Hall effect but can exceed the latter by orders of magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11979v2</guid>
      <category>cond-mat.mes-hall</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.mtrl-sci</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J. Voss, I. A. Ado, M. Titov</dc:creator>
    </item>
  </channel>
</rss>
