<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 03 Dec 2024 05:00:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>C2HLSC: Leveraging Large Language Models to Bridge the Software-to-Hardware Design Gap</title>
      <link>https://arxiv.org/abs/2412.00214</link>
      <description>arXiv:2412.00214v1 Announce Type: new 
Abstract: High-Level Synthesis (HLS) tools offer rapid hardware design from C code, but their compatibility is limited by code constructs. This paper investigates Large Language Models (LLMs) for automatically refactoring C code into HLS-compatible formats. We present a case study using an LLM to rewrite C code for NIST 800-22 randomness tests, a QuickSort algorithm, and AES-128 into HLS-synthesizable C. The LLM iteratively transforms the C code guided by the, implementing functions like streaming data and hardware-specific signals. With the hindsight obtained from the case study, we implement a fully automated framework to refactor C code into HLS-compatible formats using LLMs. To tackle complex designs, we implement a preprocessing step that breaks down the hierarchy in order to approach the problem in a divide-and-conquer bottom-up way. We validated our framework on three ciphers, one hash function, five NIST 800-22 randomness tests, and a QuickSort algorithm. Our results show a high success rate on benchmarks that are orders of magnitude more complex than what has been achieved generating Verilog with LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00214v1</guid>
      <category>cs.AR</category>
      <category>cs.SE</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luca Collini, Siddharth Garg, Ramesh Karri</dc:creator>
    </item>
    <item>
      <title>Instruction Scheduling in the Saturn Vector Unit</title>
      <link>https://arxiv.org/abs/2412.00997</link>
      <description>arXiv:2412.00997v1 Announce Type: new 
Abstract: While the challenges and solutions for efficient execution of scalable vector ISAs on long-vector-length microarchitectures have been well established, not all of these solutions are suitable for short-vector-length implementations. This work proposes a novel microarchitecture for instruction sequencing in vector units with short architectural vector lengths. The proposed microarchitecture supports fine-granularity chaining, multi-issue out-of-order execution, zero dead-time, and run-ahead memory accesses with low area or complexity costs. We present the Saturn Vector Unit, a RTL implementation of a RVV vector unit. With our instruction scheduling mechanism, Saturn exhibits comparable or superior power, performance, and area characteristics compared to state-of-the-art long-vector and short-vector implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00997v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jerry Zhao, Daniel Grubb, Miles Rusch, Tianrui Wei, Kevin Anderson, Borivoje Nikolic, Krste Asanovic</dc:creator>
    </item>
    <item>
      <title>Fast Bipartitioned Hybrid Adder Utilizing Carry Select and Carry Lookahead Logic</title>
      <link>https://arxiv.org/abs/2412.01764</link>
      <description>arXiv:2412.01764v1 Announce Type: new 
Abstract: We present a novel fast bipartitioned hybrid adder (FBHA) that utilizes carry-select and carry-lookahead logic. The proposed FBHA is an accurate adder with a significant part and a less significant part joined together by a carry signal. In an N-bit FBHA, the K-bit less significant part is realized using carry-lookahead adder logic, and the (N-K)-bit significant part is realized using carry-select adder logic. The 32-bit addition was considered as an example operation for this work. Many 32-bit adders ranging from the slow ripple carry adder to the fast parallel-prefix Kogge-Stone adder and the proposed adder were synthesized using a 28-nm CMOS standard cell library and their design metrics were compared. A well-optimized FBHA achieved significant optimizations in design metrics compared to its high-speed adder counterparts and some examples are mentioned as follows: (a) 19.8% reduction in delay compared to a carry-lookahead adder; (b) 19.8% reduction in delay, 24.4% reduction in area, and 19.4% reduction in power compared to a carry-select adder; (c) 45.6% reduction in delay, and 13.5% reduction in power compared to a conditional sum adder; and (d) 46.5% reduction in area, and 29.3% reduction in power compared to the Kogge-Stone adder.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01764v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proc. IEEE 9th Intl. Conf. on Mathematics and Computers in Sciences and Industry, pp. 1-6, August 22-24, 2024, Greece</arxiv:journal_reference>
      <dc:creator>Padmanabhan Balasubramanian, Douglas L. Maskell</dc:creator>
    </item>
    <item>
      <title>Mixture of Cache-Conditional Experts for Efficient Mobile Device Inference</title>
      <link>https://arxiv.org/abs/2412.00099</link>
      <description>arXiv:2412.00099v1 Announce Type: cross 
Abstract: Mixture of Experts (MoE) LLMs have recently gained attention for their ability to enhance performance by selectively engaging specialized subnetworks or "experts" for each input. However, deploying MoEs on memory-constrained devices remains challenging, particularly when generating tokens sequentially with a batch size of one, as opposed to typical high-throughput settings involving long sequences or large batches. In this work, we optimize MoE on memory-constrained devices where only a subset of expert weights fit in DRAM. We introduce a novel cache-aware routing strategy that leverages expert reuse during token generation to improve cache locality. We evaluate our approach on language modeling, MMLU, and GSM8K benchmarks and present on-device results demonstrating 2$\times$ speedups on mobile devices, offering a flexible, training-free solution to extend MoE's applicability across real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00099v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrii Skliar, Ties van Rozendaal, Romain Lepert, Todor Boinovski, Mart van Baalen, Markus Nagel, Paul Whatmough, Babak Ehteshami Bejnordi</dc:creator>
    </item>
    <item>
      <title>MATTER: Multi-stage Adaptive Thermal Trojan for Efficiency &amp; Resilience degradation</title>
      <link>https://arxiv.org/abs/2412.00226</link>
      <description>arXiv:2412.00226v1 Announce Type: cross 
Abstract: As mobile systems become more advanced, the security of System-on-Chips (SoCs) is increasingly threatened by thermal attacks. This research introduces a new attack method called the Multi-stage Adaptive Thermal Trojan for Efficiency and Resilience Degradation (MATTER). MATTER takes advantage of weaknesses in Dynamic Thermal Management (DTM) systems by manipulating temperature sensor interfaces, which leads to incorrect thermal sensing and disrupts the SoC's ability to manage heat effectively. Our experiments show that this attack can degrade DTM performance by as much as 73%, highlighting serious vulnerabilities in modern mobile devices. By exploiting the trust placed in temperature sensors, MATTER causes DTM systems to make poor decisions i.e., failing to activate cooling when needed. This not only affects how well the system works but also threatens the lifespan of the hardware. This paper provides a thorough analysis of how MATTER works and emphasizes the need for stronger thermal management systems in SoCs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00226v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mehdi Elahi, Mohamed R. Elshamy, Abdel-Hameed Badawy, Mahdi Fazeli, Ahmad Patooghy</dc:creator>
    </item>
    <item>
      <title>Star arboricity relaxed book thickness of $K_n$</title>
      <link>https://arxiv.org/abs/2412.00971</link>
      <description>arXiv:2412.00971v1 Announce Type: cross 
Abstract: A book embedding of the complete graph $K_n$ needs $\lceil \frac{n}{2} \rceil$ pages and the page-subgraphs can be chosen to be spanning paths (for $n$ even) and one spanning star for $n$ odd. We show that all page-subgraphs can be chosen to be {\rm star forests} by including one extra {\rm cross-cap} page or two new ordinary pages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00971v1</guid>
      <category>math.CO</category>
      <category>cs.AR</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Paul C. Kainen</dc:creator>
    </item>
    <item>
      <title>Dual-Use Commercial and Military Communications on a Single Platform using RAN Domain Specific Language</title>
      <link>https://arxiv.org/abs/2412.00983</link>
      <description>arXiv:2412.00983v1 Announce Type: cross 
Abstract: Despite the success of the O-RAN Alliance in developing a set of interoperable interfaces, development of unique Radio Access Network (RAN) deployments remains challenging. This is especially true for military communications, where deployments are highly specialized with limited volume. The construction and maintenance of the RAN, which is a real time embedded system, is an ill-defined NP problem requiring teams of specialized system engineers, with specialized knowledge of the hardware platform. In this paper, we introduce a RAN Domain Specific Language (RDSL(TM)) to formally describe use cases, constraints, and multi-vendor hardware/software abstraction to allow automation of RAN construction. In this DSL, system requirements are declarative, and performance constraints are guaranteed by construction using an automated system solver. Using our RAN system solver platform, Gabriel(TM) we show how a system engineer can confidently modify RAN functionality without knowledge of the underlying hardware. We show benefits for specific system requirements when compared to the manually optimized, default configuration of the Intel FlexRAN(TM), and conclude that DSL/automation driven construction of the RAN can lead to significant power and latency benefits when the deployment constraints are tuned for a specific case. We give examples of how constraints and requirements can be formatted in a "Kubernetes style" YAML format which allows the use of other tools, such as Ansible, to integrate the generation of these requirements into higher level automation flows such as Service Management and Orchestration (SMO).</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00983v1</guid>
      <category>eess.SY</category>
      <category>cs.AR</category>
      <category>cs.PL</category>
      <category>cs.SY</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alan Gatherer, Chaitali Sengupta, Sudipta Sen, Jeffery H. Reed</dc:creator>
    </item>
    <item>
      <title>Agentic-HLS: An agentic reasoning based high-level synthesis system using large language models (AI for EDA workshop 2024)</title>
      <link>https://arxiv.org/abs/2412.01604</link>
      <description>arXiv:2412.01604v1 Announce Type: cross 
Abstract: Our aim for the ML Contest for Chip Design with HLS 2024 was to predict the validity, running latency in the form of cycle counts, utilization rate of BRAM (util-BRAM), utilization rate of lookup tables (uti-LUT), utilization rate of flip flops (util-FF), and the utilization rate of digital signal processors (util-DSP). We used Chain-of-thought techniques with large language models to perform classification and regression tasks. Our prediction is that with larger models reasoning was much improved. We release our prompts and propose a HLS benchmarking task for LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01604v1</guid>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Emre Oztas, Mahdi Jelodari</dc:creator>
    </item>
    <item>
      <title>Energy-Aware Heterogeneous Federated Learning via Approximate DNN Accelerators</title>
      <link>https://arxiv.org/abs/2402.18569</link>
      <description>arXiv:2402.18569v2 Announce Type: replace 
Abstract: In Federated Learning (FL), devices that participate in the training usually have heterogeneous resources, i.e., energy availability. In current deployments of FL, devices that do not fulfill certain hardware requirements are often dropped from the collaborative training. However, dropping devices in FL can degrade training accuracy and introduce bias or unfairness. Several works have tackled this problem on an algorithm level, e.g., by letting constrained devices train a subset of the server neural network (NN) model. However, it has been observed that these techniques are not effective w.r.t. accuracy. Importantly, they make simplistic assumptions about devices' resources via indirect metrics such as multiply accumulate (MAC) operations or peak memory requirements. We observe that memory access costs (that are currently not considered in simplistic metrics) have a significant impact on the energy consumption. In this work, for the first time, we consider on-device accelerator design for FL with heterogeneous devices. We utilize compressed arithmetic formats and approximate computing, targeting to satisfy limited energy budgets. Using a hardware-aware energy model, we observe that, contrary to the state of the art's moderate energy reduction, our technique allows for lowering the energy requirements (by 4x) while maintaining higher accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18569v2</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TCAD.2024.3509793</arxiv:DOI>
      <dc:creator>Kilian Pfeiffer, Konstantinos Balaskas, Kostas Siozios, J\"org Henkel</dc:creator>
    </item>
    <item>
      <title>Evaluating LLMs for Hardware Design and Test</title>
      <link>https://arxiv.org/abs/2405.02326</link>
      <description>arXiv:2405.02326v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated capabilities for producing code in Hardware Description Languages (HDLs). However, most of the focus remains on their abilities to write functional code, not test code. The hardware design process consists of both design and test, and so eschewing validation and verification leaves considerable potential benefit unexplored, given that a design and test framework may allow for progress towards full automation of the digital design pipeline. In this work, we perform one of the first studies exploring how a LLM can both design and test hardware modules from provided specifications. Using a suite of 8 representative benchmarks, we examined the capabilities and limitations of the state-of-the-art conversational LLMs when producing Verilog for functional and verification purposes. We taped out the benchmarks on a Skywater 130nm shuttle and received the functional chip.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02326v2</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.PL</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1109/LAD62341.2024.10691811</arxiv:DOI>
      <dc:creator>Jason Blocklove, Siddharth Garg, Ramesh Karri, Hammond Pearce</dc:creator>
    </item>
    <item>
      <title>Sorting-based FPGA Sliding Window Aggregation Engine without off-chip Memories</title>
      <link>https://arxiv.org/abs/2405.18168</link>
      <description>arXiv:2405.18168v2 Announce Type: replace 
Abstract: Aggregation queries are a series of computationally-demanding analytics operations on grouped and time series data. They include tasks such as summation or finding the median among the items of a group sharing a group ID, and within a specified number of the last observed tuples for sliding window aggregation (SWAG). They have a wide range of applications including in database analytics, operating systems, bank security and medical sensors. Existing challenges include the hardware complexity that comes with efficiently handling per-group states using hash-based approaches. This paper presents a pipelined and adaptable approach for calculating a wide range of aggregation queries with high throughput. It is then adapted for SWAG to achieve up to 476x speedup over the CPU of the same platform. It outperforms the state-of-the-art such as by being able to process 7.14x more tuples per second, and support 4x the window sizes with a fraction of the resources and no DRAM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18168v2</guid>
      <category>cs.AR</category>
      <category>cs.DB</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philippos Papaphilippou, Wayne Luk, David Gregg</dc:creator>
    </item>
    <item>
      <title>Efficient Deployment of Transformer Models in Analog In-Memory Computing Hardware</title>
      <link>https://arxiv.org/abs/2411.17367</link>
      <description>arXiv:2411.17367v2 Announce Type: replace 
Abstract: Analog in-memory computing (AIMC) has emerged as a promising solution to overcome the von Neumann bottleneck, accelerating neural network computations and improving computational efficiency. While AIMC has demonstrated success with architectures such as CNNs, MLPs, and RNNs, deploying transformer-based models using AIMC presents unique challenges. Transformers are expected to handle diverse downstream tasks and adapt to new user data or instructions after deployment, which requires more flexible approaches to suit AIMC constraints.
  In this paper, we propose a novel method for deploying pre-trained transformer models onto AIMC hardware. Unlike traditional approaches requiring hardware-aware training, our technique allows direct deployment without the need for retraining the original model. Instead, we utilize lightweight, low-rank adapters -- compact modules stored in digital cores -- to adapt the model to hardware constraints. We validate our approach on MobileBERT, demonstrating accuracy on par with, or even exceeding, a traditional hardware-aware training approach. Our method is particularly appealing in multi-task scenarios, as it enables a single analog model to be reused across multiple tasks. Moreover, it supports on-chip adaptation to new hardware constraints and tasks without updating analog weights, providing a flexible and versatile solution for real-world AI applications. Code is available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17367v2</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chen Li, Corey Lammie, Manuel Le Gallo, Bipin Rajendran</dc:creator>
    </item>
    <item>
      <title>ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD</title>
      <link>https://arxiv.org/abs/2410.03845</link>
      <description>arXiv:2410.03845v2 Announce Type: replace-cross 
Abstract: Open-source Electronic Design Automation (EDA) tools are rapidly transforming chip design by addressing key barriers of commercial EDA tools such as complexity, costs, and access. Recent advancements in Large Language Models (LLMs) have further enhanced efficiency in chip design by providing user assistance across a range of tasks like setup, decision-making, and flow automation. This paper introduces ORAssistant, a conversational assistant for OpenROAD, based on Retrieval-Augmented Generation (RAG). ORAssistant aims to improve the user experience for the OpenROAD flow, from RTL-GDSII by providing context-specific responses to common user queries, including installation, command usage, flow setup, and execution, in prose format. Currently, ORAssistant integrates OpenROAD, OpenROAD-flow-scripts, Yosys, OpenSTA, and KLayout. The data model is built from publicly available documentation and GitHub resources. The proposed architecture is scalable, supporting extensions to other open-source tools, operating modes, and LLM models. We use Google Gemini as the base LLM model to build and test ORAssistant. Early evaluation results of the RAG-based model show notable improvements in performance and accuracy compared to non-fine-tuned LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03845v2</guid>
      <category>cs.CL</category>
      <category>cs.AR</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aviral Kaintura, Palaniappan R, Shui Song Luar, Indira Iyer Almeida</dc:creator>
    </item>
  </channel>
</rss>
