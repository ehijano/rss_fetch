<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 18 Nov 2024 05:00:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Evaluating the Predictive Capacity of ChatGPT for Academic Peer Review Outcomes Across Multiple Platforms</title>
      <link>https://arxiv.org/abs/2411.09763</link>
      <description>arXiv:2411.09763v1 Announce Type: new 
Abstract: While previous studies have demonstrated that Large Language Models (LLMs) can predict peer review outcomes to some extent, this paper builds on that by introducing two new contexts and employing a more robust method - averaging multiple ChatGPT scores. The findings that averaging 30 ChatGPT predictions, based on reviewer guidelines and using only the submitted titles and abstracts, failed to predict peer review outcomes for F1000Research (Spearman's rho=0.00). However, it produced mostly weak positive correlations with the quality dimensions of SciPost Physics (rho=0.25 for validity, rho=0.25 for originality, rho=0.20 for significance, and rho = 0.08 for clarity) and a moderate positive correlation for papers from the International Conference on Learning Representations (ICLR) (rho=0.38). Including the full text of articles significantly increased the correlation for ICLR (rho=0.46) and slightly improved it for F1000Research (rho=0.09), while it had variable effects on the four quality dimension correlations for SciPost LaTeX files. The use of chain-of-thought system prompts slightly increased the correlation for F1000Research (rho=0.10), marginally reduced it for ICLR (rho=0.37), and further decreased it for SciPost Physics (rho=0.16 for validity, rho=0.18 for originality, rho=0.18 for significance, and rho=0.05 for clarity). Overall, the results suggest that in some contexts, ChatGPT can produce weak pre-publication quality assessments. However, the effectiveness of these assessments and the optimal strategies for employing them vary considerably across different platforms, journals, and conferences. Additionally, the most suitable inputs for ChatGPT appear to differ depending on the platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09763v1</guid>
      <category>cs.DL</category>
      <category>cs.CL</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mike Thelwall, Abdullah Yaghi</dc:creator>
    </item>
    <item>
      <title>Research evaluation with ChatGPT: Is it age, country, length, or field biased?</title>
      <link>https://arxiv.org/abs/2411.09768</link>
      <description>arXiv:2411.09768v1 Announce Type: new 
Abstract: Some research now suggests that ChatGPT can estimate the quality of journal articles from their titles and abstracts. This has created the possibility to use ChatGPT quality scores, perhaps alongside citation-based formulae, to support peer review for research evaluation. Nevertheless, ChatGPT's internal processes are effectively opaque, despite it writing a report to support its scores, and its biases are unknown. This article investigates whether publication date and field are biasing factors. Based on submitting a monodisciplinary journal-balanced set of 117,650 articles from 26 fields published in the years 2003, 2008, 2013, 2018 and 2023 to ChatGPT 4o-mini, the results show that average scores increased over time, and this was not due to author nationality or title and abstract length changes. The results also varied substantially between fields, and first author countries. In addition, articles with longer abstracts tended to receive higher scores, but plausibly due to such articles tending to be better rather than due to ChatGPT analysing more text. Thus, for the most accurate research quality evaluation results from ChatGPT, it is important to normalise ChatGPT scores for field and year and check for anomalies caused by sets of articles with short abstracts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09768v1</guid>
      <category>cs.DL</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mike Thelwall, Zeyneb Kurt</dc:creator>
    </item>
    <item>
      <title>Journal Quality Factors from ChatGPT: More meaningful than Impact Factors?</title>
      <link>https://arxiv.org/abs/2411.09984</link>
      <description>arXiv:2411.09984v1 Announce Type: new 
Abstract: Purpose: Journal Impact Factors and other citation-based indicators are widely used and abused to help select journals to publish in or to estimate the value of a published article. Nevertheless, citation rates primarily reflect scholarly impact rather than other quality dimensions, including societal impact, originality, and rigour. In contrast, Journal Quality Factors (JQFs) are average quality score estimates given to a journal's articles by ChatGPT. Design: JQFs were compared with Polish, Norwegian and Finnish journal ranks and with journal citation rates for 1,300 journals with 130,000 articles from 2021 in large monodisciplinary journals in the 25 out of 27 Scopus broad fields of research for which it was possible. Outliers were also examined. Findings: JQFs correlated positively and mostly strongly (median correlation: 0.641) with journal ranks in 24 out of the 25 broad fields examined, indicating a nearly science-wide ability for ChatGPT to estimate journal quality. Journal citation rates had similarly high correlations with national journal ranks, however, so JQFs are not a universally better indicator. An examination of journals with JQFs not matching their journal ranks suggested that abstract styles may affect the result, such as whether the societal contexts of research are mentioned. Limitations: Different journal rankings may have given different findings because there is no agreed meaning for journal quality. Implications: The results suggest that JQFs are plausible as journal quality indicators in all fields and may be useful for the (few) research and evaluation contexts where journal quality is an acceptable proxy for article quality, and especially for fields like mathematics for which citations are not strong indicators of quality. Originality: This is the first attempt to estimate academic journal value with a Large Language Model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09984v1</guid>
      <category>cs.DL</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mike Thelwall, Kayvan Kousha</dc:creator>
    </item>
    <item>
      <title>The Link Between Large Scientific Collaboration and Productivity. Rethinking How to Estimate the Monetary Value of Publications</title>
      <link>https://arxiv.org/abs/2411.10278</link>
      <description>arXiv:2411.10278v1 Announce Type: cross 
Abstract: This paper addresses how to assign a monetary value to scientific publications, particularly in the case of multi-author papers arising from large-scale research collaborations. Contemporary science increasingly relies on extensive and varied collaborations to tackle global challenges in fields such as life sciences, climate science, energy, high-energy physics, astronomy, and many others. We argue that existing literature fails to address the collaborative nature of research by overlooking the relationship between coauthorship and scientists productivity. Using the Marginal Cost of Production (MCP) approach, we first highlight the methodological limitations of ignoring this relationship, then propose a generalised MCP model to value co-authorship. As a case study, we examine High-Energy Physics (HEP) collaborations at the Large Hadron Collider (LHC) at CERN, analysing approximately half a million scientific outputs by over 50,000 authors from 1990 to 2021. Our findings indicate that collaborative adjustments yield monetary valuations for subsets of highly collaborative papers up to 3 orders of magnitude higher than previous estimates, with elevated values correlating with high research quality. This study contributes to the literature on research output evaluation, addressing debates in science policy around assessing research performance and impact. Our methodology is applicable to authorship valuation both within academia and in large-scale scientific collaborations, fitting diverse research impact assessment frameworks or as self-standing procedure. Additionally, we discuss the conditions under which this method may complement survey-based approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10278v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.DL</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.14141946</arxiv:DOI>
      <dc:creator>Francesco Giffoni, Emanuela Sirtori, Louis Colnot</dc:creator>
    </item>
    <item>
      <title>EHRs Data Harmonization Platform, an easy-to-use shiny app based on recodeflow for harmonizing and deriving clinical features</title>
      <link>https://arxiv.org/abs/2411.10342</link>
      <description>arXiv:2411.10342v1 Announce Type: cross 
Abstract: Electronic health records (EHRs) contain important longitudinal information on individuals who have received medical care. Traditionally, EHRs have been used to support a wide range of administrative activities such as billing and clinical workflow, but, given the depth and breadth of clinical and demographic data they contain, they are increasingly being used to provide real-world data for research. Although EHR data have enormous research potential, the full realization of that potential requires a data management strategy that extracts from large EHR databases, that are collected from a range of care settings and time periods, well-documented research-relevant data that can be used by different researchers. Having a common well-documented data management strategy for EHR will support reproducible research and sharing documentation on research variables that are derived from EHR variables is important to open science. In this short paper, we describe the EHRs Data Harmonization Platform. The platform is based on an easy to use web app a publicly available at https://poxotn-arian-aminoleslami.shinyapps.io/Arian/ and as a standalone software package at https://github.com/ArianAminoleslami/EHRs-Data Harmonization-Platform, that is linked to an existing R library for data harmonization called recodeflow. The platform can be used to extract, document, and harmonize variables from EHR and it can also be used to document and share research variables that have been derived from those EHR data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10342v1</guid>
      <category>cs.DB</category>
      <category>cs.DL</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Arian Aminoleslami, Geoffrey M. Anderson, Davide Chicco</dc:creator>
    </item>
  </channel>
</rss>
