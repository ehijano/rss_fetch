<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Feb 2026 05:01:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Unsupervised Denoising of Diffusion-Weighted Images with Bias and Variance Corrected Noise Modeling</title>
      <link>https://arxiv.org/abs/2602.22235</link>
      <description>arXiv:2602.22235v1 Announce Type: new 
Abstract: Diffusion magnetic resonance imaging (dMRI) plays a vital role in both clinical diagnostics and neuroscience research. However, its inherently low signal-to-noise ratio (SNR), especially under high diffusion weighting, significantly degrades image quality and impairs downstream analysis. Recent self-supervised and unsupervised denoising methods offer a practical solution by enhancing image quality without requiring clean references. However, most of these methods do not explicitly account for the non-Gaussian noise characteristics commonly present in dMRI magnitude data during the supervised learning process, potentially leading to systematic bias and heteroscedastic variance, particularly under low-SNR conditions. To overcome this limitation, we introduce noise-corrected training objectives that explicitly model Rician statistics. Specifically, we propose two alternative loss functions: one derived from the first-order moment to remove mean bias, and another from the second-order moment to correct squared-signal bias. Both losses include adaptive weighting to account for variance heterogeneity and can be used without changing the network architecture. These objectives are instantiated in an image-specific, unsupervised Deep Image Prior (DIP) framework. Comprehensive experiments on simulated and in-vivo dMRI show that the proposed losses effectively reduce Rician bias and suppress noise fluctuations, yielding higher image quality and more reliable diffusion metrics than state-of-the-art denoising baselines. These results underscore the importance of bias- and variance-aware noise modeling for robust dMRI analysis under low-SNR conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22235v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <category>eess.IV</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jine Xie, Zhicheng Zhang, Yunwei Chen, Yanqiu Feng, Xinyuan Zhang</dc:creator>
    </item>
    <item>
      <title>What Topological and Geometric Structure Do Biological Foundation Models Learn? Evidence from 141 Hypotheses</title>
      <link>https://arxiv.org/abs/2602.22289</link>
      <description>arXiv:2602.22289v1 Announce Type: new 
Abstract: When biological foundation models such as scGPT and Geneformer process single-cell gene expression, what geometric and topological structure forms in their internal representations? Is that structure biologically meaningful or a training artifact, and how confident should we be in such claims? We address these questions through autonomous large-scale hypothesis screening: an AI-driven executor-brainstormer loop that proposed, tested, and refined 141 geometric and topological hypotheses across 52 iterations, covering persistent homology, manifold distances, cross-model alignment, community structure, and directed topology, all with explicit null controls and disjoint gene-pool splits.
  Three principal findings emerge. First, the models learn genuine geometric structure. Gene embedding neighborhoods exhibit non-trivial topology, with persistent homology significant in 11 of 12 transformer layers at p &lt; 0.05 in the weakest domain and 12 of 12 in the other two. A multi-level distance hierarchy shows that manifold-aware metrics outperform Euclidean distance for identifying regulatory gene pairs, and graph community partitions track known transcription factor target relationships. Second, this structure is shared across independently trained models. CCA alignment between scGPT and Geneformer yields canonical correlation of 0.80 and gene retrieval accuracy of 72 percent, yet none of 19 tested methods reliably recover gene-level correspondences. The models agree on the global shape of gene space but not on precise gene placement. Third, the structure is more localized than it first appears. Under stringent null controls applied across all null families, robust signal concentrates in immune tissue, while lung and external lung signals weaken substantially.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22289v1</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <category>q-bio.GN</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ihor Kendiukhov</dc:creator>
    </item>
    <item>
      <title>An Active Learning Framework for Data-Efficient, Human-in-the-Loop Enzyme Function Prediction</title>
      <link>https://arxiv.org/abs/2602.23269</link>
      <description>arXiv:2602.23269v1 Announce Type: new 
Abstract: Generalizable protein function prediction is increasingly constrained by the growing mismatch between exponentially expanding sequences of environmental proteins and the comparatively slow accumulation of experimentally verified functional data. Active learning offers a promising path forward for accelerating biological function prediction, by selecting the most informative proteins to experimentally annotate for data-efficient training, yet its potential remains largely unexplored. We introduce HATTER (Human-in-the-loop Adaptive Toolkit for Transferable Enzyme Representations), a modular framework that integrates multiple active learning strategies with human-in-the-loop experimental annotation to efficiently fine tune function prediction models. We compare active learning training to standard supervised training for biological enzyme function prediction, demonstrating that active learning achieves performance comparable to standard training across diverse protein sequence evaluation datasets while requiring fewer model updates, processing less data, and substantially reducing computational cost. Interestingly, point-based uncertainty sampling methods like entropy or margin sampling perform as well or better than more complex acquisition functions such as bayesian sampling or BALD, highlighting the relative importance of sequence diversity in training datasets and model architecture design. These results demonstrate that human-in-the-loop active learning can efficiently accelerate enzyme discovery, providing a flexible platform for adaptive, scalable, and expert-guided protein function prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.23269v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashley Babjac, Adrienne Hoarfrost</dc:creator>
    </item>
    <item>
      <title>CryoNet.Refine: A One-step Diffusion Model for Rapid Refinement of Structural Models with Cryo-EM Density Map Restraints</title>
      <link>https://arxiv.org/abs/2602.22263</link>
      <description>arXiv:2602.22263v1 Announce Type: cross 
Abstract: High-resolution structure determination by cryo-electron microscopy (cryo-EM) requires the accurate fitting of an atomic model into an experimental density map. Traditional refinement pipelines such as Phenix.real_space_refine and Rosetta are computationally expensive, demand extensive manual tuning, and present a significant bottleneck for researchers. We present CryoNet.Refine, an end-to-end deep learning framework that automates and accelerates molecular structure refinement. Our approach utilizes a one-step diffusion model that integrates a density-aware loss function with robust stereochemical restraints, enabling rapid optimization of a structure against experimental data. CryoNet.Refine provides a unified and versatile solution capable of refining protein complexes as well as DNA/RNA-protein complexes. In benchmarks against Phenix.real_space_refine, CryoNet.Refine consistently achieves substantial improvements in both model-map correlation and overall geometric quality metrics. By offering a scalable, automated, and powerful alternative, CryoNet.Refine aims to serve as an essential tool for next-generation cryo-EM structure refinement. Web server: https://cryonet.ai/refine; Source code: https://github.com/kuixu/cryonet.refine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22263v1</guid>
      <category>q-bio.BM</category>
      <category>cs.AI</category>
      <category>eess.IV</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fuyao Huang, Xiaozhu Yu, Kui Xu, Qiangfeng Cliff Zhang</dc:creator>
    </item>
    <item>
      <title>Forecasting Antimicrobial Resistance Trends Using Machine Learning on WHO GLASS Surveillance Data: A Retrieval-Augmented Generation Approach for Policy Decision Support</title>
      <link>https://arxiv.org/abs/2602.22673</link>
      <description>arXiv:2602.22673v1 Announce Type: cross 
Abstract: Antimicrobial resistance (AMR) is a growing global crisis projected to cause 10 million deaths per year by 2050. While the WHO Global Antimicrobial Resistance and Use Surveillance System (GLASS) provides standardized surveillance data across 44 countries, few studies have applied machine learning to forecast population-level resistance trends from this data. This paper presents a two-component framework for AMR trend forecasting and evidence-grounded policy decision support. We benchmark six models -- Naive, Linear Regression, Ridge Regression, XGBoost, LightGBM, and LSTM -- on 5,909 WHO GLASS observations across six WHO regions (2021-2023). XGBoost achieved the best performance with a test MAE of 7.07% and R-squared of 0.854, outperforming the naive baseline by 83.1%. Feature importance analysis identified the prior-year resistance rate as the dominant predictor (50.5% importance), while regional MAE ranged from 4.16% (European Region) to 10.14% (South-East Asia Region). We additionally implemented a Retrieval-Augmented Generation (RAG) pipeline combining a ChromaDB vector store of WHO policy documents with a locally deployed Phi-3 Mini language model, producing source-attributed, hallucination-constrained policy answers. Code and data are available at https://github.com/TanvirTurja</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22673v1</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Tanvir Hasan Turja</dc:creator>
    </item>
    <item>
      <title>Discrete turn strategies emerge in information-limited navigation</title>
      <link>https://arxiv.org/abs/2602.23324</link>
      <description>arXiv:2602.23324v1 Announce Type: cross 
Abstract: Navigation up a sensory gradient is one of the simplest behaviours, and the simplest strategy is run and tumble. But some organisms use other strategies, such as reversing direction or turning by some angle. Here we ask what drives the choice of strategy, which we frame as maximising up-gradient speed using a given amount of sensory information per unit time. We find that, without directional information on which way to turn, behavioural strategies which make sudden turns perform better than gradual steering. We see various transitions where a different strategy becomes optimal, such as a switch from reversing direction to fully re-orienting tumbles as more information becomes available. And, among more complex re-orientation strategies, we show that discrete turn angles are best, and see transitions in how many such angles the optimal strategy employs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.23324v1</guid>
      <category>physics.bio-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jose M. Betancourt, Matthew P. Leighton, Thierry Emonet, Benjamin B. Machta, Michael C. Abbott</dc:creator>
    </item>
    <item>
      <title>Synaptic spine head morphodynamics from graph grammar rules for actin dynamics</title>
      <link>https://arxiv.org/abs/2504.13812</link>
      <description>arXiv:2504.13812v2 Announce Type: replace 
Abstract: There is a morphodynamic component to synaptic learning by which changes in dendritic (postsynaptic) spine head size are associated with the strengthening or weakening of the synaptic connection between two neurons. The membrane shape and size dynamics is sculpted by the growth dynamics of the enclosed actin cytoskeleton. We use Dynamical Graph Grammars (DGGs) governing dynamic labelled graphs embedded in two dimensions to model networks of actin filaments and the enclosing membrane in spine head morphology. We demonstrate the flexibility and extensibility of the framework by encoding detailed biophysical as well as biochemical models, obeying constraints of invariance and conservation, in DGG rule sets. From graph-local energy functions for cytoskeleton actin interacting and membrane, we specialize dissipative stochastic dynamics to an exhaustive collection of graph-local neighborhood types for the rule left hand sides. Extensively simulating the resulting model delineates effects of four actin-binding proteins, and their epistatic relationships, on morphology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13812v2</guid>
      <category>q-bio.QM</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Hur, Thomas Bartol, Padmini Rangamani, Terrence Sejnowski, Eric Mjolsness</dc:creator>
    </item>
    <item>
      <title>Decoding Translation-Related Functional Sequences in 5'UTRs Using Interpretable Deep Learning Models</title>
      <link>https://arxiv.org/abs/2507.16801</link>
      <description>arXiv:2507.16801v2 Announce Type: replace 
Abstract: Understanding how 5' untranslated regions (5'UTRs) regulate mRNA translation is critical for controlling protein expression and designing effective therapeutic mRNAs. While recent deep learning models have shown promise in predicting translational efficiency from 5'UTR sequences, most are constrained by fixed input lengths and limited interpretability. We introduce UTR-STCNet, a Transformer-based architecture for flexible and biologically grounded modeling of variable-length 5'UTRs. UTR-STCNet integrates a Saliency-Aware Token Clustering (SATC) module that iteratively aggregates nucleotide tokens into multi-scale, semantically meaningful units based on saliency scores. A Saliency-Guided Transformer (SGT) block then captures both local and distal regulatory dependencies using a lightweight attention mechanism. This combined architecture achieves efficient and interpretable modeling without input truncation or increased computational cost. Evaluated across three benchmark datasets, UTR-STCNet consistently outperforms state-of-the-art baselines in predicting mean ribosome load (MRL), a key proxy for translational efficiency. Moreover, the model recovers known functional elements such as upstream AUGs and Kozak motifs, highlighting its potential for mechanistic insight into translation regulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16801v2</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxi Lin, Yaxue Fang, Zehong Zhang, Zhouwu Liu, Siyun Zhong, Zhongfang Wang, Fulong Yu</dc:creator>
    </item>
    <item>
      <title>Understanding protein function with a multimodal retrieval-augmented foundation model</title>
      <link>https://arxiv.org/abs/2508.04724</link>
      <description>arXiv:2508.04724v2 Announce Type: replace 
Abstract: Protein language models (PLMs) learn probability distributions over natural protein sequences. By learning from hundreds of millions of natural protein sequences, protein understanding and design capabilities emerge. Recent works have shown that scaling these models improves structure prediction, but does not seem to improve mutation understanding and representation quality for protein function prediction. We introduce PoET-2, a multimodal, retrieval-augmented protein foundation model that incorporates in-context learning of family-specific evolutionary constraints with optional structure conditioning to learn generative distributions over protein sequences. PoET-2 uses a hierarchical transformer encoder that is equivariant to sequence context ordering and a dual decoder architecture with both causal and masked language modeling objectives, allowing PoET-2 to operate in both fully generative and bidirectional representation learning modes. PoET-2 achieves state-of-the-art performance on zero-shot variant effect prediction, excelling at scoring variants with multiple mutations and challenging indel mutations. In supervised settings, PoET-2 embeddings outperform previous methods for learning sequence-function relationships, especially with small datasets. This work highlights the benefits of combining retrieval augmentation with multimodal, family-centric modeling for advancing protein foundation models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04724v2</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timothy Fei Truong Jr, Tristan Bepler</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification of Bacterial Microcompartment Permeability</title>
      <link>https://arxiv.org/abs/2509.23445</link>
      <description>arXiv:2509.23445v2 Announce Type: replace 
Abstract: Salmonella expresses bacterial microcompartments (MCPs) upon 1,2-propanediol exposure. MCPs are nanoscale protein-bound shells that encase enzymes for the cofactor-dependent 1,2-propanediol metabolism. They are hypothesized to limit exposure to the toxic intermediate, propionaldehyde, decrease cofactor involvement in competing reactions, and enhance flux. We construct a mass-action mathematical model of purified MCPs and calibrate parameters to measured metabolite concentrations. We constrain mass-action kinetic parameters to previously estimated Michaelis-Menten parameters. We identified two distinct fits with different dynamics in the pathway product, propionate, but similar goodness of fit. Across fits, we inferred that the MCP 1,2-propanediol and propionaldehyde permeability should be greater than 10^{-6} and 10^{-8} m/s, respectively. Our results identify parameter ranges consistent with prevailing theories that MCPs impose preferential diffusion to 1,2-propanediol over propionaldehyde, and sequester toxic propionaldehyde away from the cell cytosol. The bimodality of the posterior distribution arises from bimodality in the estimated coenzyme-A (CoA) permeability and inhibition rates. The MCP permeability to CoA was inferred to be either less than 10^{-8.8} m/s or greater than 10^{-7.3} m/s. In a high CoA permeability environment with low rates of CoA inhibition, enzymes produced metabolites by recycling (NAD+)/(NADH). In a low CoA permeability environment with high rates of CoA inhibition, enzymes required external NAD+/H to produce metabolites. Dynamics are consistent with prevailing hypotheses about MCP function to sequester toxic propionaldehyde, and additional collection of data points between 6 and 24 hours or characterization of enzyme inhibition rates could further reduce uncertainty and provide better permeability estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23445v2</guid>
      <category>q-bio.QM</category>
      <category>q-bio.SC</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Andre Archer, Brett J. Palmero, Charlotte Abrahamson, Carolyn E. Mills, Nolan W. Kennedy, Danielle Tullman-Ercek, Niall M. Mangan</dc:creator>
    </item>
    <item>
      <title>Global graph features unveiled by unsupervised geometric deep learning</title>
      <link>https://arxiv.org/abs/2503.05560</link>
      <description>arXiv:2503.05560v3 Announce Type: replace-cross 
Abstract: Graphs provide a powerful framework for modeling complex systems, but their structural variability poses significant challenges for analysis and classification. To address these challenges, we introduce GAUDI (Graph Autoencoder Uncovering Descriptive Information), a novel unsupervised geometric deep learning framework designed to capture both local details and global structure. GAUDI employs an innovative hourglass architecture with hierarchical pooling and upsampling layers linked through skip connections, which preserve essential connectivity information throughout the encoding-decoding process. Even though identical or highly similar underlying parameters describing a system's state can lead to significant variability in graph realizations, GAUDI consistently maps them into nearby regions of a structured and continuous latent space, effectively disentangling invariant process-level features from stochastic noise. We demonstrate GAUDI's versatility across multiple applications, including small-world networks modeling, characterization of protein assemblies from super-resolution microscopy, analysis of collective motion in the Vicsek model, and identification of age-related changes in brain connectivity. Comparison with related approaches highlights GAUDI's superior performance in analyzing complex graphs, providing new insights into emergent phenomena across diverse scientific domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05560v3</guid>
      <category>cs.LG</category>
      <category>cond-mat.soft</category>
      <category>physics.bio-ph</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mirja Granfors, Jes\'us Pineda, Blanca Zufiria Gerbol\'es, Joana B. Pereira, Carlo Manzo, Giovanni Volpe</dc:creator>
    </item>
    <item>
      <title>Random Matrix Theory-guided sparse PCA for single-cell RNA-seq data</title>
      <link>https://arxiv.org/abs/2509.15429</link>
      <description>arXiv:2509.15429v2 Announce Type: replace-cross 
Abstract: Single-cell RNA-seq provides detailed molecular snapshots of individual cells but is notoriously noisy. Variability stems from biological differences and technical factors, such as amplification bias and limited RNA capture efficiency, making it challenging to adapt computational pipelines to heterogeneous datasets or evolving technologies. As a result, most studies still rely on principal component analysis (PCA) for dimensionality reduction, valued for its interpretability and robustness, in spite of its known bias in high dimensions. Here, we improve upon PCA with a Random Matrix Theory (RMT)-based approach that guides the inference of sparse principal components using existing sparse PCA algorithms. We first introduce a novel biwhitening algorithm which self-consistently estimates the magnitude of transcriptomic noise affecting each gene in individual cells, without assuming a specific noise distribution. This enables the use of an RMT-based criterion to automatically select the sparsity level, rendering sparse PCA nearly parameter-free. Our mathematically grounded approach retains the interpretability of PCA while enabling robust, hands-off inference of sparse principal components. Across seven single-cell RNA-seq technologies and four sparse PCA algorithms, we show that this method systematically improves the reconstruction of the principal subspace and consistently outperforms PCA-, autoencoder-, and diffusion-based methods in cell-type classification tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15429v2</guid>
      <category>cs.LG</category>
      <category>physics.bio-ph</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Victor Chard\`es</dc:creator>
    </item>
  </channel>
</rss>
