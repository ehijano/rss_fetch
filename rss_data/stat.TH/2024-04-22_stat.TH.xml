<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Apr 2024 04:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Pure Significance Tests for Multinomial and Binomial Distributions: the Uniform Alternative</title>
      <link>https://arxiv.org/abs/2404.13248</link>
      <description>arXiv:2404.13248v1 Announce Type: new 
Abstract: A {\it pure significance test} (PST) tests a simple null hypothesis $H_f:Y\sim f$ {\it without specifying an alternative hypothesis} by rejecting $H_f$ for {\it small} values of $f(Y)$. When the sample space supports a proper uniform pmf $f_\mathrm{unif}$, the PST can be viewed as a classical likelihood ratio test for testing $H_f$ against this uniform alternative. Under this interpretation, standard test features such as power, Kullback-Leibler divergence, and expected $p$-value can be considered. This report focuses on PSTs for multinomial and binomial distributions, and for the related goodness-of-fit testing problems with the uniform alternative. The case of repeated observations cannot be reduced to the single observation case via sufficiency. The {\it ordered binomial distribution}, apparently new, arises in the course of this study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13248v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael D. Perlman</dc:creator>
    </item>
    <item>
      <title>Higher-Order Graphon Theory: Fluctuations, Degeneracies, and Inference</title>
      <link>https://arxiv.org/abs/2404.13822</link>
      <description>arXiv:2404.13822v1 Announce Type: new 
Abstract: Exchangeable random graphs, which include some of the most widely studied network models, have emerged as the mainstay of statistical network analysis in recent years. Graphons, which are the central objects in graph limit theory, provide a natural way to sample exchangeable random graphs. It is well known that network moments (motif/subgraph counts) identify a graphon (up to an isomorphism), hence, understanding the sampling distribution of subgraph counts in random graphs sampled from a graphon is pivotal for nonparametric network inference. In this paper, we derive the joint asymptotic distribution of any finite collection of network moments in random graphs sampled from a graphon, that includes both the non-degenerate case (where the distribution is Gaussian) as well as the degenerate case (where the distribution has both Gaussian or non-Gaussian components). This provides the higher-order fluctuation theory for subgraph counts in the graphon model. We also develop a novel multiplier bootstrap for graphons that consistently approximates the limiting distribution of the network moments (both in the Gaussian and non-Gaussian regimes). Using this and a procedure for testing degeneracy, we construct joint confidence sets for any finite collection of motif densities. This provides a general framework for statistical inference based on network moments in the graphon model. To illustrate the broad scope of our results we also consider the problem of detecting global structure (that is, testing whether the graphon is a constant function) based on small subgraphs. We propose a consistent test for this problem, invoking celebrated results on quasi-random graphs, and derive its limiting distribution both under the null and the alternative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13822v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anirban Chatterjee, Soham Dan, Bhaswar B. Bhattacharya</dc:creator>
    </item>
    <item>
      <title>A Geometric Perspective on Double Robustness by Semiparametric Theory and Information Geometry</title>
      <link>https://arxiv.org/abs/2404.13960</link>
      <description>arXiv:2404.13960v1 Announce Type: new 
Abstract: Double robustness (DR) is a widely-used property of estimators that provides protection against model misspecification and slow convergence of nuisance functions. While DR is a global property on the probability distribution manifold, it often coincides with influence curves, which only ensure orthogonality to nuisance directions locally. This apparent discrepancy raises fundamental questions about the theoretical underpinnings of DR.
  In this short communication, we address two key questions: (1) Why do influence curves frequently imply DR "for free"? (2) Under what conditions do DR estimators exist for a given statistical model and parameterization? Using tools from semiparametric theory, we show that convexity is the crucial property that enables influence curves to imply DR. We then derive necessary and sufficient conditions for the existence of DR estimators under a mean squared differentiable path-connected parameterization.
  Our main contribution also lies in the novel geometric interpretation of DR using information geometry. By leveraging concepts such as parallel transport, m-flatness, and m-curvature freeness, we characterize DR in terms of invariance along submanifolds. This geometric perspective deepens the understanding of when and why DR estimators exist.
  The results not only resolve apparent mysteries surrounding DR but also have practical implications for the construction and analysis of DR estimators. The geometric insights open up new connections and directions for future research. Our findings aim to solidify the theoretical foundations of a fundamental concept and contribute to the broader understanding of robust estimation in statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13960v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Ying</dc:creator>
    </item>
    <item>
      <title>Estimation for SLS models: finite sample guarantees</title>
      <link>https://arxiv.org/abs/2404.14227</link>
      <description>arXiv:2404.14227v1 Announce Type: new 
Abstract: This note continues and extends the study from Spokoiny (2023a) about estimation for parametric models with possibly large or even infinite parameter dimension. We consider a special class of stochastically linear smooth (SLS) models satisfying three major conditions: the stochastic component of the log-likelihood is linear in the model parameter, while the expected log-likelihood is a smooth and concave function. For the penalized maximum likelihood estimators (pMLE), we establish several finite sample bounds about its concentration and large deviations as well as the Fisher and Wilks expansions and risk bounds. In all results, the remainder is given explicitly and can be evaluated in terms of the effective sample size $ n $ and effective parameter dimension $ \mathbb{p} $ which allows us to identify the so-called \emph{critical parameter dimension}. The results are also dimension and coordinate-free. Despite generality, all the presented bounds are nearly sharp and the classical asymptotic results can be obtained as simple corollaries. Our results indicate that the use of advanced fourth-order expansions allows to relax the critical dimension condition $ \mathbb{p}^{3} \ll n $ from Spokoiny (2023a) to $ \mathbb{p}^{3/2} \ll n $. Examples for classical models like logistic regression, log-density and precision matrix estimation illustrate the applicability of general results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14227v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladimir Spokoiny</dc:creator>
    </item>
    <item>
      <title>Colored Stochastic Multiplicative Processes with Additive Noise Unveil a Third-Order PDE, Defying Conventional FPE and Fick-Law Paradigms</title>
      <link>https://arxiv.org/abs/2404.14229</link>
      <description>arXiv:2404.14229v1 Announce Type: new 
Abstract: Research on stochastic differential equations (SDE) involving both additive and multiplicative noise has been extensive. In situations where the primary process is driven by a multiplicative stochastic process, additive white noise typically represents an intrinsic and unavoidable fast factor, including phenomena like thermal fluctuations, inherent uncertainties in measurement processes, or rapid wind forcing in ocean dynamics. This work focuses on a significant class of such systems, particularly those characterized by linear drift and multiplicative noise, extensively explored in the literature. Conventionally, multiplicative stochastic processes are also treated as white noise in existing studies. However, when considering colored multiplicative noise, the emphasis has been on characterizing the far tails of the probability density function (PDF), regardless of the spectral properties of the noise. In the absence of additive noise and with a general colored multiplicative SDE, standard perturbation approaches lead to a second-order PDE known as the Fokker-Planck Equation (FPE), consistent with Fick's law. This investigation unveils a notable departure from this standard behavior when introducing additive white noise. At the leading order of the stochastic process strength, perturbation approaches yield a \textit{third-order PDE}, irrespective of the white noise intensity. The breakdown of the FPE further signifies the breakdown of Fick's law. Additionally, we derive the explicit solution for the equilibrium PDF corresponding to this third-order PDE Master Equation. Through numerical simulations, we demonstrate significant deviations from outcomes derived using the FPE obtained through the application of Fick's law.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14229v1</guid>
      <category>math.ST</category>
      <category>cond-mat.stat-mech</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Bianucci, Mauro Bologna, Riccardo Mannella</dc:creator>
    </item>
    <item>
      <title>Extrapolation and generative algorithms for three applications in finance</title>
      <link>https://arxiv.org/abs/2404.13355</link>
      <description>arXiv:2404.13355v1 Announce Type: cross 
Abstract: For three applications of central interest in finance, we demonstrate the relevance of numerical algorithms based on reproducing kernel Hilbert space (RKHS) techniques. Three use cases are investigated. First, we show that extrapolating from few pricer examples leads to sufficiently accurate and computationally efficient results so that our algorithm can serve as a pricing framework. The second use case concerns reverse stress testing, which is formulated as an inversion function problem and is treated here via an optimal transport technique in combination with the notions of kernel-based encoders, decoders, and generators. Third, we show that standard techniques for time series analysis can be enhanced by using the proposed generative algorithms. Namely, we use our algorithm in order to extend the validity of any given quantitative model. Our approach allows for conditional analysis as well as for escaping the `Gaussian world'. This latter property is illustrated here with a portfolio investment strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13355v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philippe G. LeFloch, Jean-Marc Mercier, Shohruh Miryusupov</dc:creator>
    </item>
    <item>
      <title>Elicitability and identifiability of tail risk measures</title>
      <link>https://arxiv.org/abs/2404.14136</link>
      <description>arXiv:2404.14136v1 Announce Type: cross 
Abstract: Tail risk measures are fully determined by the distribution of the underlying loss beyond its quantile at a certain level, with Value-at-Risk and Expected Shortfall being prime examples. They are induced by law-based risk measures, called their generators, evaluated on the tail distribution. This paper establishes joint identifiability and elicitability results of tail risk measures together with the corresponding quantile, provided that their generators are identifiable and elicitable, respectively. As an example, we establish the joint identifiability and elicitability of the tail expectile together with the quantile. The corresponding consistent scores constitute a novel class of weighted scores, nesting the known class of scores of Fissler and Ziegel for the Expected Shortfall together with the quantile. For statistical purposes, our results pave the way to easier model fitting for tail risk measures via regression and the generalized method of moments, but also model comparison and model validation in terms of established backtesting procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14136v1</guid>
      <category>q-fin.ST</category>
      <category>math.ST</category>
      <category>q-fin.RM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tobias Fissler, Fangda Liu, Ruodu Wang, Linxiao Wei</dc:creator>
    </item>
    <item>
      <title>Asymptotic Theory for Doubly Robust Estimators with Continuous-Time Nuisance Parameters</title>
      <link>https://arxiv.org/abs/2302.06739</link>
      <description>arXiv:2302.06739v2 Announce Type: replace 
Abstract: Doubly robust estimators have gained widespread popularity in various fields due to their ability to provide unbiased estimates under model misspecification. However, the asymptotic theory for doubly robust estimators with continuous-time nuisance parameters remains largely unexplored. In this short communication, we address this gap by developing a general asymptotic theory for a class of doubly robust estimating equations involving stochastic processes and Riemann-Stieltjes integrals. We introduce generic assumptions on the nuisance parameter estimators that ensure the consistency and asymptotic normality of the resulting doubly robust estimator. Our results cover both the model doubly robust estimator, which relies on parametric or semiparametric models, and the rate doubly robust estimator, which allows for flexible machine learning methods. We discuss the implications of our findings and highlight the key differences between the continuous-time setting and the classical theory for doubly robust estimators. Our work provides a solid theoretical foundation for the use of doubly robust estimators in complex settings with continuous-time nuisance parameters, paving the way for future research and applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.06739v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Ying</dc:creator>
    </item>
    <item>
      <title>PanIC: consistent information criteria for general model selection problems</title>
      <link>https://arxiv.org/abs/2303.03649</link>
      <description>arXiv:2303.03649v3 Announce Type: replace 
Abstract: Model selection is a ubiquitous problem that arises in the application of many statistical and machine learning methods. In the likelihood and related settings, it is typical to use the method of information criteria (IC) to choose the most parsimonious among competing models by penalizing the likelihood-based objective function. Theorems guaranteeing the consistency of IC can often be difficult to verify and are often specific and bespoke. We present a set of results that guarantee consistency for a class of IC, which we call PanIC (from the Greek root 'pan', meaning 'of everything'), with easily verifiable regularity conditions. The PanIC are applicable in any loss-based learning problem and are not exclusive to likelihood problems. We illustrate the verification of regularity conditions for model selection problems regarding finite mixture models, least absolute deviation and support vector regression, and principal component analysis, and we demonstrate the effectiveness of the PanIC for such problems via numerical simulations. Furthermore, we present new sufficient conditions for the consistency of BIC-like estimators and provide comparisons of the BIC to PanIC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.03649v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hien Duy Nguyen</dc:creator>
    </item>
    <item>
      <title>Corrected generalized cross-validation for finite ensembles of penalized estimators</title>
      <link>https://arxiv.org/abs/2310.01374</link>
      <description>arXiv:2310.01374v2 Announce Type: replace 
Abstract: Generalized cross-validation (GCV) is a widely-used method for estimating the squared out-of-sample prediction risk that employs a scalar degrees of freedom adjustment (in a multiplicative sense) to the squared training error. In this paper, we examine the consistency of GCV for estimating the prediction risk of arbitrary ensembles of penalized least-squares estimators. We show that GCV is inconsistent for any finite ensemble of size greater than one. Towards repairing this shortcoming, we identify a correction that involves an additional scalar correction (in an additive sense) based on degrees of freedom adjusted training errors from each ensemble component. The proposed estimator (termed CGCV) maintains the computational advantages of GCV and requires neither sample splitting, model refitting, or out-of-bag risk estimation. The estimator stems from a finer inspection of the ensemble risk decomposition and two intermediate risk estimators for the components in this decomposition. We provide a non-asymptotic analysis of the CGCV and the two intermediate risk estimators for ensembles of convex penalized estimators under Gaussian features and a linear response model. Furthermore, in the special case of ridge regression, we extend the analysis to general feature and response distributions using random matrix theory, which establishes model-free uniform consistency of CGCV.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.01374v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre C. Bellec, Jin-Hong Du, Takuya Koriyama, Pratik Patil, Kai Tan</dc:creator>
    </item>
    <item>
      <title>High-dimensional bootstrap and asymptotic expansion</title>
      <link>https://arxiv.org/abs/2404.05006</link>
      <description>arXiv:2404.05006v2 Announce Type: replace 
Abstract: The recent seminal work of Chernozhukov, Chetverikov and Kato has shown that bootstrap approximation for the maximum of a sum of independent random vectors is justified even when the dimension is much larger than the sample size. In this context, numerical experiments suggest that third-moment match bootstrap approximations would outperform normal approximation even without studentization, but the existing theoretical results cannot explain this phenomenon. In this paper, we first show that Edgeworth expansion, if justified, can give an explanation for this phenomenon. Second, we obtain valid Edgeworth expansions in the high-dimensional setting when the random vectors have Stein kernels. Finally, we prove the second-order accuracy of a double wild bootstrap method in this setting. As a byproduct, we find an interesting blessing of dimensionality phenomenon: The single third-moment match wild bootstrap is already second-order accurate in high-dimensions if the covariance matrix has identical diagonal entries and bounded eigenvalues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05006v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuta Koike</dc:creator>
    </item>
    <item>
      <title>From naive trees to Random Forests: A general approach for proving consistency of tree-based methods</title>
      <link>https://arxiv.org/abs/2404.06850</link>
      <description>arXiv:2404.06850v3 Announce Type: replace 
Abstract: Tree-based methods such as Random Forests are learning algorithms that have become an integral part of the statistical toolbox. The last decade has shed some light on theoretical properties such as their consistency for regression tasks. However, the usual proofs assume normal error terms as well as an additive regression function and are rather technical. We overcome these issues by introducing a simple and catchy technique for proving consistency under quite general assumptions. To this end, we introduce a new class of naive trees, which do the subspacing completely at random and independent of the data. We then give a direct proof of their consistency. Using them to bound the error of more complex tree-based approaches such as univariate and multivariate CARTs, Extra Randomized Trees, or Random Forests, we deduce the consistency of all of them. Since naive trees appear to be too simple for actual application, we further analyze their finite sample properties in a simulation and small benchmark study. We find a slow convergence speed and a rather poor predictive performance. Based on these results, we finally discuss to what extent consistency proofs help to justify the application of complex learning algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06850v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nico F\"oge, Markus Pauly, Lena Schmid, Marc Ditzhaus</dc:creator>
    </item>
    <item>
      <title>On the edge eigenvalues of the precision matrices of nonstationary autoregressive processes</title>
      <link>https://arxiv.org/abs/2109.02204</link>
      <description>arXiv:2109.02204v3 Announce Type: replace-cross 
Abstract: This paper investigates the structural changes in the parameters of first-order autoregressive models by analyzing the edge eigenvalues of the precision matrices. Specifically, edge eigenvalues in the precision matrix are observed if and only if there is a structural change in the autoregressive coefficients. We demonstrate that these edge eigenvalues correspond to the zeros of some determinantal equation. Additionally, we propose a consistent estimator for detecting outliers within the panel time series framework, supported by numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.02204v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junho Yang</dc:creator>
    </item>
    <item>
      <title>Efficient Integrated Volatility Estimation in the Presence of Infinite Variation Jumps via Debiased Truncated Realized Variations</title>
      <link>https://arxiv.org/abs/2209.10128</link>
      <description>arXiv:2209.10128v3 Announce Type: replace-cross 
Abstract: Statistical inference for stochastic processes based on high-frequency observations has been an active research area for more than two decades. One of the most well-known and widely studied problems has been the estimation of the quadratic variation of the continuous component of an It\^o semimartingale with jumps. Several rate- and variance-efficient estimators have been proposed in the literature when the jump component is of bounded variation. However, to date, very few methods can deal with jumps of unbounded variation. By developing new high-order expansions of the truncated moments of a locally stable L\'evy process, we propose a new rate- and variance-efficient volatility estimator for a class of It\^o semimartingales whose jumps behave locally like those of a stable L\'evy process with Blumenthal-Getoor index $Y\in (1,8/5)$ (hence, of unbounded variation). The proposed method is based on a two-step debiasing procedure for the truncated realized quadratic variation of the process and can also cover the case $Y&lt;1$. Our Monte Carlo experiments indicate that the method outperforms other efficient alternatives in the literature in the setting covered by our theoretical framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.10128v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>B. Cooper Boniece, Jos\'e E. Figueroa-L\'opez, Yuchen Han</dc:creator>
    </item>
    <item>
      <title>Generalized Score Matching</title>
      <link>https://arxiv.org/abs/2303.08987</link>
      <description>arXiv:2303.08987v2 Announce Type: replace-cross 
Abstract: Score matching is an estimation procedure that has been developed for statistical models whose probability density function is known up to proportionality but whose normalizing constant is intractable, so that maximum likelihood is difficult or impossible to implement. To date, applications of score matching have focused more on continuous IID models. Motivated by various data modelling problems, this article proposes a unified asymptotic theory of generalized score matching developed under the independence assumption, covering both continuous and discrete response data, thereby giving a sound basis for score-matchingbased inference. Real data analyses and simulation studies provide convincing evidence of strong practical performance of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.08987v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiazhen Xu, Janice L. Scealy, Andrew T. A. Wood, Tao Zou</dc:creator>
    </item>
    <item>
      <title>Explicit mutual information for simple networks and neurons with lognormal activities</title>
      <link>https://arxiv.org/abs/2307.00017</link>
      <description>arXiv:2307.00017v2 Announce Type: replace-cross 
Abstract: Networks with stochastic variables described by heavy tailed lognormal distribution are ubiquitous in nature, and hence they deserve an exact information-theoretic characterization. We derive analytical formulas for mutual information between elements of different networks with correlated lognormally distributed activities. In a special case, we find an explicit expression for mutual information between neurons when neural activities and synaptic weights are lognormally distributed, as suggested by experimental data. Comparison of this expression with the case when these two variables have short tails, reveals that mutual information with heavy tails for neurons and synapses is generally larger and can diverge for some finite variances in presynaptic firing rates and synaptic weights. This result suggests that evolution might prefer brains with heterogeneous dynamics to optimize information processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.00017v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevE.109.014117</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. E 109, 014117 (2024)</arxiv:journal_reference>
      <dc:creator>Maurycy Chwi{\l}ka, Jan Karbowski</dc:creator>
    </item>
    <item>
      <title>Discrete Nonparametric Causal Discovery Under Latent Class Confounding</title>
      <link>https://arxiv.org/abs/2311.07454</link>
      <description>arXiv:2311.07454v3 Announce Type: replace-cross 
Abstract: An acyclic causal structure can be described using a directed acyclic graph (DAG) with arrows indicating causation. The task of learning these structures from data is known as ``causal discovery''. Diverse populations or changing environments can sometimes give rise to heterogeneous data. This heterogeneity can be thought of as a mixture model with multiple ``sources'', each exerting their own distinct signature on the observed variables. From this perspective, the source is a latent common cause for every observed variable. While some methods for causal discovery are able to work around unobserved confounding in special cases, the only known ways to deal with a global confounder (such as a latent class) involve parametric assumptions. These assumptions are restrictive, especially for discrete variables. By focusing on discrete observables, we demonstrate that globally confounded causal structures can still be identifiable without parametric assumptions, so long as the number of latent classes remains small relative to the size and sparsity of the underlying DAG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.07454v3</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bijan Mazaheri, Spencer Gordon, Yuval Rabani, Leonard Schulman</dc:creator>
    </item>
    <item>
      <title>Gaussian dependence structure pairwise goodness-of-fit testing based on conditional covariance and the 20/60/20 rule</title>
      <link>https://arxiv.org/abs/2404.12696</link>
      <description>arXiv:2404.12696v2 Announce Type: replace-cross 
Abstract: We present a novel data-oriented statistical framework that assesses the presumed Gaussian dependence structure in a pairwise setting. This refers to both multivariate normality and normal copula goodness-of-fit testing. The proposed test clusters the data according to the 20/60/20 rule and confronts conditional covariance (or correlation) estimates on the obtained subsets. The corresponding test statistic has a natural practical interpretation, desirable statistical properties, and asymptotic pivotal distribution under the multivariate normality assumption. We illustrate the usefulness of the introduced framework using extensive power simulation studies and show that our approach outperforms popular benchmark alternatives. Also, we apply the proposed methodology to commodities market data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12696v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jakub Wo\'zny, Piotr Jaworski, Damian Jelito, Marcin Pitera, Agnieszka Wy{\l}oma\'nska</dc:creator>
    </item>
  </channel>
</rss>
