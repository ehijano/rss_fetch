<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 02 Jul 2025 01:32:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Oobleck: Low-Compromise Design for Fault Tolerant Accelerators</title>
      <link>https://arxiv.org/abs/2506.22654</link>
      <description>arXiv:2506.22654v1 Announce Type: new 
Abstract: Data center hardware refresh cycles are lengthening. However, increasing processor complexity is raising the potential for faults. To achieve longevity in the face of increasingly fault-prone datapaths, fault tolerance is needed, especially in on-chip accelerator datapaths. Previously researched methods for adding fault tolerance to accelerator designs require high area, lowering chip utilisation. We propose a novel architecture for accelerator fault tolerance, Oobleck, which leverages modular acceleration to enable fault tolerance without burdensome area requirements.
  In order to streamline the development and enforce modular conventions, we introduce the Viscosity language, an actor based approach to hardware-software co-design. Viscosity uses a single description of the accelerator's function and produces both hardware and software descriptions.
  Our high-level models of data centers indicate that our approach can decrease the number of failure-induced chip purchases inside data centers while not affecting aggregate throughput, thus reducing data center costs. To show the feasibility of our approach, we show three case-studies: FFT, AES, and DCT accelerators. We additionally profile the performance under the key parameters affecting latency. Under a single fault we can maintain speedups of between 1.7x-5.16x for accelerated applications over purely software implementations. We show further benefits can be achieved by adding hot-spare FPGAs into the chip.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22654v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guy Wilks, Brian Li, Jonathan Balkind</dc:creator>
    </item>
    <item>
      <title>Approximate Logic Synthesis Using BLASYS</title>
      <link>https://arxiv.org/abs/2506.22772</link>
      <description>arXiv:2506.22772v1 Announce Type: new 
Abstract: Approximate computing is an emerging paradigm where design accuracy can be traded for improvements in design metrics such as design area and power consumption. In this work, we overview our open-source tool, BLASYS, for synthesis of approximate circuits using Boolean Matrix Factorization (BMF). In our methodology the truth table of a given circuit is approximated using BMF to a controllable approximation degree, and the results of the factorization are used to synthesize the approximate circuit output. BLASYS scales up the computations to large circuits through the use of partition techniques, where an input circuit is partitioned into a number of interconnected subcircuits and then a design-space exploration technique identifies the best order for subcircuit approximations. BLASYS leads to a graceful trade-off between accuracy and full circuit complexity as measured by design area. Using an open-source design flow, we extensively evaluate our methodology on a number of benchmarks, where we demonstrate that the proposed methodology can achieve on average 48.14% in area savings, while introducing an average relative error of 5%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22772v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jingxiao Ma, Soheil Hashemi, Sherief Reda</dc:creator>
    </item>
    <item>
      <title>Sustainable operation of research infrastructure for novel computing</title>
      <link>https://arxiv.org/abs/2506.23901</link>
      <description>arXiv:2506.23901v1 Announce Type: new 
Abstract: Novel compute systems are an emerging research topic, aiming towards building next-generation compute platforms. For these systems to thrive, they need to be provided as research infrastructure to allow acceptance and usage by a large community. By the example of the neuromorphic BrainScaleS-2 system, we showcase the transformation from a laboratory setup to a sustainable, publicly available platform. It is embedded into a purpose-built institute, tightly coupling a conventional cluster with novel compute hardware. The network infrastructure is optimized for robust operation, even in the case of unintended behavior of individual devices. The systems themselves are packaged into 19-inch compatible units to allow for easy maintenance and extension. We operate the platform using modern CI/CD techniques and continuously assert its health using automated system monitoring. Finally, we share our lessons learned during the decade-long endeavor of operating analog neuromorphic systems as a publicly available research platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23901v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yannik Stradmann, Joscha Ilmberger, Eric M\"uller, Johannes Schemmel</dc:creator>
    </item>
    <item>
      <title>A Correlation-Based Design of RIS for Reduced Power Consumption and Simplified Control Circuitry</title>
      <link>https://arxiv.org/abs/2506.22702</link>
      <description>arXiv:2506.22702v1 Announce Type: cross 
Abstract: Aiming at simplifying the hardware structure and reducing the energy consumption in wireless communication via reconfigurable intelligent surfaces (RIS), this paper introduces a novel RIS design founded on the correlation between the phase shift values of the surface elements. First, a correlation analysis is conducted, considering the azimuth angle of a target device within a coverage region spanning from $-80^{\circ}$ to $80^{\circ}$. The correlation is demonstrated for different deployment cases, creating the basis for the new RIS structure, termed Connected-RIS, where correlated elements are designed to share the same control signal. The fundamental performance of the proposed design is then analyzed in terms of control signals, power consumption, and communication system performance, comparing it to two RIS structures with full control: one with the same size as the proposed design, and the other employing the minimum number of elements necessary to satisfy the fair coverage criterion. The correlation-based RIS design enables three-dimensional passive beamforming and significantly reduces the number of required load impedances and control signals, thereby lowering the hardware cost and simplifying the control circuitry. It also achieves substantial power savings as compared to the baseline schemes, while maintaining sufficient gain for a fair radio coverage. For instance, numerical simulations demonstrate that the proposed design reduces the power consumption by almost 86-92\% and the control signals by 83-98\% compared to operation with fully controlled RIS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22702v1</guid>
      <category>eess.SY</category>
      <category>cs.AR</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zina Mohamed, Ammar B. Kouki, Sonia A\"issa</dc:creator>
    </item>
    <item>
      <title>Not All Water Consumption Is Equal: A Water Stress Weighted Metric for Sustainable Computing</title>
      <link>https://arxiv.org/abs/2506.22773</link>
      <description>arXiv:2506.22773v2 Announce Type: cross 
Abstract: Water consumption is an increasingly critical dimension of computing sustainability, especially as AI workloads rapidly scale. However, current water impact assessment often overlooks where and when water stress is more severe. To fill in this gap, we present SCARF, the first general framework that evaluates water impact of computing by factoring in both spatial and temporal variations in water stress. SCARF calculates an Adjusted Water Impact (AWI) metric that considers both consumption volume and local water stress over time. Through three case studies on LLM serving, datacenters, and semiconductor fabrication plants, we show the hidden opportunities for reducing water impact by optimizing location and time choices, paving the way for water-sustainable computing. The code is available at https://github.com/jojacola/SCARF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22773v2</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>ACM SIGEnergy Energy Informatics Review (EIR), Volume 5 Issue 2, July 2025</arxiv:journal_reference>
      <dc:creator>Yanran Wu, Inez Hua, Yi Ding</dc:creator>
    </item>
    <item>
      <title>TriADA: Massively Parallel Trilinear Matrix-by-Tensor Multiply-Add Algorithm and Device Architecture for the Acceleration of 3D Discrete Transformations</title>
      <link>https://arxiv.org/abs/2506.22818</link>
      <description>arXiv:2506.22818v1 Announce Type: cross 
Abstract: Multilinear transformations are key in high-performance computing (HPC) and artificial intelligence (AI) workloads, where data is represented as tensors. However, their high computational and memory demands, which grow with dimensionality, often slow down critical tasks. Moreover, scaling computation by enlarging the number of parallel processing units substantially increases energy consumption, limiting widespread adoption, especially for sparse data, which is common in HPC and AI applications. This paper introduces the Trilinear Algorithm and isomorphic to algorithm Device Architecture (TriADA) to address these challenges with the following innovations: (1) a massively parallel, low-rank algorithm for computing a family of trilinear (3D) discrete orthogonal transformations (3D-DXTs), which is a special case of the more general 3-mode matrix-by-tensor multiplication (3D-GEMT); (2) a new outer-product-based GEMM kernel with decoupled streaming active memory, specially designed to accelerate 3D-GEMT operation; (3) an isomorphic to the proposed algorithm, fully distributed 3D network of mesh interconnected processing elements or cells with a coordinate-free, data-driven local processing activity, which is independent of problem size; (4) an elastic sparse outer-product (ESOP) method that avoids unnecessary computing and communication operations with zero-valued operands, thereby enhancing energy efficiency, computational accuracy, and stability. TriADA is capable of performing a variety of trilinear transformations with hypercubic arithmetic complexity in a linear number of time-steps. The massively parallel, scalable, and energy-efficient architecture of TriADA is ideal for accelerating multilinear tensor operations, which are the most demanding parts of AI and HPC workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22818v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <category>eess.SP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stanislav Sedukhin (The University of Aizu, Japan), Yoichi Tomioka (The University of Aizu, Japan), Kazuya Matsumoto (The University of Aizu, Japan), Yuichi Okuyama (The University of Aizu, Japan)</dc:creator>
    </item>
    <item>
      <title>CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors upon GPGPU Platforms</title>
      <link>https://arxiv.org/abs/2506.23405</link>
      <description>arXiv:2506.23405v1 Announce Type: cross 
Abstract: In contemporary general-purpose graphics processing units (GPGPUs), the continued increase in raw arithmetic throughput is constrained by the capabilities of the register file (single-cycle) and last-level cache (high bandwidth), which require the delivery of operands at a cadence demanded by wide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity, density, or bandwidth of these memories can unlock substantial performance gains; however, the recent stagnation of SRAM bit-cell scaling leads to inequivalent losses in compute density.
  To address the challenges posed by SRAM's scaling and leakage power consumption, this paper explores the potential CMOS+X integration of amorphous oxide semiconductor (AOS) transistors in capacitive, persistent memory topologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in multi-ported and high-bandwidth banked GPGPU memories. A detailed study of the density and energy tradeoffs of back-end-of-line (BEOL) integrated memories utilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while accounting for the macro-level limitations of integrating AOS candidate structures proposed by the device community (an aspect often overlooked in prior work). By exploiting the short lifetime of register operands, we propose a multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of the footprint of SRAM with over 70% lower standby power, enabling enhancements to compute capacity, such as larger warp sizes or processor counts. Benchmarks run on a validated NVIDIA Ampere-class GPU model, using a modified version of Accel-Sim, demonstrate improvements of up to 5.2x the performance per watt and an average 8% higher geometric mean instruction per cycle (IPC) on various compute- and memory-bound tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23405v1</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Faaiq Waqar, Ming-Yen Lee, Seongwon Yoon, Seongkwang Lim, Shimeng Yu</dc:creator>
    </item>
    <item>
      <title>Data-Driven Power Modeling and Monitoring via Hardware Performance Counter Tracking</title>
      <link>https://arxiv.org/abs/2506.23672</link>
      <description>arXiv:2506.23672v1 Announce Type: cross 
Abstract: Energy-centric design is paramount in the current embedded computing era: use cases require increasingly high performance at an affordable power budget, often under real-time constraints. Hardware heterogeneity and parallelism help address the efficiency challenge, but greatly complicate online power consumption assessments, which are essential for dynamic hardware and software stack adaptations. We introduce a novel power modeling methodology with state-of-the-art accuracy, low overhead, and high responsiveness, whose implementation does not rely on microarchitectural details. Our methodology identifies the Performance Monitoring Counters (PMCs) with the highest linear correlation to the power consumption of each hardware sub-system, for each Dynamic Voltage and Frequency Scaling (DVFS) state. The individual, simple models are composed into a complete model that effectively describes the power consumption of the whole system, achieving high accuracy and low overhead. Our evaluation reports an average estimation error of 7.5% for power consumption and 1.3% for energy. We integrate these models in the Linux kernel with Runmeter, an open-source, PMC-based monitoring framework. Runmeter manages PMC sampling and processing, enabling the execution of our power models at runtime. With a worst-case time overhead of only 0.7%, Runmeter provides responsive and accurate power measurements directly in the kernel. This information can be employed for actuation policies in workload-aware DVFS and power-aware, closed-loop task scheduling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23672v1</guid>
      <category>cs.PF</category>
      <category>cs.AR</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.sysarc.2025.103504</arxiv:DOI>
      <arxiv:journal_reference>Journal of Systems Architecture, 2025, 103504, ISSN 1383-7621</arxiv:journal_reference>
      <dc:creator>Sergio Mazzola, Gabriele Ara, Thomas Benz, Bj\"orn Forsberg, Tommaso Cucinotta, Luca Benini</dc:creator>
    </item>
    <item>
      <title>Not quite a piece of CHERI-cake: Are new digital security by design architectures usable?</title>
      <link>https://arxiv.org/abs/2506.23682</link>
      <description>arXiv:2506.23682v1 Announce Type: cross 
Abstract: A digital security-by-design computer architecture, like CHERI, lets you program without fear of buffer overflows or other memory safety errors, but CHERI also rewrites some of the assumptions about how C works and how fundamental types (such as pointers) are implemented in hardware. We conducted a usability study to examine how developers react to the changes required by CHERI when porting software to run on it. We find that developers struggle with CHERI's display of warnings and errors and a lack of diverse documentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23682v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maysara Alhindi, Joseph Hallett</dc:creator>
    </item>
    <item>
      <title>Calibrating DRAMPower Model for HPC: A Runtime Perspective from Real-Time Measurements</title>
      <link>https://arxiv.org/abs/2411.17960</link>
      <description>arXiv:2411.17960v2 Announce Type: replace 
Abstract: Main memory's rising energy consumption has emerged as a critical challenge in modern computing architectures, particularly in large-scale systems, driven by frequent access patterns, growing data volumes, and insufficient power management strategies. Accurate modeling of DRAM power consumption is essential to address this challenge and optimize energy efficiency. However, existing modeling tools often rely on vendor-provided datasheet values that are obtained under worst-case or idealized conditions. As a result, they fail to capture important system-level factors, such as temperature variations, chip aging, and workload-induced variability, which leads to significant discrepancies between estimated and actual power consumption observed in real deployments. In this work, we propose a runtime calibration methodology for the DRAMPower model using energy measurements collected from real-system experiments. By applying custom memory benchmarks on an HPC cluster and leveraging fine-grained power monitoring infrastructure, we refine key current parameters (IDD values) in the model. Our calibration reduces the average energy estimation error to less than 5%, substantially improving modeling accuracy and making DRAMPower a more reliable tool for power-aware system design and optimization on the target server platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17960v2</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyu Shi, Dina Ali Abdelhamid, Thomas Ilsche, Saeideh Alinezhad Chamazcoti, Timon Evenblij, Mohit Gupta, Francky Catthoor</dc:creator>
    </item>
    <item>
      <title>StreamGrid: Streaming Point Cloud Analytics via Compulsory Splitting and Deterministic Termination</title>
      <link>https://arxiv.org/abs/2503.05197</link>
      <description>arXiv:2503.05197v2 Announce Type: replace 
Abstract: Point clouds are increasingly important in intelligent applications, but frequent off-chip memory traffic in accelerators causes pipeline stalls and leads to high energy consumption. While conventional line buffer techniques can eliminate off-chip traffic, they cannot be directly applied to point clouds due to their inherent computation patterns. To address this, we introduce two techniques: compulsory splitting and deterministic termination, enabling fully-streaming processing. We further propose StreamGrid, a framework that integrates these techniques and automatically optimizes on-chip buffer sizes. Our evaluation shows StreamGrid reduces on-chip memory by 61.3\% and energy consumption by 40.5\% with marginal accuracy loss compared to the baselines without our techniques. Additionally, we achieve 10.0$\times$ speedup and 3.9$\times$ energy efficiency over state-of-the-art accelerators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05197v2</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Feng, Zheng Liu, Weikai Lin, Zihan Liu, Jingwen Leng, Minyi Guo, Zhezhi He, Jieru Zhao, Yuhao Zhu</dc:creator>
    </item>
    <item>
      <title>VeriLoC: Line-of-Code Level Prediction of Hardware Design Quality from Verilog Code</title>
      <link>https://arxiv.org/abs/2506.07239</link>
      <description>arXiv:2506.07239v2 Announce Type: replace 
Abstract: Modern chip design is complex, and there is a crucial need for early-stage prediction of key design-quality metrics like timing and routing congestion directly from Verilog code (a commonly used programming language for hardware design). It is especially important yet complex to predict individual lines of code that cause timing violations or downstream routing congestion. Prior works have tried approaches like converting Verilog into an intermediate graph representation and using LLM embeddings alongside other features to predict module-level quality, but did not consider line-level quality prediction. We propose VeriLoC, the first method that predicts design quality directly from Verilog at both the line- and module-level. To this end, VeriLoC leverages recent Verilog code-generation LLMs to extract local line-level and module-level embeddings, and train downstream classifiers/regressors on concatenations of these embeddings. VeriLoC achieves high F1-scores of 0.86-0.95 for line-level congestion and timing prediction, and reduces the mean average percentage error from 14% - 18% for SOTA methods down to only 4%. We believe that VeriLoC embeddings and insights from our work will also be of value for other predictive and optimization tasks for complex hardware design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07239v2</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raghu Vamshi Hemadri, Jitendra Bhandari, Andre Nakkab, Johann Knechtel, Badri P Gopalan, Ramesh Narayanaswamy, Ramesh Karri, Siddharth Garg</dc:creator>
    </item>
    <item>
      <title>When Servers Meet Species: A Fab-to-Grave Lens on Computing's Biodiversity Impact</title>
      <link>https://arxiv.org/abs/2506.20442</link>
      <description>arXiv:2506.20442v3 Announce Type: replace-cross 
Abstract: Biodiversity loss is a critical planetary boundary, yet its connection to computing remains largely unexamined. Prior sustainability efforts in computing have focused on carbon and water, overlooking biodiversity due to the lack of appropriate metrics and modeling frameworks. This paper presents the first end-to-end analysis of biodiversity impact from computing systems. We introduce two new metrics--Embodied Biodiversity Index (EBI) and Operational Biodiversity Index (OBI)--to quantify biodiversity impact across the lifecycle, and present FABRIC, a modeling framework that links computing workloads to biodiversity impacts. Our evaluation highlights the need to consider biodiversity alongside carbon and water in sustainable computing design and optimization. The code is available at https://github.com/TianyaoShi/FABRIC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20442v3</guid>
      <category>cs.CY</category>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>ACM SIGEnergy Energy Informatics Review (EIR), Volume 5 Issue 2, July 2025</arxiv:journal_reference>
      <dc:creator>Tianyao Shi, Ritbik Kumar, Inez Hua, Yi Ding</dc:creator>
    </item>
  </channel>
</rss>
