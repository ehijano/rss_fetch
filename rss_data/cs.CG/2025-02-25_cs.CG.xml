<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CG updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CG</link>
    <description>cs.CG updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CG" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Feb 2025 02:53:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Purely Geometric Variant of the Gale--Berlekamp Switching Game</title>
      <link>https://arxiv.org/abs/2502.16305</link>
      <description>arXiv:2502.16305v2 Announce Type: new 
Abstract: We introduce the following variant of the Gale--Berlekamp switching game. Let $P$ be a set of n noncollinear points in the plane, each of them having weight $+1$ or $-1$. At each step, we pick a line $\ell$ passing through at least two points of $P$, and switch the sign of every point $p \in P\cap\ell$ to its opposite. The objective is to maximize the total weight of the elements of $P$. We show that one can always achieve that this quantity is at least $n/3$. Moreover, this can be attained by a polynomial time algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16305v2</guid>
      <category>cs.CG</category>
      <category>math.CO</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrian Dumitrescu, J\'anos Pach</dc:creator>
    </item>
    <item>
      <title>Towards a GPU-Native Adaptive Mesh Refinement Scheme for the Lattice Boltzmann Method in Complex Geometries</title>
      <link>https://arxiv.org/abs/2502.16310</link>
      <description>arXiv:2502.16310v1 Announce Type: new 
Abstract: We present a GPU-native mesh adaptation procedure that incorporates a complex geometry represented with a triangle mesh within a primary Cartesian computational grid organized as a forest of octrees. A C++/CUDA program implements the procedure for execution on a single GPU as part of a new module with the AGAL framework, which was originally developed for GPU-native adaptive mesh refinement (AMR) and fluid flow simulation with the Lattice Boltzmann Method (LBM). Traditional LBM is limited to grids with regular prismatic cells with domain boundaries aligned with the cell faces. This work is a first step towards an implementation of the LBM that can simulate flow over irregular surfaces while retaining both adaptation of the mesh and the temporal integration routines entirely on the GPU. Geometries can be inputted as a text file (which generates primitive objects such as circles and spheres) or as an STL file (which can be generated by most 3D modeling software). The procedure is divided into three steps: 1) an import step where the geometry is loaded into either an index list arrangement or directly as a face-vertex coordinates list, 2) a spatial binning step where the faces are distributed to a set of bins with user-defined density, and 3) a near-wall refinement step where the cells of the computational grid detect adjacency to the faces stored in the appropriate bin to form the links between the geometry and the boundary nodes. We validate the implementation and assess its performance in terms of total execution time and speedup relative to a serial CPU implementation using a 2D circle and a 3D Stanford bunny.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16310v1</guid>
      <category>cs.CG</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Khodr Jaber, Ebenezer E. Essel, Pierre E. Sullivan</dc:creator>
    </item>
    <item>
      <title>Segment Intersection Representations, Level Planarity and Constrained Ordering Problems</title>
      <link>https://arxiv.org/abs/2502.16621</link>
      <description>arXiv:2502.16621v1 Announce Type: new 
Abstract: In the Segment Intersection Graph Representation Problem, we want to represent the vertices of a graph as straight line segments in the plane such that two segments cross if and only if there is an edge between the corresponding vertices. This problem is NP-hard (even $\exists\mathbb{R}$-complete [Schaefer, 2010]) in the general case [Kratochv\'il &amp; Ne\^setril, 1992] and remains so if we restrict the segments to be axis-aligned, i.e., horizontal and vertical [Kratochv\'il, 1994]. A long standing open question for the latter variant is its complexity when the order of segments along one axis (say the vertical order of horizontal segments) is already given [Kratochv\'il &amp; Ne\^setril, 1992; Kratochv\'il, 1994].
  We resolve this question by giving efficient solutions using two very different approaches that are interesting on their own. First, using a graph-drawing perspective, we relate the problem to a variant of the well-known Level Planarity problem, where vertices have to lie on pre-assigned horizontal levels. In our case, each level also carries consecutivity constraints on its vertices; this Level Planarity variant is known to have a quadratic solution.
  Second, we use an entirely combinatorial approach, and show that both problems can equivalently be formulated as a linear ordering problem subject to certain consecutivity constraints. While the complexity of such problems varies greatly, we show that in this case the constraints are well-structured in a way that allows a direct quadratic solution. Thus, we obtain three different-but-equivalent perspectives on this problem: the initial geometric one, one from planar graph drawing and a purely combinatorial one.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16621v1</guid>
      <category>cs.CG</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon D. Fink, Matthias Pfretzschner, Peter Stumpf</dc:creator>
    </item>
    <item>
      <title>Property Testing of Curve Similarity</title>
      <link>https://arxiv.org/abs/2502.17277</link>
      <description>arXiv:2502.17277v1 Announce Type: new 
Abstract: We propose sublinear algorithms for probabilistic testing of the discrete and continuous Fr\'echet distance - a standard similarity measure for curves. We assume the algorithm is given access to the input curves via a query oracle: a query returns the set of vertices of the curve that lie within a radius $\delta$ of a specified vertex of the other curve. The goal is to use a small number of queries to determine with constant probability whether the two curves are similar (i.e., their discrete Fr\'echet distance is at most $\delta$) or they are ''$\varepsilon$-far'' (for $0 &lt; \varepsilon &lt; 2$) from being similar, i.e., more than an $\varepsilon$-fraction of the two curves must be ignored for them to become similar. We present two algorithms which are sublinear assuming that the curves are $t$-approximate shortest paths in the ambient metric space, for some $t\ll n$. The first algorithm uses $O(\frac{t}{\varepsilon}\log\frac{t}{\varepsilon})$ queries and is given the value of $t$ in advance. The second algorithm does not have explicit knowledge of the value of $t$ and therefore needs to gain implicit knowledge of the straightness of the input curves through its queries. We show that the discrete Fr\'echet distance can still be tested using roughly $O(\frac{t^3+t^2\log n}{\varepsilon})$ queries ignoring logarithmic factors in $t$. Our algorithms work in a matrix representation of the input and may be of independent interest to matrix testing. Our algorithms use a mild uniform sampling condition that constrains the edge lengths of the curves, similar to a polynomially bounded aspect ratio. Applied to testing the continuous Fr\'echet distance of $t$-straight curves, our algorithms can be used for $(1+\varepsilon')$-approximate testing using essentially the same bounds as stated above with an additional factor of poly$(\frac{1}{\varepsilon'})$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17277v1</guid>
      <category>cs.CG</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peyman Afshani, Maike Buchin, Anne Driemel, Marena Richter, Sampson Wong</dc:creator>
    </item>
    <item>
      <title>Computing the Polytope Diameter is Even Harder than NP-hard (Already for Perfect Matchings)</title>
      <link>https://arxiv.org/abs/2502.16398</link>
      <description>arXiv:2502.16398v1 Announce Type: cross 
Abstract: The diameter of a polytope is a fundamental geometric parameter that plays a crucial role in understanding the efficiency of the simplex method. Despite its central nature, the computational complexity of computing the diameter of a given polytope is poorly understood. Already in 1994, Frieze and Teng [Comp. Compl.] recognized the possibility that this task could potentially be harder than NP-hard, and asked whether the corresponding decision problem is complete for the second stage of the polynomial hierarchy, i.e. $\Pi^p_2$-complete. In the following years, partial results could be obtained. In a cornerstone result, Frieze and Teng themselves proved weak NP-hardness for a family of custom defined polytopes. Sanit\`a [FOCS18] in a break-through result proved that already for the much simpler fractional matching polytope the problem is strongly NP-hard. Very recently, Steiner and N\"obel [SODA25] generalized this result to the even simpler bipartite perfect matching polytope and the circuit diameter. In this paper, we finally show that computing the diameter of the bipartite perfect matching polytope is $\Pi^p_2$-hard. Since the corresponding decision problem is also trivially contained in $\Pi^p_2$, this decidedly answers Frieze and Teng's 30 year old question. Our results also hold when the diameter is replaced by the circuit diameter. As our second main result, we prove that for some $\varepsilon &gt; 0$ the (circuit) diameter of the bipartite perfect matching polytope cannot be approximated by a factor better than $(1 + \varepsilon)$. This answers a recent question by N\"obel and Steiner. It is the first known inapproximability result for the circuit diameter, and extends Sanit\`a's inapproximability result of the diameter to the totally unimodular case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16398v1</guid>
      <category>cs.CC</category>
      <category>cs.CG</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lasse Wulf</dc:creator>
    </item>
    <item>
      <title>Planar Network Diversion</title>
      <link>https://arxiv.org/abs/2502.16714</link>
      <description>arXiv:2502.16714v1 Announce Type: cross 
Abstract: Network Diversion is a graph problem that has been extensively studied in both the network-analysis and operations-research communities as a measure of how robust a network is against adversarial disruption. This problem is especially well motivated in transportation networks, which are often assumed to be planar. Motivated by this and recent theoretical advances for Network Diversion on planar input graphs, we develop a fast O(n log n) time algorithm and present a practical implementation of this algorithm that is able to solve instances with millions of vertices in a matter of seconds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16714v1</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Bentert, P{\aa}l Gr{\o}n{\aa}s Drange, Fedor V. Fomin, Steinar Simonnes</dc:creator>
    </item>
    <item>
      <title>Translating TPMS models to STEP files</title>
      <link>https://arxiv.org/abs/2405.07946</link>
      <description>arXiv:2405.07946v3 Announce Type: replace 
Abstract: Triply periodic minimal surface (TPMS) is emerging as an important way of designing microstructures. However, there has been limited use of commercial CAD/CAM/CAE software packages for TPMS design and manufacturing. This is mainly because TPMS is consistently described in the functional representation (F-rep) format, while modern CAD/CAM/CAE tools are built upon the boundary representation (B-rep) format. One possible solution to this gap is translating TPMS to STEP, which is the standard data exchange format of CAD/CAM/CAE. Following this direction, this paper proposes a new translation method with error-controlling and $C^2$ continuity-preserving features. It is based on an approximation error-driven TPMS sampling algorithm and a constrained-PIA algorithm. The sampling algorithm controls the deviation between the original and translated models. With it, an error bound of $2\epsilon$ on the deviation can be ensured if two conditions called $\epsilon$-density and $\epsilon$-approximation are satisfied. The constrained-PIA algorithm enforces $C^2$ continuity constraints during TPMS approximation, and meanwhile attaining high efficiency. A theoretical convergence proof of this algorithm is also given. The effectiveness of the translation method has been demonstrated by a series of examples and comparisons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07946v3</guid>
      <category>cs.CG</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yaonaiming Zhao, Qiang Zou</dc:creator>
    </item>
    <item>
      <title>Warp-centric GPU meta-meshing and fast triangulation of billion-scale lattice structures</title>
      <link>https://arxiv.org/abs/2405.15197</link>
      <description>arXiv:2405.15197v3 Announce Type: replace 
Abstract: Lattice structures have been widely used in applications due to their superior mechanical properties. To fabricate such structures, a geometric processing step called triangulation is often employed to transform them into the STL format before sending them to 3D printers. Because lattice structures tend to have high geometric complexity, this step usually generates a large amount of triangles, a memory and compute-intensive task. This problem manifests itself clearly through large-scale lattice structures that have millions or billions of struts. To address this problem, this paper proposes to transform a lattice structure into an intermediate model called meta-mesh before undergoing real triangulation. Compared to triangular meshes, meta-meshes are very lightweight and much less compute-demanding. The meta-mesh can also work as a base mesh reusable for conveniently and efficiently triangulating lattice structures with arbitrary resolutions. A CPU+GPU asynchronous meta-meshing pipeline has been developed to efficiently generate meta-meshes from lattice structures. It shifts from the thread-centric GPU algorithm design paradigm commonly used in CAD to the recent warp-centric design paradigm to achieve high performance. This is achieved by a new data compression method, a GPU cache-aware data structure, and a workload-balanced scheduling method that can significantly reduce memory divergence and branch divergence. Experimenting with various billion-scale lattice structures, the proposed method is seen to be two orders of magnitude faster than previously achievable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15197v3</guid>
      <category>cs.CG</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Qiang Zou, Yunzhu Gao</dc:creator>
    </item>
    <item>
      <title>Connected Matchings</title>
      <link>https://arxiv.org/abs/2407.06131</link>
      <description>arXiv:2407.06131v2 Announce Type: replace 
Abstract: We show that each set of $n\ge 2$ points in the plane in general position has a straight-line matching with at least $(5n+1)/27$ edges whose segments form a connected set, and such a matching can be computed in $O(n \log n)$ time. As an upper bound, we show that for some planar point sets in general position the largest matching whose segments form a connected set has $\lceil \frac{n-1}{3}\rceil$ edges. We also consider a colored version, where each edge of the matching should connect points with different colors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06131v2</guid>
      <category>cs.CG</category>
      <category>math.CO</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oswin Aichholzer, Sergio Cabello, Viola M\'esz\'aros, Patrick Schnider, Jan Soukup</dc:creator>
    </item>
    <item>
      <title>On the MST-ratio: Theoretical Bounds and Complexity of Finding the Maximum</title>
      <link>https://arxiv.org/abs/2409.11079</link>
      <description>arXiv:2409.11079v3 Announce Type: replace 
Abstract: Given a finite set of red and blue points in $\Rspace^d$, the MST-ratio is defined as the total length of the Euclidean minimum spanning trees of the red points and the blue points, divided by the length of the Euclidean minimum spanning tree of their union. The MST-ratio has recently gained attention due to its direct interpretation in topological models for studying point sets with applications in spatial biology. The maximum MST-ratio of a point set is the maximum MST-ratio over all proper colorings of its points by red and blue. We prove that finding the maximum MST-ratio of a given point set is NP-hard when the dimension is part of the input. Moreover, we present a quadratic-time $3$-approximation algorithm for this problem. As part of the proof, we show that, in any metric space, the maximum MST-ratio is smaller than $3$. Additionally, we study the average MST-ratio over all colorings of a set of $n$ points. We show that this average is always at least $\frac{n-2}{n-1}$, and for $n$ random points uniformly distributed in a $d$-dimensional unit cube, the average tends to $\sqrt[d]{2}$ in expectation as $n$ approaches infinity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11079v3</guid>
      <category>cs.CG</category>
      <category>cs.CC</category>
      <category>math.CO</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Afrouz Jabal Ameli, Faezeh Motiei, Morteza Saghafian</dc:creator>
    </item>
    <item>
      <title>Non-Euclidean Erd\H{o}s-Anning Theorems</title>
      <link>https://arxiv.org/abs/2401.06328</link>
      <description>arXiv:2401.06328v3 Announce Type: replace-cross 
Abstract: The Erd\H{o}s-Anning theorem states that every point set in the Euclidean plane with integer distances must be either collinear or finite. More strongly, for any (non-degenerate) triangle of diameter~$\delta$, at most $O(\delta^2)$ points can have integer distances from all three triangle vertices. We prove the same results for any strictly convex distance function on the plane, and analogous results for every two-dimensional complete Riemannian manifold of bounded genus and for geodesic distance on the boundary of every three-dimensional Euclidean convex set. As a consequence, we resolve a 1983 question of Richard Guy on the equilateral dimension of Riemannian manifolds. Our proofs are based on the properties of additively weighted Voronoi diagrams of these distances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06328v3</guid>
      <category>math.MG</category>
      <category>cs.CG</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Eppstein</dc:creator>
    </item>
    <item>
      <title>Hyperplanes Avoiding Problem and Integer Points Counting in Polyhedra</title>
      <link>https://arxiv.org/abs/2411.07030</link>
      <description>arXiv:2411.07030v2 Announce Type: replace-cross 
Abstract: In our work, we consider the problem of computing a vector $x \in Z^n$ of minimum $\|\cdot\|_p$-norm such that $a^\top x \not= a_0$, for any vector $(a,a_0)$ from a given subset of $Z^n$ of size $m$. In other words, we search for a vector of minimum norm that avoids a given finite set of hyperplanes, which is natural to call as the $\textit{Hyperplanes Avoiding Problem}$. This problem naturally appears as a subproblem in Barvinok-type algorithms for counting integer points in polyhedra. We show that:
  1) With respect to $\|\cdot\|_1$, the problem admits a feasible solution $x$ with $\|x\|_1 \leq (m+n)/2$, and show that such solution can be constructed by a deterministic polynomial-time algorithm with $O(n \cdot m)$ operations. Moreover, this inequality is the best possible. This is a significant improvement over the previous randomized algorithm, which computes $x$ with a guaranty $\|x\|_{1} \leq n \cdot m$. The original approach of A.~Barvinok can guarantee only $\|x\|_1 = O\bigl((n \cdot m)^n\bigr)$. To prove this result, we use a newly established algorithmic variant of the Combinatorial Nullstellensatz;
  2) The problem is NP-hard with respect to any norm $\|\cdot\|_p$, for $p \in \bigl(R_{\geq 1} \cup \{\infty\}\bigr)$.
  3) As an application, we show that the problem to count integer points in a polytope $P = \{x \in R^n \colon A x \leq b\}$, for given $A \in Z^{m \times n}$ and $b \in Q^m$, can be solved by an algorithm with $O\bigl(\nu^2 \cdot n^3 \cdot \Delta^3 \bigr)$ operations, where $\nu$ is the maximum size of a normal fan triangulation of $P$, and $\Delta$ is the maximum value of rank-order subdeterminants of $A$. As a further application, it provides a refined complexity bound for the counting problem in polyhedra of bounded codimension. For example, in the polyhedra of the Unbounded Subset-Sum problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07030v2</guid>
      <category>cs.CC</category>
      <category>cs.CG</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Grigorii Dakhno, Dmitry Gribanov, Nikita Kasianov, Anastasiia Kats, Andrey Kupavskii, Nikita Kuz'min</dc:creator>
    </item>
    <item>
      <title>Multi-Agent Disk Inspection</title>
      <link>https://arxiv.org/abs/2411.15391</link>
      <description>arXiv:2411.15391v2 Announce Type: replace-cross 
Abstract: We consider $n$ unit-speed mobile agents initially positioned at the center of a unit disk, tasked with inspecting all points on the disk's perimeter. A perimeter point is considered covered if an agent positioned outside the disk's interior has unobstructed visibility of it, treating the disk itself as an obstacle. For $n=1$, this problem is referred to as the shoreline problem with a known distance. Isbell in 1957 derived an optimal trajectory that minimizes the worst-case inspection time for that problem. The one-agent version of the problem was originally proposed as a more tractable variant of Bellman's famous lost-in-the-forest problem.
  Our contributions are threefold. First, and as a warm-up, we extend Isbell's findings by deriving worst-case optimal trajectories addressing the partial inspection of a section of the disk, hence deriving an alternative proof of optimality for inspecting the disk with $n \geq 2$ agents. Second, we analyze the average-case inspection time, assuming a uniform distribution of perimeter points (equivalent to randomized inspection algorithms). Using spatial discretization and Nonlinear Programming (NLP), we propose feasible solutions to the continuous problem and evaluate their effectiveness compared to NLP solutions. Third, we establish Pareto-optimal bounds for the multi-objective problem of jointly minimizing the worst-case and average-case inspection times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15391v2</guid>
      <category>cs.DM</category>
      <category>cs.CG</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James Conley, Konstantinos Georgiou</dc:creator>
    </item>
  </channel>
</rss>
