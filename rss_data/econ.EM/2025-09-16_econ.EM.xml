<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Sep 2025 04:10:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Decision Theoretic Perspective on Artificial Superintelligence: Coping with Missing Data Problems in Prediction and Treatment Choice</title>
      <link>https://arxiv.org/abs/2509.12388</link>
      <description>arXiv:2509.12388v1 Announce Type: new 
Abstract: Enormous attention and resources are being devoted to the quest for artificial general intelligence and, even more ambitiously, artificial superintelligence. We wonder about the implications for our methodological research, which aims to help decision makers cope with what econometricians call identification problems, inferential problems in empirical research that do not diminish as sample size grows. Of particular concern are missing data problems in prediction and treatment choice. Essentially all data collection intended to inform decision making is subject to missing data, which gives rise to identification problems. Thus far, we see no indication that the current dominant architecture of machine learning (ML)-based artificial intelligence (AI) systems will outperform humans in this context. In this paper, we explain why we have reached this conclusion and why we see the missing data problem as a cautionary case study in the quest for superintelligence more generally. We first discuss the concept of intelligence, before presenting a decision-theoretic perspective that formalizes the connection between intelligence and identification problems. We next apply this perspective to two leading cases of missing data problems. Then we explain why we are skeptical that AI research is currently on a path toward machines doing better than humans at solving these identification problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12388v1</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeff Dominitz, Charles F. Manski</dc:creator>
    </item>
    <item>
      <title>Policy-relevant causal effect estimation using instrumental variables with interference</title>
      <link>https://arxiv.org/abs/2509.12538</link>
      <description>arXiv:2509.12538v1 Announce Type: new 
Abstract: Many policy evaluations using instrumental variable (IV) methods include individuals who interact with each other, potentially violating the standard IV assumptions. This paper defines and partially identifies direct and spillover effects with a clear policy-relevant interpretation under relatively mild assumptions on interference. Our framework accommodates both spillovers from the instrument to treatment and from treatment to outcomes and allows for multiple peers. By generalizing monotone treatment response and selection assumptions, we derive informative bounds on policy-relevant effects without restricting the type or direction of interference. The results extend IV estimation to more realistic social contexts, informing program evaluation and treatment scaling when interference is present.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12538v1</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Didier Nibbering, Matthijs Oosterveen</dc:creator>
    </item>
    <item>
      <title>Dynamic Local Average Treatment Effects in Time Series</title>
      <link>https://arxiv.org/abs/2509.12985</link>
      <description>arXiv:2509.12985v1 Announce Type: new 
Abstract: This paper discusses identification, estimation, and inference on dynamic local average treatment effects (LATEs) in instrumental variables (IVs) settings. First, we show that compliers--observations whose treatment status is affected by the instrument--can be identified individually in time series data using smoothness assumptions and local comparisons of treatment assignments. Second, we show that this result enables not only better interpretability of IV estimates but also direct testing of the exclusion restriction by comparing outcomes among identified non-compliers across instrument values. Third, we document pervasive weak identification in applied work using IVs with time series data by surveying recent publications in leading economics journals. However, we find that strong identification often holds in large subsamples for which the instrument induces changes in the treatment. Motivated by this, we introduce a method based on dynamic programming to detect the most strongly-identified subsample and show how to use this subsample to improve estimation and inference. We also develop new identification-robust inference procedures that focus on the most strongly-identified subsample, offering efficiency gains relative to existing full sample identification-robust inference when identification fails over parts of the sample. Finally, we apply our results to heteroskedasticity-based identification of monetary policy effects. We find that about 75% of observations are compliers (i.e., cases where the variance of the policy shifts up on FOMC announcement days), and we fail to reject the exclusion restriction. Estimation using the most strongly-identified subsample helps reconcile conflicting IV and GMM estimates in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12985v1</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Casini, Adam McCloskey, Luca Rolla, Raimondo Pala</dc:creator>
    </item>
    <item>
      <title>Semiparametric Triple Difference Estimators</title>
      <link>https://arxiv.org/abs/2502.19788</link>
      <description>arXiv:2502.19788v3 Announce Type: replace 
Abstract: The triple difference causal inference framework is an extension of the well-known difference-in-differences framework. It relaxes the parallel trends assumption of the difference-in-differences framework through leveraging data from an auxiliary domain. Despite being commonly applied in empirical research, the triple difference framework has received relatively limited attention in the statistics literature. Specifically, investigating the intricacies of identification and the design of robust and efficient estimators for this framework has remained largely unexplored. This work aims to address these gaps in the literature. From the identification standpoint, we present outcome regression and weighting methods to identify the average treatment effect on the treated in both panel data and repeated cross-section settings. For the latter, we relax the commonly made assumption of time-invariant composition of units. From the estimation perspective, we develop semiparametric estimators for the triple difference framework in both panel data and repeated cross-sections settings. These estimators are based on the cross-fitting technique, and flexible machine learning tools can be used to estimate the nuisance components. We characterize conditions under which our proposed estimators are efficient, doubly robust, root-n consistent and asymptotically normal. As an application of our proposed methodology, we examined the effect of mandated maternity benefits on the hourly wages of women of childbearing age and found that these mandates result in a 2.6% drop in hourly wages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19788v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sina Akbari, Negar Kiyavash, AmirEmad Ghassami</dc:creator>
    </item>
    <item>
      <title>Assignment at the Frontier: Identifying the Frontier Structural Function and Bounding Mean Deviations</title>
      <link>https://arxiv.org/abs/2504.19832</link>
      <description>arXiv:2504.19832v5 Announce Type: replace 
Abstract: This paper analyzes a model in which an outcome equals a frontier function of inputs minus a nonnegative unobserved deviation. We allow the distribution of the deviation to depend on inputs. If zero lies in the support of the deviation given inputs -- an assumption we term assignment at the frontier -- then the frontier is identified by the supremum of the outcome at those inputs, obviating the need for instrumental variables. We then estimate the frontier, allowing for random error whose distribution may also depend on inputs. Finally, we derive a lower bound on the mean deviation, using only variance and skewness, that is robust to a scarcity of data near the frontier. We apply our methods to estimate a firm-level frontier production function and mean inefficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19832v5</guid>
      <category>econ.EM</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dan Ben-Moshe, David Genesove</dc:creator>
    </item>
    <item>
      <title>Adaptive Neyman Allocation</title>
      <link>https://arxiv.org/abs/2309.08808</link>
      <description>arXiv:2309.08808v3 Announce Type: replace-cross 
Abstract: In the experimental design literature, Neyman allocation refers to the practice of allocating units into treated and control groups, potentially in unequal numbers proportional to their respective standard deviations, with the objective of minimizing the variance of the treatment effect estimator. This widely recognized approach increases statistical power in scenarios where the treated and control groups have different standard deviations, as is often the case in social experiments, clinical trials, marketing research, and online A/B testing. However, Neyman allocation cannot be implemented unless the standard deviations are known in advance. Fortunately, the multi-stage nature of the aforementioned applications allows the use of earlier stage observations to estimate the standard deviations, which further guide allocation decisions in later stages. In this paper, we introduce a competitive analysis framework to study this multi-stage experimental design problem. We propose a simple adaptive Neyman allocation algorithm, which almost matches the information-theoretic limit of conducting experiments. We provide theory for estimation and inference using data collected from our adaptive Neyman allocation algorithm. Using online A/B testing data from a social media site, we demonstrate the effectiveness of our adaptive Neyman allocation algorithm, highlighting its practicality especially when applied with only a limited number of stages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.08808v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinglong Zhao</dc:creator>
    </item>
    <item>
      <title>Returns and Order Flow Imbalances: Intraday Dynamics and Macroeconomic News Effects</title>
      <link>https://arxiv.org/abs/2508.06788</link>
      <description>arXiv:2508.06788v3 Announce Type: replace-cross 
Abstract: We study the interaction between returns and order flow imbalances in the S&amp;P 500 E-mini futures market using a structural VAR model identified through heteroskedasticity. The model is estimated at one-second frequency for each 15-minute interval, capturing both intraday variation and endogeneity due to time aggregation. We find that macroeconomic news announcements sharply reshape price-flow dynamics: price impact rises, flow impact declines, return volatility spikes, and flow volatility falls. Pooling across days, both price and flow impacts are significant at the one-second horizon, with estimates broadly consistent with stylized limit-order-book predictions. Impulse responses indicate that shocks dissipate almost entirely within a second. Structural parameters and volatilities also exhibit pronounced intraday variation tied to liquidity, trading intensity, and spreads. These results provide new evidence on high-frequency price formation and liquidity, highlighting the role of public information and order submission in shaping market quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06788v3</guid>
      <category>q-fin.TR</category>
      <category>econ.EM</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Makoto Takahashi</dc:creator>
    </item>
  </channel>
</rss>
