<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Sep 2024 01:44:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Impact of Virtual Achievements on Online Learning Applications</title>
      <link>https://arxiv.org/abs/2409.05877</link>
      <description>arXiv:2409.05877v1 Announce Type: new 
Abstract: In recent times a number of platforms are using badge-based achievements or leaderboards to increase user involvement and participation. Due to recent advancements, there is a question of up to what extent virtual achievement systems have on users using particular platforms. Here in this paper, we discuss measuring the impact of the leaderboard-based achievement system by integrating it into an online learning android application UPSC Pre that has thousands of questions and answers categorized topic wise related to UPSC exams, one of the toughest exams to crack in the world. We are conducting the experiment on 10 randomly chosen students who are using the app in a controlled setting and the data measurement is done using the Firebase Analytics tool by Google. We observed that the students using the leaderboard have increased participation without any reductions in their quality and the time of using the platform has increased compared to previous engagements. Students who participated in the experiment felt the leaderboard was competitive and enjoyed gaining positions on the leaderboard and wanted it in the User interface for other platforms as well. The research has an impact in designing the learning applications to cater and improve the user experience in the future. The results are not limited to educational purposes but can be expanded to other fields such as self development applications, other research projects, Gaming industry and many more.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05877v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Venkata Sai Bhargav Bathini, Lokesh Meesala, Ahamad Shaik, Hrushyang Adloori</dc:creator>
    </item>
    <item>
      <title>Bridging Research and Practice Through Conversation: Reflecting on Our Experience</title>
      <link>https://arxiv.org/abs/2409.05880</link>
      <description>arXiv:2409.05880v1 Announce Type: new 
Abstract: While some research fields have a long history of collaborating with domain experts outside academia, many quantitative researchers do not have natural avenues to meet experts in areas where the research is later deployed. We explain how conversations -- interviews without a specific research objective -- can bridge research and practice. Using collaborative autoethnography, we reflect on our experience of conducting conversations with practitioners from a range of different backgrounds, including refugee rights, conservation, addiction counseling, and municipal data science. Despite these varied backgrounds, common lessons emerged, including the importance of valuing the knowledge of experts, recognizing that academic research and practice have differing objectives and timelines, understanding the limits of quantification, and avoiding data extractivism. We consider the impact of these conversations on our work, the potential roles we can serve as researchers, and the challenges we anticipate as we move forward in these collaborations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05880v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mayra Russo, Mackenzie Jorgensen, Kristen M. Scott, Wendy Xu, Di H. Nguyen, Jessie Finocchiaro, Matthew Olckers</dc:creator>
    </item>
    <item>
      <title>Big-Thick Data generation via reference and personal context unification</title>
      <link>https://arxiv.org/abs/2409.05883</link>
      <description>arXiv:2409.05883v1 Announce Type: new 
Abstract: Smart devices generate vast amounts of big data, mainly in the form of sensor data. While allowing for the prediction of many aspects of human behaviour (e.g., physical activities, transportation modes), this data has a major limitation in that it is not thick, that is, it does not carry information about the context within which it was generated. Context - what was accomplished by a user, how and why, and in which overall situation - all these factors must be explicitly represented for the data to be self-explanatory and meaningful. In this paper, we introduce Big-Thick Data as highly contextualized data encoding, for each and every user, both her subjective personal view of the world and the objective view of an all-observing third party taken as reference. We model big-thick data by enforcing the distinction between personal context and reference context. We show that these two types of context can be unified in many different ways, thus allowing for different types of questions about the users' behaviour and the world around them and, also, for multiple different answers to the same question. We validate the model with a case study that integrates the personal big-thick data of one hundred and fifty-eight University students over a period of four weeks with the reference context built using the data provided by OpenStreetMap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05883v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fausto Giunchiglia, Xiaoyue Li</dc:creator>
    </item>
    <item>
      <title>Challenges and Opportunities of Teaching Data Visualization Together with Data Science</title>
      <link>https://arxiv.org/abs/2409.05969</link>
      <description>arXiv:2409.05969v1 Announce Type: new 
Abstract: With the increasing amount of data globally, analyzing and visualizing data are becoming essential skills across various professions. It is important to equip university students with these essential data skills. To learn, design, and develop data visualization, students need knowledge of programming and data science topics. Many university programs lack dedicated data science courses for undergraduate students, making it important to introduce these concepts through integrated courses. However, combining data science and data visualization into one course can be challenging due to the time constraints and the heavy load of learning. In this paper, we discuss the development of teaching data science and data visualization together in one course and share the results of the post-course evaluation survey. From the survey's results, we identified four challenges, including difficulty in learning multiple tools and diverse data science topics, varying proficiency levels with tools and libraries, and selecting and cleaning datasets. We also distilled five opportunities for developing a successful data science and visualization course. These opportunities include clarifying the course structure, emphasizing visualization literacy early in the course, updating the course content according to student needs, using large real-world datasets, learning from industry professionals, and promoting collaboration among students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05969v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shri Harini Ramesh, Fateme Rajabiyazdi</dc:creator>
    </item>
    <item>
      <title>Investigating the effects of housing instability on depression, anxiety, and mental health treatment in childhood and adolescence</title>
      <link>https://arxiv.org/abs/2409.06011</link>
      <description>arXiv:2409.06011v1 Announce Type: new 
Abstract: Housing instability is a widespread phenomenon in the United States. In combination with other social determinants of health, housing instability affects children's overall health and development. Drawing on data from the 2022 National Survey of Children's Health, we employed multiple logistic regression models to understand how sociodemographic factors, especially housing instability, affect mental health outcomes and treatment access for youth aged 6-17 years. Our results show that youth facing housing instability have a higher likelihood of experiencing anxiety (OR: 1.42, p&lt;0.001) and depression (OR: 1.57, p&lt;0.001). Furthermore, youth experiencing both mental health conditions and housing instability are significantly less likely to receive mental health services in the past year, indicating the substantial barriers they face in accessing mental health care. Based on our findings, we highlight opportunities for digital mental health interventions to provide children experiencing housing instability with more accessible and consistent mental health services.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06011v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rachael Zehrung, Di Hu, Yawen Guo, Kai Zheng, Yunan Chen</dc:creator>
    </item>
    <item>
      <title>MemoVis: A GenAI-Powered Tool for Creating Companion Reference Images for 3D Design Feedback</title>
      <link>https://arxiv.org/abs/2409.06082</link>
      <description>arXiv:2409.06082v1 Announce Type: new 
Abstract: Providing asynchronous feedback is a critical step in the 3D design workflow. A common approach to providing feedback is to pair textual comments with companion reference images, which helps illustrate the gist of text. Ideally, feedback providers should possess 3D and image editing skills to create reference images that can effectively describe what they have in mind. However, they often lack such skills, so they have to resort to sketches or online images which might not match well with the current 3D design. To address this, we introduce MemoVis, a text editor interface that assists feedback providers in creating reference images with generative AI driven by the feedback comments. First, a novel real-time viewpoint suggestion feature, based on a vision-language foundation model, helps feedback providers anchor a comment with a camera viewpoint. Second, given a camera viewpoint, we introduce three types of image modifiers, based on pre-trained 2D generative models, to turn a text comment into an updated version of the 3D scene from that viewpoint. We conducted a within-subjects study with feedback providers, demonstrating the effectiveness of MemoVis. The quality and explicitness of the companion images were evaluated by another eight participants with prior 3D design experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06082v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3694681</arxiv:DOI>
      <arxiv:journal_reference>ACM Transactions on Computer-Human Interaction, 2024</arxiv:journal_reference>
      <dc:creator>Chen Chen, Cuong Nguyen, Thibault Groueix, Vladimir G. Kim, Nadir Weibel</dc:creator>
    </item>
    <item>
      <title>Hevelius Report: Visualizing Web-Based Mobility Test Data For Clinical Decision and Learning Support</title>
      <link>https://arxiv.org/abs/2409.06088</link>
      <description>arXiv:2409.06088v1 Announce Type: new 
Abstract: Hevelius, a web-based computer mouse test, measures arm movement and has been shown to accurately evaluate severity for patients with Parkinson's disease and ataxias. A Hevelius session produces 32 numeric features, which may be hard to interpret, especially in time-constrained clinical settings. This work aims to support clinicians (and other stakeholders) in interpreting and connecting Hevelius features to clinical concepts. Through an iterative design process, we developed a visualization tool (Hevelius Report) that (1) abstracts six clinically relevant concepts from 32 features, (2) visualizes patient test results, and compares them to results from healthy controls and other patients, and (3) is an interactive app to meet the specific needs in different usage scenarios. Then, we conducted a preliminary user study through an online interview with three clinicians who were not involved in the project. They expressed interest in using Hevelius Report, especially for identifying subtle changes in their patients' mobility that are hard to capture with existing clinical tests. Future work will integrate the visualization tool into the current clinical workflow of a neurology team and conduct systematic evaluations of the tool's usefulness, usability, and effectiveness. Hevelius Report represents a promising solution for analyzing fine-motor test results and monitoring patients' conditions and progressions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06088v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3663548.3688490</arxiv:DOI>
      <dc:creator>Hongjin Lin, Tessa Han, Krzysztof Z. Gajos, Anoopum S. Gupta</dc:creator>
    </item>
    <item>
      <title>Human Impedance Modulation to Improve Visuo-Haptic Perception</title>
      <link>https://arxiv.org/abs/2409.06124</link>
      <description>arXiv:2409.06124v1 Announce Type: new 
Abstract: Humans activate muscles to shape the mechanical interaction with their environment, but can they harness this control mechanism to best sense the environment? We investigated how participants adapt their muscle activation to visual and haptic information when tracking a randomly moving target with a robotic interface. The results exhibit a differentiated effect of these sensory modalities, where participants' muscle cocontraction increases with the haptic noise and decreases with the visual noise, in apparent contradiction to previous results. These results can be explained, and reconciled with previous findings, when considering muscle spring like mechanics, where stiffness increases with cocontraction to regulate motion guidance. Increasing cocontraction to more closely follow the motion plan favors accurate visual over haptic information, while decreasing it avoids injecting visual noise and relies on accurate haptic information. We formulated this active sensing mechanism as the optimization of visuo-haptic information and effort. This OIE model can explain the adaptation of muscle activity to unimodal and multimodal sensory information when interacting with fixed or dynamic environments, or with another human, and can be used to optimize human-robot interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06124v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoxiao Cheng, Shixian Shen, Ekaterina Ivanova, Gerolamo Carboni, Atsushi Takagi, Etienne Burdet</dc:creator>
    </item>
    <item>
      <title>SQLucid: Grounding Natural Language Database Queries with Interactive Explanations</title>
      <link>https://arxiv.org/abs/2409.06178</link>
      <description>arXiv:2409.06178v1 Announce Type: new 
Abstract: Though recent advances in machine learning have led to significant improvements in natural language interfaces for databases, the accuracy and reliability of these systems remain limited, especially in high-stakes domains. This paper introduces SQLucid, a novel user interface that bridges the gap between non-expert users and complex database querying processes. SQLucid addresses existing limitations by integrating visual correspondence, intermediate query results, and editable step-by-step SQL explanations in natural language to facilitate user understanding and engagement. This unique blend of features empowers users to understand and refine SQL queries easily and precisely. Two user studies and one quantitative experiment were conducted to validate SQLucid's effectiveness, showing significant improvement in task completion accuracy and user confidence compared to existing interfaces. Our code is available at https://github.com/magic-YuanTian/SQLucid.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06178v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuan Tian, Jonathan K. Kummerfeld, Toby Jia-Jun Li, Tianyi Zhang</dc:creator>
    </item>
    <item>
      <title>SHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing Behaviors with LLMs</title>
      <link>https://arxiv.org/abs/2409.06205</link>
      <description>arXiv:2409.06205v1 Announce Type: new 
Abstract: This paper introduces text-to-shape-display, a novel approach to generating dynamic shape changes in pin-based shape displays through natural language commands. By leveraging large language models (LLMs) and AI-chaining, our approach allows users to author shape-changing behaviors on demand through text prompts without programming. We describe the foundational aspects necessary for such a system, including the identification of key generative elements (primitive, animation, and interaction) and design requirements to enhance user interaction, based on formative exploration and iterative design processes. Based on these insights, we develop SHAPE-IT, an LLM-based authoring tool for a 24 x 24 shape display, which translates the user's textual command into executable code and allows for quick exploration through a web-based control interface. We evaluate the effectiveness of SHAPE-IT in two ways: 1) performance evaluation and 2) user evaluation (N= 10). The study conclusions highlight the ability to facilitate rapid ideation of a wide range of shape-changing behaviors with AI. However, the findings also expose accuracy-related challenges and limitations, prompting further exploration into refining the framework for leveraging AI to better suit the unique requirements of shape-changing systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06205v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3654777.3676348</arxiv:DOI>
      <dc:creator>Wanli Qian, Chenfeng Gao, Anup Sathya, Ryo Suzuki, Ken Nakagaki</dc:creator>
    </item>
    <item>
      <title>VBIT: Towards Enhancing Privacy Control Over IoT Devices</title>
      <link>https://arxiv.org/abs/2409.06233</link>
      <description>arXiv:2409.06233v1 Announce Type: new 
Abstract: Internet-of-Things (IoT) devices are increasingly deployed at home, at work, and in other shared and public spaces. IoT devices collect and share data with service providers and third parties, which poses privacy concerns. Although privacy enhancing tools are quite advanced in other applications domains (\eg~ advertising and tracker blockers for browsers), users have currently no convenient way to know or manage what and how data is collected and shared by IoT devices. In this paper, we present VBIT, an interactive system combining Mixed Reality (MR) and web-based applications that allows users to: (1) uncover and visualize tracking services by IoT devices in an instrumented space and (2) take action to stop or limit that tracking. We design and implement VBIT to operate at the network traffic level, and we show that it has negligible performance overhead, and offers flexibility and good usability. We perform a mixed-method user study consisting of an online survey and an in-person interview study. We show that VBIT users appreciate VBIT's transparency, control, and customization features, and they become significantly more willing to install an IoT advertising and tracking blocker, after using VBIT. In the process, we obtain design insights that can be used to further iterate and improve the design of VBIT and other systems for IoT transparency and control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06233v1</guid>
      <category>cs.HC</category>
      <category>cs.CR</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jad Al Aaraj, Olivia Figueira, Tu Le, Isabela Figueira, Rahmadi Trimananda, Athina Markopoulou</dc:creator>
    </item>
    <item>
      <title>Exploring AI Futures Through Fictional News Articles</title>
      <link>https://arxiv.org/abs/2409.06354</link>
      <description>arXiv:2409.06354v1 Announce Type: new 
Abstract: The aim of this workshop was to enable critical discussion on AI futures using fictional news articles and discussion groups. By collaboratively imagining and presenting future scenarios in a journalistic news article format, participants explored the socio-political, ethical and sustainability factors of AI through an accessible narrative form. Participants engaged in further anticipatory work by analyzing the issues raised by the articles in a group discussion, emphasizing the underlying motivations, assumptions and expectations conveyed within the news articles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06354v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>Part of the Proceedings of WASP-HS Workshop Living with AI. Critical Questions for the Social Sciences and Humanities 2023 WASP-HS Conference</arxiv:journal_reference>
      <dc:creator>Martin Lindstam, Elin Sporrong, Camilo Sanchez, Petra J\"a\"askel\"ainen</dc:creator>
    </item>
    <item>
      <title>Reflections on Visualization in Motion for Fitness Trackers</title>
      <link>https://arxiv.org/abs/2409.06401</link>
      <description>arXiv:2409.06401v1 Announce Type: new 
Abstract: In this paper, we reflect on our past work towards understanding how to design visualizations for fitness trackers that are used in motion. We have coined the term "visualization in motion" for visualizations that are used in the presence of relative motion between a viewer and the visualization. Here, we describe how visualization in motion is relevant to sports scenarios. We also provide new data on current smartwatch visualizations for sports and discuss future challenges for visualizations in motion for fitness tracker.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06401v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>MobileHCI 2022 Workshop on New Trends in HCI and Sports, Sep 2022, Vancouver, Canada</arxiv:journal_reference>
      <dc:creator>Alaul Islam, Lijie Yao, Anastasia Bezerianos, Tanja Blascheck, Tingying He, Bongshin Lee, Romain Vuillemot, Petra Isenberg</dc:creator>
    </item>
    <item>
      <title>Collecting Information Needs for Egocentric Visualizations while Running</title>
      <link>https://arxiv.org/abs/2409.06426</link>
      <description>arXiv:2409.06426v1 Announce Type: new 
Abstract: We investigate research challenges and opportunities for visualization in motion during outdoor physical activities via an initial corpus of real-world recordings that pair egocentric video, biometrics, and think-aloud observations. With the increasing use of tracking and recording devices, such as smartwatches and head-mounted displays, more and more data are available in real-time about a person's activity and the context of the activity. However, not all data will be relevant all the time. Instead, athletes have information needs that change throughout their activity depending on the context and their performance. To address this challenge, we describe the collection of a diverse corpus of information needs paired with contextualizing audio, video, and sensor data. Next, we propose a first set of research challenges and design considerations that explore the difficulties of visualizing those real data needs in-context and demonstrate a prototype tool for browsing, aggregating, and analyzing this information. Our ultimate goal is to understand and support embedding visualizations into outdoor contexts with changing environments and varying data needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06426v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ahmed Elshabasi, Lijie Yao, Petra Isenberg, Charles Perin, Wesley Willett</dc:creator>
    </item>
    <item>
      <title>"The struggle is a part of the experience": Engaging Discontents in the Design of Family Meal Technologies</title>
      <link>https://arxiv.org/abs/2409.06627</link>
      <description>arXiv:2409.06627v1 Announce Type: new 
Abstract: Meals are a central (and messy) part of family life. Previous design framings for mealtime technologies have focused on supporting dietary needs or social and celebratory interactions at the dinner table; however, family meals involve the coordination of many activities and complicated family dynamics. In this paper, we report on findings from interviews and design sessions with 18 families from the Midwestern United States (including both partners/parents and children) to uncover important family differences and tensions that arise around domestic meal experiences. Drawing on feminist theory, we unpack the work of feeding a family as a form of care, drawing attention to the social and emotional complexity of family meals. Critically situating our data within current design narratives, we propose the sensitizing concepts of generative and systemic discontents as a productive way towards troubling the design space of family-food interaction to contend with the struggles that are a part of everyday family meal experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06627v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3687016</arxiv:DOI>
      <arxiv:journal_reference>Proc. ACM Hum.-Comput. Interact 8, CSCW2, Article 477 (November 2024), 33 pages</arxiv:journal_reference>
      <dc:creator>Yuxing Wu, Andrew D Miller, Chia-Fang Chung, Elizabeth Kaziunas</dc:creator>
    </item>
    <item>
      <title>Advanced Gaze Analytics Dashboard</title>
      <link>https://arxiv.org/abs/2409.06628</link>
      <description>arXiv:2409.06628v1 Announce Type: new 
Abstract: Eye movements can provide informative cues to understand human visual scan/search behavior and cognitive load during varying tasks. Visualizations of real-time gaze measures during tasks, provide an understanding of human behavior as the experiment is being conducted. Even though existing eye tracking analysis tools provide calculation and visualization of eye-tracking data, none of them support real-time visualizations of advanced gaze measures, such as ambient or focal processing, or eye-tracked measures of cognitive load. In this paper, we present an eye movements analytics dashboard that enables visualizations of various gaze measures, fixations, saccades, cognitive load, ambient-focal attention, and gaze transitions analysis by extracting eye movements from participants utilizing common off-the-shelf eye trackers. We validate the proposed eye movement visualizations by using two publicly available eye-tracking datasets. We showcase that, the proposed dashboard could be utilized to visualize advanced eye movement measures generated using multiple data sources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06628v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>2024 IEEE 25th International Conference on Information Reuse and Integration for Data Science (IRI)</arxiv:journal_reference>
      <dc:creator>Gavindya Jayawardena, Vikas Ashok, Sampath Jayarathna</dc:creator>
    </item>
    <item>
      <title>Strategic management analysis: from data to strategy diagram by LLM</title>
      <link>https://arxiv.org/abs/2409.06643</link>
      <description>arXiv:2409.06643v1 Announce Type: new 
Abstract: Strategy management analyses are created by business consultants with common analysis frameworks (i.e. comparative analyses) and associated diagrams. We show these can be largely constructed using LLMs, starting with the extraction of insights from data, organization of those insights according to a strategy management framework, and then depiction in the typical strategy management diagram for that framework (static textual visualizations). We discuss caveats and future directions to generalize for broader uses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06643v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Richard Brath, Adam Bradley, David Jonker</dc:creator>
    </item>
    <item>
      <title>Human Perception of LLM-generated Text Content in Social Media Environments</title>
      <link>https://arxiv.org/abs/2409.06653</link>
      <description>arXiv:2409.06653v1 Announce Type: new 
Abstract: Emerging technologies, particularly artificial intelligence (AI), and more specifically Large Language Models (LLMs) have provided malicious actors with powerful tools for manipulating digital discourse. LLMs have the potential to affect traditional forms of democratic engagements, such as voter choice, government surveys, or even online communication with regulators; since bots are capable of producing large quantities of credible text. To investigate the human perception of LLM-generated content, we recruited over 1,000 participants who then tried to differentiate bot from human posts in social media discussion threads. We found that humans perform poorly at identifying the true nature of user posts on social media. We also found patterns in how humans identify LLM-generated text content in social media discourse. Finally, we observed the Uncanny Valley effect in text dialogue in both user perception and identification. This indicates that despite humans being poor at the identification process, they can still sense discomfort when reading LLM-generated content.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06653v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kristina Radivojevic, Matthew Chou, Karla Badillo-Urquiola, Paul Brenner</dc:creator>
    </item>
    <item>
      <title>Designing Resource Allocation Tools to Promote Fair Allocation: Do Visualization and Information Framing Matter?</title>
      <link>https://arxiv.org/abs/2409.06688</link>
      <description>arXiv:2409.06688v1 Announce Type: new 
Abstract: Studies on human decision-making focused on humanitarian aid have found that cognitive biases can hinder the fair allocation of resources. However, few HCI and Information Visualization studies have explored ways to overcome those cognitive biases. This work investigates whether the design of interactive resource allocation tools can help to promote allocation fairness. We specifically study the effect of presentation format (using text or visualization) and a specific framing strategy (showing resources allocated to groups or individuals). In our three crowdsourced experiments, we provided different tool designs to split money between two fictional programs that benefit two distinct communities. Our main finding indicates that individual-framed visualizations and text may be able to curb unfair allocations caused by group-framed designs. This work opens new perspectives that can motivate research on how interactive tools and visualizations can be engineered to combat cognitive biases that lead to inequitable decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06688v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3544548.3580739</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pp. 1-16. 2023</arxiv:journal_reference>
      <dc:creator>Arnav Verma, Luiz Morais, Pierre Dragicevic, Fanny Chevalier</dc:creator>
    </item>
    <item>
      <title>Exploring and Visualizing COVID-19 Trends in India: Vulnerabilities and Mitigation Strategies</title>
      <link>https://arxiv.org/abs/2409.05876</link>
      <description>arXiv:2409.05876v1 Announce Type: cross 
Abstract: Visualizing data plays a pivotal role in portraying important scientific information. Hence, visualization techniques aid in displaying relevant graphical interpretations from the varied structures of data, which is found otherwise. In this paper, we explore the COVID-19 pandemic influence trends in the subcontinent of India in the context of how far the infection rate spiked in the year 2020 and how the public health division of the country India has helped to curb the spread of the novel virus by installing vaccination centers across the diaspora of the country. The paper contributes to the empirical study of understanding the impact caused by the novel virus to the country by doing extensive explanatory data analysis of the data collected from the official government portal. Our work contributes to the understanding that data visualization is prime in understanding public health problems and beyond and taking necessary measures to curb the existing pandemic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05876v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Swayamjit Saha, Kuntal Ghosh, Garga Chatterjee, J. Edward Swan II</dc:creator>
    </item>
    <item>
      <title>Visual Analytics of Performance of Quantum Computing Systems and Circuit Optimization</title>
      <link>https://arxiv.org/abs/2409.06159</link>
      <description>arXiv:2409.06159v1 Announce Type: cross 
Abstract: Driven by potential exponential speedups in business, security, and scientific scenarios, interest in quantum computing is surging. This interest feeds the development of quantum computing hardware, but several challenges arise in optimizing application performance for hardware metrics (e.g., qubit coherence and gate fidelity). In this work, we describe a visual analytics approach for analyzing the performance properties of quantum devices and quantum circuit optimization. Our approach allows users to explore spatial and temporal patterns in quantum device performance data and it computes similarities and variances in key performance metrics. Detailed analysis of the error properties characterizing individual qubits is also supported. We also describe a method for visualizing the optimization of quantum circuits. The resulting visualization tool allows researchers to design more efficient quantum algorithms and applications by increasing the interpretability of quantum computations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06159v1</guid>
      <category>quant-ph</category>
      <category>cs.HC</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>2024 International Workshop on Quantum Computing: Circuits Systems Automation and Applications</arxiv:journal_reference>
      <dc:creator>Junghoon Chae, Chad A. Steed, Travis S. Humble</dc:creator>
    </item>
    <item>
      <title>Can Large Language Models Unlock Novel Scientific Research Ideas?</title>
      <link>https://arxiv.org/abs/2409.06185</link>
      <description>arXiv:2409.06185v1 Announce Type: cross 
Abstract: "An idea is nothing more nor less than a new combination of old elements" (Young, J.W.). The widespread adoption of Large Language Models (LLMs) and publicly available ChatGPT have marked a significant turning point in the integration of Artificial Intelligence (AI) into people's everyday lives. This study explores the capability of LLMs in generating novel research ideas based on information from research papers. We conduct a thorough examination of 4 LLMs in five domains (e.g., Chemistry, Computer, Economics, Medical, and Physics). We found that the future research ideas generated by Claude-2 and GPT-4 are more aligned with the author's perspective than GPT-3.5 and Gemini. We also found that Claude-2 generates more diverse future research ideas than GPT-4, GPT-3.5, and Gemini 1.0. We further performed a human evaluation of the novelty, relevancy, and feasibility of the generated future research ideas. This investigation offers insights into the evolving role of LLMs in idea generation, highlighting both its capability and limitations. Our work contributes to the ongoing efforts in evaluating and utilizing language models for generating future research ideas. We make our datasets and codes publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06185v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sandeep Kumar, Tirthankar Ghosal, Vinayak Goyal, Asif Ekbal</dc:creator>
    </item>
    <item>
      <title>User Preferences for Large Language Model versus Template-Based Explanations of Movie Recommendations: A Pilot Study</title>
      <link>https://arxiv.org/abs/2409.06297</link>
      <description>arXiv:2409.06297v1 Announce Type: cross 
Abstract: Recommender systems have become integral to our digital experiences, from online shopping to streaming platforms. Still, the rationale behind their suggestions often remains opaque to users. While some systems employ a graph-based approach, offering inherent explainability through paths associating recommended items and seed items, non-experts could not easily understand these explanations. A popular alternative is to convert graph-based explanations into textual ones using a template and an algorithm, which we denote here as ''template-based'' explanations. Yet, these can sometimes come across as impersonal or uninspiring. A novel method would be to employ large language models (LLMs) for this purpose, which we denote as ''LLM-based''. To assess the effectiveness of LLMs in generating more resonant explanations, we conducted a pilot study with 25 participants. They were presented with three explanations: (1) traditional template-based, (2) LLM-based rephrasing of the template output, and (3) purely LLM-based explanations derived from the graph-based explanations. Although subject to high variance, preliminary findings suggest that LLM-based explanations may provide a richer and more engaging user experience, further aligning with user expectations. This study sheds light on the potential limitations of current explanation methods and offers promising directions for leveraging large language models to improve user satisfaction and trust in recommender systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06297v1</guid>
      <category>cs.IR</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julien Albert, Martin Balfroid, Miriam Doh, Jeremie Bogaert, Luca La Fisca, Liesbet De Vos, Bryan Renard, Vincent Stragier, Emmanuel Jean</dc:creator>
    </item>
    <item>
      <title>Social Mediation through Robots -- A Scoping Review on Improving Group Interactions through Directed Robot Action using an Extended Group Process Model</title>
      <link>https://arxiv.org/abs/2409.06557</link>
      <description>arXiv:2409.06557v1 Announce Type: cross 
Abstract: Group processes refer to the dynamics that occur within a group and are critical for understanding how groups function. With robots being increasingly placed within small groups, improving these processes has emerged as an important application of social robotics. Social Mediation Robots elicit behavioral change within groups by deliberately influencing the processes of groups. While research in this field has demonstrated that robots can effectively affect interpersonal dynamics, there is a notable gap in integrating these insights to develop coherent understanding and theory. We present a scoping review of literature targeting changes in social interactions between multiple humans through intentional action from robotic agents. To guide our review, we adapt the classical Input-Process-Output (I-P-O) models that we call "Mediation I-P-O model". We evaluated 1633 publications, which yielded 89 distinct social mediation concepts. We construct 11 mediation approaches robots can use to shape processes in small groups and teams. This work strives to produce generalizable insights and evaluate the extent to which the potential of social mediation through robots has been realized thus far. We hope that the proposed framework encourages a holistic approach to the study of social mediation and provides a foundation to standardize future reporting in the domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06557v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas H. Weisswange, Hifza Javed, Manuel Dietrich, Malte F. Jung, Nawid Jamali</dc:creator>
    </item>
    <item>
      <title>Past, Present, and Future of Citation Practices in HCI</title>
      <link>https://arxiv.org/abs/2405.16526</link>
      <description>arXiv:2405.16526v4 Announce Type: replace 
Abstract: Science is a complex system comprised of many scientists who individually make collective decisions that, due to the size and nature of the academic system, largely do not affect the system as a whole. However, certain decisions at the meso-level of research communities, such as the Human-Computer Interaction (HCI) community, may result in deep and long-lasting behavioral changes in scientists. In this article, we provide evidence on how a change in editorial policies introduced at the ACM CHI Conference in 2016 launched the CHI community on an expansive path, denoted by a year-by-year increase in the mean number of references included in CHI articles. If this near-linear trend continues undisrupted, an article in CHI 2030 will include on average almost 130 references. The trend towards more citations reflects a citation culture where quantity is prioritized over quality, contributing to both author and peer reviewer fatigue. This article underscores the profound impact that meso-level policy adjustments have on the evolution of scientific fields and disciplines, urging stakeholders to carefully consider the broader implications of such changes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16526v4</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonas Oppenlaender</dc:creator>
    </item>
    <item>
      <title>Towards Dataset-scale and Feature-oriented Evaluation of Text Summarization in Large Language Model Prompts</title>
      <link>https://arxiv.org/abs/2407.12192</link>
      <description>arXiv:2407.12192v3 Announce Type: replace 
Abstract: Recent advancements in Large Language Models (LLMs) and Prompt Engineering have made chatbot customization more accessible, significantly reducing barriers to tasks that previously required programming skills. However, prompt evaluation, especially at the dataset scale, remains complex due to the need to assess prompts across thousands of test instances within a dataset. Our study, based on a comprehensive literature review and pilot study, summarized five critical challenges in prompt evaluation. In response, we introduce a feature-oriented workflow for systematic prompt evaluation. In the context of text summarization, our workflow advocates evaluation with summary characteristics (feature metrics) such as complexity, formality, or naturalness, instead of using traditional quality metrics like ROUGE. This design choice enables a more user-friendly evaluation of prompts, as it guides users in sorting through the ambiguity inherent in natural language. To support this workflow, we introduce Awesum, a visual analytics system that facilitates identifying optimal prompt refinements for text summarization through interactive visualizations, featuring a novel Prompt Comparator design that employs a BubbleSet-inspired design enhanced by dimensionality reduction techniques. We evaluate the effectiveness and general applicability of the system with practitioners from various domains and found that (1) our design helps overcome the learning curve for non-technical people to conduct a systematic evaluation of summarization prompts, and (2) our feature-oriented workflow has the potential to generalize to other NLG and image-generation tasks. For future works, we advocate moving towards feature-oriented evaluation of LLM prompts and discuss unsolved challenges in terms of human-agent interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12192v3</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TVCG.2024.3456398</arxiv:DOI>
      <dc:creator>Sam Yu-Te Lee, Aryaman Bahukhandi, Dongyu Liu, Kwan-Liu Ma</dc:creator>
    </item>
    <item>
      <title>Evaluating Force-based Haptics for Immersive Tangible Interactions with Surface Visualizations</title>
      <link>https://arxiv.org/abs/2408.04031</link>
      <description>arXiv:2408.04031v3 Announce Type: replace 
Abstract: Haptic feedback provides an essential sensory stimulus crucial for interaction and analyzing three-dimensional spatio-temporal phenomena on surface visualizations. Given its ability to provide enhanced spatial perception and scene maneuverability, virtual reality (VR) catalyzes haptic interactions on surface visualizations. Various interaction modes, encompassing both mid-air and on-surface interactions -- with or without the application of assisting force stimuli -- have been explored using haptic force feedback devices. In this paper, we evaluate the use of on-surface and assisted on-surface haptic modes of interaction compared to a no-haptic interaction mode. A force-based haptic stylus is used for all three modalities; the on-surface mode uses collision based forces, whereas the assisted on-surface mode is accompanied by an additional snapping force. We conducted a within-subjects user study involving fundamental interaction tasks performed on surface visualizations. Keeping a consistent visual design across all three modes, our study incorporates tasks that require the localization of the highest, lowest, and random points on surfaces; and tasks that focus on brushing curves on surfaces with varying complexity and occlusion levels. Our findings show that participants took almost the same time to brush curves using all the interaction modes. They could draw smoother curves using the on-surface interaction modes compared to the no-haptic mode. However, the assisted on-surface mode provided better accuracy than the on-surface mode. The on-surface mode was slower in point localization, but the accuracy depended on the visual cues and occlusions associated with the tasks. Finally, we discuss participant feedback on using haptic force feedback as a tangible input modality and share takeaways to aid the design of haptics-based tangible interactions for surface visualizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04031v3</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TVCG.2024.3456316</arxiv:DOI>
      <dc:creator>Hamza Afzaal, Usman Alim</dc:creator>
    </item>
    <item>
      <title>Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment</title>
      <link>https://arxiv.org/abs/2409.05015</link>
      <description>arXiv:2409.05015v2 Announce Type: replace 
Abstract: Multimodal Emotion Recognition (MER) aims to automatically identify and understand human emotional states by integrating information from various modalities. However, the scarcity of annotated multimodal data significantly hinders the advancement of this research field. This paper presents our solution for the MER-SEMI sub-challenge of MER 2024. First, to better adapt acoustic modality features for the MER task, we experimentally evaluate the contributions of different layers of the pre-trained speech model HuBERT in emotion recognition. Based on these observations, we perform Parameter-Efficient Fine-Tuning (PEFT) on the layers identified as most effective for emotion recognition tasks, thereby achieving optimal adaptation for emotion recognition with a minimal number of learnable parameters. Second, leveraging the strengths of the acoustic modality, we propose a feature alignment pre-training method. This approach uses large-scale unlabeled data to train a visual encoder, thereby promoting the semantic alignment of visual features within the acoustic feature space. Finally, using the adapted acoustic features, aligned visual features, and lexical features, we employ an attention mechanism for feature fusion. On the MER2024-SEMI test set, the proposed method achieves a weighted F1 score of 88.90%, ranking fourth among all participating teams, validating the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05015v2</guid>
      <category>cs.HC</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, Lei Xie</dc:creator>
    </item>
    <item>
      <title>What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence</title>
      <link>https://arxiv.org/abs/2409.05731</link>
      <description>arXiv:2409.05731v2 Announce Type: replace 
Abstract: Explanations for autonomous vehicle (AV) decisions may build trust, however, explanations can contain errors. In a simulated driving study (n = 232), we tested how AV explanation errors, driving context characteristics (perceived harm and driving difficulty), and personal traits (prior trust and expertise) affected a passenger's comfort in relying on an AV, preference for control, confidence in the AV's ability, and explanation satisfaction. Errors negatively affected all outcomes. Surprisingly, despite identical driving, explanation errors reduced ratings of the AV's driving ability. Severity and potential harm amplified the negative impact of errors. Contextual harm and driving difficulty directly impacted outcome ratings and influenced the relationship between errors and outcomes. Prior trust and expertise were positively associated with outcome ratings. Results emphasize the need for accurate, contextually adaptive, and personalized AV explanations to foster trust, reliance, satisfaction, and confidence. We conclude with design, research, and deployment recommendations for trustworthy AV explanation systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05731v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert Kaufman, Aaron Broukhim, David Kirsh, Nadir Weibel</dc:creator>
    </item>
    <item>
      <title>PhishLang: A Lightweight, Client-Side Phishing Detection Framework using MobileBERT for Real-Time, Explainable Threat Mitigation</title>
      <link>https://arxiv.org/abs/2408.05667</link>
      <description>arXiv:2408.05667v2 Announce Type: replace-cross 
Abstract: In this paper, we introduce PhishLang, an open-source, lightweight language model specifically designed for phishing website detection through contextual analysis of the website. Unlike traditional heuristic or machine learning models that rely on static features and struggle to adapt to new threats, and deep learning models that are computationally intensive, our model leverages MobileBERT, a fast and memory-efficient variant of the BERT architecture, to learn granular features characteristic of phishing attacks. PhishLang operates with minimal data preprocessing and offers performance comparable to leading deep learning anti-phishing tools, while being significantly faster and less resource-intensive. Over a 3.5-month testing period, PhishLang successfully identified 25,796 phishing URLs, many of which were undetected by popular antiphishing blocklists, thus demonstrating its potential to enhance current detection measures. Capitalizing on PhishLang's resource efficiency, we release the first open-source fully client-side Chromium browser extension that provides inference locally without requiring to consult an online blocklist and can be run on low-end systems with no impact on inference times. Our implementation not only outperforms prevalent (server-side) phishing tools, but is significantly more effective than the limited commercial client-side measures available. Furthermore, we study how PhishLang can be integrated with GPT-3.5 Turbo to create explainable blocklisting -- which, upon detection of a website, provides users with detailed contextual information about the features that led to a website being marked as phishing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05667v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sayak Saha Roy, Shirin Nilizadeh</dc:creator>
    </item>
  </channel>
</rss>
