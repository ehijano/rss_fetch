<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Nov 2025 05:01:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Heuristic Solutions for the Best Secretary Problem</title>
      <link>https://arxiv.org/abs/2511.10206</link>
      <description>arXiv:2511.10206v1 Announce Type: new 
Abstract: This paper introduces a heuristic framework for the Best Secretary Problem, where one item must be selected using rank information only. We develop five data-responsive rules extending classical fixed-cutoff methods: an expected-record threshold, an adaptive deviation correction, a probabilistic early-accept rule, a two-phase relaxation, and a local dynamic programming approximation. These rules adjust thresholds sequentially as information accumulates. Simulations across diverse sample sizes, distributions, and autocorrelated settings show that the heuristics match or exceed traditional optimal rules in stability and efficiency. The expected-record rule remains strong despite its simplicity, the adaptive correction performs well under asymmetry, and the adaptive and probabilistic rules reduce average stopping times. An ensemble combining multiple rules yields the most stable performance. Overall, a few intuitive parameters achieve near-optimal results, demonstrating that data-responsive heuristics can effectively extend rank-based optimal stopping to dynamic decision environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10206v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eugene Seong</dc:creator>
    </item>
    <item>
      <title>Bayesian inference for precise and uncertainty-quantified single-shot widefield interferometric geometrical nanometrology</title>
      <link>https://arxiv.org/abs/2511.09666</link>
      <description>arXiv:2511.09666v1 Announce Type: cross 
Abstract: Advanced geometrical nanometrology is critical for process control in semiconductor manufacturing, supporting applications in, e.g., photonic integrated circuits, nanoelectronics, and emerging quantum and optoelectronic technologies. Widefield interferometric approach provide a cost-effective, non-destructive solution for characterizing semiconductor optical waveguides, which are fundamental to nanophotonic devices. This work presents a Bayesian inference framework, implemented using Dynamic Nested Sampling, for estimating geometric parameters - such as width and height - of a semiconductor optical waveguide from a single widefield interferogram. The proposed framework reduces the need of leveraging near field scanning microscopy methods for measurements. The notable advantage is that Bayesian statistics not only provide the estimated parameter values but also quantify the uncertainty of the inference results and the fitness of the used model. The proposed full-field, single-shot interferometric approach, supported by Bayesian-based data analysis, achieves high accuracy and sensitivity - down to successful measurement of 8 nm rib waveguide - while remaining resilient to noise. Thus, the demonstrated methodology provides a cost-effective, robust, and scalable tool for semiconductor fabrication monitoring and process verification, as confirmed by both numerical simulations and experimental validation on optical waveguides. This method contributes to high-precision nanometrology by integrating advanced statistical modeling and inference techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09666v1</guid>
      <category>physics.optics</category>
      <category>stat.AP</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Damian Suski, Maria Cywinska, Julianna Winnik, Michal Jozwik, Piotr Zdankowski, Azeem Ahmad, Balpreet S. Ahluwalia, Maciej Trusiak</dc:creator>
    </item>
    <item>
      <title>Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling</title>
      <link>https://arxiv.org/abs/2511.09722</link>
      <description>arXiv:2511.09722v1 Announce Type: cross 
Abstract: Minerals play a critical role in the advanced energy technologies necessary for decarbonization, but characterizing mineral deposits hidden underground remains costly and challenging. Inspired by recent progress in generative modeling, we develop a learning method which infers the locations of minerals by masking and infilling geospatial maps of resource availability. We demonstrate this technique using mineral data for the conterminous United States, and train performant models, with the best achieving Dice coefficients of $0.31 \pm 0.01$ and recalls of $0.22 \pm 0.02$ on test data at 1$\times$1 mi$^2$ spatial resolution. One major advantage of our approach is that it can easily incorporate auxiliary data sources for prediction which may be more abundant than mineral data. We highlight the capabilities of our model by adding input layers derived from geophysical sources, along with a nation-wide ground survey of soils originally intended for agronomic purposes. We find that employing such auxiliary features can improve inference performance, while also enabling model evaluation in regions with no recorded minerals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09722v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sujay Nair, Evan Coleman, Sherrie Wang, Elsa Olivetti</dc:creator>
    </item>
    <item>
      <title>Modelos Empiricos de Pos-Dupla Selecao por LASSO: Discussoes para Estudos do Transporte Aereo</title>
      <link>https://arxiv.org/abs/2511.09767</link>
      <description>arXiv:2511.09767v1 Announce Type: cross 
Abstract: This paper presents and discusses forms of estimation by regularized regression and model selection using the LASSO method - Least Absolute Shrinkage and Selection Operator. LASSO is recognized as one of the main supervised learning methods applied to high-dimensional econometrics, allowing work with large volumes of data and multiple correlated controls. Conceptual issues related to the consequences of high dimensionality in modern econometrics and the principle of sparsity, which underpins regularization procedures, are addressed. The study examines the main post-double selection and post-regularization models, including variations applied to instrumental variable models. A brief description of the lassopack routine package, its syntaxes, and examples of HD, HDS (High-Dimension Sparse), and IV-HDS models, with combinations involving fixed effects estimators, is also presented. Finally, the potential application of the approach in research focused on air transport is discussed, with emphasis on an empirical study on the operational efficiency of airlines and aircraft fuel consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09767v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>econ.GN</category>
      <category>eess.SY</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.17594515</arxiv:DOI>
      <arxiv:journal_reference>Communications in Airline Economics Research, 201717804h, 2021</arxiv:journal_reference>
      <dc:creator>Alessandro V. M. Oliveira</dc:creator>
    </item>
    <item>
      <title>A Clustering Approach for Basket Trials Based on Treatment Response Trajectories</title>
      <link>https://arxiv.org/abs/2511.09890</link>
      <description>arXiv:2511.09890v1 Announce Type: cross 
Abstract: Heterogeneity in efficacy is sometimes observed across baskets in basket trials. In this study, we propose a model-free clustering framework that groups baskets based on transition probabilities derived from the trajectories of treatment response, rather than relying solely on a single efficacy endpoint such as the objective response rate. The number of clusters is not predetermined but is automatically determined in a data-driven manner based on the similarity structure among baskets. After clustering, baskets within the same cluster are analyzed using a hierarchical Bayesian model. This framework aims to improve the estimation precision of efficacy endpoints and enhance statistical power while maintaining the type~I error rate at the nominal level. The performance of the proposed method was evaluated through simulation studies. The results demonstrated that the proposed method can accurately identify cluster structures in heterogeneous settings and, even under such conditions, maintain the type~I error rate at the nominal level while improving statistical power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09890v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masahiro Kojima, Keisuke Hanada, Atsuya Sato</dc:creator>
    </item>
    <item>
      <title>A tutorial for propensity score weighting methods under violations of the positivity assumption</title>
      <link>https://arxiv.org/abs/2511.10077</link>
      <description>arXiv:2511.10077v1 Announce Type: cross 
Abstract: Violations of the positivity assumption can render conventional causal estimands unidentifiable, including the average treatment effect (ATE), the average treatment effect on the treated (ATT), and the average treatment effect on the controls (ATC). Shifting the inferential focus to their alternative counterparts -- the weighted ATE (WATE), the weighted ATT (WATT), and the weighted ATC (WATC) -- offers valuable insights into treatment effects while preserving internal validity. In this tutorial, we provide a comprehensive review of recent advances in propensity score (PS) weighting methods, along with practical guidance on how to select a primary target estimand (while other estimands serve as supplementary analyses), implement the corresponding PS-weighted estimators, and conduct post-weighting diagnostic assessments. The tutorial is accompanied by a user-friendly R package, ChiPS. We demonstrate the pertinence of various estimators through extensive simulation studies. We illustrate the flow of the tutorial on two real-world case studies: (i) Effect of smoking on blood lead level using data from the 2007-2008 National Health and Nutrition Examination Survey (NHANES); and (ii) Impact of history of sex work on HIV status among transgender women in South Africa.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10077v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Liu, Yuan Wang, Ying Gao, Tonia Poteat, Roland A. Matsouaka</dc:creator>
    </item>
    <item>
      <title>Estimating the true number of principal components under the random design</title>
      <link>https://arxiv.org/abs/2511.10419</link>
      <description>arXiv:2511.10419v1 Announce Type: cross 
Abstract: Principal component analysis (PCA) is frequently employed as a dimension reduction tool when the number of covariates is large. However, the number of principal components to be retained in PCA is typically determined in a researcher-dependent manner. To mitigate the subjectivity in PCA, this paper proposes a data-driven testing procedure to estimate the number of underlying principal components. While existing work such as G'Sell et al. (2016), Taylor et al. (2016) and Choi et al. (2017) discuss similar tests under fixed design, this paper investigates an extension of their framework to a more general econometric setup with the random design. The proposed test is proved to achieve asymptotically exact type 1 error controls under a locally defined null hypothesis, with simulation examples indicating an asymptotic validity of our test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10419v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yasuyuki Matsumura</dc:creator>
    </item>
    <item>
      <title>Two Americas of Well-Being: Divergent Rural-Urban Patterns of Life Satisfaction and Happiness from 2.6 B Social Media Posts</title>
      <link>https://arxiv.org/abs/2511.10542</link>
      <description>arXiv:2511.10542v1 Announce Type: cross 
Abstract: Using 2.6 billion geolocated social-media posts (2014-2022) and a fine-tuned generative language model, we construct county-level indicators of life satisfaction and happiness for the United States. We document an apparent rural-urban paradox: rural counties express higher life satisfaction while urban counties exhibit greater happiness. We reconcile this by treating the two as distinct layers of subjective well-being, evaluative vs. hedonic, showing that each maps differently onto place, politics, and time. Republican-leaning areas appear more satisfied in evaluative terms, but partisan gaps in happiness largely flatten outside major metros, indicating context-dependent political effects. Temporal shocks dominate the hedonic layer: happiness falls sharply during 2020-2022, whereas life satisfaction moves more modestly. These patterns are robust across logistic and OLS specifications and align with well-being theory. Interpreted as associations for the population of social-media posts, the results show that large-scale, language-based indicators can resolve conflicting findings about the rural-urban divide by distinguishing the type of well-being expressed, offering a transparent, reproducible complement to traditional surveys.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10542v1</guid>
      <category>cs.SI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefano Maria Iacus, Giuseppe Porro</dc:creator>
    </item>
    <item>
      <title>Transportability of Prognostic Markers: Rethinking Common Practices through a Sufficient-Component-Cause Perspective</title>
      <link>https://arxiv.org/abs/2511.04065</link>
      <description>arXiv:2511.04065v2 Announce Type: replace 
Abstract: Transportability, the ability to maintain performance across populations, is a desirable property of of markers of clinical outcomes. However, empirical findings indicate that markers often exhibit varying performances across populations. For prognostic markers that are advertised as predictive risk equations, oftentimes a form of updating is required when the equation is transported to populations with different disease prevalences. Here, we revisit transportability of prognostic markers through the lens of the foundational framework of sufficient component causes (SCC). We argue that transporting a marker "as is" implicitly assumes predictive values are transportable, whereas conventional prevalence adjustment shifts the locus of transportability to accuracy metrics (sensitivity and specificity). Using a minimalist SCC framework that decomposes risk prediction into its causal constituents, we show that both approaches rely on strong assumptions about the stability of cause distributions. A SCC framework instead invites making transparent assumptions about how different causes vary across populations, leading to different transportation methods. For example, in the absence of any external information other than disease prevalence, a cause-neutral perspective can assume all causes are responsible for change in prevalence, leading to a new form of marker transportation. Numerical experiments demonstrate that different transportability assumptions lead to varying degrees of information loss, depending on the distribution of causes. A SCC perspective challenges common assumptions and practices for marker transportability, and proposes transportation algorithms that reflect our knowledge or assumptions about how causes vary across populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04065v2</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Sadatsafavi, Gavin Pereira, Wenjia Chen</dc:creator>
    </item>
    <item>
      <title>On the Estimation of Climate Normals and Anomalies</title>
      <link>https://arxiv.org/abs/2511.05071</link>
      <description>arXiv:2511.05071v2 Announce Type: replace 
Abstract: The quantification of the interannual component of variability in climatological time series is essential for the assessment and prediction of the El Ni\~{n}o - Southern Oscillation phenomenon. This is achieved by estimating the deviation of a climate variable (e.g., temperature, pressure, precipitation, or wind strength) from its normal conditions, defined by its baseline level and seasonal patterns. Climate normals are currently estimated by simple arithmetic averages calculated over the most recent 30-year period ending in a year divisible by 10. The suitability of the standard methodology has been questioned in the context of a changing climate, characterized by nonstationary conditions. The literature has focused on the choice of the bandwidth and the ability to account for trends induced by climate change. The paper contributes to the literature by proposing a regularized real time filter based on local trigonometric regression, optimizing the estimation bias-variance trade-off in the presence of climate change, and by introducing a class of seasonal kernels enhancing the localization of the estimates of climate normals. Application to sea surface temperature series in the Ni\~{n}o 3.4 region and zonal and trade winds strength in the equatorial and tropical Pacific region, illustrates the relevance of our proposal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05071v2</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tommaso Proietti, Alessandro Giovannelli</dc:creator>
    </item>
    <item>
      <title>Latent Knowledge-Guided Video Diffusion for Scientific Phenomena Generation from a Single Initial Frame</title>
      <link>https://arxiv.org/abs/2411.11343</link>
      <description>arXiv:2411.11343v2 Announce Type: replace-cross 
Abstract: Video diffusion models have achieved impressive results in natural scene generation, yet they struggle to generalize to scientific phenomena such as fluid simulations and meteorological processes, where underlying dynamics are governed by scientific laws. These tasks pose unique challenges, including severe domain gaps, limited training data, and the lack of descriptive language annotations. To handle this dilemma, we extracted the latent scientific phenomena knowledge and further proposed a fresh framework that teaches video diffusion models to generate scientific phenomena from a single initial frame. Particularly, static knowledge is extracted via pre-trained masked autoencoders, while dynamic knowledge is derived from pre-trained optical flow prediction. Subsequently, based on the aligned spatial relations between the CLIP vision and language encoders, the visual embeddings of scientific phenomena, guided by latent scientific phenomena knowledge, are projected to generate the pseudo-language prompt embeddings in both spatial and frequency domains. By incorporating these prompts and fine-tuning the video diffusion model, we enable the generation of videos that better adhere to scientific laws. Extensive experiments on both computational fluid dynamics simulations and real-world typhoon observations demonstrate the effectiveness of our approach, achieving superior fidelity and consistency across diverse scientific scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11343v2</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qinglong Cao, Xirui Li, Ding Wang, Chao Ma, Yuntian Chen, Xiaokang Yang</dc:creator>
    </item>
    <item>
      <title>While-alive regression analysis of composite survival endpoints</title>
      <link>https://arxiv.org/abs/2504.21710</link>
      <description>arXiv:2504.21710v3 Announce Type: replace-cross 
Abstract: Composite endpoints, which combine two or more distinct outcomes, are frequently used in clinical trials to enhance the event rate and improve the statistical power. In the recent literature, the while-alive cumulative frequency measure offers a strong alternative to define composite survival outcomes, by relating the average event rate to the survival time. Although non-parametric methods have been proposed for two-sample comparisons between cumulative frequency measures in clinical trials, limited attention has been given to regression methods that directly address time-varying effects in while-alive measures for composite survival outcomes. Motivated by an individually randomized trial (HF-ACTION) and a cluster randomized trial (STRIDE), we address this gap by developing a regression framework for while-alive measures for composite survival outcomes that include a terminal component event. Our regression approach uses splines to model time-varying association between covariates and a while-alive loss rate of all component events, and can be applied to both independent and clustered data. We derive the asymptotic properties of the regression estimator in each setting and evaluate its performance through simulations. Finally, we apply our regression method to analyze data from the HF-ACTION individually randomized trial and the STRIDE cluster randomized trial. The proposed methods are implemented in the WAreg R package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21710v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xi Fang, Hajime Uno, Fan Li</dc:creator>
    </item>
    <item>
      <title>Prediction of linear fractional stable motions using codifference, with application to non-Gaussian rough volatility</title>
      <link>https://arxiv.org/abs/2507.15437</link>
      <description>arXiv:2507.15437v2 Announce Type: replace-cross 
Abstract: The linear fractional stable motion (LFSM) extends the fractional Brownian motion (fBm) by considering $\alpha$-stable increments. We propose a method to forecast future increments of the LFSM from past discrete-time observations, using the conditional expectation when $\alpha&gt;1$ or a semimetric projection otherwise. It relies on the codifference, which describes the serial dependence of the process, instead of the covariance. Indeed, covariance is commonly used for predicting an fBm but it is infinite when $\alpha&lt;2$. Some theoretical properties of the method and of its accuracy are studied and both a simulation study and an application to real data confirm the relevance of the approach. The LFSM-based method outperforms the fBm, when forecasting high-frequency FX rates. It also shows a promising performance in the forecast of time series of volatilities, decomposing properly, in the fractal dynamic of rough volatilities, the contribution of the kurtosis of the increments and the contribution of their serial dependence. Moreover, the analysis of hit ratios suggests that, beside independence, persistence, and antipersistence, a fourth regime of serial dependence exists for fractional processes, characterized by a selective memory controlled by a few large increments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15437v2</guid>
      <category>stat.ME</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Matthieu Garcin, Karl Sawaya, Thomas Valade</dc:creator>
    </item>
  </channel>
</rss>
