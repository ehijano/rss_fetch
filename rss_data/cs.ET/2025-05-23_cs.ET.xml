<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 May 2025 04:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Simulation-Guided Approximate Logic Synthesis Under the Maximum Error Constraint</title>
      <link>https://arxiv.org/abs/2505.16769</link>
      <description>arXiv:2505.16769v1 Announce Type: new 
Abstract: Approximate computing is an effective computing paradigm for improving energy efficiency of error-tolerant applications. Approximate logic synthesis (ALS) is an automatic process to generate approximate circuits with reduced area, delay, and power, while satisfying user-specified error constraints. This paper focuses on ALS under the maximum error constraint. As an essential error metric that provides a worst-case error guarantee, the maximum error is crucial for many applications such as image processing and machine learning. This work proposes an efficient simulation-guided ALS flow that handles this constraint. It utilizes logic simulation to 1) prune local approximate changes (LACs) with large errors that violate the error constraint, and 2) accelerate the SAT-based LAC selection process. Furthermore, to enhance scalability, our ALS flow iteratively selects a set of promising LACs satisfying the error constraint to improve the efficiency. The experimental results show that compared with the state-of-the-art method, our ALS flow accelerates by 30.6 times, and further reduces circuit area and delay by 18.2% and 4.9%, respectively. Notably, our flow scales to large EPFL benchmarks with up to 38540 nodes, which cannot be handled by any existing ALS method for maximum error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16769v1</guid>
      <category>cs.ET</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chang Meng, Weikang Qian, Giovanni De Micheli</dc:creator>
    </item>
    <item>
      <title>Dynamic Reservoir Computing with Physical Neuromorphic Networks</title>
      <link>https://arxiv.org/abs/2505.16813</link>
      <description>arXiv:2505.16813v1 Announce Type: new 
Abstract: Reservoir Computing (RC) with physical systems requires an understanding of the underlying structure and internal dynamics of the specific physical reservoir. In this study, physical nano-electronic networks with neuromorphic dynamics are investigated for their use as physical reservoirs in an RC framework. These neuromorphic networks operate as dynamic reservoirs, with node activities in general coupled to the edge dynamics through nonlinear nano-electronic circuit elements, and the reservoir outputs influenced by the underlying network connectivity structure. This study finds that networks with varying degrees of sparsity generate more useful nonlinear temporal outputs for dynamic RC compared to dense networks. Dynamic RC is also tested on an autonomous multivariate chaotic time series prediction task with networks of varying densities, which revealed the importance of network sparsity in maintaining network activity and overall dynamics, that in turn enabled the learning of the chaotic Lorenz63 system's attractor behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16813v1</guid>
      <category>cs.ET</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinhao Xu, Georg A. Gottwald, Zdenka Kuncic</dc:creator>
    </item>
    <item>
      <title>Integration of TinyML and LargeML: A Survey of 6G and Beyond</title>
      <link>https://arxiv.org/abs/2505.15854</link>
      <description>arXiv:2505.15854v1 Announce Type: cross 
Abstract: The transition from 5G networks to 6G highlights a significant demand for machine learning (ML). Deep learning models, in particular, have seen wide application in mobile networking and communications to support advanced services in emerging wireless environments, such as smart healthcare, smart grids, autonomous vehicles, aerial platforms, digital twins, and the metaverse. The rapid expansion of Internet-of-Things (IoT) devices, many with limited computational capabilities, has accelerated the development of tiny machine learning (TinyML) and resource-efficient ML approaches for cost-effective services. However, the deployment of large-scale machine learning (LargeML) solutions require major computing resources and complex management strategies to support extensive IoT services and ML-generated content applications. Consequently, the integration of TinyML and LargeML is projected as a promising approach for future seamless connectivity and efficient resource management.
  Although the integration of TinyML and LargeML shows abundant potential, several challenges persist, including performance optimization, practical deployment strategies, effective resource management, and security considerations. In this survey, we review and analyze the latest research aimed at enabling the integration of TinyML and LargeML models for the realization of smart services and applications in future 6G networks and beyond. The paper concludes by outlining critical challenges and identifying future research directions for the holistic integration of TinyML and LargeML in next-generation wireless networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15854v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thai-Hoc Vu, Ngo Hoang Tu, Thien Huynh-The, Kyungchun Lee, Sunghwan Kim, Miroslav Voznak, Quoc-Viet Pham</dc:creator>
    </item>
    <item>
      <title>Advanced Integration Strategies for ESD Protection and Termination in High-Speed LVDS Systems</title>
      <link>https://arxiv.org/abs/2505.16200</link>
      <description>arXiv:2505.16200v1 Announce Type: cross 
Abstract: This technical article explores comprehensive strategies for integrating Electrostatic Discharge (ESD) protection diodes and termination resistors in LowVoltage Differential Signaling (LVDS) designs. The article examines critical aspects of protection mechanisms, design considerations, impedance matching, and placement optimization techniques. Through detailed analysis of layout considerations and advanced design strategies, the article presents solutions for common integration challenges. It emphasizes the importance of signal integrity maintenance and protection effectiveness while providing practical guidelines for implementing robust LVDS systems. Various methodologies for performance optimization and validation are discussed, offering designers a thorough framework for creating reliable high-speed digital systems that balance protection requirements with signal integrity demands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16200v1</guid>
      <category>cs.AR</category>
      <category>cs.CE</category>
      <category>cs.ET</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.56726/IJRCAIT.2025.08.01.199</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Research in Computer Applications and Information Technology Vol. 8, Issue 1, 2025</arxiv:journal_reference>
      <dc:creator>Kavya Gaddipati</dc:creator>
    </item>
    <item>
      <title>Is Circuit Depth Accurate for Comparing Quantum Circuit Runtimes?</title>
      <link>https://arxiv.org/abs/2505.16908</link>
      <description>arXiv:2505.16908v1 Announce Type: cross 
Abstract: Although quantum circuit depth is commonly used to estimate differences in circuit runtimes, it overlooks a prevailing trait of current hardware implementation: different gates have different execution times. Consequently, the use of depth may lead to inaccurate comparisons of circuit runtimes, especially for circuits of similar scale. In this paper, we introduce an alternative metric, gate-aware depth, that uses unique gate weights, and investigate how its accuracy in comparing circuit runtimes compares to the existing metrics of traditional and multi-qubit circuit depth. To do so, we compiled a suite of 15 practical circuits using different algorithms and compared depths and runtimes between the compiled versions to determine how accurately the size of the change in depth approximated the size of the change in runtime, and how accurately the order of circuits by depth matched their order by runtime. When approximating the size of runtime changes, gate-aware depth decreased the approximation error by an average of 412 times relative to traditional depth and 124 times relative to multi-qubit depth. When matching the order of true runtimes, gate-aware depth achieved the highest accuracy on all devices and a perfect accuracy of 100% on five out of six devices. Additionally, we show that the optimal weights needed to achieve these accuracy improvements can be easily calculated using device gate times, and provide good general weight values for the IBM Eagle and Heron architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16908v1</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew Tremba, Ji Liu, Paul Hovland</dc:creator>
    </item>
    <item>
      <title>What is Cognitive Computing? An Architecture and State of The Art</title>
      <link>https://arxiv.org/abs/2301.00882</link>
      <description>arXiv:2301.00882v2 Announce Type: replace 
Abstract: Cognitive Computing (COC) aims to build highly cognitive machines with low computational resources that respond in real-time. However, scholarly literature shows varying research areas and various interpretations of COC. This calls for a cohesive architecture that delineates the nature of COC. We argue that if Herbert Simon considered the design science is the science of artificial, cognitive systems are the products of cognitive science or 'the newest science of the artificial'. Therefore, building a conceptual basis for COC is an essential step into prospective cognitive computing-based systems. This paper proposes an architecture of COC through analyzing the literature on COC using a myriad of statistical analysis methods. Then, we compare the statistical analysis results with previous qualitative analysis results to confirm our findings. The study also comprehensively surveys the recent research on COC to identify the state of the art and connect the advances in varied research disciplines in COC. The study found that there are three underlaying computing paradigms, Von-Neuman, Neuromorphic Engineering and Quantum Computing, that comprehensively complement the structure of cognitive computation. The research discuss possible applications and open research directions under the COC umbrella.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.00882v2</guid>
      <category>cs.ET</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samaa Elnagar, Manoj A. Thomas, Kweku-Muata Osei-Bryson</dc:creator>
    </item>
    <item>
      <title>Transforming the Hybrid Cloud for Emerging AI Workloads</title>
      <link>https://arxiv.org/abs/2411.13239</link>
      <description>arXiv:2411.13239v2 Announce Type: replace-cross 
Abstract: This white paper, developed through close collaboration between IBM Research and UIUC researchers within the IIDAI Institute, envisions transforming hybrid cloud systems to meet the growing complexity of AI workloads through innovative, full-stack co-design approaches, emphasizing usability, manageability, affordability, adaptability, efficiency, and scalability. By integrating cutting-edge technologies such as generative and agentic AI, cross-layer automation and optimization, unified control plane, and composable and adaptive system architecture, the proposed framework addresses critical challenges in energy efficiency, performance, and cost-effectiveness. Incorporating quantum computing as it matures will enable quantum-accelerated simulations for materials science, climate modeling, and other high-impact domains. Collaborative efforts between academia and industry are central to this vision, driving advancements in foundation models for material design and climate solutions, scalable multimodal data processing, and enhanced physics-based AI emulators for applications like weather forecasting and carbon sequestration. Research priorities include advancing AI agentic systems, LLM as an Abstraction (LLMaaA), AI model optimization and unified abstractions across heterogeneous infrastructure, end-to-end edge-cloud transformation, efficient programming model, middleware and platform, secure infrastructure, application-adaptive cloud systems, and new quantum-classical collaborative workflows. These ideas and solutions encompass both theoretical and practical research questions, requiring coordinated input and support from the research community. This joint initiative aims to establish hybrid clouds as secure, efficient, and sustainable platforms, fostering breakthroughs in AI-driven applications and scientific discovery across academia, industry, and society.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13239v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <category>cs.MA</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deming Chen, Alaa Youssef, Ruchi Pendse, Andr\'e Schleife, Bryan K. Clark, Hendrik Hamann, Jingrui He, Teodoro Laino, Lav Varshney, Yuxiong Wang, Avirup Sil, Reyhaneh Jabbarvand, Tianyin Xu, Volodymyr Kindratenko, Carlos Costa, Sarita Adve, Charith Mendis, Minjia Zhang, Santiago N\'u\~nez-Corrales, Raghu Ganti, Mudhakar Srivatsa, Nam Sung Kim, Josep Torrellas, Jian Huang, Seetharami Seelam, Klara Nahrstedt, Tarek Abdelzaher, Tamar Eilam, Huimin Zhao, Matteo Manica, Ravishankar Iyer, Martin Hirzel, Vikram Adve, Darko Marinov, Hubertus Franke, Hanghang Tong, Elizabeth Ainsworth, Han Zhao, Deepak Vasisht, Minh Do, Sahil Suneja, Fabio Oliveira, Giovanni Pacifici, Ruchir Puri, Priya Nagpurkar</dc:creator>
    </item>
  </channel>
</rss>
